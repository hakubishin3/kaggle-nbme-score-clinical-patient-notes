{"cells":[{"cell_type":"markdown","id":"colored-security","metadata":{"id":"colored-security"},"source":["## References"]},{"cell_type":"markdown","id":"educational-operator","metadata":{"id":"educational-operator"},"source":["- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"]},{"cell_type":"markdown","id":"incorrect-greek","metadata":{"id":"incorrect-greek"},"source":["## Configurations"]},{"cell_type":"code","execution_count":1,"id":"alive-granny","metadata":{"id":"alive-granny","executionInfo":{"status":"ok","timestamp":1649167960883,"user_tz":-540,"elapsed":11,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["EXP_NAME = \"nbme-exp074\"\n","ENV = \"colab\"\n","DEBUG_MODE = False\n","SUBMISSION_MODE = False"]},{"cell_type":"code","execution_count":2,"id":"heavy-prophet","metadata":{"id":"heavy-prophet","executionInfo":{"status":"ok","timestamp":1649167960883,"user_tz":-540,"elapsed":11,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["class CFG:\n","    env=ENV\n","    exp_name=EXP_NAME\n","    debug=DEBUG_MODE\n","    submission=SUBMISSION_MODE\n","    apex=True\n","    input_dir=None\n","    output_dir=None\n","    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n","    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n","    competition_name=\"nbme-score-clinical-patient-notes\"\n","    id_col=\"id\"\n","    target_col=\"location\"\n","    pretrained_model_name=\"microsoft/deberta-v3-large\"\n","    tokenizer=None\n","    max_len=None\n","    output_dim=1\n","    dropout=0.2\n","    num_workers=4\n","    batch_size=3\n","    lr=2e-5\n","    betas=(0.9, 0.98)\n","    weight_decay=0.1\n","    num_warmup_steps_rate=0.1\n","    batch_scheduler=True\n","    epochs=5\n","    n_fold=4\n","    train_fold=[0, 1, 2, 3]\n","    seed=71\n","    gradient_accumulation_steps=2\n","    max_grad_norm=1000\n","    print_freq=100\n","    train=True\n","    inference=True"]},{"cell_type":"code","execution_count":3,"id":"vocational-coating","metadata":{"id":"vocational-coating","executionInfo":{"status":"ok","timestamp":1649167960884,"user_tz":-540,"elapsed":11,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.train_fold = [0, 1]\n","\n","if CFG.submission:\n","    CFG.train = False\n","    CFG.inference = True"]},{"cell_type":"markdown","id":"private-moderator","metadata":{"id":"private-moderator"},"source":["## Directory Settings"]},{"cell_type":"code","execution_count":4,"id":"married-tokyo","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"married-tokyo","outputId":"b267b750-eba2-4a8b-f834-62e6ad35ea81","executionInfo":{"status":"ok","timestamp":1649167973175,"user_tz":-540,"elapsed":12302,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["colab\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: transformers==4.16.2 in /usr/local/lib/python3.7/dist-packages (4.16.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (6.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (3.6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (21.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (0.4.0)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (0.11.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.63.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (0.0.49)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (1.21.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2019.12.20)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.16.2) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.16.2) (3.7.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (1.24.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (1.15.0)\n"]}],"source":["import sys\n","from pathlib import Path\n","\n","\n","print(CFG.env)\n","if CFG.env == \"colab\":\n","    # colab環境\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","    # install packages\n","    !pip install transformers==4.16.2\n","    !pip install -q sentencepiece==0.1.96\n","\n","elif CFG.env == \"local\":\n","    # ローカルサーバ\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","\n","elif CFG.env == \"kaggle\":\n","    # kaggle環境\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./\")"]},{"cell_type":"code","source":["# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n","# This must be done before importing transformers\n","import shutil\n","from pathlib import Path\n","\n","if CFG.env == \"colab\":\n","    input_dir = Path(\"./drive/MyDrive/00.kaggle/input/deberta-v2-3-fast-tokenizer\")\n","    transformers_path = Path(\"/usr/local/lib/python3.7/dist-packages/transformers\")\n","else:\n","    input_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n","    transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n","\n","convert_file = input_dir / \"convert_slow_tokenizer.py\"\n","conversion_path = transformers_path/convert_file.name\n","\n","if conversion_path.exists():\n","    conversion_path.unlink()\n","\n","shutil.copy(convert_file, transformers_path)\n","deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n","\n","for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n","    filepath = deberta_v2_path/filename\n","    if filepath.exists():\n","        filepath.unlink()\n","\n","    shutil.copy(input_dir/filename, filepath)\n","    \n","    \n","from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast"],"metadata":{"id":"cnGM_g9c3WJW","executionInfo":{"status":"ok","timestamp":1649167975506,"user_tz":-540,"elapsed":2335,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"id":"cnGM_g9c3WJW","execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"id":"blank-pierre","metadata":{"id":"blank-pierre","executionInfo":{"status":"ok","timestamp":1649167977738,"user_tz":-540,"elapsed":2235,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["import gc\n","import os\n","import ast\n","import time\n","import math\n","import random\n","import itertools\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","from scipy.optimize import minimize\n","from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as T\n","from torchvision.io import read_image\n","from torch.utils.data import DataLoader, Dataset\n","\n","from transformers import AutoModelForMaskedLM\n","from transformers import BartModel,BertModel,BertTokenizer\n","from transformers import DebertaModel,DebertaTokenizer\n","from transformers import RobertaModel,RobertaTokenizer\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","id":"sound-still","metadata":{"id":"sound-still"},"source":["## Utilities"]},{"cell_type":"code","execution_count":7,"id":"surprised-commercial","metadata":{"id":"surprised-commercial","executionInfo":{"status":"ok","timestamp":1649167977738,"user_tz":-540,"elapsed":3,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on binary arrays.\n","\n","    Args:\n","        preds (list of lists of ints): Predictions.\n","        truths (list of lists of ints): Ground truths.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    # Micro : aggregating over all instances\n","    preds = np.concatenate(preds)\n","    truths = np.concatenate(truths)\n","    return f1_score(truths, preds)\n","\n","\n","def spans_to_binary(spans, length=None):\n","    \"\"\"\n","    Converts spans to a binary array indicating whether each character is in the span.\n","\n","    Args:\n","        spans (list of lists of two ints): Spans.\n","\n","    Returns:\n","        np array [length]: Binarized spans.\n","    \"\"\"\n","    length = np.max(spans) if length is None else length\n","    binary = np.zeros(length)\n","    for start, end in spans:\n","        binary[start:end] = 1\n","    return binary\n","\n","\n","def span_micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on spans.\n","\n","    Args:\n","        preds (list of lists of two ints): Prediction spans.\n","        truths (list of lists of two ints): Ground truth spans.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    bin_preds = []\n","    bin_truths = []\n","    for pred, truth in zip(preds, truths):\n","        if not len(pred) and not len(truth):\n","            continue\n","        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n","        bin_preds.append(spans_to_binary(pred, length))\n","        bin_truths.append(spans_to_binary(truth, length))\n","    return micro_f1(bin_preds, bin_truths)\n","\n","\n","def get_score(y_true, y_pred):\n","    score = span_micro_f1(y_true, y_pred)\n","    return score"]},{"cell_type":"code","execution_count":8,"id":"interstate-accident","metadata":{"id":"interstate-accident","executionInfo":{"status":"ok","timestamp":1649167978148,"user_tz":-540,"elapsed":413,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def create_labels_for_scoring(df):\n","    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n","    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n","    for i in range(len(df)):\n","        lst = df.loc[i, \"location\"]\n","        if lst:\n","            new_lst = \";\".join(lst)\n","            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n","\n","    # create labels\n","    truths = []\n","    for location_list in df[\"location_for_create_labels\"].values:\n","        truth = []\n","        if len(location_list) > 0:\n","            location = location_list[0]\n","            for loc in [s.split() for s in location.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                truth.append([start, end])\n","        truths.append(truth)\n","\n","    return truths\n","\n","\n","def get_char_probs(texts, token_probs, tokenizer):\n","    res = [np.zeros(len(t)) for t in texts]\n","    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n","        encoded = tokenizer(\n","            text=text,\n","            max_length=CFG.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n","            start, end = offset_mapping\n","            res[i][start:end] = pred\n","    return res\n","\n","\n","def get_predicted_location_str(char_probs, th=0.5):\n","    results = []\n","    for char_prob in char_probs:\n","        result = np.where(char_prob >= th)[0] + 1\n","        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n","        result = [f\"{min(r)} {max(r)}\" for r in result]\n","        result = \";\".join(result)\n","        results.append(result)\n","    return results\n","\n","\n","def get_predictions(results):\n","    predictions = []\n","    for result in results:\n","        prediction = []\n","        if result != \"\":\n","            for loc in [s.split() for s in result.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                prediction.append([start, end])\n","        predictions.append(prediction)\n","    return predictions\n","\n","\n","def scoring(df, th=0.5):\n","    labels = create_labels_for_scoring(df)\n","\n","    token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n","    char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n","    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n","    preds = get_predictions(predicted_location_str)\n","\n","    score = get_score(labels, preds)\n","    return score\n","\n","\n","def get_best_thres(oof_df):\n","    def f1_opt(x):\n","        return -1 * scoring(oof_df, th=x)\n","\n","    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n","    return best_thres"]},{"cell_type":"code","execution_count":9,"id":"coated-pioneer","metadata":{"id":"coated-pioneer","executionInfo":{"status":"ok","timestamp":1649167978149,"user_tz":-540,"elapsed":3,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return \"%dm %ds\" % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":10,"id":"nervous-delaware","metadata":{"id":"nervous-delaware","executionInfo":{"status":"ok","timestamp":1649167978149,"user_tz":-540,"elapsed":3,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["seed_everything()"]},{"cell_type":"markdown","id":"functioning-destruction","metadata":{"id":"functioning-destruction"},"source":["## Data Loading"]},{"cell_type":"code","execution_count":11,"id":"global-monte","metadata":{"id":"global-monte","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649167978562,"user_tz":-540,"elapsed":416,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"ab0853ed-0672-47c6-fa62-b6f516643a41"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14300, 6), (143, 3), (42146, 3), (5, 4))"]},"metadata":{},"execution_count":11}],"source":["train = pd.read_csv(CFG.input_dir / \"train.csv\")\n","features = pd.read_csv(CFG.input_dir / \"features.csv\")\n","patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n","test = pd.read_csv(CFG.input_dir / \"test.csv\")\n","\n","train.shape, features.shape, patient_notes.shape, test.shape"]},{"cell_type":"code","execution_count":12,"id":"independent-airfare","metadata":{"id":"independent-airfare","executionInfo":{"status":"ok","timestamp":1649167978563,"user_tz":-540,"elapsed":5,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["if CFG.debug:\n","    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n","    print(train.shape)"]},{"cell_type":"markdown","id":"silent-locator","metadata":{"id":"silent-locator"},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":13,"id":"unusual-fifty","metadata":{"id":"unusual-fifty","executionInfo":{"status":"ok","timestamp":1649167978563,"user_tz":-540,"elapsed":4,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def preprocess_features(features):\n","    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n","    return features\n","\n","\n","features = preprocess_features(features)"]},{"cell_type":"code","execution_count":14,"id":"decreased-mustang","metadata":{"id":"decreased-mustang","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649167978563,"user_tz":-540,"elapsed":4,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"40b3d1ea-62db-48a4-f29c-765f734d6780"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14300, 8), (5, 6))"]},"metadata":{},"execution_count":14}],"source":["train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","\n","train.shape, test.shape"]},{"cell_type":"code","execution_count":15,"id":"boolean-trade","metadata":{"id":"boolean-trade","executionInfo":{"status":"ok","timestamp":1649167979017,"user_tz":-540,"elapsed":457,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n","train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"]},{"cell_type":"code","execution_count":16,"id":"accomplished-dakota","metadata":{"id":"accomplished-dakota","colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"status":"ok","timestamp":1649167979017,"user_tz":-540,"elapsed":8,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"336e17ae-3de8-4da7-f5e1-562e78001c3d"},"outputs":[{"output_type":"display_data","data":{"text/plain":["0    4399\n","1    8181\n","2    1296\n","3     287\n","4      99\n","5      27\n","6       9\n","7       1\n","8       1\n","Name: annotation_length, dtype: int64"]},"metadata":{}}],"source":["train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n","display(train['annotation_length'].value_counts().sort_index())"]},{"cell_type":"markdown","id":"funded-elizabeth","metadata":{"id":"funded-elizabeth"},"source":["## CV split"]},{"cell_type":"code","execution_count":17,"id":"unexpected-columbia","metadata":{"id":"unexpected-columbia","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1649167979018,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"554af3b1-e735-49af-ccab-5049f81a578f"},"outputs":[{"output_type":"display_data","data":{"text/plain":["fold\n","0    3575\n","1    3575\n","2    3575\n","3    3575\n","dtype: int64"]},"metadata":{}}],"source":["Fold = GroupKFold(n_splits=CFG.n_fold)\n","groups = train['pn_num'].values\n","for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n","    train.loc[val_index, 'fold'] = int(n)\n","train['fold'] = train['fold'].astype(int)\n","display(train.groupby('fold').size())"]},{"cell_type":"markdown","id":"critical-archive","metadata":{"id":"critical-archive"},"source":["## Setup tokenizer"]},{"cell_type":"code","execution_count":18,"id":"broken-generator","metadata":{"id":"broken-generator","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649167987561,"user_tz":-540,"elapsed":8548,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"a357a551-ade8-4ee3-ca44-6045a438dd43"},"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["if CFG.submission:\n","    tokenizer = DebertaV2TokenizerFast.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n","else:\n","    tokenizer = DebertaV2TokenizerFast.from_pretrained(CFG.pretrained_model_name)\n","    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n","\n","CFG.tokenizer = tokenizer"]},{"cell_type":"markdown","id":"compatible-lincoln","metadata":{"id":"compatible-lincoln"},"source":["## Create dataset"]},{"cell_type":"code","execution_count":19,"id":"fluid-nancy","metadata":{"id":"fluid-nancy","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["b622999d368f462497b44a1bdde58026","c8364e7f36f341dd904a1a839ebff4ee","2f4c77eff3f54a288254be249755af0b","b9167e8982e6483a8f76de7f2dcd3a8f","fcf50e9fb2c845d9b6747df774017c4f","7eebd0e36ffd475dbd73997ec1246fd3","27fd3d0f816b4926a41b599682e722be","34152585b441449fb0a4f22ca9904b0d","21efec1049b44e4b84d1b00434d7cc33","8f96ac5b7aa045068e5b0f53c1535a19","af11b872c2c74af595c17e6a778b235c"]},"executionInfo":{"status":"ok","timestamp":1649168020556,"user_tz":-540,"elapsed":33010,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"366aeeef-c617-42ba-824a-bbf962127ec5"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/42146 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b622999d368f462497b44a1bdde58026"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 323\n"]}],"source":["pn_history_lengths = []\n","tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    pn_history_lengths.append(length)\n","\n","print(\"max length:\", np.max(pn_history_lengths))"]},{"cell_type":"code","execution_count":20,"id":"posted-humidity","metadata":{"id":"posted-humidity","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["e1f40cc06403408b82f6e023125309da","ed3ab8520f4b450cb89f2e900a7ac211","61ed9c24cd9a4f0d90d09fefa0ae9982","09ce14321a4e42988612d677e9665abb","e9c02a359c9340ccab2676b0f454bc96","d4173fce864e4662a2975810b5a28772","b34183983e454d78bde1f80442163188","ae57241309e247ce9405aee67ba4428b","177cae8328264c509e9bd392d589cd98","abc1a522591a49568ac9071c88218d78","e65fa2f37ae34c7ba14a631be783f549"]},"executionInfo":{"status":"ok","timestamp":1649168020557,"user_tz":-540,"elapsed":21,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"f1633b81-ba6a-45f6-d809-335506cfb068"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/143 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1f40cc06403408b82f6e023125309da"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 28\n"]}],"source":["feature_text_lengths = []\n","tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    feature_text_lengths.append(length)\n","\n","print(\"max length:\", np.max(feature_text_lengths))"]},{"cell_type":"code","execution_count":21,"id":"resistant-amount","metadata":{"id":"resistant-amount","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649168020557,"user_tz":-540,"elapsed":18,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"b8574f39-629d-4416-ab85-d17ad22b7ea7"},"outputs":[{"output_type":"stream","name":"stdout","text":["max length: 354\n"]}],"source":["CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n","\n","print(\"max length:\", CFG.max_len)"]},{"cell_type":"code","execution_count":22,"id":"august-equity","metadata":{"id":"august-equity","executionInfo":{"status":"ok","timestamp":1649168020558,"user_tz":-540,"elapsed":17,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["class TrainingDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","        self.annotation_lengths = self.df[\"annotation_length\"].values\n","        self.locations = self.df[\"location\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def _create_label(self, pn_history, annotation_length, location_list):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        offset_mapping = encoded[\"offset_mapping\"]\n","        ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n","        label = np.zeros(len(offset_mapping))\n","        label[ignore_idxes] = -1\n","\n","        if annotation_length > 0:\n","            for location in location_list:\n","                for loc in [s.split() for s in location.split(\";\")]:\n","                    start, end = int(loc[0]), int(loc[1])\n","                    start_idx = -1\n","                    end_idx = -1\n","                    for idx in range(len(offset_mapping)):\n","                        if (start_idx == -1) & (start < offset_mapping[idx][0]):\n","                            start_idx = idx - 1\n","                        if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n","                            end_idx = idx + 1\n","                    if start_idx == -1:\n","                        start_idx = end_idx\n","                    if (start_idx != -1) & (end_idx != -1):\n","                        label[start_idx:end_idx] = 1\n","\n","        return torch.tensor(label, dtype=torch.float)\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n","        return input_, label"]},{"cell_type":"code","execution_count":23,"id":"weird-interaction","metadata":{"id":"weird-interaction","executionInfo":{"status":"ok","timestamp":1649168020558,"user_tz":-540,"elapsed":16,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["class TestDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        return input_"]},{"cell_type":"markdown","id":"upper-mobility","metadata":{"id":"upper-mobility"},"source":["## Model"]},{"cell_type":"code","source":["from transformers.modeling_outputs import MaskedLMOutput\n","\n","class MaskedModel(nn.Module):\n","    def __init__(self, cfg, config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","        if config_path is None:\n","            self.config = AutoConfig.from_pretrained(\n","                cfg.pretrained_model_name,\n","                output_hidden_states=False\n","                )\n","        else:\n","            self.config = torch.load(config_path)\n","        \n","        if pretrained:\n","            self.model = AutoModel.from_pretrained(cfg.pretrained_model_name, config=self.config)\n","            self.lm_head = AutoModelForMaskedLM.from_pretrained(cfg.pretrained_model_name, config=self.config).cls # [cls, lm_head]\n","        else:\n","            self.model = AutoModel(self.config)\n","            self.lm_head = AutoModelForMaskedLM(self.config).cls # [cls, lm_head]\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def forward(\n","            self, \n","            input_ids=None,\n","            attention_mask=None,\n","            token_type_ids=None,\n","            #position_ids=None,\n","            inputs_embeds=None,\n","            labels=None,\n","            output_attentions=None,\n","            output_hidden_states=None,\n","            return_dict=None):\n","        \n","        outputs = self.model(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            #position_ids=position_ids,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,)\n","        \n","        sequence_output = outputs[0]\n","        prediction_scores = self.lm_head(sequence_output)\n","\n","        masked_lm_loss = None\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss()\n","            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n","\n","        return MaskedLMOutput(loss=masked_lm_loss,\n","                              logits=prediction_scores,\n","                              hidden_states=outputs.hidden_states,\n","                              attentions=outputs.attentions)"],"metadata":{"id":"a4HSgs6b8wQT","executionInfo":{"status":"ok","timestamp":1649168021088,"user_tz":-540,"elapsed":546,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"id":"a4HSgs6b8wQT","execution_count":24,"outputs":[]},{"cell_type":"code","execution_count":25,"id":"spanish-destruction","metadata":{"id":"spanish-destruction","executionInfo":{"status":"ok","timestamp":1649168021089,"user_tz":-540,"elapsed":8,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["class CustomModel(nn.Module):\n","    def __init__(self, cfg, model_config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        if model_config_path is None:\n","            self.model_config = AutoConfig.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                output_hidden_states=True,\n","            )\n","        else:\n","            self.model_config = torch.load(model_config_path)\n","\n","        if pretrained:\n","            self.backbone = AutoModel.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                config=self.model_config,\n","            )\n","            print(f\"Load weight from pretrained\")\n","        else:\n","            #self.backbone = AutoModel.from_config(self.model_config)\n","            # itpt = AutoModelForMaskedLM.from_config(self.model_config)\n","            #path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp010/checkpoint-130170/pytorch_model.bin\")\n","            # path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n","            # state_dict = torch.load(path)\n","            # itpt.load_state_dict(state_dict)\n","            path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\")\n","            masked_model = MaskedModel(CFG, config_path=None, pretrained=True)\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            masked_model.load_state_dict(state)\n","            self.backbone = masked_model.model\n","            print(f\"Load weight from {path}\")\n","\n","        self.fc = nn.Sequential(\n","            nn.Dropout(self.cfg.dropout),\n","            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n","        )\n","\n","    def forward(self, inputs):\n","        h = self.backbone(**inputs)[\"last_hidden_state\"]\n","        output = self.fc(h)\n","        return output"]},{"cell_type":"markdown","id":"chronic-bullet","metadata":{"id":"chronic-bullet"},"source":["## Training"]},{"cell_type":"code","execution_count":26,"id":"biological-hunger","metadata":{"id":"biological-hunger","executionInfo":{"status":"ok","timestamp":1649168021089,"user_tz":-540,"elapsed":8,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def train_fn(\n","    train_dataloader,\n","    model,\n","    criterion,\n","    optimizer,\n","    epoch,\n","    scheduler,\n","    device,\n","):\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(train_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","        loss = loss.mean()\n","\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","\n","        if CFG.batch_scheduler:\n","            scheduler.step()\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n","            print(\n","                \"Epoch: [{0}][{1}/{2}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                \"Grad: {grad_norm:.4f}  \"\n","                \"LR: {lr:.6f}  \"\n","                .format(\n","                    epoch+1,\n","                    step,\n","                    len(train_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n","                    loss=losses,\n","                     grad_norm=grad_norm,\n","                     lr=scheduler.get_lr()[0],\n","                )\n","            )\n","    return losses.avg"]},{"cell_type":"code","execution_count":27,"id":"satisfied-sterling","metadata":{"id":"satisfied-sterling","executionInfo":{"status":"ok","timestamp":1649168021089,"user_tz":-540,"elapsed":8,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def valid_fn(\n","    val_dataloader,\n","    model,\n","    criterion,\n","    device,\n","):\n","    model.eval()\n","    preds = []\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(val_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.no_grad():\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","        loss = loss.mean()\n","\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n","            print(\n","                \"EVAL: [{0}/{1}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                .format(\n","                    step, len(val_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n","                    loss=losses,\n","                )\n","            )\n","    preds = np.concatenate(preds)\n","    return losses.avg, preds"]},{"cell_type":"code","execution_count":28,"id":"incorporate-viking","metadata":{"id":"incorporate-viking","executionInfo":{"status":"ok","timestamp":1649168021090,"user_tz":-540,"elapsed":8,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def inference_fn(test_dataloader, model, device):\n","    model.eval()\n","    model.to(device)\n","    preds = []\n","    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n","    for inputs in tk0:\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        with torch.no_grad():\n","            output = model(inputs)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","    preds = np.concatenate(preds)\n","    return preds"]},{"cell_type":"code","execution_count":29,"id":"dental-sunset","metadata":{"id":"dental-sunset","executionInfo":{"status":"ok","timestamp":1649168021090,"user_tz":-540,"elapsed":8,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def train_loop(df, i_fold, device):\n","    print(f\"========== fold: {i_fold} training ==========\")\n","    train_idx = df[df[\"fold\"] != i_fold].index\n","    val_idx = df[df[\"fold\"] == i_fold].index\n","\n","    train_folds = df.loc[train_idx].reset_index(drop=True)\n","    val_folds = df.loc[val_idx].reset_index(drop=True)\n","\n","    train_dataset = TrainingDataset(CFG, train_folds)\n","    val_dataset = TrainingDataset(CFG, val_folds)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=True,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=True,\n","    )\n","    val_dataloader = DataLoader(\n","        val_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=False,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=False,\n","    )\n","\n","    # model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","    model = CustomModel(CFG, model_config_path=None, pretrained=False)   # itptを使うため\n","    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n","    model.to(device)\n","\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\"params\": [p for n, p in param_optimizer if not any(\n","            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n","        {\"params\": [p for n, p in param_optimizer if any(\n","            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n","    ]\n","    optimizer = AdamW(\n","        optimizer_grouped_parameters,\n","        lr=CFG.lr,\n","        betas=CFG.betas,\n","        weight_decay=CFG.weight_decay,\n","    )\n","    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n","    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        num_training_steps=num_train_optimization_steps,\n","    )\n","\n","    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    best_score = -1 * np.inf\n","\n","    for epoch in range(CFG.epochs):\n","        start_time = time.time()\n","        avg_loss = train_fn(\n","            train_dataloader,\n","            model,\n","            criterion,\n","            optimizer,\n","            epoch,\n","            scheduler,\n","            device,\n","        )\n","        avg_val_loss, val_preds = valid_fn(\n","            val_dataloader,\n","            model,\n","            criterion,\n","            device,\n","        )\n","\n","        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n","            scheduler.step()\n","\n","        # scoring\n","        val_folds[[str(i) for i in range(CFG.max_len)]] = val_preds\n","        score = scoring(val_folds, th=0.5)\n","\n","        elapsed = time.time() - start_time\n","\n","        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n","        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n","        if score > best_score:\n","            best_score = score\n","            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n","            torch.save({\n","                \"model\": model.state_dict(),\n","                \"predictions\": val_preds,\n","                },\n","                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","            )\n","\n","    predictions = torch.load(\n","        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","        map_location=torch.device(\"cpu\"),\n","    )[\"predictions\"]\n","    val_folds[[str(i) for i in range(CFG.max_len)]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return val_folds"]},{"cell_type":"markdown","id":"brazilian-graphics","metadata":{"id":"brazilian-graphics"},"source":["## Main"]},{"cell_type":"code","execution_count":30,"id":"connected-protein","metadata":{"id":"connected-protein","executionInfo":{"status":"ok","timestamp":1649168021090,"user_tz":-540,"elapsed":8,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def main():\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    if CFG.train:\n","        oof_df = pd.DataFrame()\n","        for i_fold in range(CFG.n_fold):\n","            if i_fold in CFG.train_fold:\n","                _oof_df = train_loop(train, i_fold, device)\n","                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n","        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    if CFG.submission:\n","        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n","    else:\n","        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    score = scoring(oof_df, th=0.5)\n","    print(f\"Best thres: 0.5, Score: {score:.4f}\")\n","    best_thres = get_best_thres(oof_df)\n","    score = scoring(oof_df, th=best_thres)\n","    print(f\"Best thres: {best_thres}, Score: {score:.4f}\")\n","\n","    if CFG.inference:\n","        test_dataset = TestDataset(CFG, test)\n","        test_dataloader = DataLoader(\n","            test_dataset,\n","            batch_size=CFG.batch_size,\n","            shuffle=False,\n","            num_workers=CFG.num_workers,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","        predictions = []\n","        for i_fold in CFG.train_fold:\n","            if CFG.submission:\n","                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n","                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n","            else:\n","                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n","\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            model.load_state_dict(state[\"model\"])\n","            test_token_probs = inference_fn(test_dataloader, model, device)\n","            test[[f\"fold{i_fold}_{i}\" for i in range(CFG.max_len)]] = test_token_probs\n","            test_char_probs = get_char_probs(test[\"pn_history\"].values, test_token_probs, CFG.tokenizer)\n","            predictions.append(test_char_probs)\n","\n","            del state, test_token_probs, model; gc.collect()\n","            torch.cuda.empty_cache()\n","\n","        predictions = np.mean(predictions, axis=0)\n","        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n","        test[CFG.target_col] = predicted_location_str\n","        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n","        test[[CFG.id_col, CFG.target_col]].to_csv(\n","            CFG.output_dir / \"submission.csv\", index=False\n","        )"]},{"cell_type":"code","execution_count":31,"id":"serious-bunny","metadata":{"id":"serious-bunny","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["18f97f51777f4a508de043f5fc46aed9","c99a1ea14f5b451eb4dcdbe176125bb7","72c6c9a22120445f98b54304a4a126eb","834bf7915e864023a79602d1b6625a6a","9b2af3df6d3643c58df2dba8d46ba981","712bfa8359a24221936c8e54f4a6d9b3","3d3df510be6a4f9a840acd16cec77aa0","63fb64722b554406b98c8f03ae61dfdd","1ca3ed8064084a7a9e4c37017ec35a7d","8aaef039787d487daeb140cd144e60d5","5ac7568e8d394aa9b402e49c2abf61b5","29ec628d736b46f9a626469bdb8a7dba","07f85e30620e495cb543b2b2ad295e94","f9ea345e103d4c5697783485bb3d7368","f2a8c13ba9f142319b61bb2d1c99a900","21367b1b7d424b9784da103ca9ba08f8","d9665608ced042e8a58b24dbda64a51c","434cf5b9d19440c7bf3e4fe67fb5878d","81ec77c78b1440e39093e79560e1ff2e","83c8a95d49c04346b24094dfb3d9dc55","4b53674073334ed097472af0a0835925","96a796f8a0124af3b231d6a8671a4be0","9abb0340892b406faa4c1243436356ac","1e8f996fceee4ccfa111a81cd6b10b1b","91828c5639ae46a691d6d02794d01a9a","a24e46d743414932b5b65c5db1abc75e","877c0896301b49769895a6f2a0ef794d","4131446f577342dea87fafed4da5b61e","50c2df816c534d60b67fc8891ad81137","c7b2ac2723dd4ca481cea24c3d4e84dc","967a72ab0f314d799caf7a246eef0308","8ef817ce62684ae387d3fb7374058461","782a73f82a74413f80f9b2e3c4d59d81","9bd6642ecbd24fd2a09e3c3665193618","f75a5054123b44ae81644024ecd04aa9","6d9fe4f2f22140a4997154380a4c71fe","4e043dc5a8cb4b52b53b9c9647dff92e","c0d6b63bc7504beaaa9c420f965eb540","115f541992344fe6bab38965689f00db","c863ed2448ef4fa68669b6ce93fdf585","d8eff25c3a994e9eb18761c92c355beb","3198f4b4bbbb4ae3b96d100ad4200123","53e497b701f14e5eb5473a5696bb39d3","9395424bcdca41e69787729f90a17707","e86016676bf74eaf9beecdaecd06ad71","1beb9ba25a514238aede0c5b6f4df4f1","b6cd03fcb5464b9db40c6d70dd140992","2cf4ee08245343c68218f037aca4f9e0","d4177f6a811a4180aae4832dc1ce21bc","84cce2317c1148bda39f240bce67ba33","8f6f4290ea9e462b8fd7ae93a4d51e06","fbe7c4115fc2472b9ff70247bb21d1c4","5f5bf4104fb44803a64c2d9373081882","6affdef5d47a44468201bebf5202dfc1","bdfde0e657e74cc3a20d2ed57fd34200"]},"executionInfo":{"status":"ok","timestamp":1649189217374,"user_tz":-540,"elapsed":21196291,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"d6e72884-6ae5-40d3-96b5-05dec498afd9"},"outputs":[{"output_type":"stream","name":"stdout","text":["========== fold: 0 training ==========\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/833M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18f97f51777f4a508de043f5fc46aed9"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight']\n","- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\n","Epoch: [1][0/3575] Elapsed 0m 1s (remain 72m 39s) Loss: 0.3468(0.3468) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/3575] Elapsed 0m 27s (remain 15m 34s) Loss: 0.2334(0.3011) Grad: 56216.1445  LR: 0.000001  \n","Epoch: [1][200/3575] Elapsed 0m 52s (remain 14m 48s) Loss: 0.0429(0.2195) Grad: 12456.2285  LR: 0.000002  \n","Epoch: [1][300/3575] Elapsed 1m 18s (remain 14m 15s) Loss: 0.0231(0.1616) Grad: 1997.9277  LR: 0.000003  \n","Epoch: [1][400/3575] Elapsed 1m 44s (remain 13m 46s) Loss: 0.0252(0.1305) Grad: 2020.7988  LR: 0.000004  \n","Epoch: [1][500/3575] Elapsed 2m 11s (remain 13m 25s) Loss: 0.0523(0.1116) Grad: 3315.5862  LR: 0.000006  \n","Epoch: [1][600/3575] Elapsed 2m 38s (remain 13m 3s) Loss: 0.0206(0.0987) Grad: 3860.6650  LR: 0.000007  \n","Epoch: [1][700/3575] Elapsed 3m 5s (remain 12m 41s) Loss: 0.0306(0.0881) Grad: 6933.5078  LR: 0.000008  \n","Epoch: [1][800/3575] Elapsed 3m 32s (remain 12m 17s) Loss: 0.0215(0.0793) Grad: 4443.7896  LR: 0.000009  \n","Epoch: [1][900/3575] Elapsed 4m 0s (remain 11m 52s) Loss: 0.0058(0.0722) Grad: 2686.6199  LR: 0.000010  \n","Epoch: [1][1000/3575] Elapsed 4m 27s (remain 11m 27s) Loss: 0.0189(0.0665) Grad: 14609.4893  LR: 0.000011  \n","Epoch: [1][1100/3575] Elapsed 4m 54s (remain 11m 2s) Loss: 0.0007(0.0614) Grad: 495.2056  LR: 0.000012  \n","Epoch: [1][1200/3575] Elapsed 5m 21s (remain 10m 36s) Loss: 0.0053(0.0574) Grad: 7255.0151  LR: 0.000013  \n","Epoch: [1][1300/3575] Elapsed 5m 49s (remain 10m 10s) Loss: 0.0035(0.0537) Grad: 3318.5371  LR: 0.000015  \n","Epoch: [1][1400/3575] Elapsed 6m 16s (remain 9m 43s) Loss: 0.0155(0.0508) Grad: 4869.7451  LR: 0.000016  \n","Epoch: [1][1500/3575] Elapsed 6m 43s (remain 9m 17s) Loss: 0.0054(0.0480) Grad: 4924.7217  LR: 0.000017  \n","Epoch: [1][1600/3575] Elapsed 7m 11s (remain 8m 51s) Loss: 0.0125(0.0456) Grad: 3385.9822  LR: 0.000018  \n","Epoch: [1][1700/3575] Elapsed 7m 38s (remain 8m 25s) Loss: 0.0496(0.0434) Grad: 7699.0020  LR: 0.000019  \n","Epoch: [1][1800/3575] Elapsed 8m 6s (remain 7m 58s) Loss: 0.0100(0.0416) Grad: 5166.8550  LR: 0.000020  \n","Epoch: [1][1900/3575] Elapsed 8m 33s (remain 7m 32s) Loss: 0.0141(0.0398) Grad: 3551.6067  LR: 0.000020  \n","Epoch: [1][2000/3575] Elapsed 9m 1s (remain 7m 5s) Loss: 0.0003(0.0385) Grad: 339.0835  LR: 0.000020  \n","Epoch: [1][2100/3575] Elapsed 9m 28s (remain 6m 38s) Loss: 0.0001(0.0370) Grad: 128.4831  LR: 0.000020  \n","Epoch: [1][2200/3575] Elapsed 9m 55s (remain 6m 11s) Loss: 0.0038(0.0358) Grad: 2971.7217  LR: 0.000019  \n","Epoch: [1][2300/3575] Elapsed 10m 23s (remain 5m 45s) Loss: 0.0014(0.0347) Grad: 1365.6666  LR: 0.000019  \n","Epoch: [1][2400/3575] Elapsed 10m 50s (remain 5m 18s) Loss: 0.0006(0.0337) Grad: 791.1871  LR: 0.000019  \n","Epoch: [1][2500/3575] Elapsed 11m 18s (remain 4m 51s) Loss: 0.0003(0.0327) Grad: 334.1547  LR: 0.000019  \n","Epoch: [1][2600/3575] Elapsed 11m 45s (remain 4m 24s) Loss: 0.0087(0.0318) Grad: 6394.2788  LR: 0.000019  \n","Epoch: [1][2700/3575] Elapsed 12m 12s (remain 3m 57s) Loss: 0.0016(0.0308) Grad: 7583.2734  LR: 0.000019  \n","Epoch: [1][2800/3575] Elapsed 12m 39s (remain 3m 29s) Loss: 0.0238(0.0301) Grad: 18357.8516  LR: 0.000019  \n","Epoch: [1][2900/3575] Elapsed 13m 7s (remain 3m 2s) Loss: 0.0017(0.0293) Grad: 1389.6200  LR: 0.000019  \n","Epoch: [1][3000/3575] Elapsed 13m 34s (remain 2m 35s) Loss: 0.0005(0.0286) Grad: 967.3419  LR: 0.000018  \n","Epoch: [1][3100/3575] Elapsed 14m 1s (remain 2m 8s) Loss: 0.0001(0.0279) Grad: 823.9689  LR: 0.000018  \n","Epoch: [1][3200/3575] Elapsed 14m 28s (remain 1m 41s) Loss: 0.0013(0.0274) Grad: 984.0421  LR: 0.000018  \n","Epoch: [1][3300/3575] Elapsed 14m 55s (remain 1m 14s) Loss: 0.0116(0.0268) Grad: 2723.1277  LR: 0.000018  \n","Epoch: [1][3400/3575] Elapsed 15m 22s (remain 0m 47s) Loss: 0.0061(0.0263) Grad: 6196.0479  LR: 0.000018  \n","Epoch: [1][3500/3575] Elapsed 15m 49s (remain 0m 20s) Loss: 0.0003(0.0257) Grad: 303.2751  LR: 0.000018  \n","Epoch: [1][3574/3575] Elapsed 16m 8s (remain 0m 0s) Loss: 0.0002(0.0253) Grad: 122.1751  LR: 0.000018  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 17m 36s) Loss: 0.0005(0.0005) \n","EVAL: [100/1192] Elapsed 0m 17s (remain 3m 11s) Loss: 0.0106(0.0040) \n","EVAL: [200/1192] Elapsed 0m 34s (remain 2m 50s) Loss: 0.0089(0.0053) \n","EVAL: [300/1192] Elapsed 0m 51s (remain 2m 32s) Loss: 0.0132(0.0059) \n","EVAL: [400/1192] Elapsed 1m 8s (remain 2m 14s) Loss: 0.0037(0.0063) \n","EVAL: [500/1192] Elapsed 1m 25s (remain 1m 57s) Loss: 0.0120(0.0059) \n","EVAL: [600/1192] Elapsed 1m 41s (remain 1m 40s) Loss: 0.0080(0.0064) \n","EVAL: [700/1192] Elapsed 1m 58s (remain 1m 23s) Loss: 0.0699(0.0074) \n","EVAL: [800/1192] Elapsed 2m 15s (remain 1m 6s) Loss: 0.0093(0.0078) \n","EVAL: [900/1192] Elapsed 2m 31s (remain 0m 49s) Loss: 0.0122(0.0080) \n","EVAL: [1000/1192] Elapsed 2m 48s (remain 0m 32s) Loss: 0.0001(0.0079) \n","EVAL: [1100/1192] Elapsed 3m 5s (remain 0m 15s) Loss: 0.0006(0.0075) \n","EVAL: [1191/1192] Elapsed 3m 20s (remain 0m 0s) Loss: 0.0001(0.0073) \n","Epoch 1 - avg_train_loss: 0.0253  avg_val_loss: 0.0073  time: 1205s\n","Epoch 1 - Score: 0.8485\n","Epoch 1 - Save Best Score: 0.8485 Model\n","Epoch: [2][0/3575] Elapsed 0m 0s (remain 35m 51s) Loss: 0.0023(0.0023) Grad: 13090.0869  LR: 0.000018  \n","Epoch: [2][100/3575] Elapsed 0m 23s (remain 13m 15s) Loss: 0.0014(0.0072) Grad: 5338.7056  LR: 0.000018  \n","Epoch: [2][200/3575] Elapsed 0m 46s (remain 12m 59s) Loss: 0.0074(0.0068) Grad: 29687.1895  LR: 0.000018  \n","Epoch: [2][300/3575] Elapsed 1m 7s (remain 12m 18s) Loss: 0.0175(0.0068) Grad: 33051.5000  LR: 0.000017  \n","Epoch: [2][400/3575] Elapsed 1m 28s (remain 11m 42s) Loss: 0.0064(0.0069) Grad: 11551.2861  LR: 0.000017  \n","Epoch: [2][500/3575] Elapsed 1m 50s (remain 11m 16s) Loss: 0.0108(0.0067) Grad: 29585.6191  LR: 0.000017  \n","Epoch: [2][600/3575] Elapsed 2m 11s (remain 10m 50s) Loss: 0.0000(0.0066) Grad: 318.6053  LR: 0.000017  \n","Epoch: [2][700/3575] Elapsed 2m 32s (remain 10m 25s) Loss: 0.0002(0.0065) Grad: 433.8730  LR: 0.000017  \n","Epoch: [2][800/3575] Elapsed 2m 53s (remain 10m 0s) Loss: 0.0144(0.0064) Grad: 21645.1387  LR: 0.000017  \n","Epoch: [2][900/3575] Elapsed 3m 14s (remain 9m 36s) Loss: 0.0000(0.0062) Grad: 29.6428  LR: 0.000017  \n","Epoch: [2][1000/3575] Elapsed 3m 35s (remain 9m 13s) Loss: 0.0416(0.0064) Grad: 54828.1328  LR: 0.000017  \n","Epoch: [2][1100/3575] Elapsed 3m 55s (remain 8m 49s) Loss: 0.0000(0.0065) Grad: 57.4942  LR: 0.000016  \n","Epoch: [2][1200/3575] Elapsed 4m 16s (remain 8m 27s) Loss: 0.0076(0.0065) Grad: 10360.6230  LR: 0.000016  \n","Epoch: [2][1300/3575] Elapsed 4m 37s (remain 8m 5s) Loss: 0.0000(0.0064) Grad: 77.9142  LR: 0.000016  \n","Epoch: [2][1400/3575] Elapsed 4m 58s (remain 7m 43s) Loss: 0.0002(0.0064) Grad: 2650.7397  LR: 0.000016  \n","Epoch: [2][1500/3575] Elapsed 5m 19s (remain 7m 21s) Loss: 0.0018(0.0066) Grad: 34867.8125  LR: 0.000016  \n","Epoch: [2][1600/3575] Elapsed 5m 40s (remain 6m 59s) Loss: 0.0280(0.0066) Grad: 19609.0059  LR: 0.000016  \n","Epoch: [2][1700/3575] Elapsed 6m 1s (remain 6m 38s) Loss: 0.0064(0.0066) Grad: 72011.1172  LR: 0.000016  \n","Epoch: [2][1800/3575] Elapsed 6m 22s (remain 6m 16s) Loss: 0.0000(0.0066) Grad: 33.8165  LR: 0.000016  \n","Epoch: [2][1900/3575] Elapsed 6m 42s (remain 5m 54s) Loss: 0.0013(0.0066) Grad: 9059.0371  LR: 0.000015  \n","Epoch: [2][2000/3575] Elapsed 7m 3s (remain 5m 33s) Loss: 0.0090(0.0066) Grad: 28436.2148  LR: 0.000015  \n","Epoch: [2][2100/3575] Elapsed 7m 24s (remain 5m 12s) Loss: 0.0140(0.0065) Grad: 33982.6406  LR: 0.000015  \n","Epoch: [2][2200/3575] Elapsed 7m 46s (remain 4m 51s) Loss: 0.0006(0.0066) Grad: 2267.6753  LR: 0.000015  \n","Epoch: [2][2300/3575] Elapsed 8m 7s (remain 4m 30s) Loss: 0.0106(0.0066) Grad: 23900.4922  LR: 0.000015  \n","Epoch: [2][2400/3575] Elapsed 8m 29s (remain 4m 8s) Loss: 0.0034(0.0066) Grad: 4003.7034  LR: 0.000015  \n","Epoch: [2][2500/3575] Elapsed 8m 49s (remain 3m 47s) Loss: 0.0426(0.0065) Grad: 28386.8809  LR: 0.000015  \n","Epoch: [2][2600/3575] Elapsed 9m 10s (remain 3m 26s) Loss: 0.0000(0.0065) Grad: 520.9988  LR: 0.000015  \n","Epoch: [2][2700/3575] Elapsed 9m 31s (remain 3m 5s) Loss: 0.0024(0.0064) Grad: 13818.0967  LR: 0.000014  \n","Epoch: [2][2800/3575] Elapsed 9m 52s (remain 2m 43s) Loss: 0.0011(0.0064) Grad: 6141.4194  LR: 0.000014  \n","Epoch: [2][2900/3575] Elapsed 10m 13s (remain 2m 22s) Loss: 0.0000(0.0064) Grad: 86.8981  LR: 0.000014  \n","Epoch: [2][3000/3575] Elapsed 10m 34s (remain 2m 1s) Loss: 0.0003(0.0064) Grad: 1948.4213  LR: 0.000014  \n","Epoch: [2][3100/3575] Elapsed 10m 55s (remain 1m 40s) Loss: 0.0000(0.0064) Grad: 78.1699  LR: 0.000014  \n","Epoch: [2][3200/3575] Elapsed 11m 16s (remain 1m 19s) Loss: 0.0141(0.0063) Grad: 17218.5508  LR: 0.000014  \n","Epoch: [2][3300/3575] Elapsed 11m 37s (remain 0m 57s) Loss: 0.0034(0.0063) Grad: 8543.2080  LR: 0.000014  \n","Epoch: [2][3400/3575] Elapsed 11m 58s (remain 0m 36s) Loss: 0.0009(0.0063) Grad: 5697.9170  LR: 0.000014  \n","Epoch: [2][3500/3575] Elapsed 12m 19s (remain 0m 15s) Loss: 0.0043(0.0062) Grad: 27741.2363  LR: 0.000013  \n","Epoch: [2][3574/3575] Elapsed 12m 35s (remain 0m 0s) Loss: 0.0021(0.0062) Grad: 6683.3872  LR: 0.000013  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 15s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 6s) Loss: 0.0085(0.0045) \n","EVAL: [200/1192] Elapsed 0m 22s (remain 1m 53s) Loss: 0.0029(0.0057) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 41s) Loss: 0.0120(0.0064) \n","EVAL: [400/1192] Elapsed 0m 45s (remain 1m 29s) Loss: 0.0045(0.0068) \n","EVAL: [500/1192] Elapsed 0m 56s (remain 1m 18s) Loss: 0.0109(0.0063) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 6s) Loss: 0.0010(0.0068) \n","EVAL: [700/1192] Elapsed 1m 19s (remain 0m 55s) Loss: 0.0936(0.0079) \n","EVAL: [800/1192] Elapsed 1m 30s (remain 0m 44s) Loss: 0.0025(0.0082) \n","EVAL: [900/1192] Elapsed 1m 41s (remain 0m 32s) Loss: 0.0015(0.0087) \n","EVAL: [1000/1192] Elapsed 1m 53s (remain 0m 21s) Loss: 0.0000(0.0085) \n","EVAL: [1100/1192] Elapsed 2m 4s (remain 0m 10s) Loss: 0.0008(0.0081) \n","EVAL: [1191/1192] Elapsed 2m 14s (remain 0m 0s) Loss: 0.0000(0.0078) \n","Epoch 2 - avg_train_loss: 0.0062  avg_val_loss: 0.0078  time: 894s\n","Epoch 2 - Score: 0.8693\n","Epoch 2 - Save Best Score: 0.8693 Model\n","Epoch: [3][0/3575] Elapsed 0m 0s (remain 32m 54s) Loss: 0.0207(0.0207) Grad: 93713.9219  LR: 0.000013  \n","Epoch: [3][100/3575] Elapsed 0m 23s (remain 13m 19s) Loss: 0.0203(0.0033) Grad: 52802.0156  LR: 0.000013  \n","Epoch: [3][200/3575] Elapsed 0m 46s (remain 13m 2s) Loss: 0.0008(0.0050) Grad: 2791.6648  LR: 0.000013  \n","Epoch: [3][300/3575] Elapsed 1m 8s (remain 12m 19s) Loss: 0.0000(0.0047) Grad: 12.3606  LR: 0.000013  \n","Epoch: [3][400/3575] Elapsed 1m 29s (remain 11m 44s) Loss: 0.0180(0.0049) Grad: 73898.2109  LR: 0.000013  \n","Epoch: [3][500/3575] Elapsed 1m 50s (remain 11m 14s) Loss: 0.0000(0.0047) Grad: 57.9005  LR: 0.000013  \n","Epoch: [3][600/3575] Elapsed 2m 10s (remain 10m 47s) Loss: 0.0010(0.0049) Grad: 7749.4102  LR: 0.000013  \n","Epoch: [3][700/3575] Elapsed 2m 31s (remain 10m 21s) Loss: 0.0000(0.0048) Grad: 170.7694  LR: 0.000012  \n","Epoch: [3][800/3575] Elapsed 2m 52s (remain 9m 57s) Loss: 0.0001(0.0049) Grad: 269.2882  LR: 0.000012  \n","Epoch: [3][900/3575] Elapsed 3m 13s (remain 9m 34s) Loss: 0.0118(0.0048) Grad: 29732.2793  LR: 0.000012  \n","Epoch: [3][1000/3575] Elapsed 3m 34s (remain 9m 11s) Loss: 0.0352(0.0050) Grad: 105830.6641  LR: 0.000012  \n","Epoch: [3][1100/3575] Elapsed 3m 55s (remain 8m 48s) Loss: 0.0254(0.0050) Grad: 52224.1328  LR: 0.000012  \n","Epoch: [3][1200/3575] Elapsed 4m 15s (remain 8m 25s) Loss: 0.0002(0.0050) Grad: 2346.0498  LR: 0.000012  \n","Epoch: [3][1300/3575] Elapsed 4m 36s (remain 8m 3s) Loss: 0.0100(0.0051) Grad: 15971.6445  LR: 0.000012  \n","Epoch: [3][1400/3575] Elapsed 4m 57s (remain 7m 41s) Loss: 0.0144(0.0051) Grad: 31298.9961  LR: 0.000012  \n","Epoch: [3][1500/3575] Elapsed 5m 18s (remain 7m 19s) Loss: 0.0001(0.0051) Grad: 388.1945  LR: 0.000011  \n","Epoch: [3][1600/3575] Elapsed 5m 38s (remain 6m 57s) Loss: 0.0007(0.0051) Grad: 2959.3486  LR: 0.000011  \n","Epoch: [3][1700/3575] Elapsed 5m 59s (remain 6m 36s) Loss: 0.0083(0.0051) Grad: 25048.0215  LR: 0.000011  \n","Epoch: [3][1800/3575] Elapsed 6m 20s (remain 6m 14s) Loss: 0.0084(0.0052) Grad: 24349.5508  LR: 0.000011  \n","Epoch: [3][1900/3575] Elapsed 6m 40s (remain 5m 53s) Loss: 0.0199(0.0051) Grad: 24602.0391  LR: 0.000011  \n","Epoch: [3][2000/3575] Elapsed 7m 1s (remain 5m 31s) Loss: 0.0013(0.0051) Grad: 5590.4580  LR: 0.000011  \n","Epoch: [3][2100/3575] Elapsed 7m 22s (remain 5m 10s) Loss: 0.0000(0.0051) Grad: 96.6609  LR: 0.000011  \n","Epoch: [3][2200/3575] Elapsed 7m 43s (remain 4m 49s) Loss: 0.0044(0.0051) Grad: 5728.6929  LR: 0.000011  \n","Epoch: [3][2300/3575] Elapsed 8m 4s (remain 4m 28s) Loss: 0.0122(0.0051) Grad: 18479.5215  LR: 0.000010  \n","Epoch: [3][2400/3575] Elapsed 8m 25s (remain 4m 6s) Loss: 0.0001(0.0051) Grad: 489.9859  LR: 0.000010  \n","Epoch: [3][2500/3575] Elapsed 8m 45s (remain 3m 45s) Loss: 0.0153(0.0052) Grad: 40208.5859  LR: 0.000010  \n","Epoch: [3][2600/3575] Elapsed 9m 6s (remain 3m 24s) Loss: 0.0072(0.0051) Grad: 40153.8594  LR: 0.000010  \n","Epoch: [3][2700/3575] Elapsed 9m 27s (remain 3m 3s) Loss: 0.0002(0.0051) Grad: 3216.4419  LR: 0.000010  \n","Epoch: [3][2800/3575] Elapsed 9m 48s (remain 2m 42s) Loss: 0.0039(0.0052) Grad: 6469.5303  LR: 0.000010  \n","Epoch: [3][2900/3575] Elapsed 10m 8s (remain 2m 21s) Loss: 0.0003(0.0053) Grad: 1589.2759  LR: 0.000010  \n","Epoch: [3][3000/3575] Elapsed 10m 29s (remain 2m 0s) Loss: 0.0078(0.0052) Grad: 47790.5312  LR: 0.000010  \n","Epoch: [3][3100/3575] Elapsed 10m 50s (remain 1m 39s) Loss: 0.0000(0.0052) Grad: 67.7401  LR: 0.000009  \n","Epoch: [3][3200/3575] Elapsed 11m 11s (remain 1m 18s) Loss: 0.0060(0.0052) Grad: 14353.4795  LR: 0.000009  \n","Epoch: [3][3300/3575] Elapsed 11m 31s (remain 0m 57s) Loss: 0.0067(0.0051) Grad: 37163.6680  LR: 0.000009  \n","Epoch: [3][3400/3575] Elapsed 11m 52s (remain 0m 36s) Loss: 0.0001(0.0051) Grad: 1530.3818  LR: 0.000009  \n","Epoch: [3][3500/3575] Elapsed 12m 13s (remain 0m 15s) Loss: 0.0000(0.0051) Grad: 29.9721  LR: 0.000009  \n","Epoch: [3][3574/3575] Elapsed 12m 28s (remain 0m 0s) Loss: 0.0107(0.0051) Grad: 12904.6201  LR: 0.000009  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 31s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 6s) Loss: 0.0108(0.0049) \n","EVAL: [200/1192] Elapsed 0m 22s (remain 1m 52s) Loss: 0.0019(0.0051) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 41s) Loss: 0.0146(0.0060) \n","EVAL: [400/1192] Elapsed 0m 45s (remain 1m 29s) Loss: 0.0038(0.0063) \n","EVAL: [500/1192] Elapsed 0m 56s (remain 1m 17s) Loss: 0.0048(0.0058) \n","EVAL: [600/1192] Elapsed 1m 7s (remain 1m 6s) Loss: 0.0003(0.0062) \n","EVAL: [700/1192] Elapsed 1m 18s (remain 0m 55s) Loss: 0.0756(0.0074) \n","EVAL: [800/1192] Elapsed 1m 30s (remain 0m 43s) Loss: 0.0009(0.0077) \n","EVAL: [900/1192] Elapsed 1m 41s (remain 0m 32s) Loss: 0.0071(0.0079) \n","EVAL: [1000/1192] Elapsed 1m 52s (remain 0m 21s) Loss: 0.0000(0.0077) \n","EVAL: [1100/1192] Elapsed 2m 3s (remain 0m 10s) Loss: 0.0015(0.0074) \n","EVAL: [1191/1192] Elapsed 2m 13s (remain 0m 0s) Loss: 0.0000(0.0071) \n","Epoch 3 - avg_train_loss: 0.0051  avg_val_loss: 0.0071  time: 888s\n","Epoch 3 - Score: 0.8792\n","Epoch 3 - Save Best Score: 0.8792 Model\n","Epoch: [4][0/3575] Elapsed 0m 0s (remain 31m 42s) Loss: 0.0001(0.0001) Grad: 1079.7953  LR: 0.000009  \n","Epoch: [4][100/3575] Elapsed 0m 22s (remain 13m 4s) Loss: 0.0232(0.0044) Grad: 165115.0156  LR: 0.000009  \n","Epoch: [4][200/3575] Elapsed 0m 46s (remain 13m 3s) Loss: 0.0000(0.0037) Grad: 9.8385  LR: 0.000009  \n","Epoch: [4][300/3575] Elapsed 1m 8s (remain 12m 20s) Loss: 0.0011(0.0034) Grad: 19026.6973  LR: 0.000009  \n","Epoch: [4][400/3575] Elapsed 1m 29s (remain 11m 45s) Loss: 0.0000(0.0034) Grad: 36.1612  LR: 0.000008  \n","Epoch: [4][500/3575] Elapsed 1m 50s (remain 11m 17s) Loss: 0.0000(0.0036) Grad: 11.6551  LR: 0.000008  \n","Epoch: [4][600/3575] Elapsed 2m 11s (remain 10m 49s) Loss: 0.0010(0.0036) Grad: 16599.5977  LR: 0.000008  \n","Epoch: [4][700/3575] Elapsed 2m 32s (remain 10m 24s) Loss: 0.0015(0.0038) Grad: 9341.8398  LR: 0.000008  \n","Epoch: [4][800/3575] Elapsed 2m 53s (remain 9m 59s) Loss: 0.0008(0.0039) Grad: 6060.0952  LR: 0.000008  \n","Epoch: [4][900/3575] Elapsed 3m 14s (remain 9m 36s) Loss: 0.0016(0.0038) Grad: 12224.5820  LR: 0.000008  \n","Epoch: [4][1000/3575] Elapsed 3m 35s (remain 9m 13s) Loss: 0.0020(0.0038) Grad: 9228.1504  LR: 0.000008  \n","Epoch: [4][1100/3575] Elapsed 3m 56s (remain 8m 50s) Loss: 0.0000(0.0038) Grad: 54.3311  LR: 0.000008  \n","Epoch: [4][1200/3575] Elapsed 4m 17s (remain 8m 28s) Loss: 0.0000(0.0037) Grad: 52.2398  LR: 0.000007  \n","Epoch: [4][1300/3575] Elapsed 4m 38s (remain 8m 6s) Loss: 0.0004(0.0037) Grad: 1655.0514  LR: 0.000007  \n","Epoch: [4][1400/3575] Elapsed 4m 59s (remain 7m 44s) Loss: 0.0056(0.0037) Grad: 38646.8867  LR: 0.000007  \n","Epoch: [4][1500/3575] Elapsed 5m 20s (remain 7m 22s) Loss: 0.0002(0.0038) Grad: 1900.4430  LR: 0.000007  \n","Epoch: [4][1600/3575] Elapsed 5m 41s (remain 7m 0s) Loss: 0.0038(0.0037) Grad: 150651.8594  LR: 0.000007  \n","Epoch: [4][1700/3575] Elapsed 6m 1s (remain 6m 38s) Loss: 0.0050(0.0037) Grad: 5375.0581  LR: 0.000007  \n","Epoch: [4][1800/3575] Elapsed 6m 22s (remain 6m 16s) Loss: 0.0001(0.0038) Grad: 2284.3369  LR: 0.000007  \n","Epoch: [4][1900/3575] Elapsed 6m 43s (remain 5m 55s) Loss: 0.0000(0.0038) Grad: 37.6663  LR: 0.000007  \n","Epoch: [4][2000/3575] Elapsed 7m 4s (remain 5m 33s) Loss: 0.0000(0.0038) Grad: 258.6864  LR: 0.000006  \n","Epoch: [4][2100/3575] Elapsed 7m 25s (remain 5m 12s) Loss: 0.0018(0.0038) Grad: 14152.5156  LR: 0.000006  \n","Epoch: [4][2200/3575] Elapsed 7m 46s (remain 4m 51s) Loss: 0.0000(0.0038) Grad: 135.3261  LR: 0.000006  \n","Epoch: [4][2300/3575] Elapsed 8m 7s (remain 4m 29s) Loss: 0.0185(0.0038) Grad: 36937.2109  LR: 0.000006  \n","Epoch: [4][2400/3575] Elapsed 8m 28s (remain 4m 8s) Loss: 0.0028(0.0037) Grad: 15959.8525  LR: 0.000006  \n","Epoch: [4][2500/3575] Elapsed 8m 49s (remain 3m 47s) Loss: 0.0000(0.0039) Grad: 168.0950  LR: 0.000006  \n","Epoch: [4][2600/3575] Elapsed 9m 10s (remain 3m 26s) Loss: 0.0016(0.0039) Grad: 7955.2529  LR: 0.000006  \n","Epoch: [4][2700/3575] Elapsed 9m 31s (remain 3m 4s) Loss: 0.0179(0.0038) Grad: 29558.0410  LR: 0.000006  \n","Epoch: [4][2800/3575] Elapsed 9m 52s (remain 2m 43s) Loss: 0.0030(0.0039) Grad: 6849.4585  LR: 0.000005  \n","Epoch: [4][2900/3575] Elapsed 10m 12s (remain 2m 22s) Loss: 0.0000(0.0039) Grad: 44.3721  LR: 0.000005  \n","Epoch: [4][3000/3575] Elapsed 10m 33s (remain 2m 1s) Loss: 0.0003(0.0038) Grad: 1321.8984  LR: 0.000005  \n","Epoch: [4][3100/3575] Elapsed 10m 54s (remain 1m 40s) Loss: 0.0000(0.0038) Grad: 59.4129  LR: 0.000005  \n","Epoch: [4][3200/3575] Elapsed 11m 15s (remain 1m 18s) Loss: 0.0006(0.0038) Grad: 3836.6885  LR: 0.000005  \n","Epoch: [4][3300/3575] Elapsed 11m 36s (remain 0m 57s) Loss: 0.0043(0.0039) Grad: 9851.0283  LR: 0.000005  \n","Epoch: [4][3400/3575] Elapsed 11m 57s (remain 0m 36s) Loss: 0.0588(0.0039) Grad: 235295.5469  LR: 0.000005  \n","Epoch: [4][3500/3575] Elapsed 12m 18s (remain 0m 15s) Loss: 0.0000(0.0039) Grad: 110.4614  LR: 0.000005  \n","Epoch: [4][3574/3575] Elapsed 12m 33s (remain 0m 0s) Loss: 0.0006(0.0039) Grad: 6478.0049  LR: 0.000004  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 7m 49s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 6s) Loss: 0.0120(0.0058) \n","EVAL: [200/1192] Elapsed 0m 22s (remain 1m 53s) Loss: 0.0133(0.0064) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 41s) Loss: 0.0067(0.0071) \n","EVAL: [400/1192] Elapsed 0m 45s (remain 1m 29s) Loss: 0.0086(0.0072) \n","EVAL: [500/1192] Elapsed 0m 56s (remain 1m 18s) Loss: 0.0141(0.0067) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 7s) Loss: 0.0003(0.0071) \n","EVAL: [700/1192] Elapsed 1m 19s (remain 0m 55s) Loss: 0.0782(0.0082) \n","EVAL: [800/1192] Elapsed 1m 30s (remain 0m 44s) Loss: 0.0019(0.0086) \n","EVAL: [900/1192] Elapsed 1m 41s (remain 0m 32s) Loss: 0.0016(0.0088) \n","EVAL: [1000/1192] Elapsed 1m 53s (remain 0m 21s) Loss: 0.0000(0.0086) \n","EVAL: [1100/1192] Elapsed 2m 4s (remain 0m 10s) Loss: 0.0025(0.0083) \n","EVAL: [1191/1192] Elapsed 2m 14s (remain 0m 0s) Loss: 0.0000(0.0081) \n","Epoch 4 - avg_train_loss: 0.0039  avg_val_loss: 0.0081  time: 893s\n","Epoch 4 - Score: 0.8807\n","Epoch 4 - Save Best Score: 0.8807 Model\n","Epoch: [5][0/3575] Elapsed 0m 0s (remain 33m 17s) Loss: 0.0000(0.0000) Grad: 1000.2624  LR: 0.000004  \n","Epoch: [5][100/3575] Elapsed 0m 23s (remain 13m 35s) Loss: 0.0000(0.0025) Grad: 168.6309  LR: 0.000004  \n","Epoch: [5][200/3575] Elapsed 0m 47s (remain 13m 16s) Loss: 0.0000(0.0027) Grad: 7.6701  LR: 0.000004  \n","Epoch: [5][300/3575] Elapsed 1m 8s (remain 12m 27s) Loss: 0.0358(0.0029) Grad: 44534.3047  LR: 0.000004  \n","Epoch: [5][400/3575] Elapsed 1m 29s (remain 11m 52s) Loss: 0.0000(0.0028) Grad: 13.6587  LR: 0.000004  \n","Epoch: [5][500/3575] Elapsed 1m 51s (remain 11m 21s) Loss: 0.0001(0.0031) Grad: 1751.1847  LR: 0.000004  \n","Epoch: [5][600/3575] Elapsed 2m 12s (remain 10m 53s) Loss: 0.0002(0.0032) Grad: 1710.3621  LR: 0.000004  \n","Epoch: [5][700/3575] Elapsed 2m 33s (remain 10m 27s) Loss: 0.0000(0.0032) Grad: 33.6831  LR: 0.000004  \n","Epoch: [5][800/3575] Elapsed 2m 54s (remain 10m 3s) Loss: 0.0000(0.0031) Grad: 68.6162  LR: 0.000003  \n","Epoch: [5][900/3575] Elapsed 3m 15s (remain 9m 39s) Loss: 0.0025(0.0031) Grad: 9004.4570  LR: 0.000003  \n","Epoch: [5][1000/3575] Elapsed 3m 36s (remain 9m 15s) Loss: 0.0000(0.0032) Grad: 68.1197  LR: 0.000003  \n","Epoch: [5][1100/3575] Elapsed 3m 56s (remain 8m 52s) Loss: 0.0000(0.0031) Grad: 164.2486  LR: 0.000003  \n","Epoch: [5][1200/3575] Elapsed 4m 17s (remain 8m 29s) Loss: 0.0000(0.0031) Grad: 29.5254  LR: 0.000003  \n","Epoch: [5][1300/3575] Elapsed 4m 38s (remain 8m 6s) Loss: 0.0000(0.0030) Grad: 13.4680  LR: 0.000003  \n","Epoch: [5][1400/3575] Elapsed 4m 59s (remain 7m 44s) Loss: 0.0000(0.0029) Grad: 197.7765  LR: 0.000003  \n","Epoch: [5][1500/3575] Elapsed 5m 20s (remain 7m 22s) Loss: 0.0192(0.0031) Grad: 47540.9258  LR: 0.000003  \n","Epoch: [5][1600/3575] Elapsed 5m 41s (remain 7m 0s) Loss: 0.0000(0.0030) Grad: 18.4547  LR: 0.000002  \n","Epoch: [5][1700/3575] Elapsed 6m 1s (remain 6m 38s) Loss: 0.0006(0.0030) Grad: 5968.5991  LR: 0.000002  \n","Epoch: [5][1800/3575] Elapsed 6m 22s (remain 6m 16s) Loss: 0.0000(0.0031) Grad: 41.1219  LR: 0.000002  \n","Epoch: [5][1900/3575] Elapsed 6m 43s (remain 5m 55s) Loss: 0.0164(0.0031) Grad: 39452.9102  LR: 0.000002  \n","Epoch: [5][2000/3575] Elapsed 7m 4s (remain 5m 33s) Loss: 0.0003(0.0030) Grad: 3410.2534  LR: 0.000002  \n","Epoch: [5][2100/3575] Elapsed 7m 25s (remain 5m 12s) Loss: 0.0560(0.0031) Grad: 47566.1133  LR: 0.000002  \n","Epoch: [5][2200/3575] Elapsed 7m 46s (remain 4m 50s) Loss: 0.0001(0.0032) Grad: 1130.4930  LR: 0.000002  \n","Epoch: [5][2300/3575] Elapsed 8m 6s (remain 4m 29s) Loss: 0.0075(0.0032) Grad: 9916.6738  LR: 0.000002  \n","Epoch: [5][2400/3575] Elapsed 8m 27s (remain 4m 8s) Loss: 0.0000(0.0032) Grad: 5.5461  LR: 0.000001  \n","Epoch: [5][2500/3575] Elapsed 8m 48s (remain 3m 46s) Loss: 0.0000(0.0033) Grad: 93.9802  LR: 0.000001  \n","Epoch: [5][2600/3575] Elapsed 9m 9s (remain 3m 25s) Loss: 0.0000(0.0033) Grad: 12.0703  LR: 0.000001  \n","Epoch: [5][2700/3575] Elapsed 9m 30s (remain 3m 4s) Loss: 0.0007(0.0032) Grad: 10725.3369  LR: 0.000001  \n","Epoch: [5][2800/3575] Elapsed 9m 51s (remain 2m 43s) Loss: 0.0000(0.0032) Grad: 53.2006  LR: 0.000001  \n","Epoch: [5][2900/3575] Elapsed 10m 12s (remain 2m 22s) Loss: 0.0125(0.0032) Grad: 63067.7578  LR: 0.000001  \n","Epoch: [5][3000/3575] Elapsed 10m 32s (remain 2m 1s) Loss: 0.0001(0.0032) Grad: 1089.1013  LR: 0.000001  \n","Epoch: [5][3100/3575] Elapsed 10m 53s (remain 1m 39s) Loss: 0.0034(0.0032) Grad: 6483.6621  LR: 0.000001  \n","Epoch: [5][3200/3575] Elapsed 11m 14s (remain 1m 18s) Loss: 0.0034(0.0032) Grad: 23792.2402  LR: 0.000000  \n","Epoch: [5][3300/3575] Elapsed 11m 35s (remain 0m 57s) Loss: 0.0146(0.0032) Grad: 32953.7852  LR: 0.000000  \n","Epoch: [5][3400/3575] Elapsed 11m 56s (remain 0m 36s) Loss: 0.0000(0.0032) Grad: 2.8360  LR: 0.000000  \n","Epoch: [5][3500/3575] Elapsed 12m 16s (remain 0m 15s) Loss: 0.0000(0.0032) Grad: 2.0541  LR: 0.000000  \n","Epoch: [5][3574/3575] Elapsed 12m 32s (remain 0m 0s) Loss: 0.0000(0.0032) Grad: 247.2794  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 32s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 6s) Loss: 0.0279(0.0057) \n","EVAL: [200/1192] Elapsed 0m 22s (remain 1m 53s) Loss: 0.0118(0.0065) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 41s) Loss: 0.0083(0.0075) \n","EVAL: [400/1192] Elapsed 0m 45s (remain 1m 29s) Loss: 0.0082(0.0077) \n","EVAL: [500/1192] Elapsed 0m 56s (remain 1m 18s) Loss: 0.0121(0.0073) \n","EVAL: [600/1192] Elapsed 1m 7s (remain 1m 6s) Loss: 0.0001(0.0077) \n","EVAL: [700/1192] Elapsed 1m 19s (remain 0m 55s) Loss: 0.0935(0.0089) \n","EVAL: [800/1192] Elapsed 1m 31s (remain 0m 44s) Loss: 0.0015(0.0093) \n","EVAL: [900/1192] Elapsed 1m 43s (remain 0m 33s) Loss: 0.0017(0.0094) \n","EVAL: [1000/1192] Elapsed 1m 56s (remain 0m 22s) Loss: 0.0000(0.0093) \n","EVAL: [1100/1192] Elapsed 2m 9s (remain 0m 10s) Loss: 0.0029(0.0089) \n","EVAL: [1191/1192] Elapsed 2m 20s (remain 0m 0s) Loss: 0.0000(0.0087) \n","Epoch 5 - avg_train_loss: 0.0032  avg_val_loss: 0.0087  time: 899s\n","Epoch 5 - Score: 0.8825\n","Epoch 5 - Save Best Score: 0.8825 Model\n","========== fold: 1 training ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight']\n","- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\n","Epoch: [1][0/3575] Elapsed 0m 1s (remain 102m 28s) Loss: 0.3456(0.3456) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/3575] Elapsed 0m 33s (remain 19m 27s) Loss: 0.2164(0.2881) Grad: 51928.0703  LR: 0.000001  \n","Epoch: [1][200/3575] Elapsed 1m 5s (remain 18m 22s) Loss: 0.0932(0.2150) Grad: 11784.3555  LR: 0.000002  \n","Epoch: [1][300/3575] Elapsed 1m 37s (remain 17m 35s) Loss: 0.0249(0.1579) Grad: 1713.0634  LR: 0.000003  \n","Epoch: [1][400/3575] Elapsed 2m 8s (remain 16m 54s) Loss: 0.0240(0.1276) Grad: 1961.2963  LR: 0.000004  \n","Epoch: [1][500/3575] Elapsed 2m 39s (remain 16m 15s) Loss: 0.0330(0.1088) Grad: 2541.0032  LR: 0.000006  \n","Epoch: [1][600/3575] Elapsed 3m 9s (remain 15m 39s) Loss: 0.0169(0.0955) Grad: 3038.3962  LR: 0.000007  \n","Epoch: [1][700/3575] Elapsed 3m 40s (remain 15m 4s) Loss: 0.0060(0.0844) Grad: 4109.4014  LR: 0.000008  \n","Epoch: [1][800/3575] Elapsed 4m 11s (remain 14m 31s) Loss: 0.0025(0.0758) Grad: 3440.6069  LR: 0.000009  \n","Epoch: [1][900/3575] Elapsed 4m 42s (remain 13m 57s) Loss: 0.0302(0.0686) Grad: 9061.0332  LR: 0.000010  \n","Epoch: [1][1000/3575] Elapsed 5m 13s (remain 13m 24s) Loss: 0.0085(0.0630) Grad: 4526.1704  LR: 0.000011  \n","Epoch: [1][1100/3575] Elapsed 5m 43s (remain 12m 52s) Loss: 0.0004(0.0585) Grad: 470.8829  LR: 0.000012  \n","Epoch: [1][1200/3575] Elapsed 6m 14s (remain 12m 20s) Loss: 0.0027(0.0549) Grad: 4394.9302  LR: 0.000013  \n","Epoch: [1][1300/3575] Elapsed 6m 43s (remain 11m 44s) Loss: 0.0051(0.0516) Grad: 36705.2188  LR: 0.000015  \n","Epoch: [1][1400/3575] Elapsed 7m 10s (remain 11m 8s) Loss: 0.0013(0.0489) Grad: 1663.6980  LR: 0.000016  \n","Epoch: [1][1500/3575] Elapsed 7m 38s (remain 10m 32s) Loss: 0.0011(0.0462) Grad: 2302.1748  LR: 0.000017  \n","Epoch: [1][1600/3575] Elapsed 8m 5s (remain 9m 58s) Loss: 0.0017(0.0440) Grad: 1104.4480  LR: 0.000018  \n","Epoch: [1][1700/3575] Elapsed 8m 33s (remain 9m 25s) Loss: 0.0004(0.0419) Grad: 261.5602  LR: 0.000019  \n","Epoch: [1][1800/3575] Elapsed 9m 0s (remain 8m 52s) Loss: 0.0562(0.0402) Grad: 10626.2529  LR: 0.000020  \n","Epoch: [1][1900/3575] Elapsed 9m 27s (remain 8m 20s) Loss: 0.0067(0.0385) Grad: 6484.6055  LR: 0.000020  \n","Epoch: [1][2000/3575] Elapsed 9m 55s (remain 7m 48s) Loss: 0.0209(0.0372) Grad: 4171.3628  LR: 0.000020  \n","Epoch: [1][2100/3575] Elapsed 10m 22s (remain 7m 16s) Loss: 0.0071(0.0358) Grad: 3700.5098  LR: 0.000020  \n","Epoch: [1][2200/3575] Elapsed 10m 49s (remain 6m 45s) Loss: 0.0013(0.0347) Grad: 937.2045  LR: 0.000019  \n","Epoch: [1][2300/3575] Elapsed 11m 17s (remain 6m 14s) Loss: 0.0019(0.0335) Grad: 3259.8979  LR: 0.000019  \n","Epoch: [1][2400/3575] Elapsed 11m 44s (remain 5m 44s) Loss: 0.0059(0.0324) Grad: 12922.5332  LR: 0.000019  \n","Epoch: [1][2500/3575] Elapsed 12m 12s (remain 5m 14s) Loss: 0.0000(0.0314) Grad: 61.9468  LR: 0.000019  \n","Epoch: [1][2600/3575] Elapsed 12m 39s (remain 4m 44s) Loss: 0.0042(0.0306) Grad: 2012.7439  LR: 0.000019  \n","Epoch: [1][2700/3575] Elapsed 13m 6s (remain 4m 14s) Loss: 0.0004(0.0298) Grad: 394.1018  LR: 0.000019  \n","Epoch: [1][2800/3575] Elapsed 13m 33s (remain 3m 44s) Loss: 0.0014(0.0291) Grad: 1085.5154  LR: 0.000019  \n","Epoch: [1][2900/3575] Elapsed 14m 0s (remain 3m 15s) Loss: 0.0009(0.0283) Grad: 454.9180  LR: 0.000019  \n","Epoch: [1][3000/3575] Elapsed 14m 28s (remain 2m 46s) Loss: 0.0055(0.0277) Grad: 9911.5977  LR: 0.000018  \n","Epoch: [1][3100/3575] Elapsed 14m 55s (remain 2m 16s) Loss: 0.0016(0.0271) Grad: 1677.6666  LR: 0.000018  \n","Epoch: [1][3200/3575] Elapsed 15m 22s (remain 1m 47s) Loss: 0.0028(0.0264) Grad: 4058.7971  LR: 0.000018  \n","Epoch: [1][3300/3575] Elapsed 15m 50s (remain 1m 18s) Loss: 0.0002(0.0259) Grad: 119.2630  LR: 0.000018  \n","Epoch: [1][3400/3575] Elapsed 16m 17s (remain 0m 50s) Loss: 0.0018(0.0254) Grad: 1867.2867  LR: 0.000018  \n","Epoch: [1][3500/3575] Elapsed 16m 44s (remain 0m 21s) Loss: 0.0025(0.0249) Grad: 1529.0358  LR: 0.000018  \n","Epoch: [1][3574/3575] Elapsed 17m 4s (remain 0m 0s) Loss: 0.0030(0.0245) Grad: 6075.2007  LR: 0.000018  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 13m 56s) Loss: 0.0003(0.0003) \n","EVAL: [100/1192] Elapsed 0m 15s (remain 2m 48s) Loss: 0.0016(0.0045) \n","EVAL: [200/1192] Elapsed 0m 30s (remain 2m 30s) Loss: 0.0022(0.0060) \n","EVAL: [300/1192] Elapsed 0m 45s (remain 2m 14s) Loss: 0.0034(0.0078) \n","EVAL: [400/1192] Elapsed 1m 0s (remain 1m 58s) Loss: 0.0146(0.0079) \n","EVAL: [500/1192] Elapsed 1m 15s (remain 1m 43s) Loss: 0.0131(0.0078) \n","EVAL: [600/1192] Elapsed 1m 29s (remain 1m 28s) Loss: 0.1038(0.0083) \n","EVAL: [700/1192] Elapsed 1m 44s (remain 1m 13s) Loss: 0.0127(0.0092) \n","EVAL: [800/1192] Elapsed 1m 59s (remain 0m 58s) Loss: 0.0309(0.0089) \n","EVAL: [900/1192] Elapsed 2m 14s (remain 0m 43s) Loss: 0.0075(0.0089) \n","EVAL: [1000/1192] Elapsed 2m 29s (remain 0m 28s) Loss: 0.0003(0.0086) \n","EVAL: [1100/1192] Elapsed 2m 44s (remain 0m 13s) Loss: 0.0046(0.0081) \n","EVAL: [1191/1192] Elapsed 2m 58s (remain 0m 0s) Loss: 0.0052(0.0077) \n","Epoch 1 - avg_train_loss: 0.0245  avg_val_loss: 0.0077  time: 1329s\n","Epoch 1 - Score: 0.8535\n","Epoch 1 - Save Best Score: 0.8535 Model\n","Epoch: [2][0/3575] Elapsed 0m 0s (remain 41m 2s) Loss: 0.0123(0.0123) Grad: 9409.6367  LR: 0.000018  \n","Epoch: [2][100/3575] Elapsed 0m 25s (remain 14m 24s) Loss: 0.0000(0.0086) Grad: 78.6954  LR: 0.000018  \n","Epoch: [2][200/3575] Elapsed 0m 49s (remain 13m 54s) Loss: 0.0000(0.0079) Grad: 816.5635  LR: 0.000018  \n","Epoch: [2][300/3575] Elapsed 1m 11s (remain 12m 54s) Loss: 0.0007(0.0074) Grad: 2624.0996  LR: 0.000017  \n","Epoch: [2][400/3575] Elapsed 1m 32s (remain 12m 14s) Loss: 0.0000(0.0072) Grad: 123.9336  LR: 0.000017  \n","Epoch: [2][500/3575] Elapsed 1m 54s (remain 11m 41s) Loss: 0.0000(0.0071) Grad: 104.1498  LR: 0.000017  \n","Epoch: [2][600/3575] Elapsed 2m 15s (remain 11m 12s) Loss: 0.0167(0.0072) Grad: 8963.5547  LR: 0.000017  \n","Epoch: [2][700/3575] Elapsed 2m 37s (remain 10m 45s) Loss: 0.0000(0.0071) Grad: 83.1610  LR: 0.000017  \n","Epoch: [2][800/3575] Elapsed 2m 59s (remain 10m 20s) Loss: 0.0026(0.0071) Grad: 5778.7749  LR: 0.000017  \n","Epoch: [2][900/3575] Elapsed 3m 20s (remain 9m 55s) Loss: 0.0032(0.0069) Grad: 8248.7100  LR: 0.000017  \n","Epoch: [2][1000/3575] Elapsed 3m 41s (remain 9m 30s) Loss: 0.0032(0.0068) Grad: 6577.9717  LR: 0.000017  \n","Epoch: [2][1100/3575] Elapsed 4m 3s (remain 9m 6s) Loss: 0.0001(0.0067) Grad: 196.5563  LR: 0.000016  \n","Epoch: [2][1200/3575] Elapsed 4m 24s (remain 8m 42s) Loss: 0.0001(0.0067) Grad: 294.4816  LR: 0.000016  \n","Epoch: [2][1300/3575] Elapsed 4m 46s (remain 8m 19s) Loss: 0.0058(0.0064) Grad: 13722.2998  LR: 0.000016  \n","Epoch: [2][1400/3575] Elapsed 5m 7s (remain 7m 57s) Loss: 0.0002(0.0065) Grad: 700.8382  LR: 0.000016  \n","Epoch: [2][1500/3575] Elapsed 5m 29s (remain 7m 34s) Loss: 0.0060(0.0065) Grad: 23658.1836  LR: 0.000016  \n","Epoch: [2][1600/3575] Elapsed 5m 50s (remain 7m 12s) Loss: 0.0029(0.0064) Grad: 5977.1460  LR: 0.000016  \n","Epoch: [2][1700/3575] Elapsed 6m 12s (remain 6m 50s) Loss: 0.0003(0.0063) Grad: 13723.7275  LR: 0.000016  \n","Epoch: [2][1800/3575] Elapsed 6m 33s (remain 6m 27s) Loss: 0.0259(0.0063) Grad: 35947.7656  LR: 0.000016  \n","Epoch: [2][1900/3575] Elapsed 6m 55s (remain 6m 5s) Loss: 0.0007(0.0063) Grad: 2293.1057  LR: 0.000015  \n","Epoch: [2][2000/3575] Elapsed 7m 16s (remain 5m 43s) Loss: 0.0001(0.0062) Grad: 351.2983  LR: 0.000015  \n","Epoch: [2][2100/3575] Elapsed 7m 38s (remain 5m 21s) Loss: 0.0043(0.0062) Grad: 4663.6196  LR: 0.000015  \n","Epoch: [2][2200/3575] Elapsed 7m 59s (remain 4m 59s) Loss: 0.0073(0.0062) Grad: 17908.9492  LR: 0.000015  \n","Epoch: [2][2300/3575] Elapsed 8m 21s (remain 4m 37s) Loss: 0.0005(0.0062) Grad: 1703.6307  LR: 0.000015  \n","Epoch: [2][2400/3575] Elapsed 8m 43s (remain 4m 15s) Loss: 0.0022(0.0062) Grad: 4630.0894  LR: 0.000015  \n","Epoch: [2][2500/3575] Elapsed 9m 4s (remain 3m 53s) Loss: 0.0000(0.0061) Grad: 75.9336  LR: 0.000015  \n","Epoch: [2][2600/3575] Elapsed 9m 25s (remain 3m 31s) Loss: 0.0001(0.0061) Grad: 223.5132  LR: 0.000015  \n","Epoch: [2][2700/3575] Elapsed 9m 47s (remain 3m 10s) Loss: 0.0047(0.0061) Grad: 7434.5498  LR: 0.000014  \n","Epoch: [2][2800/3575] Elapsed 10m 8s (remain 2m 48s) Loss: 0.0033(0.0061) Grad: 5511.8809  LR: 0.000014  \n","Epoch: [2][2900/3575] Elapsed 10m 30s (remain 2m 26s) Loss: 0.0000(0.0061) Grad: 44.7711  LR: 0.000014  \n","Epoch: [2][3000/3575] Elapsed 10m 51s (remain 2m 4s) Loss: 0.0001(0.0061) Grad: 277.0123  LR: 0.000014  \n","Epoch: [2][3100/3575] Elapsed 11m 13s (remain 1m 42s) Loss: 0.0005(0.0060) Grad: 2283.8464  LR: 0.000014  \n","Epoch: [2][3200/3575] Elapsed 11m 34s (remain 1m 21s) Loss: 0.0115(0.0060) Grad: 12661.1143  LR: 0.000014  \n","Epoch: [2][3300/3575] Elapsed 11m 56s (remain 0m 59s) Loss: 0.0363(0.0060) Grad: 49439.4453  LR: 0.000014  \n","Epoch: [2][3400/3575] Elapsed 12m 17s (remain 0m 37s) Loss: 0.0272(0.0060) Grad: 39776.0859  LR: 0.000014  \n","Epoch: [2][3500/3575] Elapsed 12m 38s (remain 0m 16s) Loss: 0.0000(0.0060) Grad: 30.6243  LR: 0.000013  \n","Epoch: [2][3574/3575] Elapsed 12m 54s (remain 0m 0s) Loss: 0.0002(0.0060) Grad: 3857.2285  LR: 0.000013  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 9m 35s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 8s) Loss: 0.0001(0.0050) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 54s) Loss: 0.0007(0.0065) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 42s) Loss: 0.0003(0.0093) \n","EVAL: [400/1192] Elapsed 0m 45s (remain 1m 30s) Loss: 0.0232(0.0097) \n","EVAL: [500/1192] Elapsed 0m 57s (remain 1m 18s) Loss: 0.0187(0.0093) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 7s) Loss: 0.0923(0.0092) \n","EVAL: [700/1192] Elapsed 1m 20s (remain 0m 56s) Loss: 0.0202(0.0101) \n","EVAL: [800/1192] Elapsed 1m 31s (remain 0m 44s) Loss: 0.0043(0.0097) \n","EVAL: [900/1192] Elapsed 1m 42s (remain 0m 33s) Loss: 0.0025(0.0095) \n","EVAL: [1000/1192] Elapsed 1m 54s (remain 0m 21s) Loss: 0.0000(0.0091) \n","EVAL: [1100/1192] Elapsed 2m 5s (remain 0m 10s) Loss: 0.0056(0.0087) \n","EVAL: [1191/1192] Elapsed 2m 15s (remain 0m 0s) Loss: 0.0092(0.0081) \n","Epoch 2 - avg_train_loss: 0.0060  avg_val_loss: 0.0081  time: 916s\n","Epoch 2 - Score: 0.8726\n","Epoch 2 - Save Best Score: 0.8726 Model\n","Epoch: [3][0/3575] Elapsed 0m 0s (remain 41m 49s) Loss: 0.0000(0.0000) Grad: 1004.0419  LR: 0.000013  \n","Epoch: [3][100/3575] Elapsed 0m 24s (remain 13m 54s) Loss: 0.0003(0.0035) Grad: 6072.4282  LR: 0.000013  \n","Epoch: [3][200/3575] Elapsed 0m 49s (remain 13m 46s) Loss: 0.0000(0.0040) Grad: 30.9847  LR: 0.000013  \n","Epoch: [3][300/3575] Elapsed 1m 10s (remain 12m 48s) Loss: 0.0049(0.0050) Grad: 6817.0713  LR: 0.000013  \n","Epoch: [3][400/3575] Elapsed 1m 32s (remain 12m 10s) Loss: 0.0237(0.0049) Grad: 65694.8125  LR: 0.000013  \n","Epoch: [3][500/3575] Elapsed 1m 53s (remain 11m 37s) Loss: 0.0000(0.0048) Grad: 10.8034  LR: 0.000013  \n","Epoch: [3][600/3575] Elapsed 2m 15s (remain 11m 8s) Loss: 0.0011(0.0051) Grad: 6100.9150  LR: 0.000013  \n","Epoch: [3][700/3575] Elapsed 2m 36s (remain 10m 41s) Loss: 0.0097(0.0048) Grad: 14552.0889  LR: 0.000012  \n","Epoch: [3][800/3575] Elapsed 2m 57s (remain 10m 15s) Loss: 0.0010(0.0048) Grad: 3731.2747  LR: 0.000012  \n","Epoch: [3][900/3575] Elapsed 3m 19s (remain 9m 50s) Loss: 0.0016(0.0050) Grad: 6025.0312  LR: 0.000012  \n","Epoch: [3][1000/3575] Elapsed 3m 40s (remain 9m 26s) Loss: 0.0001(0.0050) Grad: 326.3708  LR: 0.000012  \n","Epoch: [3][1100/3575] Elapsed 4m 1s (remain 9m 3s) Loss: 0.0090(0.0050) Grad: 27981.0020  LR: 0.000012  \n","Epoch: [3][1200/3575] Elapsed 4m 22s (remain 8m 39s) Loss: 0.0012(0.0050) Grad: 6753.1846  LR: 0.000012  \n","Epoch: [3][1300/3575] Elapsed 4m 44s (remain 8m 16s) Loss: 0.0001(0.0049) Grad: 517.5110  LR: 0.000012  \n","Epoch: [3][1400/3575] Elapsed 5m 5s (remain 7m 54s) Loss: 0.0000(0.0048) Grad: 93.2806  LR: 0.000012  \n","Epoch: [3][1500/3575] Elapsed 5m 26s (remain 7m 31s) Loss: 0.0141(0.0048) Grad: 9192.0693  LR: 0.000011  \n","Epoch: [3][1600/3575] Elapsed 5m 48s (remain 7m 9s) Loss: 0.0094(0.0048) Grad: 18222.8008  LR: 0.000011  \n","Epoch: [3][1700/3575] Elapsed 6m 9s (remain 6m 47s) Loss: 0.0212(0.0048) Grad: 39530.6055  LR: 0.000011  \n","Epoch: [3][1800/3575] Elapsed 6m 31s (remain 6m 25s) Loss: 0.0021(0.0048) Grad: 7023.8188  LR: 0.000011  \n","Epoch: [3][1900/3575] Elapsed 6m 52s (remain 6m 3s) Loss: 0.0003(0.0048) Grad: 1727.6548  LR: 0.000011  \n","Epoch: [3][2000/3575] Elapsed 7m 13s (remain 5m 41s) Loss: 0.0001(0.0048) Grad: 190.1659  LR: 0.000011  \n","Epoch: [3][2100/3575] Elapsed 7m 35s (remain 5m 19s) Loss: 0.0000(0.0048) Grad: 42.7245  LR: 0.000011  \n","Epoch: [3][2200/3575] Elapsed 7m 56s (remain 4m 57s) Loss: 0.0000(0.0048) Grad: 89.1093  LR: 0.000011  \n","Epoch: [3][2300/3575] Elapsed 8m 18s (remain 4m 35s) Loss: 0.0003(0.0048) Grad: 2681.7173  LR: 0.000010  \n","Epoch: [3][2400/3575] Elapsed 8m 39s (remain 4m 14s) Loss: 0.0000(0.0048) Grad: 10.3194  LR: 0.000010  \n","Epoch: [3][2500/3575] Elapsed 9m 0s (remain 3m 52s) Loss: 0.0145(0.0048) Grad: 22308.7852  LR: 0.000010  \n","Epoch: [3][2600/3575] Elapsed 9m 22s (remain 3m 30s) Loss: 0.0003(0.0047) Grad: 2918.6628  LR: 0.000010  \n","Epoch: [3][2700/3575] Elapsed 9m 43s (remain 3m 8s) Loss: 0.0000(0.0047) Grad: 61.6694  LR: 0.000010  \n","Epoch: [3][2800/3575] Elapsed 10m 4s (remain 2m 47s) Loss: 0.0041(0.0048) Grad: 10808.7783  LR: 0.000010  \n","Epoch: [3][2900/3575] Elapsed 10m 25s (remain 2m 25s) Loss: 0.0247(0.0048) Grad: 38090.3047  LR: 0.000010  \n","Epoch: [3][3000/3575] Elapsed 10m 47s (remain 2m 3s) Loss: 0.0001(0.0047) Grad: 1103.6964  LR: 0.000010  \n","Epoch: [3][3100/3575] Elapsed 11m 8s (remain 1m 42s) Loss: 0.0000(0.0047) Grad: 61.7464  LR: 0.000009  \n","Epoch: [3][3200/3575] Elapsed 11m 29s (remain 1m 20s) Loss: 0.0000(0.0048) Grad: 12.4600  LR: 0.000009  \n","Epoch: [3][3300/3575] Elapsed 11m 50s (remain 0m 59s) Loss: 0.0193(0.0048) Grad: 22525.9180  LR: 0.000009  \n","Epoch: [3][3400/3575] Elapsed 12m 11s (remain 0m 37s) Loss: 0.0000(0.0048) Grad: 15.9260  LR: 0.000009  \n","Epoch: [3][3500/3575] Elapsed 12m 33s (remain 0m 15s) Loss: 0.0014(0.0048) Grad: 6037.6123  LR: 0.000009  \n","Epoch: [3][3574/3575] Elapsed 12m 48s (remain 0m 0s) Loss: 0.0018(0.0048) Grad: 6828.9326  LR: 0.000009  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 9m 3s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 7s) Loss: 0.0000(0.0045) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 54s) Loss: 0.0002(0.0062) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 41s) Loss: 0.0004(0.0091) \n","EVAL: [400/1192] Elapsed 0m 45s (remain 1m 30s) Loss: 0.0222(0.0095) \n","EVAL: [500/1192] Elapsed 0m 57s (remain 1m 18s) Loss: 0.0220(0.0090) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 7s) Loss: 0.0871(0.0090) \n","EVAL: [700/1192] Elapsed 1m 19s (remain 0m 55s) Loss: 0.0176(0.0102) \n","EVAL: [800/1192] Elapsed 1m 30s (remain 0m 44s) Loss: 0.0425(0.0096) \n","EVAL: [900/1192] Elapsed 1m 42s (remain 0m 33s) Loss: 0.0004(0.0094) \n","EVAL: [1000/1192] Elapsed 1m 53s (remain 0m 21s) Loss: 0.0000(0.0090) \n","EVAL: [1100/1192] Elapsed 2m 4s (remain 0m 10s) Loss: 0.0047(0.0086) \n","EVAL: [1191/1192] Elapsed 2m 15s (remain 0m 0s) Loss: 0.0075(0.0081) \n","Epoch 3 - avg_train_loss: 0.0048  avg_val_loss: 0.0081  time: 909s\n","Epoch 3 - Score: 0.8753\n","Epoch 3 - Save Best Score: 0.8753 Model\n","Epoch: [4][0/3575] Elapsed 0m 0s (remain 36m 26s) Loss: 0.0080(0.0080) Grad: 79556.6484  LR: 0.000009  \n","Epoch: [4][100/3575] Elapsed 0m 24s (remain 13m 53s) Loss: 0.0007(0.0027) Grad: 2093.2646  LR: 0.000009  \n","Epoch: [4][200/3575] Elapsed 0m 49s (remain 13m 46s) Loss: 0.0000(0.0038) Grad: 48.9749  LR: 0.000009  \n","Epoch: [4][300/3575] Elapsed 1m 10s (remain 12m 48s) Loss: 0.0019(0.0038) Grad: 13007.8203  LR: 0.000009  \n","Epoch: [4][400/3575] Elapsed 1m 32s (remain 12m 8s) Loss: 0.0000(0.0042) Grad: 95.6173  LR: 0.000008  \n","Epoch: [4][500/3575] Elapsed 1m 53s (remain 11m 35s) Loss: 0.0000(0.0042) Grad: 234.6633  LR: 0.000008  \n","Epoch: [4][600/3575] Elapsed 2m 14s (remain 11m 6s) Loss: 0.0029(0.0042) Grad: 4897.9087  LR: 0.000008  \n","Epoch: [4][700/3575] Elapsed 2m 35s (remain 10m 39s) Loss: 0.0021(0.0042) Grad: 24695.1758  LR: 0.000008  \n","Epoch: [4][800/3575] Elapsed 2m 57s (remain 10m 13s) Loss: 0.0002(0.0043) Grad: 916.7521  LR: 0.000008  \n","Epoch: [4][900/3575] Elapsed 3m 18s (remain 9m 48s) Loss: 0.0053(0.0042) Grad: 12319.4658  LR: 0.000008  \n","Epoch: [4][1000/3575] Elapsed 3m 39s (remain 9m 24s) Loss: 0.0092(0.0042) Grad: 21447.3848  LR: 0.000008  \n","Epoch: [4][1100/3575] Elapsed 4m 0s (remain 9m 0s) Loss: 0.0002(0.0042) Grad: 2614.1714  LR: 0.000008  \n","Epoch: [4][1200/3575] Elapsed 4m 21s (remain 8m 37s) Loss: 0.0000(0.0040) Grad: 215.3208  LR: 0.000007  \n","Epoch: [4][1300/3575] Elapsed 4m 42s (remain 8m 14s) Loss: 0.0220(0.0040) Grad: 22397.1250  LR: 0.000007  \n","Epoch: [4][1400/3575] Elapsed 5m 3s (remain 7m 51s) Loss: 0.0006(0.0040) Grad: 15862.6543  LR: 0.000007  \n","Epoch: [4][1500/3575] Elapsed 5m 24s (remain 7m 29s) Loss: 0.0036(0.0040) Grad: 7849.8955  LR: 0.000007  \n","Epoch: [4][1600/3575] Elapsed 5m 46s (remain 7m 6s) Loss: 0.0000(0.0039) Grad: 14.4056  LR: 0.000007  \n","Epoch: [4][1700/3575] Elapsed 6m 7s (remain 6m 44s) Loss: 0.0011(0.0038) Grad: 6364.6709  LR: 0.000007  \n","Epoch: [4][1800/3575] Elapsed 6m 28s (remain 6m 22s) Loss: 0.0000(0.0038) Grad: 11.3468  LR: 0.000007  \n","Epoch: [4][1900/3575] Elapsed 6m 50s (remain 6m 1s) Loss: 0.0077(0.0038) Grad: 54134.6250  LR: 0.000007  \n","Epoch: [4][2000/3575] Elapsed 7m 11s (remain 5m 39s) Loss: 0.0033(0.0038) Grad: 10896.4922  LR: 0.000006  \n","Epoch: [4][2100/3575] Elapsed 7m 32s (remain 5m 17s) Loss: 0.0006(0.0037) Grad: 4849.3594  LR: 0.000006  \n","Epoch: [4][2200/3575] Elapsed 7m 54s (remain 4m 55s) Loss: 0.0018(0.0037) Grad: 5907.3276  LR: 0.000006  \n","Epoch: [4][2300/3575] Elapsed 8m 15s (remain 4m 34s) Loss: 0.0059(0.0037) Grad: 79509.5703  LR: 0.000006  \n","Epoch: [4][2400/3575] Elapsed 8m 36s (remain 4m 12s) Loss: 0.0000(0.0037) Grad: 11.7552  LR: 0.000006  \n","Epoch: [4][2500/3575] Elapsed 8m 58s (remain 3m 51s) Loss: 0.0104(0.0037) Grad: 125529.1016  LR: 0.000006  \n","Epoch: [4][2600/3575] Elapsed 9m 19s (remain 3m 29s) Loss: 0.0023(0.0037) Grad: 21098.6289  LR: 0.000006  \n","Epoch: [4][2700/3575] Elapsed 9m 40s (remain 3m 7s) Loss: 0.0000(0.0037) Grad: 29.5919  LR: 0.000006  \n","Epoch: [4][2800/3575] Elapsed 10m 1s (remain 2m 46s) Loss: 0.0000(0.0037) Grad: 13.6761  LR: 0.000005  \n","Epoch: [4][2900/3575] Elapsed 10m 22s (remain 2m 24s) Loss: 0.0000(0.0036) Grad: 45.9370  LR: 0.000005  \n","Epoch: [4][3000/3575] Elapsed 10m 43s (remain 2m 3s) Loss: 0.0000(0.0037) Grad: 21.8174  LR: 0.000005  \n","Epoch: [4][3100/3575] Elapsed 11m 4s (remain 1m 41s) Loss: 0.0044(0.0037) Grad: 6025.5889  LR: 0.000005  \n","Epoch: [4][3200/3575] Elapsed 11m 25s (remain 1m 20s) Loss: 0.0078(0.0037) Grad: 6579.6660  LR: 0.000005  \n","Epoch: [4][3300/3575] Elapsed 11m 47s (remain 0m 58s) Loss: 0.0041(0.0037) Grad: 18531.5605  LR: 0.000005  \n","Epoch: [4][3400/3575] Elapsed 12m 8s (remain 0m 37s) Loss: 0.0006(0.0037) Grad: 2122.0955  LR: 0.000005  \n","Epoch: [4][3500/3575] Elapsed 12m 29s (remain 0m 15s) Loss: 0.0002(0.0037) Grad: 729.8835  LR: 0.000005  \n","Epoch: [4][3574/3575] Elapsed 12m 45s (remain 0m 0s) Loss: 0.0078(0.0037) Grad: 63608.6562  LR: 0.000004  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 9m 3s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 7s) Loss: 0.0000(0.0056) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 53s) Loss: 0.0000(0.0070) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 41s) Loss: 0.0001(0.0102) \n","EVAL: [400/1192] Elapsed 0m 45s (remain 1m 30s) Loss: 0.0321(0.0107) \n","EVAL: [500/1192] Elapsed 0m 57s (remain 1m 18s) Loss: 0.0229(0.0101) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 7s) Loss: 0.0911(0.0099) \n","EVAL: [700/1192] Elapsed 1m 20s (remain 0m 56s) Loss: 0.0213(0.0112) \n","EVAL: [800/1192] Elapsed 1m 31s (remain 0m 44s) Loss: 0.0372(0.0107) \n","EVAL: [900/1192] Elapsed 1m 42s (remain 0m 33s) Loss: 0.0025(0.0105) \n","EVAL: [1000/1192] Elapsed 1m 54s (remain 0m 21s) Loss: 0.0000(0.0101) \n","EVAL: [1100/1192] Elapsed 2m 5s (remain 0m 10s) Loss: 0.0052(0.0096) \n","EVAL: [1191/1192] Elapsed 2m 15s (remain 0m 0s) Loss: 0.0086(0.0090) \n","Epoch 4 - avg_train_loss: 0.0037  avg_val_loss: 0.0090  time: 906s\n","Epoch 4 - Score: 0.8788\n","Epoch 4 - Save Best Score: 0.8788 Model\n","Epoch: [5][0/3575] Elapsed 0m 0s (remain 39m 53s) Loss: 0.0000(0.0000) Grad: 1000.5255  LR: 0.000004  \n","Epoch: [5][100/3575] Elapsed 0m 25s (remain 14m 27s) Loss: 0.0037(0.0035) Grad: 11521.4941  LR: 0.000004  \n","Epoch: [5][200/3575] Elapsed 0m 49s (remain 13m 53s) Loss: 0.0007(0.0028) Grad: 2784.1453  LR: 0.000004  \n","Epoch: [5][300/3575] Elapsed 1m 11s (remain 12m 53s) Loss: 0.0130(0.0038) Grad: 26918.3750  LR: 0.000004  \n","Epoch: [5][400/3575] Elapsed 1m 32s (remain 12m 12s) Loss: 0.0000(0.0037) Grad: 62.2646  LR: 0.000004  \n","Epoch: [5][500/3575] Elapsed 1m 53s (remain 11m 38s) Loss: 0.0041(0.0036) Grad: 16321.2910  LR: 0.000004  \n","Epoch: [5][600/3575] Elapsed 2m 15s (remain 11m 8s) Loss: 0.0000(0.0035) Grad: 59.9476  LR: 0.000004  \n","Epoch: [5][700/3575] Elapsed 2m 36s (remain 10m 41s) Loss: 0.0037(0.0036) Grad: 13345.0879  LR: 0.000004  \n","Epoch: [5][800/3575] Elapsed 2m 58s (remain 10m 17s) Loss: 0.0000(0.0036) Grad: 18.2946  LR: 0.000003  \n","Epoch: [5][900/3575] Elapsed 3m 19s (remain 9m 52s) Loss: 0.0056(0.0036) Grad: 28140.4453  LR: 0.000003  \n","Epoch: [5][1000/3575] Elapsed 3m 41s (remain 9m 28s) Loss: 0.0001(0.0035) Grad: 414.8684  LR: 0.000003  \n","Epoch: [5][1100/3575] Elapsed 4m 2s (remain 9m 4s) Loss: 0.0001(0.0034) Grad: 535.0565  LR: 0.000003  \n","Epoch: [5][1200/3575] Elapsed 4m 24s (remain 8m 42s) Loss: 0.0000(0.0033) Grad: 180.2346  LR: 0.000003  \n","Epoch: [5][1300/3575] Elapsed 4m 45s (remain 8m 19s) Loss: 0.0005(0.0032) Grad: 4287.4170  LR: 0.000003  \n","Epoch: [5][1400/3575] Elapsed 5m 6s (remain 7m 55s) Loss: 0.0000(0.0031) Grad: 42.8980  LR: 0.000003  \n","Epoch: [5][1500/3575] Elapsed 5m 28s (remain 7m 33s) Loss: 0.0097(0.0032) Grad: 16274.1084  LR: 0.000003  \n","Epoch: [5][1600/3575] Elapsed 5m 49s (remain 7m 10s) Loss: 0.0000(0.0031) Grad: 2.6903  LR: 0.000002  \n","Epoch: [5][1700/3575] Elapsed 6m 10s (remain 6m 48s) Loss: 0.0010(0.0031) Grad: 30150.2559  LR: 0.000002  \n","Epoch: [5][1800/3575] Elapsed 6m 32s (remain 6m 26s) Loss: 0.0000(0.0030) Grad: 18.2585  LR: 0.000002  \n","Epoch: [5][1900/3575] Elapsed 6m 53s (remain 6m 4s) Loss: 0.0007(0.0031) Grad: 3649.4553  LR: 0.000002  \n","Epoch: [5][2000/3575] Elapsed 7m 15s (remain 5m 42s) Loss: 0.0000(0.0030) Grad: 12.5158  LR: 0.000002  \n","Epoch: [5][2100/3575] Elapsed 7m 36s (remain 5m 20s) Loss: 0.0002(0.0031) Grad: 3364.2585  LR: 0.000002  \n","Epoch: [5][2200/3575] Elapsed 7m 58s (remain 4m 58s) Loss: 0.0000(0.0031) Grad: 31.7505  LR: 0.000002  \n","Epoch: [5][2300/3575] Elapsed 8m 19s (remain 4m 36s) Loss: 0.0000(0.0032) Grad: 20.1147  LR: 0.000002  \n","Epoch: [5][2400/3575] Elapsed 8m 41s (remain 4m 14s) Loss: 0.0025(0.0032) Grad: 10100.4043  LR: 0.000001  \n","Epoch: [5][2500/3575] Elapsed 9m 2s (remain 3m 52s) Loss: 0.0001(0.0032) Grad: 1054.9011  LR: 0.000001  \n","Epoch: [5][2600/3575] Elapsed 9m 23s (remain 3m 31s) Loss: 0.0001(0.0031) Grad: 980.4258  LR: 0.000001  \n","Epoch: [5][2700/3575] Elapsed 9m 45s (remain 3m 9s) Loss: 0.0291(0.0031) Grad: 32009.6738  LR: 0.000001  \n","Epoch: [5][2800/3575] Elapsed 10m 6s (remain 2m 47s) Loss: 0.0021(0.0031) Grad: 10379.4404  LR: 0.000001  \n","Epoch: [5][2900/3575] Elapsed 10m 28s (remain 2m 25s) Loss: 0.0000(0.0031) Grad: 106.6872  LR: 0.000001  \n","Epoch: [5][3000/3575] Elapsed 10m 49s (remain 2m 4s) Loss: 0.0036(0.0031) Grad: 9039.1455  LR: 0.000001  \n","Epoch: [5][3100/3575] Elapsed 11m 10s (remain 1m 42s) Loss: 0.0000(0.0030) Grad: 24.0849  LR: 0.000001  \n","Epoch: [5][3200/3575] Elapsed 11m 32s (remain 1m 20s) Loss: 0.0001(0.0030) Grad: 800.1154  LR: 0.000000  \n","Epoch: [5][3300/3575] Elapsed 11m 53s (remain 0m 59s) Loss: 0.0000(0.0030) Grad: 31.3609  LR: 0.000000  \n","Epoch: [5][3400/3575] Elapsed 12m 15s (remain 0m 37s) Loss: 0.0011(0.0030) Grad: 11740.3721  LR: 0.000000  \n","Epoch: [5][3500/3575] Elapsed 12m 36s (remain 0m 15s) Loss: 0.0006(0.0031) Grad: 5671.1577  LR: 0.000000  \n","Epoch: [5][3574/3575] Elapsed 12m 52s (remain 0m 0s) Loss: 0.0051(0.0031) Grad: 18656.1523  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 9m 13s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 8s) Loss: 0.0000(0.0057) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 54s) Loss: 0.0000(0.0076) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 42s) Loss: 0.0001(0.0111) \n","EVAL: [400/1192] Elapsed 0m 46s (remain 1m 30s) Loss: 0.0333(0.0117) \n","EVAL: [500/1192] Elapsed 0m 57s (remain 1m 19s) Loss: 0.0255(0.0110) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 7s) Loss: 0.1106(0.0109) \n","EVAL: [700/1192] Elapsed 1m 20s (remain 0m 56s) Loss: 0.0194(0.0123) \n","EVAL: [800/1192] Elapsed 1m 31s (remain 0m 44s) Loss: 0.0444(0.0119) \n","EVAL: [900/1192] Elapsed 1m 42s (remain 0m 33s) Loss: 0.0008(0.0117) \n","EVAL: [1000/1192] Elapsed 1m 54s (remain 0m 21s) Loss: 0.0000(0.0113) \n","EVAL: [1100/1192] Elapsed 2m 5s (remain 0m 10s) Loss: 0.0058(0.0107) \n","EVAL: [1191/1192] Elapsed 2m 15s (remain 0m 0s) Loss: 0.0085(0.0101) \n","Epoch 5 - avg_train_loss: 0.0031  avg_val_loss: 0.0101  time: 914s\n","Epoch 5 - Score: 0.8783\n","========== fold: 2 training ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight']\n","- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\n","Epoch: [1][0/3575] Elapsed 0m 1s (remain 67m 10s) Loss: 0.3716(0.3716) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/3575] Elapsed 0m 31s (remain 18m 10s) Loss: 0.2549(0.3176) Grad: 63788.8125  LR: 0.000001  \n","Epoch: [1][200/3575] Elapsed 1m 2s (remain 17m 27s) Loss: 0.0531(0.2337) Grad: 14337.9639  LR: 0.000002  \n","Epoch: [1][300/3575] Elapsed 1m 33s (remain 16m 52s) Loss: 0.0526(0.1711) Grad: 3001.9678  LR: 0.000003  \n","Epoch: [1][400/3575] Elapsed 2m 3s (remain 16m 19s) Loss: 0.0168(0.1385) Grad: 2639.8284  LR: 0.000004  \n","Epoch: [1][500/3575] Elapsed 2m 34s (remain 15m 47s) Loss: 0.0325(0.1176) Grad: 2102.5203  LR: 0.000006  \n","Epoch: [1][600/3575] Elapsed 3m 5s (remain 15m 16s) Loss: 0.0367(0.1034) Grad: 4118.3242  LR: 0.000007  \n","Epoch: [1][700/3575] Elapsed 3m 35s (remain 14m 45s) Loss: 0.0213(0.0918) Grad: 9051.6992  LR: 0.000008  \n","Epoch: [1][800/3575] Elapsed 4m 6s (remain 14m 14s) Loss: 0.0216(0.0827) Grad: 11107.6484  LR: 0.000009  \n","Epoch: [1][900/3575] Elapsed 4m 37s (remain 13m 42s) Loss: 0.0080(0.0752) Grad: 4223.0586  LR: 0.000010  \n","Epoch: [1][1000/3575] Elapsed 5m 8s (remain 13m 12s) Loss: 0.0061(0.0692) Grad: 4658.5156  LR: 0.000011  \n","Epoch: [1][1100/3575] Elapsed 5m 38s (remain 12m 41s) Loss: 0.1524(0.0640) Grad: 37864.2344  LR: 0.000012  \n","Epoch: [1][1200/3575] Elapsed 6m 9s (remain 12m 10s) Loss: 0.0061(0.0596) Grad: 7067.8140  LR: 0.000013  \n","Epoch: [1][1300/3575] Elapsed 6m 30s (remain 11m 23s) Loss: 0.0233(0.0558) Grad: 6922.0845  LR: 0.000015  \n","Epoch: [1][1400/3575] Elapsed 6m 52s (remain 10m 40s) Loss: 0.0040(0.0525) Grad: 2602.8496  LR: 0.000016  \n","Epoch: [1][1500/3575] Elapsed 7m 13s (remain 9m 59s) Loss: 0.0020(0.0497) Grad: 1313.4166  LR: 0.000017  \n","Epoch: [1][1600/3575] Elapsed 7m 35s (remain 9m 21s) Loss: 0.0187(0.0472) Grad: 32063.6172  LR: 0.000018  \n","Epoch: [1][1700/3575] Elapsed 7m 56s (remain 8m 45s) Loss: 0.0197(0.0450) Grad: 6365.5024  LR: 0.000019  \n","Epoch: [1][1800/3575] Elapsed 8m 18s (remain 8m 10s) Loss: 0.0050(0.0429) Grad: 3423.3840  LR: 0.000020  \n","Epoch: [1][1900/3575] Elapsed 8m 39s (remain 7m 37s) Loss: 0.0022(0.0413) Grad: 1653.3741  LR: 0.000020  \n","Epoch: [1][2000/3575] Elapsed 9m 1s (remain 7m 5s) Loss: 0.0300(0.0398) Grad: 11045.6572  LR: 0.000020  \n","Epoch: [1][2100/3575] Elapsed 9m 22s (remain 6m 34s) Loss: 0.0059(0.0383) Grad: 15525.9004  LR: 0.000020  \n","Epoch: [1][2200/3575] Elapsed 9m 44s (remain 6m 4s) Loss: 0.0082(0.0370) Grad: 34727.1602  LR: 0.000019  \n","Epoch: [1][2300/3575] Elapsed 10m 5s (remain 5m 35s) Loss: 0.0044(0.0358) Grad: 2676.6885  LR: 0.000019  \n","Epoch: [1][2400/3575] Elapsed 10m 26s (remain 5m 6s) Loss: 0.0152(0.0347) Grad: 4537.6401  LR: 0.000019  \n","Epoch: [1][2500/3575] Elapsed 10m 48s (remain 4m 38s) Loss: 0.0020(0.0336) Grad: 4017.2000  LR: 0.000019  \n","Epoch: [1][2600/3575] Elapsed 11m 9s (remain 4m 10s) Loss: 0.0052(0.0326) Grad: 13913.8242  LR: 0.000019  \n","Epoch: [1][2700/3575] Elapsed 11m 30s (remain 3m 43s) Loss: 0.0000(0.0317) Grad: 61.6122  LR: 0.000019  \n","Epoch: [1][2800/3575] Elapsed 11m 52s (remain 3m 16s) Loss: 0.0001(0.0308) Grad: 61.6553  LR: 0.000019  \n","Epoch: [1][2900/3575] Elapsed 12m 13s (remain 2m 50s) Loss: 0.0038(0.0301) Grad: 2249.3389  LR: 0.000019  \n","Epoch: [1][3000/3575] Elapsed 12m 34s (remain 2m 24s) Loss: 0.1418(0.0295) Grad: 52815.4414  LR: 0.000018  \n","Epoch: [1][3100/3575] Elapsed 12m 56s (remain 1m 58s) Loss: 0.0037(0.0288) Grad: 1944.2659  LR: 0.000018  \n","Epoch: [1][3200/3575] Elapsed 13m 17s (remain 1m 33s) Loss: 0.0013(0.0282) Grad: 924.7885  LR: 0.000018  \n","Epoch: [1][3300/3575] Elapsed 13m 38s (remain 1m 7s) Loss: 0.0183(0.0276) Grad: 11026.9043  LR: 0.000018  \n","Epoch: [1][3400/3575] Elapsed 13m 59s (remain 0m 42s) Loss: 0.0182(0.0270) Grad: 8076.8315  LR: 0.000018  \n","Epoch: [1][3500/3575] Elapsed 14m 21s (remain 0m 18s) Loss: 0.0068(0.0265) Grad: 3054.8909  LR: 0.000018  \n","Epoch: [1][3574/3575] Elapsed 14m 37s (remain 0m 0s) Loss: 0.0004(0.0261) Grad: 1236.7578  LR: 0.000018  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 13m 59s) Loss: 0.0002(0.0002) \n","EVAL: [100/1192] Elapsed 0m 12s (remain 2m 10s) Loss: 0.0324(0.0067) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 55s) Loss: 0.0068(0.0075) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 42s) Loss: 0.0230(0.0074) \n","EVAL: [400/1192] Elapsed 0m 45s (remain 1m 30s) Loss: 0.0006(0.0077) \n","EVAL: [500/1192] Elapsed 0m 57s (remain 1m 19s) Loss: 0.0136(0.0074) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 7s) Loss: 0.0140(0.0077) \n","EVAL: [700/1192] Elapsed 1m 20s (remain 0m 56s) Loss: 0.0093(0.0084) \n","EVAL: [800/1192] Elapsed 1m 31s (remain 0m 44s) Loss: 0.0024(0.0082) \n","EVAL: [900/1192] Elapsed 1m 42s (remain 0m 33s) Loss: 0.0011(0.0084) \n","EVAL: [1000/1192] Elapsed 1m 54s (remain 0m 21s) Loss: 0.0005(0.0083) \n","EVAL: [1100/1192] Elapsed 2m 5s (remain 0m 10s) Loss: 0.0022(0.0080) \n","EVAL: [1191/1192] Elapsed 2m 15s (remain 0m 0s) Loss: 0.0001(0.0076) \n","Epoch 1 - avg_train_loss: 0.0261  avg_val_loss: 0.0076  time: 1020s\n","Epoch 1 - Score: 0.8475\n","Epoch 1 - Save Best Score: 0.8475 Model\n","Epoch: [2][0/3575] Elapsed 0m 0s (remain 43m 54s) Loss: 0.0038(0.0038) Grad: 12536.6846  LR: 0.000018  \n","Epoch: [2][100/3575] Elapsed 0m 25s (remain 14m 21s) Loss: 0.0118(0.0080) Grad: 16967.7109  LR: 0.000018  \n","Epoch: [2][200/3575] Elapsed 0m 49s (remain 13m 55s) Loss: 0.0000(0.0063) Grad: 47.9610  LR: 0.000018  \n","Epoch: [2][300/3575] Elapsed 1m 11s (remain 12m 54s) Loss: 0.0085(0.0059) Grad: 8395.3262  LR: 0.000017  \n","Epoch: [2][400/3575] Elapsed 1m 32s (remain 12m 13s) Loss: 0.0041(0.0056) Grad: 12184.4775  LR: 0.000017  \n","Epoch: [2][500/3575] Elapsed 1m 54s (remain 11m 40s) Loss: 0.0004(0.0054) Grad: 1310.2988  LR: 0.000017  \n","Epoch: [2][600/3575] Elapsed 2m 15s (remain 11m 10s) Loss: 0.0000(0.0059) Grad: 151.7385  LR: 0.000017  \n","Epoch: [2][700/3575] Elapsed 2m 36s (remain 10m 43s) Loss: 0.0343(0.0064) Grad: 98068.6953  LR: 0.000017  \n","Epoch: [2][800/3575] Elapsed 2m 58s (remain 10m 17s) Loss: 0.0035(0.0065) Grad: 31605.5293  LR: 0.000017  \n","Epoch: [2][900/3575] Elapsed 3m 19s (remain 9m 52s) Loss: 0.0002(0.0065) Grad: 1679.8901  LR: 0.000017  \n","Epoch: [2][1000/3575] Elapsed 3m 41s (remain 9m 28s) Loss: 0.0003(0.0066) Grad: 902.6925  LR: 0.000017  \n","Epoch: [2][1100/3575] Elapsed 4m 2s (remain 9m 5s) Loss: 0.0002(0.0064) Grad: 986.1707  LR: 0.000016  \n","Epoch: [2][1200/3575] Elapsed 4m 24s (remain 8m 41s) Loss: 0.0376(0.0064) Grad: 148648.7031  LR: 0.000016  \n","Epoch: [2][1300/3575] Elapsed 4m 45s (remain 8m 19s) Loss: 0.0001(0.0062) Grad: 1149.0546  LR: 0.000016  \n","Epoch: [2][1400/3575] Elapsed 5m 7s (remain 7m 56s) Loss: 0.0216(0.0063) Grad: 18417.1074  LR: 0.000016  \n","Epoch: [2][1500/3575] Elapsed 5m 28s (remain 7m 34s) Loss: 0.0041(0.0064) Grad: 8143.2095  LR: 0.000016  \n","Epoch: [2][1600/3575] Elapsed 5m 50s (remain 7m 12s) Loss: 0.0012(0.0064) Grad: 9854.4062  LR: 0.000016  \n","Epoch: [2][1700/3575] Elapsed 6m 12s (remain 6m 49s) Loss: 0.0055(0.0064) Grad: 9037.0752  LR: 0.000016  \n","Epoch: [2][1800/3575] Elapsed 6m 33s (remain 6m 27s) Loss: 0.0076(0.0064) Grad: 11426.3398  LR: 0.000016  \n","Epoch: [2][1900/3575] Elapsed 6m 54s (remain 6m 5s) Loss: 0.0050(0.0063) Grad: 6922.5840  LR: 0.000015  \n","Epoch: [2][2000/3575] Elapsed 7m 16s (remain 5m 43s) Loss: 0.0003(0.0064) Grad: 2289.8555  LR: 0.000015  \n","Epoch: [2][2100/3575] Elapsed 7m 37s (remain 5m 21s) Loss: 0.0000(0.0064) Grad: 52.8747  LR: 0.000015  \n","Epoch: [2][2200/3575] Elapsed 7m 59s (remain 4m 59s) Loss: 0.0061(0.0064) Grad: 19612.2168  LR: 0.000015  \n","Epoch: [2][2300/3575] Elapsed 8m 20s (remain 4m 37s) Loss: 0.0032(0.0064) Grad: 9411.5742  LR: 0.000015  \n","Epoch: [2][2400/3575] Elapsed 8m 42s (remain 4m 15s) Loss: 0.0004(0.0063) Grad: 1298.6025  LR: 0.000015  \n","Epoch: [2][2500/3575] Elapsed 9m 3s (remain 3m 53s) Loss: 0.0105(0.0063) Grad: 34562.9883  LR: 0.000015  \n","Epoch: [2][2600/3575] Elapsed 9m 25s (remain 3m 31s) Loss: 0.0097(0.0063) Grad: 43693.4023  LR: 0.000015  \n","Epoch: [2][2700/3575] Elapsed 9m 46s (remain 3m 9s) Loss: 0.0005(0.0063) Grad: 2249.2734  LR: 0.000014  \n","Epoch: [2][2800/3575] Elapsed 10m 8s (remain 2m 48s) Loss: 0.0063(0.0063) Grad: 24265.9297  LR: 0.000014  \n","Epoch: [2][2900/3575] Elapsed 10m 30s (remain 2m 26s) Loss: 0.0001(0.0063) Grad: 186.6798  LR: 0.000014  \n","Epoch: [2][3000/3575] Elapsed 10m 51s (remain 2m 4s) Loss: 0.0000(0.0063) Grad: 66.6022  LR: 0.000014  \n","Epoch: [2][3100/3575] Elapsed 11m 13s (remain 1m 42s) Loss: 0.0003(0.0063) Grad: 2642.0024  LR: 0.000014  \n","Epoch: [2][3200/3575] Elapsed 11m 35s (remain 1m 21s) Loss: 0.0000(0.0063) Grad: 220.5937  LR: 0.000014  \n","Epoch: [2][3300/3575] Elapsed 11m 56s (remain 0m 59s) Loss: 0.0000(0.0063) Grad: 98.4892  LR: 0.000014  \n","Epoch: [2][3400/3575] Elapsed 12m 18s (remain 0m 37s) Loss: 0.0000(0.0062) Grad: 90.8253  LR: 0.000014  \n","Epoch: [2][3500/3575] Elapsed 12m 40s (remain 0m 16s) Loss: 0.0002(0.0062) Grad: 608.6979  LR: 0.000013  \n","Epoch: [2][3574/3575] Elapsed 12m 56s (remain 0m 0s) Loss: 0.0180(0.0062) Grad: 32250.2832  LR: 0.000013  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 11m 9s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 12s (remain 2m 10s) Loss: 0.0125(0.0068) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 56s) Loss: 0.0188(0.0072) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 43s) Loss: 0.0061(0.0066) \n","EVAL: [400/1192] Elapsed 0m 46s (remain 1m 31s) Loss: 0.0003(0.0074) \n","EVAL: [500/1192] Elapsed 0m 57s (remain 1m 19s) Loss: 0.0003(0.0067) \n","EVAL: [600/1192] Elapsed 1m 9s (remain 1m 7s) Loss: 0.0081(0.0070) \n","EVAL: [700/1192] Elapsed 1m 20s (remain 0m 56s) Loss: 0.0100(0.0076) \n","EVAL: [800/1192] Elapsed 1m 31s (remain 0m 44s) Loss: 0.0000(0.0074) \n","EVAL: [900/1192] Elapsed 1m 43s (remain 0m 33s) Loss: 0.0005(0.0075) \n","EVAL: [1000/1192] Elapsed 1m 54s (remain 0m 21s) Loss: 0.0001(0.0073) \n","EVAL: [1100/1192] Elapsed 2m 5s (remain 0m 10s) Loss: 0.0066(0.0070) \n","EVAL: [1191/1192] Elapsed 2m 16s (remain 0m 0s) Loss: 0.0001(0.0067) \n","Epoch 2 - avg_train_loss: 0.0062  avg_val_loss: 0.0067  time: 920s\n","Epoch 2 - Score: 0.8815\n","Epoch 2 - Save Best Score: 0.8815 Model\n","Epoch: [3][0/3575] Elapsed 0m 0s (remain 39m 34s) Loss: 0.0172(0.0172) Grad: 38330.7930  LR: 0.000013  \n","Epoch: [3][100/3575] Elapsed 0m 24s (remain 13m 49s) Loss: 0.0000(0.0031) Grad: 205.6738  LR: 0.000013  \n","Epoch: [3][200/3575] Elapsed 0m 48s (remain 13m 37s) Loss: 0.0061(0.0044) Grad: 10273.6406  LR: 0.000013  \n","Epoch: [3][300/3575] Elapsed 1m 10s (remain 12m 42s) Loss: 0.0021(0.0053) Grad: 4662.5459  LR: 0.000013  \n","Epoch: [3][400/3575] Elapsed 1m 31s (remain 12m 4s) Loss: 0.0070(0.0050) Grad: 16822.4941  LR: 0.000013  \n","Epoch: [3][500/3575] Elapsed 1m 53s (remain 11m 33s) Loss: 0.0002(0.0049) Grad: 817.3055  LR: 0.000013  \n","Epoch: [3][600/3575] Elapsed 2m 14s (remain 11m 4s) Loss: 0.0250(0.0048) Grad: 12998.0586  LR: 0.000013  \n","Epoch: [3][700/3575] Elapsed 2m 35s (remain 10m 38s) Loss: 0.0058(0.0049) Grad: 8382.8516  LR: 0.000012  \n","Epoch: [3][800/3575] Elapsed 2m 57s (remain 10m 13s) Loss: 0.0011(0.0049) Grad: 10368.7529  LR: 0.000012  \n","Epoch: [3][900/3575] Elapsed 3m 18s (remain 9m 49s) Loss: 0.0003(0.0049) Grad: 1849.9249  LR: 0.000012  \n","Epoch: [3][1000/3575] Elapsed 3m 40s (remain 9m 26s) Loss: 0.0006(0.0049) Grad: 1913.4445  LR: 0.000012  \n","Epoch: [3][1100/3575] Elapsed 4m 2s (remain 9m 4s) Loss: 0.0000(0.0050) Grad: 41.5122  LR: 0.000012  \n","Epoch: [3][1200/3575] Elapsed 4m 23s (remain 8m 41s) Loss: 0.0023(0.0049) Grad: 5811.2173  LR: 0.000012  \n","Epoch: [3][1300/3575] Elapsed 4m 45s (remain 8m 18s) Loss: 0.0000(0.0050) Grad: 27.8990  LR: 0.000012  \n","Epoch: [3][1400/3575] Elapsed 5m 6s (remain 7m 55s) Loss: 0.0002(0.0049) Grad: 490.7885  LR: 0.000012  \n","Epoch: [3][1500/3575] Elapsed 5m 27s (remain 7m 32s) Loss: 0.0007(0.0048) Grad: 7165.1592  LR: 0.000011  \n","Epoch: [3][1600/3575] Elapsed 5m 48s (remain 7m 10s) Loss: 0.0000(0.0049) Grad: 26.8010  LR: 0.000011  \n","Epoch: [3][1700/3575] Elapsed 6m 10s (remain 6m 47s) Loss: 0.0000(0.0049) Grad: 84.2137  LR: 0.000011  \n","Epoch: [3][1800/3575] Elapsed 6m 31s (remain 6m 25s) Loss: 0.0000(0.0049) Grad: 161.3157  LR: 0.000011  \n","Epoch: [3][1900/3575] Elapsed 6m 52s (remain 6m 3s) Loss: 0.0001(0.0049) Grad: 516.8618  LR: 0.000011  \n","Epoch: [3][2000/3575] Elapsed 7m 14s (remain 5m 41s) Loss: 0.0000(0.0049) Grad: 51.3099  LR: 0.000011  \n","Epoch: [3][2100/3575] Elapsed 7m 35s (remain 5m 19s) Loss: 0.0010(0.0049) Grad: 6106.8433  LR: 0.000011  \n","Epoch: [3][2200/3575] Elapsed 7m 56s (remain 4m 57s) Loss: 0.0000(0.0049) Grad: 154.1876  LR: 0.000011  \n","Epoch: [3][2300/3575] Elapsed 8m 18s (remain 4m 35s) Loss: 0.0001(0.0049) Grad: 325.4724  LR: 0.000010  \n","Epoch: [3][2400/3575] Elapsed 8m 39s (remain 4m 14s) Loss: 0.0087(0.0049) Grad: 18297.8730  LR: 0.000010  \n","Epoch: [3][2500/3575] Elapsed 9m 1s (remain 3m 52s) Loss: 0.0033(0.0049) Grad: 7374.2422  LR: 0.000010  \n","Epoch: [3][2600/3575] Elapsed 9m 22s (remain 3m 30s) Loss: 0.0005(0.0049) Grad: 1726.7578  LR: 0.000010  \n","Epoch: [3][2700/3575] Elapsed 9m 44s (remain 3m 9s) Loss: 0.0030(0.0050) Grad: 10003.7959  LR: 0.000010  \n","Epoch: [3][2800/3575] Elapsed 10m 6s (remain 2m 47s) Loss: 0.0000(0.0049) Grad: 159.6138  LR: 0.000010  \n","Epoch: [3][2900/3575] Elapsed 10m 27s (remain 2m 25s) Loss: 0.0000(0.0049) Grad: 72.9833  LR: 0.000010  \n","Epoch: [3][3000/3575] Elapsed 10m 49s (remain 2m 4s) Loss: 0.0021(0.0049) Grad: 14408.6592  LR: 0.000010  \n","Epoch: [3][3100/3575] Elapsed 11m 10s (remain 1m 42s) Loss: 0.0000(0.0049) Grad: 119.8889  LR: 0.000009  \n","Epoch: [3][3200/3575] Elapsed 11m 32s (remain 1m 20s) Loss: 0.0095(0.0049) Grad: 64852.5078  LR: 0.000009  \n","Epoch: [3][3300/3575] Elapsed 11m 53s (remain 0m 59s) Loss: 0.0000(0.0049) Grad: 174.7566  LR: 0.000009  \n","Epoch: [3][3400/3575] Elapsed 12m 15s (remain 0m 37s) Loss: 0.0000(0.0049) Grad: 35.7727  LR: 0.000009  \n","Epoch: [3][3500/3575] Elapsed 12m 37s (remain 0m 16s) Loss: 0.0095(0.0049) Grad: 49885.6328  LR: 0.000009  \n","Epoch: [3][3574/3575] Elapsed 12m 52s (remain 0m 0s) Loss: 0.0012(0.0049) Grad: 5396.4746  LR: 0.000009  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 10m 2s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 8s) Loss: 0.0214(0.0072) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 55s) Loss: 0.0048(0.0076) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 42s) Loss: 0.0123(0.0068) \n","EVAL: [400/1192] Elapsed 0m 46s (remain 1m 30s) Loss: 0.0006(0.0074) \n","EVAL: [500/1192] Elapsed 0m 57s (remain 1m 19s) Loss: 0.0018(0.0070) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 7s) Loss: 0.0099(0.0070) \n","EVAL: [700/1192] Elapsed 1m 20s (remain 0m 56s) Loss: 0.0073(0.0079) \n","EVAL: [800/1192] Elapsed 1m 31s (remain 0m 44s) Loss: 0.0000(0.0077) \n","EVAL: [900/1192] Elapsed 1m 42s (remain 0m 33s) Loss: 0.0029(0.0079) \n","EVAL: [1000/1192] Elapsed 1m 54s (remain 0m 21s) Loss: 0.0001(0.0078) \n","EVAL: [1100/1192] Elapsed 2m 5s (remain 0m 10s) Loss: 0.0116(0.0075) \n","EVAL: [1191/1192] Elapsed 2m 15s (remain 0m 0s) Loss: 0.0000(0.0071) \n","Epoch 3 - avg_train_loss: 0.0049  avg_val_loss: 0.0071  time: 915s\n","Epoch 3 - Score: 0.8809\n","Epoch: [4][0/3575] Elapsed 0m 0s (remain 36m 36s) Loss: 0.0000(0.0000) Grad: 998.6163  LR: 0.000009  \n","Epoch: [4][100/3575] Elapsed 0m 22s (remain 12m 37s) Loss: 0.0477(0.0034) Grad: 108519.7266  LR: 0.000009  \n","Epoch: [4][200/3575] Elapsed 0m 43s (remain 12m 11s) Loss: 0.0003(0.0034) Grad: 2403.1609  LR: 0.000009  \n","Epoch: [4][300/3575] Elapsed 1m 5s (remain 11m 47s) Loss: 0.0026(0.0037) Grad: 15231.8145  LR: 0.000009  \n","Epoch: [4][400/3575] Elapsed 1m 26s (remain 11m 25s) Loss: 0.0005(0.0036) Grad: 2719.7400  LR: 0.000008  \n","Epoch: [4][500/3575] Elapsed 1m 48s (remain 11m 3s) Loss: 0.0032(0.0042) Grad: 10836.8643  LR: 0.000008  \n","Epoch: [4][600/3575] Elapsed 2m 9s (remain 10m 42s) Loss: 0.0007(0.0041) Grad: 9830.9834  LR: 0.000008  \n","Epoch: [4][700/3575] Elapsed 2m 31s (remain 10m 21s) Loss: 0.0009(0.0041) Grad: 4042.2327  LR: 0.000008  \n","Epoch: [4][800/3575] Elapsed 2m 53s (remain 9m 59s) Loss: 0.0166(0.0040) Grad: 7324.7925  LR: 0.000008  \n","Epoch: [4][900/3575] Elapsed 3m 14s (remain 9m 37s) Loss: 0.0000(0.0040) Grad: 51.2187  LR: 0.000008  \n","Epoch: [4][1000/3575] Elapsed 3m 36s (remain 9m 15s) Loss: 0.0000(0.0040) Grad: 112.0305  LR: 0.000008  \n","Epoch: [4][1100/3575] Elapsed 3m 57s (remain 8m 54s) Loss: 0.0000(0.0039) Grad: 27.9382  LR: 0.000008  \n","Epoch: [4][1200/3575] Elapsed 4m 19s (remain 8m 32s) Loss: 0.0187(0.0040) Grad: 67581.0312  LR: 0.000007  \n","Epoch: [4][1300/3575] Elapsed 4m 40s (remain 8m 10s) Loss: 0.0019(0.0040) Grad: 6109.6519  LR: 0.000007  \n","Epoch: [4][1400/3575] Elapsed 5m 2s (remain 7m 49s) Loss: 0.0001(0.0040) Grad: 1028.8168  LR: 0.000007  \n","Epoch: [4][1500/3575] Elapsed 5m 23s (remain 7m 27s) Loss: 0.0043(0.0040) Grad: 7240.9561  LR: 0.000007  \n","Epoch: [4][1600/3575] Elapsed 5m 45s (remain 7m 5s) Loss: 0.0000(0.0040) Grad: 5.3718  LR: 0.000007  \n","Epoch: [4][1700/3575] Elapsed 6m 6s (remain 6m 44s) Loss: 0.0003(0.0040) Grad: 3364.8206  LR: 0.000007  \n","Epoch: [4][1800/3575] Elapsed 6m 28s (remain 6m 22s) Loss: 0.0015(0.0040) Grad: 13038.6396  LR: 0.000007  \n","Epoch: [4][1900/3575] Elapsed 6m 49s (remain 6m 0s) Loss: 0.0000(0.0040) Grad: 3.7496  LR: 0.000007  \n","Epoch: [4][2000/3575] Elapsed 7m 11s (remain 5m 39s) Loss: 0.0002(0.0040) Grad: 3248.5920  LR: 0.000006  \n","Epoch: [4][2100/3575] Elapsed 7m 32s (remain 5m 17s) Loss: 0.0001(0.0040) Grad: 711.1096  LR: 0.000006  \n","Epoch: [4][2200/3575] Elapsed 7m 53s (remain 4m 55s) Loss: 0.0055(0.0040) Grad: 10828.5000  LR: 0.000006  \n","Epoch: [4][2300/3575] Elapsed 8m 15s (remain 4m 34s) Loss: 0.0000(0.0040) Grad: 37.4701  LR: 0.000006  \n","Epoch: [4][2400/3575] Elapsed 8m 37s (remain 4m 12s) Loss: 0.0000(0.0040) Grad: 72.2613  LR: 0.000006  \n","Epoch: [4][2500/3575] Elapsed 8m 58s (remain 3m 51s) Loss: 0.0000(0.0041) Grad: 35.4514  LR: 0.000006  \n","Epoch: [4][2600/3575] Elapsed 9m 20s (remain 3m 29s) Loss: 0.0019(0.0040) Grad: 8152.7476  LR: 0.000006  \n","Epoch: [4][2700/3575] Elapsed 9m 41s (remain 3m 8s) Loss: 0.0001(0.0041) Grad: 618.3395  LR: 0.000006  \n","Epoch: [4][2800/3575] Elapsed 10m 3s (remain 2m 46s) Loss: 0.0000(0.0041) Grad: 24.8286  LR: 0.000005  \n","Epoch: [4][2900/3575] Elapsed 10m 24s (remain 2m 25s) Loss: 0.0082(0.0041) Grad: 35153.8789  LR: 0.000005  \n","Epoch: [4][3000/3575] Elapsed 10m 46s (remain 2m 3s) Loss: 0.0047(0.0041) Grad: 8977.2197  LR: 0.000005  \n","Epoch: [4][3100/3575] Elapsed 11m 7s (remain 1m 42s) Loss: 0.0110(0.0040) Grad: 30737.3652  LR: 0.000005  \n","Epoch: [4][3200/3575] Elapsed 11m 28s (remain 1m 20s) Loss: 0.0000(0.0040) Grad: 211.3365  LR: 0.000005  \n","Epoch: [4][3300/3575] Elapsed 11m 50s (remain 0m 58s) Loss: 0.0000(0.0040) Grad: 32.8513  LR: 0.000005  \n","Epoch: [4][3400/3575] Elapsed 12m 12s (remain 0m 37s) Loss: 0.0020(0.0040) Grad: 11833.3691  LR: 0.000005  \n","Epoch: [4][3500/3575] Elapsed 12m 33s (remain 0m 15s) Loss: 0.0001(0.0040) Grad: 188.3932  LR: 0.000005  \n","Epoch: [4][3574/3575] Elapsed 12m 49s (remain 0m 0s) Loss: 0.0003(0.0039) Grad: 2606.6824  LR: 0.000004  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 9m 49s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 8s) Loss: 0.0196(0.0084) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 55s) Loss: 0.0046(0.0085) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 42s) Loss: 0.0201(0.0077) \n","EVAL: [400/1192] Elapsed 0m 46s (remain 1m 30s) Loss: 0.0002(0.0082) \n","EVAL: [500/1192] Elapsed 0m 57s (remain 1m 19s) Loss: 0.0005(0.0076) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 7s) Loss: 0.0118(0.0078) \n","EVAL: [700/1192] Elapsed 1m 20s (remain 0m 56s) Loss: 0.0062(0.0086) \n","EVAL: [800/1192] Elapsed 1m 31s (remain 0m 44s) Loss: 0.0000(0.0083) \n","EVAL: [900/1192] Elapsed 1m 42s (remain 0m 33s) Loss: 0.0055(0.0085) \n","EVAL: [1000/1192] Elapsed 1m 54s (remain 0m 21s) Loss: 0.0000(0.0083) \n","EVAL: [1100/1192] Elapsed 2m 5s (remain 0m 10s) Loss: 0.0096(0.0080) \n","EVAL: [1191/1192] Elapsed 2m 15s (remain 0m 0s) Loss: 0.0000(0.0076) \n","Epoch 4 - avg_train_loss: 0.0039  avg_val_loss: 0.0076  time: 911s\n","Epoch 4 - Score: 0.8863\n","Epoch 4 - Save Best Score: 0.8863 Model\n","Epoch: [5][0/3575] Elapsed 0m 0s (remain 37m 6s) Loss: 0.0000(0.0000) Grad: 1028.8123  LR: 0.000004  \n","Epoch: [5][100/3575] Elapsed 0m 24s (remain 14m 15s) Loss: 0.0000(0.0044) Grad: 110.4277  LR: 0.000004  \n","Epoch: [5][200/3575] Elapsed 0m 48s (remain 13m 39s) Loss: 0.0041(0.0035) Grad: 3459.1123  LR: 0.000004  \n","Epoch: [5][300/3575] Elapsed 1m 10s (remain 12m 45s) Loss: 0.0005(0.0032) Grad: 7490.9561  LR: 0.000004  \n","Epoch: [5][400/3575] Elapsed 1m 31s (remain 12m 6s) Loss: 0.0010(0.0029) Grad: 8222.1406  LR: 0.000004  \n","Epoch: [5][500/3575] Elapsed 1m 53s (remain 11m 34s) Loss: 0.0000(0.0029) Grad: 20.1798  LR: 0.000004  \n","Epoch: [5][600/3575] Elapsed 2m 14s (remain 11m 5s) Loss: 0.0000(0.0027) Grad: 5.6440  LR: 0.000004  \n","Epoch: [5][700/3575] Elapsed 2m 36s (remain 10m 40s) Loss: 0.0116(0.0027) Grad: 32287.5391  LR: 0.000004  \n","Epoch: [5][800/3575] Elapsed 2m 57s (remain 10m 15s) Loss: 0.0104(0.0027) Grad: 29206.8418  LR: 0.000003  \n","Epoch: [5][900/3575] Elapsed 3m 19s (remain 9m 52s) Loss: 0.0043(0.0028) Grad: 10215.3408  LR: 0.000003  \n","Epoch: [5][1000/3575] Elapsed 3m 41s (remain 9m 29s) Loss: 0.0009(0.0028) Grad: 11034.4932  LR: 0.000003  \n","Epoch: [5][1100/3575] Elapsed 4m 3s (remain 9m 6s) Loss: 0.0000(0.0029) Grad: 2.7386  LR: 0.000003  \n","Epoch: [5][1200/3575] Elapsed 4m 24s (remain 8m 43s) Loss: 0.0000(0.0028) Grad: 218.9536  LR: 0.000003  \n","Epoch: [5][1300/3575] Elapsed 4m 46s (remain 8m 20s) Loss: 0.0000(0.0029) Grad: 35.0949  LR: 0.000003  \n","Epoch: [5][1400/3575] Elapsed 5m 7s (remain 7m 57s) Loss: 0.0001(0.0029) Grad: 2573.3923  LR: 0.000003  \n","Epoch: [5][1500/3575] Elapsed 5m 29s (remain 7m 34s) Loss: 0.0000(0.0029) Grad: 5.6752  LR: 0.000003  \n","Epoch: [5][1600/3575] Elapsed 5m 50s (remain 7m 12s) Loss: 0.0005(0.0029) Grad: 4886.8066  LR: 0.000002  \n","Epoch: [5][1700/3575] Elapsed 6m 12s (remain 6m 50s) Loss: 0.0000(0.0030) Grad: 98.4868  LR: 0.000002  \n","Epoch: [5][1800/3575] Elapsed 6m 33s (remain 6m 27s) Loss: 0.0076(0.0030) Grad: 27258.9355  LR: 0.000002  \n","Epoch: [5][1900/3575] Elapsed 6m 55s (remain 6m 5s) Loss: 0.0002(0.0030) Grad: 2457.5227  LR: 0.000002  \n","Epoch: [5][2000/3575] Elapsed 7m 16s (remain 5m 43s) Loss: 0.0002(0.0031) Grad: 964.1871  LR: 0.000002  \n","Epoch: [5][2100/3575] Elapsed 7m 38s (remain 5m 21s) Loss: 0.0000(0.0031) Grad: 20.2380  LR: 0.000002  \n","Epoch: [5][2200/3575] Elapsed 7m 59s (remain 4m 59s) Loss: 0.0000(0.0031) Grad: 9.1895  LR: 0.000002  \n","Epoch: [5][2300/3575] Elapsed 8m 21s (remain 4m 37s) Loss: 0.0192(0.0031) Grad: 26652.3438  LR: 0.000002  \n","Epoch: [5][2400/3575] Elapsed 8m 42s (remain 4m 15s) Loss: 0.0001(0.0032) Grad: 1813.6831  LR: 0.000001  \n","Epoch: [5][2500/3575] Elapsed 9m 4s (remain 3m 53s) Loss: 0.0001(0.0031) Grad: 2213.2378  LR: 0.000001  \n","Epoch: [5][2600/3575] Elapsed 9m 26s (remain 3m 32s) Loss: 0.0001(0.0031) Grad: 1960.9451  LR: 0.000001  \n","Epoch: [5][2700/3575] Elapsed 9m 48s (remain 3m 10s) Loss: 0.0001(0.0031) Grad: 1157.0708  LR: 0.000001  \n","Epoch: [5][2800/3575] Elapsed 10m 9s (remain 2m 48s) Loss: 0.0001(0.0031) Grad: 287.7560  LR: 0.000001  \n","Epoch: [5][2900/3575] Elapsed 10m 30s (remain 2m 26s) Loss: 0.0000(0.0031) Grad: 99.4432  LR: 0.000001  \n","Epoch: [5][3000/3575] Elapsed 10m 52s (remain 2m 4s) Loss: 0.0000(0.0031) Grad: 279.2576  LR: 0.000001  \n","Epoch: [5][3100/3575] Elapsed 11m 14s (remain 1m 43s) Loss: 0.0000(0.0030) Grad: 131.4205  LR: 0.000001  \n","Epoch: [5][3200/3575] Elapsed 11m 35s (remain 1m 21s) Loss: 0.0052(0.0030) Grad: 81386.1641  LR: 0.000000  \n","Epoch: [5][3300/3575] Elapsed 11m 57s (remain 0m 59s) Loss: 0.0000(0.0030) Grad: 4.1491  LR: 0.000000  \n","Epoch: [5][3400/3575] Elapsed 12m 19s (remain 0m 37s) Loss: 0.0000(0.0030) Grad: 121.4565  LR: 0.000000  \n","Epoch: [5][3500/3575] Elapsed 12m 41s (remain 0m 16s) Loss: 0.0037(0.0030) Grad: 11513.8115  LR: 0.000000  \n","Epoch: [5][3574/3575] Elapsed 12m 57s (remain 0m 0s) Loss: 0.0000(0.0030) Grad: 81.5025  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 9m 29s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 8s) Loss: 0.0248(0.0094) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 55s) Loss: 0.0006(0.0093) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 43s) Loss: 0.0210(0.0083) \n","EVAL: [400/1192] Elapsed 0m 46s (remain 1m 31s) Loss: 0.0001(0.0087) \n","EVAL: [500/1192] Elapsed 0m 57s (remain 1m 19s) Loss: 0.0022(0.0082) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 7s) Loss: 0.0137(0.0083) \n","EVAL: [700/1192] Elapsed 1m 20s (remain 0m 56s) Loss: 0.0079(0.0091) \n","EVAL: [800/1192] Elapsed 1m 31s (remain 0m 44s) Loss: 0.0000(0.0089) \n","EVAL: [900/1192] Elapsed 1m 43s (remain 0m 33s) Loss: 0.0058(0.0091) \n","EVAL: [1000/1192] Elapsed 1m 54s (remain 0m 21s) Loss: 0.0000(0.0090) \n","EVAL: [1100/1192] Elapsed 2m 5s (remain 0m 10s) Loss: 0.0127(0.0087) \n","EVAL: [1191/1192] Elapsed 2m 16s (remain 0m 0s) Loss: 0.0000(0.0083) \n","Epoch 5 - avg_train_loss: 0.0030  avg_val_loss: 0.0083  time: 919s\n","Epoch 5 - Score: 0.8880\n","Epoch 5 - Save Best Score: 0.8880 Model\n","========== fold: 3 training ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight']\n","- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\n","Epoch: [1][0/3575] Elapsed 0m 1s (remain 68m 52s) Loss: 0.2500(0.2500) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/3575] Elapsed 0m 30s (remain 17m 15s) Loss: 0.1819(0.2309) Grad: 90709.7891  LR: 0.000001  \n","Epoch: [1][200/3575] Elapsed 0m 58s (remain 16m 29s) Loss: 0.0687(0.1777) Grad: 30141.6758  LR: 0.000002  \n","Epoch: [1][300/3575] Elapsed 1m 27s (remain 15m 56s) Loss: 0.0325(0.1345) Grad: 3908.3909  LR: 0.000003  \n","Epoch: [1][400/3575] Elapsed 1m 56s (remain 15m 23s) Loss: 0.0583(0.1104) Grad: 8762.7646  LR: 0.000004  \n","Epoch: [1][500/3575] Elapsed 2m 25s (remain 14m 52s) Loss: 0.0352(0.0953) Grad: 6591.1558  LR: 0.000006  \n","Epoch: [1][600/3575] Elapsed 2m 54s (remain 14m 22s) Loss: 0.0529(0.0849) Grad: 8406.0312  LR: 0.000007  \n","Epoch: [1][700/3575] Elapsed 3m 23s (remain 13m 52s) Loss: 0.0590(0.0775) Grad: 9451.7363  LR: 0.000008  \n","Epoch: [1][800/3575] Elapsed 3m 51s (remain 13m 23s) Loss: 0.0166(0.0718) Grad: 9905.2910  LR: 0.000009  \n","Epoch: [1][900/3575] Elapsed 4m 20s (remain 12m 53s) Loss: 0.0131(0.0663) Grad: 7299.6924  LR: 0.000010  \n","Epoch: [1][1000/3575] Elapsed 4m 49s (remain 12m 25s) Loss: 0.0099(0.0612) Grad: 19414.9980  LR: 0.000011  \n","Epoch: [1][1100/3575] Elapsed 5m 18s (remain 11m 56s) Loss: 0.0054(0.0572) Grad: 11232.3066  LR: 0.000012  \n","Epoch: [1][1200/3575] Elapsed 5m 48s (remain 11m 28s) Loss: 0.0133(0.0536) Grad: 32803.5273  LR: 0.000013  \n","Epoch: [1][1300/3575] Elapsed 6m 10s (remain 10m 47s) Loss: 0.0317(0.0504) Grad: 29278.2148  LR: 0.000015  \n","Epoch: [1][1400/3575] Elapsed 6m 32s (remain 10m 8s) Loss: 0.0646(0.0478) Grad: 30609.6758  LR: 0.000016  \n","Epoch: [1][1500/3575] Elapsed 6m 53s (remain 9m 31s) Loss: 0.1106(0.0455) Grad: 24554.0977  LR: 0.000017  \n","Epoch: [1][1600/3575] Elapsed 7m 15s (remain 8m 56s) Loss: 0.0002(0.0435) Grad: 370.1252  LR: 0.000018  \n","Epoch: [1][1700/3575] Elapsed 7m 37s (remain 8m 23s) Loss: 0.0016(0.0416) Grad: 9213.7080  LR: 0.000019  \n","Epoch: [1][1800/3575] Elapsed 7m 58s (remain 7m 51s) Loss: 0.0166(0.0399) Grad: 19212.0176  LR: 0.000020  \n","Epoch: [1][1900/3575] Elapsed 8m 20s (remain 7m 20s) Loss: 0.0007(0.0383) Grad: 3047.8230  LR: 0.000020  \n","Epoch: [1][2000/3575] Elapsed 8m 41s (remain 6m 50s) Loss: 0.0003(0.0369) Grad: 673.8807  LR: 0.000020  \n","Epoch: [1][2100/3575] Elapsed 9m 2s (remain 6m 20s) Loss: 0.0141(0.0356) Grad: 11776.0244  LR: 0.000020  \n","Epoch: [1][2200/3575] Elapsed 9m 24s (remain 5m 52s) Loss: 0.0196(0.0345) Grad: 10928.8359  LR: 0.000019  \n","Epoch: [1][2300/3575] Elapsed 9m 46s (remain 5m 24s) Loss: 0.0035(0.0334) Grad: 8235.7715  LR: 0.000019  \n","Epoch: [1][2400/3575] Elapsed 10m 7s (remain 4m 57s) Loss: 0.0183(0.0325) Grad: 9830.7803  LR: 0.000019  \n","Epoch: [1][2500/3575] Elapsed 10m 29s (remain 4m 30s) Loss: 0.0020(0.0316) Grad: 2345.2415  LR: 0.000019  \n","Epoch: [1][2600/3575] Elapsed 10m 50s (remain 4m 3s) Loss: 0.0001(0.0308) Grad: 138.7321  LR: 0.000019  \n","Epoch: [1][2700/3575] Elapsed 11m 12s (remain 3m 37s) Loss: 0.0005(0.0300) Grad: 961.3259  LR: 0.000019  \n","Epoch: [1][2800/3575] Elapsed 11m 33s (remain 3m 11s) Loss: 0.0014(0.0292) Grad: 5072.0630  LR: 0.000019  \n","Epoch: [1][2900/3575] Elapsed 11m 55s (remain 2m 46s) Loss: 0.0115(0.0286) Grad: 4620.6470  LR: 0.000019  \n","Epoch: [1][3000/3575] Elapsed 12m 16s (remain 2m 20s) Loss: 0.0027(0.0279) Grad: 5424.7515  LR: 0.000018  \n","Epoch: [1][3100/3575] Elapsed 12m 38s (remain 1m 55s) Loss: 0.0134(0.0273) Grad: 6366.5620  LR: 0.000018  \n","Epoch: [1][3200/3575] Elapsed 12m 59s (remain 1m 31s) Loss: 0.0026(0.0266) Grad: 6410.3306  LR: 0.000018  \n","Epoch: [1][3300/3575] Elapsed 13m 21s (remain 1m 6s) Loss: 0.0011(0.0261) Grad: 2523.1545  LR: 0.000018  \n","Epoch: [1][3400/3575] Elapsed 13m 42s (remain 0m 42s) Loss: 0.0097(0.0256) Grad: 7764.5742  LR: 0.000018  \n","Epoch: [1][3500/3575] Elapsed 14m 4s (remain 0m 17s) Loss: 0.0000(0.0251) Grad: 94.5711  LR: 0.000018  \n","Epoch: [1][3574/3575] Elapsed 14m 20s (remain 0m 0s) Loss: 0.0013(0.0247) Grad: 2739.5227  LR: 0.000018  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 14m 11s) Loss: 0.0160(0.0160) \n","EVAL: [100/1192] Elapsed 0m 12s (remain 2m 11s) Loss: 0.0300(0.0096) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 56s) Loss: 0.0235(0.0087) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 43s) Loss: 0.0038(0.0086) \n","EVAL: [400/1192] Elapsed 0m 46s (remain 1m 31s) Loss: 0.0002(0.0085) \n","EVAL: [500/1192] Elapsed 0m 57s (remain 1m 19s) Loss: 0.0266(0.0077) \n","EVAL: [600/1192] Elapsed 1m 9s (remain 1m 8s) Loss: 0.0092(0.0080) \n","EVAL: [700/1192] Elapsed 1m 20s (remain 0m 56s) Loss: 0.0045(0.0084) \n","EVAL: [800/1192] Elapsed 1m 32s (remain 0m 44s) Loss: 0.0095(0.0082) \n","EVAL: [900/1192] Elapsed 1m 43s (remain 0m 33s) Loss: 0.0063(0.0082) \n","EVAL: [1000/1192] Elapsed 1m 54s (remain 0m 21s) Loss: 0.0004(0.0079) \n","EVAL: [1100/1192] Elapsed 2m 6s (remain 0m 10s) Loss: 0.0048(0.0076) \n","EVAL: [1191/1192] Elapsed 2m 16s (remain 0m 0s) Loss: 0.0001(0.0074) \n","Epoch 1 - avg_train_loss: 0.0247  avg_val_loss: 0.0074  time: 1004s\n","Epoch 1 - Score: 0.8396\n","Epoch 1 - Save Best Score: 0.8396 Model\n","Epoch: [2][0/3575] Elapsed 0m 0s (remain 46m 20s) Loss: 0.0066(0.0066) Grad: 34834.9766  LR: 0.000018  \n","Epoch: [2][100/3575] Elapsed 0m 25s (remain 14m 52s) Loss: 0.0001(0.0069) Grad: 543.6203  LR: 0.000018  \n","Epoch: [2][200/3575] Elapsed 0m 50s (remain 14m 10s) Loss: 0.0008(0.0067) Grad: 3520.5415  LR: 0.000018  \n","Epoch: [2][300/3575] Elapsed 1m 12s (remain 13m 5s) Loss: 0.0375(0.0072) Grad: 16552.0781  LR: 0.000017  \n","Epoch: [2][400/3575] Elapsed 1m 33s (remain 12m 23s) Loss: 0.0120(0.0075) Grad: 17576.7402  LR: 0.000017  \n","Epoch: [2][500/3575] Elapsed 1m 55s (remain 11m 48s) Loss: 0.0001(0.0074) Grad: 330.6380  LR: 0.000017  \n","Epoch: [2][600/3575] Elapsed 2m 17s (remain 11m 18s) Loss: 0.0037(0.0073) Grad: 11805.8975  LR: 0.000017  \n","Epoch: [2][700/3575] Elapsed 2m 38s (remain 10m 49s) Loss: 0.0001(0.0072) Grad: 381.3626  LR: 0.000017  \n","Epoch: [2][800/3575] Elapsed 3m 0s (remain 10m 24s) Loss: 0.0108(0.0070) Grad: 18555.0898  LR: 0.000017  \n","Epoch: [2][900/3575] Elapsed 3m 21s (remain 9m 58s) Loss: 0.0200(0.0071) Grad: 38619.6484  LR: 0.000017  \n","Epoch: [2][1000/3575] Elapsed 3m 43s (remain 9m 33s) Loss: 0.0003(0.0073) Grad: 653.3406  LR: 0.000017  \n","Epoch: [2][1100/3575] Elapsed 4m 4s (remain 9m 9s) Loss: 0.0026(0.0073) Grad: 7164.5137  LR: 0.000016  \n","Epoch: [2][1200/3575] Elapsed 4m 26s (remain 8m 46s) Loss: 0.0000(0.0073) Grad: 60.4963  LR: 0.000016  \n","Epoch: [2][1300/3575] Elapsed 4m 47s (remain 8m 22s) Loss: 0.0002(0.0073) Grad: 616.7134  LR: 0.000016  \n","Epoch: [2][1400/3575] Elapsed 5m 8s (remain 7m 59s) Loss: 0.0166(0.0073) Grad: 38702.8477  LR: 0.000016  \n","Epoch: [2][1500/3575] Elapsed 5m 30s (remain 7m 37s) Loss: 0.0000(0.0071) Grad: 366.7704  LR: 0.000016  \n","Epoch: [2][1600/3575] Elapsed 5m 52s (remain 7m 15s) Loss: 0.0000(0.0070) Grad: 100.1849  LR: 0.000016  \n","Epoch: [2][1700/3575] Elapsed 6m 14s (remain 6m 52s) Loss: 0.0056(0.0071) Grad: 4339.3921  LR: 0.000016  \n","Epoch: [2][1800/3575] Elapsed 6m 36s (remain 6m 30s) Loss: 0.0000(0.0071) Grad: 21.1215  LR: 0.000016  \n","Epoch: [2][1900/3575] Elapsed 6m 58s (remain 6m 8s) Loss: 0.0113(0.0071) Grad: 20378.3887  LR: 0.000015  \n","Epoch: [2][2000/3575] Elapsed 7m 19s (remain 5m 46s) Loss: 0.0082(0.0070) Grad: 26981.3086  LR: 0.000015  \n","Epoch: [2][2100/3575] Elapsed 7m 41s (remain 5m 23s) Loss: 0.0052(0.0070) Grad: 72868.3516  LR: 0.000015  \n","Epoch: [2][2200/3575] Elapsed 8m 3s (remain 5m 1s) Loss: 0.0011(0.0070) Grad: 9265.1289  LR: 0.000015  \n","Epoch: [2][2300/3575] Elapsed 8m 24s (remain 4m 39s) Loss: 0.0034(0.0070) Grad: 9294.7383  LR: 0.000015  \n","Epoch: [2][2400/3575] Elapsed 8m 46s (remain 4m 17s) Loss: 0.0041(0.0070) Grad: 37676.0859  LR: 0.000015  \n","Epoch: [2][2500/3575] Elapsed 9m 8s (remain 3m 55s) Loss: 0.0001(0.0069) Grad: 382.4461  LR: 0.000015  \n","Epoch: [2][2600/3575] Elapsed 9m 29s (remain 3m 33s) Loss: 0.0076(0.0070) Grad: 26450.4551  LR: 0.000015  \n","Epoch: [2][2700/3575] Elapsed 9m 51s (remain 3m 11s) Loss: 0.0093(0.0069) Grad: 128448.3047  LR: 0.000014  \n","Epoch: [2][2800/3575] Elapsed 10m 12s (remain 2m 49s) Loss: 0.0000(0.0069) Grad: 39.8203  LR: 0.000014  \n","Epoch: [2][2900/3575] Elapsed 10m 34s (remain 2m 27s) Loss: 0.0000(0.0069) Grad: 21.1299  LR: 0.000014  \n","Epoch: [2][3000/3575] Elapsed 10m 55s (remain 2m 5s) Loss: 0.0020(0.0069) Grad: 43444.7695  LR: 0.000014  \n","Epoch: [2][3100/3575] Elapsed 11m 17s (remain 1m 43s) Loss: 0.0179(0.0069) Grad: 64482.5547  LR: 0.000014  \n","Epoch: [2][3200/3575] Elapsed 11m 38s (remain 1m 21s) Loss: 0.0074(0.0069) Grad: 10279.7109  LR: 0.000014  \n","Epoch: [2][3300/3575] Elapsed 12m 0s (remain 0m 59s) Loss: 0.0206(0.0069) Grad: 11067.3633  LR: 0.000014  \n","Epoch: [2][3400/3575] Elapsed 12m 22s (remain 0m 37s) Loss: 0.0070(0.0069) Grad: 13854.8164  LR: 0.000014  \n","Epoch: [2][3500/3575] Elapsed 12m 43s (remain 0m 16s) Loss: 0.0153(0.0069) Grad: 24534.4551  LR: 0.000013  \n","Epoch: [2][3574/3575] Elapsed 12m 59s (remain 0m 0s) Loss: 0.0054(0.0068) Grad: 32740.0391  LR: 0.000013  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 10m 28s) Loss: 0.0001(0.0001) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 9s) Loss: 0.0636(0.0099) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 54s) Loss: 0.0128(0.0091) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 42s) Loss: 0.0074(0.0089) \n","EVAL: [400/1192] Elapsed 0m 46s (remain 1m 31s) Loss: 0.0000(0.0086) \n","EVAL: [500/1192] Elapsed 0m 57s (remain 1m 19s) Loss: 0.0470(0.0079) \n","EVAL: [600/1192] Elapsed 1m 9s (remain 1m 7s) Loss: 0.0122(0.0082) \n","EVAL: [700/1192] Elapsed 1m 20s (remain 0m 56s) Loss: 0.0032(0.0090) \n","EVAL: [800/1192] Elapsed 1m 31s (remain 0m 44s) Loss: 0.0125(0.0089) \n","EVAL: [900/1192] Elapsed 1m 43s (remain 0m 33s) Loss: 0.0011(0.0091) \n","EVAL: [1000/1192] Elapsed 1m 54s (remain 0m 21s) Loss: 0.0000(0.0088) \n","EVAL: [1100/1192] Elapsed 2m 6s (remain 0m 10s) Loss: 0.0097(0.0084) \n","EVAL: [1191/1192] Elapsed 2m 16s (remain 0m 0s) Loss: 0.0000(0.0082) \n","Epoch 2 - avg_train_loss: 0.0068  avg_val_loss: 0.0082  time: 922s\n","Epoch 2 - Score: 0.8734\n","Epoch 2 - Save Best Score: 0.8734 Model\n","Epoch: [3][0/3575] Elapsed 0m 0s (remain 39m 45s) Loss: 0.0095(0.0095) Grad: 36274.2539  LR: 0.000013  \n","Epoch: [3][100/3575] Elapsed 0m 25s (remain 14m 30s) Loss: 0.0001(0.0046) Grad: 973.4727  LR: 0.000013  \n","Epoch: [3][200/3575] Elapsed 0m 49s (remain 13m 51s) Loss: 0.0014(0.0049) Grad: 8452.0537  LR: 0.000013  \n","Epoch: [3][300/3575] Elapsed 1m 11s (remain 12m 53s) Loss: 0.0069(0.0049) Grad: 226822.5625  LR: 0.000013  \n","Epoch: [3][400/3575] Elapsed 1m 32s (remain 12m 13s) Loss: 0.0014(0.0048) Grad: 2444.5771  LR: 0.000013  \n","Epoch: [3][500/3575] Elapsed 1m 54s (remain 11m 42s) Loss: 0.0001(0.0046) Grad: 192.0355  LR: 0.000013  \n","Epoch: [3][600/3575] Elapsed 2m 16s (remain 11m 12s) Loss: 0.0000(0.0050) Grad: 43.8176  LR: 0.000013  \n","Epoch: [3][700/3575] Elapsed 2m 37s (remain 10m 45s) Loss: 0.0149(0.0051) Grad: 53308.6875  LR: 0.000012  \n","Epoch: [3][800/3575] Elapsed 2m 58s (remain 10m 18s) Loss: 0.0023(0.0051) Grad: 13736.3604  LR: 0.000012  \n","Epoch: [3][900/3575] Elapsed 3m 20s (remain 9m 54s) Loss: 0.0122(0.0053) Grad: 8241.9756  LR: 0.000012  \n","Epoch: [3][1000/3575] Elapsed 3m 41s (remain 9m 29s) Loss: 0.0342(0.0053) Grad: 109117.6797  LR: 0.000012  \n","Epoch: [3][1100/3575] Elapsed 4m 3s (remain 9m 6s) Loss: 0.0007(0.0053) Grad: 3265.3914  LR: 0.000012  \n","Epoch: [3][1200/3575] Elapsed 4m 24s (remain 8m 42s) Loss: 0.0095(0.0055) Grad: 36727.9141  LR: 0.000012  \n","Epoch: [3][1300/3575] Elapsed 4m 45s (remain 8m 19s) Loss: 0.0002(0.0055) Grad: 890.1292  LR: 0.000012  \n","Epoch: [3][1400/3575] Elapsed 5m 7s (remain 7m 57s) Loss: 0.0008(0.0055) Grad: 3359.2327  LR: 0.000012  \n","Epoch: [3][1500/3575] Elapsed 5m 28s (remain 7m 34s) Loss: 0.0035(0.0055) Grad: 7610.3037  LR: 0.000011  \n","Epoch: [3][1600/3575] Elapsed 5m 50s (remain 7m 11s) Loss: 0.0016(0.0054) Grad: 12991.6143  LR: 0.000011  \n","Epoch: [3][1700/3575] Elapsed 6m 11s (remain 6m 49s) Loss: 0.0040(0.0056) Grad: 10789.5596  LR: 0.000011  \n","Epoch: [3][1800/3575] Elapsed 6m 33s (remain 6m 27s) Loss: 0.0040(0.0055) Grad: 30317.2207  LR: 0.000011  \n","Epoch: [3][1900/3575] Elapsed 6m 54s (remain 6m 5s) Loss: 0.0019(0.0055) Grad: 12136.7900  LR: 0.000011  \n","Epoch: [3][2000/3575] Elapsed 7m 16s (remain 5m 43s) Loss: 0.0000(0.0055) Grad: 188.1723  LR: 0.000011  \n","Epoch: [3][2100/3575] Elapsed 7m 37s (remain 5m 21s) Loss: 0.0000(0.0054) Grad: 178.1973  LR: 0.000011  \n","Epoch: [3][2200/3575] Elapsed 7m 59s (remain 4m 59s) Loss: 0.0001(0.0054) Grad: 224.9906  LR: 0.000011  \n","Epoch: [3][2300/3575] Elapsed 8m 20s (remain 4m 37s) Loss: 0.0000(0.0054) Grad: 77.3356  LR: 0.000010  \n","Epoch: [3][2400/3575] Elapsed 8m 41s (remain 4m 15s) Loss: 0.0082(0.0054) Grad: 19935.9746  LR: 0.000010  \n","Epoch: [3][2500/3575] Elapsed 9m 3s (remain 3m 53s) Loss: 0.0005(0.0055) Grad: 4470.6187  LR: 0.000010  \n","Epoch: [3][2600/3575] Elapsed 9m 24s (remain 3m 31s) Loss: 0.0000(0.0055) Grad: 34.9392  LR: 0.000010  \n","Epoch: [3][2700/3575] Elapsed 9m 46s (remain 3m 9s) Loss: 0.0244(0.0055) Grad: 21650.9590  LR: 0.000010  \n","Epoch: [3][2800/3575] Elapsed 10m 7s (remain 2m 47s) Loss: 0.0073(0.0055) Grad: 21432.9551  LR: 0.000010  \n","Epoch: [3][2900/3575] Elapsed 10m 28s (remain 2m 26s) Loss: 0.0206(0.0056) Grad: 58118.3984  LR: 0.000010  \n","Epoch: [3][3000/3575] Elapsed 10m 50s (remain 2m 4s) Loss: 0.0000(0.0055) Grad: 41.4848  LR: 0.000010  \n","Epoch: [3][3100/3575] Elapsed 11m 11s (remain 1m 42s) Loss: 0.0030(0.0055) Grad: 6352.4106  LR: 0.000009  \n","Epoch: [3][3200/3575] Elapsed 11m 32s (remain 1m 20s) Loss: 0.0037(0.0055) Grad: 3670.7781  LR: 0.000009  \n","Epoch: [3][3300/3575] Elapsed 11m 54s (remain 0m 59s) Loss: 0.0000(0.0055) Grad: 73.0796  LR: 0.000009  \n","Epoch: [3][3400/3575] Elapsed 12m 15s (remain 0m 37s) Loss: 0.0000(0.0055) Grad: 153.1035  LR: 0.000009  \n","Epoch: [3][3500/3575] Elapsed 12m 37s (remain 0m 16s) Loss: 0.0036(0.0055) Grad: 10352.5107  LR: 0.000009  \n","Epoch: [3][3574/3575] Elapsed 12m 52s (remain 0m 0s) Loss: 0.0008(0.0055) Grad: 3121.9622  LR: 0.000009  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 10m 4s) Loss: 0.0002(0.0002) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 9s) Loss: 0.0333(0.0073) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 54s) Loss: 0.0039(0.0061) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 42s) Loss: 0.0081(0.0063) \n","EVAL: [400/1192] Elapsed 0m 46s (remain 1m 30s) Loss: 0.0000(0.0062) \n","EVAL: [500/1192] Elapsed 0m 57s (remain 1m 19s) Loss: 0.0335(0.0060) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 7s) Loss: 0.0098(0.0062) \n","EVAL: [700/1192] Elapsed 1m 20s (remain 0m 56s) Loss: 0.0051(0.0070) \n","EVAL: [800/1192] Elapsed 1m 31s (remain 0m 44s) Loss: 0.0079(0.0069) \n","EVAL: [900/1192] Elapsed 1m 42s (remain 0m 33s) Loss: 0.0002(0.0071) \n","EVAL: [1000/1192] Elapsed 1m 54s (remain 0m 21s) Loss: 0.0000(0.0068) \n","EVAL: [1100/1192] Elapsed 2m 5s (remain 0m 10s) Loss: 0.0095(0.0066) \n","EVAL: [1191/1192] Elapsed 2m 16s (remain 0m 0s) Loss: 0.0000(0.0065) \n","Epoch 3 - avg_train_loss: 0.0055  avg_val_loss: 0.0065  time: 915s\n","Epoch 3 - Score: 0.8827\n","Epoch 3 - Save Best Score: 0.8827 Model\n","Epoch: [4][0/3575] Elapsed 0m 0s (remain 40m 7s) Loss: 0.0014(0.0014) Grad: 10558.7588  LR: 0.000009  \n","Epoch: [4][100/3575] Elapsed 0m 25s (remain 14m 43s) Loss: 0.0005(0.0046) Grad: 2950.4397  LR: 0.000009  \n","Epoch: [4][200/3575] Elapsed 0m 50s (remain 14m 7s) Loss: 0.0000(0.0041) Grad: 173.4174  LR: 0.000009  \n","Epoch: [4][300/3575] Elapsed 1m 12s (remain 13m 5s) Loss: 0.0497(0.0043) Grad: 111093.5469  LR: 0.000009  \n","Epoch: [4][400/3575] Elapsed 1m 33s (remain 12m 23s) Loss: 0.0000(0.0043) Grad: 219.7241  LR: 0.000008  \n","Epoch: [4][500/3575] Elapsed 1m 55s (remain 11m 49s) Loss: 0.0000(0.0038) Grad: 42.3261  LR: 0.000008  \n","Epoch: [4][600/3575] Elapsed 2m 17s (remain 11m 20s) Loss: 0.0000(0.0037) Grad: 95.5221  LR: 0.000008  \n","Epoch: [4][700/3575] Elapsed 2m 39s (remain 10m 52s) Loss: 0.0014(0.0040) Grad: 14765.3652  LR: 0.000008  \n","Epoch: [4][800/3575] Elapsed 3m 1s (remain 10m 27s) Loss: 0.0002(0.0040) Grad: 1826.2994  LR: 0.000008  \n","Epoch: [4][900/3575] Elapsed 3m 22s (remain 10m 1s) Loss: 0.0109(0.0039) Grad: 19997.9414  LR: 0.000008  \n","Epoch: [4][1000/3575] Elapsed 3m 44s (remain 9m 36s) Loss: 0.0003(0.0039) Grad: 2607.1411  LR: 0.000008  \n","Epoch: [4][1100/3575] Elapsed 4m 5s (remain 9m 12s) Loss: 0.0026(0.0039) Grad: 17218.0488  LR: 0.000008  \n","Epoch: [4][1200/3575] Elapsed 4m 27s (remain 8m 49s) Loss: 0.0029(0.0039) Grad: 20468.8848  LR: 0.000007  \n","Epoch: [4][1300/3575] Elapsed 4m 49s (remain 8m 25s) Loss: 0.0001(0.0039) Grad: 1482.7000  LR: 0.000007  \n","Epoch: [4][1400/3575] Elapsed 5m 10s (remain 8m 2s) Loss: 0.0001(0.0039) Grad: 1655.8175  LR: 0.000007  \n","Epoch: [4][1500/3575] Elapsed 5m 32s (remain 7m 39s) Loss: 0.0041(0.0039) Grad: 38894.2969  LR: 0.000007  \n","Epoch: [4][1600/3575] Elapsed 5m 54s (remain 7m 16s) Loss: 0.0027(0.0039) Grad: 27998.5332  LR: 0.000007  \n","Epoch: [4][1700/3575] Elapsed 6m 15s (remain 6m 54s) Loss: 0.0000(0.0040) Grad: 120.6133  LR: 0.000007  \n","Epoch: [4][1800/3575] Elapsed 6m 37s (remain 6m 31s) Loss: 0.0078(0.0040) Grad: 43370.3438  LR: 0.000007  \n","Epoch: [4][1900/3575] Elapsed 6m 59s (remain 6m 9s) Loss: 0.0036(0.0040) Grad: 10386.2490  LR: 0.000007  \n","Epoch: [4][2000/3575] Elapsed 7m 20s (remain 5m 46s) Loss: 0.0000(0.0040) Grad: 80.2424  LR: 0.000006  \n","Epoch: [4][2100/3575] Elapsed 7m 42s (remain 5m 24s) Loss: 0.0032(0.0040) Grad: 19242.8613  LR: 0.000006  \n","Epoch: [4][2200/3575] Elapsed 8m 4s (remain 5m 2s) Loss: 0.0000(0.0041) Grad: 35.6844  LR: 0.000006  \n","Epoch: [4][2300/3575] Elapsed 8m 26s (remain 4m 40s) Loss: 0.0000(0.0041) Grad: 17.1729  LR: 0.000006  \n","Epoch: [4][2400/3575] Elapsed 8m 47s (remain 4m 18s) Loss: 0.0014(0.0042) Grad: 14094.8604  LR: 0.000006  \n","Epoch: [4][2500/3575] Elapsed 9m 9s (remain 3m 55s) Loss: 0.0000(0.0042) Grad: 21.8276  LR: 0.000006  \n","Epoch: [4][2600/3575] Elapsed 9m 31s (remain 3m 33s) Loss: 0.0010(0.0043) Grad: 27048.4492  LR: 0.000006  \n","Epoch: [4][2700/3575] Elapsed 9m 52s (remain 3m 11s) Loss: 0.0013(0.0042) Grad: 19096.9258  LR: 0.000006  \n","Epoch: [4][2800/3575] Elapsed 10m 14s (remain 2m 49s) Loss: 0.0008(0.0042) Grad: 9652.3154  LR: 0.000005  \n","Epoch: [4][2900/3575] Elapsed 10m 35s (remain 2m 27s) Loss: 0.0000(0.0042) Grad: 22.4373  LR: 0.000005  \n","Epoch: [4][3000/3575] Elapsed 10m 57s (remain 2m 5s) Loss: 0.0000(0.0043) Grad: 328.9300  LR: 0.000005  \n","Epoch: [4][3100/3575] Elapsed 11m 18s (remain 1m 43s) Loss: 0.0015(0.0043) Grad: 8793.7979  LR: 0.000005  \n","Epoch: [4][3200/3575] Elapsed 11m 39s (remain 1m 21s) Loss: 0.0061(0.0043) Grad: 16491.7207  LR: 0.000005  \n","Epoch: [4][3300/3575] Elapsed 12m 1s (remain 0m 59s) Loss: 0.0000(0.0044) Grad: 75.2723  LR: 0.000005  \n","Epoch: [4][3400/3575] Elapsed 12m 22s (remain 0m 37s) Loss: 0.0047(0.0043) Grad: 27722.8574  LR: 0.000005  \n","Epoch: [4][3500/3575] Elapsed 12m 44s (remain 0m 16s) Loss: 0.0003(0.0043) Grad: 1509.0486  LR: 0.000005  \n","Epoch: [4][3574/3575] Elapsed 12m 59s (remain 0m 0s) Loss: 0.0074(0.0043) Grad: 62600.9844  LR: 0.000004  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 9m 58s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 8s) Loss: 0.0376(0.0077) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 55s) Loss: 0.0066(0.0068) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 42s) Loss: 0.0085(0.0072) \n","EVAL: [400/1192] Elapsed 0m 46s (remain 1m 30s) Loss: 0.0000(0.0072) \n","EVAL: [500/1192] Elapsed 0m 57s (remain 1m 19s) Loss: 0.0254(0.0069) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 7s) Loss: 0.0111(0.0072) \n","EVAL: [700/1192] Elapsed 1m 20s (remain 0m 56s) Loss: 0.0044(0.0081) \n","EVAL: [800/1192] Elapsed 1m 31s (remain 0m 44s) Loss: 0.0067(0.0082) \n","EVAL: [900/1192] Elapsed 1m 42s (remain 0m 33s) Loss: 0.0036(0.0082) \n","EVAL: [1000/1192] Elapsed 1m 54s (remain 0m 21s) Loss: 0.0000(0.0080) \n","EVAL: [1100/1192] Elapsed 2m 5s (remain 0m 10s) Loss: 0.0125(0.0077) \n","EVAL: [1191/1192] Elapsed 2m 15s (remain 0m 0s) Loss: 0.0000(0.0075) \n","Epoch 4 - avg_train_loss: 0.0043  avg_val_loss: 0.0075  time: 921s\n","Epoch 4 - Score: 0.8861\n","Epoch 4 - Save Best Score: 0.8861 Model\n","Epoch: [5][0/3575] Elapsed 0m 0s (remain 38m 36s) Loss: 0.0000(0.0000) Grad: 1000.0280  LR: 0.000004  \n","Epoch: [5][100/3575] Elapsed 0m 25s (remain 14m 33s) Loss: 0.0000(0.0031) Grad: 105.0788  LR: 0.000004  \n","Epoch: [5][200/3575] Elapsed 0m 49s (remain 13m 52s) Loss: 0.0007(0.0030) Grad: 3425.5886  LR: 0.000004  \n","Epoch: [5][300/3575] Elapsed 1m 11s (remain 12m 53s) Loss: 0.0025(0.0032) Grad: 28705.4434  LR: 0.000004  \n","Epoch: [5][400/3575] Elapsed 1m 32s (remain 12m 13s) Loss: 0.0000(0.0030) Grad: 15.6807  LR: 0.000004  \n","Epoch: [5][500/3575] Elapsed 1m 54s (remain 11m 39s) Loss: 0.0000(0.0036) Grad: 5.7334  LR: 0.000004  \n","Epoch: [5][600/3575] Elapsed 2m 15s (remain 11m 9s) Loss: 0.0000(0.0037) Grad: 38.2233  LR: 0.000004  \n","Epoch: [5][700/3575] Elapsed 2m 36s (remain 10m 41s) Loss: 0.0000(0.0035) Grad: 19.2311  LR: 0.000004  \n","Epoch: [5][800/3575] Elapsed 2m 57s (remain 10m 16s) Loss: 0.0063(0.0035) Grad: 48587.5586  LR: 0.000003  \n","Epoch: [5][900/3575] Elapsed 3m 19s (remain 9m 52s) Loss: 0.0000(0.0034) Grad: 22.0492  LR: 0.000003  \n","Epoch: [5][1000/3575] Elapsed 3m 41s (remain 9m 28s) Loss: 0.0001(0.0033) Grad: 266.9322  LR: 0.000003  \n","Epoch: [5][1100/3575] Elapsed 4m 2s (remain 9m 5s) Loss: 0.0154(0.0033) Grad: 43455.1172  LR: 0.000003  \n","Epoch: [5][1200/3575] Elapsed 4m 23s (remain 8m 41s) Loss: 0.0525(0.0034) Grad: 55161.9297  LR: 0.000003  \n","Epoch: [5][1300/3575] Elapsed 4m 44s (remain 8m 17s) Loss: 0.0200(0.0034) Grad: 59765.3516  LR: 0.000003  \n","Epoch: [5][1400/3575] Elapsed 5m 5s (remain 7m 54s) Loss: 0.0052(0.0033) Grad: 8558.1680  LR: 0.000003  \n","Epoch: [5][1500/3575] Elapsed 5m 26s (remain 7m 31s) Loss: 0.0000(0.0034) Grad: 119.7961  LR: 0.000003  \n","Epoch: [5][1600/3575] Elapsed 5m 47s (remain 7m 9s) Loss: 0.0000(0.0034) Grad: 459.3757  LR: 0.000002  \n","Epoch: [5][1700/3575] Elapsed 6m 9s (remain 6m 46s) Loss: 0.0083(0.0035) Grad: 43715.2773  LR: 0.000002  \n","Epoch: [5][1800/3575] Elapsed 6m 29s (remain 6m 24s) Loss: 0.0037(0.0034) Grad: 18723.2090  LR: 0.000002  \n","Epoch: [5][1900/3575] Elapsed 6m 51s (remain 6m 1s) Loss: 0.0041(0.0034) Grad: 37384.3086  LR: 0.000002  \n","Epoch: [5][2000/3575] Elapsed 7m 12s (remain 5m 39s) Loss: 0.0006(0.0034) Grad: 3745.3149  LR: 0.000002  \n","Epoch: [5][2100/3575] Elapsed 7m 33s (remain 5m 17s) Loss: 0.0002(0.0035) Grad: 1632.6500  LR: 0.000002  \n","Epoch: [5][2200/3575] Elapsed 7m 54s (remain 4m 56s) Loss: 0.0012(0.0034) Grad: 6428.5122  LR: 0.000002  \n","Epoch: [5][2300/3575] Elapsed 8m 15s (remain 4m 34s) Loss: 0.0071(0.0034) Grad: 43507.5273  LR: 0.000002  \n","Epoch: [5][2400/3575] Elapsed 8m 36s (remain 4m 12s) Loss: 0.0030(0.0034) Grad: 10307.1719  LR: 0.000001  \n","Epoch: [5][2500/3575] Elapsed 8m 57s (remain 3m 50s) Loss: 0.0000(0.0034) Grad: 56.1655  LR: 0.000001  \n","Epoch: [5][2600/3575] Elapsed 9m 18s (remain 3m 29s) Loss: 0.0007(0.0034) Grad: 7728.5728  LR: 0.000001  \n","Epoch: [5][2700/3575] Elapsed 9m 40s (remain 3m 7s) Loss: 0.0045(0.0034) Grad: 3461.4880  LR: 0.000001  \n","Epoch: [5][2800/3575] Elapsed 10m 1s (remain 2m 46s) Loss: 0.0121(0.0034) Grad: 12756.4131  LR: 0.000001  \n","Epoch: [5][2900/3575] Elapsed 10m 21s (remain 2m 24s) Loss: 0.0001(0.0034) Grad: 467.2831  LR: 0.000001  \n","Epoch: [5][3000/3575] Elapsed 10m 42s (remain 2m 2s) Loss: 0.0021(0.0034) Grad: 13384.9580  LR: 0.000001  \n","Epoch: [5][3100/3575] Elapsed 11m 3s (remain 1m 41s) Loss: 0.0465(0.0034) Grad: 44645.1211  LR: 0.000001  \n","Epoch: [5][3200/3575] Elapsed 11m 24s (remain 1m 20s) Loss: 0.0000(0.0034) Grad: 17.3639  LR: 0.000000  \n","Epoch: [5][3300/3575] Elapsed 11m 45s (remain 0m 58s) Loss: 0.0409(0.0034) Grad: 60812.6680  LR: 0.000000  \n","Epoch: [5][3400/3575] Elapsed 12m 6s (remain 0m 37s) Loss: 0.0012(0.0034) Grad: 15119.8262  LR: 0.000000  \n","Epoch: [5][3500/3575] Elapsed 12m 27s (remain 0m 15s) Loss: 0.0000(0.0034) Grad: 44.4458  LR: 0.000000  \n","Epoch: [5][3574/3575] Elapsed 12m 43s (remain 0m 0s) Loss: 0.0051(0.0034) Grad: 32080.1875  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 9m 7s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 6s) Loss: 0.0370(0.0077) \n","EVAL: [200/1192] Elapsed 0m 22s (remain 1m 53s) Loss: 0.0078(0.0068) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 41s) Loss: 0.0112(0.0074) \n","EVAL: [400/1192] Elapsed 0m 45s (remain 1m 29s) Loss: 0.0000(0.0074) \n","EVAL: [500/1192] Elapsed 0m 56s (remain 1m 18s) Loss: 0.0304(0.0071) \n","EVAL: [600/1192] Elapsed 1m 7s (remain 1m 6s) Loss: 0.0112(0.0074) \n","EVAL: [700/1192] Elapsed 1m 18s (remain 0m 55s) Loss: 0.0046(0.0083) \n","EVAL: [800/1192] Elapsed 1m 30s (remain 0m 44s) Loss: 0.0158(0.0085) \n","EVAL: [900/1192] Elapsed 1m 41s (remain 0m 32s) Loss: 0.0036(0.0085) \n","EVAL: [1000/1192] Elapsed 1m 52s (remain 0m 21s) Loss: 0.0000(0.0083) \n","EVAL: [1100/1192] Elapsed 2m 3s (remain 0m 10s) Loss: 0.0127(0.0080) \n","EVAL: [1191/1192] Elapsed 2m 13s (remain 0m 0s) Loss: 0.0000(0.0078) \n","Epoch 5 - avg_train_loss: 0.0034  avg_val_loss: 0.0078  time: 902s\n","Epoch 5 - Score: 0.8868\n","Epoch 5 - Save Best Score: 0.8868 Model\n","Best thres: 0.5, Score: 0.8840\n","Best thres: 0.475, Score: 0.8842\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29ec628d736b46f9a626469bdb8a7dba"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9abb0340892b406faa4c1243436356ac"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bd6642ecbd24fd2a09e3c3665193618"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e86016676bf74eaf9beecdaecd06ad71"}},"metadata":{}}],"source":["if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"name":"nbme-exp074.ipynb","provenance":[{"file_id":"10yG4L3_nzpdL2CDwqxa9r-KWq6jYkWfl","timestamp":1649164439720}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"papermill":{"default_parameters":{},"duration":641.572862,"end_time":"2022-02-27T11:39:50.972497","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-02-27T11:29:09.399635","version":"2.3.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b622999d368f462497b44a1bdde58026":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c8364e7f36f341dd904a1a839ebff4ee","IPY_MODEL_2f4c77eff3f54a288254be249755af0b","IPY_MODEL_b9167e8982e6483a8f76de7f2dcd3a8f"],"layout":"IPY_MODEL_fcf50e9fb2c845d9b6747df774017c4f"}},"c8364e7f36f341dd904a1a839ebff4ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7eebd0e36ffd475dbd73997ec1246fd3","placeholder":"​","style":"IPY_MODEL_27fd3d0f816b4926a41b599682e722be","value":"100%"}},"2f4c77eff3f54a288254be249755af0b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_34152585b441449fb0a4f22ca9904b0d","max":42146,"min":0,"orientation":"horizontal","style":"IPY_MODEL_21efec1049b44e4b84d1b00434d7cc33","value":42146}},"b9167e8982e6483a8f76de7f2dcd3a8f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f96ac5b7aa045068e5b0f53c1535a19","placeholder":"​","style":"IPY_MODEL_af11b872c2c74af595c17e6a778b235c","value":" 42146/42146 [00:33&lt;00:00, 1927.71it/s]"}},"fcf50e9fb2c845d9b6747df774017c4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7eebd0e36ffd475dbd73997ec1246fd3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27fd3d0f816b4926a41b599682e722be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"34152585b441449fb0a4f22ca9904b0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21efec1049b44e4b84d1b00434d7cc33":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8f96ac5b7aa045068e5b0f53c1535a19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af11b872c2c74af595c17e6a778b235c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e1f40cc06403408b82f6e023125309da":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ed3ab8520f4b450cb89f2e900a7ac211","IPY_MODEL_61ed9c24cd9a4f0d90d09fefa0ae9982","IPY_MODEL_09ce14321a4e42988612d677e9665abb"],"layout":"IPY_MODEL_e9c02a359c9340ccab2676b0f454bc96"}},"ed3ab8520f4b450cb89f2e900a7ac211":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4173fce864e4662a2975810b5a28772","placeholder":"​","style":"IPY_MODEL_b34183983e454d78bde1f80442163188","value":"100%"}},"61ed9c24cd9a4f0d90d09fefa0ae9982":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae57241309e247ce9405aee67ba4428b","max":143,"min":0,"orientation":"horizontal","style":"IPY_MODEL_177cae8328264c509e9bd392d589cd98","value":143}},"09ce14321a4e42988612d677e9665abb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_abc1a522591a49568ac9071c88218d78","placeholder":"​","style":"IPY_MODEL_e65fa2f37ae34c7ba14a631be783f549","value":" 143/143 [00:00&lt;00:00, 2638.81it/s]"}},"e9c02a359c9340ccab2676b0f454bc96":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4173fce864e4662a2975810b5a28772":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b34183983e454d78bde1f80442163188":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae57241309e247ce9405aee67ba4428b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"177cae8328264c509e9bd392d589cd98":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"abc1a522591a49568ac9071c88218d78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e65fa2f37ae34c7ba14a631be783f549":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18f97f51777f4a508de043f5fc46aed9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c99a1ea14f5b451eb4dcdbe176125bb7","IPY_MODEL_72c6c9a22120445f98b54304a4a126eb","IPY_MODEL_834bf7915e864023a79602d1b6625a6a"],"layout":"IPY_MODEL_9b2af3df6d3643c58df2dba8d46ba981"}},"c99a1ea14f5b451eb4dcdbe176125bb7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_712bfa8359a24221936c8e54f4a6d9b3","placeholder":"​","style":"IPY_MODEL_3d3df510be6a4f9a840acd16cec77aa0","value":"Downloading: 100%"}},"72c6c9a22120445f98b54304a4a126eb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_63fb64722b554406b98c8f03ae61dfdd","max":873673253,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1ca3ed8064084a7a9e4c37017ec35a7d","value":873673253}},"834bf7915e864023a79602d1b6625a6a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8aaef039787d487daeb140cd144e60d5","placeholder":"​","style":"IPY_MODEL_5ac7568e8d394aa9b402e49c2abf61b5","value":" 833M/833M [00:17&lt;00:00, 52.9MB/s]"}},"9b2af3df6d3643c58df2dba8d46ba981":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"712bfa8359a24221936c8e54f4a6d9b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d3df510be6a4f9a840acd16cec77aa0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"63fb64722b554406b98c8f03ae61dfdd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ca3ed8064084a7a9e4c37017ec35a7d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8aaef039787d487daeb140cd144e60d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ac7568e8d394aa9b402e49c2abf61b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"29ec628d736b46f9a626469bdb8a7dba":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_07f85e30620e495cb543b2b2ad295e94","IPY_MODEL_f9ea345e103d4c5697783485bb3d7368","IPY_MODEL_f2a8c13ba9f142319b61bb2d1c99a900"],"layout":"IPY_MODEL_21367b1b7d424b9784da103ca9ba08f8"}},"07f85e30620e495cb543b2b2ad295e94":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d9665608ced042e8a58b24dbda64a51c","placeholder":"​","style":"IPY_MODEL_434cf5b9d19440c7bf3e4fe67fb5878d","value":"100%"}},"f9ea345e103d4c5697783485bb3d7368":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_81ec77c78b1440e39093e79560e1ff2e","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_83c8a95d49c04346b24094dfb3d9dc55","value":2}},"f2a8c13ba9f142319b61bb2d1c99a900":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b53674073334ed097472af0a0835925","placeholder":"​","style":"IPY_MODEL_96a796f8a0124af3b231d6a8671a4be0","value":" 2/2 [00:01&lt;00:00,  1.21s/it]"}},"21367b1b7d424b9784da103ca9ba08f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9665608ced042e8a58b24dbda64a51c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"434cf5b9d19440c7bf3e4fe67fb5878d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"81ec77c78b1440e39093e79560e1ff2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83c8a95d49c04346b24094dfb3d9dc55":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4b53674073334ed097472af0a0835925":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96a796f8a0124af3b231d6a8671a4be0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9abb0340892b406faa4c1243436356ac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1e8f996fceee4ccfa111a81cd6b10b1b","IPY_MODEL_91828c5639ae46a691d6d02794d01a9a","IPY_MODEL_a24e46d743414932b5b65c5db1abc75e"],"layout":"IPY_MODEL_877c0896301b49769895a6f2a0ef794d"}},"1e8f996fceee4ccfa111a81cd6b10b1b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4131446f577342dea87fafed4da5b61e","placeholder":"​","style":"IPY_MODEL_50c2df816c534d60b67fc8891ad81137","value":"100%"}},"91828c5639ae46a691d6d02794d01a9a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7b2ac2723dd4ca481cea24c3d4e84dc","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_967a72ab0f314d799caf7a246eef0308","value":2}},"a24e46d743414932b5b65c5db1abc75e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ef817ce62684ae387d3fb7374058461","placeholder":"​","style":"IPY_MODEL_782a73f82a74413f80f9b2e3c4d59d81","value":" 2/2 [00:01&lt;00:00,  1.19s/it]"}},"877c0896301b49769895a6f2a0ef794d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4131446f577342dea87fafed4da5b61e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50c2df816c534d60b67fc8891ad81137":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c7b2ac2723dd4ca481cea24c3d4e84dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"967a72ab0f314d799caf7a246eef0308":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8ef817ce62684ae387d3fb7374058461":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"782a73f82a74413f80f9b2e3c4d59d81":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9bd6642ecbd24fd2a09e3c3665193618":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f75a5054123b44ae81644024ecd04aa9","IPY_MODEL_6d9fe4f2f22140a4997154380a4c71fe","IPY_MODEL_4e043dc5a8cb4b52b53b9c9647dff92e"],"layout":"IPY_MODEL_c0d6b63bc7504beaaa9c420f965eb540"}},"f75a5054123b44ae81644024ecd04aa9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_115f541992344fe6bab38965689f00db","placeholder":"​","style":"IPY_MODEL_c863ed2448ef4fa68669b6ce93fdf585","value":"100%"}},"6d9fe4f2f22140a4997154380a4c71fe":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d8eff25c3a994e9eb18761c92c355beb","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3198f4b4bbbb4ae3b96d100ad4200123","value":2}},"4e043dc5a8cb4b52b53b9c9647dff92e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53e497b701f14e5eb5473a5696bb39d3","placeholder":"​","style":"IPY_MODEL_9395424bcdca41e69787729f90a17707","value":" 2/2 [00:01&lt;00:00,  1.39s/it]"}},"c0d6b63bc7504beaaa9c420f965eb540":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"115f541992344fe6bab38965689f00db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c863ed2448ef4fa68669b6ce93fdf585":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d8eff25c3a994e9eb18761c92c355beb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3198f4b4bbbb4ae3b96d100ad4200123":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"53e497b701f14e5eb5473a5696bb39d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9395424bcdca41e69787729f90a17707":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e86016676bf74eaf9beecdaecd06ad71":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1beb9ba25a514238aede0c5b6f4df4f1","IPY_MODEL_b6cd03fcb5464b9db40c6d70dd140992","IPY_MODEL_2cf4ee08245343c68218f037aca4f9e0"],"layout":"IPY_MODEL_d4177f6a811a4180aae4832dc1ce21bc"}},"1beb9ba25a514238aede0c5b6f4df4f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84cce2317c1148bda39f240bce67ba33","placeholder":"​","style":"IPY_MODEL_8f6f4290ea9e462b8fd7ae93a4d51e06","value":"100%"}},"b6cd03fcb5464b9db40c6d70dd140992":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fbe7c4115fc2472b9ff70247bb21d1c4","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5f5bf4104fb44803a64c2d9373081882","value":2}},"2cf4ee08245343c68218f037aca4f9e0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6affdef5d47a44468201bebf5202dfc1","placeholder":"​","style":"IPY_MODEL_bdfde0e657e74cc3a20d2ed57fd34200","value":" 2/2 [00:02&lt;00:00,  1.58s/it]"}},"d4177f6a811a4180aae4832dc1ce21bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84cce2317c1148bda39f240bce67ba33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f6f4290ea9e462b8fd7ae93a4d51e06":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fbe7c4115fc2472b9ff70247bb21d1c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f5bf4104fb44803a64c2d9373081882":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6affdef5d47a44468201bebf5202dfc1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bdfde0e657e74cc3a20d2ed57fd34200":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}