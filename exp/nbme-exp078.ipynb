{"cells":[{"cell_type":"markdown","id":"colored-security","metadata":{"id":"colored-security"},"source":["## References"]},{"cell_type":"markdown","id":"educational-operator","metadata":{"id":"educational-operator"},"source":["- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"]},{"cell_type":"markdown","id":"incorrect-greek","metadata":{"id":"incorrect-greek"},"source":["## Configurations"]},{"cell_type":"code","execution_count":1,"id":"alive-granny","metadata":{"id":"alive-granny","executionInfo":{"status":"ok","timestamp":1649326531562,"user_tz":-540,"elapsed":641,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["EXP_NAME = \"nbme-exp078\"\n","ENV = \"colab\"\n","DEBUG_MODE = False\n","SUBMISSION_MODE = False"]},{"cell_type":"code","execution_count":2,"id":"heavy-prophet","metadata":{"id":"heavy-prophet","executionInfo":{"status":"ok","timestamp":1649326532443,"user_tz":-540,"elapsed":438,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["class CFG:\n","    env=ENV\n","    exp_name=EXP_NAME\n","    debug=DEBUG_MODE\n","    submission=SUBMISSION_MODE\n","    apex=True\n","    input_dir=None\n","    output_dir=None\n","    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n","    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n","    competition_name=\"nbme-score-clinical-patient-notes\"\n","    id_col=\"id\"\n","    target_col=\"location\"\n","    pretrained_model_name=\"microsoft/deberta-v3-large\"\n","    tokenizer=None\n","    max_len=None\n","    output_dim=1\n","    dropout=0.2\n","    num_workers=4\n","    batch_size=3 # 8\n","    test_batch_size=3\n","    lr=2e-5\n","    fc_dropout=0.6\n","    reinit_layers=0 # https://openreview.net/pdf?id=cO1IH43yUF\n","    min_lr=1e-6\n","    eps=1e-6\n","    betas=(0.9, 0.999)\n","    weight_decay=00.1\n","    encoder_lr=2e-5\n","    decoder_lr=2e-5\n","    group_step=8\n","    lr_scale=1e-6\n","    alpha=1\n","    gamma=2\n","    smoothing=0.0001\n","    p_aug=0.5\n","    p_mask_aug=0. # 0.5\n","    mask_ratio=0. # 0.15\n","    p_aug_epoch=2\n","    pos_length_weight_decay=0.0\n","    num_cycles=0.5\n","    num_warmup_steps=0\n","    batch_scheduler=True\n","    epochs=4\n","    n_fold=4\n","    train_fold=[0, 1, 2, 3]\n","    seed=71\n","    gradient_accumulation_steps=1\n","    max_grad_norm=1000\n","    print_freq=100\n","    train=True\n","    inference=True"]},{"cell_type":"code","execution_count":3,"id":"vocational-coating","metadata":{"id":"vocational-coating","executionInfo":{"status":"ok","timestamp":1649326532443,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.train_fold = [0, 1]\n","\n","if CFG.submission:\n","    CFG.train = False\n","    CFG.inference = True"]},{"cell_type":"markdown","id":"private-moderator","metadata":{"id":"private-moderator"},"source":["## Directory Settings"]},{"cell_type":"code","execution_count":4,"id":"married-tokyo","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"married-tokyo","outputId":"b9e9b43e-0477-459b-d273-718d80f6e1fe","executionInfo":{"status":"ok","timestamp":1649326545610,"user_tz":-540,"elapsed":13172,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["colab\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: transformers==4.16.2 in /usr/local/lib/python3.7/dist-packages (4.16.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (3.6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (1.21.5)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (0.0.49)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (0.11.6)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (0.5.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.11.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.63.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2019.12.20)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.16.2) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.16.2) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (7.1.2)\n"]}],"source":["import sys\n","from pathlib import Path\n","\n","\n","print(CFG.env)\n","if CFG.env == \"colab\":\n","    # colab環境\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","    # install packages\n","    !pip install transformers==4.16.2\n","    !pip install -q sentencepiece==0.1.96\n","\n","elif CFG.env == \"local\":\n","    # ローカルサーバ\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","\n","elif CFG.env == \"kaggle\":\n","    # kaggle環境\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./\")"]},{"cell_type":"code","source":["# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n","# This must be done before importing transformers\n","import shutil\n","from pathlib import Path\n","\n","if CFG.env == \"colab\":\n","    input_dir = Path(\"./drive/MyDrive/00.kaggle/input/deberta-v2-3-fast-tokenizer\")\n","    transformers_path = Path(\"/usr/local/lib/python3.7/dist-packages/transformers\")\n","else:\n","    input_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n","    transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n","\n","convert_file = input_dir / \"convert_slow_tokenizer.py\"\n","conversion_path = transformers_path/convert_file.name\n","\n","if conversion_path.exists():\n","    conversion_path.unlink()\n","\n","shutil.copy(convert_file, transformers_path)\n","deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n","\n","for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n","    filepath = deberta_v2_path/filename\n","    if filepath.exists():\n","        filepath.unlink()\n","\n","    shutil.copy(input_dir/filename, filepath)\n","    \n","    \n","from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast"],"metadata":{"id":"cnGM_g9c3WJW","executionInfo":{"status":"ok","timestamp":1649326557574,"user_tz":-540,"elapsed":11972,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"id":"cnGM_g9c3WJW","execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"id":"blank-pierre","metadata":{"id":"blank-pierre","executionInfo":{"status":"ok","timestamp":1649326558685,"user_tz":-540,"elapsed":1129,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["import gc\n","import os\n","import ast\n","import time\n","import math\n","import random\n","import itertools\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","from scipy.optimize import minimize\n","from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision.transforms as T\n","from torchvision.io import read_image\n","from torch.utils.data import DataLoader, Dataset\n","\n","from transformers import AutoModelForMaskedLM\n","from transformers import BartModel,BertModel,BertTokenizer\n","from transformers import DebertaModel,DebertaTokenizer\n","from transformers import RobertaModel,RobertaTokenizer\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n","from transformers import AdamW, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n","from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","id":"sound-still","metadata":{"id":"sound-still"},"source":["## Utilities"]},{"cell_type":"code","execution_count":7,"id":"surprised-commercial","metadata":{"id":"surprised-commercial","executionInfo":{"status":"ok","timestamp":1649326558686,"user_tz":-540,"elapsed":10,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on binary arrays.\n","\n","    Args:\n","        preds (list of lists of ints): Predictions.\n","        truths (list of lists of ints): Ground truths.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    # Micro : aggregating over all instances\n","    preds = np.concatenate(preds)\n","    truths = np.concatenate(truths)\n","    return f1_score(truths, preds)\n","\n","\n","def spans_to_binary(spans, length=None):\n","    \"\"\"\n","    Converts spans to a binary array indicating whether each character is in the span.\n","\n","    Args:\n","        spans (list of lists of two ints): Spans.\n","\n","    Returns:\n","        np array [length]: Binarized spans.\n","    \"\"\"\n","    length = np.max(spans) if length is None else length\n","    binary = np.zeros(length)\n","    for start, end in spans:\n","        binary[start:end] = 1\n","    return binary\n","\n","\n","def span_micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on spans.\n","\n","    Args:\n","        preds (list of lists of two ints): Prediction spans.\n","        truths (list of lists of two ints): Ground truth spans.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    bin_preds = []\n","    bin_truths = []\n","    for pred, truth in zip(preds, truths):\n","        if not len(pred) and not len(truth):\n","            continue\n","        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n","        bin_preds.append(spans_to_binary(pred, length))\n","        bin_truths.append(spans_to_binary(truth, length))\n","    return micro_f1(bin_preds, bin_truths)\n","\n","\n","def get_score(y_true, y_pred):\n","    score = span_micro_f1(y_true, y_pred)\n","    return score"]},{"cell_type":"code","execution_count":8,"id":"interstate-accident","metadata":{"id":"interstate-accident","executionInfo":{"status":"ok","timestamp":1649326558686,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def create_labels_for_scoring(df):\n","    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n","    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n","    for i in range(len(df)):\n","        lst = df.loc[i, \"location\"]\n","        if lst:\n","            new_lst = \";\".join(lst)\n","            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n","\n","    # create labels\n","    truths = []\n","    for location_list in df[\"location_for_create_labels\"].values:\n","        truth = []\n","        if len(location_list) > 0:\n","            location = location_list[0]\n","            for loc in [s.split() for s in location.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                truth.append([start, end])\n","        truths.append(truth)\n","\n","    return truths\n","\n","\n","def get_char_probs(texts, token_probs, tokenizer):\n","    res = [np.zeros(len(t)) for t in texts]\n","    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n","        encoded = tokenizer(\n","            text=text,\n","            max_length=CFG.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n","            start, end = offset_mapping\n","            res[i][start:end] = pred\n","    return res\n","\n","\n","def get_predicted_location_str(char_probs, th=0.5):\n","    results = []\n","    for char_prob in char_probs:\n","        result = np.where(char_prob >= th)[0] + 1\n","        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n","        result = [f\"{min(r)} {max(r)}\" for r in result]\n","        result = \";\".join(result)\n","        results.append(result)\n","    return results\n","\n","\n","def get_predictions(results):\n","    predictions = []\n","    for result in results:\n","        prediction = []\n","        if result != \"\":\n","            for loc in [s.split() for s in result.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                prediction.append([start, end])\n","        predictions.append(prediction)\n","    return predictions\n","\n","\n","def scoring(df, th=0.5):\n","    labels = create_labels_for_scoring(df)\n","\n","    token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n","    char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n","    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n","    preds = get_predictions(predicted_location_str)\n","\n","    score = get_score(labels, preds)\n","    return score\n","\n","\n","def get_best_thres(oof_df):\n","    def f1_opt(x):\n","        return -1 * scoring(oof_df, th=x)\n","\n","    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n","    return best_thres"]},{"cell_type":"code","execution_count":9,"id":"coated-pioneer","metadata":{"id":"coated-pioneer","executionInfo":{"status":"ok","timestamp":1649326558687,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return \"%dm %ds\" % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":10,"id":"nervous-delaware","metadata":{"id":"nervous-delaware","executionInfo":{"status":"ok","timestamp":1649326558688,"user_tz":-540,"elapsed":10,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["seed_everything()"]},{"cell_type":"markdown","id":"functioning-destruction","metadata":{"id":"functioning-destruction"},"source":["## Data Loading"]},{"cell_type":"code","execution_count":11,"id":"global-monte","metadata":{"id":"global-monte","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649326559227,"user_tz":-540,"elapsed":549,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"194b6d7d-611f-4762-f57c-3e51dbecbd7b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14300, 6), (143, 3), (42146, 3), (5, 4))"]},"metadata":{},"execution_count":11}],"source":["train = pd.read_csv(CFG.input_dir / \"train.csv\")\n","features = pd.read_csv(CFG.input_dir / \"features.csv\")\n","patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n","test = pd.read_csv(CFG.input_dir / \"test.csv\")\n","\n","train.shape, features.shape, patient_notes.shape, test.shape"]},{"cell_type":"code","execution_count":12,"id":"independent-airfare","metadata":{"id":"independent-airfare","executionInfo":{"status":"ok","timestamp":1649326559227,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["if CFG.debug:\n","    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n","    print(train.shape)"]},{"cell_type":"markdown","id":"silent-locator","metadata":{"id":"silent-locator"},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":13,"id":"unusual-fifty","metadata":{"id":"unusual-fifty","executionInfo":{"status":"ok","timestamp":1649326559228,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def preprocess_features(features):\n","    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n","    return features\n","\n","\n","features = preprocess_features(features)"]},{"cell_type":"code","source":["features['feature_text'] = features['feature_text'].str.lower()\n","patient_notes['pn_history'] = patient_notes['pn_history'].str.lower()"],"metadata":{"id":"yKAC5UFJIOwQ","executionInfo":{"status":"ok","timestamp":1649326559228,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"id":"yKAC5UFJIOwQ","execution_count":14,"outputs":[]},{"cell_type":"code","source":["features['feature_text'] = features['feature_text'].str.lower()\n","patient_notes['pn_history'] = patient_notes['pn_history'].str.lower()"],"metadata":{"id":"gGzAPm7Img02","executionInfo":{"status":"ok","timestamp":1649326559229,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"id":"gGzAPm7Img02","execution_count":15,"outputs":[]},{"cell_type":"code","execution_count":16,"id":"decreased-mustang","metadata":{"id":"decreased-mustang","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649326559747,"user_tz":-540,"elapsed":524,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"03789165-fb2a-40e4-96d1-b9e3682215aa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14300, 8), (5, 6))"]},"metadata":{},"execution_count":16}],"source":["train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","\n","train.shape, test.shape"]},{"cell_type":"code","execution_count":17,"id":"boolean-trade","metadata":{"id":"boolean-trade","executionInfo":{"status":"ok","timestamp":1649326559748,"user_tz":-540,"elapsed":13,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n","train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"]},{"cell_type":"code","source":["# incorrect annotation\n","train.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n","train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n","\n","train.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n","train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n","\n","train.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n","train.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n","\n","train.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n","train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n","\n","train.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n","train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n","\n","train.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n","train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n","\n","train.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n","train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n","\n","train.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n","train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n","\n","train.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n","train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n","\n","train.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\n","train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n","\n","train.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n","train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n","\n","train.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\n","train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n","\n","train.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\n","train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n","\n","train.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n","train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n","\n","train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n","train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n","\n","train.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n","train.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n","\n","train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n","train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n","\n","train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n","train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n","\n","train.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n","train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n","\n","train.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n","train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n","\n","train.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\n","train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n","\n","train.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n","train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n","\n","train.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n","train.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n","\n","train.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n","train.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n","\n","train.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\n","train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n","\n","train.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n","train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n","\n","train.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n","train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n","\n","train.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n","train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n","\n","train.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n","train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n","\n","train.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n","train.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n","\n","train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n","train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n","\n","train.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n","train.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n","\n","train.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\n","train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n","\n","train.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n","train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n","\n","train.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\n","train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n","\n","train.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n","train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n","\n","train.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n","train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n","\n","train.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n","train.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n","\n","train.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n","train.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n","\n","train.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n","train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n","\n","train.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n","train.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n","\n","train.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\n","train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')"],"metadata":{"id":"FDleK3qdIbLT","executionInfo":{"status":"ok","timestamp":1649326559748,"user_tz":-540,"elapsed":12,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"id":"FDleK3qdIbLT","execution_count":18,"outputs":[]},{"cell_type":"code","execution_count":19,"id":"accomplished-dakota","metadata":{"id":"accomplished-dakota","colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"status":"ok","timestamp":1649326559749,"user_tz":-540,"elapsed":13,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"60d59bb8-624b-464a-a124-dd33938f5f52"},"outputs":[{"output_type":"display_data","data":{"text/plain":["0    4399\n","1    8185\n","2    1292\n","3     287\n","4      99\n","5      27\n","6       9\n","7       1\n","8       1\n","Name: annotation_length, dtype: int64"]},"metadata":{}}],"source":["train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n","display(train['annotation_length'].value_counts().sort_index())"]},{"cell_type":"markdown","id":"funded-elizabeth","metadata":{"id":"funded-elizabeth"},"source":["## CV split"]},{"cell_type":"code","execution_count":20,"id":"unexpected-columbia","metadata":{"id":"unexpected-columbia","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1649326559749,"user_tz":-540,"elapsed":11,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"a63eacca-ccf9-4de3-9b5f-558bba240bde"},"outputs":[{"output_type":"display_data","data":{"text/plain":["fold\n","0    3575\n","1    3575\n","2    3575\n","3    3575\n","dtype: int64"]},"metadata":{}}],"source":["Fold = GroupKFold(n_splits=CFG.n_fold)\n","groups = train['pn_num'].values\n","for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n","    train.loc[val_index, 'fold'] = int(n)\n","train['fold'] = train['fold'].astype(int)\n","display(train.groupby('fold').size())"]},{"cell_type":"markdown","id":"critical-archive","metadata":{"id":"critical-archive"},"source":["## Setup tokenizer"]},{"cell_type":"code","execution_count":21,"id":"broken-generator","metadata":{"id":"broken-generator","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649326564666,"user_tz":-540,"elapsed":4927,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"86ce0129-399a-4100-f6b3-390854d98036"},"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["if CFG.submission:\n","    tokenizer = DebertaV2TokenizerFast.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n","else:\n","    tokenizer = DebertaV2TokenizerFast.from_pretrained(CFG.pretrained_model_name)\n","    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n","\n","CFG.tokenizer = tokenizer"]},{"cell_type":"markdown","id":"compatible-lincoln","metadata":{"id":"compatible-lincoln"},"source":["## Create dataset"]},{"cell_type":"code","execution_count":22,"id":"fluid-nancy","metadata":{"id":"fluid-nancy","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["378da5e010f14bf6869c523abb981bf3","615f7da8d47b4ef89bc038318991956c","164d61b927a64380957f02f03769b6b2","17236a1945f447248da88aaedc35b744","a885ccb0411347d888b84f74cf841dcf","f9dc0bb2ee474f10ba1ca64bc2a3b2db","4059661ff71b48078b79e58be44bccba","003245542732434984fbbb6eaa1da2bb","8c03385d5fec428eaf97579017baa9c8","a4296a8a61494cb999da02449e79609d","5c4d7b1a6a734e46920bfc5549a52b04"]},"executionInfo":{"status":"ok","timestamp":1649326600243,"user_tz":-540,"elapsed":35586,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"98f21879-3e40-48a1-b987-0d904d68d6d2"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/42146 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"378da5e010f14bf6869c523abb981bf3"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 284\n"]}],"source":["pn_history_lengths = []\n","tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    pn_history_lengths.append(length)\n","\n","print(\"max length:\", np.max(pn_history_lengths))"]},{"cell_type":"code","execution_count":23,"id":"posted-humidity","metadata":{"id":"posted-humidity","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["a196430b3e814d0e99abab1fcfae6a8c","43e23125f7454c5a8fd87466ae9154a2","0b40d0c53e8e4664be0a7b18da35925e","fd74d4dddada4d96b5f05ebd48eac498","6ca0062f0fea4eceac7fbaf7cc8bb0c5","5a949132d5de490c97c8606a19025036","49e236c4bbeb4dfcb518990831057d30","8d136e0f56a44e70bd9bc08fff8ffadc","174bd5fbe1e14eb69315e6f9bc9dbc5f","1a6bd7761c3e43988a22cfd7daa10984","b468187903744a5999ea4170182b741c"]},"executionInfo":{"status":"ok","timestamp":1649326600245,"user_tz":-540,"elapsed":34,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"069ff57b-4382-44a4-d9b5-eff6caeed369"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/143 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a196430b3e814d0e99abab1fcfae6a8c"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 28\n"]}],"source":["feature_text_lengths = []\n","tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    feature_text_lengths.append(length)\n","\n","print(\"max length:\", np.max(feature_text_lengths))"]},{"cell_type":"code","execution_count":24,"id":"resistant-amount","metadata":{"id":"resistant-amount","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649326600246,"user_tz":-540,"elapsed":31,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"ec61c0f0-f44d-49a1-87c3-9fbe3b8de1d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["max length: 315\n"]}],"source":["CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n","\n","print(\"max length:\", CFG.max_len)"]},{"cell_type":"code","execution_count":25,"id":"august-equity","metadata":{"id":"august-equity","executionInfo":{"status":"ok","timestamp":1649326600247,"user_tz":-540,"elapsed":29,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["# ====================================================\n","# Dataset\n","# ====================================================\n","def prepare_input(cfg, text, feature_text):\n","    inputs = cfg.tokenizer(text, feature_text, \n","                           add_special_tokens=True,\n","                           max_length=cfg.max_len,\n","                           padding=\"max_length\",\n","                           return_offsets_mapping=False)\n","    for k, v in inputs.items():\n","        inputs[k] = torch.tensor(v, dtype=torch.long)\n","    return inputs\n","\n","\n","def create_label(cfg, text, annotation_length, location_list):\n","    encoded = cfg.tokenizer(text,\n","                            add_special_tokens=True,\n","                            max_length=cfg.max_len,\n","                            padding=\"max_length\",\n","                            return_offsets_mapping=True)\n","    offset_mapping = encoded['offset_mapping']\n","    ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n","    label = np.zeros(len(offset_mapping))\n","    weight = np.ones(len(offset_mapping))\n","    label[ignore_idxes] = -1\n","    if annotation_length != 0:\n","        for location in location_list:\n","            for loc in [s.split() for s in location.split(';')]:\n","                start_idx = -1\n","                end_idx = -1\n","                start, end = int(loc[0]), int(loc[1])\n","                for idx in range(len(offset_mapping)):\n","                    if (start_idx == -1) & (start < offset_mapping[idx][0]):\n","                        start_idx = idx - 1\n","                    if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n","                        end_idx = idx + 1\n","                if start_idx == -1:\n","                    start_idx = end_idx\n","                if (start_idx != -1) & (end_idx != -1):\n","                    label[start_idx:end_idx] = 1\n","                    if end - start > 1:\n","                        weight[start_idx:end_idx] = weight[start_idx:end_idx] + (end - start) * CFG.pos_length_weight_decay\n","    return label, weight\n","\n","\n","class TrainDataset(Dataset):\n","    def __init__(self, cfg, df, p_aug=0., p_mask_aug=0., mask_ratio=0.):\n","        self.cfg = cfg\n","        self.feature_texts = df['feature_text'].values\n","        self.pn_historys = df['pn_history'].values\n","        self.annotation_lengths = df['annotation_length'].values\n","        self.locations = df['location'].values\n","        self.p_aug = p_aug\n","        self.p_mask_aug = p_mask_aug\n","        self.mask_ratio = mask_ratio\n","\n","    def __len__(self):\n","        return len(self.feature_texts)\n","\n","    def augment_feature_text(self, feature_text):\n","        if feature_text.find('-or-') >= 0:\n","            augmented_feature_text = '-or-'.join(np.random.permutation(feature_text.split('-or-')))\n","        elif feature_text.find('-OR-') >= 0:\n","            augmented_feature_text = '-OR-'.join(np.random.permutation(feature_text.split('-OR-')))\n","        else:\n","            augmented_feature_text = feature_text\n","        return augmented_feature_text\n","\n","    def mask_augment(self, inputs):\n","        input_ids = inputs['input_ids']\n","        all_inds = np.arange(1, len(input_ids) - 1)\n","        n_mask = max(int(len(all_inds) * self.mask_ratio), 1)\n","        np.random.shuffle(all_inds)\n","        mask_inds = all_inds[:n_mask]\n","        sep_ind = np.where(np.array(input_ids) == 2)[0]\n","        mask_inds = np.array([i for i in mask_inds if i < sep_ind[0]])\n","        input_ids[mask_inds] = tokenizer.mask_token_id\n","        inputs[\"input_ids\"] = input_ids\n","        return inputs\n","\n","    def __getitem__(self, item):\n","        if float(torch.rand(1)) < self.p_aug:\n","            feature_text = self.augment_feature_text(self.feature_texts[item])\n","        else:\n","            feature_text = self.feature_texts[item]\n","        inputs = prepare_input(self.cfg, \n","                               self.pn_historys[item], \n","                               feature_text)\n","        if float(torch.rand(1)) < self.p_mask_aug:\n","            inputs = self.mask_augment(inputs)\n","        label, weight = create_label(self.cfg,\n","                             self.pn_historys[item], \n","                             self.annotation_lengths[item], \n","                             self.locations[item])\n","\n","        return inputs, torch.tensor(label, dtype=torch.float), torch.tensor(weight, dtype=torch.float)"]},{"cell_type":"code","execution_count":26,"id":"weird-interaction","metadata":{"id":"weird-interaction","executionInfo":{"status":"ok","timestamp":1649326600248,"user_tz":-540,"elapsed":29,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["class TestDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        return input_"]},{"cell_type":"markdown","id":"upper-mobility","metadata":{"id":"upper-mobility"},"source":["## Model"]},{"cell_type":"code","source":["from transformers.modeling_outputs import MaskedLMOutput\n","\n","class MaskedModel(nn.Module):\n","    def __init__(self, cfg, config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","        if config_path is None:\n","            self.config = AutoConfig.from_pretrained(\n","                cfg.pretrained_model_name,\n","                output_hidden_states=False\n","                )\n","        else:\n","            self.config = torch.load(config_path)\n","        \n","        if pretrained:\n","            self.model = AutoModel.from_pretrained(cfg.pretrained_model_name, config=self.config)\n","            self.lm_head = AutoModelForMaskedLM.from_pretrained(cfg.pretrained_model_name, config=self.config).cls # [cls, lm_head]\n","        else:\n","            self.model = AutoModel(self.config)\n","            self.lm_head = AutoModelForMaskedLM(self.config).cls # [cls, lm_head]\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def forward(\n","            self, \n","            input_ids=None,\n","            attention_mask=None,\n","            token_type_ids=None,\n","            #position_ids=None,\n","            inputs_embeds=None,\n","            labels=None,\n","            output_attentions=None,\n","            output_hidden_states=None,\n","            return_dict=None):\n","        \n","        outputs = self.model(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            #position_ids=position_ids,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,)\n","        \n","        sequence_output = outputs[0]\n","        prediction_scores = self.lm_head(sequence_output)\n","\n","        masked_lm_loss = None\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss()\n","            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n","\n","        return MaskedLMOutput(loss=masked_lm_loss,\n","                              logits=prediction_scores,\n","                              hidden_states=outputs.hidden_states,\n","                              attentions=outputs.attentions)"],"metadata":{"id":"a4HSgs6b8wQT","executionInfo":{"status":"ok","timestamp":1649326600250,"user_tz":-540,"elapsed":31,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"id":"a4HSgs6b8wQT","execution_count":27,"outputs":[]},{"cell_type":"code","execution_count":28,"id":"spanish-destruction","metadata":{"id":"spanish-destruction","executionInfo":{"status":"ok","timestamp":1649326600251,"user_tz":-540,"elapsed":32,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["class CustomModel(nn.Module):\n","    def __init__(self, cfg, model_config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        if model_config_path is None:\n","            self.model_config = AutoConfig.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                output_hidden_states=True,\n","            )\n","        else:\n","            self.model_config = torch.load(model_config_path)\n","\n","        if pretrained:\n","            self.backbone = AutoModel.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                config=self.model_config,\n","            )\n","            print(f\"Load weight from pretrained\")\n","        else:\n","            #self.backbone = AutoModel.from_config(self.model_config)\n","            # itpt = AutoModelForMaskedLM.from_config(self.model_config)\n","            #path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp010/checkpoint-130170/pytorch_model.bin\")\n","            # path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n","            # state_dict = torch.load(path)\n","            # itpt.load_state_dict(state_dict)\n","            path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-10.bin\")\n","            masked_model = MaskedModel(CFG, config_path=None, pretrained=True)\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            masked_model.load_state_dict(state)\n","            self.backbone = masked_model.model\n","            print(f\"Load weight from {path}\")\n","\n","        self.fc = nn.Sequential(\n","            nn.Dropout(self.cfg.fc_dropout),\n","            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n","        )\n","        self._init_weights(self.fc)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def forward(self, inputs):\n","        h = self.backbone(**inputs)[\"last_hidden_state\"]\n","        output = self.fc(h)\n","        return output"]},{"cell_type":"markdown","id":"chronic-bullet","metadata":{"id":"chronic-bullet"},"source":["## Training"]},{"cell_type":"code","source":["class FocalLoss(nn.Module):\n","    def __init__(self, reduction='none', alpha=1, gamma=2):\n","        super().__init__()\n","        self.reduction = reduction\n","        self.alpha = alpha\n","        self.gamma = gamma\n","\n","    def forward(self, inputs, targets):\n","        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n","        pt = torch.exp(-bce_loss)\n","        loss = self.alpha * (1. - pt)**self.gamma * bce_loss\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss\n","\n","\n","class SmoothFocalLoss(nn.Module):\n","    def __init__(self, reduction='none', alpha=1, gamma=2, smoothing=0.0):\n","        super().__init__()\n","        self.reduction = reduction\n","        self.focal_loss = FocalLoss(reduction='none', alpha=alpha, gamma=gamma)\n","        self.smoothing = smoothing\n","\n","    @staticmethod\n","    def _smooth(targets:torch.Tensor, smoothing=0.0):\n","        assert 0 <= smoothing < 1\n","        with torch.no_grad():\n","            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n","        return targets\n","\n","    def forward(self, inputs, targets):\n","        targets = SmoothFocalLoss._smooth(targets, self.smoothing)\n","        loss = self.focal_loss(inputs, targets)\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss\n","\n","    \n","class CEFocalLoss(nn.Module):\n","    def __init__(self, reduction='none', alpha=1, gamma=2):\n","        super(CEFocalLoss, self).__init__()\n","        self.reduction = reduction\n","        self.alpha = alpha\n","        self.gamma = gamma\n","\n","    def forward(self, inputs, targets):\n","        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n","        pt = torch.exp(-ce_loss)\n","        loss = self.alpha * (1. - pt)**self.gamma * ce_loss\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss\n","\n","    \n","class SmoothCEFocalLoss(nn.Module):\n","    def __init__(self, reduction='none', alpha=1, gamma=2, smoothing=0.0):\n","        super(SmoothCEFocalLoss, self).__init__()\n","        self.reduction = reduction\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.smoothing = smoothing\n","\n","    def forward(self, inputs, targets):\n","        ce_loss = F.cross_entropy(inputs, targets, reduction='none', label_smoothing=self.smoothing) # torch >= 1.10.0\n","        pt = torch.exp(-ce_loss)\n","        loss = self.alpha * (1. - pt)**self.gamma * ce_loss\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss"],"metadata":{"id":"jVXgmwt9nJll","executionInfo":{"status":"ok","timestamp":1649326600925,"user_tz":-540,"elapsed":705,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"id":"jVXgmwt9nJll","execution_count":29,"outputs":[]},{"cell_type":"code","execution_count":30,"id":"biological-hunger","metadata":{"id":"biological-hunger","executionInfo":{"status":"ok","timestamp":1649326600925,"user_tz":-540,"elapsed":12,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def train_fn(\n","    train_dataloader,\n","    model,\n","    criterion,\n","    optimizer,\n","    epoch,\n","    scheduler,\n","    device,\n","):\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels, weights) in enumerate(train_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        weights = weights.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            output = model(inputs)\n","\n","        loss_mask = labels.view(-1, 1) != -1\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = (loss.view(batch_size, -1) * weights.view(batch_size, -1)).view(-1, 1)[loss_mask]\n","        loss = loss.mean()\n","\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","\n","        if CFG.batch_scheduler:\n","            scheduler.step()\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n","            print(\n","                \"Epoch: [{0}][{1}/{2}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                \"Grad: {grad_norm:.4f}  \"\n","                \"LR: {lr:.6f}  \"\n","                .format(\n","                    epoch+1,\n","                    step,\n","                    len(train_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n","                    loss=losses,\n","                     grad_norm=grad_norm,\n","                     lr=scheduler.get_lr()[0],\n","                )\n","            )\n","    return losses.avg"]},{"cell_type":"code","execution_count":31,"id":"satisfied-sterling","metadata":{"id":"satisfied-sterling","executionInfo":{"status":"ok","timestamp":1649326600926,"user_tz":-540,"elapsed":13,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def valid_fn(\n","    val_dataloader,\n","    model,\n","    criterion,\n","    device,\n","):\n","    model.eval()\n","    preds = []\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels, weights) in enumerate(val_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        weights = weights.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            output = model(inputs)\n","\n","        loss_mask = labels.view(-1, 1) != -1\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = (loss.view(batch_size, -1) * weights.view(batch_size, -1)).view(-1, 1)[loss_mask]\n","        loss = loss.mean()\n","\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n","            print(\n","                \"EVAL: [{0}/{1}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                .format(\n","                    step, len(val_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n","                    loss=losses,\n","                )\n","            )\n","    preds = np.concatenate(preds)\n","    return losses.avg, preds"]},{"cell_type":"code","execution_count":32,"id":"incorporate-viking","metadata":{"id":"incorporate-viking","executionInfo":{"status":"ok","timestamp":1649326600927,"user_tz":-540,"elapsed":13,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def inference_fn(test_dataloader, model, device):\n","    model.eval()\n","    model.to(device)\n","    preds = []\n","    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n","    for inputs in tk0:\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        with torch.no_grad():\n","            output = model(inputs)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","    preds = np.concatenate(preds)\n","    return preds"]},{"cell_type":"code","execution_count":33,"id":"dental-sunset","metadata":{"id":"dental-sunset","executionInfo":{"status":"ok","timestamp":1649326600928,"user_tz":-540,"elapsed":14,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def train_loop(df, i_fold, device):\n","    print(f\"========== fold: {i_fold} training ==========\")\n","    train_idx = df[df[\"fold\"] != i_fold].index\n","    val_idx = df[df[\"fold\"] == i_fold].index\n","\n","    train_folds = df.loc[train_idx].reset_index(drop=True)\n","    val_folds = df.loc[val_idx].reset_index(drop=True)\n","\n","    train_dataset = TrainDataset(CFG, train_folds, p_aug=CFG.p_aug, p_mask_aug=CFG.p_mask_aug, mask_ratio=CFG.mask_ratio)\n","    val_dataset = TrainDataset(CFG, val_folds, p_aug=0., p_mask_aug=0., mask_ratio=0.)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=True,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=True,\n","    )\n","    val_dataloader = DataLoader(\n","        val_dataset,\n","        batch_size=CFG.test_batch_size,\n","        shuffle=False,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=False,\n","    )\n","\n","    # model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","    model = CustomModel(CFG, model_config_path=None, pretrained=False)   # itptを使うため\n","    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n","    model.to(device)\n","\n","    def get_optimizer_params(model, encoder_lr, decoder_lr, group_step=1, lr_scale=1.0, weight_decay=0.0):\n","        num_hidden_layers = model.model_config.num_hidden_layers\n","        param_optimizer = list(model.named_parameters())\n","        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","        groups = np.array([f'layer.{i}.' for i in range(num_hidden_layers)]).reshape(-1, group_step).tolist()\n","        optimizer_parameters = []\n","        for i, group in enumerate(groups):\n","            lr_factor = (num_hidden_layers - (i * lr_scale)) / num_hidden_layers\n","            optimizer_parameters.append(\n","                {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group)], 'weight_decay': weight_decay,\n","                 'lr': encoder_lr * lr_factor}\n","            )\n","            optimizer_parameters.append(\n","                {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group)], 'weight_decay': 0.0,\n","                 'lr': encoder_lr * lr_factor}\n","            )\n","        optimizer_parameters.append(\n","            {'params': [p for n, p in model.named_parameters() if \"backbone\" not in n], 'weight_decay': 0.0,\n","             'lr': decoder_lr, \"momentum\" : 0.99}\n","        )\n","        return optimizer_parameters\n","\n","    optimizer_parameters = get_optimizer_params(model,\n","                                                encoder_lr=CFG.encoder_lr, \n","                                                decoder_lr=CFG.decoder_lr,\n","                                                group_step=CFG.group_step,\n","                                                lr_scale=CFG.lr_scale,\n","                                                weight_decay=CFG.weight_decay)\n","    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n","\n","    num_train_steps = int(len(train_dataloader) / CFG.batch_size * CFG.epochs)\n","    scheduler = get_cosine_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=CFG.num_warmup_steps,\n","        num_training_steps=num_train_steps,\n","        num_cycles=CFG.num_cycles,\n","    )\n","\n","    criterion = SmoothFocalLoss(reduction='none', alpha=CFG.alpha, gamma=CFG.gamma, smoothing=CFG.smoothing)\n","    #criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    best_score = -1 * np.inf\n","\n","    for epoch in range(CFG.epochs):\n","        start_time = time.time()\n","\n","        if CFG.p_aug_epoch < epoch + 1:\n","            train_dataset = TrainDataset(CFG, train_folds, p_aug=0., p_mask_aug=0., mask_ratio=0.)\n","            train_dataloader = DataLoader(\n","                train_dataset,\n","                batch_size=CFG.batch_size,\n","                shuffle=True,\n","                num_workers=CFG.num_workers,\n","                pin_memory=True,\n","                drop_last=True,\n","            )\n","\n","        avg_loss = train_fn(\n","            train_dataloader,\n","            model,\n","            criterion,\n","            optimizer,\n","            epoch,\n","            scheduler,\n","            device,\n","        )\n","        avg_val_loss, val_preds = valid_fn(\n","            val_dataloader,\n","            model,\n","            criterion,\n","            device,\n","        )\n","\n","        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n","            scheduler.step()\n","\n","        # scoring\n","        val_folds[[str(i) for i in range(CFG.max_len)]] = val_preds\n","        score = scoring(val_folds, th=0.5)\n","\n","        elapsed = time.time() - start_time\n","\n","        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n","        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n","        if score > best_score:\n","            best_score = score\n","            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n","            torch.save({\n","                \"model\": model.state_dict(),\n","                \"predictions\": val_preds,\n","                },\n","                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","            )\n","\n","    predictions = torch.load(\n","        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","        map_location=torch.device(\"cpu\"),\n","    )[\"predictions\"]\n","    val_folds[[str(i) for i in range(CFG.max_len)]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return val_folds"]},{"cell_type":"markdown","id":"brazilian-graphics","metadata":{"id":"brazilian-graphics"},"source":["## Main"]},{"cell_type":"code","execution_count":34,"id":"connected-protein","metadata":{"id":"connected-protein","executionInfo":{"status":"ok","timestamp":1649326600928,"user_tz":-540,"elapsed":13,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def main():\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    if CFG.train:\n","        oof_df = pd.DataFrame()\n","        for i_fold in range(CFG.n_fold):\n","            if i_fold in CFG.train_fold:\n","                _oof_df = train_loop(train, i_fold, device)\n","                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n","        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    if CFG.submission:\n","        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n","    else:\n","        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    score = scoring(oof_df, th=0.5)\n","    print(f\"Best thres: 0.5, Score: {score:.4f}\")\n","    best_thres = get_best_thres(oof_df)\n","    score = scoring(oof_df, th=best_thres)\n","    print(f\"Best thres: {best_thres}, Score: {score:.4f}\")\n","\n","    if CFG.inference:\n","        test_dataset = TestDataset(CFG, test)\n","        test_dataloader = DataLoader(\n","            test_dataset,\n","            batch_size=CFG.batch_size,\n","            shuffle=False,\n","            num_workers=CFG.num_workers,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","        predictions = []\n","        for i_fold in CFG.train_fold:\n","            if CFG.submission:\n","                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n","                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n","            else:\n","                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n","\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            model.load_state_dict(state[\"model\"])\n","            test_token_probs = inference_fn(test_dataloader, model, device)\n","            test[[f\"fold{i_fold}_{i}\" for i in range(CFG.max_len)]] = test_token_probs\n","            test_char_probs = get_char_probs(test[\"pn_history\"].values, test_token_probs, CFG.tokenizer)\n","            predictions.append(test_char_probs)\n","\n","            del state, test_token_probs, model; gc.collect()\n","            torch.cuda.empty_cache()\n","\n","        predictions = np.mean(predictions, axis=0)\n","        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n","        test[CFG.target_col] = predicted_location_str\n","        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n","        test[[CFG.id_col, CFG.target_col]].to_csv(\n","            CFG.output_dir / \"submission.csv\", index=False\n","        )"]},{"cell_type":"code","execution_count":35,"id":"serious-bunny","metadata":{"id":"serious-bunny","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["72c9b77bde1f468ca443805a7c8ea3a8","423b5a5d4f4f40629fd53560de4b9805","df71e8a2c9ad488aa9c5821f20500a90","53257d26a0b44d9a9339dba3f989e5bd","f59e452801d546e9a75ff2ab5c38c9bb","8529d1203ccc4f6cb8311e67f7ea3dca","0f605c21ed5f4abb8849ea59aeefba00","d5c1747fc8fb4cdc8877f7a123671c8a","fda2d7a7fdd449429a3c53ea2efcc55f","6522a937786c4189b13cec4949bf557d","874932e818e64bdcaede97854d32868d","dee94ec8145541c2ad74d9c9f97dd8a7","692eecf29b0549e9a43de8eb43044858","d373dbb712d34255a8d6fea0d584a0f0","91b20518fe6c4d8881dd10f64cf4c71c","96e70e70a15b4763954935de367473c3","edd00ddc2bac4f1f8f240e87ed4f7105","ef987b3f29e64562b0c9e1dd20cea0fd","9f2f0d5938924082ab15e0d428a09794","81e74ebae5e04f2bb2d66e95792506bc","6fd77c560c1b4529b3109a11521f93ce","3ec203f8aa0c4c3a81d5ad88f48370f8","5303600af42f4fa29d5ef8f34c1e1716","08be1a5cab18475191923886eef94105","4800d4887a3846a9b0077f337ad233f9","1376cf0d23ea42cdb93dfc002bf40829","d7895c66d23b4094bae8624b7b5a2d34","58e4593a23004f53acf61c2b0b3f56ca","855ff300af7543d59f68fc4402774cd5","24b44a09a5c04b48a7947efa869e006e","0149baa53279452fac4498174bc67b49","4d9bfe291cba47939816d51cb6f27f29","4e19ab09ce584253af210c2faadb758c","c9347608935e4c70a7fad0068c044b8b","4b69620ac66d4a2591dcf59c6973af4d","c7959a82499a48dc89ea3aabbe291f1d","504440fe56be486cbcd29a09e0878c05","87fb535eb3f1497b8e1a245d0edabda4","058a80ee4333420cbd2cb585c50199ac","8e21fe2e19f04eac9a7696eb407ef13a","233ac86a7fe84505a4636ff6bb00063b","db420071b6e644ebab25c572c85426bd","7e2f7cea6a7948fa8f7923b84cd64a24","9763d8d2f2ac40a398cef57e4fb74ffd"]},"executionInfo":{"status":"ok","timestamp":1649343583908,"user_tz":-540,"elapsed":16982993,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"cc6627f5-7d83-42a3-88e1-d8d0ed50ca5e"},"outputs":[{"output_type":"stream","name":"stdout","text":["========== fold: 0 training ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-10.bin\n","Epoch: [1][0/3575] Elapsed 0m 0s (remain 51m 8s) Loss: 0.2863(0.2863) Grad: inf  LR: 0.000020  \n","Epoch: [1][100/3575] Elapsed 0m 24s (remain 13m 53s) Loss: 0.0176(0.0463) Grad: 3001.7090  LR: 0.000020  \n","Epoch: [1][200/3575] Elapsed 0m 47s (remain 13m 21s) Loss: 0.0100(0.0320) Grad: 2530.4795  LR: 0.000020  \n","Epoch: [1][300/3575] Elapsed 1m 11s (remain 12m 52s) Loss: 0.0011(0.0256) Grad: 671.1666  LR: 0.000020  \n","Epoch: [1][400/3575] Elapsed 1m 34s (remain 12m 31s) Loss: 0.0031(0.0214) Grad: 1935.3771  LR: 0.000020  \n","Epoch: [1][500/3575] Elapsed 1m 58s (remain 12m 8s) Loss: 0.0110(0.0189) Grad: 1764.0732  LR: 0.000019  \n","Epoch: [1][600/3575] Elapsed 2m 22s (remain 11m 43s) Loss: 0.0014(0.0171) Grad: 1507.5759  LR: 0.000019  \n","Epoch: [1][700/3575] Elapsed 2m 45s (remain 11m 18s) Loss: 0.0125(0.0160) Grad: 3304.8464  LR: 0.000019  \n","Epoch: [1][800/3575] Elapsed 3m 8s (remain 10m 54s) Loss: 0.0062(0.0149) Grad: 2418.6433  LR: 0.000019  \n","Epoch: [1][900/3575] Elapsed 3m 32s (remain 10m 29s) Loss: 0.0069(0.0140) Grad: 2248.1746  LR: 0.000018  \n","Epoch: [1][1000/3575] Elapsed 3m 55s (remain 10m 6s) Loss: 0.0041(0.0134) Grad: 2212.2200  LR: 0.000018  \n","Epoch: [1][1100/3575] Elapsed 4m 19s (remain 9m 42s) Loss: 0.0002(0.0126) Grad: 181.6646  LR: 0.000017  \n","Epoch: [1][1200/3575] Elapsed 4m 42s (remain 9m 18s) Loss: 0.0107(0.0121) Grad: 3018.9402  LR: 0.000017  \n","Epoch: [1][1300/3575] Elapsed 5m 5s (remain 8m 54s) Loss: 0.0014(0.0116) Grad: 806.2330  LR: 0.000017  \n","Epoch: [1][1400/3575] Elapsed 5m 29s (remain 8m 31s) Loss: 0.0033(0.0112) Grad: 1195.5992  LR: 0.000016  \n","Epoch: [1][1500/3575] Elapsed 5m 52s (remain 8m 7s) Loss: 0.0036(0.0109) Grad: 1356.4698  LR: 0.000015  \n","Epoch: [1][1600/3575] Elapsed 6m 16s (remain 7m 43s) Loss: 0.0086(0.0105) Grad: 2474.8191  LR: 0.000015  \n","Epoch: [1][1700/3575] Elapsed 6m 39s (remain 7m 20s) Loss: 0.0377(0.0101) Grad: 3106.4758  LR: 0.000014  \n","Epoch: [1][1800/3575] Elapsed 7m 3s (remain 6m 56s) Loss: 0.0046(0.0099) Grad: 1616.8987  LR: 0.000014  \n","Epoch: [1][1900/3575] Elapsed 7m 26s (remain 6m 33s) Loss: 0.0069(0.0096) Grad: 2279.6118  LR: 0.000013  \n","Epoch: [1][2000/3575] Elapsed 7m 49s (remain 6m 9s) Loss: 0.0003(0.0094) Grad: 470.9549  LR: 0.000012  \n","Epoch: [1][2100/3575] Elapsed 8m 13s (remain 5m 46s) Loss: 0.0001(0.0092) Grad: 395.6732  LR: 0.000012  \n","Epoch: [1][2200/3575] Elapsed 8m 36s (remain 5m 22s) Loss: 0.0005(0.0090) Grad: 567.4004  LR: 0.000011  \n","Epoch: [1][2300/3575] Elapsed 9m 0s (remain 4m 59s) Loss: 0.0014(0.0089) Grad: 2381.6248  LR: 0.000011  \n","Epoch: [1][2400/3575] Elapsed 9m 23s (remain 4m 35s) Loss: 0.0001(0.0087) Grad: 300.1072  LR: 0.000010  \n","Epoch: [1][2500/3575] Elapsed 9m 46s (remain 4m 11s) Loss: 0.0012(0.0085) Grad: 5301.7168  LR: 0.000009  \n","Epoch: [1][2600/3575] Elapsed 10m 10s (remain 3m 48s) Loss: 0.0016(0.0084) Grad: 1804.3188  LR: 0.000009  \n","Epoch: [1][2700/3575] Elapsed 10m 39s (remain 3m 26s) Loss: 0.0002(0.0082) Grad: 399.9435  LR: 0.000008  \n","Epoch: [1][2800/3575] Elapsed 11m 3s (remain 3m 3s) Loss: 0.0153(0.0081) Grad: 8904.4902  LR: 0.000007  \n","Epoch: [1][2900/3575] Elapsed 11m 26s (remain 2m 39s) Loss: 0.0022(0.0079) Grad: 2257.4551  LR: 0.000007  \n","Epoch: [1][3000/3575] Elapsed 11m 50s (remain 2m 15s) Loss: 0.0001(0.0078) Grad: 211.6356  LR: 0.000006  \n","Epoch: [1][3100/3575] Elapsed 12m 14s (remain 1m 52s) Loss: 0.0000(0.0077) Grad: 225.8194  LR: 0.000005  \n","Epoch: [1][3200/3575] Elapsed 12m 37s (remain 1m 28s) Loss: 0.0005(0.0076) Grad: 623.1307  LR: 0.000005  \n","Epoch: [1][3300/3575] Elapsed 13m 1s (remain 1m 4s) Loss: 0.0047(0.0075) Grad: 3273.9404  LR: 0.000004  \n","Epoch: [1][3400/3575] Elapsed 13m 24s (remain 0m 41s) Loss: 0.0010(0.0074) Grad: 1294.6011  LR: 0.000004  \n","Epoch: [1][3500/3575] Elapsed 13m 47s (remain 0m 17s) Loss: 0.0001(0.0073) Grad: 279.8866  LR: 0.000003  \n","Epoch: [1][3574/3575] Elapsed 14m 5s (remain 0m 0s) Loss: 0.0000(0.0072) Grad: 188.9914  LR: 0.000003  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 10m 18s) Loss: 0.0004(0.0004) \n","EVAL: [100/1192] Elapsed 0m 8s (remain 1m 33s) Loss: 0.0021(0.0023) \n","EVAL: [200/1192] Elapsed 0m 16s (remain 1m 21s) Loss: 0.0016(0.0032) \n","EVAL: [300/1192] Elapsed 0m 24s (remain 1m 12s) Loss: 0.0044(0.0034) \n","EVAL: [400/1192] Elapsed 0m 32s (remain 1m 4s) Loss: 0.0025(0.0034) \n","EVAL: [500/1192] Elapsed 0m 40s (remain 0m 55s) Loss: 0.0046(0.0032) \n","EVAL: [600/1192] Elapsed 0m 48s (remain 0m 47s) Loss: 0.0010(0.0034) \n","EVAL: [700/1192] Elapsed 0m 56s (remain 0m 39s) Loss: 0.0618(0.0042) \n","EVAL: [800/1192] Elapsed 1m 4s (remain 0m 31s) Loss: 0.0026(0.0043) \n","EVAL: [900/1192] Elapsed 1m 12s (remain 0m 23s) Loss: 0.0010(0.0043) \n","EVAL: [1000/1192] Elapsed 1m 20s (remain 0m 15s) Loss: 0.0000(0.0042) \n","EVAL: [1100/1192] Elapsed 1m 28s (remain 0m 7s) Loss: 0.0016(0.0040) \n","EVAL: [1191/1192] Elapsed 1m 36s (remain 0m 0s) Loss: 0.0000(0.0039) \n","Epoch 1 - avg_train_loss: 0.0072  avg_val_loss: 0.0039  time: 947s\n","Epoch 1 - Score: 0.8741\n","Epoch 1 - Save Best Score: 0.8741 Model\n","Epoch: [2][0/3575] Elapsed 0m 0s (remain 41m 57s) Loss: 0.0019(0.0019) Grad: 6959.2900  LR: 0.000003  \n","Epoch: [2][100/3575] Elapsed 0m 29s (remain 17m 3s) Loss: 0.0015(0.0031) Grad: 4992.5425  LR: 0.000002  \n","Epoch: [2][200/3575] Elapsed 0m 54s (remain 15m 15s) Loss: 0.0000(0.0033) Grad: 247.6868  LR: 0.000002  \n","Epoch: [2][300/3575] Elapsed 1m 17s (remain 14m 5s) Loss: 0.0041(0.0034) Grad: 23174.6367  LR: 0.000002  \n","Epoch: [2][400/3575] Elapsed 1m 41s (remain 13m 25s) Loss: 0.0003(0.0034) Grad: 1827.2633  LR: 0.000001  \n","Epoch: [2][500/3575] Elapsed 2m 5s (remain 12m 52s) Loss: 0.0005(0.0032) Grad: 3229.5615  LR: 0.000001  \n","Epoch: [2][600/3575] Elapsed 2m 29s (remain 12m 21s) Loss: 0.0000(0.0030) Grad: 288.1454  LR: 0.000001  \n","Epoch: [2][700/3575] Elapsed 2m 53s (remain 11m 51s) Loss: 0.0001(0.0030) Grad: 502.0890  LR: 0.000001  \n","Epoch: [2][800/3575] Elapsed 3m 16s (remain 11m 21s) Loss: 0.0041(0.0029) Grad: 11366.7656  LR: 0.000000  \n","Epoch: [2][900/3575] Elapsed 3m 40s (remain 10m 53s) Loss: 0.0000(0.0028) Grad: 234.5254  LR: 0.000000  \n","Epoch: [2][1000/3575] Elapsed 4m 3s (remain 10m 25s) Loss: 0.0072(0.0029) Grad: 26547.4805  LR: 0.000000  \n","Epoch: [2][1100/3575] Elapsed 4m 26s (remain 9m 59s) Loss: 0.0000(0.0030) Grad: 201.8094  LR: 0.000000  \n","Epoch: [2][1200/3575] Elapsed 4m 50s (remain 9m 33s) Loss: 0.0066(0.0030) Grad: 15695.1729  LR: 0.000000  \n","Epoch: [2][1300/3575] Elapsed 5m 13s (remain 9m 7s) Loss: 0.0000(0.0030) Grad: 225.0244  LR: 0.000000  \n","Epoch: [2][1400/3575] Elapsed 5m 36s (remain 8m 42s) Loss: 0.0001(0.0030) Grad: 617.6448  LR: 0.000000  \n","Epoch: [2][1500/3575] Elapsed 5m 59s (remain 8m 17s) Loss: 0.0016(0.0030) Grad: 3934.0520  LR: 0.000000  \n","Epoch: [2][1600/3575] Elapsed 6m 23s (remain 7m 52s) Loss: 0.0114(0.0030) Grad: 38992.6641  LR: 0.000000  \n","Epoch: [2][1700/3575] Elapsed 6m 46s (remain 7m 28s) Loss: 0.0007(0.0030) Grad: 5618.2393  LR: 0.000001  \n","Epoch: [2][1800/3575] Elapsed 7m 9s (remain 7m 3s) Loss: 0.0000(0.0031) Grad: 259.6689  LR: 0.000001  \n","Epoch: [2][1900/3575] Elapsed 7m 33s (remain 6m 39s) Loss: 0.0006(0.0032) Grad: 5004.9131  LR: 0.000001  \n","Epoch: [2][2000/3575] Elapsed 7m 56s (remain 6m 14s) Loss: 0.0002(0.0032) Grad: 13473.7412  LR: 0.000001  \n","Epoch: [2][2100/3575] Elapsed 8m 19s (remain 5m 50s) Loss: 0.0044(0.0032) Grad: 27796.6816  LR: 0.000002  \n","Epoch: [2][2200/3575] Elapsed 8m 42s (remain 5m 26s) Loss: 0.0002(0.0032) Grad: 1950.0295  LR: 0.000002  \n","Epoch: [2][2300/3575] Elapsed 9m 6s (remain 5m 2s) Loss: 0.0069(0.0032) Grad: 34127.5820  LR: 0.000003  \n","Epoch: [2][2400/3575] Elapsed 9m 29s (remain 4m 38s) Loss: 0.0010(0.0032) Grad: 10349.0068  LR: 0.000003  \n","Epoch: [2][2500/3575] Elapsed 9m 52s (remain 4m 14s) Loss: 0.0207(0.0032) Grad: 69202.8750  LR: 0.000004  \n","Epoch: [2][2600/3575] Elapsed 10m 15s (remain 3m 50s) Loss: 0.0001(0.0032) Grad: 2033.0447  LR: 0.000004  \n","Epoch: [2][2700/3575] Elapsed 10m 39s (remain 3m 26s) Loss: 0.0223(0.0032) Grad: 40208.5039  LR: 0.000005  \n","Epoch: [2][2800/3575] Elapsed 11m 2s (remain 3m 3s) Loss: 0.0002(0.0032) Grad: 2129.7283  LR: 0.000005  \n","Epoch: [2][2900/3575] Elapsed 11m 25s (remain 2m 39s) Loss: 0.0000(0.0032) Grad: 201.2070  LR: 0.000006  \n","Epoch: [2][3000/3575] Elapsed 11m 49s (remain 2m 15s) Loss: 0.0000(0.0032) Grad: 175.5854  LR: 0.000006  \n","Epoch: [2][3100/3575] Elapsed 12m 12s (remain 1m 51s) Loss: 0.0000(0.0032) Grad: 537.0071  LR: 0.000007  \n","Epoch: [2][3200/3575] Elapsed 12m 35s (remain 1m 28s) Loss: 0.0058(0.0032) Grad: 85215.8594  LR: 0.000008  \n","Epoch: [2][3300/3575] Elapsed 12m 58s (remain 1m 4s) Loss: 0.0037(0.0032) Grad: 27007.2207  LR: 0.000008  \n","Epoch: [2][3400/3575] Elapsed 13m 22s (remain 0m 41s) Loss: 0.0004(0.0033) Grad: 6071.1943  LR: 0.000009  \n","Epoch: [2][3500/3575] Elapsed 13m 45s (remain 0m 17s) Loss: 0.0001(0.0032) Grad: 2127.0317  LR: 0.000010  \n","Epoch: [2][3574/3575] Elapsed 14m 2s (remain 0m 0s) Loss: 0.0005(0.0032) Grad: 7663.2422  LR: 0.000010  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 43s) Loss: 0.0001(0.0001) \n","EVAL: [100/1192] Elapsed 0m 8s (remain 1m 30s) Loss: 0.0047(0.0032) \n","EVAL: [200/1192] Elapsed 0m 16s (remain 1m 20s) Loss: 0.0018(0.0042) \n","EVAL: [300/1192] Elapsed 0m 24s (remain 1m 11s) Loss: 0.0041(0.0044) \n","EVAL: [400/1192] Elapsed 0m 32s (remain 1m 3s) Loss: 0.0039(0.0045) \n","EVAL: [500/1192] Elapsed 0m 40s (remain 0m 55s) Loss: 0.0040(0.0042) \n","EVAL: [600/1192] Elapsed 0m 47s (remain 0m 47s) Loss: 0.0006(0.0045) \n","EVAL: [700/1192] Elapsed 0m 55s (remain 0m 39s) Loss: 0.0849(0.0053) \n","EVAL: [800/1192] Elapsed 1m 3s (remain 0m 31s) Loss: 0.0014(0.0055) \n","EVAL: [900/1192] Elapsed 1m 11s (remain 0m 23s) Loss: 0.0004(0.0055) \n","EVAL: [1000/1192] Elapsed 1m 19s (remain 0m 15s) Loss: 0.0000(0.0055) \n","EVAL: [1100/1192] Elapsed 1m 27s (remain 0m 7s) Loss: 0.0021(0.0052) \n","EVAL: [1191/1192] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0000(0.0050) \n","Epoch 2 - avg_train_loss: 0.0032  avg_val_loss: 0.0050  time: 941s\n","Epoch 2 - Score: 0.8769\n","Epoch 2 - Save Best Score: 0.8769 Model\n","Epoch: [3][0/3575] Elapsed 0m 0s (remain 41m 46s) Loss: 0.0067(0.0067) Grad: 25358.1621  LR: 0.000010  \n","Epoch: [3][100/3575] Elapsed 0m 30s (remain 17m 13s) Loss: 0.0069(0.0029) Grad: 14172.9990  LR: 0.000011  \n","Epoch: [3][200/3575] Elapsed 0m 54s (remain 15m 7s) Loss: 0.0003(0.0037) Grad: 1992.6991  LR: 0.000011  \n","Epoch: [3][300/3575] Elapsed 1m 17s (remain 14m 3s) Loss: 0.0000(0.0034) Grad: 273.9159  LR: 0.000012  \n","Epoch: [3][400/3575] Elapsed 1m 40s (remain 13m 17s) Loss: 0.0003(0.0034) Grad: 1509.3676  LR: 0.000013  \n","Epoch: [3][500/3575] Elapsed 2m 3s (remain 12m 40s) Loss: 0.0000(0.0032) Grad: 280.3702  LR: 0.000013  \n","Epoch: [3][600/3575] Elapsed 2m 27s (remain 12m 8s) Loss: 0.0046(0.0033) Grad: 21100.8301  LR: 0.000014  \n","Epoch: [3][700/3575] Elapsed 2m 50s (remain 11m 38s) Loss: 0.0027(0.0034) Grad: 34048.1523  LR: 0.000014  \n","Epoch: [3][800/3575] Elapsed 3m 13s (remain 11m 9s) Loss: 0.0000(0.0034) Grad: 262.6167  LR: 0.000015  \n","Epoch: [3][900/3575] Elapsed 3m 36s (remain 10m 42s) Loss: 0.0026(0.0034) Grad: 13069.4258  LR: 0.000016  \n","Epoch: [3][1000/3575] Elapsed 3m 59s (remain 10m 16s) Loss: 0.0164(0.0035) Grad: 51280.4609  LR: 0.000016  \n","Epoch: [3][1100/3575] Elapsed 4m 23s (remain 9m 51s) Loss: 0.0135(0.0035) Grad: 36651.4375  LR: 0.000017  \n","Epoch: [3][1200/3575] Elapsed 4m 46s (remain 9m 25s) Loss: 0.0002(0.0035) Grad: 1013.5798  LR: 0.000017  \n","Epoch: [3][1300/3575] Elapsed 5m 9s (remain 9m 0s) Loss: 0.0081(0.0036) Grad: 14391.7881  LR: 0.000018  \n","Epoch: [3][1400/3575] Elapsed 5m 32s (remain 8m 36s) Loss: 0.0065(0.0036) Grad: 18562.3125  LR: 0.000018  \n","Epoch: [3][1500/3575] Elapsed 5m 55s (remain 8m 11s) Loss: 0.0001(0.0036) Grad: 473.4186  LR: 0.000018  \n","Epoch: [3][1600/3575] Elapsed 6m 19s (remain 7m 47s) Loss: 0.0001(0.0036) Grad: 967.6490  LR: 0.000019  \n","Epoch: [3][1700/3575] Elapsed 6m 42s (remain 7m 23s) Loss: 0.0010(0.0037) Grad: 3200.5720  LR: 0.000019  \n","Epoch: [3][1800/3575] Elapsed 7m 5s (remain 6m 58s) Loss: 0.0073(0.0037) Grad: 22726.0215  LR: 0.000019  \n","Epoch: [3][1900/3575] Elapsed 7m 28s (remain 6m 34s) Loss: 0.0081(0.0037) Grad: 24284.5273  LR: 0.000020  \n","Epoch: [3][2000/3575] Elapsed 7m 51s (remain 6m 10s) Loss: 0.0020(0.0036) Grad: 16688.1406  LR: 0.000020  \n","Epoch: [3][2100/3575] Elapsed 8m 14s (remain 5m 47s) Loss: 0.0000(0.0037) Grad: 674.2654  LR: 0.000020  \n","Epoch: [3][2200/3575] Elapsed 8m 37s (remain 5m 23s) Loss: 0.0034(0.0037) Grad: 23349.5996  LR: 0.000020  \n","Epoch: [3][2300/3575] Elapsed 9m 1s (remain 4m 59s) Loss: 0.0089(0.0038) Grad: 28481.9863  LR: 0.000020  \n","Epoch: [3][2400/3575] Elapsed 9m 24s (remain 4m 35s) Loss: 0.0000(0.0038) Grad: 584.6968  LR: 0.000020  \n","Epoch: [3][2500/3575] Elapsed 9m 47s (remain 4m 12s) Loss: 0.0105(0.0038) Grad: 38309.2031  LR: 0.000020  \n","Epoch: [3][2600/3575] Elapsed 10m 10s (remain 3m 48s) Loss: 0.0017(0.0038) Grad: 13184.9688  LR: 0.000020  \n","Epoch: [3][2700/3575] Elapsed 10m 33s (remain 3m 25s) Loss: 0.0003(0.0038) Grad: 4156.5068  LR: 0.000020  \n","Epoch: [3][2800/3575] Elapsed 10m 56s (remain 3m 1s) Loss: 0.0015(0.0039) Grad: 14184.3809  LR: 0.000020  \n","Epoch: [3][2900/3575] Elapsed 11m 20s (remain 2m 37s) Loss: 0.0004(0.0039) Grad: 4095.6213  LR: 0.000019  \n","Epoch: [3][3000/3575] Elapsed 11m 43s (remain 2m 14s) Loss: 0.0056(0.0039) Grad: 58338.9258  LR: 0.000019  \n","Epoch: [3][3100/3575] Elapsed 12m 6s (remain 1m 51s) Loss: 0.0010(0.0039) Grad: 16284.0186  LR: 0.000019  \n","Epoch: [3][3200/3575] Elapsed 12m 29s (remain 1m 27s) Loss: 0.0051(0.0039) Grad: 28265.6934  LR: 0.000019  \n","Epoch: [3][3300/3575] Elapsed 12m 52s (remain 1m 4s) Loss: 0.0006(0.0038) Grad: 8097.7021  LR: 0.000018  \n","Epoch: [3][3400/3575] Elapsed 13m 15s (remain 0m 40s) Loss: 0.0000(0.0038) Grad: 444.8499  LR: 0.000018  \n","Epoch: [3][3500/3575] Elapsed 13m 38s (remain 0m 17s) Loss: 0.0000(0.0038) Grad: 136.2211  LR: 0.000017  \n","Epoch: [3][3574/3575] Elapsed 13m 56s (remain 0m 0s) Loss: 0.0100(0.0038) Grad: 32222.8594  LR: 0.000017  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 9m 1s) Loss: 0.0002(0.0002) \n","EVAL: [100/1192] Elapsed 0m 8s (remain 1m 29s) Loss: 0.0072(0.0033) \n","EVAL: [200/1192] Elapsed 0m 16s (remain 1m 19s) Loss: 0.0010(0.0041) \n","EVAL: [300/1192] Elapsed 0m 23s (remain 1m 10s) Loss: 0.0046(0.0042) \n","EVAL: [400/1192] Elapsed 0m 31s (remain 1m 2s) Loss: 0.0012(0.0043) \n","EVAL: [500/1192] Elapsed 0m 39s (remain 0m 54s) Loss: 0.0033(0.0040) \n","EVAL: [600/1192] Elapsed 0m 47s (remain 0m 46s) Loss: 0.0035(0.0044) \n","EVAL: [700/1192] Elapsed 0m 55s (remain 0m 38s) Loss: 0.0794(0.0053) \n","EVAL: [800/1192] Elapsed 1m 3s (remain 0m 30s) Loss: 0.0009(0.0055) \n","EVAL: [900/1192] Elapsed 1m 11s (remain 0m 23s) Loss: 0.0015(0.0055) \n","EVAL: [1000/1192] Elapsed 1m 19s (remain 0m 15s) Loss: 0.0000(0.0053) \n","EVAL: [1100/1192] Elapsed 1m 27s (remain 0m 7s) Loss: 0.0001(0.0051) \n","EVAL: [1191/1192] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0000(0.0050) \n","Epoch 3 - avg_train_loss: 0.0038  avg_val_loss: 0.0050  time: 936s\n","Epoch 3 - Score: 0.8802\n","Epoch 3 - Save Best Score: 0.8802 Model\n","Epoch: [4][0/3575] Elapsed 0m 0s (remain 43m 24s) Loss: 0.0000(0.0000) Grad: 188.6583  LR: 0.000017  \n","Epoch: [4][100/3575] Elapsed 0m 28s (remain 16m 32s) Loss: 0.0044(0.0031) Grad: 17159.9727  LR: 0.000017  \n","Epoch: [4][200/3575] Elapsed 0m 53s (remain 14m 55s) Loss: 0.0000(0.0026) Grad: 214.3034  LR: 0.000016  \n","Epoch: [4][300/3575] Elapsed 1m 17s (remain 13m 57s) Loss: 0.0001(0.0025) Grad: 675.4707  LR: 0.000016  \n","Epoch: [4][400/3575] Elapsed 1m 40s (remain 13m 14s) Loss: 0.0000(0.0026) Grad: 360.5583  LR: 0.000015  \n","Epoch: [4][500/3575] Elapsed 2m 3s (remain 12m 39s) Loss: 0.0000(0.0027) Grad: 428.8785  LR: 0.000014  \n","Epoch: [4][600/3575] Elapsed 2m 27s (remain 12m 8s) Loss: 0.0079(0.0027) Grad: 17773.0586  LR: 0.000014  \n","Epoch: [4][700/3575] Elapsed 2m 50s (remain 11m 40s) Loss: 0.0002(0.0029) Grad: 1081.3335  LR: 0.000013  \n","Epoch: [4][800/3575] Elapsed 3m 14s (remain 11m 12s) Loss: 0.0010(0.0029) Grad: 4901.6382  LR: 0.000013  \n","Epoch: [4][900/3575] Elapsed 3m 37s (remain 10m 46s) Loss: 0.0023(0.0029) Grad: 14478.0801  LR: 0.000012  \n","Epoch: [4][1000/3575] Elapsed 4m 1s (remain 10m 20s) Loss: 0.0014(0.0028) Grad: 16738.7246  LR: 0.000011  \n","Epoch: [4][1100/3575] Elapsed 4m 24s (remain 9m 54s) Loss: 0.0000(0.0028) Grad: 164.7423  LR: 0.000011  \n","Epoch: [4][1200/3575] Elapsed 4m 47s (remain 9m 29s) Loss: 0.0001(0.0028) Grad: 1127.6400  LR: 0.000010  \n","Epoch: [4][1300/3575] Elapsed 5m 11s (remain 9m 4s) Loss: 0.0002(0.0028) Grad: 1220.8574  LR: 0.000009  \n","Epoch: [4][1400/3575] Elapsed 5m 34s (remain 8m 39s) Loss: 0.0013(0.0028) Grad: 9780.6406  LR: 0.000009  \n","Epoch: [4][1500/3575] Elapsed 5m 58s (remain 8m 14s) Loss: 0.0260(0.0029) Grad: 55952.5000  LR: 0.000008  \n","Epoch: [4][1600/3575] Elapsed 6m 21s (remain 7m 50s) Loss: 0.0014(0.0028) Grad: 28667.6230  LR: 0.000007  \n","Epoch: [4][1700/3575] Elapsed 6m 44s (remain 7m 25s) Loss: 0.0033(0.0028) Grad: 8087.3198  LR: 0.000007  \n","Epoch: [4][1800/3575] Elapsed 7m 8s (remain 7m 1s) Loss: 0.0008(0.0028) Grad: 6461.9468  LR: 0.000006  \n","Epoch: [4][1900/3575] Elapsed 7m 31s (remain 6m 37s) Loss: 0.0000(0.0028) Grad: 242.2599  LR: 0.000005  \n","Epoch: [4][2000/3575] Elapsed 7m 54s (remain 6m 13s) Loss: 0.0000(0.0028) Grad: 292.2115  LR: 0.000005  \n","Epoch: [4][2100/3575] Elapsed 8m 18s (remain 5m 49s) Loss: 0.0000(0.0028) Grad: 679.1341  LR: 0.000004  \n","Epoch: [4][2200/3575] Elapsed 8m 41s (remain 5m 25s) Loss: 0.0000(0.0028) Grad: 286.4443  LR: 0.000004  \n","Epoch: [4][2300/3575] Elapsed 9m 5s (remain 5m 1s) Loss: 0.0028(0.0028) Grad: 36541.4062  LR: 0.000003  \n","Epoch: [4][2400/3575] Elapsed 9m 29s (remain 4m 38s) Loss: 0.0025(0.0027) Grad: 27114.8047  LR: 0.000003  \n","Epoch: [4][2500/3575] Elapsed 9m 53s (remain 4m 14s) Loss: 0.0000(0.0028) Grad: 432.7816  LR: 0.000002  \n","Epoch: [4][2600/3575] Elapsed 10m 16s (remain 3m 50s) Loss: 0.0033(0.0028) Grad: 24256.0801  LR: 0.000002  \n","Epoch: [4][2700/3575] Elapsed 10m 40s (remain 3m 27s) Loss: 0.0116(0.0028) Grad: 37726.1367  LR: 0.000002  \n","Epoch: [4][2800/3575] Elapsed 11m 3s (remain 3m 3s) Loss: 0.0006(0.0028) Grad: 7435.2578  LR: 0.000001  \n","Epoch: [4][2900/3575] Elapsed 11m 27s (remain 2m 39s) Loss: 0.0000(0.0028) Grad: 192.0755  LR: 0.000001  \n","Epoch: [4][3000/3575] Elapsed 11m 51s (remain 2m 16s) Loss: 0.0001(0.0028) Grad: 818.9106  LR: 0.000001  \n","Epoch: [4][3100/3575] Elapsed 12m 14s (remain 1m 52s) Loss: 0.0000(0.0028) Grad: 284.4532  LR: 0.000000  \n","Epoch: [4][3200/3575] Elapsed 12m 38s (remain 1m 28s) Loss: 0.0012(0.0028) Grad: 24587.5059  LR: 0.000000  \n","Epoch: [4][3300/3575] Elapsed 13m 1s (remain 1m 4s) Loss: 0.0008(0.0028) Grad: 10141.7568  LR: 0.000000  \n","Epoch: [4][3400/3575] Elapsed 13m 25s (remain 0m 41s) Loss: 0.0120(0.0028) Grad: 120342.1406  LR: 0.000000  \n","Epoch: [4][3500/3575] Elapsed 13m 48s (remain 0m 17s) Loss: 0.0000(0.0028) Grad: 337.8651  LR: 0.000000  \n","Epoch: [4][3574/3575] Elapsed 14m 5s (remain 0m 0s) Loss: 0.0025(0.0028) Grad: 40955.2109  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 10m 21s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 8s (remain 1m 32s) Loss: 0.0094(0.0032) \n","EVAL: [200/1192] Elapsed 0m 16s (remain 1m 22s) Loss: 0.0033(0.0043) \n","EVAL: [300/1192] Elapsed 0m 24s (remain 1m 13s) Loss: 0.0042(0.0049) \n","EVAL: [400/1192] Elapsed 0m 33s (remain 1m 5s) Loss: 0.0030(0.0052) \n","EVAL: [500/1192] Elapsed 0m 41s (remain 0m 56s) Loss: 0.0026(0.0049) \n","EVAL: [600/1192] Elapsed 0m 49s (remain 0m 48s) Loss: 0.0019(0.0051) \n","EVAL: [700/1192] Elapsed 0m 57s (remain 0m 40s) Loss: 0.0789(0.0060) \n","EVAL: [800/1192] Elapsed 1m 5s (remain 0m 32s) Loss: 0.0009(0.0061) \n","EVAL: [900/1192] Elapsed 1m 13s (remain 0m 23s) Loss: 0.0001(0.0061) \n","EVAL: [1000/1192] Elapsed 1m 21s (remain 0m 15s) Loss: 0.0000(0.0060) \n","EVAL: [1100/1192] Elapsed 1m 30s (remain 0m 7s) Loss: 0.0000(0.0057) \n","EVAL: [1191/1192] Elapsed 1m 37s (remain 0m 0s) Loss: 0.0000(0.0056) \n","Epoch 4 - avg_train_loss: 0.0028  avg_val_loss: 0.0056  time: 949s\n","Epoch 4 - Score: 0.8856\n","Epoch 4 - Save Best Score: 0.8856 Model\n","========== fold: 1 training ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-10.bin\n","Epoch: [1][0/3575] Elapsed 0m 1s (remain 101m 24s) Loss: 0.2841(0.2841) Grad: inf  LR: 0.000020  \n","Epoch: [1][100/3575] Elapsed 0m 34s (remain 19m 34s) Loss: 0.0198(0.0480) Grad: 4729.2534  LR: 0.000020  \n","Epoch: [1][200/3575] Elapsed 1m 6s (remain 18m 34s) Loss: 0.0242(0.0361) Grad: 12612.0518  LR: 0.000020  \n","Epoch: [1][300/3575] Elapsed 1m 38s (remain 17m 53s) Loss: 0.0012(0.0281) Grad: 2344.8464  LR: 0.000020  \n","Epoch: [1][400/3575] Elapsed 2m 10s (remain 17m 16s) Loss: 0.0014(0.0240) Grad: 3042.2456  LR: 0.000020  \n","Epoch: [1][500/3575] Elapsed 2m 43s (remain 16m 41s) Loss: 0.0061(0.0207) Grad: 6662.4150  LR: 0.000019  \n","Epoch: [1][600/3575] Elapsed 3m 15s (remain 16m 7s) Loss: 0.0099(0.0186) Grad: 6818.3677  LR: 0.000019  \n","Epoch: [1][700/3575] Elapsed 3m 47s (remain 15m 33s) Loss: 0.0078(0.0170) Grad: 5137.7109  LR: 0.000019  \n","Epoch: [1][800/3575] Elapsed 4m 19s (remain 15m 0s) Loss: 0.0028(0.0158) Grad: 21259.9023  LR: 0.000019  \n","Epoch: [1][900/3575] Elapsed 4m 52s (remain 14m 26s) Loss: 0.0046(0.0148) Grad: 3656.0840  LR: 0.000018  \n","Epoch: [1][1000/3575] Elapsed 5m 24s (remain 13m 53s) Loss: 0.0106(0.0140) Grad: 7498.6743  LR: 0.000018  \n","Epoch: [1][1100/3575] Elapsed 5m 56s (remain 13m 20s) Loss: 0.0025(0.0134) Grad: 3334.3037  LR: 0.000017  \n","Epoch: [1][1200/3575] Elapsed 6m 28s (remain 12m 47s) Loss: 0.0035(0.0128) Grad: 2208.3169  LR: 0.000017  \n","Epoch: [1][1300/3575] Elapsed 6m 56s (remain 12m 7s) Loss: 0.0061(0.0123) Grad: 3934.5032  LR: 0.000017  \n","Epoch: [1][1400/3575] Elapsed 7m 22s (remain 11m 26s) Loss: 0.0080(0.0119) Grad: 5342.7324  LR: 0.000016  \n","Epoch: [1][1500/3575] Elapsed 7m 49s (remain 10m 48s) Loss: 0.0047(0.0114) Grad: 4046.7024  LR: 0.000015  \n","Epoch: [1][1600/3575] Elapsed 8m 15s (remain 10m 11s) Loss: 0.0020(0.0110) Grad: 4506.8164  LR: 0.000015  \n","Epoch: [1][1700/3575] Elapsed 8m 42s (remain 9m 35s) Loss: 0.0043(0.0106) Grad: 4717.6255  LR: 0.000014  \n","Epoch: [1][1800/3575] Elapsed 9m 8s (remain 9m 0s) Loss: 0.0087(0.0104) Grad: 7968.2803  LR: 0.000014  \n","Epoch: [1][1900/3575] Elapsed 9m 35s (remain 8m 26s) Loss: 0.0030(0.0102) Grad: 3157.9507  LR: 0.000013  \n","Epoch: [1][2000/3575] Elapsed 10m 2s (remain 7m 53s) Loss: 0.0092(0.0100) Grad: 3970.8877  LR: 0.000012  \n","Epoch: [1][2100/3575] Elapsed 10m 29s (remain 7m 21s) Loss: 0.0210(0.0097) Grad: 19089.4395  LR: 0.000012  \n","Epoch: [1][2200/3575] Elapsed 10m 55s (remain 6m 49s) Loss: 0.0039(0.0095) Grad: 11657.2412  LR: 0.000011  \n","Epoch: [1][2300/3575] Elapsed 11m 22s (remain 6m 17s) Loss: 0.0000(0.0093) Grad: 168.2031  LR: 0.000011  \n","Epoch: [1][2400/3575] Elapsed 11m 49s (remain 5m 46s) Loss: 0.0002(0.0090) Grad: 1517.5526  LR: 0.000010  \n","Epoch: [1][2500/3575] Elapsed 12m 16s (remain 5m 16s) Loss: 0.0046(0.0088) Grad: 8180.3159  LR: 0.000009  \n","Epoch: [1][2600/3575] Elapsed 12m 43s (remain 4m 45s) Loss: 0.0006(0.0086) Grad: 1574.7533  LR: 0.000009  \n","Epoch: [1][2700/3575] Elapsed 13m 9s (remain 4m 15s) Loss: 0.0054(0.0085) Grad: 8099.9395  LR: 0.000008  \n","Epoch: [1][2800/3575] Elapsed 13m 36s (remain 3m 45s) Loss: 0.0000(0.0084) Grad: 228.4319  LR: 0.000007  \n","Epoch: [1][2900/3575] Elapsed 14m 2s (remain 3m 15s) Loss: 0.0005(0.0082) Grad: 1241.1927  LR: 0.000007  \n","Epoch: [1][3000/3575] Elapsed 14m 29s (remain 2m 46s) Loss: 0.0200(0.0081) Grad: 13954.5488  LR: 0.000006  \n","Epoch: [1][3100/3575] Elapsed 14m 56s (remain 2m 17s) Loss: 0.0109(0.0079) Grad: 47805.4258  LR: 0.000005  \n","Epoch: [1][3200/3575] Elapsed 15m 22s (remain 1m 47s) Loss: 0.0078(0.0078) Grad: 8629.9385  LR: 0.000005  \n","Epoch: [1][3300/3575] Elapsed 15m 48s (remain 1m 18s) Loss: 0.0000(0.0077) Grad: 177.4195  LR: 0.000004  \n","Epoch: [1][3400/3575] Elapsed 16m 14s (remain 0m 49s) Loss: 0.0006(0.0076) Grad: 1599.1057  LR: 0.000004  \n","Epoch: [1][3500/3575] Elapsed 16m 40s (remain 0m 21s) Loss: 0.0001(0.0075) Grad: 411.8787  LR: 0.000003  \n","Epoch: [1][3574/3575] Elapsed 16m 59s (remain 0m 0s) Loss: 0.0014(0.0074) Grad: 3583.2122  LR: 0.000003  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 13m 45s) Loss: 0.0001(0.0001) \n","EVAL: [100/1192] Elapsed 0m 8s (remain 1m 35s) Loss: 0.0005(0.0031) \n","EVAL: [200/1192] Elapsed 0m 16s (remain 1m 22s) Loss: 0.0002(0.0036) \n","EVAL: [300/1192] Elapsed 0m 24s (remain 1m 13s) Loss: 0.0007(0.0054) \n","EVAL: [400/1192] Elapsed 0m 32s (remain 1m 4s) Loss: 0.0111(0.0056) \n","EVAL: [500/1192] Elapsed 0m 40s (remain 0m 56s) Loss: 0.0092(0.0053) \n","EVAL: [600/1192] Elapsed 0m 48s (remain 0m 47s) Loss: 0.0763(0.0054) \n","EVAL: [700/1192] Elapsed 0m 56s (remain 0m 39s) Loss: 0.0146(0.0059) \n","EVAL: [800/1192] Elapsed 1m 4s (remain 0m 31s) Loss: 0.0041(0.0057) \n","EVAL: [900/1192] Elapsed 1m 12s (remain 0m 23s) Loss: 0.0031(0.0055) \n","EVAL: [1000/1192] Elapsed 1m 20s (remain 0m 15s) Loss: 0.0001(0.0052) \n","EVAL: [1100/1192] Elapsed 1m 28s (remain 0m 7s) Loss: 0.0059(0.0049) \n","EVAL: [1191/1192] Elapsed 1m 35s (remain 0m 0s) Loss: 0.0063(0.0047) \n","Epoch 1 - avg_train_loss: 0.0074  avg_val_loss: 0.0047  time: 1121s\n","Epoch 1 - Score: 0.8702\n","Epoch 1 - Save Best Score: 0.8702 Model\n","Epoch: [2][0/3575] Elapsed 0m 0s (remain 49m 5s) Loss: 0.0017(0.0017) Grad: 8972.9570  LR: 0.000003  \n","Epoch: [2][100/3575] Elapsed 0m 28s (remain 16m 16s) Loss: 0.0001(0.0029) Grad: 1727.9000  LR: 0.000002  \n","Epoch: [2][200/3575] Elapsed 0m 53s (remain 14m 49s) Loss: 0.0011(0.0032) Grad: 6401.0898  LR: 0.000002  \n","Epoch: [2][300/3575] Elapsed 1m 16s (remain 13m 50s) Loss: 0.0054(0.0028) Grad: 60240.0703  LR: 0.000002  \n","Epoch: [2][400/3575] Elapsed 1m 39s (remain 13m 7s) Loss: 0.0085(0.0028) Grad: 20207.8711  LR: 0.000001  \n","Epoch: [2][500/3575] Elapsed 2m 2s (remain 12m 32s) Loss: 0.0013(0.0029) Grad: 7726.8511  LR: 0.000001  \n","Epoch: [2][600/3575] Elapsed 2m 25s (remain 12m 2s) Loss: 0.0002(0.0028) Grad: 2520.3647  LR: 0.000001  \n","Epoch: [2][700/3575] Elapsed 2m 49s (remain 11m 34s) Loss: 0.0008(0.0027) Grad: 5237.6694  LR: 0.000001  \n","Epoch: [2][800/3575] Elapsed 3m 12s (remain 11m 6s) Loss: 0.0000(0.0028) Grad: 212.2859  LR: 0.000000  \n","Epoch: [2][900/3575] Elapsed 3m 35s (remain 10m 40s) Loss: 0.0022(0.0027) Grad: 11544.8301  LR: 0.000000  \n","Epoch: [2][1000/3575] Elapsed 3m 59s (remain 10m 14s) Loss: 0.0031(0.0028) Grad: 19277.7773  LR: 0.000000  \n","Epoch: [2][1100/3575] Elapsed 4m 22s (remain 9m 49s) Loss: 0.0000(0.0029) Grad: 146.6826  LR: 0.000000  \n","Epoch: [2][1200/3575] Elapsed 4m 45s (remain 9m 24s) Loss: 0.0004(0.0031) Grad: 3117.9277  LR: 0.000000  \n","Epoch: [2][1300/3575] Elapsed 5m 9s (remain 9m 0s) Loss: 0.0001(0.0030) Grad: 540.3215  LR: 0.000000  \n","Epoch: [2][1400/3575] Elapsed 5m 32s (remain 8m 35s) Loss: 0.0000(0.0031) Grad: 215.3967  LR: 0.000000  \n","Epoch: [2][1500/3575] Elapsed 5m 55s (remain 8m 11s) Loss: 0.0000(0.0030) Grad: 1283.7961  LR: 0.000000  \n","Epoch: [2][1600/3575] Elapsed 6m 19s (remain 7m 47s) Loss: 0.0003(0.0031) Grad: 2251.3889  LR: 0.000000  \n","Epoch: [2][1700/3575] Elapsed 6m 42s (remain 7m 23s) Loss: 0.0000(0.0031) Grad: 251.7800  LR: 0.000001  \n","Epoch: [2][1800/3575] Elapsed 7m 5s (remain 6m 59s) Loss: 0.0190(0.0031) Grad: 78995.0000  LR: 0.000001  \n","Epoch: [2][1900/3575] Elapsed 7m 28s (remain 6m 35s) Loss: 0.0034(0.0031) Grad: 47127.9570  LR: 0.000001  \n","Epoch: [2][2000/3575] Elapsed 7m 52s (remain 6m 11s) Loss: 0.0159(0.0031) Grad: 40732.4141  LR: 0.000001  \n","Epoch: [2][2100/3575] Elapsed 8m 15s (remain 5m 47s) Loss: 0.0060(0.0031) Grad: 15508.2578  LR: 0.000002  \n","Epoch: [2][2200/3575] Elapsed 8m 38s (remain 5m 23s) Loss: 0.0020(0.0032) Grad: 11398.2148  LR: 0.000002  \n","Epoch: [2][2300/3575] Elapsed 9m 1s (remain 4m 59s) Loss: 0.0002(0.0031) Grad: 3724.0249  LR: 0.000003  \n","Epoch: [2][2400/3575] Elapsed 9m 24s (remain 4m 36s) Loss: 0.0091(0.0031) Grad: 17258.7422  LR: 0.000003  \n","Epoch: [2][2500/3575] Elapsed 9m 47s (remain 4m 12s) Loss: 0.0000(0.0031) Grad: 343.2598  LR: 0.000004  \n","Epoch: [2][2600/3575] Elapsed 10m 11s (remain 3m 48s) Loss: 0.0002(0.0031) Grad: 1638.4161  LR: 0.000004  \n","Epoch: [2][2700/3575] Elapsed 10m 34s (remain 3m 25s) Loss: 0.0002(0.0031) Grad: 2075.9824  LR: 0.000005  \n","Epoch: [2][2800/3575] Elapsed 10m 57s (remain 3m 1s) Loss: 0.0052(0.0032) Grad: 24592.7773  LR: 0.000005  \n","Epoch: [2][2900/3575] Elapsed 11m 20s (remain 2m 38s) Loss: 0.0001(0.0032) Grad: 786.8326  LR: 0.000006  \n","Epoch: [2][3000/3575] Elapsed 11m 43s (remain 2m 14s) Loss: 0.0002(0.0032) Grad: 718.9269  LR: 0.000006  \n","Epoch: [2][3100/3575] Elapsed 12m 6s (remain 1m 51s) Loss: 0.0007(0.0032) Grad: 5220.1299  LR: 0.000007  \n","Epoch: [2][3200/3575] Elapsed 12m 29s (remain 1m 27s) Loss: 0.0004(0.0032) Grad: 2695.0710  LR: 0.000008  \n","Epoch: [2][3300/3575] Elapsed 12m 53s (remain 1m 4s) Loss: 0.0000(0.0032) Grad: 182.4759  LR: 0.000008  \n","Epoch: [2][3400/3575] Elapsed 13m 16s (remain 0m 40s) Loss: 0.0043(0.0032) Grad: 13809.4482  LR: 0.000009  \n","Epoch: [2][3500/3575] Elapsed 13m 39s (remain 0m 17s) Loss: 0.0011(0.0032) Grad: 7068.7256  LR: 0.000010  \n","Epoch: [2][3574/3575] Elapsed 13m 56s (remain 0m 0s) Loss: 0.0009(0.0032) Grad: 6819.9951  LR: 0.000010  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 10m 50s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 8s (remain 1m 31s) Loss: 0.0001(0.0037) \n","EVAL: [200/1192] Elapsed 0m 16s (remain 1m 20s) Loss: 0.0001(0.0044) \n","EVAL: [300/1192] Elapsed 0m 24s (remain 1m 11s) Loss: 0.0004(0.0069) \n","EVAL: [400/1192] Elapsed 0m 32s (remain 1m 3s) Loss: 0.0141(0.0071) \n","EVAL: [500/1192] Elapsed 0m 39s (remain 0m 55s) Loss: 0.0151(0.0068) \n","EVAL: [600/1192] Elapsed 0m 47s (remain 0m 46s) Loss: 0.1074(0.0070) \n","EVAL: [700/1192] Elapsed 0m 55s (remain 0m 38s) Loss: 0.0142(0.0078) \n","EVAL: [800/1192] Elapsed 1m 3s (remain 0m 30s) Loss: 0.0443(0.0075) \n","EVAL: [900/1192] Elapsed 1m 11s (remain 0m 22s) Loss: 0.0035(0.0073) \n","EVAL: [1000/1192] Elapsed 1m 18s (remain 0m 15s) Loss: 0.0000(0.0070) \n","EVAL: [1100/1192] Elapsed 1m 26s (remain 0m 7s) Loss: 0.0075(0.0066) \n","EVAL: [1191/1192] Elapsed 1m 33s (remain 0m 0s) Loss: 0.0082(0.0062) \n","Epoch 2 - avg_train_loss: 0.0032  avg_val_loss: 0.0062  time: 936s\n","Epoch 2 - Score: 0.8711\n","Epoch 2 - Save Best Score: 0.8711 Model\n","Epoch: [3][0/3575] Elapsed 0m 0s (remain 45m 50s) Loss: 0.0082(0.0082) Grad: 19895.5977  LR: 0.000010  \n","Epoch: [3][100/3575] Elapsed 0m 28s (remain 16m 36s) Loss: 0.0000(0.0037) Grad: 153.2134  LR: 0.000011  \n","Epoch: [3][200/3575] Elapsed 0m 53s (remain 14m 50s) Loss: 0.0030(0.0039) Grad: 35881.6094  LR: 0.000011  \n","Epoch: [3][300/3575] Elapsed 1m 16s (remain 13m 49s) Loss: 0.0001(0.0039) Grad: 739.2531  LR: 0.000012  \n","Epoch: [3][400/3575] Elapsed 1m 39s (remain 13m 7s) Loss: 0.0000(0.0038) Grad: 157.6420  LR: 0.000013  \n","Epoch: [3][500/3575] Elapsed 2m 2s (remain 12m 32s) Loss: 0.0000(0.0037) Grad: 272.0007  LR: 0.000013  \n","Epoch: [3][600/3575] Elapsed 2m 25s (remain 12m 1s) Loss: 0.0184(0.0037) Grad: 25826.8496  LR: 0.000014  \n","Epoch: [3][700/3575] Elapsed 2m 49s (remain 11m 33s) Loss: 0.0001(0.0037) Grad: 1355.9171  LR: 0.000014  \n","Epoch: [3][800/3575] Elapsed 3m 12s (remain 11m 6s) Loss: 0.0018(0.0039) Grad: 6667.5137  LR: 0.000015  \n","Epoch: [3][900/3575] Elapsed 3m 35s (remain 10m 39s) Loss: 0.0019(0.0037) Grad: 8285.2275  LR: 0.000016  \n","Epoch: [3][1000/3575] Elapsed 3m 58s (remain 10m 13s) Loss: 0.0017(0.0037) Grad: 5365.1128  LR: 0.000016  \n","Epoch: [3][1100/3575] Elapsed 4m 21s (remain 9m 48s) Loss: 0.0000(0.0037) Grad: 253.2040  LR: 0.000017  \n","Epoch: [3][1200/3575] Elapsed 4m 44s (remain 9m 23s) Loss: 0.0002(0.0037) Grad: 2402.7942  LR: 0.000017  \n","Epoch: [3][1300/3575] Elapsed 5m 8s (remain 8m 58s) Loss: 0.0032(0.0036) Grad: 8227.7754  LR: 0.000018  \n","Epoch: [3][1400/3575] Elapsed 5m 31s (remain 8m 34s) Loss: 0.0003(0.0037) Grad: 2545.1423  LR: 0.000018  \n","Epoch: [3][1500/3575] Elapsed 5m 54s (remain 8m 10s) Loss: 0.0042(0.0037) Grad: 18876.9609  LR: 0.000018  \n","Epoch: [3][1600/3575] Elapsed 6m 18s (remain 7m 46s) Loss: 0.0002(0.0037) Grad: 1750.5259  LR: 0.000019  \n","Epoch: [3][1700/3575] Elapsed 6m 42s (remain 7m 22s) Loss: 0.0001(0.0037) Grad: 318.0589  LR: 0.000019  \n","Epoch: [3][1800/3575] Elapsed 7m 5s (remain 6m 58s) Loss: 0.0092(0.0037) Grad: 30660.4648  LR: 0.000019  \n","Epoch: [3][1900/3575] Elapsed 7m 28s (remain 6m 34s) Loss: 0.0001(0.0037) Grad: 589.7209  LR: 0.000020  \n","Epoch: [3][2000/3575] Elapsed 7m 51s (remain 6m 11s) Loss: 0.0001(0.0037) Grad: 1265.1569  LR: 0.000020  \n","Epoch: [3][2100/3575] Elapsed 8m 14s (remain 5m 47s) Loss: 0.0018(0.0037) Grad: 16794.7832  LR: 0.000020  \n","Epoch: [3][2200/3575] Elapsed 8m 38s (remain 5m 23s) Loss: 0.0016(0.0038) Grad: 26736.6328  LR: 0.000020  \n","Epoch: [3][2300/3575] Elapsed 9m 1s (remain 4m 59s) Loss: 0.0007(0.0038) Grad: 5742.7329  LR: 0.000020  \n","Epoch: [3][2400/3575] Elapsed 9m 24s (remain 4m 35s) Loss: 0.0015(0.0038) Grad: 6258.9438  LR: 0.000020  \n","Epoch: [3][2500/3575] Elapsed 9m 47s (remain 4m 12s) Loss: 0.0000(0.0037) Grad: 137.0351  LR: 0.000020  \n","Epoch: [3][2600/3575] Elapsed 10m 10s (remain 3m 48s) Loss: 0.0002(0.0038) Grad: 6565.4473  LR: 0.000020  \n","Epoch: [3][2700/3575] Elapsed 10m 34s (remain 3m 25s) Loss: 0.0054(0.0037) Grad: 32327.1250  LR: 0.000020  \n","Epoch: [3][2800/3575] Elapsed 10m 57s (remain 3m 1s) Loss: 0.0018(0.0037) Grad: 6312.2866  LR: 0.000020  \n","Epoch: [3][2900/3575] Elapsed 11m 20s (remain 2m 38s) Loss: 0.0000(0.0038) Grad: 228.5733  LR: 0.000019  \n","Epoch: [3][3000/3575] Elapsed 11m 43s (remain 2m 14s) Loss: 0.0000(0.0038) Grad: 290.0135  LR: 0.000019  \n","Epoch: [3][3100/3575] Elapsed 12m 6s (remain 1m 51s) Loss: 0.0026(0.0038) Grad: 30532.7891  LR: 0.000019  \n","Epoch: [3][3200/3575] Elapsed 12m 30s (remain 1m 27s) Loss: 0.0066(0.0038) Grad: 18791.3809  LR: 0.000019  \n","Epoch: [3][3300/3575] Elapsed 12m 53s (remain 1m 4s) Loss: 0.0273(0.0038) Grad: 46898.3438  LR: 0.000018  \n","Epoch: [3][3400/3575] Elapsed 13m 16s (remain 0m 40s) Loss: 0.0171(0.0038) Grad: 36581.8203  LR: 0.000018  \n","Epoch: [3][3500/3575] Elapsed 13m 39s (remain 0m 17s) Loss: 0.0000(0.0038) Grad: 206.3015  LR: 0.000017  \n","Epoch: [3][3574/3575] Elapsed 13m 57s (remain 0m 0s) Loss: 0.0000(0.0038) Grad: 226.6891  LR: 0.000017  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 11m 6s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 8s (remain 1m 30s) Loss: 0.0019(0.0042) \n","EVAL: [200/1192] Elapsed 0m 16s (remain 1m 20s) Loss: 0.0001(0.0049) \n","EVAL: [300/1192] Elapsed 0m 24s (remain 1m 12s) Loss: 0.0003(0.0067) \n","EVAL: [400/1192] Elapsed 0m 32s (remain 1m 3s) Loss: 0.0080(0.0070) \n","EVAL: [500/1192] Elapsed 0m 39s (remain 0m 55s) Loss: 0.0184(0.0068) \n","EVAL: [600/1192] Elapsed 0m 47s (remain 0m 46s) Loss: 0.0910(0.0069) \n","EVAL: [700/1192] Elapsed 0m 55s (remain 0m 38s) Loss: 0.0155(0.0075) \n","EVAL: [800/1192] Elapsed 1m 3s (remain 0m 30s) Loss: 0.0078(0.0072) \n","EVAL: [900/1192] Elapsed 1m 11s (remain 0m 23s) Loss: 0.0022(0.0070) \n","EVAL: [1000/1192] Elapsed 1m 19s (remain 0m 15s) Loss: 0.0000(0.0067) \n","EVAL: [1100/1192] Elapsed 1m 26s (remain 0m 7s) Loss: 0.0056(0.0064) \n","EVAL: [1191/1192] Elapsed 1m 33s (remain 0m 0s) Loss: 0.0095(0.0060) \n","Epoch 3 - avg_train_loss: 0.0038  avg_val_loss: 0.0060  time: 936s\n","Epoch 3 - Score: 0.8671\n","Epoch: [4][0/3575] Elapsed 0m 0s (remain 42m 58s) Loss: 0.0000(0.0000) Grad: 230.9597  LR: 0.000017  \n","Epoch: [4][100/3575] Elapsed 0m 24s (remain 14m 0s) Loss: 0.0000(0.0021) Grad: 347.8281  LR: 0.000017  \n","Epoch: [4][200/3575] Elapsed 0m 48s (remain 13m 26s) Loss: 0.0000(0.0024) Grad: 221.0278  LR: 0.000016  \n","Epoch: [4][300/3575] Elapsed 1m 11s (remain 12m 57s) Loss: 0.0006(0.0028) Grad: 6204.3457  LR: 0.000016  \n","Epoch: [4][400/3575] Elapsed 1m 35s (remain 12m 33s) Loss: 0.0118(0.0027) Grad: 60420.8711  LR: 0.000015  \n","Epoch: [4][500/3575] Elapsed 1m 58s (remain 12m 7s) Loss: 0.0000(0.0028) Grad: 144.0350  LR: 0.000014  \n","Epoch: [4][600/3575] Elapsed 2m 21s (remain 11m 42s) Loss: 0.0031(0.0030) Grad: 6832.7109  LR: 0.000014  \n","Epoch: [4][700/3575] Elapsed 2m 45s (remain 11m 17s) Loss: 0.0072(0.0029) Grad: 11715.2471  LR: 0.000013  \n","Epoch: [4][800/3575] Elapsed 3m 8s (remain 10m 53s) Loss: 0.0001(0.0029) Grad: 600.1878  LR: 0.000013  \n","Epoch: [4][900/3575] Elapsed 3m 32s (remain 10m 29s) Loss: 0.0010(0.0030) Grad: 4671.1108  LR: 0.000012  \n","Epoch: [4][1000/3575] Elapsed 3m 55s (remain 10m 5s) Loss: 0.0003(0.0030) Grad: 2848.7539  LR: 0.000011  \n","Epoch: [4][1100/3575] Elapsed 4m 18s (remain 9m 41s) Loss: 0.0080(0.0030) Grad: 24994.3438  LR: 0.000011  \n","Epoch: [4][1200/3575] Elapsed 4m 41s (remain 9m 16s) Loss: 0.0021(0.0030) Grad: 20852.5312  LR: 0.000010  \n","Epoch: [4][1300/3575] Elapsed 5m 5s (remain 8m 53s) Loss: 0.0001(0.0030) Grad: 312.3335  LR: 0.000009  \n","Epoch: [4][1400/3575] Elapsed 5m 28s (remain 8m 29s) Loss: 0.0000(0.0029) Grad: 299.5519  LR: 0.000009  \n","Epoch: [4][1500/3575] Elapsed 5m 51s (remain 8m 5s) Loss: 0.0044(0.0029) Grad: 15437.9238  LR: 0.000008  \n","Epoch: [4][1600/3575] Elapsed 6m 14s (remain 7m 42s) Loss: 0.0029(0.0028) Grad: 35938.6445  LR: 0.000007  \n","Epoch: [4][1700/3575] Elapsed 6m 37s (remain 7m 18s) Loss: 0.0054(0.0028) Grad: 19972.5547  LR: 0.000007  \n","Epoch: [4][1800/3575] Elapsed 7m 1s (remain 6m 54s) Loss: 0.0017(0.0028) Grad: 7321.8521  LR: 0.000006  \n","Epoch: [4][1900/3575] Elapsed 7m 24s (remain 6m 31s) Loss: 0.0001(0.0028) Grad: 629.2079  LR: 0.000005  \n","Epoch: [4][2000/3575] Elapsed 7m 47s (remain 6m 7s) Loss: 0.0000(0.0028) Grad: 234.6072  LR: 0.000005  \n","Epoch: [4][2100/3575] Elapsed 8m 10s (remain 5m 44s) Loss: 0.0000(0.0027) Grad: 148.8865  LR: 0.000004  \n","Epoch: [4][2200/3575] Elapsed 8m 34s (remain 5m 20s) Loss: 0.0000(0.0028) Grad: 177.0598  LR: 0.000004  \n","Epoch: [4][2300/3575] Elapsed 8m 57s (remain 4m 57s) Loss: 0.0001(0.0028) Grad: 1235.0619  LR: 0.000003  \n","Epoch: [4][2400/3575] Elapsed 9m 20s (remain 4m 34s) Loss: 0.0000(0.0028) Grad: 137.4906  LR: 0.000003  \n","Epoch: [4][2500/3575] Elapsed 9m 43s (remain 4m 10s) Loss: 0.0088(0.0028) Grad: 37587.1484  LR: 0.000002  \n","Epoch: [4][2600/3575] Elapsed 10m 6s (remain 3m 47s) Loss: 0.0017(0.0028) Grad: 29297.8105  LR: 0.000002  \n","Epoch: [4][2700/3575] Elapsed 10m 30s (remain 3m 23s) Loss: 0.0000(0.0028) Grad: 145.4715  LR: 0.000002  \n","Epoch: [4][2800/3575] Elapsed 10m 53s (remain 3m 0s) Loss: 0.0001(0.0028) Grad: 900.6138  LR: 0.000001  \n","Epoch: [4][2900/3575] Elapsed 11m 16s (remain 2m 37s) Loss: 0.0114(0.0028) Grad: 38532.2812  LR: 0.000001  \n","Epoch: [4][3000/3575] Elapsed 11m 39s (remain 2m 13s) Loss: 0.0000(0.0028) Grad: 250.1930  LR: 0.000001  \n","Epoch: [4][3100/3575] Elapsed 12m 3s (remain 1m 50s) Loss: 0.0000(0.0028) Grad: 204.0439  LR: 0.000000  \n","Epoch: [4][3200/3575] Elapsed 12m 26s (remain 1m 27s) Loss: 0.0000(0.0028) Grad: 206.1618  LR: 0.000000  \n","Epoch: [4][3300/3575] Elapsed 12m 49s (remain 1m 3s) Loss: 0.0068(0.0028) Grad: 163563.5000  LR: 0.000000  \n","Epoch: [4][3400/3575] Elapsed 13m 12s (remain 0m 40s) Loss: 0.0000(0.0028) Grad: 244.2188  LR: 0.000000  \n","Epoch: [4][3500/3575] Elapsed 13m 36s (remain 0m 17s) Loss: 0.0002(0.0028) Grad: 2451.8887  LR: 0.000000  \n","Epoch: [4][3574/3575] Elapsed 13m 53s (remain 0m 0s) Loss: 0.0023(0.0028) Grad: 23679.2715  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 11m 23s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 8s (remain 1m 30s) Loss: 0.0000(0.0040) \n","EVAL: [200/1192] Elapsed 0m 16s (remain 1m 20s) Loss: 0.0000(0.0048) \n","EVAL: [300/1192] Elapsed 0m 24s (remain 1m 11s) Loss: 0.0000(0.0071) \n","EVAL: [400/1192] Elapsed 0m 32s (remain 1m 3s) Loss: 0.0167(0.0077) \n","EVAL: [500/1192] Elapsed 0m 39s (remain 0m 55s) Loss: 0.0161(0.0073) \n","EVAL: [600/1192] Elapsed 0m 47s (remain 0m 46s) Loss: 0.0918(0.0073) \n","EVAL: [700/1192] Elapsed 0m 55s (remain 0m 38s) Loss: 0.0151(0.0081) \n","EVAL: [800/1192] Elapsed 1m 3s (remain 0m 31s) Loss: 0.0083(0.0078) \n","EVAL: [900/1192] Elapsed 1m 11s (remain 0m 23s) Loss: 0.0016(0.0076) \n","EVAL: [1000/1192] Elapsed 1m 19s (remain 0m 15s) Loss: 0.0000(0.0072) \n","EVAL: [1100/1192] Elapsed 1m 27s (remain 0m 7s) Loss: 0.0050(0.0068) \n","EVAL: [1191/1192] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0071(0.0064) \n","Epoch 4 - avg_train_loss: 0.0028  avg_val_loss: 0.0064  time: 933s\n","Epoch 4 - Score: 0.8796\n","Epoch 4 - Save Best Score: 0.8796 Model\n","========== fold: 2 training ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-10.bin\n","Epoch: [1][0/3575] Elapsed 0m 2s (remain 124m 50s) Loss: 0.2365(0.2365) Grad: inf  LR: 0.000020  \n","Epoch: [1][100/3575] Elapsed 0m 34s (remain 19m 52s) Loss: 0.0313(0.0493) Grad: 16719.4609  LR: 0.000020  \n","Epoch: [1][200/3575] Elapsed 1m 7s (remain 18m 46s) Loss: 0.0372(0.0379) Grad: 14951.5381  LR: 0.000020  \n","Epoch: [1][300/3575] Elapsed 1m 39s (remain 17m 59s) Loss: 0.0080(0.0307) Grad: 9120.8135  LR: 0.000020  \n","Epoch: [1][400/3575] Elapsed 2m 11s (remain 17m 21s) Loss: 0.0534(0.0260) Grad: 59869.2266  LR: 0.000020  \n","Epoch: [1][500/3575] Elapsed 2m 43s (remain 16m 45s) Loss: 0.0006(0.0230) Grad: 1774.5154  LR: 0.000019  \n","Epoch: [1][600/3575] Elapsed 3m 16s (remain 16m 10s) Loss: 0.0248(0.0210) Grad: 91870.5703  LR: 0.000019  \n","Epoch: [1][700/3575] Elapsed 3m 48s (remain 15m 35s) Loss: 0.0025(0.0193) Grad: 7492.6704  LR: 0.000019  \n","Epoch: [1][800/3575] Elapsed 4m 20s (remain 15m 2s) Loss: 0.0080(0.0180) Grad: 8310.6289  LR: 0.000019  \n","Epoch: [1][900/3575] Elapsed 4m 52s (remain 14m 29s) Loss: 0.0003(0.0168) Grad: 1030.8551  LR: 0.000018  \n","Epoch: [1][1000/3575] Elapsed 5m 25s (remain 13m 55s) Loss: 0.0024(0.0156) Grad: 5474.4302  LR: 0.000018  \n","Epoch: [1][1100/3575] Elapsed 5m 57s (remain 13m 22s) Loss: 0.0037(0.0148) Grad: 4636.7256  LR: 0.000017  \n","Epoch: [1][1200/3575] Elapsed 6m 30s (remain 12m 51s) Loss: 0.0067(0.0142) Grad: 6519.1821  LR: 0.000017  \n","Epoch: [1][1300/3575] Elapsed 6m 56s (remain 12m 7s) Loss: 0.0006(0.0136) Grad: 2931.2070  LR: 0.000017  \n","Epoch: [1][1400/3575] Elapsed 7m 22s (remain 11m 26s) Loss: 0.0095(0.0131) Grad: 5862.5425  LR: 0.000016  \n","Epoch: [1][1500/3575] Elapsed 7m 48s (remain 10m 47s) Loss: 0.0008(0.0127) Grad: 1211.1389  LR: 0.000015  \n","Epoch: [1][1600/3575] Elapsed 8m 14s (remain 10m 9s) Loss: 0.0012(0.0124) Grad: 3478.9993  LR: 0.000015  \n","Epoch: [1][1700/3575] Elapsed 8m 40s (remain 9m 32s) Loss: 0.0084(0.0120) Grad: 20199.9766  LR: 0.000014  \n","Epoch: [1][1800/3575] Elapsed 9m 5s (remain 8m 57s) Loss: 0.0002(0.0117) Grad: 341.0374  LR: 0.000014  \n","Epoch: [1][1900/3575] Elapsed 9m 31s (remain 8m 23s) Loss: 0.0012(0.0113) Grad: 3051.8801  LR: 0.000013  \n","Epoch: [1][2000/3575] Elapsed 9m 58s (remain 7m 50s) Loss: 0.0037(0.0110) Grad: 6232.8472  LR: 0.000012  \n","Epoch: [1][2100/3575] Elapsed 10m 24s (remain 7m 17s) Loss: 0.0001(0.0107) Grad: 651.8504  LR: 0.000012  \n","Epoch: [1][2200/3575] Elapsed 10m 50s (remain 6m 45s) Loss: 0.0003(0.0104) Grad: 1323.6750  LR: 0.000011  \n","Epoch: [1][2300/3575] Elapsed 11m 16s (remain 6m 14s) Loss: 0.0029(0.0102) Grad: 7778.1143  LR: 0.000011  \n","Epoch: [1][2400/3575] Elapsed 11m 42s (remain 5m 43s) Loss: 0.0000(0.0100) Grad: 183.3064  LR: 0.000010  \n","Epoch: [1][2500/3575] Elapsed 12m 8s (remain 5m 12s) Loss: 0.0004(0.0098) Grad: 2503.9807  LR: 0.000009  \n","Epoch: [1][2600/3575] Elapsed 12m 34s (remain 4m 42s) Loss: 0.0020(0.0096) Grad: 9952.4971  LR: 0.000009  \n","Epoch: [1][2700/3575] Elapsed 13m 0s (remain 4m 12s) Loss: 0.0051(0.0094) Grad: 13491.6875  LR: 0.000008  \n","Epoch: [1][2800/3575] Elapsed 13m 26s (remain 3m 42s) Loss: 0.0027(0.0092) Grad: 15628.1318  LR: 0.000007  \n","Epoch: [1][2900/3575] Elapsed 13m 52s (remain 3m 13s) Loss: 0.0029(0.0090) Grad: 12906.9443  LR: 0.000007  \n","Epoch: [1][3000/3575] Elapsed 14m 18s (remain 2m 44s) Loss: 0.0017(0.0089) Grad: 5241.8003  LR: 0.000006  \n","Epoch: [1][3100/3575] Elapsed 14m 44s (remain 2m 15s) Loss: 0.0032(0.0087) Grad: 16001.1660  LR: 0.000005  \n","Epoch: [1][3200/3575] Elapsed 15m 10s (remain 1m 46s) Loss: 0.0037(0.0086) Grad: 11922.8760  LR: 0.000005  \n","Epoch: [1][3300/3575] Elapsed 15m 37s (remain 1m 17s) Loss: 0.0024(0.0084) Grad: 7720.5020  LR: 0.000004  \n","Epoch: [1][3400/3575] Elapsed 16m 4s (remain 0m 49s) Loss: 0.0002(0.0083) Grad: 1543.9176  LR: 0.000004  \n","Epoch: [1][3500/3575] Elapsed 16m 30s (remain 0m 20s) Loss: 0.0027(0.0082) Grad: 6487.9111  LR: 0.000003  \n","Epoch: [1][3574/3575] Elapsed 16m 50s (remain 0m 0s) Loss: 0.0046(0.0081) Grad: 24119.6582  LR: 0.000003  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 14m 54s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 9s (remain 1m 38s) Loss: 0.0129(0.0038) \n","EVAL: [200/1192] Elapsed 0m 17s (remain 1m 25s) Loss: 0.0025(0.0043) \n","EVAL: [300/1192] Elapsed 0m 25s (remain 1m 15s) Loss: 0.0092(0.0042) \n","EVAL: [400/1192] Elapsed 0m 34s (remain 1m 7s) Loss: 0.0002(0.0045) \n","EVAL: [500/1192] Elapsed 0m 42s (remain 0m 58s) Loss: 0.0000(0.0042) \n","EVAL: [600/1192] Elapsed 0m 50s (remain 0m 49s) Loss: 0.0022(0.0042) \n","EVAL: [700/1192] Elapsed 0m 59s (remain 0m 41s) Loss: 0.0037(0.0046) \n","EVAL: [800/1192] Elapsed 1m 7s (remain 0m 32s) Loss: 0.0000(0.0045) \n","EVAL: [900/1192] Elapsed 1m 15s (remain 0m 24s) Loss: 0.0008(0.0047) \n","EVAL: [1000/1192] Elapsed 1m 24s (remain 0m 16s) Loss: 0.0006(0.0046) \n","EVAL: [1100/1192] Elapsed 1m 32s (remain 0m 7s) Loss: 0.0018(0.0044) \n","EVAL: [1191/1192] Elapsed 1m 40s (remain 0m 0s) Loss: 0.0002(0.0042) \n","Epoch 1 - avg_train_loss: 0.0081  avg_val_loss: 0.0042  time: 1117s\n","Epoch 1 - Score: 0.8765\n","Epoch 1 - Save Best Score: 0.8765 Model\n","Epoch: [2][0/3575] Elapsed 0m 0s (remain 52m 16s) Loss: 0.0004(0.0004) Grad: 3861.3650  LR: 0.000003  \n","Epoch: [2][100/3575] Elapsed 0m 31s (remain 18m 11s) Loss: 0.0000(0.0026) Grad: 121.3465  LR: 0.000002  \n","Epoch: [2][200/3575] Elapsed 0m 57s (remain 16m 2s) Loss: 0.0095(0.0036) Grad: 26627.6543  LR: 0.000002  \n","Epoch: [2][300/3575] Elapsed 1m 21s (remain 14m 43s) Loss: 0.0003(0.0037) Grad: 1063.4636  LR: 0.000002  \n","Epoch: [2][400/3575] Elapsed 1m 45s (remain 13m 52s) Loss: 0.0003(0.0035) Grad: 1422.7488  LR: 0.000001  \n","Epoch: [2][500/3575] Elapsed 2m 9s (remain 13m 12s) Loss: 0.0003(0.0037) Grad: 2507.6570  LR: 0.000001  \n","Epoch: [2][600/3575] Elapsed 2m 32s (remain 12m 36s) Loss: 0.0000(0.0038) Grad: 219.7528  LR: 0.000001  \n","Epoch: [2][700/3575] Elapsed 2m 56s (remain 12m 4s) Loss: 0.0105(0.0037) Grad: 16997.7227  LR: 0.000001  \n","Epoch: [2][800/3575] Elapsed 3m 20s (remain 11m 35s) Loss: 0.0022(0.0037) Grad: 7790.1123  LR: 0.000000  \n","Epoch: [2][900/3575] Elapsed 3m 44s (remain 11m 6s) Loss: 0.0016(0.0036) Grad: 4531.8599  LR: 0.000000  \n","Epoch: [2][1000/3575] Elapsed 4m 8s (remain 10m 38s) Loss: 0.0000(0.0036) Grad: 527.9542  LR: 0.000000  \n","Epoch: [2][1100/3575] Elapsed 4m 32s (remain 10m 11s) Loss: 0.0010(0.0036) Grad: 5979.4766  LR: 0.000000  \n","Epoch: [2][1200/3575] Elapsed 4m 56s (remain 9m 45s) Loss: 0.0007(0.0034) Grad: 3466.7498  LR: 0.000000  \n","Epoch: [2][1300/3575] Elapsed 5m 20s (remain 9m 19s) Loss: 0.0001(0.0035) Grad: 575.5200  LR: 0.000000  \n","Epoch: [2][1400/3575] Elapsed 5m 44s (remain 8m 53s) Loss: 0.0003(0.0034) Grad: 2595.1216  LR: 0.000000  \n","Epoch: [2][1500/3575] Elapsed 6m 7s (remain 8m 28s) Loss: 0.0034(0.0034) Grad: 7473.6924  LR: 0.000000  \n","Epoch: [2][1600/3575] Elapsed 6m 31s (remain 8m 2s) Loss: 0.0033(0.0034) Grad: 8106.7202  LR: 0.000000  \n","Epoch: [2][1700/3575] Elapsed 6m 55s (remain 7m 37s) Loss: 0.0002(0.0035) Grad: 1564.5452  LR: 0.000001  \n","Epoch: [2][1800/3575] Elapsed 7m 19s (remain 7m 12s) Loss: 0.0004(0.0035) Grad: 1450.3732  LR: 0.000001  \n","Epoch: [2][1900/3575] Elapsed 7m 43s (remain 6m 47s) Loss: 0.0005(0.0034) Grad: 4557.1577  LR: 0.000001  \n","Epoch: [2][2000/3575] Elapsed 8m 7s (remain 6m 23s) Loss: 0.0022(0.0035) Grad: 41233.9805  LR: 0.000001  \n","Epoch: [2][2100/3575] Elapsed 8m 31s (remain 5m 58s) Loss: 0.0098(0.0035) Grad: 39888.2344  LR: 0.000002  \n","Epoch: [2][2200/3575] Elapsed 8m 54s (remain 5m 33s) Loss: 0.0111(0.0034) Grad: 86453.5703  LR: 0.000002  \n","Epoch: [2][2300/3575] Elapsed 9m 18s (remain 5m 9s) Loss: 0.0000(0.0034) Grad: 262.9839  LR: 0.000003  \n","Epoch: [2][2400/3575] Elapsed 9m 42s (remain 4m 44s) Loss: 0.0009(0.0034) Grad: 13582.4082  LR: 0.000003  \n","Epoch: [2][2500/3575] Elapsed 10m 6s (remain 4m 20s) Loss: 0.0000(0.0034) Grad: 792.5795  LR: 0.000004  \n","Epoch: [2][2600/3575] Elapsed 10m 30s (remain 3m 56s) Loss: 0.0000(0.0034) Grad: 726.4684  LR: 0.000004  \n","Epoch: [2][2700/3575] Elapsed 10m 54s (remain 3m 31s) Loss: 0.0004(0.0034) Grad: 9136.7637  LR: 0.000005  \n","Epoch: [2][2800/3575] Elapsed 11m 18s (remain 3m 7s) Loss: 0.0084(0.0034) Grad: 61921.8242  LR: 0.000005  \n","Epoch: [2][2900/3575] Elapsed 11m 42s (remain 2m 43s) Loss: 0.0203(0.0034) Grad: 273785.4688  LR: 0.000006  \n","Epoch: [2][3000/3575] Elapsed 12m 6s (remain 2m 18s) Loss: 0.0000(0.0034) Grad: 191.4068  LR: 0.000006  \n","Epoch: [2][3100/3575] Elapsed 12m 29s (remain 1m 54s) Loss: 0.0004(0.0034) Grad: 3923.3281  LR: 0.000007  \n","Epoch: [2][3200/3575] Elapsed 12m 53s (remain 1m 30s) Loss: 0.0000(0.0035) Grad: 184.3373  LR: 0.000008  \n","Epoch: [2][3300/3575] Elapsed 13m 17s (remain 1m 6s) Loss: 0.0000(0.0034) Grad: 124.0827  LR: 0.000008  \n","Epoch: [2][3400/3575] Elapsed 13m 41s (remain 0m 42s) Loss: 0.0072(0.0035) Grad: 53282.4805  LR: 0.000009  \n","Epoch: [2][3500/3575] Elapsed 14m 5s (remain 0m 17s) Loss: 0.0002(0.0035) Grad: 3836.5398  LR: 0.000010  \n","Epoch: [2][3574/3575] Elapsed 14m 22s (remain 0m 0s) Loss: 0.0009(0.0035) Grad: 8021.8896  LR: 0.000010  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 9m 51s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 8s (remain 1m 33s) Loss: 0.0214(0.0056) \n","EVAL: [200/1192] Elapsed 0m 16s (remain 1m 22s) Loss: 0.0029(0.0061) \n","EVAL: [300/1192] Elapsed 0m 25s (remain 1m 14s) Loss: 0.0093(0.0059) \n","EVAL: [400/1192] Elapsed 0m 33s (remain 1m 5s) Loss: 0.0000(0.0063) \n","EVAL: [500/1192] Elapsed 0m 41s (remain 0m 57s) Loss: 0.0000(0.0058) \n","EVAL: [600/1192] Elapsed 0m 50s (remain 0m 49s) Loss: 0.0030(0.0058) \n","EVAL: [700/1192] Elapsed 0m 58s (remain 0m 40s) Loss: 0.0054(0.0062) \n","EVAL: [800/1192] Elapsed 1m 6s (remain 0m 32s) Loss: 0.0000(0.0061) \n","EVAL: [900/1192] Elapsed 1m 14s (remain 0m 24s) Loss: 0.0034(0.0063) \n","EVAL: [1000/1192] Elapsed 1m 22s (remain 0m 15s) Loss: 0.0000(0.0062) \n","EVAL: [1100/1192] Elapsed 1m 31s (remain 0m 7s) Loss: 0.0027(0.0059) \n","EVAL: [1191/1192] Elapsed 1m 38s (remain 0m 0s) Loss: 0.0001(0.0056) \n","Epoch 2 - avg_train_loss: 0.0035  avg_val_loss: 0.0056  time: 966s\n","Epoch 2 - Score: 0.8755\n","Epoch: [3][0/3575] Elapsed 0m 0s (remain 42m 15s) Loss: 0.0034(0.0034) Grad: 12372.2939  LR: 0.000010  \n","Epoch: [3][100/3575] Elapsed 0m 24s (remain 14m 8s) Loss: 0.0006(0.0035) Grad: 4217.9824  LR: 0.000011  \n","Epoch: [3][200/3575] Elapsed 0m 48s (remain 13m 31s) Loss: 0.0002(0.0036) Grad: 1674.0416  LR: 0.000011  \n","Epoch: [3][300/3575] Elapsed 1m 12s (remain 13m 3s) Loss: 0.0045(0.0038) Grad: 15872.0430  LR: 0.000012  \n","Epoch: [3][400/3575] Elapsed 1m 35s (remain 12m 37s) Loss: 0.0000(0.0037) Grad: 270.7491  LR: 0.000013  \n","Epoch: [3][500/3575] Elapsed 1m 59s (remain 12m 12s) Loss: 0.0002(0.0038) Grad: 3191.4336  LR: 0.000013  \n","Epoch: [3][600/3575] Elapsed 2m 22s (remain 11m 47s) Loss: 0.0012(0.0039) Grad: 4246.8994  LR: 0.000014  \n","Epoch: [3][700/3575] Elapsed 2m 46s (remain 11m 22s) Loss: 0.0016(0.0039) Grad: 13163.1963  LR: 0.000014  \n","Epoch: [3][800/3575] Elapsed 3m 10s (remain 10m 59s) Loss: 0.0007(0.0039) Grad: 5072.1040  LR: 0.000015  \n","Epoch: [3][900/3575] Elapsed 3m 33s (remain 10m 35s) Loss: 0.0001(0.0040) Grad: 1151.5398  LR: 0.000016  \n","Epoch: [3][1000/3575] Elapsed 3m 57s (remain 10m 11s) Loss: 0.0000(0.0040) Grad: 389.7557  LR: 0.000016  \n","Epoch: [3][1100/3575] Elapsed 4m 21s (remain 9m 47s) Loss: 0.1088(0.0041) Grad: 184558.1250  LR: 0.000017  \n","Epoch: [3][1200/3575] Elapsed 4m 44s (remain 9m 23s) Loss: 0.0053(0.0040) Grad: 22263.5176  LR: 0.000017  \n","Epoch: [3][1300/3575] Elapsed 5m 8s (remain 8m 59s) Loss: 0.0114(0.0040) Grad: 19673.7422  LR: 0.000018  \n","Epoch: [3][1400/3575] Elapsed 5m 32s (remain 8m 35s) Loss: 0.0000(0.0040) Grad: 161.9646  LR: 0.000018  \n","Epoch: [3][1500/3575] Elapsed 5m 56s (remain 8m 12s) Loss: 0.0015(0.0040) Grad: 9003.6123  LR: 0.000018  \n","Epoch: [3][1600/3575] Elapsed 6m 20s (remain 7m 48s) Loss: 0.0095(0.0040) Grad: 67780.0000  LR: 0.000019  \n","Epoch: [3][1700/3575] Elapsed 6m 43s (remain 7m 24s) Loss: 0.0137(0.0040) Grad: 46833.5625  LR: 0.000019  \n","Epoch: [3][1800/3575] Elapsed 7m 7s (remain 7m 1s) Loss: 0.0010(0.0040) Grad: 16845.2930  LR: 0.000019  \n","Epoch: [3][1900/3575] Elapsed 7m 31s (remain 6m 37s) Loss: 0.0058(0.0040) Grad: 14116.9824  LR: 0.000020  \n","Epoch: [3][2000/3575] Elapsed 7m 55s (remain 6m 13s) Loss: 0.0110(0.0041) Grad: 38931.0312  LR: 0.000020  \n","Epoch: [3][2100/3575] Elapsed 8m 18s (remain 5m 50s) Loss: 0.0020(0.0040) Grad: 17083.3789  LR: 0.000020  \n","Epoch: [3][2200/3575] Elapsed 8m 42s (remain 5m 26s) Loss: 0.0067(0.0040) Grad: 34150.1953  LR: 0.000020  \n","Epoch: [3][2300/3575] Elapsed 9m 6s (remain 5m 2s) Loss: 0.0008(0.0041) Grad: 13154.6377  LR: 0.000020  \n","Epoch: [3][2400/3575] Elapsed 9m 30s (remain 4m 38s) Loss: 0.0093(0.0041) Grad: 32074.0312  LR: 0.000020  \n","Epoch: [3][2500/3575] Elapsed 9m 53s (remain 4m 15s) Loss: 0.0000(0.0041) Grad: 193.3785  LR: 0.000020  \n","Epoch: [3][2600/3575] Elapsed 10m 17s (remain 3m 51s) Loss: 0.0007(0.0040) Grad: 7939.8623  LR: 0.000020  \n","Epoch: [3][2700/3575] Elapsed 10m 41s (remain 3m 27s) Loss: 0.0000(0.0041) Grad: 215.6536  LR: 0.000020  \n","Epoch: [3][2800/3575] Elapsed 11m 5s (remain 3m 3s) Loss: 0.0000(0.0040) Grad: 227.8708  LR: 0.000020  \n","Epoch: [3][2900/3575] Elapsed 11m 28s (remain 2m 40s) Loss: 0.0055(0.0041) Grad: 33974.7148  LR: 0.000019  \n","Epoch: [3][3000/3575] Elapsed 11m 52s (remain 2m 16s) Loss: 0.0652(0.0041) Grad: 273088.1250  LR: 0.000019  \n","Epoch: [3][3100/3575] Elapsed 12m 16s (remain 1m 52s) Loss: 0.0011(0.0041) Grad: 6827.5986  LR: 0.000019  \n","Epoch: [3][3200/3575] Elapsed 12m 39s (remain 1m 28s) Loss: 0.0000(0.0041) Grad: 241.2649  LR: 0.000019  \n","Epoch: [3][3300/3575] Elapsed 13m 3s (remain 1m 5s) Loss: 0.0098(0.0041) Grad: 50065.2695  LR: 0.000018  \n","Epoch: [3][3400/3575] Elapsed 13m 27s (remain 0m 41s) Loss: 0.0015(0.0041) Grad: 25507.9336  LR: 0.000018  \n","Epoch: [3][3500/3575] Elapsed 13m 50s (remain 0m 17s) Loss: 0.0025(0.0041) Grad: 36590.0898  LR: 0.000017  \n","Epoch: [3][3574/3575] Elapsed 14m 8s (remain 0m 0s) Loss: 0.0000(0.0040) Grad: 452.9436  LR: 0.000017  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 10m 1s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 8s (remain 1m 33s) Loss: 0.0270(0.0060) \n","EVAL: [200/1192] Elapsed 0m 16s (remain 1m 22s) Loss: 0.0031(0.0057) \n","EVAL: [300/1192] Elapsed 0m 24s (remain 1m 13s) Loss: 0.0073(0.0052) \n","EVAL: [400/1192] Elapsed 0m 32s (remain 1m 4s) Loss: 0.0002(0.0054) \n","EVAL: [500/1192] Elapsed 0m 41s (remain 0m 56s) Loss: 0.0000(0.0051) \n","EVAL: [600/1192] Elapsed 0m 49s (remain 0m 48s) Loss: 0.0058(0.0051) \n","EVAL: [700/1192] Elapsed 0m 57s (remain 0m 40s) Loss: 0.0057(0.0057) \n","EVAL: [800/1192] Elapsed 1m 5s (remain 0m 31s) Loss: 0.0000(0.0056) \n","EVAL: [900/1192] Elapsed 1m 13s (remain 0m 23s) Loss: 0.0007(0.0057) \n","EVAL: [1000/1192] Elapsed 1m 21s (remain 0m 15s) Loss: 0.0000(0.0056) \n","EVAL: [1100/1192] Elapsed 1m 29s (remain 0m 7s) Loss: 0.0018(0.0053) \n","EVAL: [1191/1192] Elapsed 1m 37s (remain 0m 0s) Loss: 0.0000(0.0050) \n","Epoch 3 - avg_train_loss: 0.0040  avg_val_loss: 0.0050  time: 950s\n","Epoch 3 - Score: 0.8811\n","Epoch 3 - Save Best Score: 0.8811 Model\n","Epoch: [4][0/3575] Elapsed 0m 0s (remain 45m 19s) Loss: 0.0000(0.0000) Grad: 226.4492  LR: 0.000017  \n","Epoch: [4][100/3575] Elapsed 0m 29s (remain 17m 1s) Loss: 0.0050(0.0036) Grad: 8314.7666  LR: 0.000017  \n","Epoch: [4][200/3575] Elapsed 0m 55s (remain 15m 28s) Loss: 0.0000(0.0031) Grad: 198.7523  LR: 0.000016  \n","Epoch: [4][300/3575] Elapsed 1m 18s (remain 14m 16s) Loss: 0.0053(0.0028) Grad: 9157.6709  LR: 0.000016  \n","Epoch: [4][400/3575] Elapsed 1m 42s (remain 13m 29s) Loss: 0.0012(0.0026) Grad: 6484.4819  LR: 0.000015  \n","Epoch: [4][500/3575] Elapsed 2m 5s (remain 12m 52s) Loss: 0.0001(0.0025) Grad: 517.1547  LR: 0.000014  \n","Epoch: [4][600/3575] Elapsed 2m 29s (remain 12m 18s) Loss: 0.0000(0.0027) Grad: 344.9731  LR: 0.000014  \n","Epoch: [4][700/3575] Elapsed 2m 52s (remain 11m 48s) Loss: 0.0008(0.0029) Grad: 12771.6250  LR: 0.000013  \n","Epoch: [4][800/3575] Elapsed 3m 16s (remain 11m 19s) Loss: 0.0026(0.0029) Grad: 5469.3594  LR: 0.000013  \n","Epoch: [4][900/3575] Elapsed 3m 39s (remain 10m 52s) Loss: 0.0004(0.0029) Grad: 4087.2092  LR: 0.000012  \n","Epoch: [4][1000/3575] Elapsed 4m 3s (remain 10m 25s) Loss: 0.0017(0.0029) Grad: 7352.6201  LR: 0.000011  \n","Epoch: [4][1100/3575] Elapsed 4m 26s (remain 9m 59s) Loss: 0.0001(0.0029) Grad: 726.9121  LR: 0.000011  \n","Epoch: [4][1200/3575] Elapsed 4m 50s (remain 9m 33s) Loss: 0.0148(0.0029) Grad: 34543.8047  LR: 0.000010  \n","Epoch: [4][1300/3575] Elapsed 5m 13s (remain 9m 8s) Loss: 0.0000(0.0029) Grad: 257.2793  LR: 0.000009  \n","Epoch: [4][1400/3575] Elapsed 5m 37s (remain 8m 43s) Loss: 0.0098(0.0029) Grad: 19263.6680  LR: 0.000009  \n","Epoch: [4][1500/3575] Elapsed 6m 0s (remain 8m 18s) Loss: 0.0028(0.0029) Grad: 9637.9922  LR: 0.000008  \n","Epoch: [4][1600/3575] Elapsed 6m 24s (remain 7m 53s) Loss: 0.0003(0.0030) Grad: 3183.0591  LR: 0.000007  \n","Epoch: [4][1700/3575] Elapsed 6m 47s (remain 7m 29s) Loss: 0.0039(0.0030) Grad: 9842.1387  LR: 0.000007  \n","Epoch: [4][1800/3575] Elapsed 7m 11s (remain 7m 4s) Loss: 0.0015(0.0030) Grad: 7932.1719  LR: 0.000006  \n","Epoch: [4][1900/3575] Elapsed 7m 35s (remain 6m 40s) Loss: 0.0012(0.0030) Grad: 7524.5649  LR: 0.000005  \n","Epoch: [4][2000/3575] Elapsed 7m 58s (remain 6m 16s) Loss: 0.0000(0.0030) Grad: 544.7972  LR: 0.000005  \n","Epoch: [4][2100/3575] Elapsed 8m 22s (remain 5m 52s) Loss: 0.0000(0.0030) Grad: 256.5711  LR: 0.000004  \n","Epoch: [4][2200/3575] Elapsed 8m 45s (remain 5m 28s) Loss: 0.0001(0.0030) Grad: 915.4788  LR: 0.000004  \n","Epoch: [4][2300/3575] Elapsed 9m 9s (remain 5m 4s) Loss: 0.0067(0.0030) Grad: 41572.6680  LR: 0.000003  \n","Epoch: [4][2400/3575] Elapsed 9m 33s (remain 4m 40s) Loss: 0.0001(0.0029) Grad: 1502.7683  LR: 0.000003  \n","Epoch: [4][2500/3575] Elapsed 9m 56s (remain 4m 16s) Loss: 0.0040(0.0030) Grad: 71065.2266  LR: 0.000002  \n","Epoch: [4][2600/3575] Elapsed 10m 20s (remain 3m 52s) Loss: 0.0016(0.0029) Grad: 9925.1465  LR: 0.000002  \n","Epoch: [4][2700/3575] Elapsed 10m 43s (remain 3m 28s) Loss: 0.0000(0.0029) Grad: 230.6071  LR: 0.000002  \n","Epoch: [4][2800/3575] Elapsed 11m 7s (remain 3m 4s) Loss: 0.0054(0.0029) Grad: 42892.0508  LR: 0.000001  \n","Epoch: [4][2900/3575] Elapsed 11m 30s (remain 2m 40s) Loss: 0.0000(0.0030) Grad: 231.8274  LR: 0.000001  \n","Epoch: [4][3000/3575] Elapsed 11m 54s (remain 2m 16s) Loss: 0.0000(0.0030) Grad: 286.0145  LR: 0.000001  \n","Epoch: [4][3100/3575] Elapsed 12m 18s (remain 1m 52s) Loss: 0.0000(0.0030) Grad: 113.4885  LR: 0.000000  \n","Epoch: [4][3200/3575] Elapsed 12m 41s (remain 1m 28s) Loss: 0.0000(0.0030) Grad: 177.2871  LR: 0.000000  \n","Epoch: [4][3300/3575] Elapsed 13m 5s (remain 1m 5s) Loss: 0.0000(0.0030) Grad: 211.8449  LR: 0.000000  \n","Epoch: [4][3400/3575] Elapsed 13m 28s (remain 0m 41s) Loss: 0.0000(0.0030) Grad: 252.9247  LR: 0.000000  \n","Epoch: [4][3500/3575] Elapsed 13m 52s (remain 0m 17s) Loss: 0.0001(0.0030) Grad: 1554.2383  LR: 0.000000  \n","Epoch: [4][3574/3575] Elapsed 14m 10s (remain 0m 0s) Loss: 0.0173(0.0030) Grad: 127255.9844  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 9m 31s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 8s (remain 1m 34s) Loss: 0.0221(0.0058) \n","EVAL: [200/1192] Elapsed 0m 16s (remain 1m 23s) Loss: 0.0056(0.0054) \n","EVAL: [300/1192] Elapsed 0m 25s (remain 1m 14s) Loss: 0.0040(0.0049) \n","EVAL: [400/1192] Elapsed 0m 33s (remain 1m 5s) Loss: 0.0002(0.0052) \n","EVAL: [500/1192] Elapsed 0m 43s (remain 0m 59s) Loss: 0.0000(0.0048) \n","EVAL: [600/1192] Elapsed 0m 53s (remain 0m 53s) Loss: 0.0106(0.0050) \n","EVAL: [700/1192] Elapsed 1m 2s (remain 0m 43s) Loss: 0.0039(0.0056) \n","EVAL: [800/1192] Elapsed 1m 10s (remain 0m 34s) Loss: 0.0000(0.0055) \n","EVAL: [900/1192] Elapsed 1m 18s (remain 0m 25s) Loss: 0.0015(0.0057) \n","EVAL: [1000/1192] Elapsed 1m 27s (remain 0m 16s) Loss: 0.0000(0.0056) \n","EVAL: [1100/1192] Elapsed 1m 35s (remain 0m 7s) Loss: 0.0123(0.0053) \n","EVAL: [1191/1192] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0000(0.0050) \n","Epoch 4 - avg_train_loss: 0.0030  avg_val_loss: 0.0050  time: 958s\n","Epoch 4 - Score: 0.8849\n","Epoch 4 - Save Best Score: 0.8849 Model\n","========== fold: 3 training ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-10.bin\n","Epoch: [1][0/3575] Elapsed 0m 1s (remain 70m 0s) Loss: 0.2155(0.2155) Grad: inf  LR: 0.000020  \n","Epoch: [1][100/3575] Elapsed 0m 32s (remain 18m 48s) Loss: 0.0314(0.0404) Grad: 8313.4336  LR: 0.000020  \n","Epoch: [1][200/3575] Elapsed 1m 4s (remain 17m 59s) Loss: 0.0038(0.0304) Grad: 6781.2041  LR: 0.000020  \n","Epoch: [1][300/3575] Elapsed 1m 38s (remain 17m 50s) Loss: 0.0021(0.0245) Grad: 2494.7764  LR: 0.000020  \n","Epoch: [1][400/3575] Elapsed 2m 9s (remain 17m 7s) Loss: 0.0054(0.0211) Grad: 4801.3149  LR: 0.000020  \n","Epoch: [1][500/3575] Elapsed 2m 42s (remain 16m 38s) Loss: 0.0005(0.0190) Grad: 1018.6940  LR: 0.000019  \n","Epoch: [1][600/3575] Elapsed 3m 14s (remain 16m 0s) Loss: 0.0037(0.0174) Grad: 5754.3423  LR: 0.000019  \n","Epoch: [1][700/3575] Elapsed 3m 45s (remain 15m 24s) Loss: 0.0002(0.0162) Grad: 777.3763  LR: 0.000019  \n","Epoch: [1][800/3575] Elapsed 4m 17s (remain 14m 50s) Loss: 0.0016(0.0152) Grad: 2068.5139  LR: 0.000019  \n","Epoch: [1][900/3575] Elapsed 4m 48s (remain 14m 16s) Loss: 0.0025(0.0144) Grad: 2398.8086  LR: 0.000018  \n","Epoch: [1][1000/3575] Elapsed 5m 19s (remain 13m 42s) Loss: 0.0026(0.0137) Grad: 2209.8513  LR: 0.000018  \n","Epoch: [1][1100/3575] Elapsed 5m 51s (remain 13m 9s) Loss: 0.0051(0.0131) Grad: 9938.9629  LR: 0.000017  \n","Epoch: [1][1200/3575] Elapsed 6m 21s (remain 12m 33s) Loss: 0.0068(0.0125) Grad: 4787.2393  LR: 0.000017  \n","Epoch: [1][1300/3575] Elapsed 6m 45s (remain 11m 48s) Loss: 0.0053(0.0120) Grad: 5365.9082  LR: 0.000017  \n","Epoch: [1][1400/3575] Elapsed 7m 8s (remain 11m 5s) Loss: 0.0018(0.0116) Grad: 2696.3406  LR: 0.000016  \n","Epoch: [1][1500/3575] Elapsed 7m 32s (remain 10m 24s) Loss: 0.0146(0.0113) Grad: 39701.0234  LR: 0.000015  \n","Epoch: [1][1600/3575] Elapsed 7m 55s (remain 9m 46s) Loss: 0.0006(0.0110) Grad: 970.6685  LR: 0.000015  \n","Epoch: [1][1700/3575] Elapsed 8m 19s (remain 9m 10s) Loss: 0.0073(0.0106) Grad: 5507.2783  LR: 0.000014  \n","Epoch: [1][1800/3575] Elapsed 8m 42s (remain 8m 35s) Loss: 0.0000(0.0103) Grad: 157.8246  LR: 0.000014  \n","Epoch: [1][1900/3575] Elapsed 9m 6s (remain 8m 1s) Loss: 0.0061(0.0100) Grad: 4170.1982  LR: 0.000013  \n","Epoch: [1][2000/3575] Elapsed 9m 30s (remain 7m 28s) Loss: 0.0048(0.0097) Grad: 10662.6758  LR: 0.000012  \n","Epoch: [1][2100/3575] Elapsed 9m 53s (remain 6m 56s) Loss: 0.0015(0.0096) Grad: 3363.6157  LR: 0.000012  \n","Epoch: [1][2200/3575] Elapsed 10m 17s (remain 6m 25s) Loss: 0.0064(0.0094) Grad: 18279.2715  LR: 0.000011  \n","Epoch: [1][2300/3575] Elapsed 10m 41s (remain 5m 54s) Loss: 0.0001(0.0092) Grad: 619.8618  LR: 0.000011  \n","Epoch: [1][2400/3575] Elapsed 11m 4s (remain 5m 24s) Loss: 0.0018(0.0091) Grad: 2697.8237  LR: 0.000010  \n","Epoch: [1][2500/3575] Elapsed 11m 28s (remain 4m 55s) Loss: 0.0020(0.0090) Grad: 5188.4355  LR: 0.000009  \n","Epoch: [1][2600/3575] Elapsed 11m 51s (remain 4m 26s) Loss: 0.0002(0.0088) Grad: 524.5359  LR: 0.000009  \n","Epoch: [1][2700/3575] Elapsed 12m 15s (remain 3m 58s) Loss: 0.0000(0.0086) Grad: 200.9989  LR: 0.000008  \n","Epoch: [1][2800/3575] Elapsed 12m 38s (remain 3m 29s) Loss: 0.0008(0.0085) Grad: 1589.3518  LR: 0.000007  \n","Epoch: [1][2900/3575] Elapsed 13m 2s (remain 3m 1s) Loss: 0.0004(0.0083) Grad: 2105.6658  LR: 0.000007  \n","Epoch: [1][3000/3575] Elapsed 13m 26s (remain 2m 34s) Loss: 0.0002(0.0081) Grad: 572.3486  LR: 0.000006  \n","Epoch: [1][3100/3575] Elapsed 13m 49s (remain 2m 6s) Loss: 0.0003(0.0080) Grad: 909.8498  LR: 0.000005  \n","Epoch: [1][3200/3575] Elapsed 14m 13s (remain 1m 39s) Loss: 0.0080(0.0078) Grad: 13012.9355  LR: 0.000005  \n","Epoch: [1][3300/3575] Elapsed 14m 36s (remain 1m 12s) Loss: 0.0002(0.0077) Grad: 938.2348  LR: 0.000004  \n","Epoch: [1][3400/3575] Elapsed 15m 0s (remain 0m 46s) Loss: 0.0343(0.0076) Grad: 89490.8984  LR: 0.000004  \n","Epoch: [1][3500/3575] Elapsed 15m 23s (remain 0m 19s) Loss: 0.0028(0.0076) Grad: 4324.3433  LR: 0.000003  \n","Epoch: [1][3574/3575] Elapsed 15m 41s (remain 0m 0s) Loss: 0.0001(0.0075) Grad: 192.1246  LR: 0.000003  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 12m 58s) Loss: 0.0002(0.0002) \n","EVAL: [100/1192] Elapsed 0m 8s (remain 1m 36s) Loss: 0.0129(0.0039) \n","EVAL: [200/1192] Elapsed 0m 17s (remain 1m 24s) Loss: 0.0035(0.0034) \n","EVAL: [300/1192] Elapsed 0m 25s (remain 1m 15s) Loss: 0.0023(0.0035) \n","EVAL: [400/1192] Elapsed 0m 33s (remain 1m 6s) Loss: 0.0002(0.0036) \n","EVAL: [500/1192] Elapsed 0m 41s (remain 0m 57s) Loss: 0.0078(0.0035) \n","EVAL: [600/1192] Elapsed 0m 50s (remain 0m 49s) Loss: 0.0016(0.0037) \n","EVAL: [700/1192] Elapsed 0m 58s (remain 0m 40s) Loss: 0.0028(0.0041) \n","EVAL: [800/1192] Elapsed 1m 6s (remain 0m 32s) Loss: 0.0062(0.0041) \n","EVAL: [900/1192] Elapsed 1m 14s (remain 0m 24s) Loss: 0.0017(0.0041) \n","EVAL: [1000/1192] Elapsed 1m 22s (remain 0m 15s) Loss: 0.0001(0.0039) \n","EVAL: [1100/1192] Elapsed 1m 31s (remain 0m 7s) Loss: 0.0069(0.0038) \n","EVAL: [1191/1192] Elapsed 1m 38s (remain 0m 0s) Loss: 0.0000(0.0037) \n","Epoch 1 - avg_train_loss: 0.0075  avg_val_loss: 0.0037  time: 1046s\n","Epoch 1 - Score: 0.8749\n","Epoch 1 - Save Best Score: 0.8749 Model\n","Epoch: [2][0/3575] Elapsed 0m 0s (remain 51m 5s) Loss: 0.0169(0.0169) Grad: 49144.0469  LR: 0.000003  \n","Epoch: [2][100/3575] Elapsed 0m 28s (remain 16m 27s) Loss: 0.0000(0.0033) Grad: 196.9888  LR: 0.000002  \n","Epoch: [2][200/3575] Elapsed 0m 55s (remain 15m 25s) Loss: 0.0125(0.0033) Grad: 18988.0117  LR: 0.000002  \n","Epoch: [2][300/3575] Elapsed 1m 18s (remain 14m 16s) Loss: 0.0008(0.0037) Grad: 4720.3335  LR: 0.000002  \n","Epoch: [2][400/3575] Elapsed 1m 42s (remain 13m 31s) Loss: 0.0019(0.0037) Grad: 17666.4316  LR: 0.000001  \n","Epoch: [2][500/3575] Elapsed 2m 6s (remain 12m 54s) Loss: 0.0255(0.0037) Grad: 20232.3965  LR: 0.000001  \n","Epoch: [2][600/3575] Elapsed 2m 29s (remain 12m 21s) Loss: 0.0018(0.0036) Grad: 5580.2617  LR: 0.000001  \n","Epoch: [2][700/3575] Elapsed 2m 53s (remain 11m 51s) Loss: 0.0019(0.0036) Grad: 8115.4897  LR: 0.000001  \n","Epoch: [2][800/3575] Elapsed 3m 17s (remain 11m 22s) Loss: 0.0044(0.0036) Grad: 10431.6182  LR: 0.000000  \n","Epoch: [2][900/3575] Elapsed 3m 40s (remain 10m 55s) Loss: 0.0012(0.0036) Grad: 5075.6001  LR: 0.000000  \n","Epoch: [2][1000/3575] Elapsed 4m 4s (remain 10m 28s) Loss: 0.0004(0.0035) Grad: 2772.6323  LR: 0.000000  \n","Epoch: [2][1100/3575] Elapsed 4m 28s (remain 10m 2s) Loss: 0.0014(0.0036) Grad: 11487.4863  LR: 0.000000  \n","Epoch: [2][1200/3575] Elapsed 4m 51s (remain 9m 36s) Loss: 0.0040(0.0036) Grad: 6104.0503  LR: 0.000000  \n","Epoch: [2][1300/3575] Elapsed 5m 15s (remain 9m 11s) Loss: 0.0076(0.0036) Grad: 12573.9053  LR: 0.000000  \n","Epoch: [2][1400/3575] Elapsed 5m 39s (remain 8m 46s) Loss: 0.0458(0.0036) Grad: 45638.6406  LR: 0.000000  \n","Epoch: [2][1500/3575] Elapsed 6m 2s (remain 8m 21s) Loss: 0.0001(0.0035) Grad: 807.8109  LR: 0.000000  \n","Epoch: [2][1600/3575] Elapsed 6m 26s (remain 7m 56s) Loss: 0.0046(0.0035) Grad: 10955.4531  LR: 0.000000  \n","Epoch: [2][1700/3575] Elapsed 6m 50s (remain 7m 31s) Loss: 0.0003(0.0035) Grad: 3392.2043  LR: 0.000001  \n","Epoch: [2][1800/3575] Elapsed 7m 13s (remain 7m 7s) Loss: 0.0005(0.0035) Grad: 3092.4570  LR: 0.000001  \n","Epoch: [2][1900/3575] Elapsed 7m 37s (remain 6m 42s) Loss: 0.0000(0.0035) Grad: 221.2433  LR: 0.000001  \n","Epoch: [2][2000/3575] Elapsed 8m 1s (remain 6m 18s) Loss: 0.0000(0.0035) Grad: 657.2551  LR: 0.000001  \n","Epoch: [2][2100/3575] Elapsed 8m 24s (remain 5m 54s) Loss: 0.0000(0.0035) Grad: 534.6736  LR: 0.000002  \n","Epoch: [2][2200/3575] Elapsed 8m 48s (remain 5m 29s) Loss: 0.0001(0.0035) Grad: 962.3484  LR: 0.000002  \n","Epoch: [2][2300/3575] Elapsed 9m 12s (remain 5m 5s) Loss: 0.0088(0.0035) Grad: 22228.2598  LR: 0.000003  \n","Epoch: [2][2400/3575] Elapsed 9m 35s (remain 4m 41s) Loss: 0.0101(0.0035) Grad: 46529.9102  LR: 0.000003  \n","Epoch: [2][2500/3575] Elapsed 9m 59s (remain 4m 17s) Loss: 0.0017(0.0035) Grad: 8651.3906  LR: 0.000004  \n","Epoch: [2][2600/3575] Elapsed 10m 23s (remain 3m 53s) Loss: 0.0014(0.0034) Grad: 6892.7944  LR: 0.000004  \n","Epoch: [2][2700/3575] Elapsed 10m 47s (remain 3m 29s) Loss: 0.0003(0.0034) Grad: 2644.4226  LR: 0.000005  \n","Epoch: [2][2800/3575] Elapsed 11m 10s (remain 3m 5s) Loss: 0.0007(0.0034) Grad: 5536.1904  LR: 0.000005  \n","Epoch: [2][2900/3575] Elapsed 11m 34s (remain 2m 41s) Loss: 0.0000(0.0034) Grad: 225.9857  LR: 0.000006  \n","Epoch: [2][3000/3575] Elapsed 11m 58s (remain 2m 17s) Loss: 0.0093(0.0034) Grad: 28314.3535  LR: 0.000006  \n","Epoch: [2][3100/3575] Elapsed 12m 21s (remain 1m 53s) Loss: 0.0002(0.0034) Grad: 1390.1211  LR: 0.000007  \n","Epoch: [2][3200/3575] Elapsed 12m 45s (remain 1m 29s) Loss: 0.0000(0.0034) Grad: 306.5185  LR: 0.000008  \n","Epoch: [2][3300/3575] Elapsed 13m 9s (remain 1m 5s) Loss: 0.0000(0.0034) Grad: 157.3475  LR: 0.000008  \n","Epoch: [2][3400/3575] Elapsed 13m 32s (remain 0m 41s) Loss: 0.0025(0.0034) Grad: 25132.2305  LR: 0.000009  \n","Epoch: [2][3500/3575] Elapsed 13m 56s (remain 0m 17s) Loss: 0.0001(0.0034) Grad: 1752.0861  LR: 0.000010  \n","Epoch: [2][3574/3575] Elapsed 14m 13s (remain 0m 0s) Loss: 0.0005(0.0034) Grad: 4872.1514  LR: 0.000010  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 10m 25s) Loss: 0.0002(0.0002) \n","EVAL: [100/1192] Elapsed 0m 8s (remain 1m 35s) Loss: 0.0147(0.0048) \n","EVAL: [200/1192] Elapsed 0m 17s (remain 1m 24s) Loss: 0.0216(0.0046) \n","EVAL: [300/1192] Elapsed 0m 25s (remain 1m 15s) Loss: 0.0045(0.0047) \n","EVAL: [400/1192] Elapsed 0m 33s (remain 1m 6s) Loss: 0.0002(0.0045) \n","EVAL: [500/1192] Elapsed 0m 42s (remain 0m 57s) Loss: 0.0088(0.0045) \n","EVAL: [600/1192] Elapsed 0m 50s (remain 0m 49s) Loss: 0.0017(0.0047) \n","EVAL: [700/1192] Elapsed 0m 58s (remain 0m 40s) Loss: 0.0029(0.0050) \n","EVAL: [800/1192] Elapsed 1m 6s (remain 0m 32s) Loss: 0.0062(0.0051) \n","EVAL: [900/1192] Elapsed 1m 14s (remain 0m 24s) Loss: 0.0021(0.0050) \n","EVAL: [1000/1192] Elapsed 1m 23s (remain 0m 15s) Loss: 0.0000(0.0048) \n","EVAL: [1100/1192] Elapsed 1m 31s (remain 0m 7s) Loss: 0.0082(0.0046) \n","EVAL: [1191/1192] Elapsed 1m 38s (remain 0m 0s) Loss: 0.0000(0.0045) \n","Epoch 2 - avg_train_loss: 0.0034  avg_val_loss: 0.0045  time: 958s\n","Epoch 2 - Score: 0.8739\n","Epoch: [3][0/3575] Elapsed 0m 0s (remain 41m 12s) Loss: 0.0009(0.0009) Grad: 5264.3252  LR: 0.000010  \n","Epoch: [3][100/3575] Elapsed 0m 24s (remain 14m 1s) Loss: 0.0064(0.0043) Grad: 22115.0488  LR: 0.000011  \n","Epoch: [3][200/3575] Elapsed 0m 48s (remain 13m 28s) Loss: 0.0018(0.0039) Grad: 6061.0757  LR: 0.000011  \n","Epoch: [3][300/3575] Elapsed 1m 11s (remain 13m 1s) Loss: 0.0182(0.0037) Grad: 46408.4531  LR: 0.000012  \n","Epoch: [3][400/3575] Elapsed 1m 35s (remain 12m 36s) Loss: 0.0013(0.0038) Grad: 6703.2764  LR: 0.000013  \n","Epoch: [3][500/3575] Elapsed 1m 59s (remain 12m 11s) Loss: 0.0000(0.0040) Grad: 257.8348  LR: 0.000013  \n","Epoch: [3][600/3575] Elapsed 2m 22s (remain 11m 47s) Loss: 0.0000(0.0040) Grad: 202.3307  LR: 0.000014  \n","Epoch: [3][700/3575] Elapsed 2m 46s (remain 11m 23s) Loss: 0.0001(0.0038) Grad: 1372.8746  LR: 0.000014  \n","Epoch: [3][800/3575] Elapsed 3m 10s (remain 10m 59s) Loss: 0.0113(0.0037) Grad: 23944.9668  LR: 0.000015  \n","Epoch: [3][900/3575] Elapsed 3m 33s (remain 10m 35s) Loss: 0.0003(0.0039) Grad: 3553.9282  LR: 0.000016  \n","Epoch: [3][1000/3575] Elapsed 3m 57s (remain 10m 11s) Loss: 0.0002(0.0039) Grad: 926.4080  LR: 0.000016  \n","Epoch: [3][1100/3575] Elapsed 4m 21s (remain 9m 47s) Loss: 0.0000(0.0039) Grad: 370.4937  LR: 0.000017  \n","Epoch: [3][1200/3575] Elapsed 4m 44s (remain 9m 23s) Loss: 0.0001(0.0039) Grad: 341.4129  LR: 0.000017  \n","Epoch: [3][1300/3575] Elapsed 5m 8s (remain 8m 59s) Loss: 0.0002(0.0039) Grad: 4185.6841  LR: 0.000018  \n","Epoch: [3][1400/3575] Elapsed 5m 32s (remain 8m 35s) Loss: 0.0001(0.0039) Grad: 302.0961  LR: 0.000018  \n","Epoch: [3][1500/3575] Elapsed 5m 56s (remain 8m 11s) Loss: 0.0005(0.0041) Grad: 1662.2267  LR: 0.000018  \n","Epoch: [3][1600/3575] Elapsed 6m 19s (remain 7m 48s) Loss: 0.0212(0.0040) Grad: 48917.3789  LR: 0.000019  \n","Epoch: [3][1700/3575] Elapsed 6m 43s (remain 7m 24s) Loss: 0.0018(0.0040) Grad: 4364.2979  LR: 0.000019  \n","Epoch: [3][1800/3575] Elapsed 7m 6s (remain 7m 0s) Loss: 0.0006(0.0040) Grad: 4254.0269  LR: 0.000019  \n","Epoch: [3][1900/3575] Elapsed 7m 30s (remain 6m 36s) Loss: 0.0006(0.0040) Grad: 1057.5829  LR: 0.000020  \n","Epoch: [3][2000/3575] Elapsed 7m 54s (remain 6m 12s) Loss: 0.0000(0.0040) Grad: 226.4217  LR: 0.000020  \n","Epoch: [3][2100/3575] Elapsed 8m 17s (remain 5m 49s) Loss: 0.0110(0.0040) Grad: 14050.2334  LR: 0.000020  \n","Epoch: [3][2200/3575] Elapsed 8m 41s (remain 5m 25s) Loss: 0.0022(0.0040) Grad: 7034.1704  LR: 0.000020  \n","Epoch: [3][2300/3575] Elapsed 9m 4s (remain 5m 1s) Loss: 0.0089(0.0040) Grad: 6204.9824  LR: 0.000020  \n","Epoch: [3][2400/3575] Elapsed 9m 28s (remain 4m 37s) Loss: 0.0028(0.0040) Grad: 29869.0488  LR: 0.000020  \n","Epoch: [3][2500/3575] Elapsed 9m 52s (remain 4m 14s) Loss: 0.0004(0.0040) Grad: 1235.5771  LR: 0.000020  \n","Epoch: [3][2600/3575] Elapsed 10m 15s (remain 3m 50s) Loss: 0.0001(0.0040) Grad: 915.2938  LR: 0.000020  \n","Epoch: [3][2700/3575] Elapsed 10m 39s (remain 3m 26s) Loss: 0.0211(0.0040) Grad: 17235.0410  LR: 0.000020  \n","Epoch: [3][2800/3575] Elapsed 11m 3s (remain 3m 3s) Loss: 0.0003(0.0040) Grad: 2963.5688  LR: 0.000020  \n","Epoch: [3][2900/3575] Elapsed 11m 26s (remain 2m 39s) Loss: 0.0007(0.0040) Grad: 3741.5366  LR: 0.000019  \n","Epoch: [3][3000/3575] Elapsed 11m 50s (remain 2m 15s) Loss: 0.0049(0.0040) Grad: 7759.3169  LR: 0.000019  \n","Epoch: [3][3100/3575] Elapsed 12m 13s (remain 1m 52s) Loss: 0.0000(0.0040) Grad: 210.1556  LR: 0.000019  \n","Epoch: [3][3200/3575] Elapsed 12m 37s (remain 1m 28s) Loss: 0.0167(0.0040) Grad: 26461.1250  LR: 0.000019  \n","Epoch: [3][3300/3575] Elapsed 13m 0s (remain 1m 4s) Loss: 0.0012(0.0040) Grad: 4502.7988  LR: 0.000018  \n","Epoch: [3][3400/3575] Elapsed 13m 24s (remain 0m 41s) Loss: 0.0000(0.0040) Grad: 155.9964  LR: 0.000018  \n","Epoch: [3][3500/3575] Elapsed 13m 48s (remain 0m 17s) Loss: 0.0007(0.0040) Grad: 3782.8298  LR: 0.000017  \n","Epoch: [3][3574/3575] Elapsed 14m 5s (remain 0m 0s) Loss: 0.0000(0.0040) Grad: 435.9857  LR: 0.000017  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 9m 57s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 8s (remain 1m 34s) Loss: 0.0237(0.0044) \n","EVAL: [200/1192] Elapsed 0m 16s (remain 1m 23s) Loss: 0.0027(0.0040) \n","EVAL: [300/1192] Elapsed 0m 25s (remain 1m 14s) Loss: 0.0023(0.0042) \n","EVAL: [400/1192] Elapsed 0m 33s (remain 1m 5s) Loss: 0.0000(0.0044) \n","EVAL: [500/1192] Elapsed 0m 41s (remain 0m 56s) Loss: 0.0128(0.0042) \n","EVAL: [600/1192] Elapsed 0m 49s (remain 0m 48s) Loss: 0.0019(0.0043) \n","EVAL: [700/1192] Elapsed 0m 57s (remain 0m 40s) Loss: 0.0020(0.0046) \n","EVAL: [800/1192] Elapsed 1m 5s (remain 0m 31s) Loss: 0.0056(0.0046) \n","EVAL: [900/1192] Elapsed 1m 13s (remain 0m 23s) Loss: 0.0008(0.0046) \n","EVAL: [1000/1192] Elapsed 1m 21s (remain 0m 15s) Loss: 0.0000(0.0044) \n","EVAL: [1100/1192] Elapsed 1m 30s (remain 0m 7s) Loss: 0.0157(0.0043) \n","EVAL: [1191/1192] Elapsed 1m 37s (remain 0m 0s) Loss: 0.0000(0.0043) \n","Epoch 3 - avg_train_loss: 0.0040  avg_val_loss: 0.0043  time: 948s\n","Epoch 3 - Score: 0.8820\n","Epoch 3 - Save Best Score: 0.8820 Model\n","Epoch: [4][0/3575] Elapsed 0m 0s (remain 46m 3s) Loss: 0.0002(0.0002) Grad: 1460.0573  LR: 0.000017  \n","Epoch: [4][100/3575] Elapsed 0m 29s (remain 16m 46s) Loss: 0.0001(0.0032) Grad: 803.6637  LR: 0.000017  \n","Epoch: [4][200/3575] Elapsed 0m 55s (remain 15m 27s) Loss: 0.0015(0.0030) Grad: 10036.6777  LR: 0.000016  \n","Epoch: [4][300/3575] Elapsed 1m 18s (remain 14m 17s) Loss: 0.0000(0.0030) Grad: 251.1888  LR: 0.000016  \n","Epoch: [4][400/3575] Elapsed 1m 42s (remain 13m 32s) Loss: 0.0011(0.0031) Grad: 5375.5317  LR: 0.000015  \n","Epoch: [4][500/3575] Elapsed 2m 6s (remain 12m 54s) Loss: 0.0053(0.0032) Grad: 16053.9219  LR: 0.000014  \n","Epoch: [4][600/3575] Elapsed 2m 29s (remain 12m 21s) Loss: 0.0000(0.0031) Grad: 294.4317  LR: 0.000014  \n","Epoch: [4][700/3575] Elapsed 2m 53s (remain 11m 50s) Loss: 0.0097(0.0031) Grad: 14578.2676  LR: 0.000013  \n","Epoch: [4][800/3575] Elapsed 3m 16s (remain 11m 22s) Loss: 0.0023(0.0031) Grad: 10110.9043  LR: 0.000013  \n","Epoch: [4][900/3575] Elapsed 3m 40s (remain 10m 54s) Loss: 0.0000(0.0032) Grad: 564.9640  LR: 0.000012  \n","Epoch: [4][1000/3575] Elapsed 4m 4s (remain 10m 27s) Loss: 0.0004(0.0032) Grad: 1765.3898  LR: 0.000011  \n","Epoch: [4][1100/3575] Elapsed 4m 27s (remain 10m 1s) Loss: 0.0000(0.0031) Grad: 177.1285  LR: 0.000011  \n","Epoch: [4][1200/3575] Elapsed 4m 51s (remain 9m 35s) Loss: 0.0000(0.0031) Grad: 225.8779  LR: 0.000010  \n","Epoch: [4][1300/3575] Elapsed 5m 14s (remain 9m 10s) Loss: 0.0189(0.0030) Grad: 179871.3438  LR: 0.000009  \n","Epoch: [4][1400/3575] Elapsed 5m 38s (remain 8m 45s) Loss: 0.0014(0.0030) Grad: 6925.9165  LR: 0.000009  \n","Epoch: [4][1500/3575] Elapsed 6m 2s (remain 8m 20s) Loss: 0.0560(0.0030) Grad: 80708.9375  LR: 0.000008  \n","Epoch: [4][1600/3575] Elapsed 6m 25s (remain 7m 55s) Loss: 0.0000(0.0030) Grad: 289.3636  LR: 0.000007  \n","Epoch: [4][1700/3575] Elapsed 6m 49s (remain 7m 30s) Loss: 0.0000(0.0029) Grad: 208.8670  LR: 0.000007  \n","Epoch: [4][1800/3575] Elapsed 7m 12s (remain 7m 6s) Loss: 0.0000(0.0029) Grad: 363.0729  LR: 0.000006  \n","Epoch: [4][1900/3575] Elapsed 7m 36s (remain 6m 41s) Loss: 0.0000(0.0029) Grad: 135.7643  LR: 0.000005  \n","Epoch: [4][2000/3575] Elapsed 7m 59s (remain 6m 17s) Loss: 0.0000(0.0029) Grad: 213.0430  LR: 0.000005  \n","Epoch: [4][2100/3575] Elapsed 8m 23s (remain 5m 53s) Loss: 0.0082(0.0029) Grad: 45276.9375  LR: 0.000004  \n","Epoch: [4][2200/3575] Elapsed 8m 46s (remain 5m 28s) Loss: 0.0053(0.0028) Grad: 29357.8789  LR: 0.000004  \n","Epoch: [4][2300/3575] Elapsed 9m 10s (remain 5m 4s) Loss: 0.0033(0.0028) Grad: 26895.6973  LR: 0.000003  \n","Epoch: [4][2400/3575] Elapsed 9m 34s (remain 4m 40s) Loss: 0.0113(0.0029) Grad: 16332.9189  LR: 0.000003  \n","Epoch: [4][2500/3575] Elapsed 9m 57s (remain 4m 16s) Loss: 0.0016(0.0029) Grad: 5824.1206  LR: 0.000002  \n","Epoch: [4][2600/3575] Elapsed 10m 21s (remain 3m 52s) Loss: 0.0001(0.0028) Grad: 1097.6987  LR: 0.000002  \n","Epoch: [4][2700/3575] Elapsed 10m 44s (remain 3m 28s) Loss: 0.0000(0.0028) Grad: 155.3139  LR: 0.000002  \n","Epoch: [4][2800/3575] Elapsed 11m 8s (remain 3m 4s) Loss: 0.0000(0.0028) Grad: 226.3060  LR: 0.000001  \n","Epoch: [4][2900/3575] Elapsed 11m 32s (remain 2m 40s) Loss: 0.0000(0.0028) Grad: 279.5766  LR: 0.000001  \n","Epoch: [4][3000/3575] Elapsed 11m 55s (remain 2m 16s) Loss: 0.0038(0.0028) Grad: 9224.7158  LR: 0.000001  \n","Epoch: [4][3100/3575] Elapsed 12m 19s (remain 1m 53s) Loss: 0.0198(0.0028) Grad: 22169.3008  LR: 0.000000  \n","Epoch: [4][3200/3575] Elapsed 12m 42s (remain 1m 29s) Loss: 0.0077(0.0028) Grad: 31806.6797  LR: 0.000000  \n","Epoch: [4][3300/3575] Elapsed 13m 6s (remain 1m 5s) Loss: 0.0000(0.0028) Grad: 271.2307  LR: 0.000000  \n","Epoch: [4][3400/3575] Elapsed 13m 29s (remain 0m 41s) Loss: 0.0000(0.0028) Grad: 236.0369  LR: 0.000000  \n","Epoch: [4][3500/3575] Elapsed 13m 53s (remain 0m 17s) Loss: 0.0000(0.0027) Grad: 118.0695  LR: 0.000000  \n","Epoch: [4][3574/3575] Elapsed 14m 10s (remain 0m 0s) Loss: 0.0000(0.0027) Grad: 232.8284  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 9m 43s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 8s (remain 1m 32s) Loss: 0.0177(0.0044) \n","EVAL: [200/1192] Elapsed 0m 16s (remain 1m 22s) Loss: 0.0027(0.0036) \n","EVAL: [300/1192] Elapsed 0m 24s (remain 1m 13s) Loss: 0.0030(0.0038) \n","EVAL: [400/1192] Elapsed 0m 33s (remain 1m 5s) Loss: 0.0000(0.0041) \n","EVAL: [500/1192] Elapsed 0m 41s (remain 0m 57s) Loss: 0.0149(0.0039) \n","EVAL: [600/1192] Elapsed 0m 49s (remain 0m 48s) Loss: 0.0026(0.0042) \n","EVAL: [700/1192] Elapsed 0m 57s (remain 0m 40s) Loss: 0.0023(0.0049) \n","EVAL: [800/1192] Elapsed 1m 5s (remain 0m 32s) Loss: 0.0030(0.0049) \n","EVAL: [900/1192] Elapsed 1m 13s (remain 0m 23s) Loss: 0.0019(0.0049) \n","EVAL: [1000/1192] Elapsed 1m 22s (remain 0m 15s) Loss: 0.0000(0.0047) \n","EVAL: [1100/1192] Elapsed 1m 30s (remain 0m 7s) Loss: 0.0123(0.0046) \n","EVAL: [1191/1192] Elapsed 1m 37s (remain 0m 0s) Loss: 0.0000(0.0045) \n","Epoch 4 - avg_train_loss: 0.0027  avg_val_loss: 0.0045  time: 953s\n","Epoch 4 - Score: 0.8874\n","Epoch 4 - Save Best Score: 0.8874 Model\n","Best thres: 0.5, Score: 0.8844\n","Best thres: 0.500390625, Score: 0.8844\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72c9b77bde1f468ca443805a7c8ea3a8"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dee94ec8145541c2ad74d9c9f97dd8a7"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5303600af42f4fa29d5ef8f34c1e1716"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9347608935e4c70a7fad0068c044b8b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n"]}],"source":["if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"name":"nbme-exp078.ipynb","provenance":[{"file_id":"1VyA5CJlM6l099SW-rHcp0GctebaNOB_0","timestamp":1649299212239},{"file_id":"1RX_ZvZAkBkJKpYjOf4JkbUw7fDIvFDvK","timestamp":1649243946413},{"file_id":"10yG4L3_nzpdL2CDwqxa9r-KWq6jYkWfl","timestamp":1649164439720}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"papermill":{"default_parameters":{},"duration":641.572862,"end_time":"2022-02-27T11:39:50.972497","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-02-27T11:29:09.399635","version":"2.3.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"378da5e010f14bf6869c523abb981bf3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_615f7da8d47b4ef89bc038318991956c","IPY_MODEL_164d61b927a64380957f02f03769b6b2","IPY_MODEL_17236a1945f447248da88aaedc35b744"],"layout":"IPY_MODEL_a885ccb0411347d888b84f74cf841dcf"}},"615f7da8d47b4ef89bc038318991956c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9dc0bb2ee474f10ba1ca64bc2a3b2db","placeholder":"​","style":"IPY_MODEL_4059661ff71b48078b79e58be44bccba","value":"100%"}},"164d61b927a64380957f02f03769b6b2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_003245542732434984fbbb6eaa1da2bb","max":42146,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8c03385d5fec428eaf97579017baa9c8","value":42146}},"17236a1945f447248da88aaedc35b744":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4296a8a61494cb999da02449e79609d","placeholder":"​","style":"IPY_MODEL_5c4d7b1a6a734e46920bfc5549a52b04","value":" 42146/42146 [00:35&lt;00:00, 1987.89it/s]"}},"a885ccb0411347d888b84f74cf841dcf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9dc0bb2ee474f10ba1ca64bc2a3b2db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4059661ff71b48078b79e58be44bccba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"003245542732434984fbbb6eaa1da2bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c03385d5fec428eaf97579017baa9c8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a4296a8a61494cb999da02449e79609d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c4d7b1a6a734e46920bfc5549a52b04":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a196430b3e814d0e99abab1fcfae6a8c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_43e23125f7454c5a8fd87466ae9154a2","IPY_MODEL_0b40d0c53e8e4664be0a7b18da35925e","IPY_MODEL_fd74d4dddada4d96b5f05ebd48eac498"],"layout":"IPY_MODEL_6ca0062f0fea4eceac7fbaf7cc8bb0c5"}},"43e23125f7454c5a8fd87466ae9154a2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a949132d5de490c97c8606a19025036","placeholder":"​","style":"IPY_MODEL_49e236c4bbeb4dfcb518990831057d30","value":"100%"}},"0b40d0c53e8e4664be0a7b18da35925e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d136e0f56a44e70bd9bc08fff8ffadc","max":143,"min":0,"orientation":"horizontal","style":"IPY_MODEL_174bd5fbe1e14eb69315e6f9bc9dbc5f","value":143}},"fd74d4dddada4d96b5f05ebd48eac498":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a6bd7761c3e43988a22cfd7daa10984","placeholder":"​","style":"IPY_MODEL_b468187903744a5999ea4170182b741c","value":" 143/143 [00:00&lt;00:00, 2494.79it/s]"}},"6ca0062f0fea4eceac7fbaf7cc8bb0c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a949132d5de490c97c8606a19025036":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49e236c4bbeb4dfcb518990831057d30":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8d136e0f56a44e70bd9bc08fff8ffadc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"174bd5fbe1e14eb69315e6f9bc9dbc5f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1a6bd7761c3e43988a22cfd7daa10984":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b468187903744a5999ea4170182b741c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72c9b77bde1f468ca443805a7c8ea3a8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_423b5a5d4f4f40629fd53560de4b9805","IPY_MODEL_df71e8a2c9ad488aa9c5821f20500a90","IPY_MODEL_53257d26a0b44d9a9339dba3f989e5bd"],"layout":"IPY_MODEL_f59e452801d546e9a75ff2ab5c38c9bb"}},"423b5a5d4f4f40629fd53560de4b9805":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8529d1203ccc4f6cb8311e67f7ea3dca","placeholder":"​","style":"IPY_MODEL_0f605c21ed5f4abb8849ea59aeefba00","value":"100%"}},"df71e8a2c9ad488aa9c5821f20500a90":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5c1747fc8fb4cdc8877f7a123671c8a","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fda2d7a7fdd449429a3c53ea2efcc55f","value":2}},"53257d26a0b44d9a9339dba3f989e5bd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6522a937786c4189b13cec4949bf557d","placeholder":"​","style":"IPY_MODEL_874932e818e64bdcaede97854d32868d","value":" 2/2 [00:01&lt;00:00,  1.23s/it]"}},"f59e452801d546e9a75ff2ab5c38c9bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8529d1203ccc4f6cb8311e67f7ea3dca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f605c21ed5f4abb8849ea59aeefba00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d5c1747fc8fb4cdc8877f7a123671c8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fda2d7a7fdd449429a3c53ea2efcc55f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6522a937786c4189b13cec4949bf557d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"874932e818e64bdcaede97854d32868d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dee94ec8145541c2ad74d9c9f97dd8a7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_692eecf29b0549e9a43de8eb43044858","IPY_MODEL_d373dbb712d34255a8d6fea0d584a0f0","IPY_MODEL_91b20518fe6c4d8881dd10f64cf4c71c"],"layout":"IPY_MODEL_96e70e70a15b4763954935de367473c3"}},"692eecf29b0549e9a43de8eb43044858":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_edd00ddc2bac4f1f8f240e87ed4f7105","placeholder":"​","style":"IPY_MODEL_ef987b3f29e64562b0c9e1dd20cea0fd","value":"100%"}},"d373dbb712d34255a8d6fea0d584a0f0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f2f0d5938924082ab15e0d428a09794","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_81e74ebae5e04f2bb2d66e95792506bc","value":2}},"91b20518fe6c4d8881dd10f64cf4c71c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fd77c560c1b4529b3109a11521f93ce","placeholder":"​","style":"IPY_MODEL_3ec203f8aa0c4c3a81d5ad88f48370f8","value":" 2/2 [00:01&lt;00:00,  1.26s/it]"}},"96e70e70a15b4763954935de367473c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"edd00ddc2bac4f1f8f240e87ed4f7105":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef987b3f29e64562b0c9e1dd20cea0fd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f2f0d5938924082ab15e0d428a09794":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81e74ebae5e04f2bb2d66e95792506bc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6fd77c560c1b4529b3109a11521f93ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ec203f8aa0c4c3a81d5ad88f48370f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5303600af42f4fa29d5ef8f34c1e1716":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_08be1a5cab18475191923886eef94105","IPY_MODEL_4800d4887a3846a9b0077f337ad233f9","IPY_MODEL_1376cf0d23ea42cdb93dfc002bf40829"],"layout":"IPY_MODEL_d7895c66d23b4094bae8624b7b5a2d34"}},"08be1a5cab18475191923886eef94105":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_58e4593a23004f53acf61c2b0b3f56ca","placeholder":"​","style":"IPY_MODEL_855ff300af7543d59f68fc4402774cd5","value":"100%"}},"4800d4887a3846a9b0077f337ad233f9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_24b44a09a5c04b48a7947efa869e006e","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0149baa53279452fac4498174bc67b49","value":2}},"1376cf0d23ea42cdb93dfc002bf40829":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d9bfe291cba47939816d51cb6f27f29","placeholder":"​","style":"IPY_MODEL_4e19ab09ce584253af210c2faadb758c","value":" 2/2 [00:01&lt;00:00,  1.37s/it]"}},"d7895c66d23b4094bae8624b7b5a2d34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58e4593a23004f53acf61c2b0b3f56ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"855ff300af7543d59f68fc4402774cd5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24b44a09a5c04b48a7947efa869e006e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0149baa53279452fac4498174bc67b49":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4d9bfe291cba47939816d51cb6f27f29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e19ab09ce584253af210c2faadb758c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9347608935e4c70a7fad0068c044b8b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4b69620ac66d4a2591dcf59c6973af4d","IPY_MODEL_c7959a82499a48dc89ea3aabbe291f1d","IPY_MODEL_504440fe56be486cbcd29a09e0878c05"],"layout":"IPY_MODEL_87fb535eb3f1497b8e1a245d0edabda4"}},"4b69620ac66d4a2591dcf59c6973af4d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_058a80ee4333420cbd2cb585c50199ac","placeholder":"​","style":"IPY_MODEL_8e21fe2e19f04eac9a7696eb407ef13a","value":"100%"}},"c7959a82499a48dc89ea3aabbe291f1d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_233ac86a7fe84505a4636ff6bb00063b","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_db420071b6e644ebab25c572c85426bd","value":2}},"504440fe56be486cbcd29a09e0878c05":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e2f7cea6a7948fa8f7923b84cd64a24","placeholder":"​","style":"IPY_MODEL_9763d8d2f2ac40a398cef57e4fb74ffd","value":" 2/2 [00:02&lt;00:00,  1.55s/it]"}},"87fb535eb3f1497b8e1a245d0edabda4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"058a80ee4333420cbd2cb585c50199ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e21fe2e19f04eac9a7696eb407ef13a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"233ac86a7fe84505a4636ff6bb00063b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db420071b6e644ebab25c572c85426bd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7e2f7cea6a7948fa8f7923b84cd64a24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9763d8d2f2ac40a398cef57e4fb74ffd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}