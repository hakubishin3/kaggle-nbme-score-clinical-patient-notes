{"cells":[{"cell_type":"markdown","metadata":{"id":"blind-kingdom"},"source":["## References"],"id":"blind-kingdom"},{"cell_type":"markdown","metadata":{"id":"antique-glenn"},"source":["- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"],"id":"antique-glenn"},{"cell_type":"markdown","metadata":{"id":"bored-ministry"},"source":["## Configurations"],"id":"bored-ministry"},{"cell_type":"code","execution_count":null,"metadata":{"id":"deadly-confidence"},"outputs":[],"source":["EXP_NAME = \"nbme-exp044\"\n","ENV = \"colab\"\n","DEBUG_MODE = False\n","SUBMISSION_MODE = False"],"id":"deadly-confidence"},{"cell_type":"code","execution_count":null,"metadata":{"id":"aware-worcester"},"outputs":[],"source":["class CFG:\n","    env=ENV\n","    exp_name=EXP_NAME\n","    debug=DEBUG_MODE\n","    submission=SUBMISSION_MODE\n","    apex=True\n","    input_dir=None\n","    output_dir=None\n","    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n","    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n","    competition_name=\"nbme-score-clinical-patient-notes\"\n","    id_col=\"id\"\n","    target_col=\"location\"\n","    pretrained_model_name=\"microsoft/deberta-large\"\n","    tokenizer=None\n","    max_len=None\n","    output_dim=1\n","    dropout=0.2\n","    num_workers=4\n","    batch_size=3\n","    lr=2e-5\n","    betas=(0.9, 0.98)\n","    weight_decay=0.1\n","    num_warmup_steps_rate=0.1\n","    batch_scheduler=True\n","    epochs=5\n","    n_fold=4\n","    train_fold=[0, 1, 2, 3]\n","    seed=71\n","    gradient_accumulation_steps=2\n","    max_grad_norm=1000\n","    print_freq=100\n","    train=True\n","    inference=True"],"id":"aware-worcester"},{"cell_type":"code","execution_count":null,"metadata":{"id":"personalized-death"},"outputs":[],"source":["if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.train_fold = [0, 1]\n","\n","if CFG.submission:\n","    CFG.train = False\n","    CFG.inference = True"],"id":"personalized-death"},{"cell_type":"markdown","metadata":{"id":"cardiovascular-neutral"},"source":["## Directory Settings"],"id":"cardiovascular-neutral"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7168,"status":"ok","timestamp":1647470957711,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"checked-boards","outputId":"dcbd42c1-5716-48b3-f9e1-0574f6da2887"},"outputs":[{"name":"stdout","output_type":"stream","text":["colab\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: transformers==v4.16.2 in /usr/local/lib/python3.7/dist-packages (4.16.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==v4.16.2) (3.6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==v4.16.2) (0.4.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==v4.16.2) (4.11.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==v4.16.2) (4.63.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==v4.16.2) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==v4.16.2) (1.21.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==v4.16.2) (2019.12.20)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==v4.16.2) (6.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==v4.16.2) (0.0.49)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==v4.16.2) (0.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==v4.16.2) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==v4.16.2) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==v4.16.2) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==v4.16.2) (3.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==v4.16.2) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==v4.16.2) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==v4.16.2) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==v4.16.2) (2021.10.8)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==v4.16.2) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==v4.16.2) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==v4.16.2) (7.1.2)\n"]}],"source":["import sys\n","from pathlib import Path\n","\n","\n","print(CFG.env)\n","if CFG.env == \"colab\":\n","    # colab環境\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","    # install packages\n","    !pip install transformers==v4.16.2\n","\n","elif CFG.env == \"local\":\n","    # ローカルサーバ\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","\n","elif CFG.env == \"kaggle\":\n","    # kaggle環境\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./\")"],"id":"checked-boards"},{"cell_type":"code","execution_count":null,"metadata":{"id":"vital-mexico"},"outputs":[],"source":["import gc\n","import os\n","import ast\n","import time\n","import math\n","import random\n","import itertools\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","from scipy.optimize import minimize\n","from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as T\n","from torchvision.io import read_image\n","from torch.utils.data import DataLoader, Dataset\n","\n","from transformers import AutoModelForMaskedLM\n","from transformers import BartModel,BertModel,BertTokenizer\n","from transformers import DebertaModel,DebertaTokenizer\n","from transformers import RobertaModel,RobertaTokenizer\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"id":"vital-mexico"},{"cell_type":"markdown","metadata":{"id":"economic-ladder"},"source":["## Utilities"],"id":"economic-ladder"},{"cell_type":"code","execution_count":null,"metadata":{"id":"desperate-keyboard"},"outputs":[],"source":["def micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on binary arrays.\n","\n","    Args:\n","        preds (list of lists of ints): Predictions.\n","        truths (list of lists of ints): Ground truths.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    # Micro : aggregating over all instances\n","    preds = np.concatenate(preds)\n","    truths = np.concatenate(truths)\n","    return f1_score(truths, preds)\n","\n","\n","def spans_to_binary(spans, length=None):\n","    \"\"\"\n","    Converts spans to a binary array indicating whether each character is in the span.\n","\n","    Args:\n","        spans (list of lists of two ints): Spans.\n","\n","    Returns:\n","        np array [length]: Binarized spans.\n","    \"\"\"\n","    length = np.max(spans) if length is None else length\n","    binary = np.zeros(length)\n","    for start, end in spans:\n","        binary[start:end] = 1\n","    return binary\n","\n","\n","def span_micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on spans.\n","\n","    Args:\n","        preds (list of lists of two ints): Prediction spans.\n","        truths (list of lists of two ints): Ground truth spans.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    bin_preds = []\n","    bin_truths = []\n","    for pred, truth in zip(preds, truths):\n","        if not len(pred) and not len(truth):\n","            continue\n","        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n","        bin_preds.append(spans_to_binary(pred, length))\n","        bin_truths.append(spans_to_binary(truth, length))\n","    return micro_f1(bin_preds, bin_truths)\n","\n","\n","def get_score(y_true, y_pred):\n","    score = span_micro_f1(y_true, y_pred)\n","    return score"],"id":"desperate-keyboard"},{"cell_type":"code","execution_count":null,"metadata":{"id":"flexible-wednesday"},"outputs":[],"source":["def create_labels_for_scoring(df):\n","    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n","    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n","    for i in range(len(df)):\n","        lst = df.loc[i, \"location\"]\n","        if lst:\n","            new_lst = \";\".join(lst)\n","            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n","\n","    # create labels\n","    truths = []\n","    for location_list in df[\"location_for_create_labels\"].values:\n","        truth = []\n","        if len(location_list) > 0:\n","            location = location_list[0]\n","            for loc in [s.split() for s in location.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                truth.append([start, end])\n","        truths.append(truth)\n","\n","    return truths\n","\n","\n","def get_char_probs(texts, token_probs, tokenizer):\n","    res = [np.zeros(len(t)) for t in texts]\n","    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n","        encoded = tokenizer(\n","            text=text,\n","            max_length=CFG.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n","            start, end = offset_mapping\n","            res[i][start:end] = pred\n","    return res\n","\n","\n","def get_predicted_location_str(char_probs, th=0.5):\n","    results = []\n","    for char_prob in char_probs:\n","        result = np.where(char_prob >= th)[0] + 1\n","        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n","        result = [f\"{min(r)} {max(r)}\" for r in result]\n","        result = \";\".join(result)\n","        results.append(result)\n","    return results\n","\n","\n","def get_predictions(results):\n","    predictions = []\n","    for result in results:\n","        prediction = []\n","        if result != \"\":\n","            for loc in [s.split() for s in result.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                prediction.append([start, end])\n","        predictions.append(prediction)\n","    return predictions\n","\n","\n","def scoring(df, th=0.5):\n","    labels = create_labels_for_scoring(df)\n","\n","    token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n","    char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n","    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n","    preds = get_predictions(predicted_location_str)\n","\n","    score = get_score(labels, preds)\n","    return score\n","\n","\n","def get_best_thres(oof_df):\n","    def f1_opt(x):\n","        return -1 * scoring(oof_df, th=x)\n","\n","    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n","    return best_thres"],"id":"flexible-wednesday"},{"cell_type":"code","execution_count":null,"metadata":{"id":"logical-chemistry"},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return \"%dm %ds\" % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"],"id":"logical-chemistry"},{"cell_type":"code","execution_count":null,"metadata":{"id":"gorgeous-record"},"outputs":[],"source":["seed_everything()"],"id":"gorgeous-record"},{"cell_type":"markdown","metadata":{"id":"frozen-africa"},"source":["## Data Loading"],"id":"frozen-africa"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1109,"status":"ok","timestamp":1647470973826,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"shaped-metallic","outputId":"81d4fa82-58e4-4580-921b-e502cea0cbfd"},"outputs":[{"data":{"text/plain":["((14300, 6), (143, 3), (42146, 3), (5, 4))"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["train = pd.read_csv(CFG.input_dir / \"train.csv\")\n","features = pd.read_csv(CFG.input_dir / \"features.csv\")\n","patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n","test = pd.read_csv(CFG.input_dir / \"test.csv\")\n","\n","train.shape, features.shape, patient_notes.shape, test.shape"],"id":"shaped-metallic"},{"cell_type":"code","execution_count":null,"metadata":{"id":"visible-australia"},"outputs":[],"source":["if CFG.debug:\n","    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n","    print(train.shape)"],"id":"visible-australia"},{"cell_type":"markdown","metadata":{"id":"hydraulic-gibson"},"source":["## Preprocessing"],"id":"hydraulic-gibson"},{"cell_type":"code","execution_count":null,"metadata":{"id":"interpreted-northeast"},"outputs":[],"source":["def preprocess_features(features):\n","    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n","    return features\n","\n","\n","features = preprocess_features(features)"],"id":"interpreted-northeast"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":472,"status":"ok","timestamp":1647470974296,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"martial-blind","outputId":"0e2f97ae-460a-49b8-8f58-84e05ec2fb99"},"outputs":[{"data":{"text/plain":["((14300, 8), (5, 6))"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","\n","train.shape, test.shape"],"id":"martial-blind"},{"cell_type":"code","execution_count":null,"metadata":{"id":"electoral-favor"},"outputs":[],"source":["train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n","train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"],"id":"electoral-favor"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1647470974297,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"reported-parade","outputId":"f7ea4f93-4d76-4d45-b648-3777bdf55062"},"outputs":[{"data":{"text/plain":["0    4399\n","1    8181\n","2    1296\n","3     287\n","4      99\n","5      27\n","6       9\n","7       1\n","8       1\n","Name: annotation_length, dtype: int64"]},"metadata":{},"output_type":"display_data"}],"source":["train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n","display(train['annotation_length'].value_counts().sort_index())"],"id":"reported-parade"},{"cell_type":"markdown","metadata":{"id":"enabling-relevance"},"source":["## CV split"],"id":"enabling-relevance"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1647470974298,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"mature-coalition","outputId":"cdd25892-045b-47b2-a10c-7a56bdf60e7d"},"outputs":[{"data":{"text/plain":["fold\n","0    3575\n","1    3575\n","2    3575\n","3    3575\n","dtype: int64"]},"metadata":{},"output_type":"display_data"}],"source":["Fold = GroupKFold(n_splits=CFG.n_fold)\n","groups = train['pn_num'].values\n","for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n","    train.loc[val_index, 'fold'] = int(n)\n","train['fold'] = train['fold'].astype(int)\n","display(train.groupby('fold').size())"],"id":"mature-coalition"},{"cell_type":"markdown","metadata":{"id":"subjective-entrance"},"source":["## Setup tokenizer"],"id":"subjective-entrance"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dramatic-afghanistan"},"outputs":[],"source":["if CFG.submission:\n","    tokenizer = AutoTokenizer.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n","else:\n","    tokenizer = AutoTokenizer.from_pretrained(CFG.pretrained_model_name)\n","    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n","\n","CFG.tokenizer = tokenizer"],"id":"dramatic-afghanistan"},{"cell_type":"markdown","metadata":{"id":"divided-arrow"},"source":["## Create dataset"],"id":"divided-arrow"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["9f7159d6a4b6486c978128b5b4273c0d","c6f29027dadf4413aea8f3f2f89b5f2e","2d367b9961744ebd8afcd407b5f1dfaf","d6e802d0852c4e129f4d80049c86f853","70df2d61ff064679a8643e2695d6a351","105d11ca83ca4294986a3ebfebafc5d3","38676c6cd8dc420abb0c77a7c6597b1e","1dfc701a80674672bf048f4acc2f8db7","6d470e185c4d4755a9d2f1f3459a5bdc","7f2446c46f094cbe97f6d8de06c52452","1cf73f7de8ce41b0aec238f108d5332b"]},"executionInfo":{"elapsed":37387,"status":"ok","timestamp":1647471016267,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"immune-campbell","outputId":"a5bf8113-23a4-4b08-95c2-f6c796fb476f"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9f7159d6a4b6486c978128b5b4273c0d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/42146 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["max length: 433\n"]}],"source":["pn_history_lengths = []\n","tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    pn_history_lengths.append(length)\n","\n","print(\"max length:\", np.max(pn_history_lengths))"],"id":"immune-campbell"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["f6bb2b5c141e4dd9a312ea5053696d0f","69c6130b141545a8800076b853e89879","fa3be47befba4c0b8db44ceff1a5127b","0f5642fca45a4511a672b28cc0ac1c42","cdab58c093cd4cd39249de2048b9f376","0bcde70431ba4780a224fef22d41ed0d","b10a28a1bc82423eb7929259d85c66fd","62c353f8cb1a4d0fbc1b22f7ed49801b","7c7fdf3e9afc4636be256140b514b483","fc12727aca2f45ea94c410f6aa3df9c3","01f31e86e39a4926acfb1a23ddab7f76"]},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1647471016268,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"northern-branch","outputId":"61650752-ada7-4130-b3fd-8a46204e33ca"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f6bb2b5c141e4dd9a312ea5053696d0f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/143 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["max length: 30\n"]}],"source":["feature_text_lengths = []\n","tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    feature_text_lengths.append(length)\n","\n","print(\"max length:\", np.max(feature_text_lengths))"],"id":"northern-branch"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1647471016268,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"oriental-jacksonville","outputId":"59e4dcb4-b928-4d95-f81a-d580161e599d"},"outputs":[{"name":"stdout","output_type":"stream","text":["max length: 466\n"]}],"source":["CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n","\n","print(\"max length:\", CFG.max_len)"],"id":"oriental-jacksonville"},{"cell_type":"code","execution_count":null,"metadata":{"id":"flexible-trainer"},"outputs":[],"source":["class TrainingDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","        self.annotation_lengths = self.df[\"annotation_length\"].values\n","        self.locations = self.df[\"location\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def _create_label(self, pn_history, annotation_length, location_list):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        offset_mapping = encoded[\"offset_mapping\"]\n","        ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n","        label = np.zeros(len(offset_mapping))\n","        label[ignore_idxes] = -1\n","\n","        if annotation_length > 0:\n","            for location in location_list:\n","                for loc in [s.split() for s in location.split(\";\")]:\n","                    start, end = int(loc[0]), int(loc[1])\n","                    start_idx = -1\n","                    end_idx = -1\n","                    for idx in range(len(offset_mapping)):\n","                        if (start_idx == -1) & (start < offset_mapping[idx][0]):\n","                            start_idx = idx - 1\n","                        if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n","                            end_idx = idx + 1\n","                    if start_idx == -1:\n","                        start_idx = end_idx\n","                    if (start_idx != -1) & (end_idx != -1):\n","                        label[start_idx:end_idx] = 1\n","\n","        return torch.tensor(label, dtype=torch.float)\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n","        return input_, label"],"id":"flexible-trainer"},{"cell_type":"code","execution_count":null,"metadata":{"id":"stock-robertson"},"outputs":[],"source":["class TestDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        return input_"],"id":"stock-robertson"},{"cell_type":"markdown","metadata":{"id":"chemical-lucas"},"source":["## Model"],"id":"chemical-lucas"},{"cell_type":"code","execution_count":null,"metadata":{"id":"animated-array"},"outputs":[],"source":["class CustomModel(nn.Module):\n","    def __init__(self, cfg, model_config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        if model_config_path is None:\n","            self.model_config = AutoConfig.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                output_hidden_states=True,\n","            )\n","        else:\n","            self.model_config = torch.load(model_config_path)\n","\n","        if pretrained:\n","            self.backbone = AutoModel.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                config=self.model_config,\n","            )\n","            print(f\"Load weight from pretrained\")\n","        else:\n","            #self.backbone = AutoModel.from_config(self.model_config)\n","            itpt = AutoModelForMaskedLM.from_config(self.model_config)\n","            path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp010/checkpoint-130170/pytorch_model.bin\")\n","            #path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n","            state_dict = torch.load(path)\n","            itpt.load_state_dict(state_dict)\n","            self.backbone = itpt.deberta\n","            print(f\"Load weight from {path}\")\n","\n","        self.fc = nn.Sequential(\n","            nn.Dropout(self.cfg.dropout),\n","            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n","        )\n","\n","    def forward(self, inputs):\n","        h = self.backbone(**inputs)[\"last_hidden_state\"]\n","        output = self.fc(h)\n","        return output"],"id":"animated-array"},{"cell_type":"markdown","metadata":{"id":"thorough-bristol"},"source":["## Training"],"id":"thorough-bristol"},{"cell_type":"code","execution_count":null,"metadata":{"id":"talented-quantity"},"outputs":[],"source":["def train_fn(\n","    train_dataloader,\n","    model,\n","    criterion,\n","    optimizer,\n","    epoch,\n","    scheduler,\n","    device,\n","):\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(train_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","\n","        pos_nums = (labels == 1).sum(axis=1)\n","        pos_nums = torch.tensor([[pos_num] * CFG.max_len for pos_num in pos_nums]).view(-1, 1).to(device)\n","        pos_nums = torch.masked_select(pos_nums, labels.view(-1, 1) != -1)\n","        weight = []\n","        for pos_num in pos_nums:\n","            if pos_num == 0:\n","                weight.append(3.0)\n","            else:\n","                weight.append(1.0)\n","        weight = torch.tensor(weight).to(device)\n","        loss = loss * weight\n","\n","        loss = loss.mean()\n","\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","\n","        if CFG.batch_scheduler:\n","            scheduler.step()\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n","            print(\n","                \"Epoch: [{0}][{1}/{2}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                \"Grad: {grad_norm:.4f}  \"\n","                \"LR: {lr:.6f}  \"\n","                .format(\n","                    epoch+1,\n","                    step,\n","                    len(train_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n","                    loss=losses,\n","                     grad_norm=grad_norm,\n","                     lr=scheduler.get_lr()[0],\n","                )\n","            )\n","    return losses.avg"],"id":"talented-quantity"},{"cell_type":"code","execution_count":null,"metadata":{"id":"figured-cooperative"},"outputs":[],"source":["def valid_fn(\n","    val_dataloader,\n","    model,\n","    criterion,\n","    device,\n","):\n","    model.eval()\n","    preds = []\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(val_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.no_grad():\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","\n","        pos_nums = (labels == 1).sum(axis=1)\n","        pos_nums = torch.tensor([[pos_num] * CFG.max_len for pos_num in pos_nums]).view(-1, 1).to(device)\n","        pos_nums = torch.masked_select(pos_nums, labels.view(-1, 1) != -1)\n","        weight = []\n","        for pos_num in pos_nums:\n","            if pos_num == 0:\n","                weight.append(3.0)\n","            else:\n","                weight.append(1.0)\n","        weight = torch.tensor(weight).to(device)\n","        loss = loss * weight\n","\n","        loss = loss.mean()\n","\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n","            print(\n","                \"EVAL: [{0}/{1}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                .format(\n","                    step, len(val_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n","                    loss=losses,\n","                )\n","            )\n","    preds = np.concatenate(preds)\n","    return losses.avg, preds"],"id":"figured-cooperative"},{"cell_type":"code","execution_count":null,"metadata":{"id":"played-pointer"},"outputs":[],"source":["def inference_fn(test_dataloader, model, device):\n","    model.eval()\n","    model.to(device)\n","    preds = []\n","    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n","    for inputs in tk0:\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        with torch.no_grad():\n","            output = model(inputs)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","    preds = np.concatenate(preds)\n","    return preds"],"id":"played-pointer"},{"cell_type":"code","execution_count":null,"metadata":{"id":"brazilian-nigeria"},"outputs":[],"source":["def train_loop(df, i_fold, device):\n","    print(f\"========== fold: {i_fold} training ==========\")\n","    train_idx = df[df[\"fold\"] != i_fold].index\n","    val_idx = df[df[\"fold\"] == i_fold].index\n","\n","    train_folds = df.loc[train_idx].reset_index(drop=True)\n","    val_folds = df.loc[val_idx].reset_index(drop=True)\n","\n","    train_dataset = TrainingDataset(CFG, train_folds)\n","    val_dataset = TrainingDataset(CFG, val_folds)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=True,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=True,\n","    )\n","    val_dataloader = DataLoader(\n","        val_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=False,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=False,\n","    )\n","\n","    model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","    # model = CustomModel(CFG, model_config_path=None, pretrained=False)   # itptを使うため\n","    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n","    model.to(device)\n","\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\"params\": [p for n, p in param_optimizer if not any(\n","            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n","        {\"params\": [p for n, p in param_optimizer if any(\n","            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n","    ]\n","    optimizer = AdamW(\n","        optimizer_grouped_parameters,\n","        lr=CFG.lr,\n","        betas=CFG.betas,\n","        weight_decay=CFG.weight_decay,\n","    )\n","    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n","    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        num_training_steps=num_train_optimization_steps,\n","    )\n","\n","    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    best_score = -1 * np.inf\n","\n","    for epoch in range(CFG.epochs):\n","        start_time = time.time()\n","        avg_loss = train_fn(\n","            train_dataloader,\n","            model,\n","            criterion,\n","            optimizer,\n","            epoch,\n","            scheduler,\n","            device,\n","        )\n","        avg_val_loss, val_preds = valid_fn(\n","            val_dataloader,\n","            model,\n","            criterion,\n","            device,\n","        )\n","\n","        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n","            scheduler.step()\n","\n","        # scoring\n","        val_folds[[str(i) for i in range(CFG.max_len)]] = val_preds\n","        score = scoring(val_folds, th=0.5)\n","\n","        elapsed = time.time() - start_time\n","\n","        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n","        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n","        if score > best_score:\n","            best_score = score\n","            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n","            torch.save({\n","                \"model\": model.state_dict(),\n","                \"predictions\": val_preds,\n","                },\n","                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","            )\n","\n","    predictions = torch.load(\n","        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","        map_location=torch.device(\"cpu\"),\n","    )[\"predictions\"]\n","    val_folds[[str(i) for i in range(CFG.max_len)]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return val_folds"],"id":"brazilian-nigeria"},{"cell_type":"markdown","metadata":{"id":"bearing-switch"},"source":["## Main"],"id":"bearing-switch"},{"cell_type":"code","execution_count":null,"metadata":{"id":"desperate-crime"},"outputs":[],"source":["def main():\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    if CFG.train:\n","        oof_df = pd.DataFrame()\n","        for i_fold in range(CFG.n_fold):\n","            if i_fold in CFG.train_fold:\n","                _oof_df = train_loop(train, i_fold, device)\n","                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n","        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    if CFG.submission:\n","        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n","    else:\n","        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    score = scoring(oof_df, th=0.5)\n","    print(f\"Best thres: 0.5, Score: {score:.4f}\")\n","    best_thres = get_best_thres(oof_df)\n","    score = scoring(oof_df, th=best_thres)\n","    print(f\"Best thres: {best_thres}, Score: {score:.4f}\")\n","\n","    if CFG.inference:\n","        test_dataset = TestDataset(CFG, test)\n","        test_dataloader = DataLoader(\n","            test_dataset,\n","            batch_size=CFG.batch_size,\n","            shuffle=False,\n","            num_workers=CFG.num_workers,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","        predictions = []\n","        for i_fold in CFG.train_fold:\n","            if CFG.submission:\n","                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n","                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n","            else:\n","                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n","\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            model.load_state_dict(state[\"model\"])\n","            test_token_probs = inference_fn(test_dataloader, model, device)\n","            test[[f\"fold{i_fold}_{i}\" for i in range(CFG.max_len)]] = test_token_probs\n","            test_char_probs = get_char_probs(test[\"pn_history\"].values, test_token_probs, CFG.tokenizer)\n","            predictions.append(test_char_probs)\n","\n","            del state, test_token_probs, model; gc.collect()\n","            torch.cuda.empty_cache()\n","\n","        predictions = np.mean(predictions, axis=0)\n","        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n","        test[CFG.target_col] = predicted_location_str\n","        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n","        test[[CFG.id_col, CFG.target_col]].to_csv(\n","            CFG.output_dir / \"submission.csv\", index=False\n","        )"],"id":"desperate-crime"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["bb0d742cd788453da46ff5b3b5d2d584","444cece8449447b88213c907ad496227","7ba148e9e6154a888174b91d18dce982","db3fdbd9793e495ea654ca1d45ca9f37","5ec28e7595f14e2e9797c9a757a4b67a","d843ce8df38343fc89bd5b994eb93d1d","bc430ad6b2ca41d181f47fd1e3ed05ab","a94e8daedcc84095b902463f9146e4c7","b9ef8573be694e86825d3d88dfc0c520","0498516b744442da867c09ed858853f0","929b5ef4d04847cc92f73e75e13153d2","044ae7b300cf4ef9aff545c35658c34f","3da537198516445eaf53449f4bd25adb","0105ed1939194d89bd23ad507d5e3f65","76b8c91e39d041b3a7afadb2de5a500a","5db9f18c6037458a890a69c91910b6f8","ef23a870721b4d32a21f25db0ce8c9d2","32b365fcccf3449695b8513baadce6fa","23cb906d17704e568f748062c1835ce0","934986cb53ea40198115eae075349d69","6593ba0b19c84c44aac4a6609fdf0742","6ae7880090be4960a3e87e15e9159ab9","342e59616f3c40c2b172b96dd23c800d","02aba7aad7fe45da8da7b1733a911068","fbf9f2c4a5914d22b030ca13dbcdd32c","082e3df201e34bb18c0760399929ec9f","ecfcb2beda4b436891494f5ed4d8a72f","4447f3d5b58043cca53185ddb1eafbaa","70e3470b9ccd4aaf946d7402e7c94f7b","2c1f8730f679418db91d06f8b0037545","5f9ed6e7398843a2bf9069d8bc992387","b82ee2eb90a34425a9ae8bf3b1f2d714","295c9838a6c64e7883133766640cb0cd","a6f49600ddb745f0bbd62d956a5546db","de93b4220b2f4629bc8b39a1035f9807","1b973967650b4d659061f36e6ef141c0","722ce810722d4ef3a604a752b9d96391","feeb704265764a11836a151c44fdd488","1b17d125380943ba8714cf3879aee67f","ed596e634176410ea94c2e987a3a4f17","34183a24824d4ec589a6a1e3998f6289","31a2351ef1184cbd96f704f071971513","3e1b2c72c27f4348b9f006e1a2aa5fe6","ca8ce07a830c44e8a70a75d00f89f2f1"]},"id":"graduate-vision","outputId":"f1cec4e1-c043-43b8-b812-6f994a97308a"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["========== fold: 0 training ==========\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bb0d742cd788453da46ff5b3b5d2d584","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Load weight from pretrained\n","Epoch: [1][0/3575] Elapsed 0m 3s (remain 196m 36s) Loss: 1.0198(1.0198) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/3575] Elapsed 0m 38s (remain 21m 57s) Loss: 0.2511(0.5405) Grad: 41224.5312  LR: 0.000001  \n","Epoch: [1][200/3575] Elapsed 1m 14s (remain 20m 48s) Loss: 0.0190(0.3106) Grad: 1822.5973  LR: 0.000002  \n","Epoch: [1][300/3575] Elapsed 1m 50s (remain 20m 2s) Loss: 0.0231(0.2194) Grad: 2373.2883  LR: 0.000003  \n","Epoch: [1][400/3575] Elapsed 2m 26s (remain 19m 18s) Loss: 0.0241(0.1722) Grad: 17212.8711  LR: 0.000004  \n","Epoch: [1][500/3575] Elapsed 3m 2s (remain 18m 39s) Loss: 0.0426(0.1428) Grad: 13429.3877  LR: 0.000006  \n","Epoch: [1][600/3575] Elapsed 3m 39s (remain 18m 4s) Loss: 0.0078(0.1227) Grad: 4572.4785  LR: 0.000007  \n","Epoch: [1][700/3575] Elapsed 4m 15s (remain 17m 26s) Loss: 0.0119(0.1080) Grad: 2388.7339  LR: 0.000008  \n","Epoch: [1][800/3575] Elapsed 4m 51s (remain 16m 48s) Loss: 0.0035(0.0966) Grad: 23056.2559  LR: 0.000009  \n","Epoch: [1][900/3575] Elapsed 5m 26s (remain 16m 9s) Loss: 0.0108(0.0877) Grad: 6737.8496  LR: 0.000010  \n","Epoch: [1][1000/3575] Elapsed 6m 0s (remain 15m 26s) Loss: 0.0311(0.0803) Grad: 13369.2510  LR: 0.000011  \n","Epoch: [1][1100/3575] Elapsed 6m 35s (remain 14m 48s) Loss: 0.0543(0.0739) Grad: 21547.2734  LR: 0.000012  \n","Epoch: [1][1200/3575] Elapsed 7m 11s (remain 14m 12s) Loss: 0.0026(0.0688) Grad: 1204.1827  LR: 0.000013  \n","Epoch: [1][1300/3575] Elapsed 7m 47s (remain 13m 36s) Loss: 0.0062(0.0646) Grad: 7920.1665  LR: 0.000015  \n","Epoch: [1][1400/3575] Elapsed 8m 23s (remain 13m 0s) Loss: 0.0160(0.0609) Grad: 2378.3210  LR: 0.000016  \n","Epoch: [1][1500/3575] Elapsed 8m 59s (remain 12m 24s) Loss: 0.0061(0.0579) Grad: 710.8962  LR: 0.000017  \n","Epoch: [1][1600/3575] Elapsed 9m 34s (remain 11m 48s) Loss: 0.0374(0.0553) Grad: 13434.1846  LR: 0.000018  \n","Epoch: [1][1700/3575] Elapsed 10m 10s (remain 11m 12s) Loss: 0.0027(0.0528) Grad: 1669.2759  LR: 0.000019  \n","Epoch: [1][1800/3575] Elapsed 10m 44s (remain 10m 34s) Loss: 0.0235(0.0505) Grad: 4993.9766  LR: 0.000020  \n","Epoch: [1][1900/3575] Elapsed 11m 19s (remain 9m 58s) Loss: 0.0254(0.0486) Grad: 6606.7666  LR: 0.000020  \n","Epoch: [1][2000/3575] Elapsed 11m 54s (remain 9m 22s) Loss: 0.0316(0.0468) Grad: 13532.1611  LR: 0.000020  \n","Epoch: [1][2100/3575] Elapsed 12m 30s (remain 8m 46s) Loss: 0.0022(0.0452) Grad: 921.4319  LR: 0.000020  \n","Epoch: [1][2200/3575] Elapsed 13m 6s (remain 8m 10s) Loss: 0.0007(0.0438) Grad: 1249.1140  LR: 0.000019  \n","Epoch: [1][2300/3575] Elapsed 13m 42s (remain 7m 35s) Loss: 0.0538(0.0423) Grad: 14897.1318  LR: 0.000019  \n","Epoch: [1][2400/3575] Elapsed 14m 17s (remain 6m 59s) Loss: 0.0005(0.0410) Grad: 469.8517  LR: 0.000019  \n","Epoch: [1][2500/3575] Elapsed 14m 53s (remain 6m 23s) Loss: 0.0329(0.0398) Grad: 3541.1311  LR: 0.000019  \n","Epoch: [1][2600/3575] Elapsed 15m 27s (remain 5m 47s) Loss: 0.0073(0.0387) Grad: 1964.3527  LR: 0.000019  \n","Epoch: [1][2700/3575] Elapsed 16m 10s (remain 5m 13s) Loss: 0.0798(0.0378) Grad: 13846.3486  LR: 0.000019  \n","Epoch: [1][2800/3575] Elapsed 16m 44s (remain 4m 37s) Loss: 0.0020(0.0368) Grad: 784.3779  LR: 0.000019  \n","Epoch: [1][2900/3575] Elapsed 17m 19s (remain 4m 1s) Loss: 0.0006(0.0359) Grad: 167.7609  LR: 0.000019  \n","Epoch: [1][3000/3575] Elapsed 17m 53s (remain 3m 25s) Loss: 0.0043(0.0350) Grad: 1343.3605  LR: 0.000018  \n","Epoch: [1][3100/3575] Elapsed 18m 28s (remain 2m 49s) Loss: 0.0021(0.0343) Grad: 905.9734  LR: 0.000018  \n","Epoch: [1][3200/3575] Elapsed 19m 3s (remain 2m 13s) Loss: 0.0033(0.0335) Grad: 865.8882  LR: 0.000018  \n","Epoch: [1][3300/3575] Elapsed 19m 37s (remain 1m 37s) Loss: 0.0007(0.0328) Grad: 2115.4456  LR: 0.000018  \n","Epoch: [1][3400/3575] Elapsed 20m 11s (remain 1m 2s) Loss: 0.0001(0.0321) Grad: 54.6297  LR: 0.000018  \n","Epoch: [1][3500/3575] Elapsed 20m 46s (remain 0m 26s) Loss: 0.0224(0.0315) Grad: 3279.5706  LR: 0.000018  \n","Epoch: [1][3574/3575] Elapsed 21m 11s (remain 0m 0s) Loss: 0.0022(0.0311) Grad: 1033.2666  LR: 0.000018  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 12m 7s) Loss: 0.0008(0.0008) \n","EVAL: [100/1192] Elapsed 0m 18s (remain 3m 19s) Loss: 0.0103(0.0087) \n","EVAL: [200/1192] Elapsed 0m 36s (remain 2m 59s) Loss: 0.0047(0.0081) \n","EVAL: [300/1192] Elapsed 0m 54s (remain 2m 40s) Loss: 0.0108(0.0082) \n","EVAL: [400/1192] Elapsed 1m 12s (remain 2m 22s) Loss: 0.0032(0.0087) \n","EVAL: [500/1192] Elapsed 1m 30s (remain 2m 4s) Loss: 0.0054(0.0079) \n","EVAL: [600/1192] Elapsed 1m 47s (remain 1m 46s) Loss: 0.0046(0.0085) \n","EVAL: [700/1192] Elapsed 2m 5s (remain 1m 28s) Loss: 0.0552(0.0102) \n","EVAL: [800/1192] Elapsed 2m 23s (remain 1m 10s) Loss: 0.0010(0.0103) \n","EVAL: [900/1192] Elapsed 2m 41s (remain 0m 52s) Loss: 0.0139(0.0104) \n","EVAL: [1000/1192] Elapsed 2m 59s (remain 0m 34s) Loss: 0.0001(0.0103) \n","EVAL: [1100/1192] Elapsed 3m 17s (remain 0m 16s) Loss: 0.0021(0.0101) \n","EVAL: [1191/1192] Elapsed 3m 33s (remain 0m 0s) Loss: 0.0002(0.0098) \n","Epoch 1 - avg_train_loss: 0.0311  avg_val_loss: 0.0098  time: 1492s\n","Epoch 1 - Score: 0.8310\n","Epoch 1 - Save Best Score: 0.8310 Model\n","Epoch: [2][0/3575] Elapsed 0m 0s (remain 43m 6s) Loss: 0.0203(0.0203) Grad: 13282.2012  LR: 0.000018  \n","Epoch: [2][100/3575] Elapsed 0m 33s (remain 19m 2s) Loss: 0.0695(0.0086) Grad: 42990.0312  LR: 0.000018  \n","Epoch: [2][200/3575] Elapsed 1m 4s (remain 18m 3s) Loss: 0.0010(0.0093) Grad: 6676.3823  LR: 0.000018  \n","Epoch: [2][300/3575] Elapsed 1m 34s (remain 17m 12s) Loss: 0.0001(0.0087) Grad: 391.0167  LR: 0.000017  \n","Epoch: [2][400/3575] Elapsed 2m 5s (remain 16m 32s) Loss: 0.0042(0.0087) Grad: 6971.0996  LR: 0.000017  \n","Epoch: [2][500/3575] Elapsed 2m 35s (remain 15m 55s) Loss: 0.0012(0.0082) Grad: 2886.9441  LR: 0.000017  \n","Epoch: [2][600/3575] Elapsed 3m 6s (remain 15m 21s) Loss: 0.0000(0.0079) Grad: 184.8663  LR: 0.000017  \n","Epoch: [2][700/3575] Elapsed 3m 36s (remain 14m 47s) Loss: 0.0003(0.0078) Grad: 1235.6471  LR: 0.000017  \n","Epoch: [2][800/3575] Elapsed 4m 6s (remain 14m 15s) Loss: 0.0256(0.0080) Grad: 8497.0566  LR: 0.000017  \n","Epoch: [2][900/3575] Elapsed 4m 37s (remain 13m 43s) Loss: 0.0030(0.0078) Grad: 4215.7202  LR: 0.000017  \n","Epoch: [2][1000/3575] Elapsed 5m 7s (remain 13m 11s) Loss: 0.0027(0.0077) Grad: 8038.5596  LR: 0.000017  \n","Epoch: [2][1100/3575] Elapsed 5m 38s (remain 12m 40s) Loss: 0.0060(0.0079) Grad: 5382.1133  LR: 0.000016  \n","Epoch: [2][1200/3575] Elapsed 6m 8s (remain 12m 9s) Loss: 0.0001(0.0077) Grad: 425.1752  LR: 0.000016  \n","Epoch: [2][1300/3575] Elapsed 6m 39s (remain 11m 38s) Loss: 0.0041(0.0077) Grad: 2892.0500  LR: 0.000016  \n","Epoch: [2][1400/3575] Elapsed 7m 9s (remain 11m 7s) Loss: 0.0171(0.0079) Grad: 18410.4609  LR: 0.000016  \n","Epoch: [2][1500/3575] Elapsed 7m 40s (remain 10m 36s) Loss: 0.0001(0.0078) Grad: 98.1841  LR: 0.000016  \n","Epoch: [2][1600/3575] Elapsed 8m 10s (remain 10m 5s) Loss: 0.0014(0.0077) Grad: 4991.6675  LR: 0.000016  \n","Epoch: [2][1700/3575] Elapsed 8m 41s (remain 9m 34s) Loss: 0.0346(0.0079) Grad: 38452.2891  LR: 0.000016  \n","Epoch: [2][1800/3575] Elapsed 9m 11s (remain 9m 3s) Loss: 0.0020(0.0079) Grad: 7001.9541  LR: 0.000016  \n","Epoch: [2][1900/3575] Elapsed 9m 42s (remain 8m 32s) Loss: 0.0000(0.0079) Grad: 67.4438  LR: 0.000015  \n","Epoch: [2][2000/3575] Elapsed 10m 12s (remain 8m 1s) Loss: 0.0007(0.0080) Grad: 1111.0927  LR: 0.000015  \n","Epoch: [2][2100/3575] Elapsed 10m 43s (remain 7m 31s) Loss: 0.0061(0.0081) Grad: 8708.5859  LR: 0.000015  \n","Epoch: [2][2200/3575] Elapsed 11m 13s (remain 7m 0s) Loss: 0.0111(0.0082) Grad: 9597.5986  LR: 0.000015  \n","Epoch: [2][2300/3575] Elapsed 11m 44s (remain 6m 29s) Loss: 0.0003(0.0081) Grad: 696.5956  LR: 0.000015  \n","Epoch: [2][2400/3575] Elapsed 12m 14s (remain 5m 59s) Loss: 0.0136(0.0081) Grad: 7009.7402  LR: 0.000015  \n","Epoch: [2][2500/3575] Elapsed 12m 44s (remain 5m 28s) Loss: 0.0089(0.0081) Grad: 5609.8330  LR: 0.000015  \n","Epoch: [2][2600/3575] Elapsed 13m 15s (remain 4m 57s) Loss: 0.0794(0.0080) Grad: 49367.5586  LR: 0.000015  \n","Epoch: [2][2700/3575] Elapsed 13m 45s (remain 4m 27s) Loss: 0.0351(0.0081) Grad: 17793.9922  LR: 0.000014  \n","Epoch: [2][2800/3575] Elapsed 14m 16s (remain 3m 56s) Loss: 0.0024(0.0080) Grad: 6189.4595  LR: 0.000014  \n","Epoch: [2][2900/3575] Elapsed 14m 46s (remain 3m 25s) Loss: 0.0041(0.0081) Grad: 4713.7280  LR: 0.000014  \n","Epoch: [2][3000/3575] Elapsed 15m 16s (remain 2m 55s) Loss: 0.0127(0.0081) Grad: 2358.4766  LR: 0.000014  \n","Epoch: [2][3100/3575] Elapsed 15m 47s (remain 2m 24s) Loss: 0.0001(0.0080) Grad: 96.6977  LR: 0.000014  \n","Epoch: [2][3200/3575] Elapsed 16m 17s (remain 1m 54s) Loss: 0.0001(0.0080) Grad: 214.5984  LR: 0.000014  \n","Epoch: [2][3300/3575] Elapsed 16m 48s (remain 1m 23s) Loss: 0.0010(0.0079) Grad: 3290.7251  LR: 0.000014  \n","Epoch: [2][3400/3575] Elapsed 17m 18s (remain 0m 53s) Loss: 0.0073(0.0080) Grad: 5884.3252  LR: 0.000014  \n","Epoch: [2][3500/3575] Elapsed 17m 49s (remain 0m 22s) Loss: 0.0000(0.0080) Grad: 73.6015  LR: 0.000013  \n","Epoch: [2][3574/3575] Elapsed 18m 11s (remain 0m 0s) Loss: 0.0001(0.0080) Grad: 88.6384  LR: 0.000013  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 10m 3s) Loss: 0.0004(0.0004) \n","EVAL: [100/1192] Elapsed 0m 18s (remain 3m 18s) Loss: 0.0137(0.0072) \n","EVAL: [200/1192] Elapsed 0m 36s (remain 2m 58s) Loss: 0.0017(0.0067) \n","EVAL: [300/1192] Elapsed 0m 54s (remain 2m 40s) Loss: 0.0150(0.0077) \n","EVAL: [400/1192] Elapsed 1m 12s (remain 2m 22s) Loss: 0.0045(0.0080) \n","EVAL: [500/1192] Elapsed 1m 29s (remain 2m 4s) Loss: 0.0027(0.0074) \n","EVAL: [600/1192] Elapsed 1m 47s (remain 1m 45s) Loss: 0.0005(0.0075) \n","EVAL: [700/1192] Elapsed 2m 5s (remain 1m 27s) Loss: 0.0743(0.0089) \n","EVAL: [800/1192] Elapsed 2m 23s (remain 1m 9s) Loss: 0.0014(0.0090) \n","EVAL: [900/1192] Elapsed 2m 41s (remain 0m 52s) Loss: 0.0097(0.0090) \n","EVAL: [1000/1192] Elapsed 2m 58s (remain 0m 34s) Loss: 0.0001(0.0089) \n","EVAL: [1100/1192] Elapsed 3m 16s (remain 0m 16s) Loss: 0.0011(0.0086) \n","EVAL: [1191/1192] Elapsed 3m 32s (remain 0m 0s) Loss: 0.0001(0.0084) \n","Epoch 2 - avg_train_loss: 0.0080  avg_val_loss: 0.0084  time: 1309s\n","Epoch 2 - Score: 0.8583\n","Epoch 2 - Save Best Score: 0.8583 Model\n","Epoch: [3][0/3575] Elapsed 0m 0s (remain 41m 39s) Loss: 0.0101(0.0101) Grad: 31541.0410  LR: 0.000013  \n","Epoch: [3][100/3575] Elapsed 0m 32s (remain 18m 51s) Loss: 0.0015(0.0080) Grad: 7519.9731  LR: 0.000013  \n","Epoch: [3][200/3575] Elapsed 1m 3s (remain 17m 48s) Loss: 0.0001(0.0072) Grad: 254.9475  LR: 0.000013  \n","Epoch: [3][300/3575] Elapsed 1m 33s (remain 17m 2s) Loss: 0.0030(0.0063) Grad: 4217.5786  LR: 0.000013  \n","Epoch: [3][400/3575] Elapsed 2m 4s (remain 16m 23s) Loss: 0.0000(0.0057) Grad: 81.8499  LR: 0.000013  \n","Epoch: [3][500/3575] Elapsed 2m 34s (remain 15m 48s) Loss: 0.0484(0.0057) Grad: 34330.8125  LR: 0.000013  \n","Epoch: [3][600/3575] Elapsed 3m 4s (remain 15m 14s) Loss: 0.0187(0.0056) Grad: 31544.3340  LR: 0.000013  \n","Epoch: [3][700/3575] Elapsed 3m 35s (remain 14m 42s) Loss: 0.0001(0.0056) Grad: 19817.8320  LR: 0.000012  \n","Epoch: [3][800/3575] Elapsed 4m 5s (remain 14m 10s) Loss: 0.0095(0.0059) Grad: 33238.9453  LR: 0.000012  \n","Epoch: [3][900/3575] Elapsed 4m 35s (remain 13m 38s) Loss: 0.0098(0.0062) Grad: 12367.4619  LR: 0.000012  \n","Epoch: [3][1000/3575] Elapsed 5m 6s (remain 13m 7s) Loss: 0.0000(0.0061) Grad: 36.7681  LR: 0.000012  \n","Epoch: [3][1100/3575] Elapsed 5m 36s (remain 12m 36s) Loss: 0.0047(0.0061) Grad: 15120.6172  LR: 0.000012  \n","Epoch: [3][1200/3575] Elapsed 6m 7s (remain 12m 5s) Loss: 0.0048(0.0062) Grad: 13377.0322  LR: 0.000012  \n","Epoch: [3][1300/3575] Elapsed 6m 37s (remain 11m 34s) Loss: 0.0291(0.0062) Grad: 39131.4375  LR: 0.000012  \n","Epoch: [3][1400/3575] Elapsed 7m 7s (remain 11m 4s) Loss: 0.0702(0.0062) Grad: 48527.8594  LR: 0.000012  \n","Epoch: [3][1500/3575] Elapsed 7m 38s (remain 10m 33s) Loss: 0.0001(0.0063) Grad: 260.4077  LR: 0.000011  \n","Epoch: [3][1600/3575] Elapsed 8m 8s (remain 10m 2s) Loss: 0.0007(0.0064) Grad: 5378.9087  LR: 0.000011  \n","Epoch: [3][1700/3575] Elapsed 8m 39s (remain 9m 31s) Loss: 0.0000(0.0063) Grad: 35.1437  LR: 0.000011  \n","Epoch: [3][1800/3575] Elapsed 9m 9s (remain 9m 1s) Loss: 0.0118(0.0064) Grad: 10267.5107  LR: 0.000011  \n","Epoch: [3][1900/3575] Elapsed 9m 39s (remain 8m 30s) Loss: 0.0000(0.0063) Grad: 115.9088  LR: 0.000011  \n","Epoch: [3][2000/3575] Elapsed 10m 10s (remain 8m 0s) Loss: 0.0000(0.0063) Grad: 82.3608  LR: 0.000011  \n","Epoch: [3][2100/3575] Elapsed 10m 41s (remain 7m 29s) Loss: 0.0000(0.0064) Grad: 121.2324  LR: 0.000011  \n","Epoch: [3][2200/3575] Elapsed 11m 11s (remain 6m 59s) Loss: 0.0360(0.0064) Grad: 64482.2578  LR: 0.000011  \n","Epoch: [3][2300/3575] Elapsed 11m 41s (remain 6m 28s) Loss: 0.0002(0.0063) Grad: 1151.6158  LR: 0.000010  \n","Epoch: [3][2400/3575] Elapsed 12m 12s (remain 5m 58s) Loss: 0.0110(0.0064) Grad: 13277.1025  LR: 0.000010  \n","Epoch: [3][2500/3575] Elapsed 12m 42s (remain 5m 27s) Loss: 0.0023(0.0063) Grad: 8487.8633  LR: 0.000010  \n","Epoch: [3][2600/3575] Elapsed 13m 13s (remain 4m 57s) Loss: 0.0056(0.0063) Grad: 82757.3359  LR: 0.000010  \n","Epoch: [3][2700/3575] Elapsed 13m 43s (remain 4m 26s) Loss: 0.0023(0.0063) Grad: 3383.0828  LR: 0.000010  \n","Epoch: [3][2800/3575] Elapsed 14m 14s (remain 3m 56s) Loss: 0.0002(0.0064) Grad: 1457.2148  LR: 0.000010  \n","Epoch: [3][2900/3575] Elapsed 14m 44s (remain 3m 25s) Loss: 0.0000(0.0063) Grad: 278.0760  LR: 0.000010  \n","Epoch: [3][3000/3575] Elapsed 15m 15s (remain 2m 55s) Loss: 0.0007(0.0063) Grad: 16713.2656  LR: 0.000010  \n","Epoch: [3][3100/3575] Elapsed 15m 45s (remain 2m 24s) Loss: 0.0684(0.0062) Grad: 100082.5547  LR: 0.000009  \n","Epoch: [3][3200/3575] Elapsed 16m 16s (remain 1m 54s) Loss: 0.0001(0.0063) Grad: 537.0558  LR: 0.000009  \n","Epoch: [3][3300/3575] Elapsed 16m 46s (remain 1m 23s) Loss: 0.0021(0.0062) Grad: 6048.1675  LR: 0.000009  \n","Epoch: [3][3400/3575] Elapsed 17m 16s (remain 0m 53s) Loss: 0.0000(0.0062) Grad: 74.0796  LR: 0.000009  \n","Epoch: [3][3500/3575] Elapsed 17m 47s (remain 0m 22s) Loss: 0.0001(0.0063) Grad: 1055.4948  LR: 0.000009  \n","Epoch: [3][3574/3575] Elapsed 18m 9s (remain 0m 0s) Loss: 0.0059(0.0063) Grad: 8962.5771  LR: 0.000009  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 9m 57s) Loss: 0.0001(0.0001) \n","EVAL: [100/1192] Elapsed 0m 18s (remain 3m 18s) Loss: 0.0078(0.0081) \n","EVAL: [200/1192] Elapsed 0m 36s (remain 2m 58s) Loss: 0.0255(0.0076) \n","EVAL: [300/1192] Elapsed 0m 54s (remain 2m 40s) Loss: 0.0074(0.0083) \n","EVAL: [400/1192] Elapsed 1m 12s (remain 2m 22s) Loss: 0.0036(0.0085) \n","EVAL: [500/1192] Elapsed 1m 30s (remain 2m 4s) Loss: 0.0021(0.0078) \n","EVAL: [600/1192] Elapsed 1m 47s (remain 1m 46s) Loss: 0.0005(0.0080) \n","EVAL: [700/1192] Elapsed 2m 5s (remain 1m 28s) Loss: 0.0766(0.0100) \n","EVAL: [800/1192] Elapsed 2m 23s (remain 1m 10s) Loss: 0.0007(0.0103) \n","EVAL: [900/1192] Elapsed 2m 41s (remain 0m 52s) Loss: 0.0046(0.0104) \n","EVAL: [1000/1192] Elapsed 2m 59s (remain 0m 34s) Loss: 0.0000(0.0101) \n","EVAL: [1100/1192] Elapsed 3m 17s (remain 0m 16s) Loss: 0.0002(0.0098) \n","EVAL: [1191/1192] Elapsed 3m 33s (remain 0m 0s) Loss: 0.0000(0.0095) \n","Epoch 3 - avg_train_loss: 0.0063  avg_val_loss: 0.0095  time: 1309s\n","Epoch 3 - Score: 0.8688\n","Epoch 3 - Save Best Score: 0.8688 Model\n","Epoch: [4][0/3575] Elapsed 0m 0s (remain 39m 5s) Loss: 0.0001(0.0001) Grad: 1170.5144  LR: 0.000009  \n","Epoch: [4][100/3575] Elapsed 0m 33s (remain 19m 5s) Loss: 0.0118(0.0053) Grad: 42712.0586  LR: 0.000009  \n","Epoch: [4][200/3575] Elapsed 1m 4s (remain 17m 55s) Loss: 0.0052(0.0052) Grad: 18445.9844  LR: 0.000009  \n","Epoch: [4][300/3575] Elapsed 1m 34s (remain 17m 7s) Loss: 0.0000(0.0052) Grad: 45.6267  LR: 0.000009  \n","Epoch: [4][400/3575] Elapsed 2m 4s (remain 16m 28s) Loss: 0.0029(0.0049) Grad: 13482.0137  LR: 0.000008  \n","Epoch: [4][500/3575] Elapsed 2m 35s (remain 15m 52s) Loss: 0.0000(0.0049) Grad: 36.2719  LR: 0.000008  \n","Epoch: [4][600/3575] Elapsed 3m 5s (remain 15m 18s) Loss: 0.0000(0.0046) Grad: 78.6570  LR: 0.000008  \n","Epoch: [4][700/3575] Elapsed 3m 36s (remain 14m 45s) Loss: 0.0003(0.0049) Grad: 2434.8821  LR: 0.000008  \n","Epoch: [4][800/3575] Elapsed 4m 6s (remain 14m 13s) Loss: 0.0031(0.0053) Grad: 3656.8347  LR: 0.000008  \n","Epoch: [4][900/3575] Elapsed 4m 36s (remain 13m 41s) Loss: 0.0000(0.0052) Grad: 17.5820  LR: 0.000008  \n","Epoch: [4][1000/3575] Elapsed 5m 7s (remain 13m 10s) Loss: 0.0000(0.0050) Grad: 53.4307  LR: 0.000008  \n","Epoch: [4][1100/3575] Elapsed 5m 37s (remain 12m 38s) Loss: 0.0002(0.0051) Grad: 2489.6233  LR: 0.000008  \n","Epoch: [4][1200/3575] Elapsed 6m 7s (remain 12m 7s) Loss: 0.0041(0.0051) Grad: 22511.8828  LR: 0.000007  \n","Epoch: [4][1300/3575] Elapsed 6m 38s (remain 11m 36s) Loss: 0.0005(0.0050) Grad: 3024.5591  LR: 0.000007  \n","Epoch: [4][1400/3575] Elapsed 7m 8s (remain 11m 4s) Loss: 0.0199(0.0051) Grad: 190320.5312  LR: 0.000007  \n","Epoch: [4][1500/3575] Elapsed 7m 38s (remain 10m 33s) Loss: 0.0001(0.0051) Grad: 242.4243  LR: 0.000007  \n","Epoch: [4][1600/3575] Elapsed 8m 9s (remain 10m 2s) Loss: 0.0053(0.0053) Grad: 42260.1211  LR: 0.000007  \n","Epoch: [4][1700/3575] Elapsed 8m 39s (remain 9m 32s) Loss: 0.0132(0.0052) Grad: 47056.9375  LR: 0.000007  \n","Epoch: [4][1800/3575] Elapsed 9m 9s (remain 9m 1s) Loss: 0.0001(0.0053) Grad: 674.1747  LR: 0.000007  \n","Epoch: [4][1900/3575] Elapsed 9m 39s (remain 8m 30s) Loss: 0.0000(0.0052) Grad: 30.6566  LR: 0.000007  \n","Epoch: [4][2000/3575] Elapsed 10m 10s (remain 7m 59s) Loss: 0.0000(0.0051) Grad: 108.1529  LR: 0.000006  \n","Epoch: [4][2100/3575] Elapsed 10m 40s (remain 7m 29s) Loss: 0.0001(0.0050) Grad: 214.5301  LR: 0.000006  \n","Epoch: [4][2200/3575] Elapsed 11m 10s (remain 6m 58s) Loss: 0.0000(0.0050) Grad: 106.6965  LR: 0.000006  \n","Epoch: [4][2300/3575] Elapsed 11m 41s (remain 6m 28s) Loss: 0.0799(0.0050) Grad: 197341.1562  LR: 0.000006  \n","Epoch: [4][2400/3575] Elapsed 12m 11s (remain 5m 57s) Loss: 0.0022(0.0049) Grad: 4783.1826  LR: 0.000006  \n","Epoch: [4][2500/3575] Elapsed 12m 41s (remain 5m 27s) Loss: 0.0000(0.0049) Grad: 190.2670  LR: 0.000006  \n","Epoch: [4][2600/3575] Elapsed 13m 11s (remain 4m 56s) Loss: 0.0003(0.0049) Grad: 1677.9174  LR: 0.000006  \n","Epoch: [4][2700/3575] Elapsed 13m 42s (remain 4m 26s) Loss: 0.0000(0.0049) Grad: 212.2176  LR: 0.000006  \n","Epoch: [4][2800/3575] Elapsed 14m 12s (remain 3m 55s) Loss: 0.0035(0.0048) Grad: 9452.4219  LR: 0.000005  \n","Epoch: [4][2900/3575] Elapsed 14m 42s (remain 3m 25s) Loss: 0.0000(0.0049) Grad: 38.3392  LR: 0.000005  \n","Epoch: [4][3000/3575] Elapsed 15m 13s (remain 2m 54s) Loss: 0.0000(0.0049) Grad: 24.8064  LR: 0.000005  \n","Epoch: [4][3100/3575] Elapsed 15m 43s (remain 2m 24s) Loss: 0.0063(0.0049) Grad: 31120.9141  LR: 0.000005  \n","Epoch: [4][3200/3575] Elapsed 16m 13s (remain 1m 53s) Loss: 0.1506(0.0050) Grad: 289992.4375  LR: 0.000005  \n","Epoch: [4][3300/3575] Elapsed 16m 44s (remain 1m 23s) Loss: 0.0000(0.0049) Grad: 103.9902  LR: 0.000005  \n","Epoch: [4][3400/3575] Elapsed 17m 14s (remain 0m 52s) Loss: 0.0014(0.0049) Grad: 39915.4062  LR: 0.000005  \n","Epoch: [4][3500/3575] Elapsed 17m 45s (remain 0m 22s) Loss: 0.0000(0.0049) Grad: 48.4994  LR: 0.000005  \n","Epoch: [4][3574/3575] Elapsed 18m 7s (remain 0m 0s) Loss: 0.0117(0.0049) Grad: 88231.7656  LR: 0.000004  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 9m 24s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 18s (remain 3m 18s) Loss: 0.0161(0.0091) \n","EVAL: [200/1192] Elapsed 0m 36s (remain 2m 58s) Loss: 0.0440(0.0091) \n","EVAL: [300/1192] Elapsed 0m 54s (remain 2m 40s) Loss: 0.0039(0.0095) \n","EVAL: [400/1192] Elapsed 1m 12s (remain 2m 22s) Loss: 0.0111(0.0096) \n","EVAL: [500/1192] Elapsed 1m 29s (remain 2m 4s) Loss: 0.0030(0.0087) \n","EVAL: [600/1192] Elapsed 1m 47s (remain 1m 45s) Loss: 0.0003(0.0091) \n","EVAL: [700/1192] Elapsed 2m 5s (remain 1m 27s) Loss: 0.0839(0.0111) \n","EVAL: [800/1192] Elapsed 2m 23s (remain 1m 10s) Loss: 0.0006(0.0115) \n","EVAL: [900/1192] Elapsed 2m 41s (remain 0m 52s) Loss: 0.0042(0.0117) \n","EVAL: [1000/1192] Elapsed 2m 59s (remain 0m 34s) Loss: 0.0000(0.0114) \n","EVAL: [1100/1192] Elapsed 3m 17s (remain 0m 16s) Loss: 0.0001(0.0111) \n","EVAL: [1191/1192] Elapsed 3m 33s (remain 0m 0s) Loss: 0.0000(0.0107) \n","Epoch 4 - avg_train_loss: 0.0049  avg_val_loss: 0.0107  time: 1306s\n","Epoch 4 - Score: 0.8764\n","Epoch 4 - Save Best Score: 0.8764 Model\n","Epoch: [5][0/3575] Elapsed 0m 0s (remain 42m 22s) Loss: 0.0032(0.0032) Grad: 23465.2656  LR: 0.000004  \n","Epoch: [5][100/3575] Elapsed 0m 33s (remain 19m 6s) Loss: 0.0002(0.0044) Grad: 3680.8425  LR: 0.000004  \n","Epoch: [5][200/3575] Elapsed 1m 4s (remain 17m 56s) Loss: 0.0003(0.0039) Grad: 2631.0327  LR: 0.000004  \n","Epoch: [5][300/3575] Elapsed 1m 34s (remain 17m 7s) Loss: 0.0031(0.0042) Grad: 26317.7617  LR: 0.000004  \n","Epoch: [5][400/3575] Elapsed 2m 4s (remain 16m 28s) Loss: 0.0002(0.0038) Grad: 5713.3345  LR: 0.000004  \n","Epoch: [5][500/3575] Elapsed 2m 35s (remain 15m 52s) Loss: 0.0000(0.0042) Grad: 3.5278  LR: 0.000004  \n","Epoch: [5][600/3575] Elapsed 3m 5s (remain 15m 18s) Loss: 0.0000(0.0043) Grad: 32.1362  LR: 0.000004  \n","Epoch: [5][700/3575] Elapsed 3m 36s (remain 14m 45s) Loss: 0.0000(0.0041) Grad: 12.2271  LR: 0.000004  \n","Epoch: [5][800/3575] Elapsed 4m 6s (remain 14m 12s) Loss: 0.0000(0.0040) Grad: 69.6850  LR: 0.000003  \n","Epoch: [5][900/3575] Elapsed 4m 36s (remain 13m 41s) Loss: 0.0245(0.0040) Grad: 50920.5352  LR: 0.000003  \n","Epoch: [5][1000/3575] Elapsed 5m 7s (remain 13m 10s) Loss: 0.0000(0.0040) Grad: 12.2607  LR: 0.000003  \n","Epoch: [5][1100/3575] Elapsed 5m 37s (remain 12m 38s) Loss: 0.0000(0.0040) Grad: 4.9239  LR: 0.000003  \n","Epoch: [5][1200/3575] Elapsed 6m 8s (remain 12m 7s) Loss: 0.0000(0.0039) Grad: 43.9870  LR: 0.000003  \n","Epoch: [5][1300/3575] Elapsed 6m 38s (remain 11m 36s) Loss: 0.0000(0.0040) Grad: 399.9264  LR: 0.000003  \n","Epoch: [5][1400/3575] Elapsed 7m 8s (remain 11m 4s) Loss: 0.0000(0.0041) Grad: 21.5173  LR: 0.000003  \n","Epoch: [5][1500/3575] Elapsed 7m 38s (remain 10m 33s) Loss: 0.0000(0.0040) Grad: 54.3438  LR: 0.000003  \n","Epoch: [5][1600/3575] Elapsed 8m 9s (remain 10m 3s) Loss: 0.0000(0.0040) Grad: 230.2573  LR: 0.000002  \n","Epoch: [5][1700/3575] Elapsed 8m 39s (remain 9m 32s) Loss: 0.0001(0.0040) Grad: 676.7729  LR: 0.000002  \n","Epoch: [5][1800/3575] Elapsed 9m 9s (remain 9m 1s) Loss: 0.0000(0.0040) Grad: 77.4916  LR: 0.000002  \n","Epoch: [5][1900/3575] Elapsed 9m 40s (remain 8m 30s) Loss: 0.0727(0.0040) Grad: 72181.7812  LR: 0.000002  \n","Epoch: [5][2000/3575] Elapsed 10m 10s (remain 8m 0s) Loss: 0.0000(0.0039) Grad: 221.5007  LR: 0.000002  \n","Epoch: [5][2100/3575] Elapsed 10m 40s (remain 7m 29s) Loss: 0.0000(0.0039) Grad: 184.0890  LR: 0.000002  \n","Epoch: [5][2200/3575] Elapsed 11m 10s (remain 6m 58s) Loss: 0.0044(0.0038) Grad: 10436.3691  LR: 0.000002  \n","Epoch: [5][2300/3575] Elapsed 11m 41s (remain 6m 28s) Loss: 0.0000(0.0038) Grad: 153.4975  LR: 0.000002  \n","Epoch: [5][2400/3575] Elapsed 12m 11s (remain 5m 57s) Loss: 0.0163(0.0039) Grad: 38748.4922  LR: 0.000001  \n","Epoch: [5][2500/3575] Elapsed 12m 42s (remain 5m 27s) Loss: 0.0000(0.0039) Grad: 550.3723  LR: 0.000001  \n","Epoch: [5][2600/3575] Elapsed 13m 12s (remain 4m 56s) Loss: 0.0001(0.0038) Grad: 1450.7516  LR: 0.000001  \n","Epoch: [5][2700/3575] Elapsed 13m 42s (remain 4m 26s) Loss: 0.0000(0.0038) Grad: 24.0228  LR: 0.000001  \n","Epoch: [5][2800/3575] Elapsed 14m 13s (remain 3m 55s) Loss: 0.0000(0.0037) Grad: 106.7752  LR: 0.000001  \n","Epoch: [5][2900/3575] Elapsed 14m 43s (remain 3m 25s) Loss: 0.0033(0.0037) Grad: 46754.7852  LR: 0.000001  \n","Epoch: [5][3000/3575] Elapsed 15m 13s (remain 2m 54s) Loss: 0.0000(0.0036) Grad: 14.9162  LR: 0.000001  \n","Epoch: [5][3100/3575] Elapsed 15m 43s (remain 2m 24s) Loss: 0.0028(0.0037) Grad: 16977.2520  LR: 0.000001  \n","Epoch: [5][3200/3575] Elapsed 16m 14s (remain 1m 53s) Loss: 0.0000(0.0037) Grad: 91.2964  LR: 0.000000  \n","Epoch: [5][3300/3575] Elapsed 16m 44s (remain 1m 23s) Loss: 0.0109(0.0037) Grad: 11608.8252  LR: 0.000000  \n","Epoch: [5][3400/3575] Elapsed 17m 14s (remain 0m 52s) Loss: 0.0001(0.0037) Grad: 1570.0220  LR: 0.000000  \n","Epoch: [5][3500/3575] Elapsed 17m 45s (remain 0m 22s) Loss: 0.0018(0.0037) Grad: 30212.4590  LR: 0.000000  \n","Epoch: [5][3574/3575] Elapsed 18m 7s (remain 0m 0s) Loss: 0.0002(0.0037) Grad: 2536.2971  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 9m 27s) Loss: 0.0001(0.0001) \n","EVAL: [100/1192] Elapsed 0m 18s (remain 3m 17s) Loss: 0.0280(0.0095) \n","EVAL: [200/1192] Elapsed 0m 36s (remain 2m 58s) Loss: 0.0336(0.0094) \n","EVAL: [300/1192] Elapsed 0m 54s (remain 2m 40s) Loss: 0.0040(0.0100) \n","EVAL: [400/1192] Elapsed 1m 12s (remain 2m 22s) Loss: 0.0119(0.0102) \n","EVAL: [500/1192] Elapsed 1m 29s (remain 2m 4s) Loss: 0.0052(0.0092) \n","EVAL: [600/1192] Elapsed 1m 47s (remain 1m 45s) Loss: 0.0002(0.0094) \n","EVAL: [700/1192] Elapsed 2m 5s (remain 1m 27s) Loss: 0.0886(0.0119) \n","EVAL: [800/1192] Elapsed 2m 23s (remain 1m 10s) Loss: 0.0001(0.0124) \n","EVAL: [900/1192] Elapsed 2m 41s (remain 0m 52s) Loss: 0.0040(0.0126) \n","EVAL: [1000/1192] Elapsed 2m 59s (remain 0m 34s) Loss: 0.0000(0.0124) \n","EVAL: [1100/1192] Elapsed 3m 17s (remain 0m 16s) Loss: 0.0001(0.0121) \n","EVAL: [1191/1192] Elapsed 3m 33s (remain 0m 0s) Loss: 0.0000(0.0117) \n","Epoch 5 - avg_train_loss: 0.0037  avg_val_loss: 0.0117  time: 1306s\n","Epoch 5 - Score: 0.8763\n","========== fold: 1 training ==========\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Load weight from pretrained\n","Epoch: [1][0/3575] Elapsed 0m 0s (remain 47m 38s) Loss: 0.2649(0.2649) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/3575] Elapsed 0m 31s (remain 17m 49s) Loss: 0.1829(0.3011) Grad: 72670.5078  LR: 0.000001  \n","Epoch: [1][200/3575] Elapsed 1m 1s (remain 17m 12s) Loss: 0.0472(0.1825) Grad: 6416.8276  LR: 0.000002  \n","Epoch: [1][300/3575] Elapsed 1m 31s (remain 16m 38s) Loss: 0.0199(0.1355) Grad: 3723.3511  LR: 0.000003  \n","Epoch: [1][400/3575] Elapsed 2m 2s (remain 16m 5s) Loss: 0.0467(0.1094) Grad: 14058.7373  LR: 0.000004  \n","Epoch: [1][500/3575] Elapsed 2m 32s (remain 15m 34s) Loss: 0.0295(0.0923) Grad: 4245.5879  LR: 0.000006  \n","Epoch: [1][600/3575] Elapsed 3m 2s (remain 15m 2s) Loss: 0.0230(0.0802) Grad: 30388.2578  LR: 0.000007  \n","Epoch: [1][700/3575] Elapsed 3m 32s (remain 14m 31s) Loss: 0.0036(0.0720) Grad: 5165.4355  LR: 0.000008  \n","Epoch: [1][800/3575] Elapsed 4m 2s (remain 14m 1s) Loss: 0.0162(0.0653) Grad: 15278.0674  LR: 0.000009  \n","Epoch: [1][900/3575] Elapsed 4m 33s (remain 13m 30s) Loss: 0.0034(0.0598) Grad: 2519.8687  LR: 0.000010  \n","Epoch: [1][1000/3575] Elapsed 5m 4s (remain 13m 2s) Loss: 0.0303(0.0556) Grad: 25068.6426  LR: 0.000011  \n","Epoch: [1][1100/3575] Elapsed 5m 34s (remain 12m 31s) Loss: 0.0099(0.0519) Grad: 6942.4341  LR: 0.000012  \n","Epoch: [1][1200/3575] Elapsed 6m 4s (remain 12m 1s) Loss: 0.0008(0.0488) Grad: 2070.9124  LR: 0.000013  \n","Epoch: [1][1300/3575] Elapsed 6m 35s (remain 11m 30s) Loss: 0.0010(0.0461) Grad: 2404.2063  LR: 0.000015  \n","Epoch: [1][1400/3575] Elapsed 7m 5s (remain 11m 0s) Loss: 0.0068(0.0437) Grad: 4045.1038  LR: 0.000016  \n","Epoch: [1][1500/3575] Elapsed 7m 35s (remain 10m 29s) Loss: 0.0086(0.0417) Grad: 15759.6572  LR: 0.000017  \n","Epoch: [1][1600/3575] Elapsed 8m 5s (remain 9m 59s) Loss: 0.0013(0.0398) Grad: 2890.3916  LR: 0.000018  \n","Epoch: [1][1700/3575] Elapsed 8m 36s (remain 9m 28s) Loss: 0.0011(0.0383) Grad: 10289.9053  LR: 0.000019  \n","Epoch: [1][1800/3575] Elapsed 9m 6s (remain 8m 58s) Loss: 0.0000(0.0368) Grad: 20.6858  LR: 0.000020  \n","Epoch: [1][1900/3575] Elapsed 9m 36s (remain 8m 27s) Loss: 0.0134(0.0355) Grad: 6367.8794  LR: 0.000020  \n","Epoch: [1][2000/3575] Elapsed 10m 7s (remain 7m 57s) Loss: 0.0037(0.0344) Grad: 3506.2979  LR: 0.000020  \n","Epoch: [1][2100/3575] Elapsed 10m 37s (remain 7m 27s) Loss: 0.0034(0.0334) Grad: 13330.5283  LR: 0.000020  \n","Epoch: [1][2200/3575] Elapsed 11m 7s (remain 6m 56s) Loss: 0.0120(0.0326) Grad: 11327.9658  LR: 0.000019  \n","Epoch: [1][2300/3575] Elapsed 11m 38s (remain 6m 26s) Loss: 0.0036(0.0317) Grad: 3492.4424  LR: 0.000019  \n","Epoch: [1][2400/3575] Elapsed 12m 8s (remain 5m 56s) Loss: 0.0282(0.0309) Grad: 39828.6094  LR: 0.000019  \n","Epoch: [1][2500/3575] Elapsed 12m 38s (remain 5m 25s) Loss: 0.0545(0.0301) Grad: 25493.5039  LR: 0.000019  \n","Epoch: [1][2600/3575] Elapsed 13m 8s (remain 4m 55s) Loss: 0.0683(0.0294) Grad: 26262.7070  LR: 0.000019  \n","Epoch: [1][2700/3575] Elapsed 13m 39s (remain 4m 25s) Loss: 0.0047(0.0288) Grad: 6917.6411  LR: 0.000019  \n","Epoch: [1][2800/3575] Elapsed 14m 9s (remain 3m 54s) Loss: 0.0365(0.0282) Grad: 16217.3340  LR: 0.000019  \n","Epoch: [1][2900/3575] Elapsed 14m 39s (remain 3m 24s) Loss: 0.0026(0.0276) Grad: 1294.6787  LR: 0.000019  \n","Epoch: [1][3000/3575] Elapsed 15m 10s (remain 2m 54s) Loss: 0.0001(0.0270) Grad: 72.6839  LR: 0.000018  \n","Epoch: [1][3100/3575] Elapsed 15m 40s (remain 2m 23s) Loss: 0.0019(0.0265) Grad: 2708.4214  LR: 0.000018  \n","Epoch: [1][3200/3575] Elapsed 16m 10s (remain 1m 53s) Loss: 0.0024(0.0260) Grad: 9350.9512  LR: 0.000018  \n","Epoch: [1][3300/3575] Elapsed 16m 41s (remain 1m 23s) Loss: 0.0013(0.0256) Grad: 895.0389  LR: 0.000018  \n","Epoch: [1][3400/3575] Elapsed 17m 11s (remain 0m 52s) Loss: 0.0189(0.0253) Grad: 25842.0234  LR: 0.000018  \n","Epoch: [1][3500/3575] Elapsed 17m 41s (remain 0m 22s) Loss: 0.0012(0.0249) Grad: 4584.5479  LR: 0.000018  \n","Epoch: [1][3574/3575] Elapsed 18m 4s (remain 0m 0s) Loss: 0.0012(0.0245) Grad: 1901.5991  LR: 0.000018  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 11m 48s) Loss: 0.0003(0.0003) \n","EVAL: [100/1192] Elapsed 0m 18s (remain 3m 18s) Loss: 0.0006(0.0063) \n","EVAL: [200/1192] Elapsed 0m 36s (remain 2m 58s) Loss: 0.0012(0.0081) \n","EVAL: [300/1192] Elapsed 0m 54s (remain 2m 40s) Loss: 0.0448(0.0114) \n","EVAL: [400/1192] Elapsed 1m 11s (remain 2m 21s) Loss: 0.0080(0.0121) \n","EVAL: [500/1192] Elapsed 1m 29s (remain 2m 3s) Loss: 0.0102(0.0117) \n","EVAL: [600/1192] Elapsed 1m 47s (remain 1m 45s) Loss: 0.0968(0.0127) \n","EVAL: [700/1192] Elapsed 2m 5s (remain 1m 27s) Loss: 0.0469(0.0138) \n","EVAL: [800/1192] Elapsed 2m 23s (remain 1m 9s) Loss: 0.0415(0.0134) \n","EVAL: [900/1192] Elapsed 2m 41s (remain 0m 52s) Loss: 0.0025(0.0135) \n","EVAL: [1000/1192] Elapsed 2m 59s (remain 0m 34s) Loss: 0.0004(0.0131) \n","EVAL: [1100/1192] Elapsed 3m 16s (remain 0m 16s) Loss: 0.0118(0.0124) \n","EVAL: [1191/1192] Elapsed 3m 32s (remain 0m 0s) Loss: 0.0200(0.0119) \n","Epoch 1 - avg_train_loss: 0.0245  avg_val_loss: 0.0119  time: 1303s\n","Epoch 1 - Score: 0.8275\n","Epoch 1 - Save Best Score: 0.8275 Model\n","Epoch: [2][0/3575] Elapsed 0m 0s (remain 43m 58s) Loss: 0.0264(0.0264) Grad: 29550.6094  LR: 0.000018  \n","Epoch: [2][100/3575] Elapsed 0m 32s (remain 18m 31s) Loss: 0.0019(0.0102) Grad: 5894.1714  LR: 0.000018  \n","Epoch: [2][200/3575] Elapsed 1m 3s (remain 17m 45s) Loss: 0.0000(0.0088) Grad: 56.5140  LR: 0.000018  \n","Epoch: [2][300/3575] Elapsed 1m 33s (remain 17m 0s) Loss: 0.0214(0.0102) Grad: 15342.6826  LR: 0.000017  \n","Epoch: [2][400/3575] Elapsed 2m 4s (remain 16m 22s) Loss: 0.0000(0.0098) Grad: 51.6695  LR: 0.000017  \n","Epoch: [2][500/3575] Elapsed 2m 34s (remain 15m 48s) Loss: 0.0070(0.0097) Grad: 22063.1641  LR: 0.000017  \n","Epoch: [2][600/3575] Elapsed 3m 4s (remain 15m 14s) Loss: 0.0026(0.0096) Grad: 9748.6992  LR: 0.000017  \n","Epoch: [2][700/3575] Elapsed 3m 35s (remain 14m 42s) Loss: 0.0110(0.0094) Grad: 100955.2500  LR: 0.000017  \n","Epoch: [2][800/3575] Elapsed 4m 5s (remain 14m 11s) Loss: 0.0148(0.0092) Grad: 30004.4395  LR: 0.000017  \n","Epoch: [2][900/3575] Elapsed 4m 36s (remain 13m 39s) Loss: 0.0128(0.0092) Grad: 33661.8164  LR: 0.000017  \n","Epoch: [2][1000/3575] Elapsed 5m 6s (remain 13m 8s) Loss: 0.0175(0.0090) Grad: 17804.3203  LR: 0.000017  \n","Epoch: [2][1100/3575] Elapsed 5m 36s (remain 12m 37s) Loss: 0.0110(0.0089) Grad: 44109.6016  LR: 0.000016  \n","Epoch: [2][1200/3575] Elapsed 6m 7s (remain 12m 5s) Loss: 0.0746(0.0090) Grad: 123863.2344  LR: 0.000016  \n","Epoch: [2][1300/3575] Elapsed 6m 37s (remain 11m 35s) Loss: 0.0101(0.0090) Grad: 71148.7812  LR: 0.000016  \n","Epoch: [2][1400/3575] Elapsed 7m 8s (remain 11m 4s) Loss: 0.0003(0.0089) Grad: 1220.2438  LR: 0.000016  \n","Epoch: [2][1500/3575] Elapsed 7m 38s (remain 10m 33s) Loss: 0.0121(0.0089) Grad: 275112.0312  LR: 0.000016  \n","Epoch: [2][1600/3575] Elapsed 8m 8s (remain 10m 2s) Loss: 0.0052(0.0087) Grad: 18474.1309  LR: 0.000016  \n","Epoch: [2][1700/3575] Elapsed 8m 39s (remain 9m 31s) Loss: 0.0003(0.0087) Grad: 2289.2703  LR: 0.000016  \n","Epoch: [2][1800/3575] Elapsed 9m 9s (remain 9m 1s) Loss: 0.0093(0.0087) Grad: 27055.3184  LR: 0.000016  \n","Epoch: [2][1900/3575] Elapsed 9m 40s (remain 8m 30s) Loss: 0.0000(0.0087) Grad: 126.8184  LR: 0.000015  \n","Epoch: [2][2000/3575] Elapsed 10m 10s (remain 8m 0s) Loss: 0.0000(0.0086) Grad: 19.2655  LR: 0.000015  \n","Epoch: [2][2100/3575] Elapsed 10m 40s (remain 7m 29s) Loss: 0.0039(0.0086) Grad: 13258.9170  LR: 0.000015  \n","Epoch: [2][2200/3575] Elapsed 11m 11s (remain 6m 58s) Loss: 0.0000(0.0086) Grad: 136.7409  LR: 0.000015  \n","Epoch: [2][2300/3575] Elapsed 11m 41s (remain 6m 28s) Loss: 0.0041(0.0086) Grad: 44144.0703  LR: 0.000015  \n","Epoch: [2][2400/3575] Elapsed 12m 11s (remain 5m 57s) Loss: 0.0005(0.0085) Grad: 3115.6960  LR: 0.000015  \n","Epoch: [2][2500/3575] Elapsed 12m 42s (remain 5m 27s) Loss: 0.0098(0.0085) Grad: 11696.8574  LR: 0.000015  \n","Epoch: [2][2600/3575] Elapsed 13m 12s (remain 4m 56s) Loss: 0.0291(0.0086) Grad: 211915.9844  LR: 0.000015  \n","Epoch: [2][2700/3575] Elapsed 13m 42s (remain 4m 26s) Loss: 0.0016(0.0086) Grad: 9517.8574  LR: 0.000014  \n","Epoch: [2][2800/3575] Elapsed 14m 13s (remain 3m 55s) Loss: 0.0021(0.0085) Grad: 23798.9141  LR: 0.000014  \n","Epoch: [2][2900/3575] Elapsed 14m 43s (remain 3m 25s) Loss: 0.0138(0.0085) Grad: 25297.7070  LR: 0.000014  \n","Epoch: [2][3000/3575] Elapsed 15m 14s (remain 2m 54s) Loss: 0.0105(0.0084) Grad: 51458.2539  LR: 0.000014  \n","Epoch: [2][3100/3575] Elapsed 15m 44s (remain 2m 24s) Loss: 0.0000(0.0084) Grad: 45.7688  LR: 0.000014  \n","Epoch: [2][3200/3575] Elapsed 16m 14s (remain 1m 53s) Loss: 0.0116(0.0083) Grad: 21626.1270  LR: 0.000014  \n","Epoch: [2][3300/3575] Elapsed 16m 45s (remain 1m 23s) Loss: 0.0017(0.0082) Grad: 19474.9609  LR: 0.000014  \n","Epoch: [2][3400/3575] Elapsed 17m 15s (remain 0m 52s) Loss: 0.0108(0.0082) Grad: 15926.8643  LR: 0.000014  \n","Epoch: [2][3500/3575] Elapsed 17m 46s (remain 0m 22s) Loss: 0.0052(0.0081) Grad: 12719.0059  LR: 0.000013  \n","Epoch: [2][3574/3575] Elapsed 18m 8s (remain 0m 0s) Loss: 0.0046(0.0082) Grad: 105912.1016  LR: 0.000013  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 11m 15s) Loss: 0.0002(0.0002) \n","EVAL: [100/1192] Elapsed 0m 18s (remain 3m 18s) Loss: 0.0003(0.0064) \n","EVAL: [200/1192] Elapsed 0m 36s (remain 2m 57s) Loss: 0.0041(0.0072) \n","EVAL: [300/1192] Elapsed 0m 54s (remain 2m 40s) Loss: 0.0040(0.0098) \n","EVAL: [400/1192] Elapsed 1m 11s (remain 2m 21s) Loss: 0.0155(0.0100) \n","EVAL: [500/1192] Elapsed 1m 29s (remain 2m 3s) Loss: 0.0107(0.0098) \n","EVAL: [600/1192] Elapsed 1m 47s (remain 1m 45s) Loss: 0.0809(0.0101) \n","EVAL: [700/1192] Elapsed 2m 5s (remain 1m 27s) Loss: 0.0358(0.0113) \n","EVAL: [800/1192] Elapsed 2m 23s (remain 1m 9s) Loss: 0.0564(0.0110) \n","EVAL: [900/1192] Elapsed 2m 41s (remain 0m 52s) Loss: 0.0043(0.0110) \n","EVAL: [1000/1192] Elapsed 2m 59s (remain 0m 34s) Loss: 0.0001(0.0106) \n","EVAL: [1100/1192] Elapsed 3m 16s (remain 0m 16s) Loss: 0.0103(0.0100) \n","EVAL: [1191/1192] Elapsed 3m 32s (remain 0m 0s) Loss: 0.0152(0.0094) \n","Epoch 2 - avg_train_loss: 0.0082  avg_val_loss: 0.0094  time: 1311s\n","Epoch 2 - Score: 0.8558\n","Epoch 2 - Save Best Score: 0.8558 Model\n","Epoch: [3][0/3575] Elapsed 0m 0s (remain 39m 42s) Loss: 0.0005(0.0005) Grad: 1913.1267  LR: 0.000013  \n","Epoch: [3][100/3575] Elapsed 0m 32s (remain 18m 36s) Loss: 0.0015(0.0056) Grad: 4505.7256  LR: 0.000013  \n","Epoch: [3][200/3575] Elapsed 1m 3s (remain 17m 50s) Loss: 0.0104(0.0064) Grad: 68077.0469  LR: 0.000013  \n","Epoch: [3][300/3575] Elapsed 1m 34s (remain 17m 4s) Loss: 0.0061(0.0062) Grad: 15452.4443  LR: 0.000013  \n","Epoch: [3][400/3575] Elapsed 2m 4s (remain 16m 26s) Loss: 0.0031(0.0058) Grad: 5537.8052  LR: 0.000013  \n","Epoch: [3][500/3575] Elapsed 2m 35s (remain 15m 51s) Loss: 0.0000(0.0056) Grad: 90.8907  LR: 0.000013  \n","Epoch: [3][600/3575] Elapsed 3m 5s (remain 15m 17s) Loss: 0.0000(0.0056) Grad: 42.1359  LR: 0.000013  \n","Epoch: [3][700/3575] Elapsed 3m 36s (remain 14m 45s) Loss: 0.0004(0.0059) Grad: 3297.3135  LR: 0.000012  \n","Epoch: [3][800/3575] Elapsed 4m 6s (remain 14m 13s) Loss: 0.0280(0.0061) Grad: 32717.7070  LR: 0.000012  \n","Epoch: [3][900/3575] Elapsed 4m 36s (remain 13m 41s) Loss: 0.0003(0.0060) Grad: 8196.8369  LR: 0.000012  \n","Epoch: [3][1000/3575] Elapsed 5m 7s (remain 13m 10s) Loss: 0.0003(0.0061) Grad: 1277.9055  LR: 0.000012  \n","Epoch: [3][1100/3575] Elapsed 5m 37s (remain 12m 38s) Loss: 0.0475(0.0064) Grad: 36532.2812  LR: 0.000012  \n","Epoch: [3][1200/3575] Elapsed 6m 8s (remain 12m 7s) Loss: 0.0000(0.0063) Grad: 415.4746  LR: 0.000012  \n","Epoch: [3][1300/3575] Elapsed 6m 38s (remain 11m 36s) Loss: 0.0000(0.0065) Grad: 271.0735  LR: 0.000012  \n","Epoch: [3][1400/3575] Elapsed 7m 8s (remain 11m 5s) Loss: 0.0000(0.0063) Grad: 36.9380  LR: 0.000012  \n","Epoch: [3][1500/3575] Elapsed 7m 39s (remain 10m 34s) Loss: 0.0038(0.0063) Grad: 7690.6143  LR: 0.000011  \n","Epoch: [3][1600/3575] Elapsed 8m 9s (remain 10m 3s) Loss: 0.0003(0.0062) Grad: 2778.0857  LR: 0.000011  \n","Epoch: [3][1700/3575] Elapsed 8m 40s (remain 9m 32s) Loss: 0.0001(0.0061) Grad: 397.2734  LR: 0.000011  \n","Epoch: [3][1800/3575] Elapsed 9m 10s (remain 9m 2s) Loss: 0.0042(0.0061) Grad: 6120.3857  LR: 0.000011  \n","Epoch: [3][1900/3575] Elapsed 9m 40s (remain 8m 31s) Loss: 0.0004(0.0062) Grad: 8464.1807  LR: 0.000011  \n","Epoch: [3][2000/3575] Elapsed 10m 10s (remain 8m 0s) Loss: 0.0001(0.0063) Grad: 329.2921  LR: 0.000011  \n","Epoch: [3][2100/3575] Elapsed 10m 41s (remain 7m 29s) Loss: 0.0039(0.0063) Grad: 10000.1699  LR: 0.000011  \n","Epoch: [3][2200/3575] Elapsed 11m 11s (remain 6m 59s) Loss: 0.0310(0.0063) Grad: 497295.2500  LR: 0.000011  \n","Epoch: [3][2300/3575] Elapsed 11m 41s (remain 6m 28s) Loss: 0.0029(0.0063) Grad: 31122.7129  LR: 0.000010  \n","Epoch: [3][2400/3575] Elapsed 12m 11s (remain 5m 57s) Loss: 0.0206(0.0064) Grad: 43497.1680  LR: 0.000010  \n","Epoch: [3][2500/3575] Elapsed 12m 42s (remain 5m 27s) Loss: 0.0023(0.0063) Grad: 3307.0605  LR: 0.000010  \n","Epoch: [3][2600/3575] Elapsed 13m 12s (remain 4m 56s) Loss: 0.0018(0.0063) Grad: 2952.8193  LR: 0.000010  \n","Epoch: [3][2700/3575] Elapsed 13m 42s (remain 4m 26s) Loss: 0.0000(0.0063) Grad: 24.4754  LR: 0.000010  \n","Epoch: [3][2800/3575] Elapsed 14m 13s (remain 3m 55s) Loss: 0.0000(0.0062) Grad: 170.5461  LR: 0.000010  \n","Epoch: [3][2900/3575] Elapsed 14m 43s (remain 3m 25s) Loss: 0.0000(0.0062) Grad: 12.2970  LR: 0.000010  \n","Epoch: [3][3000/3575] Elapsed 15m 13s (remain 2m 54s) Loss: 0.0001(0.0063) Grad: 98.0884  LR: 0.000010  \n","Epoch: [3][3100/3575] Elapsed 15m 43s (remain 2m 24s) Loss: 0.0001(0.0063) Grad: 153.3590  LR: 0.000009  \n","Epoch: [3][3200/3575] Elapsed 16m 14s (remain 1m 53s) Loss: 0.0032(0.0063) Grad: 1671.7247  LR: 0.000009  \n","Epoch: [3][3300/3575] Elapsed 16m 44s (remain 1m 23s) Loss: 0.0000(0.0063) Grad: 25.4174  LR: 0.000009  \n","Epoch: [3][3400/3575] Elapsed 17m 14s (remain 0m 52s) Loss: 0.0002(0.0063) Grad: 681.5076  LR: 0.000009  \n","Epoch: [3][3500/3575] Elapsed 17m 44s (remain 0m 22s) Loss: 0.0015(0.0063) Grad: 1994.2773  LR: 0.000009  \n","Epoch: [3][3574/3575] Elapsed 18m 7s (remain 0m 0s) Loss: 0.0000(0.0063) Grad: 14.5994  LR: 0.000009  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 9m 20s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 18s (remain 3m 17s) Loss: 0.0000(0.0074) \n","EVAL: [200/1192] Elapsed 0m 36s (remain 2m 58s) Loss: 0.0001(0.0070) \n","EVAL: [300/1192] Elapsed 0m 54s (remain 2m 40s) Loss: 0.0008(0.0098) \n","EVAL: [400/1192] Elapsed 1m 11s (remain 2m 21s) Loss: 0.0150(0.0099) \n","EVAL: [500/1192] Elapsed 1m 29s (remain 2m 3s) Loss: 0.0136(0.0096) \n","EVAL: [600/1192] Elapsed 1m 47s (remain 1m 45s) Loss: 0.0937(0.0098) \n","EVAL: [700/1192] Elapsed 2m 4s (remain 1m 27s) Loss: 0.0383(0.0114) \n","EVAL: [800/1192] Elapsed 2m 22s (remain 1m 9s) Loss: 0.0129(0.0110) \n","EVAL: [900/1192] Elapsed 2m 40s (remain 0m 51s) Loss: 0.0015(0.0108) \n","EVAL: [1000/1192] Elapsed 2m 58s (remain 0m 34s) Loss: 0.0000(0.0104) \n","EVAL: [1100/1192] Elapsed 3m 16s (remain 0m 16s) Loss: 0.0134(0.0099) \n","EVAL: [1191/1192] Elapsed 3m 32s (remain 0m 0s) Loss: 0.0225(0.0094) \n","Epoch 3 - avg_train_loss: 0.0063  avg_val_loss: 0.0094  time: 1308s\n","Epoch 3 - Score: 0.8672\n","Epoch 3 - Save Best Score: 0.8672 Model\n","Epoch: [4][0/3575] Elapsed 0m 0s (remain 36m 53s) Loss: 0.0028(0.0028) Grad: 5986.7021  LR: 0.000009  \n","Epoch: [4][100/3575] Elapsed 0m 32s (remain 18m 38s) Loss: 0.0000(0.0058) Grad: 49.6321  LR: 0.000009  \n","Epoch: [4][200/3575] Elapsed 1m 3s (remain 17m 41s) Loss: 0.0043(0.0060) Grad: 20094.4648  LR: 0.000009  \n","Epoch: [4][300/3575] Elapsed 1m 33s (remain 16m 57s) Loss: 0.0000(0.0059) Grad: 119.1056  LR: 0.000009  \n","Epoch: [4][400/3575] Elapsed 2m 3s (remain 16m 20s) Loss: 0.0007(0.0063) Grad: 38101.8711  LR: 0.000008  \n","Epoch: [4][500/3575] Elapsed 2m 34s (remain 15m 46s) Loss: 0.0028(0.0060) Grad: 17958.0098  LR: 0.000008  \n","Epoch: [4][600/3575] Elapsed 3m 4s (remain 15m 14s) Loss: 0.0011(0.0058) Grad: 5729.9150  LR: 0.000008  \n","Epoch: [4][700/3575] Elapsed 3m 35s (remain 14m 42s) Loss: 0.0000(0.0057) Grad: 83.7058  LR: 0.000008  \n","Epoch: [4][800/3575] Elapsed 4m 5s (remain 14m 10s) Loss: 0.0044(0.0052) Grad: 22151.4043  LR: 0.000008  \n","Epoch: [4][900/3575] Elapsed 4m 36s (remain 13m 39s) Loss: 0.0002(0.0055) Grad: 3681.1921  LR: 0.000008  \n","Epoch: [4][1000/3575] Elapsed 5m 6s (remain 13m 7s) Loss: 0.0025(0.0054) Grad: 7510.2124  LR: 0.000008  \n","Epoch: [4][1100/3575] Elapsed 5m 36s (remain 12m 36s) Loss: 0.0067(0.0056) Grad: 81596.8984  LR: 0.000008  \n","Epoch: [4][1200/3575] Elapsed 6m 7s (remain 12m 5s) Loss: 0.0000(0.0054) Grad: 17.2302  LR: 0.000007  \n","Epoch: [4][1300/3575] Elapsed 6m 37s (remain 11m 34s) Loss: 0.0049(0.0053) Grad: 23580.6465  LR: 0.000007  \n","Epoch: [4][1400/3575] Elapsed 7m 7s (remain 11m 3s) Loss: 0.0014(0.0052) Grad: 33383.2930  LR: 0.000007  \n","Epoch: [4][1500/3575] Elapsed 7m 38s (remain 10m 32s) Loss: 0.0000(0.0052) Grad: 18.6829  LR: 0.000007  \n","Epoch: [4][1600/3575] Elapsed 8m 8s (remain 10m 2s) Loss: 0.0001(0.0051) Grad: 228.8959  LR: 0.000007  \n","Epoch: [4][1700/3575] Elapsed 8m 38s (remain 9m 31s) Loss: 0.0071(0.0050) Grad: 41655.4727  LR: 0.000007  \n","Epoch: [4][1800/3575] Elapsed 9m 9s (remain 9m 0s) Loss: 0.0000(0.0050) Grad: 40.9756  LR: 0.000007  \n","Epoch: [4][1900/3575] Elapsed 9m 39s (remain 8m 30s) Loss: 0.0210(0.0050) Grad: 98718.3594  LR: 0.000007  \n","Epoch: [4][2000/3575] Elapsed 10m 9s (remain 7m 59s) Loss: 0.0000(0.0050) Grad: 151.6851  LR: 0.000006  \n","Epoch: [4][2100/3575] Elapsed 10m 40s (remain 7m 29s) Loss: 0.0000(0.0051) Grad: 47.7976  LR: 0.000006  \n","Epoch: [4][2200/3575] Elapsed 11m 10s (remain 6m 58s) Loss: 0.0002(0.0051) Grad: 2218.7109  LR: 0.000006  \n","Epoch: [4][2300/3575] Elapsed 11m 40s (remain 6m 27s) Loss: 0.0000(0.0051) Grad: 68.4158  LR: 0.000006  \n","Epoch: [4][2400/3575] Elapsed 12m 11s (remain 5m 57s) Loss: 0.0000(0.0051) Grad: 20.1130  LR: 0.000006  \n","Epoch: [4][2500/3575] Elapsed 12m 41s (remain 5m 26s) Loss: 0.0014(0.0051) Grad: 13116.9385  LR: 0.000006  \n","Epoch: [4][2600/3575] Elapsed 13m 11s (remain 4m 56s) Loss: 0.0001(0.0050) Grad: 810.4344  LR: 0.000006  \n","Epoch: [4][2700/3575] Elapsed 13m 42s (remain 4m 26s) Loss: 0.0000(0.0050) Grad: 327.5532  LR: 0.000006  \n","Epoch: [4][2800/3575] Elapsed 14m 12s (remain 3m 55s) Loss: 0.0061(0.0050) Grad: 12261.8477  LR: 0.000005  \n","Epoch: [4][2900/3575] Elapsed 14m 43s (remain 3m 25s) Loss: 0.0000(0.0050) Grad: 46.5840  LR: 0.000005  \n","Epoch: [4][3000/3575] Elapsed 15m 13s (remain 2m 54s) Loss: 0.0000(0.0050) Grad: 25.0874  LR: 0.000005  \n","Epoch: [4][3100/3575] Elapsed 15m 43s (remain 2m 24s) Loss: 0.0052(0.0049) Grad: 9194.3281  LR: 0.000005  \n","Epoch: [4][3200/3575] Elapsed 16m 14s (remain 1m 53s) Loss: 0.0001(0.0049) Grad: 701.7819  LR: 0.000005  \n","Epoch: [4][3300/3575] Elapsed 16m 44s (remain 1m 23s) Loss: 0.0006(0.0049) Grad: 6959.4834  LR: 0.000005  \n","Epoch: [4][3400/3575] Elapsed 17m 14s (remain 0m 52s) Loss: 0.0000(0.0048) Grad: 16.2887  LR: 0.000005  \n","Epoch: [4][3500/3575] Elapsed 17m 44s (remain 0m 22s) Loss: 0.0054(0.0049) Grad: 52713.6406  LR: 0.000005  \n","Epoch: [4][3574/3575] Elapsed 18m 7s (remain 0m 0s) Loss: 0.0000(0.0048) Grad: 4.4653  LR: 0.000004  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 9m 39s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 18s (remain 3m 18s) Loss: 0.0000(0.0088) \n","EVAL: [200/1192] Elapsed 0m 36s (remain 2m 57s) Loss: 0.0000(0.0086) \n","EVAL: [300/1192] Elapsed 0m 53s (remain 2m 39s) Loss: 0.0003(0.0119) \n","EVAL: [400/1192] Elapsed 1m 11s (remain 2m 21s) Loss: 0.0206(0.0125) \n","EVAL: [500/1192] Elapsed 1m 29s (remain 2m 2s) Loss: 0.0176(0.0121) \n","EVAL: [600/1192] Elapsed 1m 46s (remain 1m 45s) Loss: 0.1191(0.0124) \n","EVAL: [700/1192] Elapsed 2m 4s (remain 1m 27s) Loss: 0.0648(0.0144) \n","EVAL: [800/1192] Elapsed 2m 22s (remain 1m 9s) Loss: 0.0193(0.0141) \n","EVAL: [900/1192] Elapsed 2m 40s (remain 0m 51s) Loss: 0.0004(0.0141) \n","EVAL: [1000/1192] Elapsed 2m 58s (remain 0m 33s) Loss: 0.0000(0.0134) \n","EVAL: [1100/1192] Elapsed 3m 15s (remain 0m 16s) Loss: 0.0141(0.0127) \n","EVAL: [1191/1192] Elapsed 3m 31s (remain 0m 0s) Loss: 0.0241(0.0120) \n","Epoch 4 - avg_train_loss: 0.0048  avg_val_loss: 0.0120  time: 1308s\n","Epoch 4 - Score: 0.8682\n","Epoch 4 - Save Best Score: 0.8682 Model\n","Epoch: [5][0/3575] Elapsed 0m 0s (remain 39m 16s) Loss: 0.0038(0.0038) Grad: 5694.0781  LR: 0.000004  \n","Epoch: [5][100/3575] Elapsed 0m 32s (remain 18m 22s) Loss: 0.0001(0.0047) Grad: 544.1756  LR: 0.000004  \n","Epoch: [5][200/3575] Elapsed 1m 3s (remain 17m 40s) Loss: 0.0000(0.0046) Grad: 129.6917  LR: 0.000004  \n","Epoch: [5][300/3575] Elapsed 1m 33s (remain 16m 55s) Loss: 0.0609(0.0046) Grad: 222302.5469  LR: 0.000004  \n","Epoch: [5][400/3575] Elapsed 2m 3s (remain 16m 18s) Loss: 0.0000(0.0045) Grad: 18.8882  LR: 0.000004  \n","Epoch: [5][500/3575] Elapsed 2m 33s (remain 15m 43s) Loss: 0.0004(0.0045) Grad: 4730.0176  LR: 0.000004  \n","Epoch: [5][600/3575] Elapsed 3m 4s (remain 15m 11s) Loss: 0.0012(0.0043) Grad: 6775.7520  LR: 0.000004  \n","Epoch: [5][700/3575] Elapsed 3m 34s (remain 14m 39s) Loss: 0.0024(0.0041) Grad: 17064.8320  LR: 0.000004  \n","Epoch: [5][800/3575] Elapsed 4m 4s (remain 14m 7s) Loss: 0.0000(0.0043) Grad: 110.6340  LR: 0.000003  \n","Epoch: [5][900/3575] Elapsed 4m 34s (remain 13m 35s) Loss: 0.0000(0.0044) Grad: 2.6723  LR: 0.000003  \n","Epoch: [5][1000/3575] Elapsed 5m 5s (remain 13m 4s) Loss: 0.0003(0.0043) Grad: 2322.6365  LR: 0.000003  \n","Epoch: [5][1100/3575] Elapsed 5m 35s (remain 12m 33s) Loss: 0.0000(0.0043) Grad: 89.4434  LR: 0.000003  \n","Epoch: [5][1200/3575] Elapsed 6m 5s (remain 12m 2s) Loss: 0.0000(0.0043) Grad: 42.0697  LR: 0.000003  \n","Epoch: [5][1300/3575] Elapsed 6m 35s (remain 11m 31s) Loss: 0.0000(0.0041) Grad: 11.4240  LR: 0.000003  \n","Epoch: [5][1400/3575] Elapsed 7m 5s (remain 11m 1s) Loss: 0.0000(0.0043) Grad: 34.3243  LR: 0.000003  \n","Epoch: [5][1500/3575] Elapsed 7m 36s (remain 10m 30s) Loss: 0.0018(0.0043) Grad: 3651.5513  LR: 0.000003  \n","Epoch: [5][1600/3575] Elapsed 8m 6s (remain 10m 0s) Loss: 0.0011(0.0042) Grad: 4242.1841  LR: 0.000002  \n","Epoch: [5][1700/3575] Elapsed 8m 37s (remain 9m 29s) Loss: 0.0002(0.0043) Grad: 957.7534  LR: 0.000002  \n","Epoch: [5][1800/3575] Elapsed 9m 7s (remain 8m 59s) Loss: 0.0000(0.0042) Grad: 202.9519  LR: 0.000002  \n","Epoch: [5][1900/3575] Elapsed 9m 37s (remain 8m 28s) Loss: 0.0001(0.0042) Grad: 580.6757  LR: 0.000002  \n","Epoch: [5][2000/3575] Elapsed 10m 7s (remain 7m 58s) Loss: 0.0000(0.0042) Grad: 529.4929  LR: 0.000002  \n","Epoch: [5][2100/3575] Elapsed 10m 37s (remain 7m 27s) Loss: 0.0000(0.0042) Grad: 150.9136  LR: 0.000002  \n","Epoch: [5][2200/3575] Elapsed 11m 8s (remain 6m 57s) Loss: 0.0001(0.0042) Grad: 830.3785  LR: 0.000002  \n","Epoch: [5][2300/3575] Elapsed 11m 38s (remain 6m 26s) Loss: 0.0039(0.0041) Grad: 6308.5239  LR: 0.000002  \n","Epoch: [5][2400/3575] Elapsed 12m 8s (remain 5m 56s) Loss: 0.0005(0.0040) Grad: 11496.4717  LR: 0.000001  \n","Epoch: [5][2500/3575] Elapsed 12m 38s (remain 5m 25s) Loss: 0.0062(0.0041) Grad: 32545.9629  LR: 0.000001  \n","Epoch: [5][2600/3575] Elapsed 13m 8s (remain 4m 55s) Loss: 0.0000(0.0041) Grad: 28.5316  LR: 0.000001  \n","Epoch: [5][2700/3575] Elapsed 13m 39s (remain 4m 25s) Loss: 0.0000(0.0041) Grad: 221.1404  LR: 0.000001  \n","Epoch: [5][2800/3575] Elapsed 14m 9s (remain 3m 54s) Loss: 0.0001(0.0040) Grad: 2833.2917  LR: 0.000001  \n","Epoch: [5][2900/3575] Elapsed 14m 39s (remain 3m 24s) Loss: 0.0000(0.0040) Grad: 15.7009  LR: 0.000001  \n","Epoch: [5][3000/3575] Elapsed 15m 9s (remain 2m 54s) Loss: 0.0000(0.0041) Grad: 210.2891  LR: 0.000001  \n","Epoch: [5][3100/3575] Elapsed 15m 40s (remain 2m 23s) Loss: 0.0000(0.0041) Grad: 63.6440  LR: 0.000001  \n","Epoch: [5][3200/3575] Elapsed 16m 10s (remain 1m 53s) Loss: 0.0007(0.0040) Grad: 6741.8008  LR: 0.000000  \n","Epoch: [5][3300/3575] Elapsed 16m 40s (remain 1m 23s) Loss: 0.0000(0.0040) Grad: 32.4790  LR: 0.000000  \n","Epoch: [5][3400/3575] Elapsed 17m 10s (remain 0m 52s) Loss: 0.0000(0.0040) Grad: 10.5175  LR: 0.000000  \n","Epoch: [5][3500/3575] Elapsed 17m 41s (remain 0m 22s) Loss: 0.0051(0.0039) Grad: 12114.9736  LR: 0.000000  \n","Epoch: [5][3574/3575] Elapsed 18m 3s (remain 0m 0s) Loss: 0.0000(0.0039) Grad: 51.4451  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 9m 47s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 18s (remain 3m 17s) Loss: 0.0000(0.0098) \n","EVAL: [200/1192] Elapsed 0m 35s (remain 2m 57s) Loss: 0.0000(0.0093) \n","EVAL: [300/1192] Elapsed 0m 53s (remain 2m 39s) Loss: 0.0001(0.0124) \n","EVAL: [400/1192] Elapsed 1m 11s (remain 2m 21s) Loss: 0.0240(0.0130) \n","EVAL: [500/1192] Elapsed 1m 29s (remain 2m 2s) Loss: 0.0182(0.0126) \n","EVAL: [600/1192] Elapsed 1m 46s (remain 1m 45s) Loss: 0.1109(0.0127) \n","EVAL: [700/1192] Elapsed 2m 4s (remain 1m 27s) Loss: 0.0599(0.0149) \n","EVAL: [800/1192] Elapsed 2m 22s (remain 1m 9s) Loss: 0.0145(0.0144) \n","EVAL: [900/1192] Elapsed 2m 40s (remain 0m 51s) Loss: 0.0001(0.0142) \n","EVAL: [1000/1192] Elapsed 2m 58s (remain 0m 33s) Loss: 0.0000(0.0135) \n","EVAL: [1100/1192] Elapsed 3m 15s (remain 0m 16s) Loss: 0.0163(0.0129) \n","EVAL: [1191/1192] Elapsed 3m 31s (remain 0m 0s) Loss: 0.0270(0.0122) \n","Epoch 5 - avg_train_loss: 0.0039  avg_val_loss: 0.0122  time: 1304s\n","Epoch 5 - Score: 0.8692\n","Epoch 5 - Save Best Score: 0.8692 Model\n","========== fold: 2 training ==========\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n","Epoch: [1][0/3575] Elapsed 0m 0s (remain 52m 40s) Loss: 0.7810(0.7810) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/3575] Elapsed 0m 33s (remain 19m 22s) Loss: 0.1373(0.4217) Grad: 12211.7783  LR: 0.000001  \n","Epoch: [1][200/3575] Elapsed 1m 4s (remain 17m 55s) Loss: 0.0984(0.2380) Grad: 2907.4717  LR: 0.000002  \n","Epoch: [1][300/3575] Elapsed 1m 34s (remain 17m 6s) Loss: 0.0655(0.1701) Grad: 7031.3027  LR: 0.000003  \n","Epoch: [1][400/3575] Elapsed 2m 4s (remain 16m 25s) Loss: 0.0077(0.1339) Grad: 1764.0074  LR: 0.000004  \n","Epoch: [1][500/3575] Elapsed 2m 34s (remain 15m 50s) Loss: 0.0186(0.1115) Grad: 3200.9216  LR: 0.000006  \n","Epoch: [1][600/3575] Elapsed 3m 5s (remain 15m 15s) Loss: 0.0083(0.0960) Grad: 3402.8179  LR: 0.000007  \n","Epoch: [1][700/3575] Elapsed 3m 35s (remain 14m 42s) Loss: 0.0097(0.0846) Grad: 1773.0034  LR: 0.000008  \n","Epoch: [1][800/3575] Elapsed 4m 5s (remain 14m 10s) Loss: 0.0392(0.0757) Grad: 5315.3486  LR: 0.000009  \n","Epoch: [1][900/3575] Elapsed 4m 36s (remain 13m 41s) Loss: 0.0298(0.0686) Grad: 12606.8896  LR: 0.000010  \n","Epoch: [1][1000/3575] Elapsed 5m 7s (remain 13m 9s) Loss: 0.0174(0.0630) Grad: 1130.2448  LR: 0.000011  \n","Epoch: [1][1100/3575] Elapsed 5m 37s (remain 12m 38s) Loss: 0.0038(0.0585) Grad: 3260.1316  LR: 0.000012  \n","Epoch: [1][1200/3575] Elapsed 6m 7s (remain 12m 6s) Loss: 0.0003(0.0549) Grad: 386.6635  LR: 0.000013  \n","Epoch: [1][1300/3575] Elapsed 6m 37s (remain 11m 35s) Loss: 0.0059(0.0518) Grad: 1214.6924  LR: 0.000015  \n","Epoch: [1][1400/3575] Elapsed 7m 8s (remain 11m 4s) Loss: 0.0102(0.0492) Grad: 2182.8960  LR: 0.000016  \n","Epoch: [1][1500/3575] Elapsed 7m 38s (remain 10m 33s) Loss: 0.0494(0.0467) Grad: 11333.7578  LR: 0.000017  \n","Epoch: [1][1600/3575] Elapsed 8m 8s (remain 10m 2s) Loss: 0.0010(0.0449) Grad: 828.1531  LR: 0.000018  \n","Epoch: [1][1700/3575] Elapsed 8m 38s (remain 9m 31s) Loss: 0.0152(0.0430) Grad: 1319.5736  LR: 0.000019  \n","Epoch: [1][1800/3575] Elapsed 9m 9s (remain 9m 0s) Loss: 0.0135(0.0414) Grad: 1844.5110  LR: 0.000020  \n","Epoch: [1][1900/3575] Elapsed 9m 39s (remain 8m 30s) Loss: 0.0089(0.0401) Grad: 1325.6429  LR: 0.000020  \n","Epoch: [1][2000/3575] Elapsed 10m 9s (remain 7m 59s) Loss: 0.0316(0.0387) Grad: 7177.0312  LR: 0.000020  \n","Epoch: [1][2100/3575] Elapsed 10m 40s (remain 7m 29s) Loss: 0.0009(0.0373) Grad: 460.7641  LR: 0.000020  \n","Epoch: [1][2200/3575] Elapsed 11m 10s (remain 6m 58s) Loss: 0.0029(0.0361) Grad: 756.8601  LR: 0.000019  \n","Epoch: [1][2300/3575] Elapsed 11m 40s (remain 6m 27s) Loss: 0.0116(0.0350) Grad: 5038.1436  LR: 0.000019  \n","Epoch: [1][2400/3575] Elapsed 12m 10s (remain 5m 57s) Loss: 0.0021(0.0340) Grad: 316.2930  LR: 0.000019  \n","Epoch: [1][2500/3575] Elapsed 12m 40s (remain 5m 26s) Loss: 0.0080(0.0331) Grad: 4026.8708  LR: 0.000019  \n","Epoch: [1][2600/3575] Elapsed 13m 11s (remain 4m 56s) Loss: 0.0001(0.0323) Grad: 20.8886  LR: 0.000019  \n","Epoch: [1][2700/3575] Elapsed 13m 41s (remain 4m 25s) Loss: 0.0582(0.0315) Grad: 4682.9497  LR: 0.000019  \n","Epoch: [1][2800/3575] Elapsed 14m 11s (remain 3m 55s) Loss: 0.0013(0.0306) Grad: 268.7876  LR: 0.000019  \n","Epoch: [1][2900/3575] Elapsed 14m 41s (remain 3m 24s) Loss: 0.0002(0.0300) Grad: 43.3380  LR: 0.000019  \n","Epoch: [1][3000/3575] Elapsed 15m 12s (remain 2m 54s) Loss: 0.0195(0.0294) Grad: 3183.7441  LR: 0.000018  \n","Epoch: [1][3100/3575] Elapsed 15m 42s (remain 2m 24s) Loss: 0.0269(0.0288) Grad: 4266.9268  LR: 0.000018  \n","Epoch: [1][3200/3575] Elapsed 16m 12s (remain 1m 53s) Loss: 0.0447(0.0282) Grad: 3513.9282  LR: 0.000018  \n","Epoch: [1][3300/3575] Elapsed 16m 42s (remain 1m 23s) Loss: 0.0205(0.0276) Grad: 1504.6776  LR: 0.000018  \n","Epoch: [1][3400/3575] Elapsed 17m 13s (remain 0m 52s) Loss: 0.0289(0.0271) Grad: 2781.5486  LR: 0.000018  \n","Epoch: [1][3500/3575] Elapsed 17m 43s (remain 0m 22s) Loss: 0.0013(0.0265) Grad: 185.4720  LR: 0.000018  \n","Epoch: [1][3574/3575] Elapsed 18m 5s (remain 0m 0s) Loss: 0.0019(0.0262) Grad: 258.1486  LR: 0.000018  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 10m 56s) Loss: 0.0114(0.0114) \n","EVAL: [100/1192] Elapsed 0m 18s (remain 3m 18s) Loss: 0.0266(0.0075) \n","EVAL: [200/1192] Elapsed 0m 36s (remain 2m 58s) Loss: 0.0406(0.0074) \n","EVAL: [300/1192] Elapsed 0m 54s (remain 2m 39s) Loss: 0.0068(0.0075) \n","EVAL: [400/1192] Elapsed 1m 11s (remain 2m 21s) Loss: 0.0024(0.0081) \n","EVAL: [500/1192] Elapsed 1m 29s (remain 2m 3s) Loss: 0.0002(0.0079) \n","EVAL: [600/1192] Elapsed 1m 47s (remain 1m 45s) Loss: 0.0047(0.0080) \n","EVAL: [700/1192] Elapsed 2m 5s (remain 1m 27s) Loss: 0.0046(0.0089) \n","EVAL: [800/1192] Elapsed 2m 23s (remain 1m 9s) Loss: 0.0030(0.0090) \n","EVAL: [900/1192] Elapsed 2m 41s (remain 0m 52s) Loss: 0.0033(0.0093) \n","EVAL: [1000/1192] Elapsed 2m 58s (remain 0m 34s) Loss: 0.0020(0.0092) \n","EVAL: [1100/1192] Elapsed 3m 16s (remain 0m 16s) Loss: 0.0233(0.0088) \n","EVAL: [1191/1192] Elapsed 3m 32s (remain 0m 0s) Loss: 0.0005(0.0085) \n","Epoch 1 - avg_train_loss: 0.0262  avg_val_loss: 0.0085  time: 1304s\n","Epoch 1 - Score: 0.8325\n","Epoch 1 - Save Best Score: 0.8325 Model\n","Epoch: [2][0/3575] Elapsed 0m 0s (remain 43m 36s) Loss: 0.0017(0.0017) Grad: 2927.2324  LR: 0.000018  \n","Epoch: [2][100/3575] Elapsed 0m 32s (remain 18m 53s) Loss: 0.0031(0.0091) Grad: 6035.5166  LR: 0.000018  \n","Epoch: [2][200/3575] Elapsed 1m 3s (remain 17m 51s) Loss: 0.0168(0.0077) Grad: 62335.2188  LR: 0.000018  \n","Epoch: [2][300/3575] Elapsed 1m 34s (remain 17m 3s) Loss: 0.0022(0.0073) Grad: 8007.2324  LR: 0.000017  \n","Epoch: [2][400/3575] Elapsed 2m 4s (remain 16m 24s) Loss: 0.0088(0.0073) Grad: 15554.2949  LR: 0.000017  \n","Epoch: [2][500/3575] Elapsed 2m 34s (remain 15m 49s) Loss: 0.0251(0.0076) Grad: 26936.0352  LR: 0.000017  \n","Epoch: [2][600/3575] Elapsed 3m 4s (remain 15m 15s) Loss: 0.0100(0.0076) Grad: 14842.8096  LR: 0.000017  \n","Epoch: [2][700/3575] Elapsed 3m 35s (remain 14m 43s) Loss: 0.0001(0.0079) Grad: 1623.9618  LR: 0.000017  \n","Epoch: [2][800/3575] Elapsed 4m 5s (remain 14m 11s) Loss: 0.0028(0.0075) Grad: 7125.7266  LR: 0.000017  \n","Epoch: [2][900/3575] Elapsed 4m 36s (remain 13m 40s) Loss: 0.0003(0.0077) Grad: 1460.5393  LR: 0.000017  \n","Epoch: [2][1000/3575] Elapsed 5m 6s (remain 13m 8s) Loss: 0.0046(0.0076) Grad: 50881.2031  LR: 0.000017  \n","Epoch: [2][1100/3575] Elapsed 5m 37s (remain 12m 37s) Loss: 0.0033(0.0077) Grad: 13914.0098  LR: 0.000016  \n","Epoch: [2][1200/3575] Elapsed 6m 7s (remain 12m 6s) Loss: 0.0292(0.0078) Grad: 57776.9141  LR: 0.000016  \n","Epoch: [2][1300/3575] Elapsed 6m 37s (remain 11m 34s) Loss: 0.0518(0.0079) Grad: 284530.3750  LR: 0.000016  \n","Epoch: [2][1400/3575] Elapsed 7m 7s (remain 11m 3s) Loss: 0.0067(0.0078) Grad: 32942.9453  LR: 0.000016  \n","Epoch: [2][1500/3575] Elapsed 7m 38s (remain 10m 33s) Loss: 0.0599(0.0078) Grad: 91985.8203  LR: 0.000016  \n","Epoch: [2][1600/3575] Elapsed 8m 8s (remain 10m 2s) Loss: 0.0008(0.0076) Grad: 2885.9290  LR: 0.000016  \n","Epoch: [2][1700/3575] Elapsed 8m 38s (remain 9m 31s) Loss: 0.0098(0.0077) Grad: 24804.3613  LR: 0.000016  \n","Epoch: [2][1800/3575] Elapsed 9m 8s (remain 9m 0s) Loss: 0.0027(0.0078) Grad: 3247.5627  LR: 0.000016  \n","Epoch: [2][1900/3575] Elapsed 9m 39s (remain 8m 30s) Loss: 0.0234(0.0078) Grad: 132803.3906  LR: 0.000015  \n","Epoch: [2][2000/3575] Elapsed 10m 9s (remain 7m 59s) Loss: 0.0018(0.0077) Grad: 16740.3066  LR: 0.000015  \n","Epoch: [2][2100/3575] Elapsed 10m 39s (remain 7m 28s) Loss: 0.0016(0.0076) Grad: 3020.3501  LR: 0.000015  \n","Epoch: [2][2200/3575] Elapsed 11m 10s (remain 6m 58s) Loss: 0.0035(0.0077) Grad: 9907.8838  LR: 0.000015  \n","Epoch: [2][2300/3575] Elapsed 11m 40s (remain 6m 27s) Loss: 0.0009(0.0078) Grad: 2762.7495  LR: 0.000015  \n","Epoch: [2][2400/3575] Elapsed 12m 10s (remain 5m 57s) Loss: 0.0000(0.0078) Grad: 149.5752  LR: 0.000015  \n","Epoch: [2][2500/3575] Elapsed 12m 41s (remain 5m 26s) Loss: 0.0024(0.0078) Grad: 12896.1133  LR: 0.000015  \n","Epoch: [2][2600/3575] Elapsed 13m 11s (remain 4m 56s) Loss: 0.0150(0.0078) Grad: 9625.3193  LR: 0.000015  \n","Epoch: [2][2700/3575] Elapsed 13m 41s (remain 4m 25s) Loss: 0.0075(0.0078) Grad: 18927.6855  LR: 0.000014  \n","Epoch: [2][2800/3575] Elapsed 14m 11s (remain 3m 55s) Loss: 0.0319(0.0078) Grad: 55661.9531  LR: 0.000014  \n","Epoch: [2][2900/3575] Elapsed 14m 42s (remain 3m 24s) Loss: 0.0158(0.0077) Grad: 49690.3555  LR: 0.000014  \n","Epoch: [2][3000/3575] Elapsed 15m 12s (remain 2m 54s) Loss: 0.0045(0.0078) Grad: 19718.2402  LR: 0.000014  \n","Epoch: [2][3100/3575] Elapsed 15m 43s (remain 2m 24s) Loss: 0.0002(0.0078) Grad: 866.7567  LR: 0.000014  \n","Epoch: [2][3200/3575] Elapsed 16m 13s (remain 1m 53s) Loss: 0.0136(0.0078) Grad: 29062.8730  LR: 0.000014  \n","Epoch: [2][3300/3575] Elapsed 16m 43s (remain 1m 23s) Loss: 0.0313(0.0077) Grad: 54038.6016  LR: 0.000014  \n","Epoch: [2][3400/3575] Elapsed 17m 14s (remain 0m 52s) Loss: 0.0036(0.0077) Grad: 12928.4404  LR: 0.000014  \n","Epoch: [2][3500/3575] Elapsed 17m 44s (remain 0m 22s) Loss: 0.0084(0.0076) Grad: 289082.3750  LR: 0.000013  \n","Epoch: [2][3574/3575] Elapsed 18m 6s (remain 0m 0s) Loss: 0.0048(0.0077) Grad: 7793.7969  LR: 0.000013  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 10m 42s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 18s (remain 3m 18s) Loss: 0.0265(0.0075) \n","EVAL: [200/1192] Elapsed 0m 36s (remain 2m 58s) Loss: 0.0093(0.0078) \n","EVAL: [300/1192] Elapsed 0m 54s (remain 2m 39s) Loss: 0.0164(0.0087) \n","EVAL: [400/1192] Elapsed 1m 11s (remain 2m 21s) Loss: 0.0004(0.0087) \n","EVAL: [500/1192] Elapsed 1m 29s (remain 2m 3s) Loss: 0.0000(0.0081) \n","EVAL: [600/1192] Elapsed 1m 47s (remain 1m 45s) Loss: 0.0114(0.0081) \n","EVAL: [700/1192] Elapsed 2m 5s (remain 1m 27s) Loss: 0.0046(0.0087) \n","EVAL: [800/1192] Elapsed 2m 23s (remain 1m 9s) Loss: 0.0000(0.0088) \n","EVAL: [900/1192] Elapsed 2m 41s (remain 0m 52s) Loss: 0.0026(0.0088) \n","EVAL: [1000/1192] Elapsed 2m 59s (remain 0m 34s) Loss: 0.0002(0.0085) \n","EVAL: [1100/1192] Elapsed 3m 16s (remain 0m 16s) Loss: 0.0442(0.0081) \n","EVAL: [1191/1192] Elapsed 3m 32s (remain 0m 0s) Loss: 0.0001(0.0079) \n","Epoch 2 - avg_train_loss: 0.0077  avg_val_loss: 0.0079  time: 1305s\n","Epoch 2 - Score: 0.8728\n","Epoch 2 - Save Best Score: 0.8728 Model\n","Epoch: [3][0/3575] Elapsed 0m 0s (remain 42m 0s) Loss: 0.0004(0.0004) Grad: 3453.3162  LR: 0.000013  \n","Epoch: [3][100/3575] Elapsed 0m 32s (remain 18m 51s) Loss: 0.0041(0.0051) Grad: 7173.1313  LR: 0.000013  \n","Epoch: [3][200/3575] Elapsed 1m 3s (remain 17m 47s) Loss: 0.0018(0.0065) Grad: 11630.1006  LR: 0.000013  \n","Epoch: [3][300/3575] Elapsed 1m 33s (remain 17m 1s) Loss: 0.0000(0.0064) Grad: 14.0496  LR: 0.000013  \n","Epoch: [3][400/3575] Elapsed 2m 4s (remain 16m 23s) Loss: 0.0031(0.0064) Grad: 4940.4688  LR: 0.000013  \n","Epoch: [3][500/3575] Elapsed 2m 34s (remain 15m 47s) Loss: 0.0510(0.0063) Grad: 51946.5703  LR: 0.000013  \n","Epoch: [3][600/3575] Elapsed 3m 4s (remain 15m 14s) Loss: 0.0000(0.0061) Grad: 95.3460  LR: 0.000013  \n","Epoch: [3][700/3575] Elapsed 3m 35s (remain 14m 41s) Loss: 0.0000(0.0059) Grad: 240.8394  LR: 0.000012  \n","Epoch: [3][800/3575] Elapsed 4m 5s (remain 14m 9s) Loss: 0.0071(0.0059) Grad: 16840.8770  LR: 0.000012  \n","Epoch: [3][900/3575] Elapsed 4m 35s (remain 13m 37s) Loss: 0.0002(0.0058) Grad: 1096.9264  LR: 0.000012  \n","Epoch: [3][1000/3575] Elapsed 5m 5s (remain 13m 6s) Loss: 0.0727(0.0056) Grad: 43430.0273  LR: 0.000012  \n","Epoch: [3][1100/3575] Elapsed 5m 36s (remain 12m 35s) Loss: 0.0000(0.0057) Grad: 6.1114  LR: 0.000012  \n","Epoch: [3][1200/3575] Elapsed 6m 6s (remain 12m 4s) Loss: 0.0064(0.0059) Grad: 11689.5430  LR: 0.000012  \n","Epoch: [3][1300/3575] Elapsed 6m 36s (remain 11m 33s) Loss: 0.0000(0.0058) Grad: 24.0722  LR: 0.000012  \n","Epoch: [3][1400/3575] Elapsed 7m 7s (remain 11m 2s) Loss: 0.0000(0.0058) Grad: 194.1817  LR: 0.000012  \n","Epoch: [3][1500/3575] Elapsed 7m 37s (remain 10m 32s) Loss: 0.0000(0.0058) Grad: 258.1564  LR: 0.000011  \n","Epoch: [3][1600/3575] Elapsed 8m 7s (remain 10m 1s) Loss: 0.0039(0.0058) Grad: 7487.0166  LR: 0.000011  \n","Epoch: [3][1700/3575] Elapsed 8m 38s (remain 9m 30s) Loss: 0.0000(0.0058) Grad: 41.4192  LR: 0.000011  \n","Epoch: [3][1800/3575] Elapsed 9m 8s (remain 9m 0s) Loss: 0.0007(0.0058) Grad: 4154.2314  LR: 0.000011  \n","Epoch: [3][1900/3575] Elapsed 9m 38s (remain 8m 29s) Loss: 0.0000(0.0058) Grad: 131.1372  LR: 0.000011  \n","Epoch: [3][2000/3575] Elapsed 10m 8s (remain 7m 58s) Loss: 0.0040(0.0057) Grad: 9056.6182  LR: 0.000011  \n","Epoch: [3][2100/3575] Elapsed 10m 39s (remain 7m 28s) Loss: 0.0000(0.0058) Grad: 51.8777  LR: 0.000011  \n","Epoch: [3][2200/3575] Elapsed 11m 9s (remain 6m 57s) Loss: 0.0018(0.0059) Grad: 4518.7822  LR: 0.000011  \n","Epoch: [3][2300/3575] Elapsed 11m 39s (remain 6m 27s) Loss: 0.0013(0.0058) Grad: 8225.6113  LR: 0.000010  \n","Epoch: [3][2400/3575] Elapsed 12m 10s (remain 5m 56s) Loss: 0.0000(0.0058) Grad: 618.5244  LR: 0.000010  \n","Epoch: [3][2500/3575] Elapsed 12m 40s (remain 5m 26s) Loss: 0.0001(0.0058) Grad: 1360.3456  LR: 0.000010  \n","Epoch: [3][2600/3575] Elapsed 13m 10s (remain 4m 56s) Loss: 0.0088(0.0060) Grad: 10657.2676  LR: 0.000010  \n","Epoch: [3][2700/3575] Elapsed 13m 40s (remain 4m 25s) Loss: 0.0123(0.0059) Grad: 49652.1172  LR: 0.000010  \n","Epoch: [3][2800/3575] Elapsed 14m 11s (remain 3m 55s) Loss: 0.0000(0.0059) Grad: 160.7990  LR: 0.000010  \n","Epoch: [3][2900/3575] Elapsed 14m 41s (remain 3m 24s) Loss: 0.0000(0.0059) Grad: 87.9413  LR: 0.000010  \n","Epoch: [3][3000/3575] Elapsed 15m 12s (remain 2m 54s) Loss: 0.0000(0.0058) Grad: 13.0980  LR: 0.000010  \n","Epoch: [3][3100/3575] Elapsed 15m 42s (remain 2m 24s) Loss: 0.0000(0.0059) Grad: 96.3336  LR: 0.000009  \n","Epoch: [3][3200/3575] Elapsed 16m 12s (remain 1m 53s) Loss: 0.0000(0.0059) Grad: 18.1662  LR: 0.000009  \n","Epoch: [3][3300/3575] Elapsed 16m 42s (remain 1m 23s) Loss: 0.0000(0.0059) Grad: 58.6467  LR: 0.000009  \n","Epoch: [3][3400/3575] Elapsed 17m 12s (remain 0m 52s) Loss: 0.0004(0.0059) Grad: 1372.4905  LR: 0.000009  \n","Epoch: [3][3500/3575] Elapsed 17m 43s (remain 0m 22s) Loss: 0.0000(0.0059) Grad: 43.9323  LR: 0.000009  \n","Epoch: [3][3574/3575] Elapsed 18m 5s (remain 0m 0s) Loss: 0.0062(0.0059) Grad: 26910.6484  LR: 0.000009  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 10m 48s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 18s (remain 3m 18s) Loss: 0.0699(0.0097) \n","EVAL: [200/1192] Elapsed 0m 36s (remain 2m 58s) Loss: 0.0091(0.0096) \n","EVAL: [300/1192] Elapsed 0m 54s (remain 2m 40s) Loss: 0.0161(0.0102) \n","EVAL: [400/1192] Elapsed 1m 11s (remain 2m 21s) Loss: 0.0003(0.0103) \n","EVAL: [500/1192] Elapsed 1m 29s (remain 2m 3s) Loss: 0.0000(0.0096) \n","EVAL: [600/1192] Elapsed 1m 47s (remain 1m 45s) Loss: 0.0040(0.0096) \n","EVAL: [700/1192] Elapsed 2m 5s (remain 1m 27s) Loss: 0.0055(0.0105) \n","EVAL: [800/1192] Elapsed 2m 23s (remain 1m 9s) Loss: 0.0000(0.0106) \n","EVAL: [900/1192] Elapsed 2m 40s (remain 0m 51s) Loss: 0.0144(0.0109) \n","EVAL: [1000/1192] Elapsed 2m 58s (remain 0m 34s) Loss: 0.0002(0.0106) \n","EVAL: [1100/1192] Elapsed 3m 16s (remain 0m 16s) Loss: 0.0516(0.0102) \n","EVAL: [1191/1192] Elapsed 3m 32s (remain 0m 0s) Loss: 0.0000(0.0098) \n","Epoch 3 - avg_train_loss: 0.0059  avg_val_loss: 0.0098  time: 1304s\n","Epoch 3 - Score: 0.8720\n","Epoch: [4][0/3575] Elapsed 0m 0s (remain 41m 1s) Loss: 0.0000(0.0000) Grad: 1006.1440  LR: 0.000009  \n","Epoch: [4][100/3575] Elapsed 0m 31s (remain 17m 47s) Loss: 0.0001(0.0027) Grad: 297.6437  LR: 0.000009  \n","Epoch: [4][200/3575] Elapsed 1m 1s (remain 17m 9s) Loss: 0.0000(0.0038) Grad: 32.3683  LR: 0.000009  \n","Epoch: [4][300/3575] Elapsed 1m 31s (remain 16m 36s) Loss: 0.0000(0.0044) Grad: 101.6513  LR: 0.000009  \n","Epoch: [4][400/3575] Elapsed 2m 1s (remain 16m 4s) Loss: 0.0000(0.0044) Grad: 28.5876  LR: 0.000008  \n","Epoch: [4][500/3575] Elapsed 2m 32s (remain 15m 33s) Loss: 0.0000(0.0050) Grad: 80.9735  LR: 0.000008  \n","Epoch: [4][600/3575] Elapsed 3m 2s (remain 15m 2s) Loss: 0.0001(0.0049) Grad: 311.8507  LR: 0.000008  \n","Epoch: [4][700/3575] Elapsed 3m 32s (remain 14m 32s) Loss: 0.0000(0.0051) Grad: 18.1124  LR: 0.000008  \n","Epoch: [4][800/3575] Elapsed 4m 2s (remain 14m 1s) Loss: 0.0002(0.0050) Grad: 1810.4319  LR: 0.000008  \n","Epoch: [4][900/3575] Elapsed 4m 33s (remain 13m 30s) Loss: 0.0000(0.0049) Grad: 260.4718  LR: 0.000008  \n","Epoch: [4][1000/3575] Elapsed 5m 3s (remain 13m 0s) Loss: 0.0000(0.0050) Grad: 33.8506  LR: 0.000008  \n","Epoch: [4][1100/3575] Elapsed 5m 33s (remain 12m 30s) Loss: 0.0000(0.0052) Grad: 116.9077  LR: 0.000008  \n","Epoch: [4][1200/3575] Elapsed 6m 4s (remain 11m 59s) Loss: 0.0156(0.0051) Grad: 135344.0938  LR: 0.000007  \n","Epoch: [4][1300/3575] Elapsed 6m 34s (remain 11m 29s) Loss: 0.0000(0.0051) Grad: 21.1624  LR: 0.000007  \n","Epoch: [4][1400/3575] Elapsed 7m 4s (remain 10m 58s) Loss: 0.0000(0.0049) Grad: 208.8714  LR: 0.000007  \n","Epoch: [4][1500/3575] Elapsed 7m 34s (remain 10m 28s) Loss: 0.0000(0.0049) Grad: 16.8987  LR: 0.000007  \n","Epoch: [4][1600/3575] Elapsed 8m 5s (remain 9m 58s) Loss: 0.0002(0.0049) Grad: 3337.0298  LR: 0.000007  \n","Epoch: [4][1700/3575] Elapsed 8m 35s (remain 9m 27s) Loss: 0.0024(0.0049) Grad: 27450.0469  LR: 0.000007  \n","Epoch: [4][1800/3575] Elapsed 9m 5s (remain 8m 57s) Loss: 0.0075(0.0049) Grad: 24352.0957  LR: 0.000007  \n","Epoch: [4][1900/3575] Elapsed 9m 35s (remain 8m 27s) Loss: 0.0029(0.0050) Grad: 7594.3589  LR: 0.000007  \n","Epoch: [4][2000/3575] Elapsed 10m 6s (remain 7m 56s) Loss: 0.0110(0.0049) Grad: 25063.9941  LR: 0.000006  \n","Epoch: [4][2100/3575] Elapsed 10m 36s (remain 7m 26s) Loss: 0.0002(0.0048) Grad: 928.7809  LR: 0.000006  \n","Epoch: [4][2200/3575] Elapsed 11m 6s (remain 6m 56s) Loss: 0.0000(0.0048) Grad: 19.6672  LR: 0.000006  \n","Epoch: [4][2300/3575] Elapsed 11m 37s (remain 6m 25s) Loss: 0.0119(0.0048) Grad: 60899.4336  LR: 0.000006  \n","Epoch: [4][2400/3575] Elapsed 12m 7s (remain 5m 55s) Loss: 0.0000(0.0049) Grad: 9.3138  LR: 0.000006  \n","Epoch: [4][2500/3575] Elapsed 12m 37s (remain 5m 25s) Loss: 0.0000(0.0048) Grad: 37.4852  LR: 0.000006  \n","Epoch: [4][2600/3575] Elapsed 13m 7s (remain 4m 55s) Loss: 0.0314(0.0049) Grad: 202170.3125  LR: 0.000006  \n","Epoch: [4][2700/3575] Elapsed 13m 38s (remain 4m 24s) Loss: 0.0000(0.0048) Grad: 211.4162  LR: 0.000006  \n","Epoch: [4][2800/3575] Elapsed 14m 8s (remain 3m 54s) Loss: 0.0000(0.0048) Grad: 154.9426  LR: 0.000005  \n","Epoch: [4][2900/3575] Elapsed 14m 38s (remain 3m 24s) Loss: 0.0001(0.0048) Grad: 2681.9441  LR: 0.000005  \n","Epoch: [4][3000/3575] Elapsed 15m 9s (remain 2m 53s) Loss: 0.0422(0.0049) Grad: 37317.2461  LR: 0.000005  \n","Epoch: [4][3100/3575] Elapsed 15m 39s (remain 2m 23s) Loss: 0.0011(0.0048) Grad: 21685.5508  LR: 0.000005  \n","Epoch: [4][3200/3575] Elapsed 16m 9s (remain 1m 53s) Loss: 0.0001(0.0048) Grad: 563.1746  LR: 0.000005  \n","Epoch: [4][3300/3575] Elapsed 16m 39s (remain 1m 23s) Loss: 0.0000(0.0048) Grad: 38.6496  LR: 0.000005  \n","Epoch: [4][3400/3575] Elapsed 17m 10s (remain 0m 52s) Loss: 0.0000(0.0048) Grad: 333.9922  LR: 0.000005  \n","Epoch: [4][3500/3575] Elapsed 17m 40s (remain 0m 22s) Loss: 0.0026(0.0047) Grad: 11586.0547  LR: 0.000005  \n","Epoch: [4][3574/3575] Elapsed 18m 2s (remain 0m 0s) Loss: 0.0058(0.0047) Grad: 74556.4531  LR: 0.000004  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 11m 10s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 18s (remain 3m 18s) Loss: 0.1099(0.0112) \n","EVAL: [200/1192] Elapsed 0m 36s (remain 2m 58s) Loss: 0.0103(0.0104) \n","EVAL: [300/1192] Elapsed 0m 54s (remain 2m 40s) Loss: 0.0151(0.0114) \n","EVAL: [400/1192] Elapsed 1m 11s (remain 2m 21s) Loss: 0.0000(0.0116) \n","EVAL: [500/1192] Elapsed 1m 29s (remain 2m 3s) Loss: 0.0000(0.0111) \n","EVAL: [600/1192] Elapsed 1m 47s (remain 1m 45s) Loss: 0.0056(0.0110) \n","EVAL: [700/1192] Elapsed 2m 5s (remain 1m 27s) Loss: 0.0042(0.0117) \n","EVAL: [800/1192] Elapsed 2m 23s (remain 1m 9s) Loss: 0.0000(0.0117) \n","EVAL: [900/1192] Elapsed 2m 41s (remain 0m 52s) Loss: 0.0090(0.0119) \n","EVAL: [1000/1192] Elapsed 2m 58s (remain 0m 34s) Loss: 0.0001(0.0115) \n","EVAL: [1100/1192] Elapsed 3m 16s (remain 0m 16s) Loss: 0.0518(0.0110) \n","EVAL: [1191/1192] Elapsed 3m 32s (remain 0m 0s) Loss: 0.0000(0.0107) \n","Epoch 4 - avg_train_loss: 0.0047  avg_val_loss: 0.0107  time: 1301s\n","Epoch 4 - Score: 0.8807\n","Epoch 4 - Save Best Score: 0.8807 Model\n","Epoch: [5][0/3575] Elapsed 0m 0s (remain 44m 54s) Loss: 0.0014(0.0014) Grad: 18603.7910  LR: 0.000004  \n","Epoch: [5][100/3575] Elapsed 0m 33s (remain 19m 1s) Loss: 0.0002(0.0021) Grad: 2890.5398  LR: 0.000004  \n","Epoch: [5][200/3575] Elapsed 1m 3s (remain 17m 51s) Loss: 0.0302(0.0026) Grad: 18568.5488  LR: 0.000004  \n","Epoch: [5][300/3575] Elapsed 1m 34s (remain 17m 3s) Loss: 0.0000(0.0029) Grad: 668.2537  LR: 0.000004  \n","Epoch: [5][400/3575] Elapsed 2m 4s (remain 16m 23s) Loss: 0.0002(0.0030) Grad: 1043.8107  LR: 0.000004  \n","Epoch: [5][500/3575] Elapsed 2m 34s (remain 15m 48s) Loss: 0.0050(0.0029) Grad: 27126.0078  LR: 0.000004  \n","Epoch: [5][600/3575] Elapsed 3m 4s (remain 15m 14s) Loss: 0.0013(0.0030) Grad: 29293.3125  LR: 0.000004  \n","Epoch: [5][700/3575] Elapsed 3m 35s (remain 14m 41s) Loss: 0.0000(0.0029) Grad: 31.4940  LR: 0.000004  \n","Epoch: [5][800/3575] Elapsed 4m 5s (remain 14m 9s) Loss: 0.0020(0.0030) Grad: 5071.1279  LR: 0.000003  \n","Epoch: [5][900/3575] Elapsed 4m 35s (remain 13m 37s) Loss: 0.0096(0.0033) Grad: 24607.8770  LR: 0.000003  \n","Epoch: [5][1000/3575] Elapsed 5m 5s (remain 13m 6s) Loss: 0.0000(0.0033) Grad: 10.5283  LR: 0.000003  \n","Epoch: [5][1100/3575] Elapsed 5m 36s (remain 12m 35s) Loss: 0.0001(0.0034) Grad: 1230.4490  LR: 0.000003  \n","Epoch: [5][1200/3575] Elapsed 6m 6s (remain 12m 4s) Loss: 0.0046(0.0034) Grad: 19039.9531  LR: 0.000003  \n","Epoch: [5][1300/3575] Elapsed 6m 36s (remain 11m 33s) Loss: 0.0127(0.0034) Grad: 36359.5742  LR: 0.000003  \n","Epoch: [5][1400/3575] Elapsed 7m 6s (remain 11m 2s) Loss: 0.0042(0.0036) Grad: 5873.5161  LR: 0.000003  \n","Epoch: [5][1500/3575] Elapsed 7m 37s (remain 10m 31s) Loss: 0.0146(0.0035) Grad: 17667.3555  LR: 0.000003  \n","Epoch: [5][1600/3575] Elapsed 8m 7s (remain 10m 1s) Loss: 0.0000(0.0036) Grad: 73.9446  LR: 0.000002  \n","Epoch: [5][1700/3575] Elapsed 8m 37s (remain 9m 30s) Loss: 0.0001(0.0036) Grad: 1003.2448  LR: 0.000002  \n","Epoch: [5][1800/3575] Elapsed 9m 7s (remain 8m 59s) Loss: 0.0001(0.0036) Grad: 741.2067  LR: 0.000002  \n","Epoch: [5][1900/3575] Elapsed 9m 38s (remain 8m 29s) Loss: 0.0001(0.0036) Grad: 1001.4075  LR: 0.000002  \n","Epoch: [5][2000/3575] Elapsed 10m 8s (remain 7m 58s) Loss: 0.0070(0.0037) Grad: 83060.4609  LR: 0.000002  \n","Epoch: [5][2100/3575] Elapsed 10m 38s (remain 7m 28s) Loss: 0.0041(0.0036) Grad: 5739.7993  LR: 0.000002  \n","Epoch: [5][2200/3575] Elapsed 11m 9s (remain 6m 57s) Loss: 0.0112(0.0036) Grad: 15246.3994  LR: 0.000002  \n","Epoch: [5][2300/3575] Elapsed 11m 39s (remain 6m 27s) Loss: 0.0000(0.0036) Grad: 60.0467  LR: 0.000002  \n","Epoch: [5][2400/3575] Elapsed 12m 9s (remain 5m 56s) Loss: 0.0001(0.0037) Grad: 221.7490  LR: 0.000001  \n","Epoch: [5][2500/3575] Elapsed 12m 39s (remain 5m 26s) Loss: 0.0000(0.0037) Grad: 4.4267  LR: 0.000001  \n","Epoch: [5][2600/3575] Elapsed 13m 10s (remain 4m 55s) Loss: 0.0000(0.0038) Grad: 72.2314  LR: 0.000001  \n","Epoch: [5][2700/3575] Elapsed 13m 40s (remain 4m 25s) Loss: 0.0051(0.0038) Grad: 28113.1055  LR: 0.000001  \n","Epoch: [5][2800/3575] Elapsed 14m 10s (remain 3m 55s) Loss: 0.0000(0.0038) Grad: 8.0349  LR: 0.000001  \n","Epoch: [5][2900/3575] Elapsed 14m 40s (remain 3m 24s) Loss: 0.0000(0.0038) Grad: 26.9666  LR: 0.000001  \n","Epoch: [5][3000/3575] Elapsed 15m 11s (remain 2m 54s) Loss: 0.0000(0.0037) Grad: 20.7568  LR: 0.000001  \n","Epoch: [5][3100/3575] Elapsed 15m 41s (remain 2m 23s) Loss: 0.0004(0.0037) Grad: 5211.6504  LR: 0.000001  \n","Epoch: [5][3200/3575] Elapsed 16m 11s (remain 1m 53s) Loss: 0.0000(0.0037) Grad: 137.2032  LR: 0.000000  \n","Epoch: [5][3300/3575] Elapsed 16m 41s (remain 1m 23s) Loss: 0.0241(0.0037) Grad: 180311.8438  LR: 0.000000  \n","Epoch: [5][3400/3575] Elapsed 17m 12s (remain 0m 52s) Loss: 0.1811(0.0038) Grad: 130538.0078  LR: 0.000000  \n","Epoch: [5][3500/3575] Elapsed 17m 42s (remain 0m 22s) Loss: 0.0000(0.0039) Grad: 16.6508  LR: 0.000000  \n","Epoch: [5][3574/3575] Elapsed 18m 4s (remain 0m 0s) Loss: 0.0000(0.0039) Grad: 10.7760  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 10m 39s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 18s (remain 3m 18s) Loss: 0.1123(0.0122) \n","EVAL: [200/1192] Elapsed 0m 36s (remain 2m 58s) Loss: 0.0133(0.0111) \n","EVAL: [300/1192] Elapsed 0m 54s (remain 2m 40s) Loss: 0.0131(0.0116) \n","EVAL: [400/1192] Elapsed 1m 11s (remain 2m 21s) Loss: 0.0000(0.0117) \n","EVAL: [500/1192] Elapsed 1m 29s (remain 2m 3s) Loss: 0.0000(0.0112) \n","EVAL: [600/1192] Elapsed 1m 47s (remain 1m 45s) Loss: 0.0069(0.0110) \n","EVAL: [700/1192] Elapsed 2m 5s (remain 1m 27s) Loss: 0.0037(0.0119) \n","EVAL: [800/1192] Elapsed 2m 23s (remain 1m 9s) Loss: 0.0000(0.0119) \n","EVAL: [900/1192] Elapsed 2m 41s (remain 0m 52s) Loss: 0.0098(0.0119) \n","EVAL: [1000/1192] Elapsed 2m 58s (remain 0m 34s) Loss: 0.0001(0.0116) \n","EVAL: [1100/1192] Elapsed 3m 16s (remain 0m 16s) Loss: 0.0537(0.0111) \n","EVAL: [1191/1192] Elapsed 3m 32s (remain 0m 0s) Loss: 0.0000(0.0108) \n","Epoch 5 - avg_train_loss: 0.0039  avg_val_loss: 0.0108  time: 1303s\n","Epoch 5 - Score: 0.8823\n","Epoch 5 - Save Best Score: 0.8823 Model\n","========== fold: 3 training ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n","Epoch: [1][0/3575] Elapsed 0m 0s (remain 52m 17s) Loss: 0.7500(0.7500) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/3575] Elapsed 0m 33s (remain 19m 10s) Loss: 0.2682(0.4008) Grad: 43841.8516  LR: 0.000001  \n","Epoch: [1][200/3575] Elapsed 1m 3s (remain 17m 49s) Loss: 0.0429(0.2334) Grad: 2146.8909  LR: 0.000002  \n","Epoch: [1][300/3575] Elapsed 1m 33s (remain 17m 1s) Loss: 0.0095(0.1673) Grad: 1338.3951  LR: 0.000003  \n","Epoch: [1][400/3575] Elapsed 2m 4s (remain 16m 23s) Loss: 0.0077(0.1320) Grad: 6672.3311  LR: 0.000004  \n","Epoch: [1][500/3575] Elapsed 2m 34s (remain 15m 47s) Loss: 0.0062(0.1103) Grad: 1444.9076  LR: 0.000006  \n","Epoch: [1][600/3575] Elapsed 3m 4s (remain 15m 14s) Loss: 0.0306(0.0958) Grad: 12105.4736  LR: 0.000007  \n","Epoch: [1][700/3575] Elapsed 3m 34s (remain 14m 41s) Loss: 0.0079(0.0848) Grad: 4832.1982  LR: 0.000008  \n","Epoch: [1][800/3575] Elapsed 4m 5s (remain 14m 9s) Loss: 0.0130(0.0763) Grad: 22877.1504  LR: 0.000009  \n","Epoch: [1][900/3575] Elapsed 4m 35s (remain 13m 37s) Loss: 0.0009(0.0697) Grad: 653.4996  LR: 0.000010  \n","Epoch: [1][1000/3575] Elapsed 5m 7s (remain 13m 9s) Loss: 0.0066(0.0642) Grad: 4975.6685  LR: 0.000011  \n","Epoch: [1][1100/3575] Elapsed 5m 37s (remain 12m 37s) Loss: 0.0334(0.0598) Grad: 6471.5791  LR: 0.000012  \n","Epoch: [1][1200/3575] Elapsed 6m 7s (remain 12m 6s) Loss: 0.0079(0.0560) Grad: 10538.0557  LR: 0.000013  \n","Epoch: [1][1300/3575] Elapsed 6m 37s (remain 11m 35s) Loss: 0.0090(0.0529) Grad: 5217.9014  LR: 0.000015  \n","Epoch: [1][1400/3575] Elapsed 7m 7s (remain 11m 4s) Loss: 0.0114(0.0502) Grad: 2317.5879  LR: 0.000016  \n","Epoch: [1][1500/3575] Elapsed 7m 38s (remain 10m 33s) Loss: 0.0010(0.0475) Grad: 435.5854  LR: 0.000017  \n","Epoch: [1][1600/3575] Elapsed 8m 8s (remain 10m 2s) Loss: 0.0009(0.0453) Grad: 637.9438  LR: 0.000018  \n","Epoch: [1][1700/3575] Elapsed 8m 38s (remain 9m 31s) Loss: 0.0106(0.0434) Grad: 1931.0157  LR: 0.000019  \n","Epoch: [1][1800/3575] Elapsed 9m 8s (remain 9m 0s) Loss: 0.0022(0.0418) Grad: 45783.3906  LR: 0.000020  \n","Epoch: [1][1900/3575] Elapsed 9m 39s (remain 8m 30s) Loss: 0.0003(0.0401) Grad: 110.7078  LR: 0.000020  \n","Epoch: [1][2000/3575] Elapsed 10m 9s (remain 7m 59s) Loss: 0.0048(0.0388) Grad: 677.7048  LR: 0.000020  \n","Epoch: [1][2100/3575] Elapsed 10m 39s (remain 7m 28s) Loss: 0.0219(0.0376) Grad: 5919.9692  LR: 0.000020  \n","Epoch: [1][2200/3575] Elapsed 11m 10s (remain 6m 58s) Loss: 0.0092(0.0364) Grad: 5362.0137  LR: 0.000019  \n","Epoch: [1][2300/3575] Elapsed 11m 40s (remain 6m 28s) Loss: 0.0409(0.0353) Grad: 5244.2783  LR: 0.000019  \n","Epoch: [1][2400/3575] Elapsed 12m 11s (remain 5m 57s) Loss: 0.0128(0.0344) Grad: 2596.8809  LR: 0.000019  \n","Epoch: [1][2500/3575] Elapsed 12m 41s (remain 5m 26s) Loss: 0.0004(0.0334) Grad: 283.7372  LR: 0.000019  \n","Epoch: [1][2600/3575] Elapsed 13m 11s (remain 4m 56s) Loss: 0.0001(0.0327) Grad: 99.8470  LR: 0.000019  \n","Epoch: [1][2700/3575] Elapsed 13m 41s (remain 4m 25s) Loss: 0.0195(0.0318) Grad: 4454.9443  LR: 0.000019  \n","Epoch: [1][2800/3575] Elapsed 14m 12s (remain 3m 55s) Loss: 0.0001(0.0311) Grad: 76.4327  LR: 0.000019  \n","Epoch: [1][2900/3575] Elapsed 14m 42s (remain 3m 24s) Loss: 0.0017(0.0303) Grad: 4779.3770  LR: 0.000019  \n","Epoch: [1][3000/3575] Elapsed 15m 12s (remain 2m 54s) Loss: 0.0001(0.0297) Grad: 38.3993  LR: 0.000018  \n","Epoch: [1][3100/3575] Elapsed 15m 42s (remain 2m 24s) Loss: 0.0030(0.0291) Grad: 874.2563  LR: 0.000018  \n","Epoch: [1][3200/3575] Elapsed 16m 13s (remain 1m 53s) Loss: 0.0055(0.0286) Grad: 1153.8641  LR: 0.000018  \n","Epoch: [1][3300/3575] Elapsed 16m 43s (remain 1m 23s) Loss: 0.0002(0.0281) Grad: 82.1716  LR: 0.000018  \n","Epoch: [1][3400/3575] Elapsed 17m 13s (remain 0m 52s) Loss: 0.0007(0.0275) Grad: 306.2365  LR: 0.000018  \n","Epoch: [1][3500/3575] Elapsed 17m 43s (remain 0m 22s) Loss: 0.0017(0.0270) Grad: 1672.8185  LR: 0.000018  \n","Epoch: [1][3574/3575] Elapsed 18m 6s (remain 0m 0s) Loss: 0.0178(0.0267) Grad: 9355.6279  LR: 0.000018  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 12m 13s) Loss: 0.0053(0.0053) \n","EVAL: [100/1192] Elapsed 0m 18s (remain 3m 19s) Loss: 0.0280(0.0065) \n","EVAL: [200/1192] Elapsed 0m 36s (remain 2m 58s) Loss: 0.0048(0.0069) \n","EVAL: [300/1192] Elapsed 0m 54s (remain 2m 40s) Loss: 0.0074(0.0083) \n","EVAL: [400/1192] Elapsed 1m 12s (remain 2m 22s) Loss: 0.0001(0.0086) \n","EVAL: [500/1192] Elapsed 1m 29s (remain 2m 3s) Loss: 0.0118(0.0082) \n","EVAL: [600/1192] Elapsed 1m 47s (remain 1m 45s) Loss: 0.0062(0.0083) \n","EVAL: [700/1192] Elapsed 2m 5s (remain 1m 27s) Loss: 0.0047(0.0091) \n","EVAL: [800/1192] Elapsed 2m 23s (remain 1m 9s) Loss: 0.0070(0.0092) \n","EVAL: [900/1192] Elapsed 2m 40s (remain 0m 51s) Loss: 0.0129(0.0093) \n","EVAL: [1000/1192] Elapsed 2m 58s (remain 0m 34s) Loss: 0.0057(0.0089) \n","EVAL: [1100/1192] Elapsed 3m 16s (remain 0m 16s) Loss: 0.0177(0.0087) \n","EVAL: [1191/1192] Elapsed 3m 32s (remain 0m 0s) Loss: 0.0007(0.0085) \n","Epoch 1 - avg_train_loss: 0.0267  avg_val_loss: 0.0085  time: 1305s\n","Epoch 1 - Score: 0.8292\n","Epoch 1 - Save Best Score: 0.8292 Model\n","Epoch: [2][0/3575] Elapsed 0m 0s (remain 44m 41s) Loss: 0.0007(0.0007) Grad: 7135.6748  LR: 0.000018  \n","Epoch: [2][100/3575] Elapsed 0m 33s (remain 18m 59s) Loss: 0.0018(0.0076) Grad: 4680.1123  LR: 0.000018  \n","Epoch: [2][200/3575] Elapsed 1m 3s (remain 17m 50s) Loss: 0.0084(0.0096) Grad: 22294.3418  LR: 0.000018  \n","Epoch: [2][300/3575] Elapsed 1m 33s (remain 17m 1s) Loss: 0.0056(0.0093) Grad: 16817.5176  LR: 0.000017  \n","Epoch: [2][400/3575] Elapsed 2m 4s (remain 16m 21s) Loss: 0.0053(0.0088) Grad: 16156.7959  LR: 0.000017  \n","Epoch: [2][500/3575] Elapsed 2m 34s (remain 15m 46s) Loss: 0.0113(0.0087) Grad: 44300.5742  LR: 0.000017  \n","Epoch: [2][600/3575] Elapsed 3m 4s (remain 15m 12s) Loss: 0.0016(0.0090) Grad: 4588.8477  LR: 0.000017  \n","Epoch: [2][700/3575] Elapsed 3m 34s (remain 14m 40s) Loss: 0.0002(0.0086) Grad: 1788.9622  LR: 0.000017  \n","Epoch: [2][800/3575] Elapsed 4m 4s (remain 14m 7s) Loss: 0.0085(0.0085) Grad: 18497.9492  LR: 0.000017  \n","Epoch: [2][900/3575] Elapsed 4m 34s (remain 13m 36s) Loss: 0.0009(0.0086) Grad: 3344.4985  LR: 0.000017  \n","Epoch: [2][1000/3575] Elapsed 5m 5s (remain 13m 5s) Loss: 0.0002(0.0084) Grad: 948.1237  LR: 0.000017  \n","Epoch: [2][1100/3575] Elapsed 5m 35s (remain 12m 33s) Loss: 0.0001(0.0085) Grad: 829.5809  LR: 0.000016  \n","Epoch: [2][1200/3575] Elapsed 6m 5s (remain 12m 2s) Loss: 0.0042(0.0084) Grad: 7712.7896  LR: 0.000016  \n","Epoch: [2][1300/3575] Elapsed 6m 35s (remain 11m 32s) Loss: 0.0044(0.0087) Grad: 8464.2021  LR: 0.000016  \n","Epoch: [2][1400/3575] Elapsed 7m 6s (remain 11m 1s) Loss: 0.0401(0.0085) Grad: 25206.0938  LR: 0.000016  \n","Epoch: [2][1500/3575] Elapsed 7m 36s (remain 10m 30s) Loss: 0.0004(0.0083) Grad: 3480.2014  LR: 0.000016  \n","Epoch: [2][1600/3575] Elapsed 8m 6s (remain 9m 59s) Loss: 0.0002(0.0082) Grad: 485.8599  LR: 0.000016  \n","Epoch: [2][1700/3575] Elapsed 8m 36s (remain 9m 29s) Loss: 0.0083(0.0082) Grad: 15550.4717  LR: 0.000016  \n","Epoch: [2][1800/3575] Elapsed 9m 6s (remain 8m 58s) Loss: 0.0259(0.0081) Grad: 60958.5469  LR: 0.000016  \n","Epoch: [2][1900/3575] Elapsed 9m 37s (remain 8m 28s) Loss: 0.0023(0.0081) Grad: 18658.7012  LR: 0.000015  \n","Epoch: [2][2000/3575] Elapsed 10m 7s (remain 7m 57s) Loss: 0.0315(0.0082) Grad: 131963.9531  LR: 0.000015  \n","Epoch: [2][2100/3575] Elapsed 10m 37s (remain 7m 27s) Loss: 0.0009(0.0081) Grad: 43087.0391  LR: 0.000015  \n","Epoch: [2][2200/3575] Elapsed 11m 7s (remain 6m 56s) Loss: 0.0903(0.0080) Grad: 124759.5703  LR: 0.000015  \n","Epoch: [2][2300/3575] Elapsed 11m 37s (remain 6m 26s) Loss: 0.0000(0.0081) Grad: 76.9171  LR: 0.000015  \n","Epoch: [2][2400/3575] Elapsed 12m 8s (remain 5m 56s) Loss: 0.0000(0.0081) Grad: 88.6997  LR: 0.000015  \n","Epoch: [2][2500/3575] Elapsed 12m 38s (remain 5m 25s) Loss: 0.0042(0.0081) Grad: 19228.0977  LR: 0.000015  \n","Epoch: [2][2600/3575] Elapsed 13m 8s (remain 4m 55s) Loss: 0.0009(0.0081) Grad: 5521.8315  LR: 0.000015  \n","Epoch: [2][2700/3575] Elapsed 13m 38s (remain 4m 24s) Loss: 0.0118(0.0081) Grad: 17056.0762  LR: 0.000014  \n","Epoch: [2][2800/3575] Elapsed 14m 8s (remain 3m 54s) Loss: 0.0030(0.0080) Grad: 11048.6768  LR: 0.000014  \n","Epoch: [2][2900/3575] Elapsed 14m 39s (remain 3m 24s) Loss: 0.0030(0.0080) Grad: 19017.0762  LR: 0.000014  \n","Epoch: [2][3000/3575] Elapsed 15m 9s (remain 2m 53s) Loss: 0.0059(0.0080) Grad: 8533.1875  LR: 0.000014  \n","Epoch: [2][3100/3575] Elapsed 15m 39s (remain 2m 23s) Loss: 0.0001(0.0081) Grad: 751.9811  LR: 0.000014  \n","Epoch: [2][3200/3575] Elapsed 16m 9s (remain 1m 53s) Loss: 0.0006(0.0081) Grad: 2235.0579  LR: 0.000014  \n","Epoch: [2][3300/3575] Elapsed 16m 39s (remain 1m 22s) Loss: 0.0000(0.0081) Grad: 58.8126  LR: 0.000014  \n","Epoch: [2][3400/3575] Elapsed 17m 10s (remain 0m 52s) Loss: 0.0394(0.0081) Grad: 42012.6758  LR: 0.000014  \n","Epoch: [2][3500/3575] Elapsed 17m 40s (remain 0m 22s) Loss: 0.0155(0.0081) Grad: 95899.1328  LR: 0.000013  \n","Epoch: [2][3574/3575] Elapsed 18m 2s (remain 0m 0s) Loss: 0.0047(0.0081) Grad: 23320.6387  LR: 0.000013  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 10m 36s) Loss: 0.0001(0.0001) \n","EVAL: [100/1192] Elapsed 0m 18s (remain 3m 17s) Loss: 0.0335(0.0103) \n","EVAL: [200/1192] Elapsed 0m 36s (remain 2m 58s) Loss: 0.0091(0.0095) \n","EVAL: [300/1192] Elapsed 0m 53s (remain 2m 39s) Loss: 0.0051(0.0096) \n","EVAL: [400/1192] Elapsed 1m 11s (remain 2m 21s) Loss: 0.0000(0.0091) \n","EVAL: [500/1192] Elapsed 1m 29s (remain 2m 3s) Loss: 0.0090(0.0094) \n","EVAL: [600/1192] Elapsed 1m 47s (remain 1m 45s) Loss: 0.0019(0.0095) \n","EVAL: [700/1192] Elapsed 2m 4s (remain 1m 27s) Loss: 0.0054(0.0107) \n","EVAL: [800/1192] Elapsed 2m 22s (remain 1m 9s) Loss: 0.0087(0.0108) \n","EVAL: [900/1192] Elapsed 2m 40s (remain 0m 51s) Loss: 0.0042(0.0106) \n","EVAL: [1000/1192] Elapsed 2m 58s (remain 0m 34s) Loss: 0.0000(0.0102) \n","EVAL: [1100/1192] Elapsed 3m 16s (remain 0m 16s) Loss: 0.0524(0.0100) \n","EVAL: [1191/1192] Elapsed 3m 31s (remain 0m 0s) Loss: 0.0000(0.0098) \n","Epoch 2 - avg_train_loss: 0.0081  avg_val_loss: 0.0098  time: 1300s\n","Epoch 2 - Score: 0.8635\n","Epoch 2 - Save Best Score: 0.8635 Model\n","Epoch: [3][0/3575] Elapsed 0m 0s (remain 41m 34s) Loss: 0.0214(0.0214) Grad: 32740.1719  LR: 0.000013  \n","Epoch: [3][100/3575] Elapsed 0m 32s (remain 18m 48s) Loss: 0.0002(0.0097) Grad: 503.5316  LR: 0.000013  \n","Epoch: [3][200/3575] Elapsed 1m 3s (remain 17m 47s) Loss: 0.0032(0.0070) Grad: 5380.2773  LR: 0.000013  \n","Epoch: [3][300/3575] Elapsed 1m 33s (remain 17m 0s) Loss: 0.0000(0.0062) Grad: 83.0678  LR: 0.000013  \n","Epoch: [3][400/3575] Elapsed 2m 4s (remain 16m 21s) Loss: 0.0156(0.0062) Grad: 25132.7773  LR: 0.000013  \n","Epoch: [3][500/3575] Elapsed 2m 34s (remain 15m 46s) Loss: 0.0031(0.0063) Grad: 15770.7764  LR: 0.000013  \n","Epoch: [3][600/3575] Elapsed 3m 4s (remain 15m 12s) Loss: 0.0016(0.0064) Grad: 20269.1133  LR: 0.000013  \n","Epoch: [3][700/3575] Elapsed 3m 34s (remain 14m 40s) Loss: 0.0039(0.0063) Grad: 8080.2725  LR: 0.000012  \n","Epoch: [3][800/3575] Elapsed 4m 4s (remain 14m 8s) Loss: 0.0001(0.0062) Grad: 1982.7892  LR: 0.000012  \n","Epoch: [3][900/3575] Elapsed 4m 35s (remain 13m 37s) Loss: 0.0352(0.0064) Grad: 42433.1992  LR: 0.000012  \n","Epoch: [3][1000/3575] Elapsed 5m 5s (remain 13m 5s) Loss: 0.0000(0.0064) Grad: 21.1796  LR: 0.000012  \n","Epoch: [3][1100/3575] Elapsed 5m 35s (remain 12m 34s) Loss: 0.0023(0.0065) Grad: 8183.8687  LR: 0.000012  \n","Epoch: [3][1200/3575] Elapsed 6m 6s (remain 12m 3s) Loss: 0.0073(0.0065) Grad: 66914.2422  LR: 0.000012  \n","Epoch: [3][1300/3575] Elapsed 6m 36s (remain 11m 32s) Loss: 0.0157(0.0066) Grad: 105732.7422  LR: 0.000012  \n","Epoch: [3][1400/3575] Elapsed 7m 6s (remain 11m 1s) Loss: 0.0001(0.0067) Grad: 730.8608  LR: 0.000012  \n","Epoch: [3][1500/3575] Elapsed 7m 36s (remain 10m 30s) Loss: 0.0175(0.0068) Grad: 24651.3750  LR: 0.000011  \n","Epoch: [3][1600/3575] Elapsed 8m 6s (remain 10m 0s) Loss: 0.0015(0.0068) Grad: 10232.5293  LR: 0.000011  \n","Epoch: [3][1700/3575] Elapsed 8m 36s (remain 9m 29s) Loss: 0.0001(0.0068) Grad: 220.1258  LR: 0.000011  \n","Epoch: [3][1800/3575] Elapsed 9m 7s (remain 8m 58s) Loss: 0.0036(0.0066) Grad: 5763.2983  LR: 0.000011  \n","Epoch: [3][1900/3575] Elapsed 9m 37s (remain 8m 28s) Loss: 0.0001(0.0066) Grad: 266.2444  LR: 0.000011  \n","Epoch: [3][2000/3575] Elapsed 10m 7s (remain 7m 57s) Loss: 0.0000(0.0065) Grad: 157.6252  LR: 0.000011  \n","Epoch: [3][2100/3575] Elapsed 10m 37s (remain 7m 27s) Loss: 0.0013(0.0064) Grad: 10240.4863  LR: 0.000011  \n","Epoch: [3][2200/3575] Elapsed 11m 8s (remain 6m 57s) Loss: 0.0070(0.0063) Grad: 15350.3301  LR: 0.000011  \n","Epoch: [3][2300/3575] Elapsed 11m 38s (remain 6m 26s) Loss: 0.0026(0.0063) Grad: 5703.0391  LR: 0.000010  \n","Epoch: [3][2400/3575] Elapsed 12m 8s (remain 5m 56s) Loss: 0.0032(0.0063) Grad: 19298.9590  LR: 0.000010  \n","Epoch: [3][2500/3575] Elapsed 12m 38s (remain 5m 25s) Loss: 0.0001(0.0064) Grad: 689.5097  LR: 0.000010  \n","Epoch: [3][2600/3575] Elapsed 13m 9s (remain 4m 55s) Loss: 0.0000(0.0064) Grad: 276.7339  LR: 0.000010  \n","Epoch: [3][2700/3575] Elapsed 13m 39s (remain 4m 25s) Loss: 0.0001(0.0064) Grad: 352.7736  LR: 0.000010  \n","Epoch: [3][2800/3575] Elapsed 14m 9s (remain 3m 54s) Loss: 0.0000(0.0064) Grad: 450.9004  LR: 0.000010  \n","Epoch: [3][2900/3575] Elapsed 14m 39s (remain 3m 24s) Loss: 0.0008(0.0064) Grad: 9767.9775  LR: 0.000010  \n","Epoch: [3][3000/3575] Elapsed 15m 10s (remain 2m 54s) Loss: 0.0000(0.0064) Grad: 43.9960  LR: 0.000010  \n","Epoch: [3][3100/3575] Elapsed 15m 40s (remain 2m 23s) Loss: 0.0000(0.0063) Grad: 42.0077  LR: 0.000009  \n","Epoch: [3][3200/3575] Elapsed 16m 10s (remain 1m 53s) Loss: 0.0011(0.0063) Grad: 16287.5771  LR: 0.000009  \n","Epoch: [3][3300/3575] Elapsed 16m 40s (remain 1m 23s) Loss: 0.0212(0.0063) Grad: 88500.2578  LR: 0.000009  \n","Epoch: [3][3400/3575] Elapsed 17m 11s (remain 0m 52s) Loss: 0.0076(0.0063) Grad: 18441.4004  LR: 0.000009  \n","Epoch: [3][3500/3575] Elapsed 17m 41s (remain 0m 22s) Loss: 0.0041(0.0063) Grad: 26626.6016  LR: 0.000009  \n","Epoch: [3][3574/3575] Elapsed 18m 3s (remain 0m 0s) Loss: 0.0000(0.0063) Grad: 110.7762  LR: 0.000009  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 10m 32s) Loss: 0.0002(0.0002) \n","EVAL: [100/1192] Elapsed 0m 18s (remain 3m 17s) Loss: 0.0392(0.0099) \n","EVAL: [200/1192] Elapsed 0m 36s (remain 2m 58s) Loss: 0.0037(0.0078) \n","EVAL: [300/1192] Elapsed 0m 54s (remain 2m 39s) Loss: 0.0037(0.0081) \n","EVAL: [400/1192] Elapsed 1m 11s (remain 2m 21s) Loss: 0.0000(0.0079) \n","EVAL: [500/1192] Elapsed 1m 29s (remain 2m 3s) Loss: 0.0227(0.0078) \n","EVAL: [600/1192] Elapsed 1m 47s (remain 1m 45s) Loss: 0.0070(0.0080) \n","EVAL: [700/1192] Elapsed 2m 4s (remain 1m 27s) Loss: 0.0042(0.0090) \n","EVAL: [800/1192] Elapsed 2m 22s (remain 1m 9s) Loss: 0.0171(0.0092) \n","EVAL: [900/1192] Elapsed 2m 40s (remain 0m 51s) Loss: 0.0052(0.0093) \n","EVAL: [1000/1192] Elapsed 2m 58s (remain 0m 34s) Loss: 0.0000(0.0089) \n","EVAL: [1100/1192] Elapsed 3m 16s (remain 0m 16s) Loss: 0.0285(0.0085) \n","EVAL: [1191/1192] Elapsed 3m 32s (remain 0m 0s) Loss: 0.0000(0.0083) \n","Epoch 3 - avg_train_loss: 0.0063  avg_val_loss: 0.0083  time: 1301s\n","Epoch 3 - Score: 0.8809\n","Epoch 3 - Save Best Score: 0.8809 Model\n","Epoch: [4][0/3575] Elapsed 0m 0s (remain 40m 22s) Loss: 0.0008(0.0008) Grad: 2603.8750  LR: 0.000009  \n","Epoch: [4][100/3575] Elapsed 0m 32s (remain 18m 39s) Loss: 0.0000(0.0039) Grad: 12.3227  LR: 0.000009  \n","Epoch: [4][200/3575] Elapsed 1m 3s (remain 17m 43s) Loss: 0.0010(0.0040) Grad: 10053.0605  LR: 0.000009  \n","Epoch: [4][300/3575] Elapsed 1m 33s (remain 16m 58s) Loss: 0.0337(0.0041) Grad: 112268.0547  LR: 0.000009  \n","Epoch: [4][400/3575] Elapsed 2m 3s (remain 16m 19s) Loss: 0.0000(0.0048) Grad: 26.9554  LR: 0.000008  \n","Epoch: [4][500/3575] Elapsed 2m 34s (remain 15m 44s) Loss: 0.0000(0.0048) Grad: 242.9816  LR: 0.000008  \n","Epoch: [4][600/3575] Elapsed 3m 4s (remain 15m 12s) Loss: 0.0093(0.0049) Grad: 801817.0000  LR: 0.000008  \n","Epoch: [4][700/3575] Elapsed 3m 34s (remain 14m 39s) Loss: 0.0055(0.0046) Grad: 23169.0898  LR: 0.000008  \n","Epoch: [4][800/3575] Elapsed 4m 4s (remain 14m 8s) Loss: 0.0014(0.0050) Grad: 11011.7158  LR: 0.000008  \n","Epoch: [4][900/3575] Elapsed 4m 35s (remain 13m 36s) Loss: 0.0001(0.0050) Grad: 511.3286  LR: 0.000008  \n","Epoch: [4][1000/3575] Elapsed 5m 5s (remain 13m 5s) Loss: 0.0000(0.0048) Grad: 17.9462  LR: 0.000008  \n","Epoch: [4][1100/3575] Elapsed 5m 35s (remain 12m 34s) Loss: 0.0169(0.0049) Grad: 114357.7891  LR: 0.000008  \n","Epoch: [4][1200/3575] Elapsed 6m 5s (remain 12m 3s) Loss: 0.0590(0.0049) Grad: 41838.7812  LR: 0.000007  \n","Epoch: [4][1300/3575] Elapsed 6m 35s (remain 11m 32s) Loss: 0.0037(0.0049) Grad: 34099.5234  LR: 0.000007  \n","Epoch: [4][1400/3575] Elapsed 7m 6s (remain 11m 1s) Loss: 0.0041(0.0048) Grad: 12656.7119  LR: 0.000007  \n","Epoch: [4][1500/3575] Elapsed 7m 36s (remain 10m 30s) Loss: 0.0090(0.0048) Grad: 9200.2070  LR: 0.000007  \n","Epoch: [4][1600/3575] Elapsed 8m 6s (remain 9m 59s) Loss: 0.0000(0.0049) Grad: 25.2075  LR: 0.000007  \n","Epoch: [4][1700/3575] Elapsed 8m 36s (remain 9m 29s) Loss: 0.0006(0.0049) Grad: 3947.6609  LR: 0.000007  \n","Epoch: [4][1800/3575] Elapsed 9m 7s (remain 8m 58s) Loss: 0.0000(0.0048) Grad: 32.7952  LR: 0.000007  \n","Epoch: [4][1900/3575] Elapsed 9m 37s (remain 8m 28s) Loss: 0.0030(0.0049) Grad: 51703.2227  LR: 0.000007  \n","Epoch: [4][2000/3575] Elapsed 10m 7s (remain 7m 57s) Loss: 0.0000(0.0049) Grad: 294.9894  LR: 0.000006  \n","Epoch: [4][2100/3575] Elapsed 10m 37s (remain 7m 27s) Loss: 0.0000(0.0050) Grad: 25.4451  LR: 0.000006  \n","Epoch: [4][2200/3575] Elapsed 11m 7s (remain 6m 56s) Loss: 0.0000(0.0050) Grad: 33.9926  LR: 0.000006  \n","Epoch: [4][2300/3575] Elapsed 11m 38s (remain 6m 26s) Loss: 0.0000(0.0050) Grad: 34.7885  LR: 0.000006  \n","Epoch: [4][2400/3575] Elapsed 12m 8s (remain 5m 56s) Loss: 0.0021(0.0050) Grad: 9029.6768  LR: 0.000006  \n","Epoch: [4][2500/3575] Elapsed 12m 38s (remain 5m 25s) Loss: 0.0000(0.0050) Grad: 32.9650  LR: 0.000006  \n","Epoch: [4][2600/3575] Elapsed 13m 8s (remain 4m 55s) Loss: 0.0001(0.0049) Grad: 517.7114  LR: 0.000006  \n","Epoch: [4][2700/3575] Elapsed 13m 39s (remain 4m 25s) Loss: 0.0000(0.0049) Grad: 105.5594  LR: 0.000006  \n","Epoch: [4][2800/3575] Elapsed 14m 9s (remain 3m 54s) Loss: 0.0104(0.0050) Grad: 58804.3633  LR: 0.000005  \n","Epoch: [4][2900/3575] Elapsed 14m 39s (remain 3m 24s) Loss: 0.0000(0.0050) Grad: 70.8524  LR: 0.000005  \n","Epoch: [4][3000/3575] Elapsed 15m 9s (remain 2m 54s) Loss: 0.0114(0.0050) Grad: 93433.7266  LR: 0.000005  \n","Epoch: [4][3100/3575] Elapsed 15m 39s (remain 2m 23s) Loss: 0.0090(0.0051) Grad: 14951.4893  LR: 0.000005  \n","Epoch: [4][3200/3575] Elapsed 16m 10s (remain 1m 53s) Loss: 0.0001(0.0050) Grad: 399.8217  LR: 0.000005  \n","Epoch: [4][3300/3575] Elapsed 16m 40s (remain 1m 23s) Loss: 0.0005(0.0050) Grad: 2950.0308  LR: 0.000005  \n","Epoch: [4][3400/3575] Elapsed 17m 10s (remain 0m 52s) Loss: 0.0000(0.0051) Grad: 169.1509  LR: 0.000005  \n","Epoch: [4][3500/3575] Elapsed 17m 40s (remain 0m 22s) Loss: 0.0003(0.0050) Grad: 1016.1019  LR: 0.000005  \n","Epoch: [4][3574/3575] Elapsed 18m 3s (remain 0m 0s) Loss: 0.0183(0.0050) Grad: 57466.8203  LR: 0.000004  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 11m 2s) Loss: 0.0001(0.0001) \n","EVAL: [100/1192] Elapsed 0m 18s (remain 3m 18s) Loss: 0.0362(0.0088) \n","EVAL: [200/1192] Elapsed 0m 36s (remain 2m 58s) Loss: 0.0064(0.0077) \n","EVAL: [300/1192] Elapsed 0m 54s (remain 2m 40s) Loss: 0.0091(0.0083) \n","EVAL: [400/1192] Elapsed 1m 12s (remain 2m 22s) Loss: 0.0000(0.0085) \n","EVAL: [500/1192] Elapsed 1m 29s (remain 2m 3s) Loss: 0.0256(0.0084) \n","EVAL: [600/1192] Elapsed 1m 47s (remain 1m 45s) Loss: 0.0054(0.0087) \n","EVAL: [700/1192] Elapsed 2m 5s (remain 1m 27s) Loss: 0.0037(0.0098) \n","EVAL: [800/1192] Elapsed 2m 23s (remain 1m 9s) Loss: 0.0135(0.0101) \n","EVAL: [900/1192] Elapsed 2m 41s (remain 0m 52s) Loss: 0.0066(0.0101) \n","EVAL: [1000/1192] Elapsed 2m 58s (remain 0m 34s) Loss: 0.0000(0.0095) \n","EVAL: [1100/1192] Elapsed 3m 16s (remain 0m 16s) Loss: 0.0102(0.0092) \n","EVAL: [1191/1192] Elapsed 3m 32s (remain 0m 0s) Loss: 0.0000(0.0092) \n","Epoch 4 - avg_train_loss: 0.0050  avg_val_loss: 0.0092  time: 1301s\n","Epoch 4 - Score: 0.8785\n","Epoch: [5][0/3575] Elapsed 0m 0s (remain 41m 50s) Loss: 0.0000(0.0000) Grad: 1000.7813  LR: 0.000004  \n","Epoch: [5][100/3575] Elapsed 0m 30s (remain 17m 41s) Loss: 0.0040(0.0056) Grad: 28214.6465  LR: 0.000004  \n","Epoch: [5][200/3575] Elapsed 1m 1s (remain 17m 4s) Loss: 0.0001(0.0039) Grad: 263.1545  LR: 0.000004  \n","Epoch: [5][300/3575] Elapsed 1m 31s (remain 16m 32s) Loss: 0.0000(0.0041) Grad: 19.2818  LR: 0.000004  \n","Epoch: [5][400/3575] Elapsed 2m 1s (remain 16m 1s) Loss: 0.0414(0.0043) Grad: 78389.7500  LR: 0.000004  \n","Epoch: [5][500/3575] Elapsed 2m 31s (remain 15m 30s) Loss: 0.0001(0.0046) Grad: 987.5809  LR: 0.000004  \n","Epoch: [5][600/3575] Elapsed 3m 1s (remain 14m 59s) Loss: 0.0000(0.0045) Grad: 346.0585  LR: 0.000004  \n","Epoch: [5][700/3575] Elapsed 3m 32s (remain 14m 29s) Loss: 0.0001(0.0044) Grad: 2042.4802  LR: 0.000004  \n","Epoch: [5][800/3575] Elapsed 4m 2s (remain 13m 59s) Loss: 0.0049(0.0042) Grad: 27297.0371  LR: 0.000003  \n","Epoch: [5][900/3575] Elapsed 4m 32s (remain 13m 29s) Loss: 0.0000(0.0041) Grad: 24.2873  LR: 0.000003  \n","Epoch: [5][1000/3575] Elapsed 5m 2s (remain 12m 58s) Loss: 0.0000(0.0043) Grad: 7.9037  LR: 0.000003  \n","Epoch: [5][1100/3575] Elapsed 5m 33s (remain 12m 28s) Loss: 0.0068(0.0042) Grad: 83177.8828  LR: 0.000003  \n","Epoch: [5][1200/3575] Elapsed 6m 3s (remain 11m 58s) Loss: 0.0024(0.0045) Grad: 13035.0264  LR: 0.000003  \n","Epoch: [5][1300/3575] Elapsed 6m 33s (remain 11m 27s) Loss: 0.0008(0.0043) Grad: 12429.5020  LR: 0.000003  \n","Epoch: [5][1400/3575] Elapsed 7m 3s (remain 10m 57s) Loss: 0.0001(0.0043) Grad: 635.3800  LR: 0.000003  \n","Epoch: [5][1500/3575] Elapsed 7m 33s (remain 10m 27s) Loss: 0.0000(0.0042) Grad: 40.3331  LR: 0.000003  \n","Epoch: [5][1600/3575] Elapsed 8m 4s (remain 9m 56s) Loss: 0.0001(0.0041) Grad: 1069.5515  LR: 0.000002  \n","Epoch: [5][1700/3575] Elapsed 8m 34s (remain 9m 26s) Loss: 0.0000(0.0041) Grad: 107.8500  LR: 0.000002  \n","Epoch: [5][1800/3575] Elapsed 9m 4s (remain 8m 56s) Loss: 0.0000(0.0041) Grad: 9.3884  LR: 0.000002  \n","Epoch: [5][1900/3575] Elapsed 9m 34s (remain 8m 26s) Loss: 0.0000(0.0040) Grad: 163.8531  LR: 0.000002  \n","Epoch: [5][2000/3575] Elapsed 10m 4s (remain 7m 55s) Loss: 0.0094(0.0040) Grad: 23732.7695  LR: 0.000002  \n","Epoch: [5][2100/3575] Elapsed 10m 35s (remain 7m 25s) Loss: 0.0000(0.0040) Grad: 14.0493  LR: 0.000002  \n","Epoch: [5][2200/3575] Elapsed 11m 5s (remain 6m 55s) Loss: 0.0000(0.0040) Grad: 69.1673  LR: 0.000002  \n","Epoch: [5][2300/3575] Elapsed 11m 35s (remain 6m 25s) Loss: 0.0000(0.0040) Grad: 272.4088  LR: 0.000002  \n","Epoch: [5][2400/3575] Elapsed 12m 5s (remain 5m 54s) Loss: 0.0057(0.0040) Grad: 42982.6133  LR: 0.000001  \n","Epoch: [5][2500/3575] Elapsed 12m 35s (remain 5m 24s) Loss: 0.0005(0.0040) Grad: 4564.7876  LR: 0.000001  \n","Epoch: [5][2600/3575] Elapsed 13m 5s (remain 4m 54s) Loss: 0.0072(0.0039) Grad: 15895.4941  LR: 0.000001  \n","Epoch: [5][2700/3575] Elapsed 13m 35s (remain 4m 24s) Loss: 0.0000(0.0039) Grad: 12.5642  LR: 0.000001  \n","Epoch: [5][2800/3575] Elapsed 14m 6s (remain 3m 53s) Loss: 0.0337(0.0040) Grad: 73537.5000  LR: 0.000001  \n","Epoch: [5][2900/3575] Elapsed 14m 36s (remain 3m 23s) Loss: 0.0228(0.0040) Grad: 39962.6406  LR: 0.000001  \n","Epoch: [5][3000/3575] Elapsed 15m 6s (remain 2m 53s) Loss: 0.0184(0.0040) Grad: 25738.9160  LR: 0.000001  \n","Epoch: [5][3100/3575] Elapsed 15m 36s (remain 2m 23s) Loss: 0.0000(0.0040) Grad: 4.7220  LR: 0.000001  \n","Epoch: [5][3200/3575] Elapsed 16m 6s (remain 1m 52s) Loss: 0.0032(0.0040) Grad: 13571.7119  LR: 0.000000  \n","Epoch: [5][3300/3575] Elapsed 16m 36s (remain 1m 22s) Loss: 0.0199(0.0039) Grad: 21677.1191  LR: 0.000000  \n","Epoch: [5][3400/3575] Elapsed 17m 7s (remain 0m 52s) Loss: 0.0001(0.0039) Grad: 1859.4163  LR: 0.000000  \n","Epoch: [5][3500/3575] Elapsed 17m 37s (remain 0m 22s) Loss: 0.0000(0.0039) Grad: 120.7763  LR: 0.000000  \n","Epoch: [5][3574/3575] Elapsed 17m 59s (remain 0m 0s) Loss: 0.0000(0.0039) Grad: 13.5994  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 11m 8s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 18s (remain 3m 17s) Loss: 0.0407(0.0084) \n","EVAL: [200/1192] Elapsed 0m 36s (remain 2m 58s) Loss: 0.0024(0.0078) \n","EVAL: [300/1192] Elapsed 0m 54s (remain 2m 39s) Loss: 0.0100(0.0091) \n","EVAL: [400/1192] Elapsed 1m 11s (remain 2m 21s) Loss: 0.0000(0.0091) \n","EVAL: [500/1192] Elapsed 1m 29s (remain 2m 3s) Loss: 0.0351(0.0092) \n","EVAL: [600/1192] Elapsed 1m 47s (remain 1m 45s) Loss: 0.0073(0.0094) \n","EVAL: [700/1192] Elapsed 2m 5s (remain 1m 27s) Loss: 0.0053(0.0106) \n","EVAL: [800/1192] Elapsed 2m 22s (remain 1m 9s) Loss: 0.0173(0.0110) \n","EVAL: [900/1192] Elapsed 2m 40s (remain 0m 51s) Loss: 0.0087(0.0110) \n","EVAL: [1000/1192] Elapsed 2m 58s (remain 0m 34s) Loss: 0.0000(0.0105) \n","EVAL: [1100/1192] Elapsed 3m 16s (remain 0m 16s) Loss: 0.0142(0.0102) \n","EVAL: [1191/1192] Elapsed 3m 32s (remain 0m 0s) Loss: 0.0000(0.0101) \n","Epoch 5 - avg_train_loss: 0.0039  avg_val_loss: 0.0101  time: 1297s\n","Epoch 5 - Score: 0.8813\n","Epoch 5 - Save Best Score: 0.8813 Model\n","Best thres: 0.5, Score: 0.8773\n","Best thres: 0.51171875, Score: 0.8774\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"044ae7b300cf4ef9aff545c35658c34f"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"342e59616f3c40c2b172b96dd23c800d"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6f49600ddb745f0bbd62d956a5546db"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]}],"source":["if __name__ == \"__main__\":\n","    main()"],"id":"graduate-vision"}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"name":"nbme-exp044.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"papermill":{"default_parameters":{},"duration":641.572862,"end_time":"2022-02-27T11:39:50.972497","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-02-27T11:29:09.399635","version":"2.3.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"01f31e86e39a4926acfb1a23ddab7f76":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0498516b744442da867c09ed858853f0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bcde70431ba4780a224fef22d41ed0d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f5642fca45a4511a672b28cc0ac1c42":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc12727aca2f45ea94c410f6aa3df9c3","placeholder":"​","style":"IPY_MODEL_01f31e86e39a4926acfb1a23ddab7f76","value":" 143/143 [00:00&lt;00:00, 2280.42it/s]"}},"105d11ca83ca4294986a3ebfebafc5d3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cf73f7de8ce41b0aec238f108d5332b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1dfc701a80674672bf048f4acc2f8db7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d367b9961744ebd8afcd407b5f1dfaf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1dfc701a80674672bf048f4acc2f8db7","max":42146,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6d470e185c4d4755a9d2f1f3459a5bdc","value":42146}},"38676c6cd8dc420abb0c77a7c6597b1e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"444cece8449447b88213c907ad496227":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d843ce8df38343fc89bd5b994eb93d1d","placeholder":"​","style":"IPY_MODEL_bc430ad6b2ca41d181f47fd1e3ed05ab","value":"Downloading: 100%"}},"5ec28e7595f14e2e9797c9a757a4b67a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62c353f8cb1a4d0fbc1b22f7ed49801b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69c6130b141545a8800076b853e89879":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0bcde70431ba4780a224fef22d41ed0d","placeholder":"​","style":"IPY_MODEL_b10a28a1bc82423eb7929259d85c66fd","value":"100%"}},"6d470e185c4d4755a9d2f1f3459a5bdc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"70df2d61ff064679a8643e2695d6a351":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ba148e9e6154a888174b91d18dce982":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a94e8daedcc84095b902463f9146e4c7","max":1627284589,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b9ef8573be694e86825d3d88dfc0c520","value":1627284589}},"7c7fdf3e9afc4636be256140b514b483":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7f2446c46f094cbe97f6d8de06c52452":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"929b5ef4d04847cc92f73e75e13153d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f7159d6a4b6486c978128b5b4273c0d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c6f29027dadf4413aea8f3f2f89b5f2e","IPY_MODEL_2d367b9961744ebd8afcd407b5f1dfaf","IPY_MODEL_d6e802d0852c4e129f4d80049c86f853"],"layout":"IPY_MODEL_70df2d61ff064679a8643e2695d6a351"}},"a94e8daedcc84095b902463f9146e4c7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b10a28a1bc82423eb7929259d85c66fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b9ef8573be694e86825d3d88dfc0c520":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bb0d742cd788453da46ff5b3b5d2d584":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_444cece8449447b88213c907ad496227","IPY_MODEL_7ba148e9e6154a888174b91d18dce982","IPY_MODEL_db3fdbd9793e495ea654ca1d45ca9f37"],"layout":"IPY_MODEL_5ec28e7595f14e2e9797c9a757a4b67a"}},"bc430ad6b2ca41d181f47fd1e3ed05ab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c6f29027dadf4413aea8f3f2f89b5f2e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_105d11ca83ca4294986a3ebfebafc5d3","placeholder":"​","style":"IPY_MODEL_38676c6cd8dc420abb0c77a7c6597b1e","value":"100%"}},"cdab58c093cd4cd39249de2048b9f376":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6e802d0852c4e129f4d80049c86f853":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f2446c46f094cbe97f6d8de06c52452","placeholder":"​","style":"IPY_MODEL_1cf73f7de8ce41b0aec238f108d5332b","value":" 42146/42146 [00:37&lt;00:00, 2004.18it/s]"}},"d843ce8df38343fc89bd5b994eb93d1d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db3fdbd9793e495ea654ca1d45ca9f37":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0498516b744442da867c09ed858853f0","placeholder":"​","style":"IPY_MODEL_929b5ef4d04847cc92f73e75e13153d2","value":" 1.52G/1.52G [00:31&lt;00:00, 51.8MB/s]"}},"f6bb2b5c141e4dd9a312ea5053696d0f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_69c6130b141545a8800076b853e89879","IPY_MODEL_fa3be47befba4c0b8db44ceff1a5127b","IPY_MODEL_0f5642fca45a4511a672b28cc0ac1c42"],"layout":"IPY_MODEL_cdab58c093cd4cd39249de2048b9f376"}},"fa3be47befba4c0b8db44ceff1a5127b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_62c353f8cb1a4d0fbc1b22f7ed49801b","max":143,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7c7fdf3e9afc4636be256140b514b483","value":143}},"fc12727aca2f45ea94c410f6aa3df9c3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"044ae7b300cf4ef9aff545c35658c34f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3da537198516445eaf53449f4bd25adb","IPY_MODEL_0105ed1939194d89bd23ad507d5e3f65","IPY_MODEL_76b8c91e39d041b3a7afadb2de5a500a"],"layout":"IPY_MODEL_5db9f18c6037458a890a69c91910b6f8"}},"3da537198516445eaf53449f4bd25adb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef23a870721b4d32a21f25db0ce8c9d2","placeholder":"​","style":"IPY_MODEL_32b365fcccf3449695b8513baadce6fa","value":"100%"}},"0105ed1939194d89bd23ad507d5e3f65":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_23cb906d17704e568f748062c1835ce0","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_934986cb53ea40198115eae075349d69","value":2}},"76b8c91e39d041b3a7afadb2de5a500a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6593ba0b19c84c44aac4a6609fdf0742","placeholder":"​","style":"IPY_MODEL_6ae7880090be4960a3e87e15e9159ab9","value":" 2/2 [00:01&lt;00:00,  1.65it/s]"}},"5db9f18c6037458a890a69c91910b6f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef23a870721b4d32a21f25db0ce8c9d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32b365fcccf3449695b8513baadce6fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"23cb906d17704e568f748062c1835ce0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"934986cb53ea40198115eae075349d69":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6593ba0b19c84c44aac4a6609fdf0742":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ae7880090be4960a3e87e15e9159ab9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"342e59616f3c40c2b172b96dd23c800d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_02aba7aad7fe45da8da7b1733a911068","IPY_MODEL_fbf9f2c4a5914d22b030ca13dbcdd32c","IPY_MODEL_082e3df201e34bb18c0760399929ec9f"],"layout":"IPY_MODEL_ecfcb2beda4b436891494f5ed4d8a72f"}},"02aba7aad7fe45da8da7b1733a911068":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4447f3d5b58043cca53185ddb1eafbaa","placeholder":"​","style":"IPY_MODEL_70e3470b9ccd4aaf946d7402e7c94f7b","value":"100%"}},"fbf9f2c4a5914d22b030ca13dbcdd32c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c1f8730f679418db91d06f8b0037545","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5f9ed6e7398843a2bf9069d8bc992387","value":2}},"082e3df201e34bb18c0760399929ec9f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b82ee2eb90a34425a9ae8bf3b1f2d714","placeholder":"​","style":"IPY_MODEL_295c9838a6c64e7883133766640cb0cd","value":" 2/2 [00:01&lt;00:00,  1.67it/s]"}},"ecfcb2beda4b436891494f5ed4d8a72f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4447f3d5b58043cca53185ddb1eafbaa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70e3470b9ccd4aaf946d7402e7c94f7b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c1f8730f679418db91d06f8b0037545":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f9ed6e7398843a2bf9069d8bc992387":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b82ee2eb90a34425a9ae8bf3b1f2d714":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"295c9838a6c64e7883133766640cb0cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a6f49600ddb745f0bbd62d956a5546db":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_de93b4220b2f4629bc8b39a1035f9807","IPY_MODEL_1b973967650b4d659061f36e6ef141c0","IPY_MODEL_722ce810722d4ef3a604a752b9d96391"],"layout":"IPY_MODEL_feeb704265764a11836a151c44fdd488"}},"de93b4220b2f4629bc8b39a1035f9807":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b17d125380943ba8714cf3879aee67f","placeholder":"​","style":"IPY_MODEL_ed596e634176410ea94c2e987a3a4f17","value":"100%"}},"1b973967650b4d659061f36e6ef141c0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_34183a24824d4ec589a6a1e3998f6289","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_31a2351ef1184cbd96f704f071971513","value":2}},"722ce810722d4ef3a604a752b9d96391":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e1b2c72c27f4348b9f006e1a2aa5fe6","placeholder":"​","style":"IPY_MODEL_ca8ce07a830c44e8a70a75d00f89f2f1","value":" 2/2 [00:02&lt;00:00,  1.40it/s]"}},"feeb704265764a11836a151c44fdd488":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b17d125380943ba8714cf3879aee67f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed596e634176410ea94c2e987a3a4f17":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"34183a24824d4ec589a6a1e3998f6289":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31a2351ef1184cbd96f704f071971513":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3e1b2c72c27f4348b9f006e1a2aa5fe6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca8ce07a830c44e8a70a75d00f89f2f1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}