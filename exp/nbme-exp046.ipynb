{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "crazy-perfume",
   "metadata": {
    "id": "blind-kingdom"
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-colorado",
   "metadata": {
    "id": "antique-glenn"
   },
   "source": [
    "- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-killer",
   "metadata": {
    "id": "bored-ministry"
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "relative-radical",
   "metadata": {
    "id": "deadly-confidence"
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"nbme-exp046\"\n",
    "ENV = \"local\"\n",
    "DEBUG_MODE = False\n",
    "SUBMISSION_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "handled-dairy",
   "metadata": {
    "id": "aware-worcester"
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    env=ENV\n",
    "    exp_name=EXP_NAME\n",
    "    debug=DEBUG_MODE\n",
    "    submission=SUBMISSION_MODE\n",
    "    apex=True\n",
    "    input_dir=None\n",
    "    output_dir=None\n",
    "    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n",
    "    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n",
    "    competition_name=\"nbme-score-clinical-patient-notes\"\n",
    "    id_col=\"id\"\n",
    "    target_col=\"location\"\n",
    "    pretrained_model_name=\"microsoft/deberta-large\"\n",
    "    tokenizer=None\n",
    "    max_len=None\n",
    "    output_dim=1\n",
    "    dropout=0.2\n",
    "    num_workers=4\n",
    "    batch_size=4\n",
    "    lr=2e-5\n",
    "    betas=(0.9, 0.98)\n",
    "    alpha=1\n",
    "    gamma=2\n",
    "    smoothing=0.0000\n",
    "    weight_decay=0.1\n",
    "    num_warmup_steps_rate=0.1\n",
    "    batch_scheduler=True\n",
    "    epochs=5\n",
    "    n_fold=4\n",
    "    train_fold=[0, 1, 2, 3]\n",
    "    seed=71\n",
    "    gradient_accumulation_steps=2\n",
    "    max_grad_norm=1000\n",
    "    print_freq=100\n",
    "    train=True\n",
    "    inference=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "posted-plant",
   "metadata": {
    "id": "personalized-death"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.train_fold = [0, 1]\n",
    "\n",
    "if CFG.submission:\n",
    "    CFG.train = False\n",
    "    CFG.inference = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-coverage",
   "metadata": {
    "id": "cardiovascular-neutral"
   },
   "source": [
    "## Directory Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intellectual-arrival",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "checked-boards",
    "outputId": "cea3ff7c-73cb-479b-82dc-d4ad1b7d6719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "print(CFG.env)\n",
    "if CFG.env == \"colab\":\n",
    "    # colab環境\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "    # install packages\n",
    "    !pip install transformers\n",
    "\n",
    "elif CFG.env == \"local\":\n",
    "    # ローカルサーバ\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "\n",
    "elif CFG.env == \"kaggle\":\n",
    "    # kaggle環境\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "voluntary-orchestra",
   "metadata": {
    "id": "vital-mexico"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import BartModel,BertModel,BertTokenizer\n",
    "from transformers import DebertaModel,DebertaTokenizer\n",
    "from transformers import RobertaModel,RobertaTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-binding",
   "metadata": {
    "id": "economic-ladder"
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "shared-orange",
   "metadata": {
    "id": "desperate-keyboard"
   },
   "outputs": [],
   "source": [
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "composed-premiere",
   "metadata": {
    "id": "flexible-wednesday"
   },
   "outputs": [],
   "source": [
    "def create_labels_for_scoring(df):\n",
    "    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n",
    "    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, \"location\"]\n",
    "        if lst:\n",
    "            new_lst = \";\".join(lst)\n",
    "            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n",
    "\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df[\"location_for_create_labels\"].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, token_probs, tokenizer):\n",
    "    res = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n",
    "        encoded = tokenizer(\n",
    "            text=text,\n",
    "            max_length=CFG.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n",
    "            start, end = offset_mapping\n",
    "            res[i][start:end] = pred\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_predicted_location_str(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def scoring(df, th=0.5):\n",
    "    labels = create_labels_for_scoring(df)\n",
    "\n",
    "    token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n",
    "    char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n",
    "    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n",
    "    preds = get_predictions(predicted_location_str)\n",
    "\n",
    "    score = get_score(labels, preds)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_best_thres(oof_df):\n",
    "    def f1_opt(x):\n",
    "        return -1 * scoring(oof_df, th=x)\n",
    "\n",
    "    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n",
    "    return best_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "plain-emphasis",
   "metadata": {
    "id": "logical-chemistry"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "heated-belize",
   "metadata": {
    "id": "gorgeous-record"
   },
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-beginning",
   "metadata": {
    "id": "frozen-africa"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "geological-wages",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "shaped-metallic",
    "outputId": "c2e09677-e2ae-4b51-b411-f1a0634026d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 6), (143, 3), (42146, 3), (5, 4))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(CFG.input_dir / \"train.csv\")\n",
    "features = pd.read_csv(CFG.input_dir / \"features.csv\")\n",
    "patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n",
    "test = pd.read_csv(CFG.input_dir / \"test.csv\")\n",
    "\n",
    "train.shape, features.shape, patient_notes.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "vietnamese-birthday",
   "metadata": {
    "id": "visible-australia"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n",
    "    print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-money",
   "metadata": {
    "id": "hydraulic-gibson"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "executed-utility",
   "metadata": {
    "id": "interpreted-northeast"
   },
   "outputs": [],
   "source": [
    "def preprocess_features(features):\n",
    "    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "\n",
    "\n",
    "features = preprocess_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "wired-seeking",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "martial-blind",
    "outputId": "5e7f2195-15c6-4e13-fc35-359b478f2af2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 8), (5, 6))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "unique-miami",
   "metadata": {
    "id": "electoral-favor"
   },
   "outputs": [],
   "source": [
    "train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n",
    "train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hispanic-fraction",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "reported-parade",
    "outputId": "bf29b91f-33b5-4aea-ce84-2e4fb8e9bf40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4399\n",
       "1    8181\n",
       "2    1296\n",
       "3     287\n",
       "4      99\n",
       "5      27\n",
       "6       9\n",
       "7       1\n",
       "8       1\n",
       "Name: annotation_length, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n",
    "display(train['annotation_length'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-function",
   "metadata": {
    "id": "enabling-relevance"
   },
   "source": [
    "## CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "russian-recipient",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "mature-coalition",
    "outputId": "ed39702e-822c-408e-ad92-eccd51f61c97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    3575\n",
       "1    3575\n",
       "2    3575\n",
       "3    3575\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Fold = GroupKFold(n_splits=CFG.n_fold)\n",
    "groups = train['pn_num'].values\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-anxiety",
   "metadata": {
    "id": "subjective-entrance"
   },
   "source": [
    "## Setup tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "burning-analysis",
   "metadata": {
    "id": "dramatic-afghanistan"
   },
   "outputs": [],
   "source": [
    "if CFG.submission:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG.pretrained_model_name)\n",
    "    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n",
    "\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-subject",
   "metadata": {
    "id": "divided-arrow"
   },
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "mighty-injury",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "8c7eb508782d4253af8cbff0a39e5d19",
      "33e4f48f7c8f40f48081c34bc992f215",
      "0c70395dca6341349fc883dc6b95090a",
      "0dfe4d3aa0354d95bb5d019420b510a9",
      "e6775e6fc2ad4b14b473b8a07e27426d",
      "40ae5d1c9e9f4feaa4207599ef17a1ec",
      "8cd3352e5e9342e4a814e9958ea6dc1d",
      "5a92171cb3d34fc8b1ed5119e32951ac",
      "a202501b76a748fe80180604dff32c7b",
      "c3d9ac191d9b4772ae4bca323a34ddd7",
      "9deae6afbd4740df8eb0289bc5f53ae7"
     ]
    },
    "id": "immune-campbell",
    "outputId": "dbda74a8-5a58-49cf-b86a-3e05c265f758"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5bd8dd142b54161ba2f08e48029194a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 433\n"
     ]
    }
   ],
   "source": [
    "pn_history_lengths = []\n",
    "tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    pn_history_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(pn_history_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "laden-hammer",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "47e8d82f628f4bd7b8b0ef0aa96852a5",
      "cc9a4ba21d664dafb8ae32a4e0a1c5e0",
      "f4421d3b3a844dbcb003f11902ee1898",
      "d02f186539954463873bb560b775894e",
      "14efac00edd349898e9fa95a63a2773d",
      "83d1b90076dc431893e0ab1c87e35f9c",
      "0b503e832e57492291cbf6e9ae66e343",
      "625abc68d2fb4fd4b8556c7cc1ae514a",
      "d2581946e9fd4f9a8dc56b3453cc70bd",
      "1f6c8df95c7845818253189f2e365ba9",
      "5e29db6be6284caa978ee223047f23c5"
     ]
    },
    "id": "northern-branch",
    "outputId": "e2c62ebe-aae9-413d-d424-6850759224a2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129b6f27de8143969ffed333c8c55837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 30\n"
     ]
    }
   ],
   "source": [
    "feature_text_lengths = []\n",
    "tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    feature_text_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(feature_text_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "covered-visibility",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oriental-jacksonville",
    "outputId": "293b5e4d-a369-433b-a5af-fbfd35f411a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 466\n"
     ]
    }
   ],
   "source": [
    "CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n",
    "\n",
    "print(\"max length:\", CFG.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "infectious-search",
   "metadata": {
    "id": "flexible-trainer"
   },
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "        self.annotation_lengths = self.df[\"annotation_length\"].values\n",
    "        self.locations = self.df[\"location\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def _create_label(self, pn_history, annotation_length, location_list):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        offset_mapping = encoded[\"offset_mapping\"]\n",
    "        ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n",
    "        label = np.zeros(len(offset_mapping))\n",
    "        label[ignore_idxes] = -1\n",
    "\n",
    "        if annotation_length > 0:\n",
    "            for location in location_list:\n",
    "                for loc in [s.split() for s in location.split(\";\")]:\n",
    "                    start, end = int(loc[0]), int(loc[1])\n",
    "                    start_idx = -1\n",
    "                    end_idx = -1\n",
    "                    for idx in range(len(offset_mapping)):\n",
    "                        if (start_idx == -1) & (start < offset_mapping[idx][0]):\n",
    "                            start_idx = idx - 1\n",
    "                        if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n",
    "                            end_idx = idx + 1\n",
    "                    if start_idx == -1:\n",
    "                        start_idx = end_idx\n",
    "                    if (start_idx != -1) & (end_idx != -1):\n",
    "                        label[start_idx:end_idx] = 1\n",
    "\n",
    "        return torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n",
    "        return input_, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "laughing-scoop",
   "metadata": {
    "id": "stock-robertson"
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        return input_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-tutorial",
   "metadata": {
    "id": "chemical-lucas"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "successful-monte",
   "metadata": {
    "id": "animated-array"
   },
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, model_config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if model_config_path is None:\n",
    "            self.model_config = AutoConfig.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "        else:\n",
    "            self.model_config = torch.load(model_config_path)\n",
    "\n",
    "        if pretrained:\n",
    "            self.backbone = AutoModel.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                config=self.model_config,\n",
    "            )\n",
    "            print(f\"Load weight from pretrained\")\n",
    "        else:\n",
    "            #self.backbone = AutoModel.from_config(self.model_config)\n",
    "            itpt = AutoModelForMaskedLM.from_config(self.model_config)\n",
    "            #path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp010/checkpoint-130170/pytorch_model.bin\")\n",
    "            path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n",
    "            state_dict = torch.load(path)\n",
    "            itpt.load_state_dict(state_dict)\n",
    "            self.backbone = itpt.deberta\n",
    "            print(f\"Load weight from {path}\")\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(self.cfg.dropout),\n",
    "            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h = self.backbone(**inputs)[\"last_hidden_state\"]\n",
    "        output = self.fc(h)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-profit",
   "metadata": {
    "id": "thorough-bristol"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "north-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, reduction='none', alpha=1, gamma=2):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        loss = self.alpha * (1. - pt)**self.gamma * bce_loss\n",
    "        if self.reduction == 'none':\n",
    "            loss = loss\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class SmoothFocalLoss(nn.Module):\n",
    "    def __init__(self, reduction='none', alpha=1, gamma=2, smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "        self.focal_loss = FocalLoss(reduction='none', alpha=alpha, gamma=gamma)\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothFocalLoss._smooth(targets, self.smoothing)\n",
    "        loss = self.focal_loss(inputs, targets)\n",
    "        if self.reduction == 'none':\n",
    "            loss = loss\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "    \n",
    "class CEFocalLoss(nn.Module):\n",
    "    def __init__(self, reduction='none', alpha=1, gamma=2):\n",
    "        super(CEFocalLoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        loss = self.alpha * (1. - pt)**self.gamma * ce_loss\n",
    "        if self.reduction == 'none':\n",
    "            loss = loss\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "    \n",
    "class SmoothCEFocalLoss(nn.Module):\n",
    "    def __init__(self, reduction='none', alpha=1, gamma=2, smoothing=0.0):\n",
    "        super(SmoothCEFocalLoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', label_smoothing=self.smoothing) # torch >= 1.10.0\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        loss = self.alpha * (1. - pt)**self.gamma * ce_loss\n",
    "        if self.reduction == 'none':\n",
    "            loss = loss\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "checked-cancer",
   "metadata": {
    "id": "talented-quantity"
   },
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    train_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    epoch,\n",
    "    scheduler,\n",
    "    device,\n",
    "):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels) in enumerate(train_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            output = model(inputs)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "\n",
    "        pos_nums = (labels == 1).sum(axis=1)\n",
    "        pos_nums = torch.tensor([[pos_num] * CFG.max_len for pos_num in pos_nums]).view(-1, 1).to(device)\n",
    "        pos_nums = torch.masked_select(pos_nums, labels.view(-1, 1) != -1)\n",
    "        weight = []\n",
    "        for pos_num in pos_nums:\n",
    "            if pos_num == 0:\n",
    "                weight.append(2.0)\n",
    "            else:\n",
    "                weight.append(1.0)\n",
    "        weight = torch.tensor(weight).to(device)\n",
    "        loss = loss * weight\n",
    "\n",
    "        loss = loss.mean()\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if CFG.batch_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n",
    "            print(\n",
    "                \"Epoch: [{0}][{1}/{2}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                \"Grad: {grad_norm:.4f}  \"\n",
    "                \"LR: {lr:.6f}  \"\n",
    "                .format(\n",
    "                    epoch+1,\n",
    "                    step,\n",
    "                    len(train_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n",
    "                    loss=losses,\n",
    "                     grad_norm=grad_norm,\n",
    "                     lr=scheduler.get_lr()[0],\n",
    "                )\n",
    "            )\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "confident-owner",
   "metadata": {
    "id": "figured-cooperative"
   },
   "outputs": [],
   "source": [
    "def valid_fn(\n",
    "    val_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    device,\n",
    "):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels) in enumerate(val_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "\n",
    "        pos_nums = (labels == 1).sum(axis=1)\n",
    "        pos_nums = torch.tensor([[pos_num] * CFG.max_len for pos_num in pos_nums]).view(-1, 1).to(device)\n",
    "        pos_nums = torch.masked_select(pos_nums, labels.view(-1, 1) != -1)\n",
    "        weight = []\n",
    "        for pos_num in pos_nums:\n",
    "            if pos_num == 0:\n",
    "                weight.append(2.0)\n",
    "            else:\n",
    "                weight.append(1.0)\n",
    "        weight = torch.tensor(weight).to(device)\n",
    "        loss = loss * weight\n",
    "\n",
    "        loss = loss.mean()\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n",
    "            print(\n",
    "                \"EVAL: [{0}/{1}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                .format(\n",
    "                    step, len(val_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n",
    "                    loss=losses,\n",
    "                )\n",
    "            )\n",
    "    preds = np.concatenate(preds)\n",
    "    return losses.avg, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "advanced-construction",
   "metadata": {
    "id": "played-pointer"
   },
   "outputs": [],
   "source": [
    "def inference_fn(test_dataloader, model, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preds = []\n",
    "    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "optimum-commons",
   "metadata": {
    "id": "brazilian-nigeria"
   },
   "outputs": [],
   "source": [
    "def train_loop(df, i_fold, device):\n",
    "    print(f\"========== fold: {i_fold} training ==========\")\n",
    "    train_idx = df[df[\"fold\"] != i_fold].index\n",
    "    val_idx = df[df[\"fold\"] == i_fold].index\n",
    "\n",
    "    train_folds = df.loc[train_idx].reset_index(drop=True)\n",
    "    val_folds = df.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = TrainingDataset(CFG, train_folds)\n",
    "    val_dataset = TrainingDataset(CFG, val_folds)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "    model = CustomModel(CFG, model_config_path=None, pretrained=False)   # itptを使うため\n",
    "    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n",
    "    model.to(device)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in param_optimizer if not any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n",
    "        {\"params\": [p for n, p in param_optimizer if any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=CFG.lr,\n",
    "        betas=CFG.betas,\n",
    "        weight_decay=CFG.weight_decay,\n",
    "    )\n",
    "    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n",
    "    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_train_optimization_steps,\n",
    "    )\n",
    "\n",
    "    #criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    criterion = SmoothFocalLoss(reduction=\"none\", alpha=CFG.alpha, gamma=CFG.gamma, smoothing=CFG.smoothing)\n",
    "    best_score = -1 * np.inf\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "        avg_loss = train_fn(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            scheduler,\n",
    "            device,\n",
    "        )\n",
    "        avg_val_loss, val_preds = valid_fn(\n",
    "            val_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "\n",
    "        # scoring\n",
    "        val_folds[[str(i) for i in range(CFG.max_len)]] = val_preds\n",
    "        score = scoring(val_folds, th=0.5)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n",
    "        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"predictions\": val_preds,\n",
    "                },\n",
    "                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "            )\n",
    "\n",
    "    predictions = torch.load(\n",
    "        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )[\"predictions\"]\n",
    "    val_folds[[str(i) for i in range(CFG.max_len)]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return val_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-buying",
   "metadata": {
    "id": "bearing-switch"
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "particular-machinery",
   "metadata": {
    "id": "desperate-crime"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for i_fold in range(CFG.n_fold):\n",
    "            if i_fold in CFG.train_fold:\n",
    "                _oof_df = train_loop(train, i_fold, device)\n",
    "                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n",
    "        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    if CFG.submission:\n",
    "        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n",
    "    else:\n",
    "        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    score = scoring(oof_df, th=0.5)\n",
    "    print(f\"Best thres: 0.5, Score: {score:.4f}\")\n",
    "    best_thres = get_best_thres(oof_df)\n",
    "    score = scoring(oof_df, th=best_thres)\n",
    "    print(f\"Best thres: {best_thres}, Score: {score:.4f}\")\n",
    "\n",
    "    if CFG.inference:\n",
    "        test_dataset = TestDataset(CFG, test)\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=CFG.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=CFG.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        predictions = []\n",
    "        for i_fold in CFG.train_fold:\n",
    "            if CFG.submission:\n",
    "                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n",
    "                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n",
    "            else:\n",
    "                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n",
    "\n",
    "            state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "            model.load_state_dict(state[\"model\"])\n",
    "            test_token_probs = inference_fn(test_dataloader, model, device)\n",
    "            test[[f\"fold{i_fold}_{i}\" for i in range(CFG.max_len)]] = test_token_probs\n",
    "            test_char_probs = get_char_probs(test[\"pn_history\"].values, test_token_probs, CFG.tokenizer)\n",
    "            predictions.append(test_char_probs)\n",
    "\n",
    "            del state, test_token_probs, model; gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        predictions = np.mean(predictions, axis=0)\n",
    "        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n",
    "        test[CFG.target_col] = predicted_location_str\n",
    "        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n",
    "        test[[CFG.id_col, CFG.target_col]].to_csv(\n",
    "            CFG.output_dir / \"submission.csv\", index=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "wired-motion",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "graduate-vision",
    "outputId": "90d56101-1cd3-4eac-8d44-5be254003857"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2681] Elapsed 0m 1s (remain 60m 47s) Loss: 0.1293(0.1293) Grad: 177140.7656  LR: 0.000000  \n",
      "Epoch: [1][100/2681] Elapsed 0m 50s (remain 21m 37s) Loss: 0.0565(0.0877) Grad: 39843.3125  LR: 0.000002  \n",
      "Epoch: [1][200/2681] Elapsed 1m 40s (remain 20m 37s) Loss: 0.0181(0.0593) Grad: 3393.4648  LR: 0.000003  \n",
      "Epoch: [1][300/2681] Elapsed 2m 30s (remain 19m 47s) Loss: 0.0102(0.0439) Grad: 2961.5247  LR: 0.000004  \n",
      "Epoch: [1][400/2681] Elapsed 3m 20s (remain 18m 58s) Loss: 0.0186(0.0362) Grad: 4776.1680  LR: 0.000006  \n",
      "Epoch: [1][500/2681] Elapsed 4m 9s (remain 18m 5s) Loss: 0.0102(0.0314) Grad: 3550.5093  LR: 0.000007  \n",
      "Epoch: [1][600/2681] Elapsed 4m 58s (remain 17m 14s) Loss: 0.0073(0.0277) Grad: 8274.9756  LR: 0.000009  \n",
      "Epoch: [1][700/2681] Elapsed 5m 49s (remain 16m 26s) Loss: 0.0017(0.0247) Grad: 1536.9468  LR: 0.000010  \n",
      "Epoch: [1][800/2681] Elapsed 6m 39s (remain 15m 36s) Loss: 0.0155(0.0223) Grad: 17019.2910  LR: 0.000012  \n",
      "Epoch: [1][900/2681] Elapsed 7m 28s (remain 14m 46s) Loss: 0.0010(0.0202) Grad: 1736.9620  LR: 0.000013  \n",
      "Epoch: [1][1000/2681] Elapsed 8m 18s (remain 13m 56s) Loss: 0.0057(0.0186) Grad: 10522.3145  LR: 0.000015  \n",
      "Epoch: [1][1100/2681] Elapsed 9m 8s (remain 13m 7s) Loss: 0.0060(0.0172) Grad: 6230.8589  LR: 0.000016  \n",
      "Epoch: [1][1200/2681] Elapsed 9m 58s (remain 12m 17s) Loss: 0.0124(0.0160) Grad: 14797.1182  LR: 0.000018  \n",
      "Epoch: [1][1300/2681] Elapsed 10m 48s (remain 11m 27s) Loss: 0.0132(0.0150) Grad: 7970.8716  LR: 0.000019  \n",
      "Epoch: [1][1400/2681] Elapsed 11m 38s (remain 10m 37s) Loss: 0.0040(0.0141) Grad: 9306.8428  LR: 0.000020  \n",
      "Epoch: [1][1500/2681] Elapsed 12m 27s (remain 9m 47s) Loss: 0.0125(0.0134) Grad: 13350.1768  LR: 0.000020  \n",
      "Epoch: [1][1600/2681] Elapsed 13m 17s (remain 8m 58s) Loss: 0.0003(0.0127) Grad: 717.2823  LR: 0.000020  \n",
      "Epoch: [1][1700/2681] Elapsed 14m 7s (remain 8m 8s) Loss: 0.0006(0.0121) Grad: 691.7079  LR: 0.000019  \n",
      "Epoch: [1][1800/2681] Elapsed 14m 57s (remain 7m 18s) Loss: 0.0015(0.0116) Grad: 1426.6891  LR: 0.000019  \n",
      "Epoch: [1][1900/2681] Elapsed 15m 46s (remain 6m 28s) Loss: 0.0037(0.0112) Grad: 4207.3403  LR: 0.000019  \n",
      "Epoch: [1][2000/2681] Elapsed 16m 36s (remain 5m 38s) Loss: 0.0018(0.0107) Grad: 1849.9409  LR: 0.000019  \n",
      "Epoch: [1][2100/2681] Elapsed 17m 26s (remain 4m 48s) Loss: 0.0016(0.0103) Grad: 1761.1873  LR: 0.000019  \n",
      "Epoch: [1][2200/2681] Elapsed 18m 16s (remain 3m 59s) Loss: 0.0008(0.0100) Grad: 1331.1648  LR: 0.000019  \n",
      "Epoch: [1][2300/2681] Elapsed 19m 5s (remain 3m 9s) Loss: 0.0007(0.0096) Grad: 937.2123  LR: 0.000018  \n",
      "Epoch: [1][2400/2681] Elapsed 19m 55s (remain 2m 19s) Loss: 0.0008(0.0093) Grad: 884.7990  LR: 0.000018  \n",
      "Epoch: [1][2500/2681] Elapsed 20m 45s (remain 1m 29s) Loss: 0.0005(0.0090) Grad: 1750.1636  LR: 0.000018  \n",
      "Epoch: [1][2600/2681] Elapsed 21m 35s (remain 0m 39s) Loss: 0.0007(0.0088) Grad: 791.0266  LR: 0.000018  \n",
      "Epoch: [1][2680/2681] Elapsed 22m 14s (remain 0m 0s) Loss: 0.0011(0.0086) Grad: 1058.2531  LR: 0.000018  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 8m 29s) Loss: 0.0010(0.0010) \n",
      "EVAL: [100/894] Elapsed 0m 27s (remain 3m 39s) Loss: 0.0002(0.0024) \n",
      "EVAL: [200/894] Elapsed 0m 55s (remain 3m 10s) Loss: 0.0021(0.0025) \n",
      "EVAL: [300/894] Elapsed 1m 22s (remain 2m 42s) Loss: 0.0006(0.0025) \n",
      "EVAL: [400/894] Elapsed 1m 49s (remain 2m 14s) Loss: 0.0020(0.0023) \n",
      "EVAL: [500/894] Elapsed 2m 16s (remain 1m 47s) Loss: 0.0006(0.0025) \n",
      "EVAL: [600/894] Elapsed 2m 43s (remain 1m 19s) Loss: 0.0008(0.0027) \n",
      "EVAL: [700/894] Elapsed 3m 10s (remain 0m 52s) Loss: 0.0020(0.0026) \n",
      "EVAL: [800/894] Elapsed 3m 38s (remain 0m 25s) Loss: 0.0001(0.0025) \n",
      "EVAL: [893/894] Elapsed 4m 2s (remain 0m 0s) Loss: 0.0002(0.0024) \n",
      "Epoch 1 - avg_train_loss: 0.0086  avg_val_loss: 0.0024  time: 1584s\n",
      "Epoch 1 - Score: 0.8379\n",
      "Epoch 1 - Save Best Score: 0.8379 Model\n",
      "Epoch: [2][0/2681] Elapsed 0m 0s (remain 35m 9s) Loss: 0.0002(0.0002) Grad: 1917.9686  LR: 0.000018  \n",
      "Epoch: [2][100/2681] Elapsed 0m 50s (remain 21m 22s) Loss: 0.0001(0.0019) Grad: 409.4371  LR: 0.000018  \n",
      "Epoch: [2][200/2681] Elapsed 1m 39s (remain 20m 29s) Loss: 0.0002(0.0021) Grad: 1474.3671  LR: 0.000017  \n",
      "Epoch: [2][300/2681] Elapsed 2m 29s (remain 19m 42s) Loss: 0.0169(0.0021) Grad: 26533.2930  LR: 0.000017  \n",
      "Epoch: [2][400/2681] Elapsed 3m 18s (remain 18m 50s) Loss: 0.0001(0.0020) Grad: 482.2161  LR: 0.000017  \n",
      "Epoch: [2][500/2681] Elapsed 4m 8s (remain 18m 0s) Loss: 0.0035(0.0020) Grad: 26102.7637  LR: 0.000017  \n",
      "Epoch: [2][600/2681] Elapsed 4m 57s (remain 17m 9s) Loss: 0.0014(0.0019) Grad: 6267.7710  LR: 0.000017  \n",
      "Epoch: [2][700/2681] Elapsed 5m 46s (remain 16m 19s) Loss: 0.0000(0.0019) Grad: 29.0969  LR: 0.000017  \n",
      "Epoch: [2][800/2681] Elapsed 6m 36s (remain 15m 30s) Loss: 0.0004(0.0019) Grad: 1615.1451  LR: 0.000016  \n",
      "Epoch: [2][900/2681] Elapsed 7m 25s (remain 14m 40s) Loss: 0.0057(0.0019) Grad: 25938.0078  LR: 0.000016  \n",
      "Epoch: [2][1000/2681] Elapsed 8m 15s (remain 13m 51s) Loss: 0.0010(0.0019) Grad: 2440.5647  LR: 0.000016  \n",
      "Epoch: [2][1100/2681] Elapsed 9m 4s (remain 13m 1s) Loss: 0.0038(0.0019) Grad: 11637.3701  LR: 0.000016  \n",
      "Epoch: [2][1200/2681] Elapsed 9m 54s (remain 12m 12s) Loss: 0.0003(0.0019) Grad: 976.1614  LR: 0.000016  \n",
      "Epoch: [2][1300/2681] Elapsed 10m 43s (remain 11m 22s) Loss: 0.0000(0.0019) Grad: 149.3252  LR: 0.000016  \n",
      "Epoch: [2][1400/2681] Elapsed 11m 33s (remain 10m 33s) Loss: 0.0001(0.0019) Grad: 270.7827  LR: 0.000015  \n",
      "Epoch: [2][1500/2681] Elapsed 12m 23s (remain 9m 44s) Loss: 0.0003(0.0019) Grad: 1009.8396  LR: 0.000015  \n",
      "Epoch: [2][1600/2681] Elapsed 13m 12s (remain 8m 54s) Loss: 0.0000(0.0019) Grad: 368.2086  LR: 0.000015  \n",
      "Epoch: [2][1700/2681] Elapsed 14m 2s (remain 8m 5s) Loss: 0.0001(0.0019) Grad: 404.7401  LR: 0.000015  \n",
      "Epoch: [2][1800/2681] Elapsed 14m 51s (remain 7m 15s) Loss: 0.0013(0.0019) Grad: 2915.7468  LR: 0.000015  \n",
      "Epoch: [2][1900/2681] Elapsed 15m 41s (remain 6m 26s) Loss: 0.0001(0.0019) Grad: 279.1236  LR: 0.000015  \n",
      "Epoch: [2][2000/2681] Elapsed 16m 31s (remain 5m 36s) Loss: 0.0010(0.0019) Grad: 5343.9854  LR: 0.000014  \n",
      "Epoch: [2][2100/2681] Elapsed 17m 20s (remain 4m 47s) Loss: 0.0003(0.0019) Grad: 651.2932  LR: 0.000014  \n",
      "Epoch: [2][2200/2681] Elapsed 18m 10s (remain 3m 57s) Loss: 0.0025(0.0019) Grad: 5016.3535  LR: 0.000014  \n",
      "Epoch: [2][2300/2681] Elapsed 18m 59s (remain 3m 8s) Loss: 0.0006(0.0019) Grad: 1665.3030  LR: 0.000014  \n",
      "Epoch: [2][2400/2681] Elapsed 19m 50s (remain 2m 18s) Loss: 0.0011(0.0019) Grad: 8406.6514  LR: 0.000014  \n",
      "Epoch: [2][2500/2681] Elapsed 20m 39s (remain 1m 29s) Loss: 0.0082(0.0019) Grad: 11396.6250  LR: 0.000014  \n",
      "Epoch: [2][2600/2681] Elapsed 21m 29s (remain 0m 39s) Loss: 0.0036(0.0019) Grad: 13613.0674  LR: 0.000013  \n",
      "Epoch: [2][2680/2681] Elapsed 22m 8s (remain 0m 0s) Loss: 0.0055(0.0019) Grad: 47207.6562  LR: 0.000013  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 8m 18s) Loss: 0.0006(0.0006) \n",
      "EVAL: [100/894] Elapsed 0m 27s (remain 3m 38s) Loss: 0.0003(0.0017) \n",
      "EVAL: [200/894] Elapsed 0m 55s (remain 3m 10s) Loss: 0.0025(0.0017) \n",
      "EVAL: [300/894] Elapsed 1m 22s (remain 2m 42s) Loss: 0.0005(0.0019) \n",
      "EVAL: [400/894] Elapsed 1m 49s (remain 2m 14s) Loss: 0.0023(0.0016) \n",
      "EVAL: [500/894] Elapsed 2m 16s (remain 1m 47s) Loss: 0.0003(0.0021) \n",
      "EVAL: [600/894] Elapsed 2m 44s (remain 1m 20s) Loss: 0.0006(0.0023) \n",
      "EVAL: [700/894] Elapsed 3m 11s (remain 0m 52s) Loss: 0.0008(0.0022) \n",
      "EVAL: [800/894] Elapsed 3m 38s (remain 0m 25s) Loss: 0.0003(0.0022) \n",
      "EVAL: [893/894] Elapsed 4m 3s (remain 0m 0s) Loss: 0.0000(0.0021) \n",
      "Epoch 2 - avg_train_loss: 0.0019  avg_val_loss: 0.0021  time: 1578s\n",
      "Epoch 2 - Score: 0.8633\n",
      "Epoch 2 - Save Best Score: 0.8633 Model\n",
      "Epoch: [3][0/2681] Elapsed 0m 0s (remain 40m 12s) Loss: 0.0006(0.0006) Grad: 2345.2151  LR: 0.000013  \n",
      "Epoch: [3][100/2681] Elapsed 0m 50s (remain 21m 21s) Loss: 0.0007(0.0014) Grad: 2929.8340  LR: 0.000013  \n",
      "Epoch: [3][200/2681] Elapsed 1m 39s (remain 20m 29s) Loss: 0.0036(0.0014) Grad: 9672.8623  LR: 0.000013  \n",
      "Epoch: [3][300/2681] Elapsed 2m 29s (remain 19m 38s) Loss: 0.0027(0.0013) Grad: 4492.5205  LR: 0.000013  \n",
      "Epoch: [3][400/2681] Elapsed 3m 18s (remain 18m 50s) Loss: 0.0001(0.0013) Grad: 320.3308  LR: 0.000013  \n",
      "Epoch: [3][500/2681] Elapsed 4m 8s (remain 18m 0s) Loss: 0.0020(0.0013) Grad: 4973.0576  LR: 0.000013  \n",
      "Epoch: [3][600/2681] Elapsed 4m 57s (remain 17m 10s) Loss: 0.0027(0.0013) Grad: 34767.7578  LR: 0.000012  \n",
      "Epoch: [3][700/2681] Elapsed 5m 46s (remain 16m 20s) Loss: 0.0005(0.0013) Grad: 2341.9250  LR: 0.000012  \n",
      "Epoch: [3][800/2681] Elapsed 6m 36s (remain 15m 31s) Loss: 0.0004(0.0013) Grad: 1508.1986  LR: 0.000012  \n",
      "Epoch: [3][900/2681] Elapsed 7m 26s (remain 14m 42s) Loss: 0.0059(0.0013) Grad: 12281.3496  LR: 0.000012  \n",
      "Epoch: [3][1000/2681] Elapsed 8m 16s (remain 13m 52s) Loss: 0.0001(0.0013) Grad: 1031.1658  LR: 0.000012  \n",
      "Epoch: [3][1100/2681] Elapsed 9m 5s (remain 13m 2s) Loss: 0.0120(0.0013) Grad: 20906.8809  LR: 0.000012  \n",
      "Epoch: [3][1200/2681] Elapsed 9m 55s (remain 12m 14s) Loss: 0.0001(0.0013) Grad: 345.0538  LR: 0.000011  \n",
      "Epoch: [3][1300/2681] Elapsed 10m 45s (remain 11m 24s) Loss: 0.0001(0.0013) Grad: 437.1537  LR: 0.000011  \n",
      "Epoch: [3][1400/2681] Elapsed 11m 34s (remain 10m 34s) Loss: 0.0000(0.0013) Grad: 43.2307  LR: 0.000011  \n",
      "Epoch: [3][1500/2681] Elapsed 12m 25s (remain 9m 45s) Loss: 0.0002(0.0013) Grad: 1171.5057  LR: 0.000011  \n",
      "Epoch: [3][1600/2681] Elapsed 13m 14s (remain 8m 55s) Loss: 0.0001(0.0013) Grad: 377.7862  LR: 0.000011  \n",
      "Epoch: [3][1700/2681] Elapsed 14m 4s (remain 8m 6s) Loss: 0.0013(0.0013) Grad: 3244.5781  LR: 0.000011  \n",
      "Epoch: [3][1800/2681] Elapsed 14m 54s (remain 7m 17s) Loss: 0.0009(0.0013) Grad: 2820.0330  LR: 0.000010  \n",
      "Epoch: [3][1900/2681] Elapsed 15m 44s (remain 6m 27s) Loss: 0.0022(0.0013) Grad: 4512.3662  LR: 0.000010  \n",
      "Epoch: [3][2000/2681] Elapsed 16m 33s (remain 5m 37s) Loss: 0.0035(0.0013) Grad: 9288.6064  LR: 0.000010  \n",
      "Epoch: [3][2100/2681] Elapsed 17m 23s (remain 4m 47s) Loss: 0.0000(0.0013) Grad: 42.8087  LR: 0.000010  \n",
      "Epoch: [3][2200/2681] Elapsed 18m 12s (remain 3m 58s) Loss: 0.0000(0.0013) Grad: 6.4076  LR: 0.000010  \n",
      "Epoch: [3][2300/2681] Elapsed 19m 2s (remain 3m 8s) Loss: 0.0000(0.0013) Grad: 174.0516  LR: 0.000010  \n",
      "Epoch: [3][2400/2681] Elapsed 19m 52s (remain 2m 19s) Loss: 0.0003(0.0013) Grad: 1031.2292  LR: 0.000009  \n",
      "Epoch: [3][2500/2681] Elapsed 20m 42s (remain 1m 29s) Loss: 0.0008(0.0013) Grad: 8699.1709  LR: 0.000009  \n",
      "Epoch: [3][2600/2681] Elapsed 21m 31s (remain 0m 39s) Loss: 0.0005(0.0013) Grad: 1248.9899  LR: 0.000009  \n",
      "Epoch: [3][2680/2681] Elapsed 22m 11s (remain 0m 0s) Loss: 0.0000(0.0014) Grad: 119.5599  LR: 0.000009  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 9m 3s) Loss: 0.0004(0.0004) \n",
      "EVAL: [100/894] Elapsed 0m 28s (remain 3m 40s) Loss: 0.0000(0.0016) \n",
      "EVAL: [200/894] Elapsed 0m 55s (remain 3m 10s) Loss: 0.0027(0.0018) \n",
      "EVAL: [300/894] Elapsed 1m 22s (remain 2m 42s) Loss: 0.0005(0.0019) \n",
      "EVAL: [400/894] Elapsed 1m 49s (remain 2m 15s) Loss: 0.0027(0.0017) \n",
      "EVAL: [500/894] Elapsed 2m 16s (remain 1m 47s) Loss: 0.0001(0.0022) \n",
      "EVAL: [600/894] Elapsed 2m 43s (remain 1m 19s) Loss: 0.0006(0.0025) \n",
      "EVAL: [700/894] Elapsed 3m 11s (remain 0m 52s) Loss: 0.0004(0.0024) \n",
      "EVAL: [800/894] Elapsed 3m 38s (remain 0m 25s) Loss: 0.0002(0.0023) \n",
      "EVAL: [893/894] Elapsed 4m 3s (remain 0m 0s) Loss: 0.0000(0.0023) \n",
      "Epoch 3 - avg_train_loss: 0.0014  avg_val_loss: 0.0023  time: 1580s\n",
      "Epoch 3 - Score: 0.8751\n",
      "Epoch 3 - Save Best Score: 0.8751 Model\n",
      "Epoch: [4][0/2681] Elapsed 0m 0s (remain 41m 32s) Loss: 0.0079(0.0079) Grad: 27350.0918  LR: 0.000009  \n",
      "Epoch: [4][100/2681] Elapsed 0m 50s (remain 21m 37s) Loss: 0.0000(0.0009) Grad: 9.8048  LR: 0.000009  \n",
      "Epoch: [4][200/2681] Elapsed 1m 40s (remain 20m 40s) Loss: 0.0000(0.0011) Grad: 44.4955  LR: 0.000009  \n",
      "Epoch: [4][300/2681] Elapsed 2m 29s (remain 19m 45s) Loss: 0.0000(0.0011) Grad: 12.5290  LR: 0.000008  \n",
      "Epoch: [4][400/2681] Elapsed 3m 19s (remain 18m 54s) Loss: 0.0106(0.0010) Grad: 17497.9609  LR: 0.000008  \n",
      "Epoch: [4][500/2681] Elapsed 4m 8s (remain 18m 2s) Loss: 0.0003(0.0010) Grad: 4083.5706  LR: 0.000008  \n",
      "Epoch: [4][600/2681] Elapsed 4m 58s (remain 17m 11s) Loss: 0.0012(0.0011) Grad: 2945.9768  LR: 0.000008  \n",
      "Epoch: [4][700/2681] Elapsed 5m 47s (remain 16m 21s) Loss: 0.0001(0.0011) Grad: 479.8120  LR: 0.000008  \n",
      "Epoch: [4][800/2681] Elapsed 6m 37s (remain 15m 33s) Loss: 0.0009(0.0011) Grad: 4948.8677  LR: 0.000008  \n",
      "Epoch: [4][900/2681] Elapsed 7m 27s (remain 14m 43s) Loss: 0.0194(0.0011) Grad: 36711.1250  LR: 0.000007  \n",
      "Epoch: [4][1000/2681] Elapsed 8m 16s (remain 13m 53s) Loss: 0.0008(0.0011) Grad: 2677.7336  LR: 0.000007  \n",
      "Epoch: [4][1100/2681] Elapsed 9m 5s (remain 13m 3s) Loss: 0.0002(0.0011) Grad: 1457.1558  LR: 0.000007  \n",
      "Epoch: [4][1200/2681] Elapsed 9m 55s (remain 12m 14s) Loss: 0.0007(0.0011) Grad: 5417.1304  LR: 0.000007  \n",
      "Epoch: [4][1300/2681] Elapsed 10m 45s (remain 11m 25s) Loss: 0.0007(0.0011) Grad: 3039.7615  LR: 0.000007  \n",
      "Epoch: [4][1400/2681] Elapsed 11m 35s (remain 10m 35s) Loss: 0.0001(0.0011) Grad: 491.9821  LR: 0.000007  \n",
      "Epoch: [4][1500/2681] Elapsed 12m 25s (remain 9m 45s) Loss: 0.0017(0.0011) Grad: 5689.9141  LR: 0.000006  \n",
      "Epoch: [4][1600/2681] Elapsed 13m 14s (remain 8m 56s) Loss: 0.0008(0.0011) Grad: 3048.3704  LR: 0.000006  \n",
      "Epoch: [4][1700/2681] Elapsed 14m 3s (remain 8m 6s) Loss: 0.0001(0.0011) Grad: 765.1868  LR: 0.000006  \n",
      "Epoch: [4][1800/2681] Elapsed 14m 52s (remain 7m 16s) Loss: 0.0004(0.0011) Grad: 3188.9468  LR: 0.000006  \n",
      "Epoch: [4][1900/2681] Elapsed 15m 42s (remain 6m 26s) Loss: 0.0000(0.0011) Grad: 121.1852  LR: 0.000006  \n",
      "Epoch: [4][2000/2681] Elapsed 16m 32s (remain 5m 37s) Loss: 0.0003(0.0011) Grad: 3551.4631  LR: 0.000006  \n",
      "Epoch: [4][2100/2681] Elapsed 17m 21s (remain 4m 47s) Loss: 0.0009(0.0011) Grad: 2548.8462  LR: 0.000005  \n",
      "Epoch: [4][2200/2681] Elapsed 18m 11s (remain 3m 57s) Loss: 0.0098(0.0011) Grad: 42323.6602  LR: 0.000005  \n",
      "Epoch: [4][2300/2681] Elapsed 19m 0s (remain 3m 8s) Loss: 0.0013(0.0011) Grad: 3611.4421  LR: 0.000005  \n",
      "Epoch: [4][2400/2681] Elapsed 19m 50s (remain 2m 18s) Loss: 0.0006(0.0011) Grad: 2169.4751  LR: 0.000005  \n",
      "Epoch: [4][2500/2681] Elapsed 20m 40s (remain 1m 29s) Loss: 0.0027(0.0011) Grad: 8170.3960  LR: 0.000005  \n",
      "Epoch: [4][2600/2681] Elapsed 21m 30s (remain 0m 39s) Loss: 0.0017(0.0011) Grad: 6480.2471  LR: 0.000005  \n",
      "Epoch: [4][2680/2681] Elapsed 22m 9s (remain 0m 0s) Loss: 0.0013(0.0011) Grad: 3764.0247  LR: 0.000004  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 9m 0s) Loss: 0.0005(0.0005) \n",
      "EVAL: [100/894] Elapsed 0m 27s (remain 3m 37s) Loss: 0.0001(0.0020) \n",
      "EVAL: [200/894] Elapsed 0m 55s (remain 3m 10s) Loss: 0.0026(0.0020) \n",
      "EVAL: [300/894] Elapsed 1m 22s (remain 2m 42s) Loss: 0.0006(0.0020) \n",
      "EVAL: [400/894] Elapsed 1m 49s (remain 2m 15s) Loss: 0.0030(0.0018) \n",
      "EVAL: [500/894] Elapsed 2m 16s (remain 1m 47s) Loss: 0.0001(0.0023) \n",
      "EVAL: [600/894] Elapsed 2m 43s (remain 1m 19s) Loss: 0.0002(0.0026) \n",
      "EVAL: [700/894] Elapsed 3m 10s (remain 0m 52s) Loss: 0.0007(0.0026) \n",
      "EVAL: [800/894] Elapsed 3m 38s (remain 0m 25s) Loss: 0.0001(0.0025) \n",
      "EVAL: [893/894] Elapsed 4m 2s (remain 0m 0s) Loss: 0.0000(0.0025) \n",
      "Epoch 4 - avg_train_loss: 0.0011  avg_val_loss: 0.0025  time: 1579s\n",
      "Epoch 4 - Score: 0.8827\n",
      "Epoch 4 - Save Best Score: 0.8827 Model\n",
      "Epoch: [5][0/2681] Elapsed 0m 0s (remain 39m 41s) Loss: 0.0001(0.0001) Grad: 1048.9976  LR: 0.000004  \n",
      "Epoch: [5][100/2681] Elapsed 0m 50s (remain 21m 25s) Loss: 0.0000(0.0008) Grad: 102.9077  LR: 0.000004  \n",
      "Epoch: [5][200/2681] Elapsed 1m 40s (remain 20m 38s) Loss: 0.0000(0.0008) Grad: 447.5575  LR: 0.000004  \n",
      "Epoch: [5][300/2681] Elapsed 2m 30s (remain 19m 46s) Loss: 0.0018(0.0008) Grad: 3743.5464  LR: 0.000004  \n",
      "Epoch: [5][400/2681] Elapsed 3m 19s (remain 18m 52s) Loss: 0.0003(0.0008) Grad: 1334.1002  LR: 0.000004  \n",
      "Epoch: [5][500/2681] Elapsed 4m 8s (remain 18m 2s) Loss: 0.0002(0.0008) Grad: 738.0618  LR: 0.000004  \n",
      "Epoch: [5][600/2681] Elapsed 4m 58s (remain 17m 14s) Loss: 0.0002(0.0008) Grad: 1590.7795  LR: 0.000003  \n",
      "Epoch: [5][700/2681] Elapsed 5m 48s (remain 16m 22s) Loss: 0.0000(0.0009) Grad: 30.6257  LR: 0.000003  \n",
      "Epoch: [5][800/2681] Elapsed 6m 37s (remain 15m 32s) Loss: 0.0001(0.0009) Grad: 437.1341  LR: 0.000003  \n",
      "Epoch: [5][900/2681] Elapsed 7m 27s (remain 14m 43s) Loss: 0.0000(0.0009) Grad: 23.8791  LR: 0.000003  \n",
      "Epoch: [5][1000/2681] Elapsed 8m 16s (remain 13m 53s) Loss: 0.0003(0.0009) Grad: 1183.3016  LR: 0.000003  \n",
      "Epoch: [5][1100/2681] Elapsed 9m 6s (remain 13m 3s) Loss: 0.0001(0.0009) Grad: 793.9770  LR: 0.000003  \n",
      "Epoch: [5][1200/2681] Elapsed 9m 56s (remain 12m 14s) Loss: 0.0001(0.0009) Grad: 569.9512  LR: 0.000002  \n",
      "Epoch: [5][1300/2681] Elapsed 10m 45s (remain 11m 24s) Loss: 0.0000(0.0009) Grad: 157.0844  LR: 0.000002  \n",
      "Epoch: [5][1400/2681] Elapsed 11m 34s (remain 10m 34s) Loss: 0.0001(0.0009) Grad: 557.2307  LR: 0.000002  \n",
      "Epoch: [5][1500/2681] Elapsed 12m 23s (remain 9m 44s) Loss: 0.0000(0.0008) Grad: 65.8296  LR: 0.000002  \n",
      "Epoch: [5][1600/2681] Elapsed 13m 12s (remain 8m 54s) Loss: 0.0000(0.0009) Grad: 166.8783  LR: 0.000002  \n",
      "Epoch: [5][1700/2681] Elapsed 14m 2s (remain 8m 5s) Loss: 0.0000(0.0008) Grad: 3.7270  LR: 0.000002  \n",
      "Epoch: [5][1800/2681] Elapsed 14m 51s (remain 7m 15s) Loss: 0.0000(0.0008) Grad: 47.8843  LR: 0.000001  \n",
      "Epoch: [5][1900/2681] Elapsed 15m 41s (remain 6m 26s) Loss: 0.0000(0.0008) Grad: 493.9959  LR: 0.000001  \n",
      "Epoch: [5][2000/2681] Elapsed 16m 30s (remain 5m 36s) Loss: 0.0000(0.0008) Grad: 209.6531  LR: 0.000001  \n",
      "Epoch: [5][2100/2681] Elapsed 17m 19s (remain 4m 46s) Loss: 0.0189(0.0008) Grad: 34180.7891  LR: 0.000001  \n",
      "Epoch: [5][2200/2681] Elapsed 18m 9s (remain 3m 57s) Loss: 0.0001(0.0008) Grad: 1087.9332  LR: 0.000001  \n",
      "Epoch: [5][2300/2681] Elapsed 18m 58s (remain 3m 8s) Loss: 0.0014(0.0008) Grad: 4170.0708  LR: 0.000001  \n",
      "Epoch: [5][2400/2681] Elapsed 19m 47s (remain 2m 18s) Loss: 0.0056(0.0008) Grad: 14618.7861  LR: 0.000000  \n",
      "Epoch: [5][2500/2681] Elapsed 20m 37s (remain 1m 29s) Loss: 0.0001(0.0008) Grad: 467.5009  LR: 0.000000  \n",
      "Epoch: [5][2600/2681] Elapsed 21m 26s (remain 0m 39s) Loss: 0.0000(0.0008) Grad: 4.8791  LR: 0.000000  \n",
      "Epoch: [5][2680/2681] Elapsed 22m 6s (remain 0m 0s) Loss: 0.0004(0.0008) Grad: 1848.0421  LR: 0.000000  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 9m 26s) Loss: 0.0005(0.0005) \n",
      "EVAL: [100/894] Elapsed 0m 27s (remain 3m 38s) Loss: 0.0000(0.0021) \n",
      "EVAL: [200/894] Elapsed 0m 55s (remain 3m 10s) Loss: 0.0027(0.0022) \n",
      "EVAL: [300/894] Elapsed 1m 22s (remain 2m 42s) Loss: 0.0010(0.0022) \n",
      "EVAL: [400/894] Elapsed 1m 49s (remain 2m 14s) Loss: 0.0033(0.0021) \n",
      "EVAL: [500/894] Elapsed 2m 16s (remain 1m 47s) Loss: 0.0001(0.0026) \n",
      "EVAL: [600/894] Elapsed 2m 43s (remain 1m 19s) Loss: 0.0002(0.0029) \n",
      "EVAL: [700/894] Elapsed 3m 10s (remain 0m 52s) Loss: 0.0004(0.0029) \n",
      "EVAL: [800/894] Elapsed 3m 38s (remain 0m 25s) Loss: 0.0002(0.0028) \n",
      "EVAL: [893/894] Elapsed 4m 3s (remain 0m 0s) Loss: 0.0000(0.0027) \n",
      "Epoch 5 - avg_train_loss: 0.0008  avg_val_loss: 0.0027  time: 1576s\n",
      "Epoch 5 - Score: 0.8829\n",
      "Epoch 5 - Save Best Score: 0.8829 Model\n",
      "========== fold: 1 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2681] Elapsed 0m 0s (remain 37m 32s) Loss: 0.1258(0.1258) Grad: 165931.3438  LR: 0.000000  \n",
      "Epoch: [1][100/2681] Elapsed 0m 50s (remain 21m 40s) Loss: 0.0944(0.1266) Grad: 59876.7617  LR: 0.000002  \n",
      "Epoch: [1][200/2681] Elapsed 1m 40s (remain 20m 41s) Loss: 0.0145(0.0882) Grad: 8370.7744  LR: 0.000003  \n",
      "Epoch: [1][300/2681] Elapsed 2m 30s (remain 19m 48s) Loss: 0.0146(0.0637) Grad: 4003.4182  LR: 0.000004  \n",
      "Epoch: [1][400/2681] Elapsed 3m 20s (remain 18m 59s) Loss: 0.0159(0.0509) Grad: 5571.0405  LR: 0.000006  \n",
      "Epoch: [1][500/2681] Elapsed 4m 10s (remain 18m 11s) Loss: 0.0215(0.0431) Grad: 5110.0918  LR: 0.000007  \n",
      "Epoch: [1][600/2681] Elapsed 5m 1s (remain 17m 22s) Loss: 0.0057(0.0376) Grad: 2825.6255  LR: 0.000009  \n",
      "Epoch: [1][700/2681] Elapsed 5m 50s (remain 16m 31s) Loss: 0.0069(0.0333) Grad: 6234.9902  LR: 0.000010  \n",
      "Epoch: [1][800/2681] Elapsed 6m 41s (remain 15m 41s) Loss: 0.0032(0.0299) Grad: 3237.8149  LR: 0.000012  \n",
      "Epoch: [1][900/2681] Elapsed 7m 31s (remain 14m 51s) Loss: 0.0062(0.0270) Grad: 7781.6919  LR: 0.000013  \n",
      "Epoch: [1][1000/2681] Elapsed 8m 21s (remain 14m 1s) Loss: 0.0045(0.0247) Grad: 4312.5908  LR: 0.000015  \n",
      "Epoch: [1][1100/2681] Elapsed 9m 10s (remain 13m 10s) Loss: 0.0009(0.0228) Grad: 1762.0837  LR: 0.000016  \n",
      "Epoch: [1][1200/2681] Elapsed 10m 0s (remain 12m 19s) Loss: 0.0015(0.0212) Grad: 1959.3895  LR: 0.000018  \n",
      "Epoch: [1][1300/2681] Elapsed 10m 50s (remain 11m 29s) Loss: 0.0033(0.0198) Grad: 5678.3735  LR: 0.000019  \n",
      "Epoch: [1][1400/2681] Elapsed 11m 40s (remain 10m 40s) Loss: 0.0022(0.0186) Grad: 3630.6663  LR: 0.000020  \n",
      "Epoch: [1][1500/2681] Elapsed 12m 31s (remain 9m 50s) Loss: 0.0036(0.0175) Grad: 4173.2388  LR: 0.000020  \n",
      "Epoch: [1][1600/2681] Elapsed 13m 21s (remain 9m 0s) Loss: 0.0039(0.0166) Grad: 9020.0039  LR: 0.000020  \n",
      "Epoch: [1][1700/2681] Elapsed 14m 11s (remain 8m 10s) Loss: 0.0022(0.0158) Grad: 1830.6112  LR: 0.000019  \n",
      "Epoch: [1][1800/2681] Elapsed 15m 0s (remain 7m 20s) Loss: 0.0011(0.0151) Grad: 731.0981  LR: 0.000019  \n",
      "Epoch: [1][1900/2681] Elapsed 15m 50s (remain 6m 29s) Loss: 0.0017(0.0144) Grad: 1711.3156  LR: 0.000019  \n",
      "Epoch: [1][2000/2681] Elapsed 16m 40s (remain 5m 40s) Loss: 0.0012(0.0139) Grad: 1992.0325  LR: 0.000019  \n",
      "Epoch: [1][2100/2681] Elapsed 17m 30s (remain 4m 49s) Loss: 0.0007(0.0133) Grad: 877.9123  LR: 0.000019  \n",
      "Epoch: [1][2200/2681] Elapsed 18m 20s (remain 3m 59s) Loss: 0.0021(0.0129) Grad: 1551.4252  LR: 0.000019  \n",
      "Epoch: [1][2300/2681] Elapsed 19m 9s (remain 3m 9s) Loss: 0.0016(0.0124) Grad: 2188.9294  LR: 0.000018  \n",
      "Epoch: [1][2400/2681] Elapsed 19m 59s (remain 2m 19s) Loss: 0.0009(0.0120) Grad: 1013.0156  LR: 0.000018  \n",
      "Epoch: [1][2500/2681] Elapsed 20m 48s (remain 1m 29s) Loss: 0.0005(0.0116) Grad: 680.5723  LR: 0.000018  \n",
      "Epoch: [1][2600/2681] Elapsed 21m 38s (remain 0m 39s) Loss: 0.0004(0.0112) Grad: 764.8496  LR: 0.000018  \n",
      "Epoch: [1][2680/2681] Elapsed 22m 18s (remain 0m 0s) Loss: 0.0026(0.0110) Grad: 1818.6343  LR: 0.000018  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 9m 30s) Loss: 0.0018(0.0018) \n",
      "EVAL: [100/894] Elapsed 0m 27s (remain 3m 37s) Loss: 0.0008(0.0014) \n",
      "EVAL: [200/894] Elapsed 0m 54s (remain 3m 9s) Loss: 0.0042(0.0028) \n",
      "EVAL: [300/894] Elapsed 1m 21s (remain 2m 40s) Loss: 0.0020(0.0036) \n",
      "EVAL: [400/894] Elapsed 1m 48s (remain 2m 13s) Loss: 0.0005(0.0032) \n",
      "EVAL: [500/894] Elapsed 2m 16s (remain 1m 46s) Loss: 0.0052(0.0032) \n",
      "EVAL: [600/894] Elapsed 2m 43s (remain 1m 19s) Loss: 0.0032(0.0031) \n",
      "EVAL: [700/894] Elapsed 3m 10s (remain 0m 52s) Loss: 0.0001(0.0029) \n",
      "EVAL: [800/894] Elapsed 3m 37s (remain 0m 25s) Loss: 0.0009(0.0028) \n",
      "EVAL: [893/894] Elapsed 4m 2s (remain 0m 0s) Loss: 0.0056(0.0026) \n",
      "Epoch 1 - avg_train_loss: 0.0110  avg_val_loss: 0.0026  time: 1588s\n",
      "Epoch 1 - Score: 0.8285\n",
      "Epoch 1 - Save Best Score: 0.8285 Model\n",
      "Epoch: [2][0/2681] Elapsed 0m 0s (remain 35m 57s) Loss: 0.0001(0.0001) Grad: 1014.6911  LR: 0.000018  \n",
      "Epoch: [2][100/2681] Elapsed 0m 50s (remain 21m 23s) Loss: 0.0059(0.0018) Grad: 15819.3135  LR: 0.000018  \n",
      "Epoch: [2][200/2681] Elapsed 1m 40s (remain 20m 37s) Loss: 0.0006(0.0019) Grad: 3261.2908  LR: 0.000017  \n",
      "Epoch: [2][300/2681] Elapsed 2m 30s (remain 19m 48s) Loss: 0.0028(0.0018) Grad: 5217.8813  LR: 0.000017  \n",
      "Epoch: [2][400/2681] Elapsed 3m 19s (remain 18m 55s) Loss: 0.0001(0.0018) Grad: 792.8917  LR: 0.000017  \n",
      "Epoch: [2][500/2681] Elapsed 4m 9s (remain 18m 6s) Loss: 0.0004(0.0018) Grad: 2322.9690  LR: 0.000017  \n",
      "Epoch: [2][600/2681] Elapsed 4m 59s (remain 17m 15s) Loss: 0.0013(0.0019) Grad: 3505.0459  LR: 0.000017  \n",
      "Epoch: [2][700/2681] Elapsed 5m 48s (remain 16m 24s) Loss: 0.0013(0.0019) Grad: 4080.1479  LR: 0.000017  \n",
      "Epoch: [2][800/2681] Elapsed 6m 38s (remain 15m 35s) Loss: 0.0000(0.0019) Grad: 52.0801  LR: 0.000016  \n",
      "Epoch: [2][900/2681] Elapsed 7m 28s (remain 14m 45s) Loss: 0.0024(0.0019) Grad: 11565.7773  LR: 0.000016  \n",
      "Epoch: [2][1000/2681] Elapsed 8m 18s (remain 13m 55s) Loss: 0.0013(0.0019) Grad: 3148.8296  LR: 0.000016  \n",
      "Epoch: [2][1100/2681] Elapsed 9m 7s (remain 13m 6s) Loss: 0.0004(0.0019) Grad: 957.1759  LR: 0.000016  \n",
      "Epoch: [2][1200/2681] Elapsed 9m 57s (remain 12m 16s) Loss: 0.0040(0.0019) Grad: 11403.5830  LR: 0.000016  \n",
      "Epoch: [2][1300/2681] Elapsed 10m 47s (remain 11m 26s) Loss: 0.0002(0.0019) Grad: 845.5878  LR: 0.000016  \n",
      "Epoch: [2][1400/2681] Elapsed 11m 37s (remain 10m 37s) Loss: 0.0002(0.0018) Grad: 789.9186  LR: 0.000015  \n",
      "Epoch: [2][1500/2681] Elapsed 12m 27s (remain 9m 47s) Loss: 0.0008(0.0019) Grad: 1353.1456  LR: 0.000015  \n",
      "Epoch: [2][1600/2681] Elapsed 13m 18s (remain 8m 58s) Loss: 0.0015(0.0019) Grad: 5085.8940  LR: 0.000015  \n",
      "Epoch: [2][1700/2681] Elapsed 14m 7s (remain 8m 8s) Loss: 0.0000(0.0018) Grad: 209.7579  LR: 0.000015  \n",
      "Epoch: [2][1800/2681] Elapsed 14m 57s (remain 7m 18s) Loss: 0.0002(0.0018) Grad: 453.4510  LR: 0.000015  \n",
      "Epoch: [2][1900/2681] Elapsed 15m 46s (remain 6m 28s) Loss: 0.0019(0.0018) Grad: 4193.5518  LR: 0.000015  \n",
      "Epoch: [2][2000/2681] Elapsed 16m 36s (remain 5m 38s) Loss: 0.0000(0.0018) Grad: 97.9966  LR: 0.000014  \n",
      "Epoch: [2][2100/2681] Elapsed 17m 26s (remain 4m 48s) Loss: 0.0002(0.0018) Grad: 771.6078  LR: 0.000014  \n",
      "Epoch: [2][2200/2681] Elapsed 18m 16s (remain 3m 59s) Loss: 0.0005(0.0018) Grad: 1291.4585  LR: 0.000014  \n",
      "Epoch: [2][2300/2681] Elapsed 19m 5s (remain 3m 9s) Loss: 0.0002(0.0018) Grad: 513.6141  LR: 0.000014  \n",
      "Epoch: [2][2400/2681] Elapsed 19m 55s (remain 2m 19s) Loss: 0.0030(0.0018) Grad: 3551.2749  LR: 0.000014  \n",
      "Epoch: [2][2500/2681] Elapsed 20m 44s (remain 1m 29s) Loss: 0.0002(0.0018) Grad: 571.8673  LR: 0.000014  \n",
      "Epoch: [2][2600/2681] Elapsed 21m 35s (remain 0m 39s) Loss: 0.0000(0.0017) Grad: 268.7891  LR: 0.000013  \n",
      "Epoch: [2][2680/2681] Elapsed 22m 15s (remain 0m 0s) Loss: 0.0002(0.0018) Grad: 709.7150  LR: 0.000013  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 10m 14s) Loss: 0.0022(0.0022) \n",
      "EVAL: [100/894] Elapsed 0m 27s (remain 3m 37s) Loss: 0.0004(0.0014) \n",
      "EVAL: [200/894] Elapsed 0m 54s (remain 3m 8s) Loss: 0.0029(0.0022) \n",
      "EVAL: [300/894] Elapsed 1m 21s (remain 2m 40s) Loss: 0.0037(0.0024) \n",
      "EVAL: [400/894] Elapsed 1m 48s (remain 2m 13s) Loss: 0.0002(0.0023) \n",
      "EVAL: [500/894] Elapsed 2m 15s (remain 1m 46s) Loss: 0.0016(0.0025) \n",
      "EVAL: [600/894] Elapsed 2m 42s (remain 1m 19s) Loss: 0.0009(0.0025) \n",
      "EVAL: [700/894] Elapsed 3m 9s (remain 0m 52s) Loss: 0.0000(0.0024) \n",
      "EVAL: [800/894] Elapsed 3m 37s (remain 0m 25s) Loss: 0.0003(0.0023) \n",
      "EVAL: [893/894] Elapsed 4m 2s (remain 0m 0s) Loss: 0.0031(0.0022) \n",
      "Epoch 2 - avg_train_loss: 0.0018  avg_val_loss: 0.0022  time: 1584s\n",
      "Epoch 2 - Score: 0.8638\n",
      "Epoch 2 - Save Best Score: 0.8638 Model\n",
      "Epoch: [3][0/2681] Elapsed 0m 0s (remain 42m 0s) Loss: 0.0007(0.0007) Grad: 1898.6036  LR: 0.000013  \n",
      "Epoch: [3][100/2681] Elapsed 0m 51s (remain 21m 44s) Loss: 0.0001(0.0016) Grad: 1137.1738  LR: 0.000013  \n",
      "Epoch: [3][200/2681] Elapsed 1m 40s (remain 20m 45s) Loss: 0.0022(0.0013) Grad: 12118.4355  LR: 0.000013  \n",
      "Epoch: [3][300/2681] Elapsed 2m 30s (remain 19m 50s) Loss: 0.0003(0.0012) Grad: 1662.6639  LR: 0.000013  \n",
      "Epoch: [3][400/2681] Elapsed 3m 20s (remain 18m 59s) Loss: 0.0008(0.0013) Grad: 2852.0461  LR: 0.000013  \n",
      "Epoch: [3][500/2681] Elapsed 4m 10s (remain 18m 10s) Loss: 0.0001(0.0013) Grad: 530.1647  LR: 0.000013  \n",
      "Epoch: [3][600/2681] Elapsed 5m 0s (remain 17m 18s) Loss: 0.0004(0.0013) Grad: 1558.6985  LR: 0.000012  \n",
      "Epoch: [3][700/2681] Elapsed 5m 49s (remain 16m 27s) Loss: 0.0002(0.0014) Grad: 1705.4692  LR: 0.000012  \n",
      "Epoch: [3][800/2681] Elapsed 6m 39s (remain 15m 38s) Loss: 0.0004(0.0014) Grad: 1049.0148  LR: 0.000012  \n",
      "Epoch: [3][900/2681] Elapsed 7m 29s (remain 14m 47s) Loss: 0.0013(0.0014) Grad: 4080.5635  LR: 0.000012  \n",
      "Epoch: [3][1000/2681] Elapsed 8m 19s (remain 13m 57s) Loss: 0.0002(0.0014) Grad: 776.6398  LR: 0.000012  \n",
      "Epoch: [3][1100/2681] Elapsed 9m 9s (remain 13m 8s) Loss: 0.0000(0.0014) Grad: 76.7038  LR: 0.000012  \n",
      "Epoch: [3][1200/2681] Elapsed 9m 59s (remain 12m 19s) Loss: 0.0000(0.0014) Grad: 62.0972  LR: 0.000011  \n",
      "Epoch: [3][1300/2681] Elapsed 10m 49s (remain 11m 28s) Loss: 0.0025(0.0014) Grad: 8423.4590  LR: 0.000011  \n",
      "Epoch: [3][1400/2681] Elapsed 11m 38s (remain 10m 38s) Loss: 0.0000(0.0014) Grad: 94.8603  LR: 0.000011  \n",
      "Epoch: [3][1500/2681] Elapsed 12m 28s (remain 9m 48s) Loss: 0.0123(0.0013) Grad: 13397.5557  LR: 0.000011  \n",
      "Epoch: [3][1600/2681] Elapsed 13m 18s (remain 8m 58s) Loss: 0.0068(0.0013) Grad: 13920.2402  LR: 0.000011  \n",
      "Epoch: [3][1700/2681] Elapsed 14m 7s (remain 8m 8s) Loss: 0.0005(0.0013) Grad: 2203.5725  LR: 0.000011  \n",
      "Epoch: [3][1800/2681] Elapsed 14m 57s (remain 7m 18s) Loss: 0.0057(0.0013) Grad: 55526.9883  LR: 0.000010  \n",
      "Epoch: [3][1900/2681] Elapsed 15m 47s (remain 6m 28s) Loss: 0.0048(0.0013) Grad: 11246.0088  LR: 0.000010  \n",
      "Epoch: [3][2000/2681] Elapsed 16m 36s (remain 5m 38s) Loss: 0.0000(0.0013) Grad: 1409.1060  LR: 0.000010  \n",
      "Epoch: [3][2100/2681] Elapsed 17m 27s (remain 4m 49s) Loss: 0.0039(0.0013) Grad: 9048.2803  LR: 0.000010  \n",
      "Epoch: [3][2200/2681] Elapsed 18m 16s (remain 3m 59s) Loss: 0.0000(0.0013) Grad: 59.1385  LR: 0.000010  \n",
      "Epoch: [3][2300/2681] Elapsed 19m 5s (remain 3m 9s) Loss: 0.0000(0.0013) Grad: 32.3988  LR: 0.000010  \n",
      "Epoch: [3][2400/2681] Elapsed 19m 55s (remain 2m 19s) Loss: 0.0002(0.0013) Grad: 481.8125  LR: 0.000009  \n",
      "Epoch: [3][2500/2681] Elapsed 20m 45s (remain 1m 29s) Loss: 0.0000(0.0013) Grad: 25.9538  LR: 0.000009  \n",
      "Epoch: [3][2600/2681] Elapsed 21m 34s (remain 0m 39s) Loss: 0.0012(0.0013) Grad: 14711.0391  LR: 0.000009  \n",
      "Epoch: [3][2680/2681] Elapsed 22m 13s (remain 0m 0s) Loss: 0.0001(0.0013) Grad: 282.5387  LR: 0.000009  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 9m 52s) Loss: 0.0015(0.0015) \n",
      "EVAL: [100/894] Elapsed 0m 27s (remain 3m 37s) Loss: 0.0003(0.0014) \n",
      "EVAL: [200/894] Elapsed 0m 54s (remain 3m 9s) Loss: 0.0049(0.0025) \n",
      "EVAL: [300/894] Elapsed 1m 21s (remain 2m 41s) Loss: 0.0041(0.0026) \n",
      "EVAL: [400/894] Elapsed 1m 48s (remain 2m 13s) Loss: 0.0003(0.0025) \n",
      "EVAL: [500/894] Elapsed 2m 15s (remain 1m 46s) Loss: 0.0012(0.0027) \n",
      "EVAL: [600/894] Elapsed 2m 42s (remain 1m 19s) Loss: 0.0023(0.0027) \n",
      "EVAL: [700/894] Elapsed 3m 10s (remain 0m 52s) Loss: 0.0000(0.0026) \n",
      "EVAL: [800/894] Elapsed 3m 37s (remain 0m 25s) Loss: 0.0004(0.0025) \n",
      "EVAL: [893/894] Elapsed 4m 2s (remain 0m 0s) Loss: 0.0028(0.0023) \n",
      "Epoch 3 - avg_train_loss: 0.0013  avg_val_loss: 0.0023  time: 1582s\n",
      "Epoch 3 - Score: 0.8760\n",
      "Epoch 3 - Save Best Score: 0.8760 Model\n",
      "Epoch: [4][0/2681] Elapsed 0m 0s (remain 41m 21s) Loss: 0.0001(0.0001) Grad: 2014.0840  LR: 0.000009  \n",
      "Epoch: [4][100/2681] Elapsed 0m 50s (remain 21m 27s) Loss: 0.0009(0.0009) Grad: 3724.3623  LR: 0.000009  \n",
      "Epoch: [4][200/2681] Elapsed 1m 39s (remain 20m 30s) Loss: 0.0000(0.0009) Grad: 97.7965  LR: 0.000009  \n",
      "Epoch: [4][300/2681] Elapsed 2m 28s (remain 19m 37s) Loss: 0.0006(0.0009) Grad: 2658.5220  LR: 0.000008  \n",
      "Epoch: [4][400/2681] Elapsed 3m 19s (remain 18m 51s) Loss: 0.0017(0.0009) Grad: 4845.6094  LR: 0.000008  \n",
      "Epoch: [4][500/2681] Elapsed 4m 8s (remain 18m 0s) Loss: 0.0003(0.0009) Grad: 1774.6146  LR: 0.000008  \n",
      "Epoch: [4][600/2681] Elapsed 4m 57s (remain 17m 9s) Loss: 0.0001(0.0009) Grad: 644.7831  LR: 0.000008  \n",
      "Epoch: [4][700/2681] Elapsed 5m 46s (remain 16m 19s) Loss: 0.0001(0.0009) Grad: 252.4833  LR: 0.000008  \n",
      "Epoch: [4][800/2681] Elapsed 6m 36s (remain 15m 30s) Loss: 0.0000(0.0010) Grad: 21.5591  LR: 0.000008  \n",
      "Epoch: [4][900/2681] Elapsed 7m 26s (remain 14m 42s) Loss: 0.0025(0.0010) Grad: 7555.9053  LR: 0.000007  \n",
      "Epoch: [4][1000/2681] Elapsed 8m 16s (remain 13m 52s) Loss: 0.0001(0.0010) Grad: 244.9273  LR: 0.000007  \n",
      "Epoch: [4][1100/2681] Elapsed 9m 5s (remain 13m 2s) Loss: 0.0014(0.0010) Grad: 4011.1931  LR: 0.000007  \n",
      "Epoch: [4][1200/2681] Elapsed 9m 55s (remain 12m 13s) Loss: 0.0027(0.0010) Grad: 7819.3081  LR: 0.000007  \n",
      "Epoch: [4][1300/2681] Elapsed 10m 45s (remain 11m 24s) Loss: 0.0018(0.0010) Grad: 4889.7446  LR: 0.000007  \n",
      "Epoch: [4][1400/2681] Elapsed 11m 34s (remain 10m 34s) Loss: 0.0000(0.0010) Grad: 114.3671  LR: 0.000007  \n",
      "Epoch: [4][1500/2681] Elapsed 12m 24s (remain 9m 45s) Loss: 0.0001(0.0011) Grad: 518.1199  LR: 0.000006  \n",
      "Epoch: [4][1600/2681] Elapsed 13m 13s (remain 8m 55s) Loss: 0.0000(0.0011) Grad: 114.2172  LR: 0.000006  \n",
      "Epoch: [4][1700/2681] Elapsed 14m 2s (remain 8m 5s) Loss: 0.0006(0.0011) Grad: 2107.8611  LR: 0.000006  \n",
      "Epoch: [4][1800/2681] Elapsed 14m 52s (remain 7m 15s) Loss: 0.0003(0.0011) Grad: 2239.3274  LR: 0.000006  \n",
      "Epoch: [4][1900/2681] Elapsed 15m 42s (remain 6m 26s) Loss: 0.0000(0.0011) Grad: 186.3291  LR: 0.000006  \n",
      "Epoch: [4][2000/2681] Elapsed 16m 31s (remain 5m 37s) Loss: 0.0000(0.0011) Grad: 35.0725  LR: 0.000006  \n",
      "Epoch: [4][2100/2681] Elapsed 17m 20s (remain 4m 47s) Loss: 0.0001(0.0011) Grad: 499.7739  LR: 0.000005  \n",
      "Epoch: [4][2200/2681] Elapsed 18m 10s (remain 3m 57s) Loss: 0.0003(0.0011) Grad: 1556.3024  LR: 0.000005  \n",
      "Epoch: [4][2300/2681] Elapsed 18m 59s (remain 3m 8s) Loss: 0.0002(0.0010) Grad: 1764.8871  LR: 0.000005  \n",
      "Epoch: [4][2400/2681] Elapsed 19m 48s (remain 2m 18s) Loss: 0.0000(0.0010) Grad: 163.2331  LR: 0.000005  \n",
      "Epoch: [4][2500/2681] Elapsed 20m 38s (remain 1m 29s) Loss: 0.0003(0.0010) Grad: 2481.3042  LR: 0.000005  \n",
      "Epoch: [4][2600/2681] Elapsed 21m 27s (remain 0m 39s) Loss: 0.0000(0.0010) Grad: 411.0329  LR: 0.000005  \n",
      "Epoch: [4][2680/2681] Elapsed 22m 7s (remain 0m 0s) Loss: 0.0000(0.0010) Grad: 257.1298  LR: 0.000004  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 9m 5s) Loss: 0.0019(0.0019) \n",
      "EVAL: [100/894] Elapsed 0m 27s (remain 3m 35s) Loss: 0.0003(0.0013) \n",
      "EVAL: [200/894] Elapsed 0m 54s (remain 3m 8s) Loss: 0.0048(0.0026) \n",
      "EVAL: [300/894] Elapsed 1m 21s (remain 2m 40s) Loss: 0.0052(0.0028) \n",
      "EVAL: [400/894] Elapsed 1m 48s (remain 2m 13s) Loss: 0.0002(0.0026) \n",
      "EVAL: [500/894] Elapsed 2m 15s (remain 1m 46s) Loss: 0.0016(0.0028) \n",
      "EVAL: [600/894] Elapsed 2m 42s (remain 1m 19s) Loss: 0.0019(0.0029) \n",
      "EVAL: [700/894] Elapsed 3m 9s (remain 0m 52s) Loss: 0.0000(0.0027) \n",
      "EVAL: [800/894] Elapsed 3m 36s (remain 0m 25s) Loss: 0.0003(0.0026) \n",
      "EVAL: [893/894] Elapsed 4m 2s (remain 0m 0s) Loss: 0.0027(0.0024) \n",
      "Epoch 4 - avg_train_loss: 0.0010  avg_val_loss: 0.0024  time: 1576s\n",
      "Epoch 4 - Score: 0.8779\n",
      "Epoch 4 - Save Best Score: 0.8779 Model\n",
      "Epoch: [5][0/2681] Elapsed 0m 0s (remain 43m 25s) Loss: 0.0002(0.0002) Grad: 1672.9210  LR: 0.000004  \n",
      "Epoch: [5][100/2681] Elapsed 0m 50s (remain 21m 25s) Loss: 0.0000(0.0005) Grad: 207.2846  LR: 0.000004  \n",
      "Epoch: [5][200/2681] Elapsed 1m 39s (remain 20m 30s) Loss: 0.0005(0.0006) Grad: 2206.5476  LR: 0.000004  \n",
      "Epoch: [5][300/2681] Elapsed 2m 29s (remain 19m 43s) Loss: 0.0003(0.0007) Grad: 1759.4581  LR: 0.000004  \n",
      "Epoch: [5][400/2681] Elapsed 3m 19s (remain 18m 53s) Loss: 0.0000(0.0007) Grad: 362.1623  LR: 0.000004  \n",
      "Epoch: [5][500/2681] Elapsed 4m 8s (remain 18m 1s) Loss: 0.0008(0.0007) Grad: 2973.3022  LR: 0.000004  \n",
      "Epoch: [5][600/2681] Elapsed 4m 57s (remain 17m 11s) Loss: 0.0000(0.0007) Grad: 124.2617  LR: 0.000003  \n",
      "Epoch: [5][700/2681] Elapsed 5m 47s (remain 16m 22s) Loss: 0.0012(0.0007) Grad: 4284.5293  LR: 0.000003  \n",
      "Epoch: [5][800/2681] Elapsed 6m 36s (remain 15m 31s) Loss: 0.0008(0.0007) Grad: 3843.9138  LR: 0.000003  \n",
      "Epoch: [5][900/2681] Elapsed 7m 26s (remain 14m 42s) Loss: 0.0002(0.0008) Grad: 817.8232  LR: 0.000003  \n",
      "Epoch: [5][1000/2681] Elapsed 8m 15s (remain 13m 52s) Loss: 0.0000(0.0008) Grad: 214.4752  LR: 0.000003  \n",
      "Epoch: [5][1100/2681] Elapsed 9m 4s (remain 13m 2s) Loss: 0.0000(0.0008) Grad: 29.5698  LR: 0.000003  \n",
      "Epoch: [5][1200/2681] Elapsed 9m 54s (remain 12m 13s) Loss: 0.0002(0.0008) Grad: 907.1979  LR: 0.000002  \n",
      "Epoch: [5][1300/2681] Elapsed 10m 44s (remain 11m 23s) Loss: 0.0003(0.0008) Grad: 1756.8632  LR: 0.000002  \n",
      "Epoch: [5][1400/2681] Elapsed 11m 33s (remain 10m 33s) Loss: 0.0001(0.0008) Grad: 438.8092  LR: 0.000002  \n",
      "Epoch: [5][1500/2681] Elapsed 12m 22s (remain 9m 44s) Loss: 0.0000(0.0008) Grad: 112.7827  LR: 0.000002  \n",
      "Epoch: [5][1600/2681] Elapsed 13m 12s (remain 8m 54s) Loss: 0.0016(0.0008) Grad: 10597.4922  LR: 0.000002  \n",
      "Epoch: [5][1700/2681] Elapsed 14m 2s (remain 8m 5s) Loss: 0.0006(0.0008) Grad: 3349.9460  LR: 0.000002  \n",
      "Epoch: [5][1800/2681] Elapsed 14m 51s (remain 7m 15s) Loss: 0.0016(0.0008) Grad: 9823.9941  LR: 0.000001  \n",
      "Epoch: [5][1900/2681] Elapsed 15m 41s (remain 6m 26s) Loss: 0.0000(0.0008) Grad: 156.5449  LR: 0.000001  \n",
      "Epoch: [5][2000/2681] Elapsed 16m 30s (remain 5m 36s) Loss: 0.0000(0.0008) Grad: 22.1038  LR: 0.000001  \n",
      "Epoch: [5][2100/2681] Elapsed 17m 20s (remain 4m 47s) Loss: 0.0010(0.0008) Grad: 3038.0820  LR: 0.000001  \n",
      "Epoch: [5][2200/2681] Elapsed 18m 9s (remain 3m 57s) Loss: 0.0001(0.0008) Grad: 325.9224  LR: 0.000001  \n",
      "Epoch: [5][2300/2681] Elapsed 18m 59s (remain 3m 8s) Loss: 0.0000(0.0008) Grad: 19.0010  LR: 0.000001  \n",
      "Epoch: [5][2400/2681] Elapsed 19m 49s (remain 2m 18s) Loss: 0.0003(0.0008) Grad: 2395.4207  LR: 0.000000  \n",
      "Epoch: [5][2500/2681] Elapsed 20m 39s (remain 1m 29s) Loss: 0.0023(0.0008) Grad: 6011.1338  LR: 0.000000  \n",
      "Epoch: [5][2600/2681] Elapsed 21m 29s (remain 0m 39s) Loss: 0.0001(0.0008) Grad: 730.5567  LR: 0.000000  \n",
      "Epoch: [5][2680/2681] Elapsed 22m 9s (remain 0m 0s) Loss: 0.0008(0.0008) Grad: 5840.4512  LR: 0.000000  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 9m 1s) Loss: 0.0031(0.0031) \n",
      "EVAL: [100/894] Elapsed 0m 27s (remain 3m 36s) Loss: 0.0005(0.0016) \n",
      "EVAL: [200/894] Elapsed 0m 54s (remain 3m 8s) Loss: 0.0065(0.0031) \n",
      "EVAL: [300/894] Elapsed 1m 21s (remain 2m 40s) Loss: 0.0061(0.0033) \n",
      "EVAL: [400/894] Elapsed 1m 48s (remain 2m 13s) Loss: 0.0002(0.0031) \n",
      "EVAL: [500/894] Elapsed 2m 15s (remain 1m 46s) Loss: 0.0014(0.0033) \n",
      "EVAL: [600/894] Elapsed 2m 43s (remain 1m 19s) Loss: 0.0030(0.0034) \n",
      "EVAL: [700/894] Elapsed 3m 10s (remain 0m 52s) Loss: 0.0000(0.0032) \n",
      "EVAL: [800/894] Elapsed 3m 37s (remain 0m 25s) Loss: 0.0001(0.0030) \n",
      "EVAL: [893/894] Elapsed 4m 1s (remain 0m 0s) Loss: 0.0028(0.0028) \n",
      "Epoch 5 - avg_train_loss: 0.0008  avg_val_loss: 0.0028  time: 1577s\n",
      "Epoch 5 - Score: 0.8795\n",
      "Epoch 5 - Save Best Score: 0.8795 Model\n",
      "========== fold: 2 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2681] Elapsed 0m 0s (remain 35m 23s) Loss: 0.1604(0.1604) Grad: 205237.0156  LR: 0.000000  \n",
      "Epoch: [1][100/2681] Elapsed 0m 50s (remain 21m 30s) Loss: 0.0766(0.1257) Grad: 51700.3281  LR: 0.000002  \n",
      "Epoch: [1][200/2681] Elapsed 1m 40s (remain 20m 34s) Loss: 0.0197(0.0845) Grad: 1167.2450  LR: 0.000003  \n",
      "Epoch: [1][300/2681] Elapsed 2m 29s (remain 19m 39s) Loss: 0.0088(0.0607) Grad: 554.5114  LR: 0.000004  \n",
      "Epoch: [1][400/2681] Elapsed 3m 18s (remain 18m 47s) Loss: 0.0124(0.0487) Grad: 778.9363  LR: 0.000006  \n",
      "Epoch: [1][500/2681] Elapsed 4m 7s (remain 17m 58s) Loss: 0.0089(0.0413) Grad: 753.7253  LR: 0.000007  \n",
      "Epoch: [1][600/2681] Elapsed 4m 57s (remain 17m 8s) Loss: 0.0048(0.0361) Grad: 520.4488  LR: 0.000009  \n",
      "Epoch: [1][700/2681] Elapsed 5m 46s (remain 16m 17s) Loss: 0.0066(0.0318) Grad: 821.1774  LR: 0.000010  \n",
      "Epoch: [1][800/2681] Elapsed 6m 35s (remain 15m 28s) Loss: 0.0014(0.0284) Grad: 252.4519  LR: 0.000012  \n",
      "Epoch: [1][900/2681] Elapsed 7m 24s (remain 14m 37s) Loss: 0.0024(0.0257) Grad: 250.9161  LR: 0.000013  \n",
      "Epoch: [1][1000/2681] Elapsed 8m 13s (remain 13m 47s) Loss: 0.0023(0.0235) Grad: 456.8358  LR: 0.000015  \n",
      "Epoch: [1][1100/2681] Elapsed 9m 2s (remain 12m 58s) Loss: 0.0027(0.0216) Grad: 428.7329  LR: 0.000016  \n",
      "Epoch: [1][1200/2681] Elapsed 9m 51s (remain 12m 8s) Loss: 0.0007(0.0201) Grad: 90.5293  LR: 0.000018  \n",
      "Epoch: [1][1300/2681] Elapsed 10m 40s (remain 11m 19s) Loss: 0.0194(0.0187) Grad: 1363.4364  LR: 0.000019  \n",
      "Epoch: [1][1400/2681] Elapsed 11m 29s (remain 10m 30s) Loss: 0.0008(0.0176) Grad: 104.6019  LR: 0.000020  \n",
      "Epoch: [1][1500/2681] Elapsed 12m 19s (remain 9m 40s) Loss: 0.0008(0.0166) Grad: 88.2562  LR: 0.000020  \n",
      "Epoch: [1][1600/2681] Elapsed 13m 8s (remain 8m 51s) Loss: 0.0004(0.0157) Grad: 57.6814  LR: 0.000020  \n",
      "Epoch: [1][1700/2681] Elapsed 13m 57s (remain 8m 2s) Loss: 0.0009(0.0150) Grad: 129.0284  LR: 0.000019  \n",
      "Epoch: [1][1800/2681] Elapsed 14m 45s (remain 7m 12s) Loss: 0.0009(0.0143) Grad: 174.9528  LR: 0.000019  \n",
      "Epoch: [1][1900/2681] Elapsed 15m 34s (remain 6m 23s) Loss: 0.0004(0.0137) Grad: 280.3564  LR: 0.000019  \n",
      "Epoch: [1][2000/2681] Elapsed 16m 23s (remain 5m 34s) Loss: 0.0129(0.0131) Grad: 2101.2354  LR: 0.000019  \n",
      "Epoch: [1][2100/2681] Elapsed 17m 12s (remain 4m 45s) Loss: 0.0014(0.0125) Grad: 184.6606  LR: 0.000019  \n",
      "Epoch: [1][2200/2681] Elapsed 18m 1s (remain 3m 55s) Loss: 0.0008(0.0121) Grad: 87.8557  LR: 0.000019  \n",
      "Epoch: [1][2300/2681] Elapsed 18m 50s (remain 3m 6s) Loss: 0.0001(0.0116) Grad: 44.3712  LR: 0.000018  \n",
      "Epoch: [1][2400/2681] Elapsed 19m 39s (remain 2m 17s) Loss: 0.0011(0.0113) Grad: 103.8798  LR: 0.000018  \n",
      "Epoch: [1][2500/2681] Elapsed 20m 28s (remain 1m 28s) Loss: 0.0021(0.0109) Grad: 127.9227  LR: 0.000018  \n",
      "Epoch: [1][2600/2681] Elapsed 21m 17s (remain 0m 39s) Loss: 0.0024(0.0105) Grad: 226.6830  LR: 0.000018  \n",
      "Epoch: [1][2680/2681] Elapsed 21m 55s (remain 0m 0s) Loss: 0.0014(0.0103) Grad: 216.2152  LR: 0.000018  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 9m 38s) Loss: 0.0012(0.0012) \n",
      "EVAL: [100/894] Elapsed 0m 27s (remain 3m 39s) Loss: 0.0005(0.0017) \n",
      "EVAL: [200/894] Elapsed 0m 55s (remain 3m 11s) Loss: 0.0003(0.0016) \n",
      "EVAL: [300/894] Elapsed 1m 22s (remain 2m 42s) Loss: 0.0010(0.0019) \n",
      "EVAL: [400/894] Elapsed 1m 50s (remain 2m 15s) Loss: 0.0001(0.0017) \n",
      "EVAL: [500/894] Elapsed 2m 17s (remain 1m 47s) Loss: 0.0068(0.0020) \n",
      "EVAL: [600/894] Elapsed 2m 44s (remain 1m 20s) Loss: 0.0001(0.0022) \n",
      "EVAL: [700/894] Elapsed 3m 11s (remain 0m 52s) Loss: 0.0048(0.0021) \n",
      "EVAL: [800/894] Elapsed 3m 39s (remain 0m 25s) Loss: 0.0004(0.0020) \n",
      "EVAL: [893/894] Elapsed 4m 4s (remain 0m 0s) Loss: 0.0002(0.0019) \n",
      "Epoch 1 - avg_train_loss: 0.0103  avg_val_loss: 0.0019  time: 1567s\n",
      "Epoch 1 - Score: 0.8192\n",
      "Epoch 1 - Save Best Score: 0.8192 Model\n",
      "Epoch: [2][0/2681] Elapsed 0m 0s (remain 39m 33s) Loss: 0.0012(0.0012) Grad: 2798.4897  LR: 0.000018  \n",
      "Epoch: [2][100/2681] Elapsed 0m 51s (remain 21m 46s) Loss: 0.0002(0.0015) Grad: 491.3069  LR: 0.000018  \n",
      "Epoch: [2][200/2681] Elapsed 1m 40s (remain 20m 40s) Loss: 0.0009(0.0017) Grad: 2077.9482  LR: 0.000017  \n",
      "Epoch: [2][300/2681] Elapsed 2m 30s (remain 19m 49s) Loss: 0.0010(0.0015) Grad: 7480.7676  LR: 0.000017  \n",
      "Epoch: [2][400/2681] Elapsed 3m 20s (remain 18m 59s) Loss: 0.0001(0.0016) Grad: 354.1754  LR: 0.000017  \n",
      "Epoch: [2][500/2681] Elapsed 4m 10s (remain 18m 9s) Loss: 0.0008(0.0016) Grad: 3881.8071  LR: 0.000017  \n",
      "Epoch: [2][600/2681] Elapsed 4m 59s (remain 17m 17s) Loss: 0.0000(0.0016) Grad: 93.7619  LR: 0.000017  \n",
      "Epoch: [2][700/2681] Elapsed 5m 49s (remain 16m 28s) Loss: 0.0016(0.0016) Grad: 4480.0889  LR: 0.000017  \n",
      "Epoch: [2][800/2681] Elapsed 6m 40s (remain 15m 39s) Loss: 0.0000(0.0016) Grad: 183.9051  LR: 0.000016  \n",
      "Epoch: [2][900/2681] Elapsed 7m 29s (remain 14m 48s) Loss: 0.0014(0.0016) Grad: 4138.0898  LR: 0.000016  \n",
      "Epoch: [2][1000/2681] Elapsed 8m 19s (remain 13m 58s) Loss: 0.0001(0.0016) Grad: 253.3831  LR: 0.000016  \n",
      "Epoch: [2][1100/2681] Elapsed 9m 9s (remain 13m 9s) Loss: 0.0004(0.0016) Grad: 3022.4397  LR: 0.000016  \n",
      "Epoch: [2][1200/2681] Elapsed 9m 59s (remain 12m 18s) Loss: 0.0008(0.0016) Grad: 4242.0215  LR: 0.000016  \n",
      "Epoch: [2][1300/2681] Elapsed 10m 48s (remain 11m 28s) Loss: 0.0003(0.0016) Grad: 610.3993  LR: 0.000016  \n",
      "Epoch: [2][1400/2681] Elapsed 11m 38s (remain 10m 37s) Loss: 0.0004(0.0016) Grad: 2422.7913  LR: 0.000015  \n",
      "Epoch: [2][1500/2681] Elapsed 12m 28s (remain 9m 48s) Loss: 0.0008(0.0016) Grad: 3294.3186  LR: 0.000015  \n",
      "Epoch: [2][1600/2681] Elapsed 13m 18s (remain 8m 58s) Loss: 0.0013(0.0016) Grad: 3914.7844  LR: 0.000015  \n",
      "Epoch: [2][1700/2681] Elapsed 14m 8s (remain 8m 8s) Loss: 0.0046(0.0016) Grad: 6677.6318  LR: 0.000015  \n",
      "Epoch: [2][1800/2681] Elapsed 14m 58s (remain 7m 19s) Loss: 0.0020(0.0016) Grad: 4848.7510  LR: 0.000015  \n",
      "Epoch: [2][1900/2681] Elapsed 15m 48s (remain 6m 29s) Loss: 0.0000(0.0016) Grad: 211.0842  LR: 0.000015  \n",
      "Epoch: [2][2000/2681] Elapsed 16m 38s (remain 5m 39s) Loss: 0.0001(0.0016) Grad: 204.7245  LR: 0.000014  \n",
      "Epoch: [2][2100/2681] Elapsed 17m 28s (remain 4m 49s) Loss: 0.0061(0.0016) Grad: 9052.1885  LR: 0.000014  \n",
      "Epoch: [2][2200/2681] Elapsed 18m 18s (remain 3m 59s) Loss: 0.0003(0.0016) Grad: 2492.7678  LR: 0.000014  \n",
      "Epoch: [2][2300/2681] Elapsed 19m 8s (remain 3m 9s) Loss: 0.0000(0.0016) Grad: 44.9122  LR: 0.000014  \n",
      "Epoch: [2][2400/2681] Elapsed 19m 58s (remain 2m 19s) Loss: 0.0005(0.0016) Grad: 1115.7267  LR: 0.000014  \n",
      "Epoch: [2][2500/2681] Elapsed 20m 48s (remain 1m 29s) Loss: 0.0021(0.0016) Grad: 3933.4934  LR: 0.000014  \n",
      "Epoch: [2][2600/2681] Elapsed 21m 37s (remain 0m 39s) Loss: 0.0017(0.0016) Grad: 3654.0212  LR: 0.000013  \n",
      "Epoch: [2][2680/2681] Elapsed 22m 17s (remain 0m 0s) Loss: 0.0044(0.0016) Grad: 12462.0078  LR: 0.000013  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 10m 17s) Loss: 0.0006(0.0006) \n",
      "EVAL: [100/894] Elapsed 0m 28s (remain 3m 40s) Loss: 0.0009(0.0018) \n",
      "EVAL: [200/894] Elapsed 0m 55s (remain 3m 10s) Loss: 0.0011(0.0018) \n",
      "EVAL: [300/894] Elapsed 1m 22s (remain 2m 42s) Loss: 0.0001(0.0020) \n",
      "EVAL: [400/894] Elapsed 1m 49s (remain 2m 15s) Loss: 0.0000(0.0018) \n",
      "EVAL: [500/894] Elapsed 2m 17s (remain 1m 47s) Loss: 0.0005(0.0019) \n",
      "EVAL: [600/894] Elapsed 2m 44s (remain 1m 20s) Loss: 0.0000(0.0021) \n",
      "EVAL: [700/894] Elapsed 3m 11s (remain 0m 52s) Loss: 0.0198(0.0021) \n",
      "EVAL: [800/894] Elapsed 3m 39s (remain 0m 25s) Loss: 0.0005(0.0020) \n",
      "EVAL: [893/894] Elapsed 4m 4s (remain 0m 0s) Loss: 0.0000(0.0019) \n",
      "Epoch 2 - avg_train_loss: 0.0016  avg_val_loss: 0.0019  time: 1588s\n",
      "Epoch 2 - Score: 0.8776\n",
      "Epoch 2 - Save Best Score: 0.8776 Model\n",
      "Epoch: [3][0/2681] Elapsed 0m 0s (remain 44m 27s) Loss: 0.0003(0.0003) Grad: 1246.9110  LR: 0.000013  \n",
      "Epoch: [3][100/2681] Elapsed 0m 50s (remain 21m 40s) Loss: 0.0000(0.0012) Grad: 8.6856  LR: 0.000013  \n",
      "Epoch: [3][200/2681] Elapsed 1m 40s (remain 20m 37s) Loss: 0.0002(0.0013) Grad: 1135.6500  LR: 0.000013  \n",
      "Epoch: [3][300/2681] Elapsed 2m 29s (remain 19m 42s) Loss: 0.0002(0.0014) Grad: 2563.7993  LR: 0.000013  \n",
      "Epoch: [3][400/2681] Elapsed 3m 19s (remain 18m 55s) Loss: 0.0026(0.0013) Grad: 8416.2100  LR: 0.000013  \n",
      "Epoch: [3][500/2681] Elapsed 4m 9s (remain 18m 4s) Loss: 0.0023(0.0013) Grad: 6581.4590  LR: 0.000013  \n",
      "Epoch: [3][600/2681] Elapsed 4m 59s (remain 17m 15s) Loss: 0.0004(0.0013) Grad: 2202.6572  LR: 0.000012  \n",
      "Epoch: [3][700/2681] Elapsed 5m 48s (remain 16m 24s) Loss: 0.0016(0.0013) Grad: 10698.1504  LR: 0.000012  \n",
      "Epoch: [3][800/2681] Elapsed 6m 38s (remain 15m 34s) Loss: 0.0001(0.0013) Grad: 387.9394  LR: 0.000012  \n",
      "Epoch: [3][900/2681] Elapsed 7m 28s (remain 14m 45s) Loss: 0.0001(0.0013) Grad: 602.5227  LR: 0.000012  \n",
      "Epoch: [3][1000/2681] Elapsed 8m 17s (remain 13m 54s) Loss: 0.0001(0.0013) Grad: 520.6978  LR: 0.000012  \n",
      "Epoch: [3][1100/2681] Elapsed 9m 6s (remain 13m 4s) Loss: 0.0000(0.0012) Grad: 187.2831  LR: 0.000012  \n",
      "Epoch: [3][1200/2681] Elapsed 9m 56s (remain 12m 15s) Loss: 0.0000(0.0012) Grad: 62.2707  LR: 0.000011  \n",
      "Epoch: [3][1300/2681] Elapsed 10m 46s (remain 11m 26s) Loss: 0.0039(0.0013) Grad: 5515.9937  LR: 0.000011  \n",
      "Epoch: [3][1400/2681] Elapsed 11m 36s (remain 10m 36s) Loss: 0.0000(0.0013) Grad: 93.2428  LR: 0.000011  \n",
      "Epoch: [3][1500/2681] Elapsed 12m 25s (remain 9m 46s) Loss: 0.0209(0.0013) Grad: 36300.2539  LR: 0.000011  \n",
      "Epoch: [3][1600/2681] Elapsed 13m 14s (remain 8m 56s) Loss: 0.0011(0.0013) Grad: 3145.9956  LR: 0.000011  \n",
      "Epoch: [3][1700/2681] Elapsed 14m 4s (remain 8m 6s) Loss: 0.0000(0.0012) Grad: 274.8014  LR: 0.000011  \n",
      "Epoch: [3][1800/2681] Elapsed 14m 54s (remain 7m 17s) Loss: 0.0006(0.0012) Grad: 1579.9958  LR: 0.000010  \n",
      "Epoch: [3][1900/2681] Elapsed 15m 44s (remain 6m 27s) Loss: 0.0001(0.0012) Grad: 808.6292  LR: 0.000010  \n",
      "Epoch: [3][2000/2681] Elapsed 16m 34s (remain 5m 37s) Loss: 0.0008(0.0013) Grad: 3007.5105  LR: 0.000010  \n",
      "Epoch: [3][2100/2681] Elapsed 17m 24s (remain 4m 48s) Loss: 0.0028(0.0012) Grad: 4962.2612  LR: 0.000010  \n",
      "Epoch: [3][2200/2681] Elapsed 18m 14s (remain 3m 58s) Loss: 0.0000(0.0012) Grad: 209.7279  LR: 0.000010  \n",
      "Epoch: [3][2300/2681] Elapsed 19m 3s (remain 3m 8s) Loss: 0.0021(0.0012) Grad: 3483.1423  LR: 0.000010  \n",
      "Epoch: [3][2400/2681] Elapsed 19m 52s (remain 2m 19s) Loss: 0.0001(0.0012) Grad: 487.8800  LR: 0.000009  \n",
      "Epoch: [3][2500/2681] Elapsed 20m 42s (remain 1m 29s) Loss: 0.0023(0.0013) Grad: 6533.1182  LR: 0.000009  \n",
      "Epoch: [3][2600/2681] Elapsed 21m 32s (remain 0m 39s) Loss: 0.0053(0.0013) Grad: 6063.4009  LR: 0.000009  \n",
      "Epoch: [3][2680/2681] Elapsed 22m 11s (remain 0m 0s) Loss: 0.0003(0.0013) Grad: 1427.2103  LR: 0.000009  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 10m 25s) Loss: 0.0003(0.0003) \n",
      "EVAL: [100/894] Elapsed 0m 28s (remain 3m 40s) Loss: 0.0011(0.0020) \n",
      "EVAL: [200/894] Elapsed 0m 55s (remain 3m 10s) Loss: 0.0001(0.0018) \n",
      "EVAL: [300/894] Elapsed 1m 22s (remain 2m 42s) Loss: 0.0001(0.0020) \n",
      "EVAL: [400/894] Elapsed 1m 49s (remain 2m 15s) Loss: 0.0000(0.0018) \n",
      "EVAL: [500/894] Elapsed 2m 17s (remain 1m 47s) Loss: 0.0004(0.0019) \n",
      "EVAL: [600/894] Elapsed 2m 44s (remain 1m 20s) Loss: 0.0000(0.0020) \n",
      "EVAL: [700/894] Elapsed 3m 11s (remain 0m 52s) Loss: 0.0096(0.0021) \n",
      "EVAL: [800/894] Elapsed 3m 39s (remain 0m 25s) Loss: 0.0002(0.0019) \n",
      "EVAL: [893/894] Elapsed 4m 4s (remain 0m 0s) Loss: 0.0000(0.0019) \n",
      "Epoch 3 - avg_train_loss: 0.0013  avg_val_loss: 0.0019  time: 1582s\n",
      "Epoch 3 - Score: 0.8830\n",
      "Epoch 3 - Save Best Score: 0.8830 Model\n",
      "Epoch: [4][0/2681] Elapsed 0m 0s (remain 42m 49s) Loss: 0.0007(0.0007) Grad: 1997.7612  LR: 0.000009  \n",
      "Epoch: [4][100/2681] Elapsed 0m 50s (remain 21m 36s) Loss: 0.0004(0.0006) Grad: 3640.0625  LR: 0.000009  \n",
      "Epoch: [4][200/2681] Elapsed 1m 40s (remain 20m 42s) Loss: 0.0003(0.0008) Grad: 1453.2303  LR: 0.000009  \n",
      "Epoch: [4][300/2681] Elapsed 2m 30s (remain 19m 49s) Loss: 0.0002(0.0008) Grad: 940.8806  LR: 0.000008  \n",
      "Epoch: [4][400/2681] Elapsed 3m 19s (remain 18m 55s) Loss: 0.0001(0.0009) Grad: 361.3163  LR: 0.000008  \n",
      "Epoch: [4][500/2681] Elapsed 4m 10s (remain 18m 7s) Loss: 0.0003(0.0009) Grad: 1347.3130  LR: 0.000008  \n",
      "Epoch: [4][600/2681] Elapsed 4m 59s (remain 17m 17s) Loss: 0.0007(0.0009) Grad: 5070.1514  LR: 0.000008  \n",
      "Epoch: [4][700/2681] Elapsed 5m 49s (remain 16m 26s) Loss: 0.0000(0.0009) Grad: 327.9038  LR: 0.000008  \n",
      "Epoch: [4][800/2681] Elapsed 6m 38s (remain 15m 35s) Loss: 0.0001(0.0008) Grad: 679.3961  LR: 0.000008  \n",
      "Epoch: [4][900/2681] Elapsed 7m 27s (remain 14m 44s) Loss: 0.0111(0.0009) Grad: 20068.7266  LR: 0.000007  \n",
      "Epoch: [4][1000/2681] Elapsed 8m 16s (remain 13m 54s) Loss: 0.0027(0.0009) Grad: 7055.1646  LR: 0.000007  \n",
      "Epoch: [4][1100/2681] Elapsed 9m 6s (remain 13m 4s) Loss: 0.0004(0.0009) Grad: 4132.2896  LR: 0.000007  \n",
      "Epoch: [4][1200/2681] Elapsed 9m 56s (remain 12m 15s) Loss: 0.0004(0.0009) Grad: 1637.5902  LR: 0.000007  \n",
      "Epoch: [4][1300/2681] Elapsed 10m 45s (remain 11m 25s) Loss: 0.0007(0.0009) Grad: 3647.6233  LR: 0.000007  \n",
      "Epoch: [4][1400/2681] Elapsed 11m 35s (remain 10m 35s) Loss: 0.0001(0.0009) Grad: 814.3954  LR: 0.000007  \n",
      "Epoch: [4][1500/2681] Elapsed 12m 25s (remain 9m 45s) Loss: 0.0010(0.0009) Grad: 2593.4514  LR: 0.000006  \n",
      "Epoch: [4][1600/2681] Elapsed 13m 14s (remain 8m 55s) Loss: 0.0014(0.0010) Grad: 5216.1992  LR: 0.000006  \n",
      "Epoch: [4][1700/2681] Elapsed 14m 4s (remain 8m 6s) Loss: 0.0073(0.0010) Grad: 8875.7793  LR: 0.000006  \n",
      "Epoch: [4][1800/2681] Elapsed 14m 53s (remain 7m 16s) Loss: 0.0001(0.0010) Grad: 615.1646  LR: 0.000006  \n",
      "Epoch: [4][1900/2681] Elapsed 15m 43s (remain 6m 27s) Loss: 0.0054(0.0010) Grad: 10444.7305  LR: 0.000006  \n",
      "Epoch: [4][2000/2681] Elapsed 16m 32s (remain 5m 37s) Loss: 0.0010(0.0009) Grad: 3380.0488  LR: 0.000006  \n",
      "Epoch: [4][2100/2681] Elapsed 17m 22s (remain 4m 47s) Loss: 0.0000(0.0009) Grad: 32.2328  LR: 0.000005  \n",
      "Epoch: [4][2200/2681] Elapsed 18m 11s (remain 3m 58s) Loss: 0.0004(0.0009) Grad: 1958.0327  LR: 0.000005  \n",
      "Epoch: [4][2300/2681] Elapsed 19m 0s (remain 3m 8s) Loss: 0.0088(0.0009) Grad: 41111.6562  LR: 0.000005  \n",
      "Epoch: [4][2400/2681] Elapsed 19m 50s (remain 2m 18s) Loss: 0.0001(0.0009) Grad: 885.1005  LR: 0.000005  \n",
      "Epoch: [4][2500/2681] Elapsed 20m 39s (remain 1m 29s) Loss: 0.0000(0.0009) Grad: 10.8550  LR: 0.000005  \n",
      "Epoch: [4][2600/2681] Elapsed 21m 30s (remain 0m 39s) Loss: 0.0001(0.0010) Grad: 849.3778  LR: 0.000005  \n",
      "Epoch: [4][2680/2681] Elapsed 22m 9s (remain 0m 0s) Loss: 0.0001(0.0010) Grad: 583.8136  LR: 0.000004  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 10m 22s) Loss: 0.0002(0.0002) \n",
      "EVAL: [100/894] Elapsed 0m 27s (remain 3m 38s) Loss: 0.0014(0.0024) \n",
      "EVAL: [200/894] Elapsed 0m 55s (remain 3m 10s) Loss: 0.0001(0.0022) \n",
      "EVAL: [300/894] Elapsed 1m 22s (remain 2m 41s) Loss: 0.0000(0.0023) \n",
      "EVAL: [400/894] Elapsed 1m 49s (remain 2m 14s) Loss: 0.0000(0.0021) \n",
      "EVAL: [500/894] Elapsed 2m 16s (remain 1m 47s) Loss: 0.0004(0.0023) \n",
      "EVAL: [600/894] Elapsed 2m 43s (remain 1m 19s) Loss: 0.0000(0.0025) \n",
      "EVAL: [700/894] Elapsed 3m 11s (remain 0m 52s) Loss: 0.0129(0.0025) \n",
      "EVAL: [800/894] Elapsed 3m 38s (remain 0m 25s) Loss: 0.0002(0.0023) \n",
      "EVAL: [893/894] Elapsed 4m 3s (remain 0m 0s) Loss: 0.0000(0.0023) \n",
      "Epoch 4 - avg_train_loss: 0.0010  avg_val_loss: 0.0023  time: 1580s\n",
      "Epoch 4 - Score: 0.8835\n",
      "Epoch 4 - Save Best Score: 0.8835 Model\n",
      "Epoch: [5][0/2681] Elapsed 0m 0s (remain 41m 50s) Loss: 0.0001(0.0001) Grad: 929.3250  LR: 0.000004  \n",
      "Epoch: [5][100/2681] Elapsed 0m 50s (remain 21m 39s) Loss: 0.0000(0.0006) Grad: 366.9589  LR: 0.000004  \n",
      "Epoch: [5][200/2681] Elapsed 1m 40s (remain 20m 37s) Loss: 0.0001(0.0006) Grad: 348.9240  LR: 0.000004  \n",
      "Epoch: [5][300/2681] Elapsed 2m 29s (remain 19m 42s) Loss: 0.0000(0.0005) Grad: 125.2564  LR: 0.000004  \n",
      "Epoch: [5][400/2681] Elapsed 3m 18s (remain 18m 50s) Loss: 0.0000(0.0006) Grad: 78.2270  LR: 0.000004  \n",
      "Epoch: [5][500/2681] Elapsed 4m 7s (remain 17m 58s) Loss: 0.0001(0.0006) Grad: 1104.7412  LR: 0.000004  \n",
      "Epoch: [5][600/2681] Elapsed 4m 57s (remain 17m 11s) Loss: 0.0000(0.0006) Grad: 21.6354  LR: 0.000003  \n",
      "Epoch: [5][700/2681] Elapsed 5m 47s (remain 16m 21s) Loss: 0.0000(0.0007) Grad: 328.1459  LR: 0.000003  \n",
      "Epoch: [5][800/2681] Elapsed 6m 37s (remain 15m 32s) Loss: 0.0000(0.0006) Grad: 93.1783  LR: 0.000003  \n",
      "Epoch: [5][900/2681] Elapsed 7m 27s (remain 14m 44s) Loss: 0.0007(0.0007) Grad: 6381.8398  LR: 0.000003  \n",
      "Epoch: [5][1000/2681] Elapsed 8m 16s (remain 13m 53s) Loss: 0.0008(0.0007) Grad: 5302.7715  LR: 0.000003  \n",
      "Epoch: [5][1100/2681] Elapsed 9m 5s (remain 13m 3s) Loss: 0.0000(0.0007) Grad: 427.3594  LR: 0.000003  \n",
      "Epoch: [5][1200/2681] Elapsed 9m 55s (remain 12m 13s) Loss: 0.0005(0.0007) Grad: 2718.5090  LR: 0.000002  \n",
      "Epoch: [5][1300/2681] Elapsed 10m 44s (remain 11m 23s) Loss: 0.0001(0.0007) Grad: 577.6700  LR: 0.000002  \n",
      "Epoch: [5][1400/2681] Elapsed 11m 34s (remain 10m 34s) Loss: 0.0000(0.0007) Grad: 303.4024  LR: 0.000002  \n",
      "Epoch: [5][1500/2681] Elapsed 12m 24s (remain 9m 44s) Loss: 0.0000(0.0007) Grad: 135.2132  LR: 0.000002  \n",
      "Epoch: [5][1600/2681] Elapsed 13m 13s (remain 8m 54s) Loss: 0.0000(0.0007) Grad: 30.4027  LR: 0.000002  \n",
      "Epoch: [5][1700/2681] Elapsed 14m 2s (remain 8m 5s) Loss: 0.0001(0.0007) Grad: 469.0710  LR: 0.000002  \n",
      "Epoch: [5][1800/2681] Elapsed 14m 52s (remain 7m 15s) Loss: 0.0003(0.0007) Grad: 1905.6304  LR: 0.000001  \n",
      "Epoch: [5][1900/2681] Elapsed 15m 41s (remain 6m 26s) Loss: 0.0001(0.0007) Grad: 475.8736  LR: 0.000001  \n",
      "Epoch: [5][2000/2681] Elapsed 16m 31s (remain 5m 36s) Loss: 0.0002(0.0007) Grad: 1711.2803  LR: 0.000001  \n",
      "Epoch: [5][2100/2681] Elapsed 17m 20s (remain 4m 47s) Loss: 0.0002(0.0007) Grad: 1317.8777  LR: 0.000001  \n",
      "Epoch: [5][2200/2681] Elapsed 18m 10s (remain 3m 57s) Loss: 0.0004(0.0007) Grad: 2599.8308  LR: 0.000001  \n",
      "Epoch: [5][2300/2681] Elapsed 18m 59s (remain 3m 8s) Loss: 0.0029(0.0007) Grad: 14677.2129  LR: 0.000001  \n",
      "Epoch: [5][2400/2681] Elapsed 19m 48s (remain 2m 18s) Loss: 0.0000(0.0007) Grad: 211.9244  LR: 0.000000  \n",
      "Epoch: [5][2500/2681] Elapsed 20m 38s (remain 1m 29s) Loss: 0.0000(0.0007) Grad: 175.4034  LR: 0.000000  \n",
      "Epoch: [5][2600/2681] Elapsed 21m 27s (remain 0m 39s) Loss: 0.0000(0.0007) Grad: 237.9554  LR: 0.000000  \n",
      "Epoch: [5][2680/2681] Elapsed 22m 7s (remain 0m 0s) Loss: 0.0000(0.0007) Grad: 83.4275  LR: 0.000000  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 10m 20s) Loss: 0.0001(0.0001) \n",
      "EVAL: [100/894] Elapsed 0m 27s (remain 3m 39s) Loss: 0.0016(0.0029) \n",
      "EVAL: [200/894] Elapsed 0m 55s (remain 3m 11s) Loss: 0.0001(0.0027) \n",
      "EVAL: [300/894] Elapsed 1m 22s (remain 2m 42s) Loss: 0.0000(0.0028) \n",
      "EVAL: [400/894] Elapsed 1m 50s (remain 2m 15s) Loss: 0.0000(0.0026) \n",
      "EVAL: [500/894] Elapsed 2m 17s (remain 1m 47s) Loss: 0.0003(0.0027) \n",
      "EVAL: [600/894] Elapsed 2m 44s (remain 1m 20s) Loss: 0.0000(0.0030) \n",
      "EVAL: [700/894] Elapsed 3m 11s (remain 0m 52s) Loss: 0.0134(0.0030) \n",
      "EVAL: [800/894] Elapsed 3m 39s (remain 0m 25s) Loss: 0.0002(0.0028) \n",
      "EVAL: [893/894] Elapsed 4m 4s (remain 0m 0s) Loss: 0.0000(0.0027) \n",
      "Epoch 5 - avg_train_loss: 0.0007  avg_val_loss: 0.0027  time: 1578s\n",
      "Epoch 5 - Score: 0.8854\n",
      "Epoch 5 - Save Best Score: 0.8854 Model\n",
      "========== fold: 3 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2681] Elapsed 0m 1s (remain 47m 40s) Loss: 0.1106(0.1106) Grad: 140134.4375  LR: 0.000000  \n",
      "Epoch: [1][100/2681] Elapsed 0m 51s (remain 22m 4s) Loss: 0.0717(0.1123) Grad: 92497.3984  LR: 0.000002  \n",
      "Epoch: [1][200/2681] Elapsed 1m 42s (remain 20m 59s) Loss: 0.0199(0.0812) Grad: 10550.7324  LR: 0.000003  \n",
      "Epoch: [1][300/2681] Elapsed 2m 32s (remain 20m 6s) Loss: 0.0176(0.0586) Grad: 4531.0137  LR: 0.000004  \n",
      "Epoch: [1][400/2681] Elapsed 3m 23s (remain 19m 19s) Loss: 0.0098(0.0472) Grad: 2287.1697  LR: 0.000006  \n",
      "Epoch: [1][500/2681] Elapsed 4m 14s (remain 18m 26s) Loss: 0.0068(0.0401) Grad: 850.6693  LR: 0.000007  \n",
      "Epoch: [1][600/2681] Elapsed 5m 4s (remain 17m 33s) Loss: 0.0057(0.0353) Grad: 1461.0831  LR: 0.000009  \n",
      "Epoch: [1][700/2681] Elapsed 5m 54s (remain 16m 42s) Loss: 0.0038(0.0315) Grad: 1698.5863  LR: 0.000010  \n",
      "Epoch: [1][800/2681] Elapsed 6m 45s (remain 15m 52s) Loss: 0.0082(0.0282) Grad: 4225.2300  LR: 0.000012  \n",
      "Epoch: [1][900/2681] Elapsed 7m 36s (remain 15m 1s) Loss: 0.0086(0.0256) Grad: 6355.4351  LR: 0.000013  \n",
      "Epoch: [1][1000/2681] Elapsed 8m 26s (remain 14m 9s) Loss: 0.0018(0.0235) Grad: 1456.1018  LR: 0.000015  \n",
      "Epoch: [1][1100/2681] Elapsed 9m 15s (remain 13m 17s) Loss: 0.0060(0.0216) Grad: 5577.0122  LR: 0.000016  \n",
      "Epoch: [1][1200/2681] Elapsed 10m 7s (remain 12m 28s) Loss: 0.0016(0.0200) Grad: 1597.4634  LR: 0.000018  \n",
      "Epoch: [1][1300/2681] Elapsed 10m 57s (remain 11m 37s) Loss: 0.0053(0.0187) Grad: 2520.6248  LR: 0.000019  \n",
      "Epoch: [1][1400/2681] Elapsed 11m 47s (remain 10m 46s) Loss: 0.0022(0.0176) Grad: 1402.9688  LR: 0.000020  \n",
      "Epoch: [1][1500/2681] Elapsed 12m 37s (remain 9m 55s) Loss: 0.0008(0.0166) Grad: 624.4532  LR: 0.000020  \n",
      "Epoch: [1][1600/2681] Elapsed 13m 27s (remain 9m 4s) Loss: 0.0011(0.0157) Grad: 877.6620  LR: 0.000020  \n",
      "Epoch: [1][1700/2681] Elapsed 14m 17s (remain 8m 14s) Loss: 0.0009(0.0150) Grad: 721.1688  LR: 0.000019  \n",
      "Epoch: [1][1800/2681] Elapsed 15m 7s (remain 7m 23s) Loss: 0.0027(0.0143) Grad: 1704.9236  LR: 0.000019  \n",
      "Epoch: [1][1900/2681] Elapsed 15m 56s (remain 6m 32s) Loss: 0.0013(0.0137) Grad: 495.2988  LR: 0.000019  \n",
      "Epoch: [1][2000/2681] Elapsed 16m 46s (remain 5m 41s) Loss: 0.0012(0.0131) Grad: 799.9047  LR: 0.000019  \n",
      "Epoch: [1][2100/2681] Elapsed 17m 35s (remain 4m 51s) Loss: 0.0003(0.0126) Grad: 190.9531  LR: 0.000019  \n",
      "Epoch: [1][2200/2681] Elapsed 18m 24s (remain 4m 0s) Loss: 0.0019(0.0121) Grad: 1179.1421  LR: 0.000019  \n",
      "Epoch: [1][2300/2681] Elapsed 19m 14s (remain 3m 10s) Loss: 0.0001(0.0117) Grad: 55.6177  LR: 0.000018  \n",
      "Epoch: [1][2400/2681] Elapsed 20m 5s (remain 2m 20s) Loss: 0.0024(0.0113) Grad: 1025.7432  LR: 0.000018  \n",
      "Epoch: [1][2500/2681] Elapsed 20m 54s (remain 1m 30s) Loss: 0.0012(0.0109) Grad: 444.2452  LR: 0.000018  \n",
      "Epoch: [1][2600/2681] Elapsed 21m 44s (remain 0m 40s) Loss: 0.0028(0.0106) Grad: 1186.0184  LR: 0.000018  \n",
      "Epoch: [1][2680/2681] Elapsed 22m 24s (remain 0m 0s) Loss: 0.0022(0.0104) Grad: 1224.1515  LR: 0.000018  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 11m 15s) Loss: 0.0006(0.0006) \n",
      "EVAL: [100/894] Elapsed 0m 28s (remain 3m 40s) Loss: 0.0002(0.0014) \n",
      "EVAL: [200/894] Elapsed 0m 55s (remain 3m 11s) Loss: 0.0024(0.0016) \n",
      "EVAL: [300/894] Elapsed 1m 23s (remain 2m 43s) Loss: 0.0009(0.0016) \n",
      "EVAL: [400/894] Elapsed 1m 50s (remain 2m 15s) Loss: 0.0011(0.0015) \n",
      "EVAL: [500/894] Elapsed 2m 17s (remain 1m 47s) Loss: 0.0010(0.0016) \n",
      "EVAL: [600/894] Elapsed 2m 44s (remain 1m 20s) Loss: 0.0016(0.0017) \n",
      "EVAL: [700/894] Elapsed 3m 12s (remain 0m 52s) Loss: 0.0012(0.0017) \n",
      "EVAL: [800/894] Elapsed 3m 39s (remain 0m 25s) Loss: 0.0005(0.0016) \n",
      "EVAL: [893/894] Elapsed 4m 3s (remain 0m 0s) Loss: 0.0001(0.0016) \n",
      "Epoch 1 - avg_train_loss: 0.0104  avg_val_loss: 0.0016  time: 1595s\n",
      "Epoch 1 - Score: 0.8647\n",
      "Epoch 1 - Save Best Score: 0.8647 Model\n",
      "Epoch: [2][0/2681] Elapsed 0m 0s (remain 41m 26s) Loss: 0.0008(0.0008) Grad: 5964.5293  LR: 0.000018  \n",
      "Epoch: [2][100/2681] Elapsed 0m 50s (remain 21m 25s) Loss: 0.0055(0.0017) Grad: 4083.9741  LR: 0.000018  \n",
      "Epoch: [2][200/2681] Elapsed 1m 40s (remain 20m 41s) Loss: 0.0001(0.0015) Grad: 471.5251  LR: 0.000017  \n",
      "Epoch: [2][300/2681] Elapsed 2m 30s (remain 19m 46s) Loss: 0.0009(0.0016) Grad: 3139.7163  LR: 0.000017  \n",
      "Epoch: [2][400/2681] Elapsed 3m 19s (remain 18m 54s) Loss: 0.0044(0.0016) Grad: 9661.0498  LR: 0.000017  \n",
      "Epoch: [2][500/2681] Elapsed 4m 9s (remain 18m 5s) Loss: 0.0000(0.0016) Grad: 210.5030  LR: 0.000017  \n",
      "Epoch: [2][600/2681] Elapsed 4m 58s (remain 17m 14s) Loss: 0.0014(0.0018) Grad: 2700.0632  LR: 0.000017  \n",
      "Epoch: [2][700/2681] Elapsed 5m 48s (remain 16m 24s) Loss: 0.0000(0.0017) Grad: 193.4420  LR: 0.000017  \n",
      "Epoch: [2][800/2681] Elapsed 6m 38s (remain 15m 35s) Loss: 0.0007(0.0018) Grad: 1349.4049  LR: 0.000016  \n",
      "Epoch: [2][900/2681] Elapsed 7m 28s (remain 14m 46s) Loss: 0.0002(0.0017) Grad: 512.7794  LR: 0.000016  \n",
      "Epoch: [2][1000/2681] Elapsed 8m 18s (remain 13m 55s) Loss: 0.0000(0.0017) Grad: 48.9450  LR: 0.000016  \n",
      "Epoch: [2][1100/2681] Elapsed 9m 8s (remain 13m 6s) Loss: 0.0001(0.0017) Grad: 472.3138  LR: 0.000016  \n",
      "Epoch: [2][1200/2681] Elapsed 9m 58s (remain 12m 17s) Loss: 0.0037(0.0017) Grad: 9055.0215  LR: 0.000016  \n",
      "Epoch: [2][1300/2681] Elapsed 10m 47s (remain 11m 26s) Loss: 0.0011(0.0017) Grad: 5317.8052  LR: 0.000016  \n",
      "Epoch: [2][1400/2681] Elapsed 11m 36s (remain 10m 36s) Loss: 0.0001(0.0017) Grad: 1056.4900  LR: 0.000015  \n",
      "Epoch: [2][1500/2681] Elapsed 12m 26s (remain 9m 46s) Loss: 0.0027(0.0017) Grad: 9430.0498  LR: 0.000015  \n",
      "Epoch: [2][1600/2681] Elapsed 13m 16s (remain 8m 57s) Loss: 0.0013(0.0017) Grad: 5275.0781  LR: 0.000015  \n",
      "Epoch: [2][1700/2681] Elapsed 14m 6s (remain 8m 7s) Loss: 0.0019(0.0017) Grad: 14895.8506  LR: 0.000015  \n",
      "Epoch: [2][1800/2681] Elapsed 14m 56s (remain 7m 17s) Loss: 0.0004(0.0017) Grad: 1350.4357  LR: 0.000015  \n",
      "Epoch: [2][1900/2681] Elapsed 15m 46s (remain 6m 28s) Loss: 0.0018(0.0017) Grad: 11143.4111  LR: 0.000015  \n",
      "Epoch: [2][2000/2681] Elapsed 16m 36s (remain 5m 38s) Loss: 0.0000(0.0017) Grad: 285.2007  LR: 0.000014  \n",
      "Epoch: [2][2100/2681] Elapsed 17m 26s (remain 4m 48s) Loss: 0.0002(0.0017) Grad: 557.7650  LR: 0.000014  \n",
      "Epoch: [2][2200/2681] Elapsed 18m 15s (remain 3m 58s) Loss: 0.0007(0.0017) Grad: 1980.7708  LR: 0.000014  \n",
      "Epoch: [2][2300/2681] Elapsed 19m 5s (remain 3m 9s) Loss: 0.0008(0.0017) Grad: 2057.9197  LR: 0.000014  \n",
      "Epoch: [2][2400/2681] Elapsed 19m 54s (remain 2m 19s) Loss: 0.0010(0.0017) Grad: 6028.8325  LR: 0.000014  \n",
      "Epoch: [2][2500/2681] Elapsed 20m 44s (remain 1m 29s) Loss: 0.0007(0.0017) Grad: 3831.9873  LR: 0.000014  \n",
      "Epoch: [2][2600/2681] Elapsed 21m 33s (remain 0m 39s) Loss: 0.0038(0.0017) Grad: 7584.6299  LR: 0.000013  \n",
      "Epoch: [2][2680/2681] Elapsed 22m 14s (remain 0m 0s) Loss: 0.0004(0.0017) Grad: 1406.7599  LR: 0.000013  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 10m 56s) Loss: 0.0004(0.0004) \n",
      "EVAL: [100/894] Elapsed 0m 27s (remain 3m 38s) Loss: 0.0000(0.0014) \n",
      "EVAL: [200/894] Elapsed 0m 55s (remain 3m 10s) Loss: 0.0024(0.0016) \n",
      "EVAL: [300/894] Elapsed 1m 23s (remain 2m 43s) Loss: 0.0007(0.0016) \n",
      "EVAL: [400/894] Elapsed 1m 50s (remain 2m 15s) Loss: 0.0013(0.0015) \n",
      "EVAL: [500/894] Elapsed 2m 17s (remain 1m 47s) Loss: 0.0022(0.0018) \n",
      "EVAL: [600/894] Elapsed 2m 44s (remain 1m 20s) Loss: 0.0018(0.0020) \n",
      "EVAL: [700/894] Elapsed 3m 11s (remain 0m 52s) Loss: 0.0057(0.0020) \n",
      "EVAL: [800/894] Elapsed 3m 39s (remain 0m 25s) Loss: 0.0005(0.0019) \n",
      "EVAL: [893/894] Elapsed 4m 4s (remain 0m 0s) Loss: 0.0000(0.0019) \n",
      "Epoch 2 - avg_train_loss: 0.0017  avg_val_loss: 0.0019  time: 1585s\n",
      "Epoch 2 - Score: 0.8793\n",
      "Epoch 2 - Save Best Score: 0.8793 Model\n",
      "Epoch: [3][0/2681] Elapsed 0m 1s (remain 46m 20s) Loss: 0.0005(0.0005) Grad: 1857.3325  LR: 0.000013  \n",
      "Epoch: [3][100/2681] Elapsed 0m 50s (remain 21m 22s) Loss: 0.0000(0.0019) Grad: 67.7060  LR: 0.000013  \n",
      "Epoch: [3][200/2681] Elapsed 1m 39s (remain 20m 26s) Loss: 0.0005(0.0014) Grad: 1633.9960  LR: 0.000013  \n",
      "Epoch: [3][300/2681] Elapsed 2m 29s (remain 19m 41s) Loss: 0.0012(0.0013) Grad: 3315.0317  LR: 0.000013  \n",
      "Epoch: [3][400/2681] Elapsed 3m 19s (remain 18m 52s) Loss: 0.0008(0.0012) Grad: 3063.9089  LR: 0.000013  \n",
      "Epoch: [3][500/2681] Elapsed 4m 8s (remain 18m 1s) Loss: 0.0008(0.0013) Grad: 2342.4585  LR: 0.000013  \n",
      "Epoch: [3][600/2681] Elapsed 4m 58s (remain 17m 11s) Loss: 0.0011(0.0013) Grad: 3477.2090  LR: 0.000012  \n",
      "Epoch: [3][700/2681] Elapsed 5m 48s (remain 16m 24s) Loss: 0.0003(0.0014) Grad: 2285.7974  LR: 0.000012  \n",
      "Epoch: [3][800/2681] Elapsed 6m 38s (remain 15m 34s) Loss: 0.0012(0.0013) Grad: 2378.8730  LR: 0.000012  \n",
      "Epoch: [3][900/2681] Elapsed 7m 28s (remain 14m 45s) Loss: 0.0005(0.0013) Grad: 3070.6990  LR: 0.000012  \n",
      "Epoch: [3][1000/2681] Elapsed 8m 17s (remain 13m 55s) Loss: 0.0025(0.0013) Grad: 6047.2002  LR: 0.000012  \n",
      "Epoch: [3][1100/2681] Elapsed 9m 7s (remain 13m 5s) Loss: 0.0007(0.0013) Grad: 2465.6833  LR: 0.000012  \n",
      "Epoch: [3][1200/2681] Elapsed 9m 56s (remain 12m 14s) Loss: 0.0054(0.0013) Grad: 13627.3271  LR: 0.000011  \n",
      "Epoch: [3][1300/2681] Elapsed 10m 46s (remain 11m 25s) Loss: 0.0002(0.0013) Grad: 734.5518  LR: 0.000011  \n",
      "Epoch: [3][1400/2681] Elapsed 11m 35s (remain 10m 35s) Loss: 0.0004(0.0013) Grad: 565.8364  LR: 0.000011  \n",
      "Epoch: [3][1500/2681] Elapsed 12m 24s (remain 9m 45s) Loss: 0.0004(0.0013) Grad: 2601.4050  LR: 0.000011  \n",
      "Epoch: [3][1600/2681] Elapsed 13m 14s (remain 8m 55s) Loss: 0.0002(0.0013) Grad: 525.2433  LR: 0.000011  \n",
      "Epoch: [3][1700/2681] Elapsed 14m 4s (remain 8m 6s) Loss: 0.0001(0.0012) Grad: 486.2794  LR: 0.000011  \n",
      "Epoch: [3][1800/2681] Elapsed 14m 53s (remain 7m 16s) Loss: 0.0006(0.0012) Grad: 1255.5461  LR: 0.000010  \n",
      "Epoch: [3][1900/2681] Elapsed 15m 42s (remain 6m 26s) Loss: 0.0009(0.0013) Grad: 1776.2113  LR: 0.000010  \n",
      "Epoch: [3][2000/2681] Elapsed 16m 32s (remain 5m 37s) Loss: 0.0001(0.0012) Grad: 510.5346  LR: 0.000010  \n",
      "Epoch: [3][2100/2681] Elapsed 17m 22s (remain 4m 47s) Loss: 0.0013(0.0012) Grad: 4573.6694  LR: 0.000010  \n",
      "Epoch: [3][2200/2681] Elapsed 18m 12s (remain 3m 58s) Loss: 0.0001(0.0012) Grad: 195.0352  LR: 0.000010  \n",
      "Epoch: [3][2300/2681] Elapsed 19m 1s (remain 3m 8s) Loss: 0.0000(0.0012) Grad: 56.5739  LR: 0.000010  \n",
      "Epoch: [3][2400/2681] Elapsed 19m 51s (remain 2m 18s) Loss: 0.0005(0.0013) Grad: 674.1248  LR: 0.000009  \n",
      "Epoch: [3][2500/2681] Elapsed 20m 41s (remain 1m 29s) Loss: 0.0005(0.0013) Grad: 1309.8961  LR: 0.000009  \n",
      "Epoch: [3][2600/2681] Elapsed 21m 30s (remain 0m 39s) Loss: 0.0002(0.0013) Grad: 466.7563  LR: 0.000009  \n",
      "Epoch: [3][2680/2681] Elapsed 22m 10s (remain 0m 0s) Loss: 0.0002(0.0013) Grad: 1071.3513  LR: 0.000009  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 10m 16s) Loss: 0.0002(0.0002) \n",
      "EVAL: [100/894] Elapsed 0m 27s (remain 3m 39s) Loss: 0.0001(0.0016) \n",
      "EVAL: [200/894] Elapsed 0m 55s (remain 3m 10s) Loss: 0.0019(0.0018) \n",
      "EVAL: [300/894] Elapsed 1m 22s (remain 2m 43s) Loss: 0.0008(0.0017) \n",
      "EVAL: [400/894] Elapsed 1m 49s (remain 2m 15s) Loss: 0.0021(0.0016) \n",
      "EVAL: [500/894] Elapsed 2m 16s (remain 1m 47s) Loss: 0.0013(0.0018) \n",
      "EVAL: [600/894] Elapsed 2m 43s (remain 1m 19s) Loss: 0.0011(0.0019) \n",
      "EVAL: [700/894] Elapsed 3m 11s (remain 0m 52s) Loss: 0.0024(0.0019) \n",
      "EVAL: [800/894] Elapsed 3m 38s (remain 0m 25s) Loss: 0.0004(0.0019) \n",
      "EVAL: [893/894] Elapsed 4m 3s (remain 0m 0s) Loss: 0.0000(0.0019) \n",
      "Epoch 3 - avg_train_loss: 0.0013  avg_val_loss: 0.0019  time: 1581s\n",
      "Epoch 3 - Score: 0.8789\n",
      "Epoch: [4][0/2681] Elapsed 0m 0s (remain 39m 43s) Loss: 0.0008(0.0008) Grad: 4044.1438  LR: 0.000009  \n",
      "Epoch: [4][100/2681] Elapsed 0m 50s (remain 21m 32s) Loss: 0.0006(0.0009) Grad: 2110.3416  LR: 0.000009  \n",
      "Epoch: [4][200/2681] Elapsed 1m 39s (remain 20m 31s) Loss: 0.0180(0.0011) Grad: 15426.7373  LR: 0.000009  \n",
      "Epoch: [4][300/2681] Elapsed 2m 29s (remain 19m 40s) Loss: 0.0000(0.0010) Grad: 44.5517  LR: 0.000008  \n",
      "Epoch: [4][400/2681] Elapsed 3m 19s (remain 18m 54s) Loss: 0.0000(0.0009) Grad: 5.3891  LR: 0.000008  \n",
      "Epoch: [4][500/2681] Elapsed 4m 9s (remain 18m 5s) Loss: 0.0001(0.0009) Grad: 330.7734  LR: 0.000008  \n",
      "Epoch: [4][600/2681] Elapsed 4m 58s (remain 17m 14s) Loss: 0.0008(0.0009) Grad: 4766.9424  LR: 0.000008  \n",
      "Epoch: [4][700/2681] Elapsed 5m 48s (remain 16m 23s) Loss: 0.0000(0.0009) Grad: 94.4445  LR: 0.000008  \n",
      "Epoch: [4][800/2681] Elapsed 6m 37s (remain 15m 33s) Loss: 0.0001(0.0009) Grad: 694.2845  LR: 0.000008  \n",
      "Epoch: [4][900/2681] Elapsed 7m 27s (remain 14m 44s) Loss: 0.0001(0.0009) Grad: 604.9224  LR: 0.000007  \n",
      "Epoch: [4][1000/2681] Elapsed 8m 17s (remain 13m 54s) Loss: 0.0001(0.0009) Grad: 520.2887  LR: 0.000007  \n",
      "Epoch: [4][1100/2681] Elapsed 9m 6s (remain 13m 4s) Loss: 0.0023(0.0009) Grad: 5293.2031  LR: 0.000007  \n",
      "Epoch: [4][1200/2681] Elapsed 9m 56s (remain 12m 15s) Loss: 0.0000(0.0009) Grad: 37.0476  LR: 0.000007  \n",
      "Epoch: [4][1300/2681] Elapsed 10m 46s (remain 11m 25s) Loss: 0.0002(0.0009) Grad: 890.8173  LR: 0.000007  \n",
      "Epoch: [4][1400/2681] Elapsed 11m 35s (remain 10m 35s) Loss: 0.0002(0.0009) Grad: 1286.1123  LR: 0.000007  \n",
      "Epoch: [4][1500/2681] Elapsed 12m 25s (remain 9m 46s) Loss: 0.0001(0.0009) Grad: 786.7095  LR: 0.000006  \n",
      "Epoch: [4][1600/2681] Elapsed 13m 14s (remain 8m 56s) Loss: 0.0010(0.0009) Grad: 6948.2456  LR: 0.000006  \n",
      "Epoch: [4][1700/2681] Elapsed 14m 3s (remain 8m 6s) Loss: 0.0000(0.0009) Grad: 199.9092  LR: 0.000006  \n",
      "Epoch: [4][1800/2681] Elapsed 14m 53s (remain 7m 16s) Loss: 0.0007(0.0009) Grad: 3082.9836  LR: 0.000006  \n",
      "Epoch: [4][1900/2681] Elapsed 15m 42s (remain 6m 26s) Loss: 0.0000(0.0009) Grad: 33.0025  LR: 0.000006  \n",
      "Epoch: [4][2000/2681] Elapsed 16m 32s (remain 5m 37s) Loss: 0.0001(0.0009) Grad: 744.9952  LR: 0.000006  \n",
      "Epoch: [4][2100/2681] Elapsed 17m 22s (remain 4m 47s) Loss: 0.0000(0.0009) Grad: 163.0943  LR: 0.000005  \n",
      "Epoch: [4][2200/2681] Elapsed 18m 11s (remain 3m 58s) Loss: 0.0002(0.0009) Grad: 3262.5073  LR: 0.000005  \n",
      "Epoch: [4][2300/2681] Elapsed 19m 1s (remain 3m 8s) Loss: 0.0006(0.0009) Grad: 2426.2454  LR: 0.000005  \n",
      "Epoch: [4][2400/2681] Elapsed 19m 50s (remain 2m 18s) Loss: 0.0000(0.0010) Grad: 406.6941  LR: 0.000005  \n",
      "Epoch: [4][2500/2681] Elapsed 20m 40s (remain 1m 29s) Loss: 0.0005(0.0009) Grad: 1780.5846  LR: 0.000005  \n",
      "Epoch: [4][2600/2681] Elapsed 21m 29s (remain 0m 39s) Loss: 0.0009(0.0009) Grad: 1991.0436  LR: 0.000005  \n",
      "Epoch: [4][2680/2681] Elapsed 22m 9s (remain 0m 0s) Loss: 0.0008(0.0009) Grad: 3674.5786  LR: 0.000004  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 10m 28s) Loss: 0.0002(0.0002) \n",
      "EVAL: [100/894] Elapsed 0m 27s (remain 3m 38s) Loss: 0.0000(0.0015) \n",
      "EVAL: [200/894] Elapsed 0m 55s (remain 3m 10s) Loss: 0.0045(0.0018) \n",
      "EVAL: [300/894] Elapsed 1m 22s (remain 2m 43s) Loss: 0.0013(0.0019) \n",
      "EVAL: [400/894] Elapsed 1m 49s (remain 2m 14s) Loss: 0.0011(0.0018) \n",
      "EVAL: [500/894] Elapsed 2m 16s (remain 1m 47s) Loss: 0.0011(0.0021) \n",
      "EVAL: [600/894] Elapsed 2m 43s (remain 1m 19s) Loss: 0.0021(0.0022) \n",
      "EVAL: [700/894] Elapsed 3m 11s (remain 0m 52s) Loss: 0.0005(0.0022) \n",
      "EVAL: [800/894] Elapsed 3m 38s (remain 0m 25s) Loss: 0.0002(0.0022) \n",
      "EVAL: [893/894] Elapsed 4m 3s (remain 0m 0s) Loss: 0.0000(0.0022) \n",
      "Epoch 4 - avg_train_loss: 0.0009  avg_val_loss: 0.0022  time: 1580s\n",
      "Epoch 4 - Score: 0.8847\n",
      "Epoch 4 - Save Best Score: 0.8847 Model\n",
      "Epoch: [5][0/2681] Elapsed 0m 1s (remain 45m 54s) Loss: 0.0001(0.0001) Grad: 1200.2783  LR: 0.000004  \n",
      "Epoch: [5][100/2681] Elapsed 0m 50s (remain 21m 36s) Loss: 0.0001(0.0006) Grad: 1266.4902  LR: 0.000004  \n",
      "Epoch: [5][200/2681] Elapsed 1m 39s (remain 20m 33s) Loss: 0.0008(0.0006) Grad: 7406.8105  LR: 0.000004  \n",
      "Epoch: [5][300/2681] Elapsed 2m 30s (remain 19m 47s) Loss: 0.0000(0.0006) Grad: 42.9272  LR: 0.000004  \n",
      "Epoch: [5][400/2681] Elapsed 3m 20s (remain 18m 58s) Loss: 0.0008(0.0005) Grad: 2972.9453  LR: 0.000004  \n",
      "Epoch: [5][500/2681] Elapsed 4m 9s (remain 18m 6s) Loss: 0.0000(0.0006) Grad: 122.9916  LR: 0.000004  \n",
      "Epoch: [5][600/2681] Elapsed 4m 58s (remain 17m 14s) Loss: 0.0001(0.0007) Grad: 616.0900  LR: 0.000003  \n",
      "Epoch: [5][700/2681] Elapsed 5m 48s (remain 16m 24s) Loss: 0.0000(0.0007) Grad: 203.6602  LR: 0.000003  \n",
      "Epoch: [5][800/2681] Elapsed 6m 38s (remain 15m 34s) Loss: 0.0000(0.0007) Grad: 138.8038  LR: 0.000003  \n",
      "Epoch: [5][900/2681] Elapsed 7m 27s (remain 14m 44s) Loss: 0.0001(0.0007) Grad: 1399.8071  LR: 0.000003  \n",
      "Epoch: [5][1000/2681] Elapsed 8m 16s (remain 13m 54s) Loss: 0.0017(0.0007) Grad: 4564.0732  LR: 0.000003  \n",
      "Epoch: [5][1100/2681] Elapsed 9m 6s (remain 13m 4s) Loss: 0.0000(0.0007) Grad: 46.4451  LR: 0.000003  \n",
      "Epoch: [5][1200/2681] Elapsed 9m 56s (remain 12m 14s) Loss: 0.0000(0.0007) Grad: 100.7081  LR: 0.000002  \n",
      "Epoch: [5][1300/2681] Elapsed 10m 45s (remain 11m 24s) Loss: 0.0005(0.0007) Grad: 2010.2524  LR: 0.000002  \n",
      "Epoch: [5][1400/2681] Elapsed 11m 35s (remain 10m 35s) Loss: 0.0002(0.0007) Grad: 2294.8689  LR: 0.000002  \n",
      "Epoch: [5][1500/2681] Elapsed 12m 25s (remain 9m 45s) Loss: 0.0000(0.0007) Grad: 445.8643  LR: 0.000002  \n",
      "Epoch: [5][1600/2681] Elapsed 13m 14s (remain 8m 55s) Loss: 0.0009(0.0007) Grad: 3569.2644  LR: 0.000002  \n",
      "Epoch: [5][1700/2681] Elapsed 14m 3s (remain 8m 6s) Loss: 0.0000(0.0007) Grad: 77.9882  LR: 0.000002  \n",
      "Epoch: [5][1800/2681] Elapsed 14m 53s (remain 7m 16s) Loss: 0.0000(0.0007) Grad: 72.4402  LR: 0.000001  \n",
      "Epoch: [5][1900/2681] Elapsed 15m 42s (remain 6m 26s) Loss: 0.0017(0.0007) Grad: 7994.2925  LR: 0.000001  \n",
      "Epoch: [5][2000/2681] Elapsed 16m 32s (remain 5m 37s) Loss: 0.0000(0.0007) Grad: 169.8469  LR: 0.000001  \n",
      "Epoch: [5][2100/2681] Elapsed 17m 21s (remain 4m 47s) Loss: 0.0009(0.0007) Grad: 7897.6768  LR: 0.000001  \n",
      "Epoch: [5][2200/2681] Elapsed 18m 11s (remain 3m 58s) Loss: 0.0003(0.0007) Grad: 1007.1484  LR: 0.000001  \n",
      "Epoch: [5][2300/2681] Elapsed 19m 1s (remain 3m 8s) Loss: 0.0000(0.0007) Grad: 218.1989  LR: 0.000001  \n",
      "Epoch: [5][2400/2681] Elapsed 19m 51s (remain 2m 18s) Loss: 0.0000(0.0008) Grad: 7.7219  LR: 0.000000  \n",
      "Epoch: [5][2500/2681] Elapsed 20m 41s (remain 1m 29s) Loss: 0.0004(0.0008) Grad: 2614.2297  LR: 0.000000  \n",
      "Epoch: [5][2600/2681] Elapsed 21m 30s (remain 0m 39s) Loss: 0.0027(0.0008) Grad: 7191.3970  LR: 0.000000  \n",
      "Epoch: [5][2680/2681] Elapsed 22m 9s (remain 0m 0s) Loss: 0.0011(0.0008) Grad: 5746.3086  LR: 0.000000  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 11m 14s) Loss: 0.0001(0.0001) \n",
      "EVAL: [100/894] Elapsed 0m 27s (remain 3m 39s) Loss: 0.0000(0.0018) \n",
      "EVAL: [200/894] Elapsed 0m 55s (remain 3m 10s) Loss: 0.0046(0.0021) \n",
      "EVAL: [300/894] Elapsed 1m 23s (remain 2m 43s) Loss: 0.0013(0.0022) \n",
      "EVAL: [400/894] Elapsed 1m 50s (remain 2m 15s) Loss: 0.0014(0.0020) \n",
      "EVAL: [500/894] Elapsed 2m 17s (remain 1m 47s) Loss: 0.0022(0.0023) \n",
      "EVAL: [600/894] Elapsed 2m 44s (remain 1m 20s) Loss: 0.0021(0.0025) \n",
      "EVAL: [700/894] Elapsed 3m 12s (remain 0m 52s) Loss: 0.0026(0.0025) \n",
      "EVAL: [800/894] Elapsed 3m 39s (remain 0m 25s) Loss: 0.0002(0.0025) \n",
      "EVAL: [893/894] Elapsed 4m 4s (remain 0m 0s) Loss: 0.0000(0.0024) \n",
      "Epoch 5 - avg_train_loss: 0.0008  avg_val_loss: 0.0024  time: 1581s\n",
      "Epoch 5 - Score: 0.8845\n",
      "Best thres: 0.5, Score: 0.8831\n",
      "Best thres: 0.5015625, Score: 0.8832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffab40467454d23aa051a1057d609b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b68d406f3794625b5b4b51af97f7ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x7f6b324479e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 132, in __del__\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66f280447ff46ca88856498af075e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d7b9472c1f45c4b932e02df6b533d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "name": "nbme-exp043.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 641.572862,
   "end_time": "2022-02-27T11:39:50.972497",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-27T11:29:09.399635",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0b503e832e57492291cbf6e9ae66e343": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0c70395dca6341349fc883dc6b95090a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a92171cb3d34fc8b1ed5119e32951ac",
      "max": 42146,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a202501b76a748fe80180604dff32c7b",
      "value": 42146
     }
    },
    "0dfe4d3aa0354d95bb5d019420b510a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3d9ac191d9b4772ae4bca323a34ddd7",
      "placeholder": "​",
      "style": "IPY_MODEL_9deae6afbd4740df8eb0289bc5f53ae7",
      "value": " 42146/42146 [00:35&lt;00:00, 2027.81it/s]"
     }
    },
    "14efac00edd349898e9fa95a63a2773d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f6c8df95c7845818253189f2e365ba9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33e4f48f7c8f40f48081c34bc992f215": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40ae5d1c9e9f4feaa4207599ef17a1ec",
      "placeholder": "​",
      "style": "IPY_MODEL_8cd3352e5e9342e4a814e9958ea6dc1d",
      "value": "100%"
     }
    },
    "40ae5d1c9e9f4feaa4207599ef17a1ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "47e8d82f628f4bd7b8b0ef0aa96852a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cc9a4ba21d664dafb8ae32a4e0a1c5e0",
       "IPY_MODEL_f4421d3b3a844dbcb003f11902ee1898",
       "IPY_MODEL_d02f186539954463873bb560b775894e"
      ],
      "layout": "IPY_MODEL_14efac00edd349898e9fa95a63a2773d"
     }
    },
    "5a92171cb3d34fc8b1ed5119e32951ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e29db6be6284caa978ee223047f23c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "625abc68d2fb4fd4b8556c7cc1ae514a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83d1b90076dc431893e0ab1c87e35f9c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c7eb508782d4253af8cbff0a39e5d19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_33e4f48f7c8f40f48081c34bc992f215",
       "IPY_MODEL_0c70395dca6341349fc883dc6b95090a",
       "IPY_MODEL_0dfe4d3aa0354d95bb5d019420b510a9"
      ],
      "layout": "IPY_MODEL_e6775e6fc2ad4b14b473b8a07e27426d"
     }
    },
    "8cd3352e5e9342e4a814e9958ea6dc1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9deae6afbd4740df8eb0289bc5f53ae7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a202501b76a748fe80180604dff32c7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c3d9ac191d9b4772ae4bca323a34ddd7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc9a4ba21d664dafb8ae32a4e0a1c5e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83d1b90076dc431893e0ab1c87e35f9c",
      "placeholder": "​",
      "style": "IPY_MODEL_0b503e832e57492291cbf6e9ae66e343",
      "value": "100%"
     }
    },
    "d02f186539954463873bb560b775894e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f6c8df95c7845818253189f2e365ba9",
      "placeholder": "​",
      "style": "IPY_MODEL_5e29db6be6284caa978ee223047f23c5",
      "value": " 143/143 [00:00&lt;00:00, 2361.05it/s]"
     }
    },
    "d2581946e9fd4f9a8dc56b3453cc70bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e6775e6fc2ad4b14b473b8a07e27426d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4421d3b3a844dbcb003f11902ee1898": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_625abc68d2fb4fd4b8556c7cc1ae514a",
      "max": 143,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d2581946e9fd4f9a8dc56b3453cc70bd",
      "value": 143
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
