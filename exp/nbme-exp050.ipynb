{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "killing-guarantee",
   "metadata": {
    "id": "brave-teach"
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-buffalo",
   "metadata": {
    "id": "orange-toilet"
   },
   "source": [
    "- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-committee",
   "metadata": {
    "id": "serious-sending"
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fitting-excitement",
   "metadata": {
    "id": "august-providence"
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"nbme-exp050\"\n",
    "ENV = \"local\"\n",
    "DEBUG_MODE = False\n",
    "SUBMISSION_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "noble-paradise",
   "metadata": {
    "id": "cathedral-horror"
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    env=ENV\n",
    "    exp_name=EXP_NAME\n",
    "    debug=DEBUG_MODE\n",
    "    submission=SUBMISSION_MODE\n",
    "    apex=True\n",
    "    input_dir=None\n",
    "    output_dir=None\n",
    "    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n",
    "    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n",
    "    competition_name=\"nbme-score-clinical-patient-notes\"\n",
    "    id_col=\"id\"\n",
    "    target_col=\"location\"\n",
    "    pretrained_model_name=\"microsoft/deberta-large\"\n",
    "    tokenizer=None\n",
    "    max_len=None\n",
    "    output_dim=1\n",
    "    dropout=0.2\n",
    "    num_workers=4\n",
    "    batch_size=4\n",
    "    lr=2e-5\n",
    "    betas=(0.9, 0.98)\n",
    "    weight_decay=0.1\n",
    "    alpha=1\n",
    "    gamma=2\n",
    "    smoothing=0.0001\n",
    "    num_warmup_steps_rate=0.1\n",
    "    batch_scheduler=True\n",
    "    epochs=5\n",
    "    n_fold=4\n",
    "    train_fold=[0, 1, 2, 3]\n",
    "    seed=71\n",
    "    gradient_accumulation_steps=2\n",
    "    max_grad_norm=1000\n",
    "    print_freq=100\n",
    "    train=True\n",
    "    inference=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bored-ideal",
   "metadata": {
    "id": "armed-norfolk"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.train_fold = [0, 1]\n",
    "\n",
    "if CFG.submission:\n",
    "    CFG.train = False\n",
    "    CFG.inference = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-portuguese",
   "metadata": {
    "id": "atlantic-warrant"
   },
   "source": [
    "## Directory Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "universal-asthma",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "federal-marsh",
    "outputId": "9a2743aa-8fbf-417a-8d92-5579f5ae114d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "print(CFG.env)\n",
    "if CFG.env == \"colab\":\n",
    "    # colab環境\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "    # install packages\n",
    "    !pip install transformers==4.16.2\n",
    "\n",
    "elif CFG.env == \"local\":\n",
    "    # ローカルサーバ\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "\n",
    "elif CFG.env == \"kaggle\":\n",
    "    # kaggle環境\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "progressive-latter",
   "metadata": {
    "id": "recent-harrison"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import BartModel,BertModel,BertTokenizer\n",
    "from transformers import DebertaModel,DebertaTokenizer\n",
    "from transformers import RobertaModel,RobertaTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-entry",
   "metadata": {
    "id": "technical-story"
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "infrared-australian",
   "metadata": {
    "id": "understanding-trial"
   },
   "outputs": [],
   "source": [
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bottom-portsmouth",
   "metadata": {
    "id": "pursuant-lover"
   },
   "outputs": [],
   "source": [
    "def create_labels_for_scoring(df):\n",
    "    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n",
    "    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, \"location\"]\n",
    "        if lst:\n",
    "            new_lst = \";\".join(lst)\n",
    "            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n",
    "\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df[\"location_for_create_labels\"].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, token_probs, tokenizer):\n",
    "    res = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n",
    "        encoded = tokenizer(\n",
    "            text=text,\n",
    "            max_length=CFG.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n",
    "            start, end = offset_mapping\n",
    "            res[i][start:end] = pred\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_predicted_location_str(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def scoring(df, th=0.5):\n",
    "    labels = create_labels_for_scoring(df)\n",
    "\n",
    "    token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n",
    "    char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n",
    "    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n",
    "    preds = get_predictions(predicted_location_str)\n",
    "\n",
    "    score = get_score(labels, preds)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_best_thres(oof_df):\n",
    "    def f1_opt(x):\n",
    "        return -1 * scoring(oof_df, th=x)\n",
    "\n",
    "    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n",
    "    return best_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "charitable-identity",
   "metadata": {
    "id": "matched-hollow"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "supposed-accommodation",
   "metadata": {
    "id": "weighted-screw"
   },
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-episode",
   "metadata": {
    "id": "following-passport"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "steady-discrimination",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "absent-performance",
    "outputId": "ecedcf0f-da9b-4763-8b75-4193ec7dcee7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 6), (143, 3), (42146, 3), (5, 4))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(CFG.input_dir / \"train.csv\")\n",
    "features = pd.read_csv(CFG.input_dir / \"features.csv\")\n",
    "patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n",
    "test = pd.read_csv(CFG.input_dir / \"test.csv\")\n",
    "\n",
    "train.shape, features.shape, patient_notes.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "agreed-violation",
   "metadata": {
    "id": "automated-proportion"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n",
    "    print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-algorithm",
   "metadata": {
    "id": "preceding-january"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "taken-armenia",
   "metadata": {
    "id": "monetary-camera"
   },
   "outputs": [],
   "source": [
    "def preprocess_features(features):\n",
    "    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "\n",
    "\n",
    "features = preprocess_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dress-charge",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fitted-current",
    "outputId": "61ace169-5959-46e0-bb05-709190133698"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 8), (5, 6))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "simple-vegetable",
   "metadata": {
    "id": "australian-vehicle"
   },
   "outputs": [],
   "source": [
    "train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n",
    "train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "desperate-apollo",
   "metadata": {
    "id": "devoted-peter"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4399\n",
       "1    8181\n",
       "2    1296\n",
       "3     287\n",
       "4      99\n",
       "5      27\n",
       "6       9\n",
       "7       1\n",
       "8       1\n",
       "Name: annotation_length, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n",
    "display(train['annotation_length'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-earth",
   "metadata": {
    "id": "incorrect-honey"
   },
   "source": [
    "## CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "behavioral-moscow",
   "metadata": {
    "id": "adjacent-antibody"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    3575\n",
       "1    3575\n",
       "2    3575\n",
       "3    3575\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Fold = GroupKFold(n_splits=CFG.n_fold)\n",
    "groups = train['pn_num'].values\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-reply",
   "metadata": {
    "id": "breathing-state"
   },
   "source": [
    "## Setup tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fewer-haiti",
   "metadata": {
    "id": "former-beast"
   },
   "outputs": [],
   "source": [
    "if CFG.submission:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG.pretrained_model_name)\n",
    "    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n",
    "\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "conscious-crime",
   "metadata": {
    "id": "TDMSkwTNgOOh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'', 0, 0\n",
      "'dad', 0, 3\n",
      "' with', 3, 8\n",
      "' recent', 8, 15\n",
      "' heart', 15, 21\n",
      "' attack', 21, 28\n",
      "'', 0, 0\n",
      "ans\n",
      "\n",
      "'', 0, 0\n",
      "'dad', 0, 3\n",
      "' with', 3, 8\n",
      "' recent', 8, 15\n",
      "' heart', 15, 21\n",
      "' attack', 21, 28\n",
      "'', 0, 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp = 'dad with recent heart attack'\n",
    "encode = tokenizer(tmp, return_offsets_mapping=True)\n",
    "for (start,end) in encode['offset_mapping']:\n",
    "    print(f\"'{tmp[start:end]}', {start}, {end}\")\n",
    "\n",
    "print(\"ans\")\n",
    "print(\"\"\"\n",
    "'', 0, 0\n",
    "'dad', 0, 3\n",
    "' with', 3, 8\n",
    "' recent', 8, 15\n",
    "' heart', 15, 21\n",
    "' attack', 21, 28\n",
    "'', 0, 0\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-championship",
   "metadata": {
    "id": "employed-foster"
   },
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "requested-jacksonville",
   "metadata": {
    "id": "biblical-mailing"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4a8caded8b4886808e53b80125ec3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 433\n"
     ]
    }
   ],
   "source": [
    "pn_history_lengths = []\n",
    "tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    pn_history_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(pn_history_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "consolidated-upper",
   "metadata": {
    "id": "renewable-mercury"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b859657be24e3caade9d456263b18a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 30\n"
     ]
    }
   ],
   "source": [
    "feature_text_lengths = []\n",
    "tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    feature_text_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(feature_text_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ceramic-single",
   "metadata": {
    "id": "latin-burlington"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 466\n"
     ]
    }
   ],
   "source": [
    "CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n",
    "\n",
    "print(\"max length:\", CFG.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "moving-fireplace",
   "metadata": {
    "id": "minor-stock"
   },
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "        self.annotation_lengths = self.df[\"annotation_length\"].values\n",
    "        self.locations = self.df[\"location\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def _create_label(self, pn_history, annotation_length, location_list):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        offset_mapping = encoded[\"offset_mapping\"]\n",
    "        ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n",
    "        label = np.zeros(len(offset_mapping))\n",
    "        label[ignore_idxes] = -1\n",
    "\n",
    "        if annotation_length > 0:\n",
    "            for location in location_list:\n",
    "                for loc in [s.split() for s in location.split(\";\")]:\n",
    "                    start, end = int(loc[0]), int(loc[1])\n",
    "                    start_idx = -1\n",
    "                    end_idx = -1\n",
    "                    for idx in range(len(offset_mapping)):\n",
    "                        if (start_idx == -1) & (start < offset_mapping[idx][0]):\n",
    "                            start_idx = idx - 1\n",
    "                        if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n",
    "                            end_idx = idx + 1\n",
    "                    if start_idx == -1:\n",
    "                        start_idx = end_idx\n",
    "                    if (start_idx != -1) & (end_idx != -1):\n",
    "                        label[start_idx:end_idx] = 1\n",
    "\n",
    "        return torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n",
    "        return input_, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bound-bundle",
   "metadata": {
    "id": "decimal-schema"
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        return input_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-visibility",
   "metadata": {
    "id": "exceptional-vertical"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "tracked-cambridge",
   "metadata": {
    "id": "dynamic-fifteen"
   },
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, model_config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if model_config_path is None:\n",
    "            self.model_config = AutoConfig.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "        else:\n",
    "            self.model_config = torch.load(model_config_path)\n",
    "\n",
    "        if pretrained:\n",
    "            self.backbone = AutoModel.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                config=self.model_config,\n",
    "            )\n",
    "            print(f\"Load weight from pretrained\")\n",
    "        else:\n",
    "            #self.backbone = AutoModel.from_config(self.model_config)\n",
    "            itpt = AutoModelForMaskedLM.from_config(self.model_config)\n",
    "            #path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp010/checkpoint-130170/pytorch_model.bin\")\n",
    "            path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n",
    "            state_dict = torch.load(path)\n",
    "            itpt.load_state_dict(state_dict)\n",
    "            self.backbone = itpt.deberta\n",
    "            print(f\"Load weight from {path}\")\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(self.cfg.dropout),\n",
    "            nn.Linear(self.model_config.hidden_size * 2, self.cfg.output_dim),\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.model_config.hidden_size,\n",
    "            self.model_config.hidden_size,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h = self.backbone(**inputs)[\"last_hidden_state\"]   # [batch, seq_len, d_model]\n",
    "        h, _ = self.lstm(h, None)\n",
    "        output = self.fc(h)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-philip",
   "metadata": {
    "id": "driving-commercial"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fixed-summit",
   "metadata": {
    "id": "86D0afuEh7Q0"
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, reduction='none', alpha=1, gamma=2):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        loss = self.alpha * (1. - pt)**self.gamma * bce_loss\n",
    "        if self.reduction == 'none':\n",
    "            loss = loss\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class SmoothFocalLoss(nn.Module):\n",
    "    def __init__(self, reduction='none', alpha=1, gamma=2, smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "        self.focal_loss = FocalLoss(reduction='none', alpha=alpha, gamma=gamma)\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothFocalLoss._smooth(targets, self.smoothing)\n",
    "        loss = self.focal_loss(inputs, targets)\n",
    "        if self.reduction == 'none':\n",
    "            loss = loss\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "    \n",
    "class CEFocalLoss(nn.Module):\n",
    "    def __init__(self, reduction='none', alpha=1, gamma=2):\n",
    "        super(CEFocalLoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        loss = self.alpha * (1. - pt)**self.gamma * ce_loss\n",
    "        if self.reduction == 'none':\n",
    "            loss = loss\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "    \n",
    "class SmoothCEFocalLoss(nn.Module):\n",
    "    def __init__(self, reduction='none', alpha=1, gamma=2, smoothing=0.0):\n",
    "        super(SmoothCEFocalLoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', label_smoothing=self.smoothing) # torch >= 1.10.0\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        loss = self.alpha * (1. - pt)**self.gamma * ce_loss\n",
    "        if self.reduction == 'none':\n",
    "            loss = loss\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bizarre-belly",
   "metadata": {
    "id": "cathedral-component"
   },
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    train_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    epoch,\n",
    "    scheduler,\n",
    "    device,\n",
    "):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels) in enumerate(train_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            output = model(inputs)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if CFG.batch_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n",
    "            print(\n",
    "                \"Epoch: [{0}][{1}/{2}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                \"Grad: {grad_norm:.4f}  \"\n",
    "                \"LR: {lr:.6f}  \"\n",
    "                .format(\n",
    "                    epoch+1,\n",
    "                    step,\n",
    "                    len(train_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n",
    "                    loss=losses,\n",
    "                     grad_norm=grad_norm,\n",
    "                     lr=scheduler.get_lr()[0],\n",
    "                )\n",
    "            )\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "developed-delivery",
   "metadata": {
    "id": "expired-wilson"
   },
   "outputs": [],
   "source": [
    "def valid_fn(\n",
    "    val_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    device,\n",
    "):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels) in enumerate(val_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n",
    "            print(\n",
    "                \"EVAL: [{0}/{1}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                .format(\n",
    "                    step, len(val_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n",
    "                    loss=losses,\n",
    "                )\n",
    "            )\n",
    "    preds = np.concatenate(preds)\n",
    "    return losses.avg, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "amber-helmet",
   "metadata": {
    "id": "chinese-sympathy"
   },
   "outputs": [],
   "source": [
    "def inference_fn(test_dataloader, model, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preds = []\n",
    "    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "native-description",
   "metadata": {
    "id": "healthy-sleep"
   },
   "outputs": [],
   "source": [
    "def train_loop(df, i_fold, device):\n",
    "    print(f\"========== fold: {i_fold} training ==========\")\n",
    "    train_idx = df[df[\"fold\"] != i_fold].index\n",
    "    val_idx = df[df[\"fold\"] == i_fold].index\n",
    "\n",
    "    train_folds = df.loc[train_idx].reset_index(drop=True)\n",
    "    val_folds = df.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = TrainingDataset(CFG, train_folds)\n",
    "    val_dataset = TrainingDataset(CFG, val_folds)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "    model = CustomModel(CFG, model_config_path=None, pretrained=False)   # itptを使うため\n",
    "    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n",
    "    model.to(device)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in param_optimizer if not any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n",
    "        {\"params\": [p for n, p in param_optimizer if any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=CFG.lr,\n",
    "        betas=CFG.betas,\n",
    "        weight_decay=CFG.weight_decay,\n",
    "    )\n",
    "    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n",
    "    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_train_optimization_steps,\n",
    "    )\n",
    "\n",
    "    criterion = SmoothFocalLoss(reduction='none', alpha=CFG.alpha, gamma=CFG.gamma, smoothing=CFG.smoothing)\n",
    "    #criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    best_score = -1 * np.inf\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "        avg_loss = train_fn(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            scheduler,\n",
    "            device,\n",
    "        )\n",
    "        avg_val_loss, val_preds = valid_fn(\n",
    "            val_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "\n",
    "        # scoring\n",
    "        val_folds[[str(i) for i in range(CFG.max_len)]] = val_preds\n",
    "        score = scoring(val_folds, th=0.5)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n",
    "        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"predictions\": val_preds,\n",
    "                },\n",
    "                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "            )\n",
    "\n",
    "    predictions = torch.load(\n",
    "        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )[\"predictions\"]\n",
    "    val_folds[[str(i) for i in range(CFG.max_len)]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return val_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-superintendent",
   "metadata": {
    "id": "balanced-novel"
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "imperial-florence",
   "metadata": {
    "id": "sound-silly"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for i_fold in range(CFG.n_fold):\n",
    "            if i_fold in CFG.train_fold:\n",
    "                _oof_df = train_loop(train, i_fold, device)\n",
    "                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n",
    "        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    if CFG.submission:\n",
    "        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n",
    "    else:\n",
    "        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    score = scoring(oof_df, th=0.5)\n",
    "    print(f\"Best thres: 0.5, Score: {score:.4f}\")\n",
    "    best_thres = get_best_thres(oof_df)\n",
    "    score = scoring(oof_df, th=best_thres)\n",
    "    print(f\"Best thres: {best_thres}, Score: {score:.4f}\")\n",
    "\n",
    "    if CFG.inference:\n",
    "        test_dataset = TestDataset(CFG, test)\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=CFG.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=CFG.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        predictions = []\n",
    "        for i_fold in CFG.train_fold:\n",
    "            if CFG.submission:\n",
    "                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n",
    "                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n",
    "            else:\n",
    "                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n",
    "\n",
    "            state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "            model.load_state_dict(state[\"model\"])\n",
    "            test_token_probs = inference_fn(test_dataloader, model, device)\n",
    "            test[[f\"fold{i_fold}_{i}\" for i in range(CFG.max_len)]] = test_token_probs\n",
    "            test_char_probs = get_char_probs(test[\"pn_history\"].values, test_token_probs, CFG.tokenizer)\n",
    "            predictions.append(test_char_probs)\n",
    "\n",
    "            del state, test_token_probs, model; gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        predictions = np.mean(predictions, axis=0)\n",
    "        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n",
    "        test[CFG.target_col] = predicted_location_str\n",
    "        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n",
    "        test[[CFG.id_col, CFG.target_col]].to_csv(\n",
    "            CFG.output_dir / \"submission.csv\", index=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "korean-reply",
   "metadata": {
    "id": "reduced-indication"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2681] Elapsed 0m 0s (remain 35m 47s) Loss: 0.0818(0.0818) Grad: 44827.3320  LR: 0.000000  \n",
      "Epoch: [1][100/2681] Elapsed 0m 49s (remain 21m 10s) Loss: 0.0652(0.0759) Grad: 37659.7422  LR: 0.000002  \n",
      "Epoch: [1][200/2681] Elapsed 1m 38s (remain 20m 17s) Loss: 0.0303(0.0625) Grad: 20220.4160  LR: 0.000003  \n",
      "Epoch: [1][300/2681] Elapsed 2m 27s (remain 19m 28s) Loss: 0.0125(0.0479) Grad: 4070.8733  LR: 0.000004  \n",
      "Epoch: [1][400/2681] Elapsed 3m 17s (remain 18m 40s) Loss: 0.0100(0.0388) Grad: 1696.9625  LR: 0.000006  \n",
      "Epoch: [1][500/2681] Elapsed 4m 6s (remain 17m 51s) Loss: 0.0147(0.0331) Grad: 3828.5005  LR: 0.000007  \n",
      "Epoch: [1][600/2681] Elapsed 4m 55s (remain 17m 3s) Loss: 0.0092(0.0294) Grad: 3071.4556  LR: 0.000009  \n",
      "Epoch: [1][700/2681] Elapsed 5m 44s (remain 16m 14s) Loss: 0.0029(0.0263) Grad: 3402.4321  LR: 0.000010  \n",
      "Epoch: [1][800/2681] Elapsed 6m 34s (remain 15m 25s) Loss: 0.0014(0.0237) Grad: 1837.3970  LR: 0.000012  \n",
      "Epoch: [1][900/2681] Elapsed 7m 23s (remain 14m 36s) Loss: 0.0024(0.0215) Grad: 8176.6753  LR: 0.000013  \n",
      "Epoch: [1][1000/2681] Elapsed 8m 13s (remain 13m 47s) Loss: 0.0043(0.0198) Grad: 12228.6514  LR: 0.000015  \n",
      "Epoch: [1][1100/2681] Elapsed 9m 2s (remain 12m 58s) Loss: 0.0006(0.0183) Grad: 2142.6453  LR: 0.000016  \n",
      "Epoch: [1][1200/2681] Elapsed 9m 51s (remain 12m 9s) Loss: 0.0059(0.0171) Grad: 12314.9551  LR: 0.000018  \n",
      "Epoch: [1][1300/2681] Elapsed 10m 40s (remain 11m 19s) Loss: 0.0005(0.0160) Grad: 1452.9392  LR: 0.000019  \n",
      "Epoch: [1][1400/2681] Elapsed 11m 30s (remain 10m 30s) Loss: 0.0031(0.0151) Grad: 9642.9551  LR: 0.000020  \n",
      "Epoch: [1][1500/2681] Elapsed 12m 19s (remain 9m 41s) Loss: 0.0066(0.0143) Grad: 22004.0527  LR: 0.000020  \n",
      "Epoch: [1][1600/2681] Elapsed 13m 9s (remain 8m 52s) Loss: 0.0004(0.0135) Grad: 599.0726  LR: 0.000020  \n",
      "Epoch: [1][1700/2681] Elapsed 13m 58s (remain 8m 3s) Loss: 0.0029(0.0129) Grad: 11434.3906  LR: 0.000019  \n",
      "Epoch: [1][1800/2681] Elapsed 14m 48s (remain 7m 13s) Loss: 0.0004(0.0123) Grad: 730.3416  LR: 0.000019  \n",
      "Epoch: [1][1900/2681] Elapsed 15m 37s (remain 6m 24s) Loss: 0.0005(0.0118) Grad: 1952.7983  LR: 0.000019  \n",
      "Epoch: [1][2000/2681] Elapsed 16m 26s (remain 5m 35s) Loss: 0.0012(0.0114) Grad: 2324.6355  LR: 0.000019  \n",
      "Epoch: [1][2100/2681] Elapsed 17m 16s (remain 4m 46s) Loss: 0.0036(0.0109) Grad: 13089.6787  LR: 0.000019  \n",
      "Epoch: [1][2200/2681] Elapsed 18m 5s (remain 3m 56s) Loss: 0.0001(0.0106) Grad: 358.0131  LR: 0.000019  \n",
      "Epoch: [1][2300/2681] Elapsed 18m 54s (remain 3m 7s) Loss: 0.0009(0.0102) Grad: 1976.7759  LR: 0.000018  \n",
      "Epoch: [1][2400/2681] Elapsed 19m 43s (remain 2m 17s) Loss: 0.0005(0.0098) Grad: 1823.0979  LR: 0.000018  \n",
      "Epoch: [1][2500/2681] Elapsed 20m 32s (remain 1m 28s) Loss: 0.0003(0.0096) Grad: 870.4833  LR: 0.000018  \n",
      "Epoch: [1][2600/2681] Elapsed 21m 22s (remain 0m 39s) Loss: 0.0041(0.0093) Grad: 9832.6152  LR: 0.000018  \n",
      "Epoch: [1][2680/2681] Elapsed 22m 1s (remain 0m 0s) Loss: 0.0040(0.0090) Grad: 10595.8701  LR: 0.000018  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 7m 41s) Loss: 0.0012(0.0012) \n",
      "EVAL: [100/894] Elapsed 0m 24s (remain 3m 14s) Loss: 0.0013(0.0015) \n",
      "EVAL: [200/894] Elapsed 0m 49s (remain 2m 49s) Loss: 0.0010(0.0016) \n",
      "EVAL: [300/894] Elapsed 1m 13s (remain 2m 24s) Loss: 0.0005(0.0019) \n",
      "EVAL: [400/894] Elapsed 1m 37s (remain 2m 0s) Loss: 0.0032(0.0017) \n",
      "EVAL: [500/894] Elapsed 2m 2s (remain 1m 35s) Loss: 0.0005(0.0021) \n",
      "EVAL: [600/894] Elapsed 2m 26s (remain 1m 11s) Loss: 0.0012(0.0023) \n",
      "EVAL: [700/894] Elapsed 2m 51s (remain 0m 47s) Loss: 0.0009(0.0022) \n",
      "EVAL: [800/894] Elapsed 3m 15s (remain 0m 22s) Loss: 0.0006(0.0021) \n",
      "EVAL: [893/894] Elapsed 3m 38s (remain 0m 0s) Loss: 0.0004(0.0021) \n",
      "Epoch 1 - avg_train_loss: 0.0090  avg_val_loss: 0.0021  time: 1547s\n",
      "Epoch 1 - Score: 0.8301\n",
      "Epoch 1 - Save Best Score: 0.8301 Model\n",
      "Epoch: [2][0/2681] Elapsed 0m 0s (remain 32m 1s) Loss: 0.0003(0.0003) Grad: 1619.0211  LR: 0.000018  \n",
      "Epoch: [2][100/2681] Elapsed 0m 50s (remain 21m 18s) Loss: 0.0015(0.0018) Grad: 3660.0745  LR: 0.000018  \n",
      "Epoch: [2][200/2681] Elapsed 1m 39s (remain 20m 27s) Loss: 0.0016(0.0022) Grad: 3561.1050  LR: 0.000017  \n",
      "Epoch: [2][300/2681] Elapsed 2m 28s (remain 19m 36s) Loss: 0.0013(0.0021) Grad: 3186.9424  LR: 0.000017  \n",
      "Epoch: [2][400/2681] Elapsed 3m 18s (remain 18m 45s) Loss: 0.0013(0.0019) Grad: 947.3281  LR: 0.000017  \n",
      "Epoch: [2][500/2681] Elapsed 4m 7s (remain 17m 56s) Loss: 0.0017(0.0019) Grad: 2945.6978  LR: 0.000017  \n",
      "Epoch: [2][600/2681] Elapsed 4m 57s (remain 17m 8s) Loss: 0.0009(0.0019) Grad: 4527.6157  LR: 0.000017  \n",
      "Epoch: [2][700/2681] Elapsed 5m 46s (remain 16m 19s) Loss: 0.0080(0.0018) Grad: 9129.8750  LR: 0.000017  \n",
      "Epoch: [2][800/2681] Elapsed 6m 36s (remain 15m 29s) Loss: 0.0036(0.0018) Grad: 14349.1660  LR: 0.000016  \n",
      "Epoch: [2][900/2681] Elapsed 7m 25s (remain 14m 40s) Loss: 0.0036(0.0018) Grad: 33390.0977  LR: 0.000016  \n",
      "Epoch: [2][1000/2681] Elapsed 8m 15s (remain 13m 51s) Loss: 0.0004(0.0018) Grad: 1157.4110  LR: 0.000016  \n",
      "Epoch: [2][1100/2681] Elapsed 9m 4s (remain 13m 1s) Loss: 0.0007(0.0018) Grad: 5757.8218  LR: 0.000016  \n",
      "Epoch: [2][1200/2681] Elapsed 9m 53s (remain 12m 11s) Loss: 0.0002(0.0017) Grad: 1055.5137  LR: 0.000016  \n",
      "Epoch: [2][1300/2681] Elapsed 10m 42s (remain 11m 21s) Loss: 0.0000(0.0017) Grad: 90.0180  LR: 0.000016  \n",
      "Epoch: [2][1400/2681] Elapsed 11m 32s (remain 10m 32s) Loss: 0.0031(0.0017) Grad: 9012.5391  LR: 0.000015  \n",
      "Epoch: [2][1500/2681] Elapsed 12m 21s (remain 9m 42s) Loss: 0.0010(0.0017) Grad: 1723.2986  LR: 0.000015  \n",
      "Epoch: [2][1600/2681] Elapsed 13m 10s (remain 8m 53s) Loss: 0.0012(0.0017) Grad: 2866.8875  LR: 0.000015  \n",
      "Epoch: [2][1700/2681] Elapsed 14m 0s (remain 8m 4s) Loss: 0.0004(0.0017) Grad: 1589.9161  LR: 0.000015  \n",
      "Epoch: [2][1800/2681] Elapsed 14m 49s (remain 7m 14s) Loss: 0.0003(0.0017) Grad: 717.1556  LR: 0.000015  \n",
      "Epoch: [2][1900/2681] Elapsed 15m 38s (remain 6m 25s) Loss: 0.0006(0.0017) Grad: 1195.8127  LR: 0.000015  \n",
      "Epoch: [2][2000/2681] Elapsed 16m 28s (remain 5m 35s) Loss: 0.0022(0.0017) Grad: 3703.1877  LR: 0.000014  \n",
      "Epoch: [2][2100/2681] Elapsed 17m 17s (remain 4m 46s) Loss: 0.0072(0.0017) Grad: 9041.1973  LR: 0.000014  \n",
      "Epoch: [2][2200/2681] Elapsed 18m 6s (remain 3m 56s) Loss: 0.0111(0.0017) Grad: 26457.1953  LR: 0.000014  \n",
      "Epoch: [2][2300/2681] Elapsed 18m 55s (remain 3m 7s) Loss: 0.0064(0.0017) Grad: 6417.7334  LR: 0.000014  \n",
      "Epoch: [2][2400/2681] Elapsed 19m 44s (remain 2m 18s) Loss: 0.0011(0.0017) Grad: 1599.7906  LR: 0.000014  \n",
      "Epoch: [2][2500/2681] Elapsed 20m 33s (remain 1m 28s) Loss: 0.0009(0.0017) Grad: 1295.6163  LR: 0.000014  \n",
      "Epoch: [2][2600/2681] Elapsed 21m 23s (remain 0m 39s) Loss: 0.0001(0.0017) Grad: 754.6661  LR: 0.000013  \n",
      "Epoch: [2][2680/2681] Elapsed 22m 3s (remain 0m 0s) Loss: 0.0004(0.0017) Grad: 1651.3829  LR: 0.000013  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 7m 11s) Loss: 0.0006(0.0006) \n",
      "EVAL: [100/894] Elapsed 0m 24s (remain 3m 14s) Loss: 0.0009(0.0014) \n",
      "EVAL: [200/894] Elapsed 0m 49s (remain 2m 49s) Loss: 0.0011(0.0016) \n",
      "EVAL: [300/894] Elapsed 1m 13s (remain 2m 25s) Loss: 0.0004(0.0017) \n",
      "EVAL: [400/894] Elapsed 1m 38s (remain 2m 0s) Loss: 0.0032(0.0015) \n",
      "EVAL: [500/894] Elapsed 2m 2s (remain 1m 36s) Loss: 0.0002(0.0018) \n",
      "EVAL: [600/894] Elapsed 2m 27s (remain 1m 11s) Loss: 0.0002(0.0020) \n",
      "EVAL: [700/894] Elapsed 2m 51s (remain 0m 47s) Loss: 0.0006(0.0019) \n",
      "EVAL: [800/894] Elapsed 3m 16s (remain 0m 22s) Loss: 0.0001(0.0019) \n",
      "EVAL: [893/894] Elapsed 3m 39s (remain 0m 0s) Loss: 0.0001(0.0018) \n",
      "Epoch 2 - avg_train_loss: 0.0017  avg_val_loss: 0.0018  time: 1549s\n",
      "Epoch 2 - Score: 0.8615\n",
      "Epoch 2 - Save Best Score: 0.8615 Model\n",
      "Epoch: [3][0/2681] Elapsed 0m 0s (remain 36m 17s) Loss: 0.0013(0.0013) Grad: 2851.0393  LR: 0.000013  \n",
      "Epoch: [3][100/2681] Elapsed 0m 50s (remain 21m 34s) Loss: 0.0003(0.0011) Grad: 1101.6613  LR: 0.000013  \n",
      "Epoch: [3][200/2681] Elapsed 1m 40s (remain 20m 34s) Loss: 0.0001(0.0012) Grad: 476.9836  LR: 0.000013  \n",
      "Epoch: [3][300/2681] Elapsed 2m 29s (remain 19m 40s) Loss: 0.0000(0.0012) Grad: 50.0464  LR: 0.000013  \n",
      "Epoch: [3][400/2681] Elapsed 3m 18s (remain 18m 49s) Loss: 0.0001(0.0013) Grad: 214.9339  LR: 0.000013  \n",
      "Epoch: [3][500/2681] Elapsed 4m 7s (remain 17m 58s) Loss: 0.0007(0.0012) Grad: 5742.4795  LR: 0.000013  \n",
      "Epoch: [3][600/2681] Elapsed 4m 57s (remain 17m 7s) Loss: 0.0000(0.0012) Grad: 8.6324  LR: 0.000012  \n",
      "Epoch: [3][700/2681] Elapsed 5m 45s (remain 16m 17s) Loss: 0.0002(0.0012) Grad: 1170.9801  LR: 0.000012  \n",
      "Epoch: [3][800/2681] Elapsed 6m 34s (remain 15m 26s) Loss: 0.0006(0.0012) Grad: 4871.2598  LR: 0.000012  \n",
      "Epoch: [3][900/2681] Elapsed 7m 23s (remain 14m 36s) Loss: 0.0006(0.0012) Grad: 2455.7358  LR: 0.000012  \n",
      "Epoch: [3][1000/2681] Elapsed 8m 12s (remain 13m 46s) Loss: 0.0006(0.0012) Grad: 1357.4613  LR: 0.000012  \n",
      "Epoch: [3][1100/2681] Elapsed 9m 1s (remain 12m 57s) Loss: 0.0006(0.0012) Grad: 1047.8877  LR: 0.000012  \n",
      "Epoch: [3][1200/2681] Elapsed 9m 50s (remain 12m 8s) Loss: 0.0047(0.0012) Grad: 7428.9756  LR: 0.000011  \n",
      "Epoch: [3][1300/2681] Elapsed 10m 40s (remain 11m 18s) Loss: 0.0002(0.0012) Grad: 1199.1923  LR: 0.000011  \n",
      "Epoch: [3][1400/2681] Elapsed 11m 29s (remain 10m 29s) Loss: 0.0017(0.0012) Grad: 2913.2656  LR: 0.000011  \n",
      "Epoch: [3][1500/2681] Elapsed 12m 18s (remain 9m 40s) Loss: 0.0013(0.0012) Grad: 3254.4431  LR: 0.000011  \n",
      "Epoch: [3][1600/2681] Elapsed 13m 7s (remain 8m 51s) Loss: 0.0008(0.0012) Grad: 1992.7759  LR: 0.000011  \n",
      "Epoch: [3][1700/2681] Elapsed 13m 56s (remain 8m 1s) Loss: 0.0000(0.0012) Grad: 38.6404  LR: 0.000011  \n",
      "Epoch: [3][1800/2681] Elapsed 14m 45s (remain 7m 12s) Loss: 0.0001(0.0012) Grad: 543.0475  LR: 0.000010  \n",
      "Epoch: [3][1900/2681] Elapsed 15m 34s (remain 6m 23s) Loss: 0.0000(0.0012) Grad: 72.6211  LR: 0.000010  \n",
      "Epoch: [3][2000/2681] Elapsed 16m 22s (remain 5m 33s) Loss: 0.0028(0.0012) Grad: 3234.4373  LR: 0.000010  \n",
      "Epoch: [3][2100/2681] Elapsed 17m 11s (remain 4m 44s) Loss: 0.0001(0.0012) Grad: 440.4945  LR: 0.000010  \n",
      "Epoch: [3][2200/2681] Elapsed 18m 0s (remain 3m 55s) Loss: 0.0013(0.0012) Grad: 5304.2422  LR: 0.000010  \n",
      "Epoch: [3][2300/2681] Elapsed 18m 49s (remain 3m 6s) Loss: 0.0006(0.0012) Grad: 3990.3293  LR: 0.000010  \n",
      "Epoch: [3][2400/2681] Elapsed 19m 38s (remain 2m 17s) Loss: 0.0001(0.0012) Grad: 686.5021  LR: 0.000009  \n",
      "Epoch: [3][2500/2681] Elapsed 20m 26s (remain 1m 28s) Loss: 0.0001(0.0012) Grad: 259.2202  LR: 0.000009  \n",
      "Epoch: [3][2600/2681] Elapsed 21m 16s (remain 0m 39s) Loss: 0.0000(0.0012) Grad: 49.2176  LR: 0.000009  \n",
      "Epoch: [3][2680/2681] Elapsed 21m 55s (remain 0m 0s) Loss: 0.0010(0.0012) Grad: 19550.9805  LR: 0.000009  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 7m 18s) Loss: 0.0005(0.0005) \n",
      "EVAL: [100/894] Elapsed 0m 24s (remain 3m 14s) Loss: 0.0025(0.0016) \n",
      "EVAL: [200/894] Elapsed 0m 49s (remain 2m 49s) Loss: 0.0015(0.0017) \n",
      "EVAL: [300/894] Elapsed 1m 13s (remain 2m 25s) Loss: 0.0004(0.0017) \n",
      "EVAL: [400/894] Elapsed 1m 38s (remain 2m 0s) Loss: 0.0024(0.0015) \n",
      "EVAL: [500/894] Elapsed 2m 2s (remain 1m 36s) Loss: 0.0002(0.0019) \n",
      "EVAL: [600/894] Elapsed 2m 27s (remain 1m 11s) Loss: 0.0003(0.0020) \n",
      "EVAL: [700/894] Elapsed 2m 52s (remain 0m 47s) Loss: 0.0001(0.0020) \n",
      "EVAL: [800/894] Elapsed 3m 16s (remain 0m 22s) Loss: 0.0000(0.0019) \n",
      "EVAL: [893/894] Elapsed 3m 39s (remain 0m 0s) Loss: 0.0001(0.0018) \n",
      "Epoch 3 - avg_train_loss: 0.0012  avg_val_loss: 0.0018  time: 1541s\n",
      "Epoch 3 - Score: 0.8729\n",
      "Epoch 3 - Save Best Score: 0.8729 Model\n",
      "Epoch: [4][0/2681] Elapsed 0m 0s (remain 37m 8s) Loss: 0.0001(0.0001) Grad: 1050.4447  LR: 0.000009  \n",
      "Epoch: [4][100/2681] Elapsed 0m 50s (remain 21m 30s) Loss: 0.0010(0.0011) Grad: 2346.2351  LR: 0.000009  \n",
      "Epoch: [4][200/2681] Elapsed 1m 39s (remain 20m 25s) Loss: 0.0001(0.0010) Grad: 414.5486  LR: 0.000009  \n",
      "Epoch: [4][300/2681] Elapsed 2m 28s (remain 19m 32s) Loss: 0.0005(0.0009) Grad: 4236.2661  LR: 0.000008  \n",
      "Epoch: [4][400/2681] Elapsed 3m 17s (remain 18m 43s) Loss: 0.0000(0.0009) Grad: 46.4159  LR: 0.000008  \n",
      "Epoch: [4][500/2681] Elapsed 4m 6s (remain 17m 52s) Loss: 0.0024(0.0009) Grad: 7231.3389  LR: 0.000008  \n",
      "Epoch: [4][600/2681] Elapsed 4m 55s (remain 17m 1s) Loss: 0.0016(0.0009) Grad: 5827.3643  LR: 0.000008  \n",
      "Epoch: [4][700/2681] Elapsed 5m 44s (remain 16m 12s) Loss: 0.0001(0.0010) Grad: 206.1500  LR: 0.000008  \n",
      "Epoch: [4][800/2681] Elapsed 6m 33s (remain 15m 23s) Loss: 0.0000(0.0010) Grad: 135.8430  LR: 0.000008  \n",
      "Epoch: [4][900/2681] Elapsed 7m 22s (remain 14m 34s) Loss: 0.0001(0.0010) Grad: 223.2253  LR: 0.000007  \n",
      "Epoch: [4][1000/2681] Elapsed 8m 11s (remain 13m 44s) Loss: 0.0042(0.0010) Grad: 21067.5449  LR: 0.000007  \n",
      "Epoch: [4][1100/2681] Elapsed 9m 0s (remain 12m 55s) Loss: 0.0000(0.0010) Grad: 121.8270  LR: 0.000007  \n",
      "Epoch: [4][1200/2681] Elapsed 9m 49s (remain 12m 5s) Loss: 0.0000(0.0010) Grad: 302.8904  LR: 0.000007  \n",
      "Epoch: [4][1300/2681] Elapsed 10m 38s (remain 11m 17s) Loss: 0.0002(0.0010) Grad: 1500.6600  LR: 0.000007  \n",
      "Epoch: [4][1400/2681] Elapsed 11m 27s (remain 10m 28s) Loss: 0.0009(0.0009) Grad: 2362.8799  LR: 0.000007  \n",
      "Epoch: [4][1500/2681] Elapsed 12m 16s (remain 9m 39s) Loss: 0.0000(0.0009) Grad: 204.4078  LR: 0.000006  \n",
      "Epoch: [4][1600/2681] Elapsed 13m 5s (remain 8m 50s) Loss: 0.0008(0.0010) Grad: 2791.4800  LR: 0.000006  \n",
      "Epoch: [4][1700/2681] Elapsed 13m 54s (remain 8m 0s) Loss: 0.0000(0.0010) Grad: 138.5562  LR: 0.000006  \n",
      "Epoch: [4][1800/2681] Elapsed 14m 43s (remain 7m 11s) Loss: 0.0043(0.0010) Grad: 6929.1909  LR: 0.000006  \n",
      "Epoch: [4][1900/2681] Elapsed 15m 32s (remain 6m 22s) Loss: 0.0001(0.0010) Grad: 499.5396  LR: 0.000006  \n",
      "Epoch: [4][2000/2681] Elapsed 16m 21s (remain 5m 33s) Loss: 0.0004(0.0010) Grad: 1469.1796  LR: 0.000006  \n",
      "Epoch: [4][2100/2681] Elapsed 17m 10s (remain 4m 44s) Loss: 0.0005(0.0010) Grad: 3588.7971  LR: 0.000005  \n",
      "Epoch: [4][2200/2681] Elapsed 17m 59s (remain 3m 55s) Loss: 0.0000(0.0009) Grad: 3.4618  LR: 0.000005  \n",
      "Epoch: [4][2300/2681] Elapsed 18m 48s (remain 3m 6s) Loss: 0.0011(0.0009) Grad: 1400.1564  LR: 0.000005  \n",
      "Epoch: [4][2400/2681] Elapsed 19m 37s (remain 2m 17s) Loss: 0.0013(0.0009) Grad: 6868.6006  LR: 0.000005  \n",
      "Epoch: [4][2500/2681] Elapsed 20m 26s (remain 1m 28s) Loss: 0.0000(0.0009) Grad: 17.3533  LR: 0.000005  \n",
      "Epoch: [4][2600/2681] Elapsed 21m 15s (remain 0m 39s) Loss: 0.0003(0.0009) Grad: 1837.6099  LR: 0.000005  \n",
      "Epoch: [4][2680/2681] Elapsed 21m 54s (remain 0m 0s) Loss: 0.0000(0.0009) Grad: 210.8965  LR: 0.000004  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 7m 31s) Loss: 0.0003(0.0003) \n",
      "EVAL: [100/894] Elapsed 0m 24s (remain 3m 14s) Loss: 0.0004(0.0018) \n",
      "EVAL: [200/894] Elapsed 0m 49s (remain 2m 49s) Loss: 0.0016(0.0017) \n",
      "EVAL: [300/894] Elapsed 1m 13s (remain 2m 24s) Loss: 0.0006(0.0017) \n",
      "EVAL: [400/894] Elapsed 1m 38s (remain 2m 0s) Loss: 0.0030(0.0016) \n",
      "EVAL: [500/894] Elapsed 2m 2s (remain 1m 36s) Loss: 0.0002(0.0020) \n",
      "EVAL: [600/894] Elapsed 2m 27s (remain 1m 11s) Loss: 0.0002(0.0023) \n",
      "EVAL: [700/894] Elapsed 2m 51s (remain 0m 47s) Loss: 0.0001(0.0022) \n",
      "EVAL: [800/894] Elapsed 3m 16s (remain 0m 22s) Loss: 0.0001(0.0022) \n",
      "EVAL: [893/894] Elapsed 3m 39s (remain 0m 0s) Loss: 0.0000(0.0021) \n",
      "Epoch 4 - avg_train_loss: 0.0009  avg_val_loss: 0.0021  time: 1540s\n",
      "Epoch 4 - Score: 0.8799\n",
      "Epoch 4 - Save Best Score: 0.8799 Model\n",
      "Epoch: [5][0/2681] Elapsed 0m 0s (remain 35m 14s) Loss: 0.0045(0.0045) Grad: 13402.4971  LR: 0.000004  \n",
      "Epoch: [5][100/2681] Elapsed 0m 49s (remain 21m 13s) Loss: 0.0008(0.0005) Grad: 2783.2615  LR: 0.000004  \n",
      "Epoch: [5][200/2681] Elapsed 1m 38s (remain 20m 14s) Loss: 0.0026(0.0006) Grad: 26835.8320  LR: 0.000004  \n",
      "Epoch: [5][300/2681] Elapsed 2m 27s (remain 19m 24s) Loss: 0.0043(0.0006) Grad: 6473.7959  LR: 0.000004  \n",
      "Epoch: [5][400/2681] Elapsed 3m 16s (remain 18m 35s) Loss: 0.0002(0.0007) Grad: 518.7302  LR: 0.000004  \n",
      "Epoch: [5][500/2681] Elapsed 4m 5s (remain 17m 47s) Loss: 0.0003(0.0007) Grad: 1940.2688  LR: 0.000004  \n",
      "Epoch: [5][600/2681] Elapsed 4m 54s (remain 17m 0s) Loss: 0.0007(0.0007) Grad: 4079.5894  LR: 0.000003  \n",
      "Epoch: [5][700/2681] Elapsed 5m 43s (remain 16m 11s) Loss: 0.0000(0.0007) Grad: 14.7553  LR: 0.000003  \n",
      "Epoch: [5][800/2681] Elapsed 6m 32s (remain 15m 21s) Loss: 0.0000(0.0007) Grad: 162.9802  LR: 0.000003  \n",
      "Epoch: [5][900/2681] Elapsed 7m 21s (remain 14m 32s) Loss: 0.0004(0.0007) Grad: 2259.4133  LR: 0.000003  \n",
      "Epoch: [5][1000/2681] Elapsed 8m 10s (remain 13m 43s) Loss: 0.0026(0.0008) Grad: 11053.4834  LR: 0.000003  \n",
      "Epoch: [5][1100/2681] Elapsed 8m 59s (remain 12m 54s) Loss: 0.0002(0.0007) Grad: 868.1659  LR: 0.000003  \n",
      "Epoch: [5][1200/2681] Elapsed 9m 48s (remain 12m 5s) Loss: 0.0004(0.0008) Grad: 5036.2856  LR: 0.000002  \n",
      "Epoch: [5][1300/2681] Elapsed 10m 37s (remain 11m 16s) Loss: 0.0008(0.0007) Grad: 3698.8850  LR: 0.000002  \n",
      "Epoch: [5][1400/2681] Elapsed 11m 26s (remain 10m 27s) Loss: 0.0001(0.0007) Grad: 211.5877  LR: 0.000002  \n",
      "Epoch: [5][1500/2681] Elapsed 12m 15s (remain 9m 38s) Loss: 0.0000(0.0007) Grad: 216.7636  LR: 0.000002  \n",
      "Epoch: [5][1600/2681] Elapsed 13m 4s (remain 8m 49s) Loss: 0.0001(0.0007) Grad: 543.8870  LR: 0.000002  \n",
      "Epoch: [5][1700/2681] Elapsed 13m 53s (remain 8m 0s) Loss: 0.0001(0.0007) Grad: 249.8817  LR: 0.000002  \n",
      "Epoch: [5][1800/2681] Elapsed 14m 42s (remain 7m 11s) Loss: 0.0001(0.0007) Grad: 1651.2327  LR: 0.000001  \n",
      "Epoch: [5][1900/2681] Elapsed 15m 31s (remain 6m 22s) Loss: 0.0025(0.0007) Grad: 2028.0245  LR: 0.000001  \n",
      "Epoch: [5][2000/2681] Elapsed 16m 20s (remain 5m 33s) Loss: 0.0001(0.0007) Grad: 467.6728  LR: 0.000001  \n",
      "Epoch: [5][2100/2681] Elapsed 17m 9s (remain 4m 44s) Loss: 0.0001(0.0007) Grad: 196.6524  LR: 0.000001  \n",
      "Epoch: [5][2200/2681] Elapsed 17m 58s (remain 3m 55s) Loss: 0.0001(0.0007) Grad: 312.6675  LR: 0.000001  \n",
      "Epoch: [5][2300/2681] Elapsed 18m 47s (remain 3m 6s) Loss: 0.0015(0.0007) Grad: 4415.2275  LR: 0.000001  \n",
      "Epoch: [5][2400/2681] Elapsed 19m 36s (remain 2m 17s) Loss: 0.0015(0.0007) Grad: 23013.6699  LR: 0.000000  \n",
      "Epoch: [5][2500/2681] Elapsed 20m 25s (remain 1m 28s) Loss: 0.0005(0.0007) Grad: 1271.3309  LR: 0.000000  \n",
      "Epoch: [5][2600/2681] Elapsed 21m 14s (remain 0m 39s) Loss: 0.0023(0.0007) Grad: 5645.6123  LR: 0.000000  \n",
      "Epoch: [5][2680/2681] Elapsed 21m 53s (remain 0m 0s) Loss: 0.0000(0.0007) Grad: 16.4443  LR: 0.000000  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 7m 45s) Loss: 0.0006(0.0006) \n",
      "EVAL: [100/894] Elapsed 0m 24s (remain 3m 14s) Loss: 0.0000(0.0019) \n",
      "EVAL: [200/894] Elapsed 0m 49s (remain 2m 49s) Loss: 0.0016(0.0018) \n",
      "EVAL: [300/894] Elapsed 1m 13s (remain 2m 24s) Loss: 0.0005(0.0018) \n",
      "EVAL: [400/894] Elapsed 1m 38s (remain 2m 0s) Loss: 0.0031(0.0017) \n",
      "EVAL: [500/894] Elapsed 2m 2s (remain 1m 36s) Loss: 0.0002(0.0021) \n",
      "EVAL: [600/894] Elapsed 2m 27s (remain 1m 11s) Loss: 0.0002(0.0024) \n",
      "EVAL: [700/894] Elapsed 2m 51s (remain 0m 47s) Loss: 0.0001(0.0023) \n",
      "EVAL: [800/894] Elapsed 3m 16s (remain 0m 22s) Loss: 0.0000(0.0023) \n",
      "EVAL: [893/894] Elapsed 3m 39s (remain 0m 0s) Loss: 0.0000(0.0022) \n",
      "Epoch 5 - avg_train_loss: 0.0007  avg_val_loss: 0.0022  time: 1539s\n",
      "Epoch 5 - Score: 0.8809\n",
      "Epoch 5 - Save Best Score: 0.8809 Model\n",
      "========== fold: 1 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2681] Elapsed 0m 0s (remain 33m 30s) Loss: 0.0854(0.0854) Grad: 45082.0859  LR: 0.000000  \n",
      "Epoch: [1][100/2681] Elapsed 0m 50s (remain 21m 20s) Loss: 0.0710(0.0810) Grad: 38570.4609  LR: 0.000002  \n",
      "Epoch: [1][200/2681] Elapsed 1m 39s (remain 20m 27s) Loss: 0.0272(0.0649) Grad: 6996.9189  LR: 0.000003  \n",
      "Epoch: [1][300/2681] Elapsed 2m 28s (remain 19m 36s) Loss: 0.0075(0.0481) Grad: 662.7780  LR: 0.000004  \n",
      "Epoch: [1][400/2681] Elapsed 3m 18s (remain 18m 48s) Loss: 0.0041(0.0390) Grad: 1345.9598  LR: 0.000006  \n",
      "Epoch: [1][500/2681] Elapsed 4m 7s (remain 17m 58s) Loss: 0.0079(0.0331) Grad: 1059.8555  LR: 0.000007  \n",
      "Epoch: [1][600/2681] Elapsed 4m 57s (remain 17m 8s) Loss: 0.0043(0.0288) Grad: 6753.3022  LR: 0.000009  \n",
      "Epoch: [1][700/2681] Elapsed 5m 46s (remain 16m 18s) Loss: 0.0029(0.0254) Grad: 4723.3921  LR: 0.000010  \n",
      "Epoch: [1][800/2681] Elapsed 6m 35s (remain 15m 29s) Loss: 0.0044(0.0229) Grad: 3519.1782  LR: 0.000012  \n",
      "Epoch: [1][900/2681] Elapsed 7m 25s (remain 14m 40s) Loss: 0.0054(0.0208) Grad: 4583.4741  LR: 0.000013  \n",
      "Epoch: [1][1000/2681] Elapsed 8m 15s (remain 13m 50s) Loss: 0.0033(0.0190) Grad: 2196.5764  LR: 0.000015  \n",
      "Epoch: [1][1100/2681] Elapsed 9m 4s (remain 13m 1s) Loss: 0.0026(0.0176) Grad: 924.2170  LR: 0.000016  \n",
      "Epoch: [1][1200/2681] Elapsed 9m 53s (remain 12m 11s) Loss: 0.0043(0.0164) Grad: 1361.8851  LR: 0.000018  \n",
      "Epoch: [1][1300/2681] Elapsed 10m 42s (remain 11m 21s) Loss: 0.0040(0.0153) Grad: 1418.9701  LR: 0.000019  \n",
      "Epoch: [1][1400/2681] Elapsed 11m 31s (remain 10m 31s) Loss: 0.0020(0.0144) Grad: 1596.8228  LR: 0.000020  \n",
      "Epoch: [1][1500/2681] Elapsed 12m 20s (remain 9m 41s) Loss: 0.0013(0.0136) Grad: 819.3777  LR: 0.000020  \n",
      "Epoch: [1][1600/2681] Elapsed 13m 8s (remain 8m 52s) Loss: 0.0015(0.0129) Grad: 1684.9524  LR: 0.000020  \n",
      "Epoch: [1][1700/2681] Elapsed 13m 57s (remain 8m 2s) Loss: 0.0011(0.0123) Grad: 570.3663  LR: 0.000019  \n",
      "Epoch: [1][1800/2681] Elapsed 14m 46s (remain 7m 13s) Loss: 0.0013(0.0117) Grad: 847.7983  LR: 0.000019  \n",
      "Epoch: [1][1900/2681] Elapsed 15m 35s (remain 6m 23s) Loss: 0.0029(0.0112) Grad: 1463.7106  LR: 0.000019  \n",
      "Epoch: [1][2000/2681] Elapsed 16m 23s (remain 5m 34s) Loss: 0.0016(0.0107) Grad: 702.1787  LR: 0.000019  \n",
      "Epoch: [1][2100/2681] Elapsed 17m 12s (remain 4m 45s) Loss: 0.0001(0.0103) Grad: 176.2755  LR: 0.000019  \n",
      "Epoch: [1][2200/2681] Elapsed 18m 1s (remain 3m 55s) Loss: 0.0023(0.0099) Grad: 500.7383  LR: 0.000019  \n",
      "Epoch: [1][2300/2681] Elapsed 18m 50s (remain 3m 6s) Loss: 0.0004(0.0096) Grad: 253.9545  LR: 0.000018  \n",
      "Epoch: [1][2400/2681] Elapsed 19m 39s (remain 2m 17s) Loss: 0.0010(0.0093) Grad: 379.2395  LR: 0.000018  \n",
      "Epoch: [1][2500/2681] Elapsed 20m 27s (remain 1m 28s) Loss: 0.0003(0.0090) Grad: 182.6778  LR: 0.000018  \n",
      "Epoch: [1][2600/2681] Elapsed 21m 16s (remain 0m 39s) Loss: 0.0003(0.0087) Grad: 157.1507  LR: 0.000018  \n",
      "Epoch: [1][2680/2681] Elapsed 21m 55s (remain 0m 0s) Loss: 0.0009(0.0085) Grad: 718.4265  LR: 0.000018  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 8m 19s) Loss: 0.0018(0.0018) \n",
      "EVAL: [100/894] Elapsed 0m 24s (remain 3m 14s) Loss: 0.0007(0.0011) \n",
      "EVAL: [200/894] Elapsed 0m 49s (remain 2m 49s) Loss: 0.0017(0.0018) \n",
      "EVAL: [300/894] Elapsed 1m 13s (remain 2m 25s) Loss: 0.0024(0.0019) \n",
      "EVAL: [400/894] Elapsed 1m 38s (remain 2m 0s) Loss: 0.0003(0.0018) \n",
      "EVAL: [500/894] Elapsed 2m 2s (remain 1m 36s) Loss: 0.0026(0.0020) \n",
      "EVAL: [600/894] Elapsed 2m 27s (remain 1m 11s) Loss: 0.0016(0.0020) \n",
      "EVAL: [700/894] Elapsed 2m 51s (remain 0m 47s) Loss: 0.0000(0.0020) \n",
      "EVAL: [800/894] Elapsed 3m 16s (remain 0m 22s) Loss: 0.0014(0.0019) \n",
      "EVAL: [893/894] Elapsed 3m 39s (remain 0m 0s) Loss: 0.0019(0.0017) \n",
      "Epoch 1 - avg_train_loss: 0.0085  avg_val_loss: 0.0017  time: 1542s\n",
      "Epoch 1 - Score: 0.8483\n",
      "Epoch 1 - Save Best Score: 0.8483 Model\n",
      "Epoch: [2][0/2681] Elapsed 0m 0s (remain 32m 48s) Loss: 0.0009(0.0009) Grad: 2400.9839  LR: 0.000018  \n",
      "Epoch: [2][100/2681] Elapsed 0m 49s (remain 21m 9s) Loss: 0.0011(0.0013) Grad: 1532.4752  LR: 0.000018  \n",
      "Epoch: [2][200/2681] Elapsed 1m 39s (remain 20m 23s) Loss: 0.0010(0.0013) Grad: 1184.0996  LR: 0.000017  \n",
      "Epoch: [2][300/2681] Elapsed 2m 28s (remain 19m 32s) Loss: 0.0012(0.0013) Grad: 5016.2705  LR: 0.000017  \n",
      "Epoch: [2][400/2681] Elapsed 3m 17s (remain 18m 43s) Loss: 0.0005(0.0013) Grad: 1458.6897  LR: 0.000017  \n",
      "Epoch: [2][500/2681] Elapsed 4m 6s (remain 17m 54s) Loss: 0.0000(0.0013) Grad: 30.4256  LR: 0.000017  \n",
      "Epoch: [2][600/2681] Elapsed 4m 55s (remain 17m 4s) Loss: 0.0000(0.0013) Grad: 151.8712  LR: 0.000017  \n",
      "Epoch: [2][700/2681] Elapsed 5m 45s (remain 16m 14s) Loss: 0.0029(0.0014) Grad: 2971.8958  LR: 0.000017  \n",
      "Epoch: [2][800/2681] Elapsed 6m 33s (remain 15m 24s) Loss: 0.0000(0.0014) Grad: 78.2745  LR: 0.000016  \n",
      "Epoch: [2][900/2681] Elapsed 7m 23s (remain 14m 35s) Loss: 0.0003(0.0014) Grad: 524.9133  LR: 0.000016  \n",
      "Epoch: [2][1000/2681] Elapsed 8m 12s (remain 13m 46s) Loss: 0.0010(0.0014) Grad: 1448.3179  LR: 0.000016  \n",
      "Epoch: [2][1100/2681] Elapsed 9m 1s (remain 12m 57s) Loss: 0.0007(0.0014) Grad: 613.0557  LR: 0.000016  \n",
      "Epoch: [2][1200/2681] Elapsed 9m 50s (remain 12m 7s) Loss: 0.0004(0.0014) Grad: 1314.8127  LR: 0.000016  \n",
      "Epoch: [2][1300/2681] Elapsed 10m 39s (remain 11m 18s) Loss: 0.0002(0.0014) Grad: 433.8502  LR: 0.000016  \n",
      "Epoch: [2][1400/2681] Elapsed 11m 28s (remain 10m 29s) Loss: 0.0223(0.0014) Grad: 37535.9492  LR: 0.000015  \n",
      "Epoch: [2][1500/2681] Elapsed 12m 17s (remain 9m 39s) Loss: 0.0004(0.0014) Grad: 1032.5398  LR: 0.000015  \n",
      "Epoch: [2][1600/2681] Elapsed 13m 6s (remain 8m 50s) Loss: 0.0001(0.0013) Grad: 367.8662  LR: 0.000015  \n",
      "Epoch: [2][1700/2681] Elapsed 13m 55s (remain 8m 1s) Loss: 0.0006(0.0013) Grad: 659.2567  LR: 0.000015  \n",
      "Epoch: [2][1800/2681] Elapsed 14m 45s (remain 7m 12s) Loss: 0.0003(0.0013) Grad: 2121.3237  LR: 0.000015  \n",
      "Epoch: [2][1900/2681] Elapsed 15m 34s (remain 6m 23s) Loss: 0.0007(0.0013) Grad: 979.1562  LR: 0.000015  \n",
      "Epoch: [2][2000/2681] Elapsed 16m 24s (remain 5m 34s) Loss: 0.0024(0.0013) Grad: 3686.8396  LR: 0.000014  \n",
      "Epoch: [2][2100/2681] Elapsed 17m 13s (remain 4m 45s) Loss: 0.0023(0.0013) Grad: 4399.0356  LR: 0.000014  \n",
      "Epoch: [2][2200/2681] Elapsed 18m 2s (remain 3m 56s) Loss: 0.0006(0.0013) Grad: 2313.6824  LR: 0.000014  \n",
      "Epoch: [2][2300/2681] Elapsed 18m 51s (remain 3m 6s) Loss: 0.0019(0.0013) Grad: 1684.7635  LR: 0.000014  \n",
      "Epoch: [2][2400/2681] Elapsed 19m 40s (remain 2m 17s) Loss: 0.0006(0.0013) Grad: 1071.1555  LR: 0.000014  \n",
      "Epoch: [2][2500/2681] Elapsed 20m 30s (remain 1m 28s) Loss: 0.0024(0.0013) Grad: 4059.4041  LR: 0.000014  \n",
      "Epoch: [2][2600/2681] Elapsed 21m 19s (remain 0m 39s) Loss: 0.0009(0.0013) Grad: 7608.9985  LR: 0.000013  \n",
      "Epoch: [2][2680/2681] Elapsed 21m 59s (remain 0m 0s) Loss: 0.0007(0.0013) Grad: 6718.8804  LR: 0.000013  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 7m 59s) Loss: 0.0019(0.0019) \n",
      "EVAL: [100/894] Elapsed 0m 24s (remain 3m 14s) Loss: 0.0034(0.0012) \n",
      "EVAL: [200/894] Elapsed 0m 49s (remain 2m 49s) Loss: 0.0037(0.0023) \n",
      "EVAL: [300/894] Elapsed 1m 13s (remain 2m 24s) Loss: 0.0033(0.0023) \n",
      "EVAL: [400/894] Elapsed 1m 38s (remain 2m 0s) Loss: 0.0003(0.0022) \n",
      "EVAL: [500/894] Elapsed 2m 2s (remain 1m 36s) Loss: 0.0030(0.0023) \n",
      "EVAL: [600/894] Elapsed 2m 27s (remain 1m 11s) Loss: 0.0028(0.0024) \n",
      "EVAL: [700/894] Elapsed 2m 51s (remain 0m 47s) Loss: 0.0000(0.0023) \n",
      "EVAL: [800/894] Elapsed 3m 16s (remain 0m 22s) Loss: 0.0021(0.0021) \n",
      "EVAL: [893/894] Elapsed 3m 39s (remain 0m 0s) Loss: 0.0015(0.0020) \n",
      "Epoch 2 - avg_train_loss: 0.0013  avg_val_loss: 0.0020  time: 1545s\n",
      "Epoch 2 - Score: 0.8690\n",
      "Epoch 2 - Save Best Score: 0.8690 Model\n",
      "Epoch: [3][0/2681] Elapsed 0m 0s (remain 37m 21s) Loss: 0.0001(0.0001) Grad: 1126.1917  LR: 0.000013  \n",
      "Epoch: [3][100/2681] Elapsed 0m 50s (remain 21m 18s) Loss: 0.0032(0.0008) Grad: 10980.7217  LR: 0.000013  \n",
      "Epoch: [3][200/2681] Elapsed 1m 38s (remain 20m 21s) Loss: 0.0008(0.0009) Grad: 2019.3656  LR: 0.000013  \n",
      "Epoch: [3][300/2681] Elapsed 2m 28s (remain 19m 31s) Loss: 0.0015(0.0009) Grad: 5723.2480  LR: 0.000013  \n",
      "Epoch: [3][400/2681] Elapsed 3m 17s (remain 18m 43s) Loss: 0.0013(0.0009) Grad: 3379.9829  LR: 0.000013  \n",
      "Epoch: [3][500/2681] Elapsed 4m 6s (remain 17m 53s) Loss: 0.0059(0.0009) Grad: 10445.3516  LR: 0.000013  \n",
      "Epoch: [3][600/2681] Elapsed 4m 55s (remain 17m 2s) Loss: 0.0004(0.0009) Grad: 1025.8759  LR: 0.000012  \n",
      "Epoch: [3][700/2681] Elapsed 5m 44s (remain 16m 12s) Loss: 0.0000(0.0009) Grad: 67.7474  LR: 0.000012  \n",
      "Epoch: [3][800/2681] Elapsed 6m 33s (remain 15m 22s) Loss: 0.0000(0.0010) Grad: 70.0944  LR: 0.000012  \n",
      "Epoch: [3][900/2681] Elapsed 7m 22s (remain 14m 33s) Loss: 0.0008(0.0010) Grad: 1402.5562  LR: 0.000012  \n",
      "Epoch: [3][1000/2681] Elapsed 8m 11s (remain 13m 44s) Loss: 0.0000(0.0010) Grad: 68.0618  LR: 0.000012  \n",
      "Epoch: [3][1100/2681] Elapsed 8m 59s (remain 12m 54s) Loss: 0.0002(0.0010) Grad: 376.9209  LR: 0.000012  \n",
      "Epoch: [3][1200/2681] Elapsed 9m 48s (remain 12m 5s) Loss: 0.0000(0.0010) Grad: 53.7066  LR: 0.000011  \n",
      "Epoch: [3][1300/2681] Elapsed 10m 37s (remain 11m 16s) Loss: 0.0008(0.0010) Grad: 921.9017  LR: 0.000011  \n",
      "Epoch: [3][1400/2681] Elapsed 11m 25s (remain 10m 26s) Loss: 0.0015(0.0010) Grad: 4440.8926  LR: 0.000011  \n",
      "Epoch: [3][1500/2681] Elapsed 12m 14s (remain 9m 37s) Loss: 0.0000(0.0010) Grad: 15.7474  LR: 0.000011  \n",
      "Epoch: [3][1600/2681] Elapsed 13m 3s (remain 8m 48s) Loss: 0.0007(0.0010) Grad: 723.5358  LR: 0.000011  \n",
      "Epoch: [3][1700/2681] Elapsed 13m 52s (remain 7m 59s) Loss: 0.0000(0.0010) Grad: 28.1651  LR: 0.000011  \n",
      "Epoch: [3][1800/2681] Elapsed 14m 40s (remain 7m 10s) Loss: 0.0014(0.0010) Grad: 1130.7493  LR: 0.000010  \n",
      "Epoch: [3][1900/2681] Elapsed 15m 29s (remain 6m 21s) Loss: 0.0000(0.0010) Grad: 9.7079  LR: 0.000010  \n",
      "Epoch: [3][2000/2681] Elapsed 16m 18s (remain 5m 32s) Loss: 0.0000(0.0010) Grad: 87.5849  LR: 0.000010  \n",
      "Epoch: [3][2100/2681] Elapsed 17m 7s (remain 4m 43s) Loss: 0.0016(0.0010) Grad: 2503.3032  LR: 0.000010  \n",
      "Epoch: [3][2200/2681] Elapsed 17m 56s (remain 3m 54s) Loss: 0.0013(0.0010) Grad: 2021.2289  LR: 0.000010  \n",
      "Epoch: [3][2300/2681] Elapsed 18m 44s (remain 3m 5s) Loss: 0.0025(0.0010) Grad: 1681.2578  LR: 0.000010  \n",
      "Epoch: [3][2400/2681] Elapsed 19m 33s (remain 2m 16s) Loss: 0.0003(0.0010) Grad: 813.4853  LR: 0.000009  \n",
      "Epoch: [3][2500/2681] Elapsed 20m 22s (remain 1m 27s) Loss: 0.0008(0.0010) Grad: 831.8361  LR: 0.000009  \n",
      "Epoch: [3][2600/2681] Elapsed 21m 10s (remain 0m 39s) Loss: 0.0011(0.0010) Grad: 2573.2256  LR: 0.000009  \n",
      "Epoch: [3][2680/2681] Elapsed 21m 50s (remain 0m 0s) Loss: 0.0002(0.0010) Grad: 544.1010  LR: 0.000009  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 7m 39s) Loss: 0.0021(0.0021) \n",
      "EVAL: [100/894] Elapsed 0m 24s (remain 3m 14s) Loss: 0.0012(0.0011) \n",
      "EVAL: [200/894] Elapsed 0m 49s (remain 2m 49s) Loss: 0.0032(0.0022) \n",
      "EVAL: [300/894] Elapsed 1m 13s (remain 2m 24s) Loss: 0.0027(0.0023) \n",
      "EVAL: [400/894] Elapsed 1m 37s (remain 2m 0s) Loss: 0.0004(0.0021) \n",
      "EVAL: [500/894] Elapsed 2m 2s (remain 1m 35s) Loss: 0.0024(0.0023) \n",
      "EVAL: [600/894] Elapsed 2m 26s (remain 1m 11s) Loss: 0.0047(0.0023) \n",
      "EVAL: [700/894] Elapsed 2m 51s (remain 0m 47s) Loss: 0.0000(0.0022) \n",
      "EVAL: [800/894] Elapsed 3m 15s (remain 0m 22s) Loss: 0.0004(0.0021) \n",
      "EVAL: [893/894] Elapsed 3m 38s (remain 0m 0s) Loss: 0.0013(0.0019) \n",
      "Epoch 3 - avg_train_loss: 0.0010  avg_val_loss: 0.0019  time: 1535s\n",
      "Epoch 3 - Score: 0.8767\n",
      "Epoch 3 - Save Best Score: 0.8767 Model\n",
      "Epoch: [4][0/2681] Elapsed 0m 0s (remain 38m 43s) Loss: 0.0003(0.0003) Grad: 1090.1239  LR: 0.000009  \n",
      "Epoch: [4][100/2681] Elapsed 0m 49s (remain 21m 14s) Loss: 0.0000(0.0006) Grad: 71.0446  LR: 0.000009  \n",
      "Epoch: [4][200/2681] Elapsed 1m 39s (remain 20m 23s) Loss: 0.0004(0.0006) Grad: 1531.2593  LR: 0.000009  \n",
      "Epoch: [4][300/2681] Elapsed 2m 28s (remain 19m 33s) Loss: 0.0001(0.0006) Grad: 565.0182  LR: 0.000008  \n",
      "Epoch: [4][400/2681] Elapsed 3m 17s (remain 18m 45s) Loss: 0.0012(0.0006) Grad: 2966.1619  LR: 0.000008  \n",
      "Epoch: [4][500/2681] Elapsed 4m 6s (remain 17m 54s) Loss: 0.0002(0.0007) Grad: 473.3596  LR: 0.000008  \n",
      "Epoch: [4][600/2681] Elapsed 4m 55s (remain 17m 3s) Loss: 0.0001(0.0007) Grad: 872.9579  LR: 0.000008  \n",
      "Epoch: [4][700/2681] Elapsed 5m 44s (remain 16m 13s) Loss: 0.0000(0.0007) Grad: 91.3216  LR: 0.000008  \n",
      "Epoch: [4][800/2681] Elapsed 6m 33s (remain 15m 23s) Loss: 0.0008(0.0007) Grad: 9369.2607  LR: 0.000008  \n",
      "Epoch: [4][900/2681] Elapsed 7m 22s (remain 14m 33s) Loss: 0.0014(0.0007) Grad: 3125.6938  LR: 0.000007  \n",
      "Epoch: [4][1000/2681] Elapsed 8m 10s (remain 13m 43s) Loss: 0.0002(0.0007) Grad: 2098.5337  LR: 0.000007  \n",
      "Epoch: [4][1100/2681] Elapsed 8m 59s (remain 12m 54s) Loss: 0.0023(0.0007) Grad: 2895.9419  LR: 0.000007  \n",
      "Epoch: [4][1200/2681] Elapsed 9m 48s (remain 12m 5s) Loss: 0.0002(0.0007) Grad: 902.7870  LR: 0.000007  \n",
      "Epoch: [4][1300/2681] Elapsed 10m 37s (remain 11m 15s) Loss: 0.0005(0.0007) Grad: 6158.3242  LR: 0.000007  \n",
      "Epoch: [4][1400/2681] Elapsed 11m 26s (remain 10m 26s) Loss: 0.0000(0.0007) Grad: 103.4625  LR: 0.000007  \n",
      "Epoch: [4][1500/2681] Elapsed 12m 15s (remain 9m 37s) Loss: 0.0002(0.0007) Grad: 908.6051  LR: 0.000006  \n",
      "Epoch: [4][1600/2681] Elapsed 13m 4s (remain 8m 49s) Loss: 0.0000(0.0007) Grad: 192.4643  LR: 0.000006  \n",
      "Epoch: [4][1700/2681] Elapsed 13m 53s (remain 8m 0s) Loss: 0.0001(0.0007) Grad: 441.9256  LR: 0.000006  \n",
      "Epoch: [4][1800/2681] Elapsed 14m 42s (remain 7m 11s) Loss: 0.0005(0.0007) Grad: 1036.7639  LR: 0.000006  \n",
      "Epoch: [4][1900/2681] Elapsed 15m 31s (remain 6m 22s) Loss: 0.0008(0.0007) Grad: 5933.0962  LR: 0.000006  \n",
      "Epoch: [4][2000/2681] Elapsed 16m 20s (remain 5m 33s) Loss: 0.0004(0.0007) Grad: 1041.8925  LR: 0.000006  \n",
      "Epoch: [4][2100/2681] Elapsed 17m 9s (remain 4m 44s) Loss: 0.0000(0.0007) Grad: 116.1789  LR: 0.000005  \n",
      "Epoch: [4][2200/2681] Elapsed 17m 58s (remain 3m 55s) Loss: 0.0005(0.0007) Grad: 1916.8359  LR: 0.000005  \n",
      "Epoch: [4][2300/2681] Elapsed 18m 46s (remain 3m 6s) Loss: 0.0006(0.0007) Grad: 1340.8119  LR: 0.000005  \n",
      "Epoch: [4][2400/2681] Elapsed 19m 36s (remain 2m 17s) Loss: 0.0000(0.0007) Grad: 45.6923  LR: 0.000005  \n",
      "Epoch: [4][2500/2681] Elapsed 20m 24s (remain 1m 28s) Loss: 0.0000(0.0007) Grad: 124.5484  LR: 0.000005  \n",
      "Epoch: [4][2600/2681] Elapsed 21m 13s (remain 0m 39s) Loss: 0.0003(0.0007) Grad: 1053.5560  LR: 0.000005  \n",
      "Epoch: [4][2680/2681] Elapsed 21m 53s (remain 0m 0s) Loss: 0.0016(0.0007) Grad: 4882.3140  LR: 0.000004  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 8m 4s) Loss: 0.0020(0.0020) \n",
      "EVAL: [100/894] Elapsed 0m 24s (remain 3m 14s) Loss: 0.0003(0.0013) \n",
      "EVAL: [200/894] Elapsed 0m 49s (remain 2m 49s) Loss: 0.0042(0.0025) \n",
      "EVAL: [300/894] Elapsed 1m 13s (remain 2m 24s) Loss: 0.0045(0.0027) \n",
      "EVAL: [400/894] Elapsed 1m 37s (remain 2m 0s) Loss: 0.0001(0.0025) \n",
      "EVAL: [500/894] Elapsed 2m 2s (remain 1m 36s) Loss: 0.0038(0.0027) \n",
      "EVAL: [600/894] Elapsed 2m 26s (remain 1m 11s) Loss: 0.0081(0.0027) \n",
      "EVAL: [700/894] Elapsed 2m 51s (remain 0m 47s) Loss: 0.0000(0.0026) \n",
      "EVAL: [800/894] Elapsed 3m 15s (remain 0m 22s) Loss: 0.0003(0.0024) \n",
      "EVAL: [893/894] Elapsed 3m 38s (remain 0m 0s) Loss: 0.0014(0.0023) \n",
      "Epoch 4 - avg_train_loss: 0.0007  avg_val_loss: 0.0023  time: 1538s\n",
      "Epoch 4 - Score: 0.8794\n",
      "Epoch 4 - Save Best Score: 0.8794 Model\n",
      "Epoch: [5][0/2681] Elapsed 0m 0s (remain 36m 44s) Loss: 0.0002(0.0002) Grad: 1149.9360  LR: 0.000004  \n",
      "Epoch: [5][100/2681] Elapsed 0m 49s (remain 21m 9s) Loss: 0.0008(0.0005) Grad: 2954.8147  LR: 0.000004  \n",
      "Epoch: [5][200/2681] Elapsed 1m 38s (remain 20m 21s) Loss: 0.0005(0.0005) Grad: 2187.3701  LR: 0.000004  \n",
      "Epoch: [5][300/2681] Elapsed 2m 27s (remain 19m 28s) Loss: 0.0002(0.0005) Grad: 2073.3499  LR: 0.000004  \n",
      "Epoch: [5][400/2681] Elapsed 3m 16s (remain 18m 36s) Loss: 0.0007(0.0005) Grad: 4860.1157  LR: 0.000004  \n",
      "Epoch: [5][500/2681] Elapsed 4m 4s (remain 17m 45s) Loss: 0.0006(0.0005) Grad: 1667.1489  LR: 0.000004  \n",
      "Epoch: [5][600/2681] Elapsed 4m 53s (remain 16m 56s) Loss: 0.0002(0.0005) Grad: 500.8671  LR: 0.000003  \n",
      "Epoch: [5][700/2681] Elapsed 5m 42s (remain 16m 6s) Loss: 0.0000(0.0005) Grad: 61.8820  LR: 0.000003  \n",
      "Epoch: [5][800/2681] Elapsed 6m 31s (remain 15m 17s) Loss: 0.0012(0.0005) Grad: 3442.7590  LR: 0.000003  \n",
      "Epoch: [5][900/2681] Elapsed 7m 19s (remain 14m 29s) Loss: 0.0027(0.0005) Grad: 5823.0186  LR: 0.000003  \n",
      "Epoch: [5][1000/2681] Elapsed 8m 9s (remain 13m 40s) Loss: 0.0000(0.0006) Grad: 165.9523  LR: 0.000003  \n",
      "Epoch: [5][1100/2681] Elapsed 8m 58s (remain 12m 52s) Loss: 0.0002(0.0006) Grad: 766.8556  LR: 0.000003  \n",
      "Epoch: [5][1200/2681] Elapsed 9m 46s (remain 12m 2s) Loss: 0.0000(0.0006) Grad: 37.0105  LR: 0.000002  \n",
      "Epoch: [5][1300/2681] Elapsed 10m 35s (remain 11m 14s) Loss: 0.0043(0.0005) Grad: 12059.6943  LR: 0.000002  \n",
      "Epoch: [5][1400/2681] Elapsed 11m 24s (remain 10m 25s) Loss: 0.0000(0.0006) Grad: 138.1042  LR: 0.000002  \n",
      "Epoch: [5][1500/2681] Elapsed 12m 13s (remain 9m 36s) Loss: 0.0004(0.0006) Grad: 1320.6157  LR: 0.000002  \n",
      "Epoch: [5][1600/2681] Elapsed 13m 1s (remain 8m 47s) Loss: 0.0001(0.0006) Grad: 406.7213  LR: 0.000002  \n",
      "Epoch: [5][1700/2681] Elapsed 13m 50s (remain 7m 58s) Loss: 0.0002(0.0006) Grad: 658.3427  LR: 0.000002  \n",
      "Epoch: [5][1800/2681] Elapsed 14m 39s (remain 7m 9s) Loss: 0.0000(0.0006) Grad: 161.1654  LR: 0.000001  \n",
      "Epoch: [5][1900/2681] Elapsed 15m 28s (remain 6m 21s) Loss: 0.0000(0.0006) Grad: 133.9771  LR: 0.000001  \n",
      "Epoch: [5][2000/2681] Elapsed 16m 17s (remain 5m 32s) Loss: 0.0006(0.0006) Grad: 1512.0859  LR: 0.000001  \n",
      "Epoch: [5][2100/2681] Elapsed 17m 5s (remain 4m 43s) Loss: 0.0001(0.0006) Grad: 644.3403  LR: 0.000001  \n",
      "Epoch: [5][2200/2681] Elapsed 17m 54s (remain 3m 54s) Loss: 0.0008(0.0006) Grad: 3052.3467  LR: 0.000001  \n",
      "Epoch: [5][2300/2681] Elapsed 18m 43s (remain 3m 5s) Loss: 0.0000(0.0006) Grad: 311.5248  LR: 0.000001  \n",
      "Epoch: [5][2400/2681] Elapsed 19m 32s (remain 2m 16s) Loss: 0.0006(0.0006) Grad: 2567.2397  LR: 0.000000  \n",
      "Epoch: [5][2500/2681] Elapsed 20m 21s (remain 1m 27s) Loss: 0.0000(0.0006) Grad: 117.2253  LR: 0.000000  \n",
      "Epoch: [5][2600/2681] Elapsed 21m 10s (remain 0m 39s) Loss: 0.0006(0.0006) Grad: 2709.3604  LR: 0.000000  \n",
      "Epoch: [5][2680/2681] Elapsed 21m 49s (remain 0m 0s) Loss: 0.0051(0.0006) Grad: 11014.0918  LR: 0.000000  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 7m 58s) Loss: 0.0019(0.0019) \n",
      "EVAL: [100/894] Elapsed 0m 24s (remain 3m 14s) Loss: 0.0006(0.0015) \n",
      "EVAL: [200/894] Elapsed 0m 49s (remain 2m 49s) Loss: 0.0047(0.0029) \n",
      "EVAL: [300/894] Elapsed 1m 13s (remain 2m 24s) Loss: 0.0050(0.0031) \n",
      "EVAL: [400/894] Elapsed 1m 37s (remain 2m 0s) Loss: 0.0001(0.0029) \n",
      "EVAL: [500/894] Elapsed 2m 2s (remain 1m 36s) Loss: 0.0059(0.0031) \n",
      "EVAL: [600/894] Elapsed 2m 26s (remain 1m 11s) Loss: 0.0099(0.0031) \n",
      "EVAL: [700/894] Elapsed 2m 51s (remain 0m 47s) Loss: 0.0000(0.0029) \n",
      "EVAL: [800/894] Elapsed 3m 16s (remain 0m 22s) Loss: 0.0002(0.0028) \n",
      "EVAL: [893/894] Elapsed 3m 38s (remain 0m 0s) Loss: 0.0014(0.0026) \n",
      "Epoch 5 - avg_train_loss: 0.0006  avg_val_loss: 0.0026  time: 1534s\n",
      "Epoch 5 - Score: 0.8775\n",
      "========== fold: 2 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2681] Elapsed 0m 0s (remain 35m 45s) Loss: 0.0815(0.0815) Grad: 41320.2812  LR: 0.000000  \n",
      "Epoch: [1][100/2681] Elapsed 0m 49s (remain 21m 16s) Loss: 0.0678(0.0767) Grad: 35371.9492  LR: 0.000002  \n",
      "Epoch: [1][200/2681] Elapsed 1m 39s (remain 20m 23s) Loss: 0.0335(0.0636) Grad: 18736.9453  LR: 0.000003  \n",
      "Epoch: [1][300/2681] Elapsed 2m 28s (remain 19m 34s) Loss: 0.0127(0.0489) Grad: 2191.1821  LR: 0.000004  \n",
      "Epoch: [1][400/2681] Elapsed 3m 18s (remain 18m 47s) Loss: 0.0136(0.0397) Grad: 3250.1392  LR: 0.000006  \n",
      "Epoch: [1][500/2681] Elapsed 4m 8s (remain 18m 0s) Loss: 0.0091(0.0339) Grad: 1743.5365  LR: 0.000007  \n",
      "Epoch: [1][600/2681] Elapsed 4m 58s (remain 17m 12s) Loss: 0.0099(0.0299) Grad: 4423.5640  LR: 0.000009  \n",
      "Epoch: [1][700/2681] Elapsed 5m 47s (remain 16m 22s) Loss: 0.0041(0.0266) Grad: 5517.3550  LR: 0.000010  \n",
      "Epoch: [1][800/2681] Elapsed 6m 37s (remain 15m 33s) Loss: 0.0106(0.0239) Grad: 29929.9648  LR: 0.000012  \n",
      "Epoch: [1][900/2681] Elapsed 7m 27s (remain 14m 44s) Loss: 0.0118(0.0218) Grad: 22153.0352  LR: 0.000013  \n",
      "Epoch: [1][1000/2681] Elapsed 8m 16s (remain 13m 53s) Loss: 0.0025(0.0200) Grad: 5077.5381  LR: 0.000015  \n",
      "Epoch: [1][1100/2681] Elapsed 9m 6s (remain 13m 3s) Loss: 0.0063(0.0185) Grad: 16278.8975  LR: 0.000016  \n",
      "Epoch: [1][1200/2681] Elapsed 9m 56s (remain 12m 14s) Loss: 0.0010(0.0173) Grad: 2247.8738  LR: 0.000018  \n",
      "Epoch: [1][1300/2681] Elapsed 10m 46s (remain 11m 25s) Loss: 0.0007(0.0162) Grad: 2900.3477  LR: 0.000019  \n",
      "Epoch: [1][1400/2681] Elapsed 11m 35s (remain 10m 35s) Loss: 0.0012(0.0152) Grad: 5178.8062  LR: 0.000020  \n",
      "Epoch: [1][1500/2681] Elapsed 12m 25s (remain 9m 46s) Loss: 0.0042(0.0144) Grad: 12150.5625  LR: 0.000020  \n",
      "Epoch: [1][1600/2681] Elapsed 13m 15s (remain 8m 56s) Loss: 0.0022(0.0136) Grad: 4998.8828  LR: 0.000020  \n",
      "Epoch: [1][1700/2681] Elapsed 14m 4s (remain 8m 6s) Loss: 0.0007(0.0130) Grad: 1085.0500  LR: 0.000019  \n",
      "Epoch: [1][1800/2681] Elapsed 14m 54s (remain 7m 17s) Loss: 0.0025(0.0124) Grad: 5090.7202  LR: 0.000019  \n",
      "Epoch: [1][1900/2681] Elapsed 15m 44s (remain 6m 27s) Loss: 0.0015(0.0119) Grad: 2541.8816  LR: 0.000019  \n",
      "Epoch: [1][2000/2681] Elapsed 16m 33s (remain 5m 37s) Loss: 0.0271(0.0114) Grad: 42719.3867  LR: 0.000019  \n",
      "Epoch: [1][2100/2681] Elapsed 17m 23s (remain 4m 47s) Loss: 0.0009(0.0110) Grad: 1963.2152  LR: 0.000019  \n",
      "Epoch: [1][2200/2681] Elapsed 18m 11s (remain 3m 58s) Loss: 0.0015(0.0106) Grad: 4043.2053  LR: 0.000019  \n",
      "Epoch: [1][2300/2681] Elapsed 19m 1s (remain 3m 8s) Loss: 0.0014(0.0102) Grad: 2420.6367  LR: 0.000018  \n",
      "Epoch: [1][2400/2681] Elapsed 19m 51s (remain 2m 18s) Loss: 0.0001(0.0099) Grad: 159.1007  LR: 0.000018  \n",
      "Epoch: [1][2500/2681] Elapsed 20m 40s (remain 1m 29s) Loss: 0.0034(0.0096) Grad: 4711.1079  LR: 0.000018  \n",
      "Epoch: [1][2600/2681] Elapsed 21m 30s (remain 0m 39s) Loss: 0.0018(0.0093) Grad: 9803.2451  LR: 0.000018  \n",
      "Epoch: [1][2680/2681] Elapsed 22m 10s (remain 0m 0s) Loss: 0.0009(0.0091) Grad: 4167.4878  LR: 0.000018  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 8m 37s) Loss: 0.0009(0.0009) \n",
      "EVAL: [100/894] Elapsed 0m 24s (remain 3m 15s) Loss: 0.0004(0.0021) \n",
      "EVAL: [200/894] Elapsed 0m 49s (remain 2m 49s) Loss: 0.0000(0.0020) \n",
      "EVAL: [300/894] Elapsed 1m 13s (remain 2m 24s) Loss: 0.0002(0.0024) \n",
      "EVAL: [400/894] Elapsed 1m 37s (remain 2m 0s) Loss: 0.0016(0.0021) \n",
      "EVAL: [500/894] Elapsed 2m 2s (remain 1m 36s) Loss: 0.0010(0.0024) \n",
      "EVAL: [600/894] Elapsed 2m 26s (remain 1m 11s) Loss: 0.0001(0.0024) \n",
      "EVAL: [700/894] Elapsed 2m 51s (remain 0m 47s) Loss: 0.0093(0.0024) \n",
      "EVAL: [800/894] Elapsed 3m 15s (remain 0m 22s) Loss: 0.0005(0.0023) \n",
      "EVAL: [893/894] Elapsed 3m 38s (remain 0m 0s) Loss: 0.0001(0.0021) \n",
      "Epoch 1 - avg_train_loss: 0.0091  avg_val_loss: 0.0021  time: 1556s\n",
      "Epoch 1 - Score: 0.8531\n",
      "Epoch 1 - Save Best Score: 0.8531 Model\n",
      "Epoch: [2][0/2681] Elapsed 0m 0s (remain 34m 26s) Loss: 0.0001(0.0001) Grad: 1363.4641  LR: 0.000018  \n",
      "Epoch: [2][100/2681] Elapsed 0m 49s (remain 21m 8s) Loss: 0.0036(0.0017) Grad: 13760.0381  LR: 0.000018  \n",
      "Epoch: [2][200/2681] Elapsed 1m 38s (remain 20m 20s) Loss: 0.0034(0.0017) Grad: 11870.2939  LR: 0.000017  \n",
      "Epoch: [2][300/2681] Elapsed 2m 28s (remain 19m 32s) Loss: 0.0010(0.0018) Grad: 1845.0596  LR: 0.000017  \n",
      "Epoch: [2][400/2681] Elapsed 3m 17s (remain 18m 42s) Loss: 0.0006(0.0019) Grad: 1631.9269  LR: 0.000017  \n",
      "Epoch: [2][500/2681] Elapsed 4m 6s (remain 17m 51s) Loss: 0.0018(0.0019) Grad: 3510.7341  LR: 0.000017  \n",
      "Epoch: [2][600/2681] Elapsed 4m 55s (remain 17m 2s) Loss: 0.0010(0.0019) Grad: 2286.9905  LR: 0.000017  \n",
      "Epoch: [2][700/2681] Elapsed 5m 44s (remain 16m 13s) Loss: 0.0011(0.0019) Grad: 1283.3541  LR: 0.000017  \n",
      "Epoch: [2][800/2681] Elapsed 6m 34s (remain 15m 24s) Loss: 0.0008(0.0018) Grad: 1264.3955  LR: 0.000016  \n",
      "Epoch: [2][900/2681] Elapsed 7m 23s (remain 14m 35s) Loss: 0.0001(0.0018) Grad: 424.1557  LR: 0.000016  \n",
      "Epoch: [2][1000/2681] Elapsed 8m 12s (remain 13m 46s) Loss: 0.0028(0.0018) Grad: 5452.6826  LR: 0.000016  \n",
      "Epoch: [2][1100/2681] Elapsed 9m 1s (remain 12m 57s) Loss: 0.0042(0.0018) Grad: 6286.3081  LR: 0.000016  \n",
      "Epoch: [2][1200/2681] Elapsed 9m 50s (remain 12m 7s) Loss: 0.0010(0.0018) Grad: 2827.6733  LR: 0.000016  \n",
      "Epoch: [2][1300/2681] Elapsed 10m 39s (remain 11m 18s) Loss: 0.0007(0.0018) Grad: 11037.8418  LR: 0.000016  \n",
      "Epoch: [2][1400/2681] Elapsed 11m 28s (remain 10m 29s) Loss: 0.0001(0.0017) Grad: 793.2120  LR: 0.000015  \n",
      "Epoch: [2][1500/2681] Elapsed 12m 17s (remain 9m 39s) Loss: 0.0002(0.0017) Grad: 802.0521  LR: 0.000015  \n",
      "Epoch: [2][1600/2681] Elapsed 13m 6s (remain 8m 50s) Loss: 0.0017(0.0017) Grad: 9510.1416  LR: 0.000015  \n",
      "Epoch: [2][1700/2681] Elapsed 13m 54s (remain 8m 1s) Loss: 0.0089(0.0017) Grad: 13721.6992  LR: 0.000015  \n",
      "Epoch: [2][1800/2681] Elapsed 14m 44s (remain 7m 12s) Loss: 0.0003(0.0017) Grad: 832.4683  LR: 0.000015  \n",
      "Epoch: [2][1900/2681] Elapsed 15m 33s (remain 6m 22s) Loss: 0.0006(0.0017) Grad: 2799.2966  LR: 0.000015  \n",
      "Epoch: [2][2000/2681] Elapsed 16m 22s (remain 5m 33s) Loss: 0.0002(0.0017) Grad: 1391.0583  LR: 0.000014  \n",
      "Epoch: [2][2100/2681] Elapsed 17m 11s (remain 4m 44s) Loss: 0.0004(0.0017) Grad: 1232.9758  LR: 0.000014  \n",
      "Epoch: [2][2200/2681] Elapsed 18m 0s (remain 3m 55s) Loss: 0.0001(0.0017) Grad: 269.2433  LR: 0.000014  \n",
      "Epoch: [2][2300/2681] Elapsed 18m 49s (remain 3m 6s) Loss: 0.0000(0.0017) Grad: 174.9763  LR: 0.000014  \n",
      "Epoch: [2][2400/2681] Elapsed 19m 38s (remain 2m 17s) Loss: 0.0064(0.0017) Grad: 6034.5078  LR: 0.000014  \n",
      "Epoch: [2][2500/2681] Elapsed 20m 27s (remain 1m 28s) Loss: 0.0002(0.0017) Grad: 753.2656  LR: 0.000014  \n",
      "Epoch: [2][2600/2681] Elapsed 21m 16s (remain 0m 39s) Loss: 0.0008(0.0017) Grad: 1269.5261  LR: 0.000013  \n",
      "Epoch: [2][2680/2681] Elapsed 21m 55s (remain 0m 0s) Loss: 0.0013(0.0017) Grad: 5751.7759  LR: 0.000013  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 8m 38s) Loss: 0.0003(0.0003) \n",
      "EVAL: [100/894] Elapsed 0m 24s (remain 3m 14s) Loss: 0.0003(0.0016) \n",
      "EVAL: [200/894] Elapsed 0m 49s (remain 2m 49s) Loss: 0.0000(0.0016) \n",
      "EVAL: [300/894] Elapsed 1m 13s (remain 2m 24s) Loss: 0.0002(0.0017) \n",
      "EVAL: [400/894] Elapsed 1m 37s (remain 2m 0s) Loss: 0.0001(0.0015) \n",
      "EVAL: [500/894] Elapsed 2m 2s (remain 1m 35s) Loss: 0.0015(0.0017) \n",
      "EVAL: [600/894] Elapsed 2m 26s (remain 1m 11s) Loss: 0.0000(0.0018) \n",
      "EVAL: [700/894] Elapsed 2m 51s (remain 0m 47s) Loss: 0.0111(0.0019) \n",
      "EVAL: [800/894] Elapsed 3m 15s (remain 0m 22s) Loss: 0.0006(0.0017) \n",
      "EVAL: [893/894] Elapsed 3m 38s (remain 0m 0s) Loss: 0.0001(0.0016) \n",
      "Epoch 2 - avg_train_loss: 0.0017  avg_val_loss: 0.0016  time: 1540s\n",
      "Epoch 2 - Score: 0.8770\n",
      "Epoch 2 - Save Best Score: 0.8770 Model\n",
      "Epoch: [3][0/2681] Elapsed 0m 0s (remain 38m 17s) Loss: 0.0004(0.0004) Grad: 1348.1199  LR: 0.000013  \n",
      "Epoch: [3][100/2681] Elapsed 0m 49s (remain 21m 11s) Loss: 0.0001(0.0010) Grad: 1099.4281  LR: 0.000013  \n",
      "Epoch: [3][200/2681] Elapsed 1m 38s (remain 20m 17s) Loss: 0.0134(0.0012) Grad: 18333.1172  LR: 0.000013  \n",
      "Epoch: [3][300/2681] Elapsed 2m 28s (remain 19m 30s) Loss: 0.0013(0.0012) Grad: 4944.7339  LR: 0.000013  \n",
      "Epoch: [3][400/2681] Elapsed 3m 17s (remain 18m 41s) Loss: 0.0001(0.0012) Grad: 220.9102  LR: 0.000013  \n",
      "Epoch: [3][500/2681] Elapsed 4m 6s (remain 17m 52s) Loss: 0.0000(0.0012) Grad: 104.0881  LR: 0.000013  \n",
      "Epoch: [3][600/2681] Elapsed 4m 55s (remain 17m 3s) Loss: 0.0006(0.0012) Grad: 1921.8588  LR: 0.000012  \n",
      "Epoch: [3][700/2681] Elapsed 5m 44s (remain 16m 13s) Loss: 0.0003(0.0012) Grad: 3238.3203  LR: 0.000012  \n",
      "Epoch: [3][800/2681] Elapsed 6m 33s (remain 15m 23s) Loss: 0.0011(0.0012) Grad: 1959.9393  LR: 0.000012  \n",
      "Epoch: [3][900/2681] Elapsed 7m 22s (remain 14m 34s) Loss: 0.0002(0.0012) Grad: 504.3352  LR: 0.000012  \n",
      "Epoch: [3][1000/2681] Elapsed 8m 11s (remain 13m 44s) Loss: 0.0004(0.0012) Grad: 1236.1111  LR: 0.000012  \n",
      "Epoch: [3][1100/2681] Elapsed 9m 0s (remain 12m 55s) Loss: 0.0021(0.0012) Grad: 3997.8789  LR: 0.000012  \n",
      "Epoch: [3][1200/2681] Elapsed 9m 49s (remain 12m 6s) Loss: 0.0012(0.0012) Grad: 820.8377  LR: 0.000011  \n",
      "Epoch: [3][1300/2681] Elapsed 10m 38s (remain 11m 17s) Loss: 0.0000(0.0012) Grad: 247.5433  LR: 0.000011  \n",
      "Epoch: [3][1400/2681] Elapsed 11m 28s (remain 10m 28s) Loss: 0.0005(0.0012) Grad: 2078.2461  LR: 0.000011  \n",
      "Epoch: [3][1500/2681] Elapsed 12m 17s (remain 9m 39s) Loss: 0.0005(0.0012) Grad: 1724.5892  LR: 0.000011  \n",
      "Epoch: [3][1600/2681] Elapsed 13m 6s (remain 8m 50s) Loss: 0.0000(0.0012) Grad: 156.5809  LR: 0.000011  \n",
      "Epoch: [3][1700/2681] Elapsed 13m 55s (remain 8m 1s) Loss: 0.0001(0.0012) Grad: 376.1621  LR: 0.000011  \n",
      "Epoch: [3][1800/2681] Elapsed 14m 44s (remain 7m 12s) Loss: 0.0000(0.0012) Grad: 90.2414  LR: 0.000010  \n",
      "Epoch: [3][1900/2681] Elapsed 15m 34s (remain 6m 23s) Loss: 0.0000(0.0012) Grad: 51.0377  LR: 0.000010  \n",
      "Epoch: [3][2000/2681] Elapsed 16m 23s (remain 5m 34s) Loss: 0.0001(0.0013) Grad: 152.9080  LR: 0.000010  \n",
      "Epoch: [3][2100/2681] Elapsed 17m 12s (remain 4m 44s) Loss: 0.0004(0.0012) Grad: 1924.9136  LR: 0.000010  \n",
      "Epoch: [3][2200/2681] Elapsed 18m 1s (remain 3m 55s) Loss: 0.0001(0.0012) Grad: 512.2956  LR: 0.000010  \n",
      "Epoch: [3][2300/2681] Elapsed 18m 50s (remain 3m 6s) Loss: 0.0021(0.0012) Grad: 4366.0645  LR: 0.000010  \n",
      "Epoch: [3][2400/2681] Elapsed 19m 39s (remain 2m 17s) Loss: 0.0001(0.0012) Grad: 270.1681  LR: 0.000009  \n",
      "Epoch: [3][2500/2681] Elapsed 20m 28s (remain 1m 28s) Loss: 0.0009(0.0012) Grad: 3612.7319  LR: 0.000009  \n",
      "Epoch: [3][2600/2681] Elapsed 21m 17s (remain 0m 39s) Loss: 0.0000(0.0012) Grad: 256.2782  LR: 0.000009  \n",
      "Epoch: [3][2680/2681] Elapsed 21m 56s (remain 0m 0s) Loss: 0.0024(0.0012) Grad: 6283.1440  LR: 0.000009  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 7m 51s) Loss: 0.0003(0.0003) \n",
      "EVAL: [100/894] Elapsed 0m 24s (remain 3m 14s) Loss: 0.0008(0.0019) \n",
      "EVAL: [200/894] Elapsed 0m 49s (remain 2m 49s) Loss: 0.0001(0.0018) \n",
      "EVAL: [300/894] Elapsed 1m 13s (remain 2m 24s) Loss: 0.0001(0.0019) \n",
      "EVAL: [400/894] Elapsed 1m 37s (remain 2m 0s) Loss: 0.0000(0.0017) \n",
      "EVAL: [500/894] Elapsed 2m 2s (remain 1m 35s) Loss: 0.0003(0.0019) \n",
      "EVAL: [600/894] Elapsed 2m 26s (remain 1m 11s) Loss: 0.0000(0.0019) \n",
      "EVAL: [700/894] Elapsed 2m 51s (remain 0m 47s) Loss: 0.0124(0.0020) \n",
      "EVAL: [800/894] Elapsed 3m 15s (remain 0m 22s) Loss: 0.0004(0.0018) \n",
      "EVAL: [893/894] Elapsed 3m 38s (remain 0m 0s) Loss: 0.0002(0.0017) \n",
      "Epoch 3 - avg_train_loss: 0.0012  avg_val_loss: 0.0017  time: 1541s\n",
      "Epoch 3 - Score: 0.8827\n",
      "Epoch 3 - Save Best Score: 0.8827 Model\n",
      "Epoch: [4][0/2681] Elapsed 0m 0s (remain 36m 33s) Loss: 0.0030(0.0030) Grad: 4606.7578  LR: 0.000009  \n",
      "Epoch: [4][100/2681] Elapsed 0m 49s (remain 21m 15s) Loss: 0.0003(0.0009) Grad: 1336.5067  LR: 0.000009  \n",
      "Epoch: [4][200/2681] Elapsed 1m 38s (remain 20m 21s) Loss: 0.0002(0.0010) Grad: 1319.5585  LR: 0.000009  \n",
      "Epoch: [4][300/2681] Elapsed 2m 28s (remain 19m 30s) Loss: 0.0001(0.0010) Grad: 278.5689  LR: 0.000008  \n",
      "Epoch: [4][400/2681] Elapsed 3m 16s (remain 18m 39s) Loss: 0.0000(0.0009) Grad: 63.6494  LR: 0.000008  \n",
      "Epoch: [4][500/2681] Elapsed 4m 5s (remain 17m 50s) Loss: 0.0000(0.0009) Grad: 168.2295  LR: 0.000008  \n",
      "Epoch: [4][600/2681] Elapsed 4m 54s (remain 17m 0s) Loss: 0.0000(0.0010) Grad: 16.5730  LR: 0.000008  \n",
      "Epoch: [4][700/2681] Elapsed 5m 43s (remain 16m 11s) Loss: 0.0005(0.0010) Grad: 2217.1006  LR: 0.000008  \n",
      "Epoch: [4][800/2681] Elapsed 6m 32s (remain 15m 21s) Loss: 0.0004(0.0010) Grad: 1208.1144  LR: 0.000008  \n",
      "Epoch: [4][900/2681] Elapsed 7m 21s (remain 14m 32s) Loss: 0.0001(0.0010) Grad: 283.5978  LR: 0.000007  \n",
      "Epoch: [4][1000/2681] Elapsed 8m 10s (remain 13m 42s) Loss: 0.0000(0.0010) Grad: 8.6201  LR: 0.000007  \n",
      "Epoch: [4][1100/2681] Elapsed 8m 59s (remain 12m 53s) Loss: 0.0002(0.0010) Grad: 393.9355  LR: 0.000007  \n",
      "Epoch: [4][1200/2681] Elapsed 9m 48s (remain 12m 4s) Loss: 0.0001(0.0010) Grad: 359.7388  LR: 0.000007  \n",
      "Epoch: [4][1300/2681] Elapsed 10m 36s (remain 11m 15s) Loss: 0.0004(0.0010) Grad: 1533.2311  LR: 0.000007  \n",
      "Epoch: [4][1400/2681] Elapsed 11m 26s (remain 10m 26s) Loss: 0.0013(0.0010) Grad: 1856.0000  LR: 0.000007  \n",
      "Epoch: [4][1500/2681] Elapsed 12m 15s (remain 9m 38s) Loss: 0.0042(0.0010) Grad: 19553.5820  LR: 0.000006  \n",
      "Epoch: [4][1600/2681] Elapsed 13m 4s (remain 8m 49s) Loss: 0.0001(0.0010) Grad: 598.1700  LR: 0.000006  \n",
      "Epoch: [4][1700/2681] Elapsed 13m 53s (remain 8m 0s) Loss: 0.0146(0.0010) Grad: 30559.2402  LR: 0.000006  \n",
      "Epoch: [4][1800/2681] Elapsed 14m 42s (remain 7m 11s) Loss: 0.0027(0.0010) Grad: 8045.4077  LR: 0.000006  \n",
      "Epoch: [4][1900/2681] Elapsed 15m 31s (remain 6m 22s) Loss: 0.0003(0.0010) Grad: 1366.7123  LR: 0.000006  \n",
      "Epoch: [4][2000/2681] Elapsed 16m 20s (remain 5m 33s) Loss: 0.0050(0.0010) Grad: 15037.8721  LR: 0.000006  \n",
      "Epoch: [4][2100/2681] Elapsed 17m 10s (remain 4m 44s) Loss: 0.0036(0.0010) Grad: 8698.1123  LR: 0.000005  \n",
      "Epoch: [4][2200/2681] Elapsed 17m 59s (remain 3m 55s) Loss: 0.0001(0.0010) Grad: 631.0723  LR: 0.000005  \n",
      "Epoch: [4][2300/2681] Elapsed 18m 48s (remain 3m 6s) Loss: 0.0000(0.0010) Grad: 23.8840  LR: 0.000005  \n",
      "Epoch: [4][2400/2681] Elapsed 19m 37s (remain 2m 17s) Loss: 0.0000(0.0010) Grad: 112.8100  LR: 0.000005  \n",
      "Epoch: [4][2500/2681] Elapsed 20m 26s (remain 1m 28s) Loss: 0.0001(0.0010) Grad: 964.2670  LR: 0.000005  \n",
      "Epoch: [4][2600/2681] Elapsed 21m 15s (remain 0m 39s) Loss: 0.0001(0.0010) Grad: 955.3455  LR: 0.000005  \n",
      "Epoch: [4][2680/2681] Elapsed 21m 54s (remain 0m 0s) Loss: 0.0000(0.0010) Grad: 88.7347  LR: 0.000004  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 8m 25s) Loss: 0.0002(0.0002) \n",
      "EVAL: [100/894] Elapsed 0m 24s (remain 3m 15s) Loss: 0.0001(0.0021) \n",
      "EVAL: [200/894] Elapsed 0m 49s (remain 2m 49s) Loss: 0.0003(0.0019) \n",
      "EVAL: [300/894] Elapsed 1m 13s (remain 2m 24s) Loss: 0.0001(0.0020) \n",
      "EVAL: [400/894] Elapsed 1m 37s (remain 2m 0s) Loss: 0.0000(0.0018) \n",
      "EVAL: [500/894] Elapsed 2m 2s (remain 1m 35s) Loss: 0.0001(0.0020) \n",
      "EVAL: [600/894] Elapsed 2m 26s (remain 1m 11s) Loss: 0.0000(0.0021) \n",
      "EVAL: [700/894] Elapsed 2m 51s (remain 0m 47s) Loss: 0.0136(0.0021) \n",
      "EVAL: [800/894] Elapsed 3m 15s (remain 0m 22s) Loss: 0.0003(0.0020) \n",
      "EVAL: [893/894] Elapsed 3m 38s (remain 0m 0s) Loss: 0.0000(0.0019) \n",
      "Epoch 4 - avg_train_loss: 0.0010  avg_val_loss: 0.0019  time: 1540s\n",
      "Epoch 4 - Score: 0.8839\n",
      "Epoch 4 - Save Best Score: 0.8839 Model\n",
      "Epoch: [5][0/2681] Elapsed 0m 0s (remain 39m 22s) Loss: 0.0005(0.0005) Grad: 1991.8046  LR: 0.000004  \n",
      "Epoch: [5][100/2681] Elapsed 0m 49s (remain 21m 8s) Loss: 0.0000(0.0006) Grad: 48.9944  LR: 0.000004  \n",
      "Epoch: [5][200/2681] Elapsed 1m 38s (remain 20m 14s) Loss: 0.0007(0.0008) Grad: 9138.4375  LR: 0.000004  \n",
      "Epoch: [5][300/2681] Elapsed 2m 27s (remain 19m 26s) Loss: 0.0003(0.0007) Grad: 5474.3867  LR: 0.000004  \n",
      "Epoch: [5][400/2681] Elapsed 3m 16s (remain 18m 36s) Loss: 0.0021(0.0006) Grad: 3846.8044  LR: 0.000004  \n",
      "Epoch: [5][500/2681] Elapsed 4m 5s (remain 17m 47s) Loss: 0.0000(0.0007) Grad: 158.3936  LR: 0.000004  \n",
      "Epoch: [5][600/2681] Elapsed 4m 53s (remain 16m 57s) Loss: 0.0001(0.0007) Grad: 493.1540  LR: 0.000003  \n",
      "Epoch: [5][700/2681] Elapsed 5m 43s (remain 16m 9s) Loss: 0.0000(0.0007) Grad: 92.3819  LR: 0.000003  \n",
      "Epoch: [5][800/2681] Elapsed 6m 32s (remain 15m 20s) Loss: 0.0000(0.0007) Grad: 194.0281  LR: 0.000003  \n",
      "Epoch: [5][900/2681] Elapsed 7m 21s (remain 14m 32s) Loss: 0.0000(0.0008) Grad: 101.4187  LR: 0.000003  \n",
      "Epoch: [5][1000/2681] Elapsed 8m 10s (remain 13m 43s) Loss: 0.0010(0.0008) Grad: 2818.0425  LR: 0.000003  \n",
      "Epoch: [5][1100/2681] Elapsed 9m 0s (remain 12m 54s) Loss: 0.0005(0.0008) Grad: 1734.0400  LR: 0.000003  \n",
      "Epoch: [5][1200/2681] Elapsed 9m 49s (remain 12m 6s) Loss: 0.0006(0.0008) Grad: 3372.3069  LR: 0.000002  \n",
      "Epoch: [5][1300/2681] Elapsed 10m 38s (remain 11m 17s) Loss: 0.0002(0.0008) Grad: 975.0446  LR: 0.000002  \n",
      "Epoch: [5][1400/2681] Elapsed 11m 27s (remain 10m 28s) Loss: 0.0013(0.0008) Grad: 3394.5559  LR: 0.000002  \n",
      "Epoch: [5][1500/2681] Elapsed 12m 16s (remain 9m 38s) Loss: 0.0037(0.0008) Grad: 7662.2285  LR: 0.000002  \n",
      "Epoch: [5][1600/2681] Elapsed 13m 4s (remain 8m 49s) Loss: 0.0010(0.0008) Grad: 6427.0327  LR: 0.000002  \n",
      "Epoch: [5][1700/2681] Elapsed 13m 54s (remain 8m 0s) Loss: 0.0000(0.0008) Grad: 47.1927  LR: 0.000002  \n",
      "Epoch: [5][1800/2681] Elapsed 14m 42s (remain 7m 11s) Loss: 0.0000(0.0008) Grad: 24.2406  LR: 0.000001  \n",
      "Epoch: [5][1900/2681] Elapsed 15m 31s (remain 6m 22s) Loss: 0.0002(0.0008) Grad: 994.1210  LR: 0.000001  \n",
      "Epoch: [5][2000/2681] Elapsed 16m 20s (remain 5m 33s) Loss: 0.0000(0.0008) Grad: 13.9445  LR: 0.000001  \n",
      "Epoch: [5][2100/2681] Elapsed 17m 9s (remain 4m 44s) Loss: 0.0087(0.0008) Grad: 21864.4316  LR: 0.000001  \n",
      "Epoch: [5][2200/2681] Elapsed 17m 57s (remain 3m 55s) Loss: 0.0012(0.0008) Grad: 3359.0303  LR: 0.000001  \n",
      "Epoch: [5][2300/2681] Elapsed 18m 46s (remain 3m 6s) Loss: 0.0000(0.0008) Grad: 217.6372  LR: 0.000001  \n",
      "Epoch: [5][2400/2681] Elapsed 19m 35s (remain 2m 17s) Loss: 0.0001(0.0008) Grad: 751.7418  LR: 0.000000  \n",
      "Epoch: [5][2500/2681] Elapsed 20m 24s (remain 1m 28s) Loss: 0.0001(0.0008) Grad: 690.9589  LR: 0.000000  \n",
      "Epoch: [5][2600/2681] Elapsed 21m 13s (remain 0m 39s) Loss: 0.0006(0.0008) Grad: 1600.2444  LR: 0.000000  \n",
      "Epoch: [5][2680/2681] Elapsed 21m 52s (remain 0m 0s) Loss: 0.0005(0.0008) Grad: 3673.7319  LR: 0.000000  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 8m 18s) Loss: 0.0002(0.0002) \n",
      "EVAL: [100/894] Elapsed 0m 24s (remain 3m 14s) Loss: 0.0002(0.0021) \n",
      "EVAL: [200/894] Elapsed 0m 49s (remain 2m 49s) Loss: 0.0002(0.0019) \n",
      "EVAL: [300/894] Elapsed 1m 13s (remain 2m 24s) Loss: 0.0001(0.0020) \n",
      "EVAL: [400/894] Elapsed 1m 37s (remain 2m 0s) Loss: 0.0000(0.0018) \n",
      "EVAL: [500/894] Elapsed 2m 2s (remain 1m 35s) Loss: 0.0001(0.0020) \n",
      "EVAL: [600/894] Elapsed 2m 26s (remain 1m 11s) Loss: 0.0000(0.0022) \n",
      "EVAL: [700/894] Elapsed 2m 51s (remain 0m 47s) Loss: 0.0123(0.0022) \n",
      "EVAL: [800/894] Elapsed 3m 15s (remain 0m 22s) Loss: 0.0003(0.0020) \n",
      "EVAL: [893/894] Elapsed 3m 38s (remain 0m 0s) Loss: 0.0000(0.0020) \n",
      "Epoch 5 - avg_train_loss: 0.0008  avg_val_loss: 0.0020  time: 1537s\n",
      "Epoch 5 - Score: 0.8842\n",
      "Epoch 5 - Save Best Score: 0.8842 Model\n",
      "========== fold: 3 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2681] Elapsed 0m 0s (remain 34m 5s) Loss: 0.0993(0.0993) Grad: 50584.2891  LR: 0.000000  \n",
      "Epoch: [1][100/2681] Elapsed 0m 50s (remain 21m 21s) Loss: 0.0775(0.0907) Grad: 20620.2773  LR: 0.000002  \n",
      "Epoch: [1][200/2681] Elapsed 1m 39s (remain 20m 26s) Loss: 0.0271(0.0718) Grad: 9046.6396  LR: 0.000003  \n",
      "Epoch: [1][300/2681] Elapsed 2m 28s (remain 19m 33s) Loss: 0.0064(0.0532) Grad: 655.2220  LR: 0.000004  \n",
      "Epoch: [1][400/2681] Elapsed 3m 17s (remain 18m 45s) Loss: 0.0037(0.0429) Grad: 1675.6052  LR: 0.000006  \n",
      "Epoch: [1][500/2681] Elapsed 4m 7s (remain 17m 56s) Loss: 0.0040(0.0363) Grad: 1330.4961  LR: 0.000007  \n",
      "Epoch: [1][600/2681] Elapsed 4m 56s (remain 17m 7s) Loss: 0.0036(0.0316) Grad: 1874.1759  LR: 0.000009  \n",
      "Epoch: [1][700/2681] Elapsed 5m 46s (remain 16m 17s) Loss: 0.0018(0.0279) Grad: 823.1157  LR: 0.000010  \n",
      "Epoch: [1][800/2681] Elapsed 6m 35s (remain 15m 27s) Loss: 0.0083(0.0249) Grad: 2845.6470  LR: 0.000012  \n",
      "Epoch: [1][900/2681] Elapsed 7m 24s (remain 14m 37s) Loss: 0.0055(0.0225) Grad: 8457.8965  LR: 0.000013  \n",
      "Epoch: [1][1000/2681] Elapsed 8m 13s (remain 13m 48s) Loss: 0.0042(0.0207) Grad: 2737.0845  LR: 0.000015  \n",
      "Epoch: [1][1100/2681] Elapsed 9m 2s (remain 12m 58s) Loss: 0.0029(0.0191) Grad: 1827.4816  LR: 0.000016  \n",
      "Epoch: [1][1200/2681] Elapsed 9m 52s (remain 12m 9s) Loss: 0.0046(0.0178) Grad: 8290.0098  LR: 0.000018  \n",
      "Epoch: [1][1300/2681] Elapsed 10m 41s (remain 11m 20s) Loss: 0.0101(0.0166) Grad: 4609.7764  LR: 0.000019  \n",
      "Epoch: [1][1400/2681] Elapsed 11m 30s (remain 10m 30s) Loss: 0.0011(0.0157) Grad: 1383.4941  LR: 0.000020  \n",
      "Epoch: [1][1500/2681] Elapsed 12m 19s (remain 9m 41s) Loss: 0.0018(0.0147) Grad: 1290.9668  LR: 0.000020  \n",
      "Epoch: [1][1600/2681] Elapsed 13m 8s (remain 8m 51s) Loss: 0.0028(0.0139) Grad: 5125.8149  LR: 0.000020  \n",
      "Epoch: [1][1700/2681] Elapsed 13m 57s (remain 8m 2s) Loss: 0.0011(0.0133) Grad: 2015.5248  LR: 0.000019  \n",
      "Epoch: [1][1800/2681] Elapsed 14m 46s (remain 7m 12s) Loss: 0.0015(0.0127) Grad: 2035.0593  LR: 0.000019  \n",
      "Epoch: [1][1900/2681] Elapsed 15m 34s (remain 6m 23s) Loss: 0.0000(0.0121) Grad: 17.6280  LR: 0.000019  \n",
      "Epoch: [1][2000/2681] Elapsed 16m 23s (remain 5m 34s) Loss: 0.0002(0.0116) Grad: 295.5858  LR: 0.000019  \n",
      "Epoch: [1][2100/2681] Elapsed 17m 12s (remain 4m 45s) Loss: 0.0001(0.0112) Grad: 166.8517  LR: 0.000019  \n",
      "Epoch: [1][2200/2681] Elapsed 18m 1s (remain 3m 55s) Loss: 0.0017(0.0108) Grad: 9452.7285  LR: 0.000019  \n",
      "Epoch: [1][2300/2681] Elapsed 18m 50s (remain 3m 6s) Loss: 0.0001(0.0104) Grad: 549.8284  LR: 0.000018  \n",
      "Epoch: [1][2400/2681] Elapsed 19m 39s (remain 2m 17s) Loss: 0.0008(0.0101) Grad: 2609.7795  LR: 0.000018  \n",
      "Epoch: [1][2500/2681] Elapsed 20m 28s (remain 1m 28s) Loss: 0.0007(0.0097) Grad: 681.3574  LR: 0.000018  \n",
      "Epoch: [1][2600/2681] Elapsed 21m 18s (remain 0m 39s) Loss: 0.0085(0.0094) Grad: 7633.7539  LR: 0.000018  \n",
      "Epoch: [1][2680/2681] Elapsed 21m 57s (remain 0m 0s) Loss: 0.0010(0.0092) Grad: 569.0696  LR: 0.000018  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 8m 16s) Loss: 0.0015(0.0015) \n",
      "EVAL: [100/894] Elapsed 0m 24s (remain 3m 14s) Loss: 0.0001(0.0017) \n",
      "EVAL: [200/894] Elapsed 0m 49s (remain 2m 49s) Loss: 0.0025(0.0024) \n",
      "EVAL: [300/894] Elapsed 1m 13s (remain 2m 24s) Loss: 0.0007(0.0024) \n",
      "EVAL: [400/894] Elapsed 1m 37s (remain 2m 0s) Loss: 0.0016(0.0021) \n",
      "EVAL: [500/894] Elapsed 2m 2s (remain 1m 35s) Loss: 0.0013(0.0022) \n",
      "EVAL: [600/894] Elapsed 2m 26s (remain 1m 11s) Loss: 0.0016(0.0022) \n",
      "EVAL: [700/894] Elapsed 2m 50s (remain 0m 47s) Loss: 0.0009(0.0021) \n",
      "EVAL: [800/894] Elapsed 3m 15s (remain 0m 22s) Loss: 0.0006(0.0020) \n",
      "EVAL: [893/894] Elapsed 3m 37s (remain 0m 0s) Loss: 0.0001(0.0019) \n",
      "Epoch 1 - avg_train_loss: 0.0092  avg_val_loss: 0.0019  time: 1542s\n",
      "Epoch 1 - Score: 0.8418\n",
      "Epoch 1 - Save Best Score: 0.8418 Model\n",
      "Epoch: [2][0/2681] Elapsed 0m 0s (remain 33m 43s) Loss: 0.0002(0.0002) Grad: 939.1401  LR: 0.000018  \n",
      "Epoch: [2][100/2681] Elapsed 0m 50s (remain 21m 25s) Loss: 0.0006(0.0018) Grad: 1594.4369  LR: 0.000018  \n",
      "Epoch: [2][200/2681] Elapsed 1m 39s (remain 20m 26s) Loss: 0.0001(0.0015) Grad: 337.1394  LR: 0.000017  \n",
      "Epoch: [2][300/2681] Elapsed 2m 28s (remain 19m 33s) Loss: 0.0003(0.0014) Grad: 1624.6509  LR: 0.000017  \n",
      "Epoch: [2][400/2681] Elapsed 3m 17s (remain 18m 44s) Loss: 0.0003(0.0015) Grad: 441.4788  LR: 0.000017  \n",
      "Epoch: [2][500/2681] Elapsed 4m 7s (remain 17m 55s) Loss: 0.0005(0.0015) Grad: 1334.8596  LR: 0.000017  \n",
      "Epoch: [2][600/2681] Elapsed 4m 56s (remain 17m 5s) Loss: 0.0002(0.0015) Grad: 647.2355  LR: 0.000017  \n",
      "Epoch: [2][700/2681] Elapsed 5m 45s (remain 16m 16s) Loss: 0.0010(0.0016) Grad: 1670.1409  LR: 0.000017  \n",
      "Epoch: [2][800/2681] Elapsed 6m 35s (remain 15m 27s) Loss: 0.0004(0.0016) Grad: 1873.1642  LR: 0.000016  \n",
      "Epoch: [2][900/2681] Elapsed 7m 24s (remain 14m 37s) Loss: 0.0002(0.0015) Grad: 868.9568  LR: 0.000016  \n",
      "Epoch: [2][1000/2681] Elapsed 8m 13s (remain 13m 48s) Loss: 0.0000(0.0015) Grad: 52.3719  LR: 0.000016  \n",
      "Epoch: [2][1100/2681] Elapsed 9m 2s (remain 12m 58s) Loss: 0.0015(0.0015) Grad: 9037.7461  LR: 0.000016  \n",
      "Epoch: [2][1200/2681] Elapsed 9m 51s (remain 12m 9s) Loss: 0.0002(0.0016) Grad: 1402.0225  LR: 0.000016  \n",
      "Epoch: [2][1300/2681] Elapsed 10m 41s (remain 11m 20s) Loss: 0.0015(0.0015) Grad: 3356.7747  LR: 0.000016  \n",
      "Epoch: [2][1400/2681] Elapsed 11m 29s (remain 10m 30s) Loss: 0.0000(0.0015) Grad: 32.9406  LR: 0.000015  \n",
      "Epoch: [2][1500/2681] Elapsed 12m 19s (remain 9m 40s) Loss: 0.0004(0.0016) Grad: 637.3397  LR: 0.000015  \n",
      "Epoch: [2][1600/2681] Elapsed 13m 8s (remain 8m 51s) Loss: 0.0011(0.0016) Grad: 3538.9211  LR: 0.000015  \n",
      "Epoch: [2][1700/2681] Elapsed 13m 57s (remain 8m 2s) Loss: 0.0026(0.0016) Grad: 8340.1025  LR: 0.000015  \n",
      "Epoch: [2][1800/2681] Elapsed 14m 46s (remain 7m 13s) Loss: 0.0004(0.0016) Grad: 1268.0831  LR: 0.000015  \n",
      "Epoch: [2][1900/2681] Elapsed 15m 35s (remain 6m 23s) Loss: 0.0002(0.0016) Grad: 349.2732  LR: 0.000015  \n",
      "Epoch: [2][2000/2681] Elapsed 16m 24s (remain 5m 34s) Loss: 0.0005(0.0015) Grad: 965.2338  LR: 0.000014  \n",
      "Epoch: [2][2100/2681] Elapsed 17m 13s (remain 4m 45s) Loss: 0.0008(0.0015) Grad: 2191.4998  LR: 0.000014  \n",
      "Epoch: [2][2200/2681] Elapsed 18m 2s (remain 3m 56s) Loss: 0.0068(0.0015) Grad: 29853.0410  LR: 0.000014  \n",
      "Epoch: [2][2300/2681] Elapsed 18m 50s (remain 3m 6s) Loss: 0.0020(0.0015) Grad: 1643.1694  LR: 0.000014  \n",
      "Epoch: [2][2400/2681] Elapsed 19m 39s (remain 2m 17s) Loss: 0.0001(0.0015) Grad: 118.5452  LR: 0.000014  \n",
      "Epoch: [2][2500/2681] Elapsed 20m 28s (remain 1m 28s) Loss: 0.0011(0.0015) Grad: 3061.1372  LR: 0.000014  \n",
      "Epoch: [2][2600/2681] Elapsed 21m 17s (remain 0m 39s) Loss: 0.0019(0.0015) Grad: 4733.0908  LR: 0.000013  \n",
      "Epoch: [2][2680/2681] Elapsed 21m 57s (remain 0m 0s) Loss: 0.0020(0.0015) Grad: 4574.0098  LR: 0.000013  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 8m 21s) Loss: 0.0006(0.0006) \n",
      "EVAL: [100/894] Elapsed 0m 24s (remain 3m 14s) Loss: 0.0001(0.0013) \n",
      "EVAL: [200/894] Elapsed 0m 49s (remain 2m 49s) Loss: 0.0039(0.0015) \n",
      "EVAL: [300/894] Elapsed 1m 13s (remain 2m 24s) Loss: 0.0008(0.0014) \n",
      "EVAL: [400/894] Elapsed 1m 37s (remain 2m 0s) Loss: 0.0036(0.0013) \n",
      "EVAL: [500/894] Elapsed 2m 2s (remain 1m 35s) Loss: 0.0026(0.0016) \n",
      "EVAL: [600/894] Elapsed 2m 26s (remain 1m 11s) Loss: 0.0027(0.0016) \n",
      "EVAL: [700/894] Elapsed 2m 50s (remain 0m 47s) Loss: 0.0004(0.0016) \n",
      "EVAL: [800/894] Elapsed 3m 15s (remain 0m 22s) Loss: 0.0006(0.0015) \n",
      "EVAL: [893/894] Elapsed 3m 38s (remain 0m 0s) Loss: 0.0000(0.0015) \n",
      "Epoch 2 - avg_train_loss: 0.0015  avg_val_loss: 0.0015  time: 1542s\n",
      "Epoch 2 - Score: 0.8760\n",
      "Epoch 2 - Save Best Score: 0.8760 Model\n",
      "Epoch: [3][0/2681] Elapsed 0m 0s (remain 40m 38s) Loss: 0.0002(0.0002) Grad: 1057.9485  LR: 0.000013  \n",
      "Epoch: [3][100/2681] Elapsed 0m 49s (remain 21m 13s) Loss: 0.0003(0.0011) Grad: 3866.4487  LR: 0.000013  \n",
      "Epoch: [3][200/2681] Elapsed 1m 38s (remain 20m 17s) Loss: 0.0041(0.0010) Grad: 6613.0659  LR: 0.000013  \n",
      "Epoch: [3][300/2681] Elapsed 2m 27s (remain 19m 25s) Loss: 0.0023(0.0010) Grad: 14847.4189  LR: 0.000013  \n",
      "Epoch: [3][400/2681] Elapsed 3m 16s (remain 18m 37s) Loss: 0.0022(0.0011) Grad: 28404.6953  LR: 0.000013  \n",
      "Epoch: [3][500/2681] Elapsed 4m 5s (remain 17m 49s) Loss: 0.0000(0.0011) Grad: 56.4754  LR: 0.000013  \n",
      "Epoch: [3][600/2681] Elapsed 4m 55s (remain 17m 2s) Loss: 0.0028(0.0010) Grad: 4667.0605  LR: 0.000012  \n",
      "Epoch: [3][700/2681] Elapsed 5m 44s (remain 16m 13s) Loss: 0.0003(0.0010) Grad: 2265.7300  LR: 0.000012  \n",
      "Epoch: [3][800/2681] Elapsed 6m 33s (remain 15m 24s) Loss: 0.0013(0.0010) Grad: 1841.4598  LR: 0.000012  \n",
      "Epoch: [3][900/2681] Elapsed 7m 22s (remain 14m 34s) Loss: 0.0024(0.0011) Grad: 13425.9316  LR: 0.000012  \n",
      "Epoch: [3][1000/2681] Elapsed 8m 12s (remain 13m 46s) Loss: 0.0030(0.0011) Grad: 12002.7461  LR: 0.000012  \n",
      "Epoch: [3][1100/2681] Elapsed 9m 1s (remain 12m 56s) Loss: 0.0012(0.0011) Grad: 3648.0591  LR: 0.000012  \n",
      "Epoch: [3][1200/2681] Elapsed 9m 50s (remain 12m 7s) Loss: 0.0008(0.0011) Grad: 3685.8823  LR: 0.000011  \n",
      "Epoch: [3][1300/2681] Elapsed 10m 39s (remain 11m 18s) Loss: 0.0061(0.0011) Grad: 21602.0859  LR: 0.000011  \n",
      "Epoch: [3][1400/2681] Elapsed 11m 28s (remain 10m 29s) Loss: 0.0001(0.0011) Grad: 387.6981  LR: 0.000011  \n",
      "Epoch: [3][1500/2681] Elapsed 12m 17s (remain 9m 39s) Loss: 0.0005(0.0011) Grad: 1210.9913  LR: 0.000011  \n",
      "Epoch: [3][1600/2681] Elapsed 13m 6s (remain 8m 50s) Loss: 0.0001(0.0011) Grad: 223.7417  LR: 0.000011  \n",
      "Epoch: [3][1700/2681] Elapsed 13m 55s (remain 8m 1s) Loss: 0.0020(0.0011) Grad: 2987.9497  LR: 0.000011  \n",
      "Epoch: [3][1800/2681] Elapsed 14m 44s (remain 7m 12s) Loss: 0.0000(0.0011) Grad: 50.9101  LR: 0.000010  \n",
      "Epoch: [3][1900/2681] Elapsed 15m 33s (remain 6m 22s) Loss: 0.0001(0.0011) Grad: 396.2784  LR: 0.000010  \n",
      "Epoch: [3][2000/2681] Elapsed 16m 22s (remain 5m 33s) Loss: 0.0008(0.0011) Grad: 4667.1641  LR: 0.000010  \n",
      "Epoch: [3][2100/2681] Elapsed 17m 11s (remain 4m 44s) Loss: 0.0008(0.0011) Grad: 1708.0624  LR: 0.000010  \n",
      "Epoch: [3][2200/2681] Elapsed 18m 0s (remain 3m 55s) Loss: 0.0005(0.0011) Grad: 1544.4708  LR: 0.000010  \n",
      "Epoch: [3][2300/2681] Elapsed 18m 49s (remain 3m 6s) Loss: 0.0005(0.0011) Grad: 1360.4421  LR: 0.000010  \n",
      "Epoch: [3][2400/2681] Elapsed 19m 38s (remain 2m 17s) Loss: 0.0003(0.0011) Grad: 7519.3091  LR: 0.000009  \n",
      "Epoch: [3][2500/2681] Elapsed 20m 27s (remain 1m 28s) Loss: 0.0021(0.0011) Grad: 10561.6895  LR: 0.000009  \n",
      "Epoch: [3][2600/2681] Elapsed 21m 16s (remain 0m 39s) Loss: 0.0007(0.0011) Grad: 2508.2700  LR: 0.000009  \n",
      "Epoch: [3][2680/2681] Elapsed 21m 56s (remain 0m 0s) Loss: 0.0006(0.0011) Grad: 1406.9297  LR: 0.000009  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 7m 45s) Loss: 0.0003(0.0003) \n",
      "EVAL: [100/894] Elapsed 0m 24s (remain 3m 14s) Loss: 0.0000(0.0014) \n",
      "EVAL: [200/894] Elapsed 0m 49s (remain 2m 49s) Loss: 0.0048(0.0016) \n",
      "EVAL: [300/894] Elapsed 1m 13s (remain 2m 24s) Loss: 0.0018(0.0016) \n",
      "EVAL: [400/894] Elapsed 1m 37s (remain 2m 0s) Loss: 0.0024(0.0015) \n",
      "EVAL: [500/894] Elapsed 2m 2s (remain 1m 35s) Loss: 0.0013(0.0017) \n",
      "EVAL: [600/894] Elapsed 2m 26s (remain 1m 11s) Loss: 0.0029(0.0017) \n",
      "EVAL: [700/894] Elapsed 2m 50s (remain 0m 47s) Loss: 0.0005(0.0017) \n",
      "EVAL: [800/894] Elapsed 3m 15s (remain 0m 22s) Loss: 0.0002(0.0017) \n",
      "EVAL: [893/894] Elapsed 3m 38s (remain 0m 0s) Loss: 0.0000(0.0016) \n",
      "Epoch 3 - avg_train_loss: 0.0011  avg_val_loss: 0.0016  time: 1541s\n",
      "Epoch 3 - Score: 0.8807\n",
      "Epoch 3 - Save Best Score: 0.8807 Model\n",
      "Epoch: [4][0/2681] Elapsed 0m 0s (remain 37m 11s) Loss: 0.0001(0.0001) Grad: 993.4490  LR: 0.000009  \n",
      "Epoch: [4][100/2681] Elapsed 0m 50s (remain 21m 20s) Loss: 0.0009(0.0008) Grad: 613.2521  LR: 0.000009  \n",
      "Epoch: [4][200/2681] Elapsed 1m 39s (remain 20m 27s) Loss: 0.0000(0.0008) Grad: 315.5488  LR: 0.000009  \n",
      "Epoch: [4][300/2681] Elapsed 2m 28s (remain 19m 30s) Loss: 0.0006(0.0008) Grad: 2363.2683  LR: 0.000008  \n",
      "Epoch: [4][400/2681] Elapsed 3m 16s (remain 18m 39s) Loss: 0.0000(0.0008) Grad: 97.7213  LR: 0.000008  \n",
      "Epoch: [4][500/2681] Elapsed 4m 5s (remain 17m 48s) Loss: 0.0000(0.0008) Grad: 130.5956  LR: 0.000008  \n",
      "Epoch: [4][600/2681] Elapsed 4m 54s (remain 16m 58s) Loss: 0.0000(0.0008) Grad: 33.0441  LR: 0.000008  \n",
      "Epoch: [4][700/2681] Elapsed 5m 43s (remain 16m 8s) Loss: 0.0001(0.0009) Grad: 445.5599  LR: 0.000008  \n",
      "Epoch: [4][800/2681] Elapsed 6m 32s (remain 15m 20s) Loss: 0.0005(0.0009) Grad: 1744.3442  LR: 0.000008  \n",
      "Epoch: [4][900/2681] Elapsed 7m 20s (remain 14m 31s) Loss: 0.0001(0.0009) Grad: 420.4903  LR: 0.000007  \n",
      "Epoch: [4][1000/2681] Elapsed 8m 9s (remain 13m 42s) Loss: 0.0000(0.0009) Grad: 29.3191  LR: 0.000007  \n",
      "Epoch: [4][1100/2681] Elapsed 8m 59s (remain 12m 54s) Loss: 0.0001(0.0009) Grad: 260.3367  LR: 0.000007  \n",
      "Epoch: [4][1200/2681] Elapsed 9m 48s (remain 12m 5s) Loss: 0.0008(0.0009) Grad: 2138.4756  LR: 0.000007  \n",
      "Epoch: [4][1300/2681] Elapsed 10m 37s (remain 11m 16s) Loss: 0.0011(0.0009) Grad: 2073.7495  LR: 0.000007  \n",
      "Epoch: [4][1400/2681] Elapsed 11m 27s (remain 10m 27s) Loss: 0.0000(0.0009) Grad: 111.0863  LR: 0.000007  \n",
      "Epoch: [4][1500/2681] Elapsed 12m 16s (remain 9m 39s) Loss: 0.0011(0.0009) Grad: 22869.9688  LR: 0.000006  \n",
      "Epoch: [4][1600/2681] Elapsed 13m 5s (remain 8m 50s) Loss: 0.0001(0.0009) Grad: 1512.7461  LR: 0.000006  \n",
      "Epoch: [4][1700/2681] Elapsed 13m 55s (remain 8m 1s) Loss: 0.0000(0.0009) Grad: 62.1992  LR: 0.000006  \n",
      "Epoch: [4][1800/2681] Elapsed 14m 44s (remain 7m 12s) Loss: 0.0001(0.0009) Grad: 1497.7693  LR: 0.000006  \n",
      "Epoch: [4][1900/2681] Elapsed 15m 33s (remain 6m 22s) Loss: 0.0012(0.0009) Grad: 3453.4656  LR: 0.000006  \n",
      "Epoch: [4][2000/2681] Elapsed 16m 22s (remain 5m 33s) Loss: 0.0008(0.0009) Grad: 1315.1506  LR: 0.000006  \n",
      "Epoch: [4][2100/2681] Elapsed 17m 12s (remain 4m 44s) Loss: 0.0010(0.0009) Grad: 2908.3577  LR: 0.000005  \n",
      "Epoch: [4][2200/2681] Elapsed 18m 1s (remain 3m 55s) Loss: 0.0000(0.0008) Grad: 63.0790  LR: 0.000005  \n",
      "Epoch: [4][2300/2681] Elapsed 18m 50s (remain 3m 6s) Loss: 0.0019(0.0009) Grad: 5150.8179  LR: 0.000005  \n",
      "Epoch: [4][2400/2681] Elapsed 19m 39s (remain 2m 17s) Loss: 0.0001(0.0009) Grad: 349.8944  LR: 0.000005  \n",
      "Epoch: [4][2500/2681] Elapsed 20m 28s (remain 1m 28s) Loss: 0.0000(0.0009) Grad: 6.4867  LR: 0.000005  \n",
      "Epoch: [4][2600/2681] Elapsed 21m 17s (remain 0m 39s) Loss: 0.0005(0.0009) Grad: 1881.4183  LR: 0.000005  \n",
      "Epoch: [4][2680/2681] Elapsed 21m 56s (remain 0m 0s) Loss: 0.0000(0.0009) Grad: 83.5759  LR: 0.000004  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 7m 53s) Loss: 0.0002(0.0002) \n",
      "EVAL: [100/894] Elapsed 0m 24s (remain 3m 14s) Loss: 0.0000(0.0016) \n",
      "EVAL: [200/894] Elapsed 0m 49s (remain 2m 49s) Loss: 0.0049(0.0017) \n",
      "EVAL: [300/894] Elapsed 1m 13s (remain 2m 24s) Loss: 0.0007(0.0016) \n",
      "EVAL: [400/894] Elapsed 1m 37s (remain 2m 0s) Loss: 0.0020(0.0015) \n",
      "EVAL: [500/894] Elapsed 2m 2s (remain 1m 35s) Loss: 0.0025(0.0018) \n",
      "EVAL: [600/894] Elapsed 2m 26s (remain 1m 11s) Loss: 0.0031(0.0019) \n",
      "EVAL: [700/894] Elapsed 2m 50s (remain 0m 47s) Loss: 0.0008(0.0019) \n",
      "EVAL: [800/894] Elapsed 3m 15s (remain 0m 22s) Loss: 0.0003(0.0019) \n",
      "EVAL: [893/894] Elapsed 3m 37s (remain 0m 0s) Loss: 0.0000(0.0018) \n",
      "Epoch 4 - avg_train_loss: 0.0009  avg_val_loss: 0.0018  time: 1540s\n",
      "Epoch 4 - Score: 0.8850\n",
      "Epoch 4 - Save Best Score: 0.8850 Model\n",
      "Epoch: [5][0/2681] Elapsed 0m 0s (remain 38m 2s) Loss: 0.0002(0.0002) Grad: 2693.1943  LR: 0.000004  \n",
      "Epoch: [5][100/2681] Elapsed 0m 49s (remain 21m 7s) Loss: 0.0002(0.0007) Grad: 1517.5427  LR: 0.000004  \n",
      "Epoch: [5][200/2681] Elapsed 1m 38s (remain 20m 14s) Loss: 0.0010(0.0008) Grad: 7462.9883  LR: 0.000004  \n",
      "Epoch: [5][300/2681] Elapsed 2m 27s (remain 19m 26s) Loss: 0.0002(0.0007) Grad: 2058.7185  LR: 0.000004  \n",
      "Epoch: [5][400/2681] Elapsed 3m 16s (remain 18m 36s) Loss: 0.0003(0.0008) Grad: 1189.3372  LR: 0.000004  \n",
      "Epoch: [5][500/2681] Elapsed 4m 5s (remain 17m 47s) Loss: 0.0007(0.0008) Grad: 490.8921  LR: 0.000004  \n",
      "Epoch: [5][600/2681] Elapsed 4m 54s (remain 16m 59s) Loss: 0.0000(0.0007) Grad: 121.3305  LR: 0.000003  \n",
      "Epoch: [5][700/2681] Elapsed 5m 43s (remain 16m 10s) Loss: 0.0001(0.0007) Grad: 873.1003  LR: 0.000003  \n",
      "Epoch: [5][800/2681] Elapsed 6m 32s (remain 15m 21s) Loss: 0.0002(0.0007) Grad: 956.6470  LR: 0.000003  \n",
      "Epoch: [5][900/2681] Elapsed 7m 21s (remain 14m 32s) Loss: 0.0010(0.0007) Grad: 1925.1514  LR: 0.000003  \n",
      "Epoch: [5][1000/2681] Elapsed 8m 10s (remain 13m 43s) Loss: 0.0002(0.0007) Grad: 1411.3422  LR: 0.000003  \n",
      "Epoch: [5][1100/2681] Elapsed 8m 59s (remain 12m 54s) Loss: 0.0000(0.0007) Grad: 6.1348  LR: 0.000003  \n",
      "Epoch: [5][1200/2681] Elapsed 9m 48s (remain 12m 4s) Loss: 0.0000(0.0007) Grad: 267.1109  LR: 0.000002  \n",
      "Epoch: [5][1300/2681] Elapsed 10m 37s (remain 11m 15s) Loss: 0.0002(0.0007) Grad: 1257.3611  LR: 0.000002  \n",
      "Epoch: [5][1400/2681] Elapsed 11m 25s (remain 10m 26s) Loss: 0.0000(0.0007) Grad: 67.2459  LR: 0.000002  \n",
      "Epoch: [5][1500/2681] Elapsed 12m 14s (remain 9m 37s) Loss: 0.0001(0.0007) Grad: 530.0501  LR: 0.000002  \n",
      "Epoch: [5][1600/2681] Elapsed 13m 3s (remain 8m 48s) Loss: 0.0000(0.0007) Grad: 308.3024  LR: 0.000002  \n",
      "Epoch: [5][1700/2681] Elapsed 13m 53s (remain 8m 0s) Loss: 0.0003(0.0007) Grad: 1798.3160  LR: 0.000002  \n",
      "Epoch: [5][1800/2681] Elapsed 14m 42s (remain 7m 11s) Loss: 0.0008(0.0007) Grad: 3260.9478  LR: 0.000001  \n",
      "Epoch: [5][1900/2681] Elapsed 15m 31s (remain 6m 22s) Loss: 0.0022(0.0007) Grad: 7742.7793  LR: 0.000001  \n",
      "Epoch: [5][2000/2681] Elapsed 16m 19s (remain 5m 33s) Loss: 0.0000(0.0007) Grad: 58.4707  LR: 0.000001  \n",
      "Epoch: [5][2100/2681] Elapsed 17m 8s (remain 4m 43s) Loss: 0.0001(0.0007) Grad: 395.1900  LR: 0.000001  \n",
      "Epoch: [5][2200/2681] Elapsed 17m 57s (remain 3m 54s) Loss: 0.0003(0.0007) Grad: 1589.6515  LR: 0.000001  \n",
      "Epoch: [5][2300/2681] Elapsed 18m 46s (remain 3m 6s) Loss: 0.0000(0.0007) Grad: 242.2778  LR: 0.000001  \n",
      "Epoch: [5][2400/2681] Elapsed 19m 35s (remain 2m 17s) Loss: 0.0003(0.0007) Grad: 7718.6221  LR: 0.000000  \n",
      "Epoch: [5][2500/2681] Elapsed 20m 23s (remain 1m 28s) Loss: 0.0000(0.0007) Grad: 9.0920  LR: 0.000000  \n",
      "Epoch: [5][2600/2681] Elapsed 21m 12s (remain 0m 39s) Loss: 0.0003(0.0007) Grad: 5653.1709  LR: 0.000000  \n",
      "Epoch: [5][2680/2681] Elapsed 21m 52s (remain 0m 0s) Loss: 0.0003(0.0007) Grad: 1601.0779  LR: 0.000000  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 8m 48s) Loss: 0.0001(0.0001) \n",
      "EVAL: [100/894] Elapsed 0m 24s (remain 3m 14s) Loss: 0.0000(0.0018) \n",
      "EVAL: [200/894] Elapsed 0m 49s (remain 2m 49s) Loss: 0.0055(0.0019) \n",
      "EVAL: [300/894] Elapsed 1m 13s (remain 2m 24s) Loss: 0.0009(0.0019) \n",
      "EVAL: [400/894] Elapsed 1m 37s (remain 2m 0s) Loss: 0.0027(0.0018) \n",
      "EVAL: [500/894] Elapsed 2m 1s (remain 1m 35s) Loss: 0.0022(0.0021) \n",
      "EVAL: [600/894] Elapsed 2m 26s (remain 1m 11s) Loss: 0.0038(0.0021) \n",
      "EVAL: [700/894] Elapsed 2m 50s (remain 0m 47s) Loss: 0.0010(0.0022) \n",
      "EVAL: [800/894] Elapsed 3m 15s (remain 0m 22s) Loss: 0.0002(0.0021) \n",
      "EVAL: [893/894] Elapsed 3m 37s (remain 0m 0s) Loss: 0.0000(0.0020) \n",
      "Epoch 5 - avg_train_loss: 0.0007  avg_val_loss: 0.0020  time: 1537s\n",
      "Epoch 5 - Score: 0.8860\n",
      "Epoch 5 - Save Best Score: 0.8860 Model\n",
      "Best thres: 0.5, Score: 0.8826\n",
      "Best thres: 0.49882812499999996, Score: 0.8826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43e981582d44309a5ea3529a0f06c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x7f669221d9e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 132, in __del__\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9d351739d8401781ae069205bc7eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4d8c5e9ab042e29b93de5060d150cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x7f669221d9e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 132, in __del__\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98392872721e4cfe9d00b1e0327bc9c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "name": "nbme-exp049.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 641.572862,
   "end_time": "2022-02-27T11:39:50.972497",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-27T11:29:09.399635",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
