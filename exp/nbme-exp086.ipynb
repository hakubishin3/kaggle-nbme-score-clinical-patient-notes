{"cells":[{"cell_type":"markdown","metadata":{"id":"national-fancy"},"source":["## References"],"id":"national-fancy"},{"cell_type":"markdown","metadata":{"id":"copyrighted-centre"},"source":["- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"],"id":"copyrighted-centre"},{"cell_type":"markdown","metadata":{"id":"imported-offset"},"source":["## Configurations"],"id":"imported-offset"},{"cell_type":"code","execution_count":1,"metadata":{"id":"complimentary-wyoming","executionInfo":{"status":"ok","timestamp":1650201182114,"user_tz":-540,"elapsed":13,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["EXP_NAME = \"nbme-exp086\"\n","ENV = \"colab\"\n","DEBUG_MODE = False\n","SUBMISSION_MODE = False"],"id":"complimentary-wyoming"},{"cell_type":"code","source":["%env TOKENIZERS_PARALLELISM=true"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y03AHjwJAlGL","executionInfo":{"status":"ok","timestamp":1650201182115,"user_tz":-540,"elapsed":12,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"b2e6ab75-c8db-4173-93a9-8e195b867980"},"id":"Y03AHjwJAlGL","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["env: TOKENIZERS_PARALLELISM=true\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"allied-circuit","executionInfo":{"status":"ok","timestamp":1650201182115,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["class CFG:\n","    env=ENV\n","    exp_name=EXP_NAME\n","    debug=DEBUG_MODE\n","    submission=SUBMISSION_MODE\n","    apex=True\n","    input_dir=None\n","    output_dir=None\n","    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n","    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n","    competition_name=\"nbme-score-clinical-patient-notes\"\n","    id_col=\"id\"\n","    target_col=\"location\"\n","    pretrained_model_name=\"microsoft/deberta-v3-large\"\n","    tokenizer=None\n","    max_len=None\n","    #pseudo_plain_path='../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl'\n","    pseudo_plain_path=\"./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\"\n","    n_pseudo_labels=100000\n","    output_dim=1\n","    dropout=0.2\n","    num_workers=4\n","    batch_size=3\n","    lr=2e-5\n","    betas=(0.9, 0.98)\n","    weight_decay=0.1\n","    alpha=1\n","    gamma=2\n","    smoothing=0.0001\n","    num_warmup_steps_rate=0.1\n","    batch_scheduler=True\n","    epochs=1\n","    n_fold=4\n","    train_fold=[1]\n","    seed=71\n","    gradient_accumulation_steps=2\n","    max_grad_norm=1000\n","    print_freq=100\n","    train=True\n","    inference=True"],"id":"allied-circuit"},{"cell_type":"code","execution_count":4,"metadata":{"id":"geographic-hindu","executionInfo":{"status":"ok","timestamp":1650201182116,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.train_fold = [0, 1]\n","\n","if CFG.submission:\n","    CFG.train = False\n","    CFG.inference = True"],"id":"geographic-hindu"},{"cell_type":"markdown","metadata":{"id":"confident-fifth"},"source":["## Directory Settings"],"id":"confident-fifth"},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"miniature-greeting","outputId":"f7611b77-5598-4b82-82a8-adf0825980ad","executionInfo":{"status":"ok","timestamp":1650201222616,"user_tz":-540,"elapsed":40509,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["colab\n","Mounted at /content/drive\n","Collecting transformers==4.16.2\n","  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 16.0 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2019.12.20)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (1.21.5)\n","Collecting tokenizers!=0.11.3,>=0.10.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 76.3 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.64.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 8.3 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.11.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (21.3)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 60.6 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 61.2 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (3.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.16.2) (3.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.16.2) (3.8.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (1.1.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.16.2\n","\u001b[K     |████████████████████████████████| 1.2 MB 14.9 MB/s \n","\u001b[?25h"]}],"source":["import sys\n","from pathlib import Path\n","\n","\n","print(CFG.env)\n","if CFG.env == \"colab\":\n","    # colab環境\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","    # install packages\n","    !pip install transformers==4.16.2\n","    !pip install -q sentencepiece==0.1.96\n","\n","elif CFG.env == \"local\":\n","    # ローカルサーバ\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","\n","elif CFG.env == \"kaggle\":\n","    # kaggle環境\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./\")"],"id":"miniature-greeting"},{"cell_type":"code","source":["# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n","# This must be done before importing transformers\n","import shutil\n","from pathlib import Path\n","\n","if CFG.env == \"colab\":\n","    input_dir = Path(\"./drive/MyDrive/00.kaggle/input/deberta-v2-3-fast-tokenizer\")\n","    transformers_path = Path(\"/usr/local/lib/python3.7/dist-packages/transformers\")\n","else:\n","    input_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n","    transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n","\n","convert_file = input_dir / \"convert_slow_tokenizer.py\"\n","conversion_path = transformers_path/convert_file.name\n","\n","if conversion_path.exists():\n","    conversion_path.unlink()\n","\n","shutil.copy(convert_file, transformers_path)\n","deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n","\n","for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n","    filepath = deberta_v2_path/filename\n","    if filepath.exists():\n","        filepath.unlink()\n","\n","    shutil.copy(input_dir/filename, filepath)\n","    \n","    \n","from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast"],"metadata":{"id":"nMFg9zv8YGcx","executionInfo":{"status":"ok","timestamp":1650201232963,"user_tz":-540,"elapsed":10361,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"id":"nMFg9zv8YGcx","execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"guilty-filename","executionInfo":{"status":"ok","timestamp":1650201234321,"user_tz":-540,"elapsed":1361,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["import gc\n","import os\n","import ast\n","import time\n","import math\n","import random\n","import itertools\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","from scipy.optimize import minimize\n","from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision.transforms as T\n","from torchvision.io import read_image\n","from torch.utils.data import DataLoader, Dataset\n","\n","from transformers import AutoModelForMaskedLM\n","from transformers import BartModel,BertModel,BertTokenizer\n","from transformers import DebertaModel,DebertaTokenizer\n","from transformers import RobertaModel,RobertaTokenizer\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"id":"guilty-filename"},{"cell_type":"markdown","metadata":{"id":"cubic-designation"},"source":["## Utilities"],"id":"cubic-designation"},{"cell_type":"code","execution_count":8,"metadata":{"id":"opposite-plasma","executionInfo":{"status":"ok","timestamp":1650201234321,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on binary arrays.\n","\n","    Args:\n","        preds (list of lists of ints): Predictions.\n","        truths (list of lists of ints): Ground truths.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    # Micro : aggregating over all instances\n","    preds = np.concatenate(preds)\n","    truths = np.concatenate(truths)\n","    return f1_score(truths, preds)\n","\n","\n","def spans_to_binary(spans, length=None):\n","    \"\"\"\n","    Converts spans to a binary array indicating whether each character is in the span.\n","\n","    Args:\n","        spans (list of lists of two ints): Spans.\n","\n","    Returns:\n","        np array [length]: Binarized spans.\n","    \"\"\"\n","    length = np.max(spans) if length is None else length\n","    binary = np.zeros(length)\n","    for start, end in spans:\n","        binary[start:end] = 1\n","    return binary\n","\n","\n","def span_micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on spans.\n","\n","    Args:\n","        preds (list of lists of two ints): Prediction spans.\n","        truths (list of lists of two ints): Ground truth spans.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    bin_preds = []\n","    bin_truths = []\n","    for pred, truth in zip(preds, truths):\n","        if not len(pred) and not len(truth):\n","            continue\n","        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n","        bin_preds.append(spans_to_binary(pred, length))\n","        bin_truths.append(spans_to_binary(truth, length))\n","    return micro_f1(bin_preds, bin_truths)\n","\n","\n","def get_score(y_true, y_pred):\n","    score = span_micro_f1(y_true, y_pred)\n","    return score"],"id":"opposite-plasma"},{"cell_type":"code","execution_count":9,"metadata":{"id":"multiple-poland","executionInfo":{"status":"ok","timestamp":1650201234322,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def create_labels_for_scoring(df):\n","    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n","    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n","    for i in range(len(df)):\n","        lst = df.loc[i, \"location\"]\n","        if lst:\n","            new_lst = \";\".join(lst)\n","            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n","\n","    # create labels\n","    truths = []\n","    for location_list in df[\"location_for_create_labels\"].values:\n","        truth = []\n","        if len(location_list) > 0:\n","            location = location_list[0]\n","            for loc in [s.split() for s in location.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                truth.append([start, end])\n","        truths.append(truth)\n","\n","    return truths\n","\n","\n","def get_char_probs(texts, token_probs, tokenizer):\n","    res = [np.zeros(len(t)) for t in texts]\n","    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n","        encoded = tokenizer(\n","            text=text,\n","            max_length=CFG.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n","            start, end = offset_mapping\n","            res[i][start:end] = pred\n","    return res\n","\n","\n","def get_predicted_location_str(char_probs, th=0.5):\n","    results = []\n","    for char_prob in char_probs:\n","        # result = np.where(char_prob >= th)[0] + 1\n","        result = np.where(char_prob >= th)[0]\n","        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n","        # result = [f\"{min(r)} {max(r)}\" for r in result]\n","        result = [f\"{min(r)} {max(r) + 1}\" for r in result]\n","        result = \";\".join(result)\n","        results.append(result)\n","    return results\n","\n","\n","def get_predictions(results):\n","    predictions = []\n","    for result in results:\n","        prediction = []\n","        if result != \"\":\n","            for loc in [s.split() for s in result.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                prediction.append([start, end])\n","        predictions.append(prediction)\n","    return predictions\n","\n","\n","def scoring(df, th=0.5, use_token_prob=True):\n","    labels = create_labels_for_scoring(df)\n","\n","    if use_token_prob:\n","        token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n","        char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n","    else:\n","        char_probs = df[[str(i) for i in range(CFG.max_char_len)]].values\n","        char_probs = [char_probs[i] for i in range(len(char_probs))]\n","\n","    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n","    preds = get_predictions(predicted_location_str)\n","\n","    score = get_score(labels, preds)\n","    return score\n","\n","\n","def get_best_thres(oof_df):\n","    def f1_opt(x):\n","        return -1 * scoring(oof_df, th=x)\n","\n","    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n","    return best_thres"],"id":"multiple-poland"},{"cell_type":"code","execution_count":10,"metadata":{"id":"seventh-fighter","executionInfo":{"status":"ok","timestamp":1650201234323,"user_tz":-540,"elapsed":8,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return \"%dm %ds\" % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"],"id":"seventh-fighter"},{"cell_type":"code","execution_count":11,"metadata":{"id":"fifty-boundary","executionInfo":{"status":"ok","timestamp":1650201234323,"user_tz":-540,"elapsed":8,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["seed_everything()"],"id":"fifty-boundary"},{"cell_type":"markdown","metadata":{"id":"unlimited-hotel"},"source":["## Data Loading"],"id":"unlimited-hotel"},{"cell_type":"code","execution_count":12,"metadata":{"id":"classical-machine","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650201236472,"user_tz":-540,"elapsed":2156,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"1b996516-2597-421a-fcc1-b3027143e94b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14300, 6), (143, 3), (42146, 3), (5, 4))"]},"metadata":{},"execution_count":12}],"source":["train = pd.read_csv(CFG.input_dir / \"train.csv\")\n","features = pd.read_csv(CFG.input_dir / \"features.csv\")\n","patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n","test = pd.read_csv(CFG.input_dir / \"test.csv\")\n","\n","train.shape, features.shape, patient_notes.shape, test.shape"],"id":"classical-machine"},{"cell_type":"code","execution_count":13,"metadata":{"id":"vanilla-iceland","executionInfo":{"status":"ok","timestamp":1650201236473,"user_tz":-540,"elapsed":4,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["if CFG.debug:\n","    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n","    print(train.shape)"],"id":"vanilla-iceland"},{"cell_type":"markdown","metadata":{"id":"convenient-plant"},"source":["## Preprocessing"],"id":"convenient-plant"},{"cell_type":"code","execution_count":14,"metadata":{"id":"convertible-thunder","executionInfo":{"status":"ok","timestamp":1650201236473,"user_tz":-540,"elapsed":3,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def preprocess_features(features):\n","    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n","    return features\n","\n","\n","features = preprocess_features(features)"],"id":"convertible-thunder"},{"cell_type":"code","source":["features['feature_text'] = features['feature_text'].str.lower()\n","patient_notes['pn_history'] = patient_notes['pn_history'].str.lower()"],"metadata":{"id":"a7YBS_idYKtL","executionInfo":{"status":"ok","timestamp":1650201236826,"user_tz":-540,"elapsed":356,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"id":"a7YBS_idYKtL","execution_count":15,"outputs":[]},{"cell_type":"code","execution_count":16,"metadata":{"id":"charitable-memphis","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650201236827,"user_tz":-540,"elapsed":8,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"099079e2-5be7-4fb4-e72a-09c847dbb9be"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14300, 8), (5, 6))"]},"metadata":{},"execution_count":16}],"source":["train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","\n","train.shape, test.shape"],"id":"charitable-memphis"},{"cell_type":"code","execution_count":17,"metadata":{"id":"governing-election","executionInfo":{"status":"ok","timestamp":1650201237163,"user_tz":-540,"elapsed":339,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n","train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"],"id":"governing-election"},{"cell_type":"code","execution_count":18,"metadata":{"id":"negative-provincial","colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"status":"ok","timestamp":1650201237163,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"46024fb1-f21e-40d1-9771-78e33604707f"},"outputs":[{"output_type":"display_data","data":{"text/plain":["0    4399\n","1    8181\n","2    1296\n","3     287\n","4      99\n","5      27\n","6       9\n","7       1\n","8       1\n","Name: annotation_length, dtype: int64"]},"metadata":{}}],"source":["train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n","display(train['annotation_length'].value_counts().sort_index())"],"id":"negative-provincial"},{"cell_type":"markdown","metadata":{"id":"arbitrary-beatles"},"source":["## CV split"],"id":"arbitrary-beatles"},{"cell_type":"code","execution_count":19,"metadata":{"id":"important-murray","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1650201237164,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"9e5b6510-8f3e-4dab-9f2e-4eb5dc6adbf6"},"outputs":[{"output_type":"display_data","data":{"text/plain":["fold\n","0    3575\n","1    3575\n","2    3575\n","3    3575\n","dtype: int64"]},"metadata":{}}],"source":["Fold = GroupKFold(n_splits=CFG.n_fold)\n","groups = train['pn_num'].values\n","for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n","    train.loc[val_index, 'fold'] = int(n)\n","train['fold'] = train['fold'].astype(int)\n","display(train.groupby('fold').size())"],"id":"important-murray"},{"cell_type":"markdown","metadata":{"id":"configured-chemistry"},"source":["## Setup tokenizer"],"id":"configured-chemistry"},{"cell_type":"code","execution_count":20,"metadata":{"id":"hindu-contest","colab":{"base_uri":"https://localhost:8080/","height":168,"referenced_widgets":["ac860605be6d4ccb80defbef465af2cf","d738411dbae4413fabbe9e58623623d2","1022bca3b4c14cb59584db471d6b6567","dff17845069e4dbe981186360646257e","a7297ea57e944ef299a9e4191b7eedb9","1fc04917e4cf4ea39aa153271a9175ca","6bd95d5cb94243bca44933ca75ef76cf","474304937e824ef4ac313a3add81b9d4","7c6b58242f81489184b58ea972d49af3","9bc718c584934caab95653139f5a4f1c","59949601219640d496830cdde9dd7105","2835be046e6743349b6a6aa83242dced","fce3cb20a54a4b748046031c5f23228d","be4044df99114b5796d225d8b93086fd","4dc5f7fd6b8548b6b473267bb677155a","2efb2ba8d65540cba144906bfced60a0","e29e07a7200645c8ace1fdd8123f42b9","9636559332e841969e13834a4e12ec9b","f7bb76f0b82e4f3cb8e167b8de7be09e","0334e068282947d19fabff46286630ec","ced6fc3b543c41da9159f653d6d6a8bb","ce81d8df9c104dffa1f371b33ebe3b1c","e52eb5ac96df49b5ad5e368afb1b9769","849895b02b56453e8e88640459ec9fc9","02fc1fa8472a4f2082543ea4fd5b5dcd","29b2a8b227e94a0d9eb0e38d1c316118","ee54527400a74635a3c41f36de397a4e","b4df97bc057c490890ac72f16d42e1f3","33052307133d47adb3d59e7de79e3b8e","4adc6c819b404afe95d4a52401a7156f","2de3b3b5335c42938db7a9f7243143f8","49a1e9b98f6d46a6a4a3fc4717703300","9ed0ff37e0cf4afe98f6b4333548eb0e"]},"executionInfo":{"status":"ok","timestamp":1650201245167,"user_tz":-540,"elapsed":8009,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"10809609-726a-49cd-aab3-6e8764e4e977"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac860605be6d4ccb80defbef465af2cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/2.35M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2835be046e6743349b6a6aa83242dced"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/580 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e52eb5ac96df49b5ad5e368afb1b9769"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["if CFG.submission:\n","    tokenizer = DebertaV2TokenizerFast.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n","else:\n","    tokenizer = DebertaV2TokenizerFast.from_pretrained(CFG.pretrained_model_name)\n","    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n","\n","CFG.tokenizer = tokenizer"],"id":"hindu-contest"},{"cell_type":"markdown","metadata":{"id":"alleged-protein"},"source":["## Create dataset"],"id":"alleged-protein"},{"cell_type":"code","execution_count":21,"metadata":{"id":"composed-stroke","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["5a93876fd9244b8594dd5472b0abf0b6","1b5187839977435ba3cbf35c80fb1c8a","e3c4faf26d2a41cf93fa0995e40d3dcd","1175cce67a6942728349a9a228a2ba5c","f14079b0364a4a3d82f175682eb42e9d","e74a8c230b2247a0bc83fe351933d04f","18cf8bc82c4f4d888f8c800839e4f1b7","5978915477c24dd9961d340739f874ce","ac54243a971746938f04112d635652d0","f8a80376aaff4e798d9ce9783856943b","6bc98b8d28a044928883739d73ee2ee7"]},"executionInfo":{"status":"ok","timestamp":1650201267295,"user_tz":-540,"elapsed":22144,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"745aed35-78d4-4a95-f4b5-89f20dcc2e84"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/42146 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a93876fd9244b8594dd5472b0abf0b6"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 284\n"]}],"source":["pn_history_lengths = []\n","tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    pn_history_lengths.append(length)\n","\n","print(\"max length:\", np.max(pn_history_lengths))"],"id":"composed-stroke"},{"cell_type":"code","execution_count":22,"metadata":{"id":"emotional-region","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["c6da57033d304960ad821287817f0144","e967f7ec4b96484997f9efe328653ddb","c0f663a411ba4349bfa0cd1c1f33582d","a20b4ace20f04bcd882b27be2b958769","fd74c58f0d9b4d05aa10e196962eadd4","63ff4c660b0b4198b45e469abde02303","7ea04854b5924b8ca46a9db186c164f9","a5dd4d2689564936b6a6a7e04e554af1","e65218f1de5243d8a074129e8943c08b","4d6b152d5ea54929933ead2cad6054a2","c5989a8f87ba442d9af7069b66e0c3ab"]},"executionInfo":{"status":"ok","timestamp":1650201267296,"user_tz":-540,"elapsed":16,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"b93fe655-d667-4320-fa1d-d2f8bdc37881"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/143 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6da57033d304960ad821287817f0144"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 28\n"]}],"source":["feature_text_lengths = []\n","tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    feature_text_lengths.append(length)\n","\n","print(\"max length:\", np.max(feature_text_lengths))"],"id":"emotional-region"},{"cell_type":"code","execution_count":23,"metadata":{"id":"wrong-leisure","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650201267296,"user_tz":-540,"elapsed":13,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"03eed666-71bf-4195-a20f-614efdb292f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["max length: 315\n"]}],"source":["CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n","\n","print(\"max length:\", CFG.max_len)"],"id":"wrong-leisure"},{"cell_type":"code","execution_count":24,"metadata":{"id":"convenient-gospel","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["50cc101e9e704b23acd7b8bc85c6ab70","19b460e2f8f6424aac3b60b32717a95f","9ca2e0ebb76a4a14ae5f55d7860fb96b","77b223f37a9e440db5cea798d0b95ec6","282ad1b60a2b4638a19618382d4e2745","668dedf9daf942e28b04c411d21c7a0b","fab427187a1641048544394ab2d9eb76","36d9c157700545db86375e7b1dc7a493","ab04f8ecac424a00966604bf9c5c9d16","4e6e58633efb4e5ca3dc5d68e058fd7f","bed8714f95f54ba7a7ac9e28bb8bd328"]},"executionInfo":{"status":"ok","timestamp":1650201267297,"user_tz":-540,"elapsed":11,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"a556218a-8097-48bf-ca03-4045b74e63c6"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/42146 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50cc101e9e704b23acd7b8bc85c6ab70"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 950\n"]}],"source":["pn_history_lengths = []\n","tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n","for text in tk0:\n","    length = len(text)\n","    pn_history_lengths.append(length)\n","\n","CFG.max_char_len = max(pn_history_lengths)\n","\n","print(\"max length:\", CFG.max_char_len)"],"id":"convenient-gospel"},{"cell_type":"code","execution_count":25,"metadata":{"id":"representative-contributor","executionInfo":{"status":"ok","timestamp":1650201267297,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["class TrainingDataset(Dataset):\n","    def __init__(self, cfg, df, pseudo_label=None):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.max_char_len = self.cfg.max_char_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","        self.annotation_lengths = self.df[\"annotation_length\"].values\n","        self.locations = self.df[\"location\"].values\n","        if \"pseudo_idx\" in df.columns:\n","            self.pseudo_idx = self.df[\"pseudo_idx\"].values\n","            self.pseudo_label = pseudo_label\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def _create_mapping_from_token_to_char(self, pn_history):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        mapping_from_token_to_char = np.zeros(self.max_char_len)\n","        offset_mapping = encoded[\"offset_mapping\"]\n","        for i, offset in enumerate(offset_mapping):\n","            start_idx, end_idx = offset\n","            mapping_from_token_to_char[start_idx:end_idx] = i\n","        return torch.tensor(mapping_from_token_to_char, dtype=torch.long)\n","\n","    def _create_label(self, pn_history, annotation_length, location_list):\n","        label = np.zeros(self.max_char_len)\n","        label[len(pn_history):] = -1\n","        if annotation_length > 0:\n","            for location in location_list:\n","                for loc in [s.split() for s in location.split(\";\")]:\n","                    start, end = int(loc[0]), int(loc[1])\n","                    label[start:end] = 1\n","        return torch.tensor(label, dtype=torch.float)\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        if not np.isnan(self.annotation_lengths[idx]):\n","            label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n","        else:\n","            p_idx = int(self.pseudo_idx[idx])\n","            label = torch.tensor(self.pseudo_label[p_idx], dtype=torch.float)\n","        mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n","        return input_, label, mapping_from_token_to_char"],"id":"representative-contributor"},{"cell_type":"code","execution_count":26,"metadata":{"id":"decent-johnson","executionInfo":{"status":"ok","timestamp":1650201267297,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["class TestDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.max_char_len = self.cfg.max_char_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def _create_mapping_from_token_to_char(self, pn_history):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        mapping_from_token_to_char = np.zeros(self.max_char_len)\n","        offset_mapping = encoded[\"offset_mapping\"]\n","        for i, offset in enumerate(offset_mapping):\n","            start_idx, end_idx = offset\n","            mapping_from_token_to_char[start_idx:end_idx] = i\n","        return torch.tensor(mapping_from_token_to_char, dtype=torch.long)\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n","        return input_, mapping_from_token_to_char"],"id":"decent-johnson"},{"cell_type":"markdown","metadata":{"id":"arctic-joint"},"source":["## Model"],"id":"arctic-joint"},{"cell_type":"code","source":["from transformers.modeling_outputs import MaskedLMOutput\n","\n","class MaskedModel(nn.Module):\n","    def __init__(self, cfg, config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","        if config_path is None:\n","            self.config = AutoConfig.from_pretrained(\n","                cfg.pretrained_model_name,\n","                output_hidden_states=False\n","                )\n","        else:\n","            self.config = torch.load(config_path)\n","        \n","        if pretrained:\n","            self.model = AutoModel.from_pretrained(cfg.pretrained_model_name, config=self.config)\n","            self.lm_head = AutoModelForMaskedLM.from_pretrained(cfg.pretrained_model_name, config=self.config).cls # [cls, lm_head]\n","        else:\n","            self.model = AutoModel(self.config)\n","            self.lm_head = AutoModelForMaskedLM(self.config).cls # [cls, lm_head]\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def forward(\n","            self, \n","            input_ids=None,\n","            attention_mask=None,\n","            token_type_ids=None,\n","            #position_ids=None,\n","            inputs_embeds=None,\n","            labels=None,\n","            output_attentions=None,\n","            output_hidden_states=None,\n","            return_dict=None):\n","        \n","        outputs = self.model(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            #position_ids=position_ids,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,)\n","        \n","        sequence_output = outputs[0]\n","        prediction_scores = self.lm_head(sequence_output)\n","\n","        masked_lm_loss = None\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss()\n","            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n","\n","        return MaskedLMOutput(loss=masked_lm_loss,\n","                              logits=prediction_scores,\n","                              hidden_states=outputs.hidden_states,\n","                              attentions=outputs.attentions)"],"metadata":{"id":"qTRu8eKOTlcX","executionInfo":{"status":"ok","timestamp":1650201267298,"user_tz":-540,"elapsed":10,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"id":"qTRu8eKOTlcX","execution_count":27,"outputs":[]},{"cell_type":"code","source":["class Exp075Model(nn.Module):\n","    def __init__(self, cfg, model_config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        if model_config_path is None:\n","            self.model_config = AutoConfig.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                output_hidden_states=True,\n","            )\n","        else:\n","            self.model_config = torch.load(model_config_path)\n","\n","        if pretrained:\n","            self.backbone = AutoModel.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                config=self.model_config,\n","            )\n","            print(f\"Load weight from pretrained\")\n","        else:\n","            #self.backbone = AutoModel.from_config(self.model_config)\n","            # itpt = AutoModelForMaskedLM.from_config(self.model_config)\n","            #path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp010/checkpoint-130170/pytorch_model.bin\")\n","            # path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n","            # state_dict = torch.load(path)\n","            # itpt.load_state_dict(state_dict)\n","            path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\")\n","            masked_model = MaskedModel(CFG, config_path=None, pretrained=True)\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            masked_model.load_state_dict(state)\n","            self.backbone = masked_model.model\n","            print(f\"Load weight from {path}\")\n","\n","        self.fc = nn.Sequential(\n","            nn.Dropout(self.cfg.dropout),\n","            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n","        )\n","\n","    def forward(self, inputs):\n","        h = self.backbone(**inputs)[\"last_hidden_state\"]\n","        output = self.fc(h)\n","        return output"],"metadata":{"id":"vMus_m69sfJZ","executionInfo":{"status":"ok","timestamp":1650201267298,"user_tz":-540,"elapsed":10,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"id":"vMus_m69sfJZ","execution_count":28,"outputs":[]},{"cell_type":"code","source":["class CustomModel(nn.Module):\n","    def __init__(self, cfg, model_config_path=None, pretrained=False, i_fold=None):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        if model_config_path is None:\n","            self.model_config = AutoConfig.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                output_hidden_states=True,\n","            )\n","        else:\n","            self.model_config = torch.load(model_config_path)\n","\n","        if pretrained:\n","            self.backbone = AutoModel.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                config=self.model_config,\n","            )\n","            print(f\"Load weight from pretrained\")\n","        else:\n","            #self.backbone = AutoModel.from_config(self.model_config)\n","\n","            model = Exp075Model(cfg, model_config_path=None, pretrained=False)\n","            path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp075\" /  f\"fold{i_fold}_best.pth\")\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            model.load_state_dict(state[\"model\"])\n","            self.backbone = model.backbone\n","            print(f\"Load weight from {path}\")\n","\n","        self.lstm = nn.GRU(\n","            input_size=self.model_config.hidden_size,\n","            bidirectional=True,\n","            hidden_size=self.model_config.hidden_size // 2,\n","            num_layers=4,\n","            dropout=self.cfg.dropout,\n","            batch_first=True,\n","        )\n","        self.fc = nn.Sequential(\n","            nn.Dropout(self.cfg.dropout),\n","            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n","        )\n","\n","    def forward(self, inputs, mappings_from_token_to_char):\n","        h = self.backbone(**inputs)[\"last_hidden_state\"]  # [batch, seq_len, d_model]\n","        mappings_from_token_to_char = mappings_from_token_to_char.unsqueeze(2).expand(-1, -1, self.model_config.hidden_size)\n","        h = torch.gather(h, 1, mappings_from_token_to_char)    # [batch, seq_len, d_model]\n","        h, _ = self.lstm(h)\n","        output = self.fc(h)\n","\n","        return output"],"metadata":{"id":"OJt_cHeyTmDS","executionInfo":{"status":"ok","timestamp":1650201267787,"user_tz":-540,"elapsed":498,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"id":"OJt_cHeyTmDS","execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"therapeutic-assembly"},"source":["## Training"],"id":"therapeutic-assembly"},{"cell_type":"code","execution_count":30,"metadata":{"id":"going-conversion","executionInfo":{"status":"ok","timestamp":1650201267787,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def train_fn(\n","    train_dataloader,\n","    model,\n","    criterion,\n","    optimizer,\n","    epoch,\n","    scheduler,\n","    device,\n","):\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels, mappings_from_token_to_char) in enumerate(train_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device) \n","        batch_size = labels.size(0)\n","        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            output = model(inputs, mappings_from_token_to_char)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","        loss = loss.mean()\n","\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","\n","        if CFG.batch_scheduler:\n","            scheduler.step()\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n","            print(\n","                \"Epoch: [{0}][{1}/{2}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                \"Grad: {grad_norm:.4f}  \"\n","                \"LR: {lr:.6f}  \"\n","                .format(\n","                    epoch+1,\n","                    step,\n","                    len(train_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n","                    loss=losses,\n","                     grad_norm=grad_norm,\n","                     lr=scheduler.get_lr()[0],\n","                )\n","            )\n","    del output, loss, inputs, labels, mappings_from_token_to_char, scaler, grad_norm; gc.collect()\n","    torch.cuda.empty_cache()\n","    return losses.avg"],"id":"going-conversion"},{"cell_type":"code","execution_count":31,"metadata":{"id":"alleged-commonwealth","executionInfo":{"status":"ok","timestamp":1650201267788,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def valid_fn(\n","    val_dataloader,\n","    model,\n","    criterion,\n","    device,\n","):\n","    model.eval()\n","    preds = []\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels, mappings_from_token_to_char) in enumerate(val_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device) \n","        batch_size = labels.size(0)\n","        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            output = model(inputs, mappings_from_token_to_char)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","        loss = loss.mean()\n","    \n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n","            print(\n","                \"EVAL: [{0}/{1}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                .format(\n","                    step, len(val_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n","                    loss=losses,\n","                )\n","            )\n","    preds = np.concatenate(preds)\n","    return losses.avg, preds"],"id":"alleged-commonwealth"},{"cell_type":"code","execution_count":32,"metadata":{"id":"middle-determination","executionInfo":{"status":"ok","timestamp":1650201267788,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def inference_fn(test_dataloader, model, device):\n","    model.eval()\n","    model.to(device)\n","    preds = []\n","    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n","    for (inputs, mappings_from_token_to_char) in tk0:\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n","\n","        with torch.no_grad():\n","            output = model(inputs, mappings_from_token_to_char)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","    preds = np.concatenate(preds)\n","    return preds"],"id":"middle-determination"},{"cell_type":"code","execution_count":33,"metadata":{"id":"familiar-participation","executionInfo":{"status":"ok","timestamp":1650201267788,"user_tz":-540,"elapsed":5,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def train_loop(df, i_fold, device):\n","    print(f\"========== fold: {i_fold} training ==========\")\n","    train_idx = df[df[\"fold\"] != i_fold].index\n","    val_idx = df[df[\"fold\"] == i_fold].index\n","\n","    train_folds = df.loc[train_idx].reset_index(drop=True)\n","    val_folds = df.loc[val_idx].reset_index(drop=True)\n","\n","    if CFG.pseudo_plain_path is not None:\n","        pseudo_plain = pd.read_pickle(CFG.pseudo_plain_path)\n","        print(f\"get pseudo plain from {CFG.pseudo_plain_path}\")\n","        pseudo_label_list = []\n","        for exp_name in [\"nbme-exp083\"]:\n","            pseudo_label_path = f'./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/{exp_name}/pseudo_labels_{i_fold}.npy'\n","            pseudo_label = np.load(pseudo_label_path)\n","            print(f\"get pseudo labels from {pseudo_label_path}\")\n","            pseudo_label_list.append(pseudo_label)\n","    \n","        pseudo_label = 1.0 * pseudo_label_list[0]\n","        print(pseudo_plain.shape, pseudo_label.shape)\n","\n","        pseudo_plain['feature_text'] = pseudo_plain['feature_text'].str.lower()\n","        pseudo_plain['pn_history'] = pseudo_plain['pn_history'].str.lower()\n","\n","        pseudo_plain[\"pseudo_idx\"] = np.arange(len(pseudo_plain))\n","        pseudo_plain = pseudo_plain.sample(n=CFG.n_pseudo_labels)\n","        print(pseudo_plain.shape)\n","        train_folds = pd.concat([train_folds, pseudo_plain], axis=0, ignore_index=True)\n","        print(train_folds.shape)\n","\n","    train_dataset = TrainingDataset(CFG, train_folds, pseudo_label)\n","    val_dataset = TrainingDataset(CFG, val_folds)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=True,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=True,\n","    )\n","    val_dataloader = DataLoader(\n","        val_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=False,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=False,\n","    )\n","\n","    # model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","    model = CustomModel(CFG, model_config_path=None, pretrained=False, i_fold=i_fold)\n","    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n","    model.to(device)\n","\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\"params\": [p for n, p in param_optimizer if not any(\n","            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n","        {\"params\": [p for n, p in param_optimizer if any(\n","            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n","    ]\n","    optimizer = AdamW(\n","        optimizer_grouped_parameters,\n","        lr=CFG.lr,\n","        betas=CFG.betas,\n","        weight_decay=CFG.weight_decay,\n","    )\n","    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n","    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        num_training_steps=num_train_optimization_steps,\n","    )\n","\n","    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    best_score = -1 * np.inf\n","\n","    for epoch in range(CFG.epochs):\n","        start_time = time.time()\n","        avg_loss = train_fn(\n","            train_dataloader,\n","            model,\n","            criterion,\n","            optimizer,\n","            epoch,\n","            scheduler,\n","            device,\n","        )\n","        avg_val_loss, val_preds = valid_fn(\n","            val_dataloader,\n","            model,\n","            criterion,\n","            device,\n","        )\n","\n","        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n","            scheduler.step()\n","\n","        # scoring\n","        val_folds[[str(i) for i in range(CFG.max_char_len)]] = val_preds\n","        score = scoring(val_folds, th=0.5, use_token_prob=False)\n","\n","        elapsed = time.time() - start_time\n","\n","        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n","        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n","        if score > best_score:\n","            best_score = score\n","            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n","            torch.save({\n","                \"model\": model.state_dict(),\n","                \"predictions\": val_preds,\n","                },\n","                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","            )\n","\n","    predictions = torch.load(\n","        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","        map_location=torch.device(\"cpu\"),\n","    )[\"predictions\"]\n","    val_folds[[str(i) for i in range(CFG.max_char_len)]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return val_folds"],"id":"familiar-participation"},{"cell_type":"markdown","metadata":{"id":"coated-cameroon"},"source":["## Main"],"id":"coated-cameroon"},{"cell_type":"code","execution_count":34,"metadata":{"id":"quality-expansion","executionInfo":{"status":"ok","timestamp":1650201267789,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def main():\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    if CFG.train:\n","        oof_df = pd.DataFrame()\n","        for i_fold in range(CFG.n_fold):\n","            if i_fold in CFG.train_fold:\n","                _oof_df = train_loop(train, i_fold, device)\n","                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n","        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    if CFG.submission:\n","        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n","    else:\n","        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    best_thres = 0.5\n","    best_score = 0.\n","    for th in np.arange(0.45, 0.55, 0.01):\n","        th = np.round(th, 2)\n","        score = scoring(oof_df, th=th, use_token_prob=False)\n","        if best_score < score:\n","            best_thres = th\n","            best_score = score\n","    print(f\"best_thres: {best_thres}  score: {best_score:.5f}\")\n","\n","    if CFG.inference:\n","        test_dataset = TestDataset(CFG, test)\n","        test_dataloader = DataLoader(\n","            test_dataset,\n","            batch_size=CFG.batch_size,\n","            shuffle=False,\n","            num_workers=CFG.num_workers,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","        predictions = []\n","        for i_fold in CFG.train_fold:\n","            if CFG.submission:\n","                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n","                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n","            else:\n","                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n","\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            model.load_state_dict(state[\"model\"])\n","            print(f\"load weights from {path}\")\n","            test_char_probs = inference_fn(test_dataloader, model, device)\n","            predictions.append(test_char_probs)\n","\n","            del state, test_char_probs, model; gc.collect()\n","            torch.cuda.empty_cache()\n","\n","        predictions = np.mean(predictions, axis=0)\n","        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n","        test[CFG.target_col] = predicted_location_str\n","        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n","        test[[CFG.id_col, CFG.target_col]].to_csv(\n","            CFG.output_dir / \"submission.csv\", index=False\n","        )"],"id":"quality-expansion"},{"cell_type":"code","execution_count":35,"metadata":{"id":"proprietary-civilian","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["3df9e2c24b36418ca041546a69d9e042","d06147a3f80d4ca2b2981d8b150f3dbe","7d737ce3eb724847bb8f6556c4558df3","4546dfab613e4840ad69c15f9ff25693","55345548bbea4594aeaad94ee870b4b3","07c2347c4a8c42f58e9fc68966dcc4ba","8d5ddeaccd7b47f8bac23b0217724a42","50631c1b50cc48c3870fd13da88ccd55","46535d3b8fca42acba3beeafeb3e7144","13f10d90402d416891351c70d25cf14b","1cfbd8fd3a144d71b0f262386a12de95","5142db37f4b54a7aa8bf53084dcd22f5","e1aa543165e94b87ae4a5579fb7db0d4","00324af6587e4680824128aa79aebbb2","279f57e6702b4d31a3488b4dc692efbf","574c8435ee3247898628a316f18c0bbd","8612a466f63946799cc212e97e574d34","a5d2624879c04586ad09b7746dc5508f","82f75bd114f4445f9bc466ba81c63914","6886c89be6774feab01f9538112fd57b","c200ea7980574b8aa7dc067fb735a043","5b36660022d4468ba9e2c0e90a9445dd"]},"outputId":"8f6a6225-ff85-4270-c70d-1947e0aec564","executionInfo":{"status":"ok","timestamp":1650225400707,"user_tz":-540,"elapsed":24132923,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["========== fold: 1 training ==========\n","get pseudo plain from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n","get pseudo labels from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp083/pseudo_labels_1.npy\n","(612602, 6) (612602, 950)\n","(100000, 7)\n","(110725, 11)\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/833M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3df9e2c24b36418ca041546a69d9e042"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp075/fold1_best.pth\n","Epoch: [1][0/36908] Elapsed 0m 2s (remain 1445m 9s) Loss: 0.3584(0.3584) Grad: 167181.0000  LR: 0.000000  \n","Epoch: [1][100/36908] Elapsed 1m 5s (remain 396m 45s) Loss: 0.3306(0.3462) Grad: 152774.9219  LR: 0.000001  \n","Epoch: [1][200/36908] Elapsed 2m 8s (remain 390m 19s) Loss: 0.2363(0.3103) Grad: 111312.4688  LR: 0.000001  \n","Epoch: [1][300/36908] Elapsed 3m 11s (remain 387m 23s) Loss: 0.1115(0.2625) Grad: 64296.9102  LR: 0.000002  \n","Epoch: [1][400/36908] Elapsed 4m 13s (remain 385m 19s) Loss: 0.0521(0.2153) Grad: 19278.8672  LR: 0.000002  \n","Epoch: [1][500/36908] Elapsed 5m 16s (remain 383m 30s) Loss: 0.0300(0.1786) Grad: 27776.8945  LR: 0.000003  \n","Epoch: [1][600/36908] Elapsed 6m 20s (remain 382m 37s) Loss: 0.0098(0.1515) Grad: 7822.0356  LR: 0.000003  \n","Epoch: [1][700/36908] Elapsed 7m 23s (remain 382m 1s) Loss: 0.0088(0.1311) Grad: 1979.9764  LR: 0.000004  \n","Epoch: [1][800/36908] Elapsed 8m 27s (remain 380m 54s) Loss: 0.0097(0.1156) Grad: 6894.4229  LR: 0.000004  \n","Epoch: [1][900/36908] Elapsed 9m 30s (remain 379m 44s) Loss: 0.0193(0.1034) Grad: 43976.5234  LR: 0.000005  \n","Epoch: [1][1000/36908] Elapsed 10m 33s (remain 378m 34s) Loss: 0.0058(0.0937) Grad: 844.2263  LR: 0.000005  \n","Epoch: [1][1100/36908] Elapsed 11m 36s (remain 377m 20s) Loss: 0.0035(0.0858) Grad: 940.4340  LR: 0.000006  \n","Epoch: [1][1200/36908] Elapsed 12m 39s (remain 376m 12s) Loss: 0.0110(0.0792) Grad: 387.9498  LR: 0.000007  \n","Epoch: [1][1300/36908] Elapsed 13m 42s (remain 375m 18s) Loss: 0.0033(0.0736) Grad: 1301.2579  LR: 0.000007  \n","Epoch: [1][1400/36908] Elapsed 14m 46s (remain 374m 22s) Loss: 0.0310(0.0688) Grad: 12684.3887  LR: 0.000008  \n","Epoch: [1][1500/36908] Elapsed 15m 50s (remain 373m 30s) Loss: 0.0004(0.0645) Grad: 54.4844  LR: 0.000008  \n","Epoch: [1][1600/36908] Elapsed 16m 53s (remain 372m 34s) Loss: 0.0123(0.0609) Grad: 18321.4141  LR: 0.000009  \n","Epoch: [1][1700/36908] Elapsed 17m 57s (remain 371m 32s) Loss: 0.0039(0.0576) Grad: 1146.2539  LR: 0.000009  \n","Epoch: [1][1800/36908] Elapsed 19m 0s (remain 370m 24s) Loss: 0.0012(0.0547) Grad: 485.3694  LR: 0.000010  \n","Epoch: [1][1900/36908] Elapsed 20m 3s (remain 369m 15s) Loss: 0.0115(0.0521) Grad: 3051.0510  LR: 0.000010  \n","Epoch: [1][2000/36908] Elapsed 21m 6s (remain 368m 21s) Loss: 0.0013(0.0499) Grad: 478.6880  LR: 0.000011  \n","Epoch: [1][2100/36908] Elapsed 22m 10s (remain 367m 21s) Loss: 0.0057(0.0478) Grad: 3617.2244  LR: 0.000011  \n","Epoch: [1][2200/36908] Elapsed 23m 13s (remain 366m 14s) Loss: 0.0091(0.0459) Grad: 32975.1133  LR: 0.000012  \n","Epoch: [1][2300/36908] Elapsed 24m 17s (remain 365m 18s) Loss: 0.0051(0.0441) Grad: 741.7172  LR: 0.000012  \n","Epoch: [1][2400/36908] Elapsed 25m 21s (remain 364m 20s) Loss: 0.0038(0.0426) Grad: 669.7762  LR: 0.000013  \n","Epoch: [1][2500/36908] Elapsed 26m 24s (remain 363m 23s) Loss: 0.0031(0.0411) Grad: 10328.7148  LR: 0.000014  \n","Epoch: [1][2600/36908] Elapsed 27m 28s (remain 362m 26s) Loss: 0.0237(0.0398) Grad: 21190.7578  LR: 0.000014  \n","Epoch: [1][2700/36908] Elapsed 28m 32s (remain 361m 26s) Loss: 0.0094(0.0385) Grad: 5342.8223  LR: 0.000015  \n","Epoch: [1][2800/36908] Elapsed 29m 36s (remain 360m 27s) Loss: 0.0024(0.0373) Grad: 142.9244  LR: 0.000015  \n","Epoch: [1][2900/36908] Elapsed 30m 39s (remain 359m 25s) Loss: 0.0091(0.0362) Grad: 28990.6543  LR: 0.000016  \n","Epoch: [1][3000/36908] Elapsed 31m 43s (remain 358m 22s) Loss: 0.0005(0.0352) Grad: 120.0675  LR: 0.000016  \n","Epoch: [1][3100/36908] Elapsed 32m 46s (remain 357m 21s) Loss: 0.0038(0.0342) Grad: 386.6735  LR: 0.000017  \n","Epoch: [1][3200/36908] Elapsed 33m 50s (remain 356m 19s) Loss: 0.0006(0.0334) Grad: 62.5378  LR: 0.000017  \n","Epoch: [1][3300/36908] Elapsed 34m 53s (remain 355m 17s) Loss: 0.0033(0.0325) Grad: 577.3224  LR: 0.000018  \n","Epoch: [1][3400/36908] Elapsed 35m 58s (remain 354m 23s) Loss: 0.0013(0.0317) Grad: 1828.0465  LR: 0.000018  \n","Epoch: [1][3500/36908] Elapsed 37m 2s (remain 353m 28s) Loss: 0.0032(0.0310) Grad: 4500.4019  LR: 0.000019  \n","Epoch: [1][3600/36908] Elapsed 38m 6s (remain 352m 28s) Loss: 0.0035(0.0303) Grad: 3666.1621  LR: 0.000020  \n","Epoch: [1][3700/36908] Elapsed 39m 10s (remain 351m 27s) Loss: 0.0026(0.0296) Grad: 1857.2557  LR: 0.000020  \n","Epoch: [1][3800/36908] Elapsed 40m 13s (remain 350m 26s) Loss: 0.0011(0.0289) Grad: 337.7130  LR: 0.000020  \n","Epoch: [1][3900/36908] Elapsed 41m 17s (remain 349m 22s) Loss: 0.0044(0.0284) Grad: 12879.0557  LR: 0.000020  \n","Epoch: [1][4000/36908] Elapsed 42m 20s (remain 348m 14s) Loss: 0.0032(0.0278) Grad: 5965.7246  LR: 0.000020  \n","Epoch: [1][4100/36908] Elapsed 43m 23s (remain 347m 6s) Loss: 0.0048(0.0272) Grad: 27476.8320  LR: 0.000020  \n","Epoch: [1][4200/36908] Elapsed 44m 26s (remain 345m 58s) Loss: 0.0147(0.0267) Grad: 126974.1797  LR: 0.000020  \n","Epoch: [1][4300/36908] Elapsed 45m 29s (remain 344m 49s) Loss: 0.0004(0.0262) Grad: 445.3496  LR: 0.000020  \n","Epoch: [1][4400/36908] Elapsed 46m 31s (remain 343m 42s) Loss: 0.0034(0.0258) Grad: 4844.6147  LR: 0.000020  \n","Epoch: [1][4500/36908] Elapsed 47m 35s (remain 342m 36s) Loss: 0.0005(0.0253) Grad: 1168.4072  LR: 0.000020  \n","Epoch: [1][4600/36908] Elapsed 48m 38s (remain 341m 30s) Loss: 0.0024(0.0249) Grad: 21053.7266  LR: 0.000019  \n","Epoch: [1][4700/36908] Elapsed 49m 41s (remain 340m 25s) Loss: 0.0102(0.0245) Grad: 21131.2129  LR: 0.000019  \n","Epoch: [1][4800/36908] Elapsed 50m 44s (remain 339m 20s) Loss: 0.0027(0.0241) Grad: 1196.2905  LR: 0.000019  \n","Epoch: [1][4900/36908] Elapsed 51m 47s (remain 338m 15s) Loss: 0.0029(0.0237) Grad: 3346.0662  LR: 0.000019  \n","Epoch: [1][5000/36908] Elapsed 52m 51s (remain 337m 16s) Loss: 0.0034(0.0233) Grad: 4626.2085  LR: 0.000019  \n","Epoch: [1][5100/36908] Elapsed 53m 55s (remain 336m 12s) Loss: 0.0028(0.0230) Grad: 691.3351  LR: 0.000019  \n","Epoch: [1][5200/36908] Elapsed 54m 58s (remain 335m 8s) Loss: 0.0021(0.0226) Grad: 5151.2642  LR: 0.000019  \n","Epoch: [1][5300/36908] Elapsed 56m 1s (remain 334m 1s) Loss: 0.0015(0.0223) Grad: 346.8577  LR: 0.000019  \n","Epoch: [1][5400/36908] Elapsed 57m 4s (remain 332m 56s) Loss: 0.0055(0.0220) Grad: 899.9708  LR: 0.000019  \n","Epoch: [1][5500/36908] Elapsed 58m 7s (remain 331m 53s) Loss: 0.0016(0.0217) Grad: 342.2431  LR: 0.000019  \n","Epoch: [1][5600/36908] Elapsed 59m 11s (remain 330m 49s) Loss: 0.0066(0.0214) Grad: 4849.8374  LR: 0.000019  \n","Epoch: [1][5700/36908] Elapsed 60m 14s (remain 329m 47s) Loss: 0.0016(0.0211) Grad: 2668.1497  LR: 0.000019  \n","Epoch: [1][5800/36908] Elapsed 61m 17s (remain 328m 42s) Loss: 0.0052(0.0208) Grad: 4321.9868  LR: 0.000019  \n","Epoch: [1][5900/36908] Elapsed 62m 21s (remain 327m 40s) Loss: 0.0033(0.0205) Grad: 2686.8584  LR: 0.000019  \n","Epoch: [1][6000/36908] Elapsed 63m 25s (remain 326m 37s) Loss: 0.0056(0.0203) Grad: 39624.2305  LR: 0.000019  \n","Epoch: [1][6100/36908] Elapsed 64m 28s (remain 325m 35s) Loss: 0.0010(0.0200) Grad: 35.6842  LR: 0.000019  \n","Epoch: [1][6200/36908] Elapsed 65m 32s (remain 324m 31s) Loss: 0.0014(0.0198) Grad: 350.8468  LR: 0.000018  \n","Epoch: [1][6300/36908] Elapsed 66m 34s (remain 323m 25s) Loss: 0.0020(0.0196) Grad: 2696.0254  LR: 0.000018  \n","Epoch: [1][6400/36908] Elapsed 67m 37s (remain 322m 17s) Loss: 0.0158(0.0193) Grad: 123100.3125  LR: 0.000018  \n","Epoch: [1][6500/36908] Elapsed 68m 39s (remain 321m 10s) Loss: 0.0036(0.0191) Grad: 5989.3149  LR: 0.000018  \n","Epoch: [1][6600/36908] Elapsed 69m 42s (remain 320m 3s) Loss: 0.0010(0.0189) Grad: 103.7224  LR: 0.000018  \n","Epoch: [1][6700/36908] Elapsed 70m 45s (remain 318m 57s) Loss: 0.0027(0.0187) Grad: 272.3851  LR: 0.000018  \n","Epoch: [1][6800/36908] Elapsed 71m 47s (remain 317m 50s) Loss: 0.0065(0.0185) Grad: 5346.4224  LR: 0.000018  \n","Epoch: [1][6900/36908] Elapsed 72m 50s (remain 316m 44s) Loss: 0.0051(0.0183) Grad: 3883.1602  LR: 0.000018  \n","Epoch: [1][7000/36908] Elapsed 73m 53s (remain 315m 38s) Loss: 0.0067(0.0181) Grad: 24075.8555  LR: 0.000018  \n","Epoch: [1][7100/36908] Elapsed 74m 56s (remain 314m 35s) Loss: 0.0029(0.0179) Grad: 7430.9990  LR: 0.000018  \n","Epoch: [1][7200/36908] Elapsed 76m 1s (remain 313m 36s) Loss: 0.0066(0.0177) Grad: 45791.5977  LR: 0.000018  \n","Epoch: [1][7300/36908] Elapsed 77m 5s (remain 312m 37s) Loss: 0.0007(0.0176) Grad: 216.3931  LR: 0.000018  \n","Epoch: [1][7400/36908] Elapsed 78m 9s (remain 311m 35s) Loss: 0.0082(0.0174) Grad: 17015.0918  LR: 0.000018  \n","Epoch: [1][7500/36908] Elapsed 79m 13s (remain 310m 34s) Loss: 0.0013(0.0172) Grad: 320.1643  LR: 0.000018  \n","Epoch: [1][7600/36908] Elapsed 80m 16s (remain 309m 32s) Loss: 0.0005(0.0170) Grad: 1822.0013  LR: 0.000018  \n","Epoch: [1][7700/36908] Elapsed 81m 21s (remain 308m 32s) Loss: 0.0068(0.0169) Grad: 1120.4220  LR: 0.000018  \n","Epoch: [1][7800/36908] Elapsed 82m 25s (remain 307m 31s) Loss: 0.0021(0.0168) Grad: 5281.6626  LR: 0.000018  \n","Epoch: [1][7900/36908] Elapsed 83m 29s (remain 306m 30s) Loss: 0.0025(0.0166) Grad: 1072.2581  LR: 0.000017  \n","Epoch: [1][8000/36908] Elapsed 84m 33s (remain 305m 31s) Loss: 0.0302(0.0165) Grad: 59590.9453  LR: 0.000017  \n","Epoch: [1][8100/36908] Elapsed 85m 38s (remain 304m 32s) Loss: 0.0016(0.0163) Grad: 38863.3906  LR: 0.000017  \n","Epoch: [1][8200/36908] Elapsed 86m 42s (remain 303m 31s) Loss: 0.0081(0.0162) Grad: 7537.7764  LR: 0.000017  \n","Epoch: [1][8300/36908] Elapsed 87m 46s (remain 302m 31s) Loss: 0.0025(0.0160) Grad: 8967.0918  LR: 0.000017  \n","Epoch: [1][8400/36908] Elapsed 88m 51s (remain 301m 31s) Loss: 0.0025(0.0159) Grad: 2605.2031  LR: 0.000017  \n","Epoch: [1][8500/36908] Elapsed 89m 55s (remain 300m 30s) Loss: 0.0022(0.0158) Grad: 4410.4683  LR: 0.000017  \n","Epoch: [1][8600/36908] Elapsed 90m 59s (remain 299m 28s) Loss: 0.0204(0.0156) Grad: 192630.3438  LR: 0.000017  \n","Epoch: [1][8700/36908] Elapsed 92m 4s (remain 298m 28s) Loss: 0.0017(0.0155) Grad: 1766.2037  LR: 0.000017  \n","Epoch: [1][8800/36908] Elapsed 93m 8s (remain 297m 28s) Loss: 0.0004(0.0154) Grad: 1814.9192  LR: 0.000017  \n","Epoch: [1][8900/36908] Elapsed 94m 13s (remain 296m 27s) Loss: 0.0033(0.0153) Grad: 679.4029  LR: 0.000017  \n","Epoch: [1][9000/36908] Elapsed 95m 17s (remain 295m 27s) Loss: 0.0128(0.0151) Grad: 57120.6641  LR: 0.000017  \n","Epoch: [1][9100/36908] Elapsed 96m 22s (remain 294m 28s) Loss: 0.0013(0.0150) Grad: 387.0527  LR: 0.000017  \n","Epoch: [1][9200/36908] Elapsed 97m 28s (remain 293m 31s) Loss: 0.0045(0.0149) Grad: 1484.4604  LR: 0.000017  \n","Epoch: [1][9300/36908] Elapsed 98m 34s (remain 292m 34s) Loss: 0.0034(0.0148) Grad: 766.4637  LR: 0.000017  \n","Epoch: [1][9400/36908] Elapsed 99m 40s (remain 291m 37s) Loss: 0.0017(0.0147) Grad: 20696.7520  LR: 0.000017  \n","Epoch: [1][9500/36908] Elapsed 100m 46s (remain 290m 40s) Loss: 0.0008(0.0146) Grad: 76.1897  LR: 0.000017  \n","Epoch: [1][9600/36908] Elapsed 101m 51s (remain 289m 42s) Loss: 0.0024(0.0145) Grad: 1685.5035  LR: 0.000016  \n","Epoch: [1][9700/36908] Elapsed 102m 57s (remain 288m 44s) Loss: 0.0021(0.0144) Grad: 1369.0641  LR: 0.000016  \n","Epoch: [1][9800/36908] Elapsed 104m 2s (remain 287m 44s) Loss: 0.0081(0.0143) Grad: 155031.2656  LR: 0.000016  \n","Epoch: [1][9900/36908] Elapsed 105m 7s (remain 286m 44s) Loss: 0.0138(0.0142) Grad: 77955.5234  LR: 0.000016  \n","Epoch: [1][10000/36908] Elapsed 106m 11s (remain 285m 43s) Loss: 0.0037(0.0141) Grad: 425.9910  LR: 0.000016  \n","Epoch: [1][10100/36908] Elapsed 107m 16s (remain 284m 43s) Loss: 0.0084(0.0140) Grad: 28090.1465  LR: 0.000016  \n","Epoch: [1][10200/36908] Elapsed 108m 21s (remain 283m 42s) Loss: 0.0067(0.0139) Grad: 1789.3281  LR: 0.000016  \n","Epoch: [1][10300/36908] Elapsed 109m 26s (remain 282m 41s) Loss: 0.0095(0.0138) Grad: 28093.2637  LR: 0.000016  \n","Epoch: [1][10400/36908] Elapsed 110m 30s (remain 281m 38s) Loss: 0.0046(0.0138) Grad: 2307.2144  LR: 0.000016  \n","Epoch: [1][10500/36908] Elapsed 111m 35s (remain 280m 36s) Loss: 0.0038(0.0137) Grad: 119.7956  LR: 0.000016  \n","Epoch: [1][10600/36908] Elapsed 112m 40s (remain 279m 35s) Loss: 0.0106(0.0136) Grad: 72933.2422  LR: 0.000016  \n","Epoch: [1][10700/36908] Elapsed 113m 44s (remain 278m 34s) Loss: 0.0085(0.0135) Grad: 89989.3828  LR: 0.000016  \n","Epoch: [1][10800/36908] Elapsed 114m 49s (remain 277m 32s) Loss: 0.0047(0.0134) Grad: 9124.4678  LR: 0.000016  \n","Epoch: [1][10900/36908] Elapsed 115m 54s (remain 276m 30s) Loss: 0.0060(0.0133) Grad: 12592.0801  LR: 0.000016  \n","Epoch: [1][11000/36908] Elapsed 116m 59s (remain 275m 29s) Loss: 0.0016(0.0132) Grad: 1574.5615  LR: 0.000016  \n","Epoch: [1][11100/36908] Elapsed 118m 3s (remain 274m 28s) Loss: 0.0027(0.0132) Grad: 20576.4141  LR: 0.000016  \n","Epoch: [1][11200/36908] Elapsed 119m 8s (remain 273m 26s) Loss: 0.0033(0.0131) Grad: 9525.0166  LR: 0.000015  \n","Epoch: [1][11300/36908] Elapsed 120m 12s (remain 272m 23s) Loss: 0.0010(0.0130) Grad: 1822.2115  LR: 0.000015  \n","Epoch: [1][11400/36908] Elapsed 121m 17s (remain 271m 20s) Loss: 0.0004(0.0129) Grad: 84.9258  LR: 0.000015  \n","Epoch: [1][11500/36908] Elapsed 122m 21s (remain 270m 18s) Loss: 0.0093(0.0129) Grad: 71719.9922  LR: 0.000015  \n","Epoch: [1][11600/36908] Elapsed 123m 25s (remain 269m 15s) Loss: 0.0015(0.0128) Grad: 5913.4575  LR: 0.000015  \n","Epoch: [1][11700/36908] Elapsed 124m 30s (remain 268m 12s) Loss: 0.0040(0.0127) Grad: 1043.6727  LR: 0.000015  \n","Epoch: [1][11800/36908] Elapsed 125m 34s (remain 267m 9s) Loss: 0.0188(0.0127) Grad: 119774.9688  LR: 0.000015  \n","Epoch: [1][11900/36908] Elapsed 126m 38s (remain 266m 6s) Loss: 0.0078(0.0126) Grad: 61006.4062  LR: 0.000015  \n","Epoch: [1][12000/36908] Elapsed 127m 42s (remain 265m 3s) Loss: 0.0108(0.0126) Grad: 3558.8689  LR: 0.000015  \n","Epoch: [1][12100/36908] Elapsed 128m 47s (remain 264m 1s) Loss: 0.0037(0.0125) Grad: 105619.3984  LR: 0.000015  \n","Epoch: [1][12200/36908] Elapsed 129m 52s (remain 262m 59s) Loss: 0.0020(0.0124) Grad: 8219.8711  LR: 0.000015  \n","Epoch: [1][12300/36908] Elapsed 130m 56s (remain 261m 55s) Loss: 0.0078(0.0124) Grad: 79014.3516  LR: 0.000015  \n","Epoch: [1][12400/36908] Elapsed 132m 0s (remain 260m 52s) Loss: 0.0054(0.0123) Grad: 13898.1094  LR: 0.000015  \n","Epoch: [1][12500/36908] Elapsed 133m 4s (remain 259m 49s) Loss: 0.0039(0.0122) Grad: 9093.0527  LR: 0.000015  \n","Epoch: [1][12600/36908] Elapsed 134m 8s (remain 258m 46s) Loss: 0.0122(0.0122) Grad: 297057.3125  LR: 0.000015  \n","Epoch: [1][12700/36908] Elapsed 135m 13s (remain 257m 43s) Loss: 0.0029(0.0121) Grad: 12916.9590  LR: 0.000015  \n","Epoch: [1][12800/36908] Elapsed 136m 18s (remain 256m 41s) Loss: 0.0034(0.0121) Grad: 14909.8955  LR: 0.000015  \n","Epoch: [1][12900/36908] Elapsed 137m 22s (remain 255m 38s) Loss: 0.0022(0.0120) Grad: 18441.9785  LR: 0.000014  \n","Epoch: [1][13000/36908] Elapsed 138m 27s (remain 254m 35s) Loss: 0.0020(0.0119) Grad: 7469.9307  LR: 0.000014  \n","Epoch: [1][13100/36908] Elapsed 139m 31s (remain 253m 32s) Loss: 0.0035(0.0119) Grad: 295.2939  LR: 0.000014  \n","Epoch: [1][13200/36908] Elapsed 140m 35s (remain 252m 29s) Loss: 0.0022(0.0118) Grad: 5214.2817  LR: 0.000014  \n","Epoch: [1][13300/36908] Elapsed 141m 40s (remain 251m 26s) Loss: 0.0020(0.0118) Grad: 18887.7305  LR: 0.000014  \n","Epoch: [1][13400/36908] Elapsed 142m 45s (remain 250m 24s) Loss: 0.0023(0.0117) Grad: 564.7375  LR: 0.000014  \n","Epoch: [1][13500/36908] Elapsed 143m 49s (remain 249m 20s) Loss: 0.0014(0.0117) Grad: 333.9503  LR: 0.000014  \n","Epoch: [1][13600/36908] Elapsed 144m 53s (remain 248m 17s) Loss: 0.0015(0.0116) Grad: 551.1476  LR: 0.000014  \n","Epoch: [1][13700/36908] Elapsed 145m 58s (remain 247m 15s) Loss: 0.0027(0.0116) Grad: 188.1112  LR: 0.000014  \n","Epoch: [1][13800/36908] Elapsed 147m 3s (remain 246m 12s) Loss: 0.0030(0.0115) Grad: 7276.3462  LR: 0.000014  \n","Epoch: [1][13900/36908] Elapsed 148m 7s (remain 245m 9s) Loss: 0.0118(0.0115) Grad: 39842.7422  LR: 0.000014  \n","Epoch: [1][14000/36908] Elapsed 149m 11s (remain 244m 5s) Loss: 0.0173(0.0114) Grad: 289046.3438  LR: 0.000014  \n","Epoch: [1][14100/36908] Elapsed 150m 15s (remain 243m 1s) Loss: 0.0010(0.0114) Grad: 1058.3896  LR: 0.000014  \n","Epoch: [1][14200/36908] Elapsed 151m 20s (remain 241m 58s) Loss: 0.0052(0.0113) Grad: 75781.2969  LR: 0.000014  \n","Epoch: [1][14300/36908] Elapsed 152m 24s (remain 240m 55s) Loss: 0.0031(0.0113) Grad: 347.4721  LR: 0.000014  \n","Epoch: [1][14400/36908] Elapsed 153m 29s (remain 239m 53s) Loss: 0.0030(0.0113) Grad: 192.1313  LR: 0.000014  \n","Epoch: [1][14500/36908] Elapsed 154m 34s (remain 238m 50s) Loss: 0.0040(0.0112) Grad: 7743.2070  LR: 0.000013  \n","Epoch: [1][14600/36908] Elapsed 155m 38s (remain 237m 47s) Loss: 0.0011(0.0112) Grad: 792.0908  LR: 0.000013  \n","Epoch: [1][14700/36908] Elapsed 156m 42s (remain 236m 43s) Loss: 0.0281(0.0111) Grad: 582641.5000  LR: 0.000013  \n","Epoch: [1][14800/36908] Elapsed 157m 47s (remain 235m 40s) Loss: 0.0064(0.0111) Grad: 5114.9790  LR: 0.000013  \n","Epoch: [1][14900/36908] Elapsed 158m 51s (remain 234m 36s) Loss: 0.0026(0.0110) Grad: 1794.2811  LR: 0.000013  \n","Epoch: [1][15000/36908] Elapsed 159m 55s (remain 233m 33s) Loss: 0.0018(0.0110) Grad: 5243.2158  LR: 0.000013  \n","Epoch: [1][15100/36908] Elapsed 160m 59s (remain 232m 29s) Loss: 0.0011(0.0110) Grad: 172.1278  LR: 0.000013  \n","Epoch: [1][15200/36908] Elapsed 162m 4s (remain 231m 26s) Loss: 0.0044(0.0109) Grad: 9913.3691  LR: 0.000013  \n","Epoch: [1][15300/36908] Elapsed 163m 8s (remain 230m 23s) Loss: 0.0057(0.0109) Grad: 29513.8828  LR: 0.000013  \n","Epoch: [1][15400/36908] Elapsed 164m 13s (remain 229m 20s) Loss: 0.0043(0.0108) Grad: 33366.7227  LR: 0.000013  \n","Epoch: [1][15500/36908] Elapsed 165m 18s (remain 228m 16s) Loss: 0.0038(0.0108) Grad: 740.2305  LR: 0.000013  \n","Epoch: [1][15600/36908] Elapsed 166m 22s (remain 227m 14s) Loss: 0.0096(0.0108) Grad: 8710.4053  LR: 0.000013  \n","Epoch: [1][15700/36908] Elapsed 167m 27s (remain 226m 10s) Loss: 0.0065(0.0107) Grad: 23691.8320  LR: 0.000013  \n","Epoch: [1][15800/36908] Elapsed 168m 32s (remain 225m 7s) Loss: 0.0103(0.0107) Grad: 190784.0312  LR: 0.000013  \n","Epoch: [1][15900/36908] Elapsed 169m 36s (remain 224m 4s) Loss: 0.0015(0.0106) Grad: 624.0880  LR: 0.000013  \n","Epoch: [1][16000/36908] Elapsed 170m 41s (remain 223m 1s) Loss: 0.0033(0.0106) Grad: 750.4461  LR: 0.000013  \n","Epoch: [1][16100/36908] Elapsed 171m 45s (remain 221m 57s) Loss: 0.0024(0.0106) Grad: 792.4566  LR: 0.000013  \n","Epoch: [1][16200/36908] Elapsed 172m 49s (remain 220m 53s) Loss: 0.0093(0.0105) Grad: 30483.6992  LR: 0.000012  \n","Epoch: [1][16300/36908] Elapsed 173m 54s (remain 219m 50s) Loss: 0.0050(0.0105) Grad: 24447.7402  LR: 0.000012  \n","Epoch: [1][16400/36908] Elapsed 174m 59s (remain 218m 47s) Loss: 0.0014(0.0105) Grad: 394.5097  LR: 0.000012  \n","Epoch: [1][16500/36908] Elapsed 176m 3s (remain 217m 44s) Loss: 0.0029(0.0104) Grad: 1657.3696  LR: 0.000012  \n","Epoch: [1][16600/36908] Elapsed 177m 8s (remain 216m 40s) Loss: 0.0043(0.0104) Grad: 66657.5703  LR: 0.000012  \n","Epoch: [1][16700/36908] Elapsed 178m 12s (remain 215m 37s) Loss: 0.0039(0.0103) Grad: 5993.8218  LR: 0.000012  \n","Epoch: [1][16800/36908] Elapsed 179m 16s (remain 214m 33s) Loss: 0.0015(0.0103) Grad: 2006.1639  LR: 0.000012  \n","Epoch: [1][16900/36908] Elapsed 180m 21s (remain 213m 29s) Loss: 0.0030(0.0103) Grad: 140749.9375  LR: 0.000012  \n","Epoch: [1][17000/36908] Elapsed 181m 25s (remain 212m 26s) Loss: 0.0052(0.0102) Grad: 13038.0781  LR: 0.000012  \n","Epoch: [1][17100/36908] Elapsed 182m 29s (remain 211m 22s) Loss: 0.0076(0.0102) Grad: 9047.0820  LR: 0.000012  \n","Epoch: [1][17200/36908] Elapsed 183m 34s (remain 210m 18s) Loss: 0.0062(0.0102) Grad: 2878.2871  LR: 0.000012  \n","Epoch: [1][17300/36908] Elapsed 184m 38s (remain 209m 14s) Loss: 0.0020(0.0102) Grad: 2035.6261  LR: 0.000012  \n","Epoch: [1][17400/36908] Elapsed 185m 42s (remain 208m 10s) Loss: 0.0028(0.0101) Grad: 46.0621  LR: 0.000012  \n","Epoch: [1][17500/36908] Elapsed 186m 46s (remain 207m 6s) Loss: 0.0003(0.0101) Grad: 589.5307  LR: 0.000012  \n","Epoch: [1][17600/36908] Elapsed 187m 50s (remain 206m 3s) Loss: 0.0018(0.0101) Grad: 4016.8979  LR: 0.000012  \n","Epoch: [1][17700/36908] Elapsed 188m 55s (remain 204m 59s) Loss: 0.0061(0.0100) Grad: 19939.8340  LR: 0.000012  \n","Epoch: [1][17800/36908] Elapsed 190m 0s (remain 203m 56s) Loss: 0.0065(0.0100) Grad: 7640.5298  LR: 0.000012  \n","Epoch: [1][17900/36908] Elapsed 191m 4s (remain 202m 53s) Loss: 0.0063(0.0100) Grad: 9897.5430  LR: 0.000011  \n","Epoch: [1][18000/36908] Elapsed 192m 9s (remain 201m 49s) Loss: 0.0050(0.0099) Grad: 4357.5640  LR: 0.000011  \n","Epoch: [1][18100/36908] Elapsed 193m 14s (remain 200m 46s) Loss: 0.0076(0.0099) Grad: 6821.3701  LR: 0.000011  \n","Epoch: [1][18200/36908] Elapsed 194m 18s (remain 199m 42s) Loss: 0.0114(0.0099) Grad: 33744.1484  LR: 0.000011  \n","Epoch: [1][18300/36908] Elapsed 195m 23s (remain 198m 39s) Loss: 0.0004(0.0098) Grad: 252.6601  LR: 0.000011  \n","Epoch: [1][18400/36908] Elapsed 196m 27s (remain 197m 35s) Loss: 0.0020(0.0098) Grad: 1079.3727  LR: 0.000011  \n","Epoch: [1][18500/36908] Elapsed 197m 32s (remain 196m 32s) Loss: 0.0005(0.0098) Grad: 513.9295  LR: 0.000011  \n","Epoch: [1][18600/36908] Elapsed 198m 37s (remain 195m 28s) Loss: 0.0079(0.0098) Grad: 91363.4531  LR: 0.000011  \n","Epoch: [1][18700/36908] Elapsed 199m 41s (remain 194m 25s) Loss: 0.0052(0.0097) Grad: 2720.5837  LR: 0.000011  \n","Epoch: [1][18800/36908] Elapsed 200m 46s (remain 193m 21s) Loss: 0.0021(0.0097) Grad: 249.7147  LR: 0.000011  \n","Epoch: [1][18900/36908] Elapsed 201m 50s (remain 192m 18s) Loss: 0.0004(0.0097) Grad: 348.5394  LR: 0.000011  \n","Epoch: [1][19000/36908] Elapsed 202m 55s (remain 191m 14s) Loss: 0.0010(0.0097) Grad: 458.9922  LR: 0.000011  \n","Epoch: [1][19100/36908] Elapsed 203m 59s (remain 190m 10s) Loss: 0.0098(0.0096) Grad: 2882.0601  LR: 0.000011  \n","Epoch: [1][19200/36908] Elapsed 205m 4s (remain 189m 6s) Loss: 0.0077(0.0096) Grad: 11397.5918  LR: 0.000011  \n","Epoch: [1][19300/36908] Elapsed 206m 8s (remain 188m 2s) Loss: 0.0046(0.0096) Grad: 81825.2734  LR: 0.000011  \n","Epoch: [1][19400/36908] Elapsed 207m 12s (remain 186m 58s) Loss: 0.0030(0.0095) Grad: 1351.3242  LR: 0.000011  \n","Epoch: [1][19500/36908] Elapsed 208m 16s (remain 185m 54s) Loss: 0.0072(0.0095) Grad: 5039.6567  LR: 0.000010  \n","Epoch: [1][19600/36908] Elapsed 209m 20s (remain 184m 50s) Loss: 0.0071(0.0095) Grad: 2411.4822  LR: 0.000010  \n","Epoch: [1][19700/36908] Elapsed 210m 24s (remain 183m 46s) Loss: 0.0076(0.0095) Grad: 6264.7778  LR: 0.000010  \n","Epoch: [1][19800/36908] Elapsed 211m 28s (remain 182m 42s) Loss: 0.0014(0.0094) Grad: 15106.8447  LR: 0.000010  \n","Epoch: [1][19900/36908] Elapsed 212m 32s (remain 181m 38s) Loss: 0.0054(0.0094) Grad: 21744.7461  LR: 0.000010  \n","Epoch: [1][20000/36908] Elapsed 213m 37s (remain 180m 34s) Loss: 0.0049(0.0094) Grad: 97558.4141  LR: 0.000010  \n","Epoch: [1][20100/36908] Elapsed 214m 41s (remain 179m 30s) Loss: 0.0047(0.0094) Grad: 1680.7130  LR: 0.000010  \n","Epoch: [1][20200/36908] Elapsed 215m 45s (remain 178m 26s) Loss: 0.0039(0.0093) Grad: 47152.0547  LR: 0.000010  \n","Epoch: [1][20300/36908] Elapsed 216m 49s (remain 177m 22s) Loss: 0.0024(0.0093) Grad: 11041.2373  LR: 0.000010  \n","Epoch: [1][20400/36908] Elapsed 217m 54s (remain 176m 18s) Loss: 0.0007(0.0093) Grad: 475.5445  LR: 0.000010  \n","Epoch: [1][20500/36908] Elapsed 218m 58s (remain 175m 14s) Loss: 0.0078(0.0093) Grad: 6839.6929  LR: 0.000010  \n","Epoch: [1][20600/36908] Elapsed 220m 2s (remain 174m 10s) Loss: 0.0047(0.0092) Grad: 13855.2451  LR: 0.000010  \n","Epoch: [1][20700/36908] Elapsed 221m 7s (remain 173m 7s) Loss: 0.0025(0.0092) Grad: 10627.7715  LR: 0.000010  \n","Epoch: [1][20800/36908] Elapsed 222m 11s (remain 172m 3s) Loss: 0.0030(0.0092) Grad: 5605.7397  LR: 0.000010  \n","Epoch: [1][20900/36908] Elapsed 223m 15s (remain 170m 59s) Loss: 0.0019(0.0092) Grad: 730.2266  LR: 0.000010  \n","Epoch: [1][21000/36908] Elapsed 224m 20s (remain 169m 55s) Loss: 0.0023(0.0092) Grad: 76437.7656  LR: 0.000010  \n","Epoch: [1][21100/36908] Elapsed 225m 25s (remain 168m 51s) Loss: 0.0024(0.0091) Grad: 5568.1162  LR: 0.000010  \n","Epoch: [1][21200/36908] Elapsed 226m 29s (remain 167m 48s) Loss: 0.0042(0.0091) Grad: 1673.3323  LR: 0.000009  \n","Epoch: [1][21300/36908] Elapsed 227m 34s (remain 166m 44s) Loss: 0.0010(0.0091) Grad: 249.7754  LR: 0.000009  \n","Epoch: [1][21400/36908] Elapsed 228m 38s (remain 165m 40s) Loss: 0.0027(0.0091) Grad: 11847.8018  LR: 0.000009  \n","Epoch: [1][21500/36908] Elapsed 229m 42s (remain 164m 36s) Loss: 0.0053(0.0091) Grad: 1691.4059  LR: 0.000009  \n","Epoch: [1][21600/36908] Elapsed 230m 46s (remain 163m 32s) Loss: 0.0006(0.0090) Grad: 280.2324  LR: 0.000009  \n","Epoch: [1][21700/36908] Elapsed 231m 50s (remain 162m 28s) Loss: 0.0002(0.0090) Grad: 1182.5627  LR: 0.000009  \n","Epoch: [1][21800/36908] Elapsed 232m 55s (remain 161m 24s) Loss: 0.0151(0.0090) Grad: 290049.4375  LR: 0.000009  \n","Epoch: [1][21900/36908] Elapsed 233m 59s (remain 160m 19s) Loss: 0.0025(0.0090) Grad: 3024.7373  LR: 0.000009  \n","Epoch: [1][22000/36908] Elapsed 235m 3s (remain 159m 15s) Loss: 0.0016(0.0089) Grad: 573.9568  LR: 0.000009  \n","Epoch: [1][22100/36908] Elapsed 236m 7s (remain 158m 11s) Loss: 0.0083(0.0089) Grad: 8270.5664  LR: 0.000009  \n","Epoch: [1][22200/36908] Elapsed 237m 11s (remain 157m 7s) Loss: 0.0012(0.0089) Grad: 893.8661  LR: 0.000009  \n","Epoch: [1][22300/36908] Elapsed 238m 15s (remain 156m 3s) Loss: 0.0050(0.0089) Grad: 1176.6632  LR: 0.000009  \n","Epoch: [1][22400/36908] Elapsed 239m 18s (remain 154m 58s) Loss: 0.0062(0.0089) Grad: 17441.6191  LR: 0.000009  \n","Epoch: [1][22500/36908] Elapsed 240m 21s (remain 153m 53s) Loss: 0.0038(0.0088) Grad: 54391.6641  LR: 0.000009  \n","Epoch: [1][22600/36908] Elapsed 241m 24s (remain 152m 48s) Loss: 0.0002(0.0088) Grad: 98.1693  LR: 0.000009  \n","Epoch: [1][22700/36908] Elapsed 242m 26s (remain 151m 43s) Loss: 0.0040(0.0088) Grad: 1692.7640  LR: 0.000009  \n","Epoch: [1][22800/36908] Elapsed 243m 29s (remain 150m 39s) Loss: 0.0106(0.0088) Grad: 49040.3047  LR: 0.000008  \n","Epoch: [1][22900/36908] Elapsed 244m 33s (remain 149m 34s) Loss: 0.0024(0.0088) Grad: 2857.2100  LR: 0.000008  \n","Epoch: [1][23000/36908] Elapsed 245m 36s (remain 148m 30s) Loss: 0.0557(0.0088) Grad: 196895.9375  LR: 0.000008  \n","Epoch: [1][23100/36908] Elapsed 246m 39s (remain 147m 25s) Loss: 0.0097(0.0087) Grad: 18822.2676  LR: 0.000008  \n","Epoch: [1][23200/36908] Elapsed 247m 43s (remain 146m 21s) Loss: 0.0021(0.0087) Grad: 1315.8915  LR: 0.000008  \n","Epoch: [1][23300/36908] Elapsed 248m 47s (remain 145m 17s) Loss: 0.0008(0.0087) Grad: 1655.6738  LR: 0.000008  \n","Epoch: [1][23400/36908] Elapsed 249m 51s (remain 144m 12s) Loss: 0.0055(0.0087) Grad: 26885.5840  LR: 0.000008  \n","Epoch: [1][23500/36908] Elapsed 250m 56s (remain 143m 9s) Loss: 0.0022(0.0087) Grad: 7362.9194  LR: 0.000008  \n","Epoch: [1][23600/36908] Elapsed 252m 0s (remain 142m 5s) Loss: 0.0064(0.0086) Grad: 66195.1953  LR: 0.000008  \n","Epoch: [1][23700/36908] Elapsed 253m 5s (remain 141m 1s) Loss: 0.0046(0.0086) Grad: 82458.2266  LR: 0.000008  \n","Epoch: [1][23800/36908] Elapsed 254m 10s (remain 139m 58s) Loss: 0.0010(0.0086) Grad: 1609.6599  LR: 0.000008  \n","Epoch: [1][23900/36908] Elapsed 255m 14s (remain 138m 54s) Loss: 0.0026(0.0086) Grad: 29202.4941  LR: 0.000008  \n","Epoch: [1][24000/36908] Elapsed 256m 18s (remain 137m 50s) Loss: 0.0011(0.0086) Grad: 588.7917  LR: 0.000008  \n","Epoch: [1][24100/36908] Elapsed 257m 23s (remain 136m 46s) Loss: 0.0014(0.0086) Grad: 1167.4834  LR: 0.000008  \n","Epoch: [1][24200/36908] Elapsed 258m 28s (remain 135m 42s) Loss: 0.0058(0.0085) Grad: 1943.3550  LR: 0.000008  \n","Epoch: [1][24300/36908] Elapsed 259m 32s (remain 134m 38s) Loss: 0.0057(0.0085) Grad: 12405.0830  LR: 0.000008  \n","Epoch: [1][24400/36908] Elapsed 260m 36s (remain 133m 34s) Loss: 0.0030(0.0085) Grad: 709.1887  LR: 0.000008  \n","Epoch: [1][24500/36908] Elapsed 261m 40s (remain 132m 30s) Loss: 0.0006(0.0085) Grad: 109.0221  LR: 0.000007  \n","Epoch: [1][24600/36908] Elapsed 262m 44s (remain 131m 26s) Loss: 0.0036(0.0085) Grad: 5884.6729  LR: 0.000007  \n","Epoch: [1][24700/36908] Elapsed 263m 48s (remain 130m 22s) Loss: 0.0046(0.0085) Grad: 305.1248  LR: 0.000007  \n","Epoch: [1][24800/36908] Elapsed 264m 53s (remain 129m 18s) Loss: 0.0040(0.0084) Grad: 8644.8975  LR: 0.000007  \n","Epoch: [1][24900/36908] Elapsed 265m 57s (remain 128m 14s) Loss: 0.0057(0.0084) Grad: 20789.3828  LR: 0.000007  \n","Epoch: [1][25000/36908] Elapsed 267m 0s (remain 127m 10s) Loss: 0.0006(0.0084) Grad: 146.5791  LR: 0.000007  \n","Epoch: [1][25100/36908] Elapsed 268m 4s (remain 126m 5s) Loss: 0.0025(0.0084) Grad: 959.9031  LR: 0.000007  \n","Epoch: [1][25200/36908] Elapsed 269m 8s (remain 125m 1s) Loss: 0.0025(0.0084) Grad: 432.3495  LR: 0.000007  \n","Epoch: [1][25300/36908] Elapsed 270m 12s (remain 123m 57s) Loss: 0.0054(0.0084) Grad: 4138.1606  LR: 0.000007  \n","Epoch: [1][25400/36908] Elapsed 271m 16s (remain 122m 53s) Loss: 0.0045(0.0083) Grad: 1403.4393  LR: 0.000007  \n","Epoch: [1][25500/36908] Elapsed 272m 20s (remain 121m 49s) Loss: 0.0023(0.0083) Grad: 326.0984  LR: 0.000007  \n","Epoch: [1][25600/36908] Elapsed 273m 25s (remain 120m 45s) Loss: 0.0027(0.0083) Grad: 11530.3779  LR: 0.000007  \n","Epoch: [1][25700/36908] Elapsed 274m 30s (remain 119m 41s) Loss: 0.0034(0.0083) Grad: 6999.0747  LR: 0.000007  \n","Epoch: [1][25800/36908] Elapsed 275m 34s (remain 118m 37s) Loss: 0.0083(0.0083) Grad: 22593.4102  LR: 0.000007  \n","Epoch: [1][25900/36908] Elapsed 276m 39s (remain 117m 34s) Loss: 0.0034(0.0083) Grad: 897.0395  LR: 0.000007  \n","Epoch: [1][26000/36908] Elapsed 277m 44s (remain 116m 30s) Loss: 0.0023(0.0083) Grad: 1812.2341  LR: 0.000007  \n","Epoch: [1][26100/36908] Elapsed 278m 48s (remain 115m 26s) Loss: 0.0019(0.0082) Grad: 200.3093  LR: 0.000007  \n","Epoch: [1][26200/36908] Elapsed 279m 53s (remain 114m 22s) Loss: 0.0009(0.0082) Grad: 288.0674  LR: 0.000006  \n","Epoch: [1][26300/36908] Elapsed 280m 58s (remain 113m 18s) Loss: 0.0019(0.0082) Grad: 636.0921  LR: 0.000006  \n","Epoch: [1][26400/36908] Elapsed 282m 2s (remain 112m 14s) Loss: 0.0039(0.0082) Grad: 2883.0405  LR: 0.000006  \n","Epoch: [1][26500/36908] Elapsed 283m 6s (remain 111m 10s) Loss: 0.0027(0.0082) Grad: 647.2867  LR: 0.000006  \n","Epoch: [1][26600/36908] Elapsed 284m 10s (remain 110m 6s) Loss: 0.0032(0.0082) Grad: 1665.3893  LR: 0.000006  \n","Epoch: [1][26700/36908] Elapsed 285m 14s (remain 109m 2s) Loss: 0.0012(0.0081) Grad: 9090.5225  LR: 0.000006  \n","Epoch: [1][26800/36908] Elapsed 286m 19s (remain 107m 58s) Loss: 0.0018(0.0081) Grad: 13410.0938  LR: 0.000006  \n","Epoch: [1][26900/36908] Elapsed 287m 24s (remain 106m 54s) Loss: 0.0137(0.0081) Grad: 21767.6348  LR: 0.000006  \n","Epoch: [1][27000/36908] Elapsed 288m 30s (remain 105m 51s) Loss: 0.0029(0.0081) Grad: 1208.0128  LR: 0.000006  \n","Epoch: [1][27100/36908] Elapsed 289m 34s (remain 104m 47s) Loss: 0.0036(0.0081) Grad: 2545.5833  LR: 0.000006  \n","Epoch: [1][27200/36908] Elapsed 290m 38s (remain 103m 43s) Loss: 0.0017(0.0081) Grad: 234.0250  LR: 0.000006  \n","Epoch: [1][27300/36908] Elapsed 291m 42s (remain 102m 39s) Loss: 0.0102(0.0081) Grad: 8469.6533  LR: 0.000006  \n","Epoch: [1][27400/36908] Elapsed 292m 46s (remain 101m 34s) Loss: 0.0006(0.0081) Grad: 4016.5093  LR: 0.000006  \n","Epoch: [1][27500/36908] Elapsed 293m 50s (remain 100m 30s) Loss: 0.0021(0.0080) Grad: 5530.7002  LR: 0.000006  \n","Epoch: [1][27600/36908] Elapsed 294m 54s (remain 99m 26s) Loss: 0.0199(0.0080) Grad: 73937.0547  LR: 0.000006  \n","Epoch: [1][27700/36908] Elapsed 295m 58s (remain 98m 22s) Loss: 0.0015(0.0080) Grad: 1021.9071  LR: 0.000006  \n","Epoch: [1][27800/36908] Elapsed 297m 2s (remain 97m 18s) Loss: 0.0062(0.0080) Grad: 70187.5156  LR: 0.000005  \n","Epoch: [1][27900/36908] Elapsed 298m 7s (remain 96m 14s) Loss: 0.0019(0.0080) Grad: 852.7922  LR: 0.000005  \n","Epoch: [1][28000/36908] Elapsed 299m 11s (remain 95m 10s) Loss: 0.0101(0.0080) Grad: 27222.6484  LR: 0.000005  \n","Epoch: [1][28100/36908] Elapsed 300m 15s (remain 94m 6s) Loss: 0.0399(0.0080) Grad: 56019.6133  LR: 0.000005  \n","Epoch: [1][28200/36908] Elapsed 301m 20s (remain 93m 2s) Loss: 0.0002(0.0080) Grad: 179.8485  LR: 0.000005  \n","Epoch: [1][28300/36908] Elapsed 302m 24s (remain 91m 58s) Loss: 0.0019(0.0079) Grad: 385.7022  LR: 0.000005  \n","Epoch: [1][28400/36908] Elapsed 303m 29s (remain 90m 54s) Loss: 0.0036(0.0079) Grad: 21865.1426  LR: 0.000005  \n","Epoch: [1][28500/36908] Elapsed 304m 34s (remain 89m 50s) Loss: 0.0020(0.0079) Grad: 2191.9736  LR: 0.000005  \n","Epoch: [1][28600/36908] Elapsed 305m 39s (remain 88m 46s) Loss: 0.0015(0.0079) Grad: 3185.3252  LR: 0.000005  \n","Epoch: [1][28700/36908] Elapsed 306m 44s (remain 87m 42s) Loss: 0.0145(0.0079) Grad: 60021.5234  LR: 0.000005  \n","Epoch: [1][28800/36908] Elapsed 307m 49s (remain 86m 38s) Loss: 0.0036(0.0079) Grad: 24920.4902  LR: 0.000005  \n","Epoch: [1][28900/36908] Elapsed 308m 54s (remain 85m 34s) Loss: 0.0069(0.0079) Grad: 11906.0977  LR: 0.000005  \n","Epoch: [1][29000/36908] Elapsed 309m 59s (remain 84m 30s) Loss: 0.0025(0.0079) Grad: 565.9757  LR: 0.000005  \n","Epoch: [1][29100/36908] Elapsed 311m 4s (remain 83m 27s) Loss: 0.0040(0.0078) Grad: 938.9890  LR: 0.000005  \n","Epoch: [1][29200/36908] Elapsed 312m 9s (remain 82m 23s) Loss: 0.0056(0.0078) Grad: 6292.8086  LR: 0.000005  \n","Epoch: [1][29300/36908] Elapsed 313m 13s (remain 81m 19s) Loss: 0.0015(0.0078) Grad: 202.5070  LR: 0.000005  \n","Epoch: [1][29400/36908] Elapsed 314m 19s (remain 80m 15s) Loss: 0.0006(0.0078) Grad: 618.3275  LR: 0.000005  \n","Epoch: [1][29500/36908] Elapsed 315m 23s (remain 79m 11s) Loss: 0.0043(0.0078) Grad: 2797.9692  LR: 0.000004  \n","Epoch: [1][29600/36908] Elapsed 316m 28s (remain 78m 7s) Loss: 0.0061(0.0078) Grad: 3575.1426  LR: 0.000004  \n","Epoch: [1][29700/36908] Elapsed 317m 33s (remain 77m 3s) Loss: 0.0014(0.0078) Grad: 1257.7366  LR: 0.000004  \n","Epoch: [1][29800/36908] Elapsed 318m 37s (remain 75m 59s) Loss: 0.0031(0.0078) Grad: 924.8293  LR: 0.000004  \n","Epoch: [1][29900/36908] Elapsed 319m 43s (remain 74m 55s) Loss: 0.0036(0.0078) Grad: 8236.4219  LR: 0.000004  \n","Epoch: [1][30000/36908] Elapsed 320m 48s (remain 73m 51s) Loss: 0.0034(0.0077) Grad: 7775.3931  LR: 0.000004  \n","Epoch: [1][30100/36908] Elapsed 321m 52s (remain 72m 47s) Loss: 0.0112(0.0077) Grad: 26043.2305  LR: 0.000004  \n","Epoch: [1][30200/36908] Elapsed 322m 57s (remain 71m 43s) Loss: 0.0025(0.0077) Grad: 2239.9648  LR: 0.000004  \n","Epoch: [1][30300/36908] Elapsed 324m 2s (remain 70m 39s) Loss: 0.0001(0.0077) Grad: 45.7417  LR: 0.000004  \n","Epoch: [1][30400/36908] Elapsed 325m 7s (remain 69m 35s) Loss: 0.0037(0.0077) Grad: 3267.9663  LR: 0.000004  \n","Epoch: [1][30500/36908] Elapsed 326m 12s (remain 68m 31s) Loss: 0.0013(0.0077) Grad: 702.8437  LR: 0.000004  \n","Epoch: [1][30600/36908] Elapsed 327m 17s (remain 67m 27s) Loss: 0.0046(0.0077) Grad: 246.0722  LR: 0.000004  \n","Epoch: [1][30700/36908] Elapsed 328m 22s (remain 66m 23s) Loss: 0.0097(0.0077) Grad: 108127.8125  LR: 0.000004  \n","Epoch: [1][30800/36908] Elapsed 329m 27s (remain 65m 19s) Loss: 0.0037(0.0077) Grad: 993.1199  LR: 0.000004  \n","Epoch: [1][30900/36908] Elapsed 330m 32s (remain 64m 15s) Loss: 0.0092(0.0076) Grad: 44242.0742  LR: 0.000004  \n","Epoch: [1][31000/36908] Elapsed 331m 37s (remain 63m 11s) Loss: 0.0118(0.0076) Grad: 29725.9688  LR: 0.000004  \n","Epoch: [1][31100/36908] Elapsed 332m 42s (remain 62m 7s) Loss: 0.0041(0.0076) Grad: 25775.6738  LR: 0.000003  \n","Epoch: [1][31200/36908] Elapsed 333m 46s (remain 61m 3s) Loss: 0.0025(0.0076) Grad: 914.0459  LR: 0.000003  \n","Epoch: [1][31300/36908] Elapsed 334m 51s (remain 59m 59s) Loss: 0.0013(0.0076) Grad: 275.3825  LR: 0.000003  \n","Epoch: [1][31400/36908] Elapsed 335m 56s (remain 58m 54s) Loss: 0.0001(0.0076) Grad: 135.0477  LR: 0.000003  \n","Epoch: [1][31500/36908] Elapsed 337m 1s (remain 57m 50s) Loss: 0.0043(0.0076) Grad: 172.1572  LR: 0.000003  \n","Epoch: [1][31600/36908] Elapsed 338m 6s (remain 56m 46s) Loss: 0.0015(0.0076) Grad: 346.3256  LR: 0.000003  \n","Epoch: [1][31700/36908] Elapsed 339m 11s (remain 55m 42s) Loss: 0.0025(0.0076) Grad: 918.8249  LR: 0.000003  \n","Epoch: [1][31800/36908] Elapsed 340m 16s (remain 54m 38s) Loss: 0.0010(0.0075) Grad: 1040.1505  LR: 0.000003  \n","Epoch: [1][31900/36908] Elapsed 341m 21s (remain 53m 34s) Loss: 0.0051(0.0075) Grad: 11630.6523  LR: 0.000003  \n","Epoch: [1][32000/36908] Elapsed 342m 26s (remain 52m 30s) Loss: 0.0009(0.0075) Grad: 644.5957  LR: 0.000003  \n","Epoch: [1][32100/36908] Elapsed 343m 30s (remain 51m 26s) Loss: 0.0075(0.0075) Grad: 12616.4375  LR: 0.000003  \n","Epoch: [1][32200/36908] Elapsed 344m 35s (remain 50m 22s) Loss: 0.0006(0.0075) Grad: 54.1722  LR: 0.000003  \n","Epoch: [1][32300/36908] Elapsed 345m 40s (remain 49m 18s) Loss: 0.0052(0.0075) Grad: 4018.0276  LR: 0.000003  \n","Epoch: [1][32400/36908] Elapsed 346m 45s (remain 48m 14s) Loss: 0.0032(0.0075) Grad: 35704.1445  LR: 0.000003  \n","Epoch: [1][32500/36908] Elapsed 347m 50s (remain 47m 9s) Loss: 0.0010(0.0075) Grad: 43680.6797  LR: 0.000003  \n","Epoch: [1][32600/36908] Elapsed 348m 54s (remain 46m 5s) Loss: 0.0012(0.0075) Grad: 2238.9922  LR: 0.000003  \n","Epoch: [1][32700/36908] Elapsed 349m 59s (remain 45m 1s) Loss: 0.0115(0.0075) Grad: 5269.2910  LR: 0.000003  \n","Epoch: [1][32800/36908] Elapsed 351m 3s (remain 43m 57s) Loss: 0.0013(0.0075) Grad: 4929.0552  LR: 0.000002  \n","Epoch: [1][32900/36908] Elapsed 352m 8s (remain 42m 53s) Loss: 0.0051(0.0074) Grad: 8583.0020  LR: 0.000002  \n","Epoch: [1][33000/36908] Elapsed 353m 12s (remain 41m 49s) Loss: 0.0045(0.0074) Grad: 16693.0566  LR: 0.000002  \n","Epoch: [1][33100/36908] Elapsed 354m 17s (remain 40m 44s) Loss: 0.0042(0.0074) Grad: 2050.1606  LR: 0.000002  \n","Epoch: [1][33200/36908] Elapsed 355m 22s (remain 39m 40s) Loss: 0.0011(0.0074) Grad: 1462.8024  LR: 0.000002  \n","Epoch: [1][33300/36908] Elapsed 356m 26s (remain 38m 36s) Loss: 0.0010(0.0074) Grad: 158.7870  LR: 0.000002  \n","Epoch: [1][33400/36908] Elapsed 357m 31s (remain 37m 32s) Loss: 0.0040(0.0074) Grad: 48602.8711  LR: 0.000002  \n","Epoch: [1][33500/36908] Elapsed 358m 36s (remain 36m 28s) Loss: 0.0022(0.0074) Grad: 392.9979  LR: 0.000002  \n","Epoch: [1][33600/36908] Elapsed 359m 41s (remain 35m 24s) Loss: 0.0049(0.0074) Grad: 3364.4880  LR: 0.000002  \n","Epoch: [1][33700/36908] Elapsed 360m 46s (remain 34m 19s) Loss: 0.0013(0.0074) Grad: 86844.8438  LR: 0.000002  \n","Epoch: [1][33800/36908] Elapsed 361m 51s (remain 33m 15s) Loss: 0.0016(0.0074) Grad: 172.9519  LR: 0.000002  \n","Epoch: [1][33900/36908] Elapsed 362m 56s (remain 32m 11s) Loss: 0.0009(0.0074) Grad: 754.3033  LR: 0.000002  \n","Epoch: [1][34000/36908] Elapsed 364m 1s (remain 31m 7s) Loss: 0.0089(0.0073) Grad: 128044.6172  LR: 0.000002  \n","Epoch: [1][34100/36908] Elapsed 365m 5s (remain 30m 3s) Loss: 0.0016(0.0073) Grad: 10186.9854  LR: 0.000002  \n","Epoch: [1][34200/36908] Elapsed 366m 10s (remain 28m 58s) Loss: 0.0049(0.0073) Grad: 23430.2715  LR: 0.000002  \n","Epoch: [1][34300/36908] Elapsed 367m 14s (remain 27m 54s) Loss: 0.0022(0.0073) Grad: 47592.8633  LR: 0.000002  \n","Epoch: [1][34400/36908] Elapsed 368m 18s (remain 26m 50s) Loss: 0.0020(0.0073) Grad: 2939.5830  LR: 0.000002  \n","Epoch: [1][34500/36908] Elapsed 369m 22s (remain 25m 46s) Loss: 0.0040(0.0073) Grad: 29679.8008  LR: 0.000001  \n","Epoch: [1][34600/36908] Elapsed 370m 27s (remain 24m 42s) Loss: 0.0069(0.0073) Grad: 58351.8867  LR: 0.000001  \n","Epoch: [1][34700/36908] Elapsed 371m 32s (remain 23m 37s) Loss: 0.0010(0.0073) Grad: 167.9102  LR: 0.000001  \n","Epoch: [1][34800/36908] Elapsed 372m 37s (remain 22m 33s) Loss: 0.0102(0.0073) Grad: 234965.7812  LR: 0.000001  \n","Epoch: [1][34900/36908] Elapsed 373m 42s (remain 21m 29s) Loss: 0.0018(0.0073) Grad: 2300.3320  LR: 0.000001  \n","Epoch: [1][35000/36908] Elapsed 374m 46s (remain 20m 25s) Loss: 0.0015(0.0073) Grad: 6944.6562  LR: 0.000001  \n","Epoch: [1][35100/36908] Elapsed 375m 50s (remain 19m 20s) Loss: 0.0009(0.0072) Grad: 12358.8057  LR: 0.000001  \n","Epoch: [1][35200/36908] Elapsed 376m 54s (remain 18m 16s) Loss: 0.0166(0.0072) Grad: 267435.1562  LR: 0.000001  \n","Epoch: [1][35300/36908] Elapsed 377m 58s (remain 17m 12s) Loss: 0.0062(0.0072) Grad: 10129.5635  LR: 0.000001  \n","Epoch: [1][35400/36908] Elapsed 379m 1s (remain 16m 8s) Loss: 0.0083(0.0072) Grad: 5928.7700  LR: 0.000001  \n","Epoch: [1][35500/36908] Elapsed 380m 5s (remain 15m 3s) Loss: 0.0097(0.0072) Grad: 3802.0400  LR: 0.000001  \n","Epoch: [1][35600/36908] Elapsed 381m 9s (remain 13m 59s) Loss: 0.0055(0.0072) Grad: 3079.6401  LR: 0.000001  \n","Epoch: [1][35700/36908] Elapsed 382m 12s (remain 12m 55s) Loss: 0.0054(0.0072) Grad: 28186.9277  LR: 0.000001  \n","Epoch: [1][35800/36908] Elapsed 383m 16s (remain 11m 51s) Loss: 0.0023(0.0072) Grad: 2211.1353  LR: 0.000001  \n","Epoch: [1][35900/36908] Elapsed 384m 20s (remain 10m 46s) Loss: 0.0136(0.0072) Grad: 81712.1094  LR: 0.000001  \n","Epoch: [1][36000/36908] Elapsed 385m 24s (remain 9m 42s) Loss: 0.0036(0.0072) Grad: 43060.2188  LR: 0.000001  \n","Epoch: [1][36100/36908] Elapsed 386m 29s (remain 8m 38s) Loss: 0.0082(0.0072) Grad: 41055.0547  LR: 0.000000  \n","Epoch: [1][36200/36908] Elapsed 387m 33s (remain 7m 34s) Loss: 0.0030(0.0072) Grad: 19301.3125  LR: 0.000000  \n","Epoch: [1][36300/36908] Elapsed 388m 38s (remain 6m 29s) Loss: 0.0085(0.0072) Grad: 67514.4062  LR: 0.000000  \n","Epoch: [1][36400/36908] Elapsed 389m 43s (remain 5m 25s) Loss: 0.0023(0.0071) Grad: 26159.9375  LR: 0.000000  \n","Epoch: [1][36500/36908] Elapsed 390m 48s (remain 4m 21s) Loss: 0.0019(0.0071) Grad: 3255.3091  LR: 0.000000  \n","Epoch: [1][36600/36908] Elapsed 391m 53s (remain 3m 17s) Loss: 0.0199(0.0071) Grad: 205591.9062  LR: 0.000000  \n","Epoch: [1][36700/36908] Elapsed 392m 58s (remain 2m 12s) Loss: 0.0024(0.0071) Grad: 10831.6279  LR: 0.000000  \n","Epoch: [1][36800/36908] Elapsed 394m 2s (remain 1m 8s) Loss: 0.0022(0.0071) Grad: 744.1564  LR: 0.000000  \n","Epoch: [1][36900/36908] Elapsed 395m 7s (remain 0m 4s) Loss: 0.0005(0.0071) Grad: 108.9304  LR: 0.000000  \n","Epoch: [1][36907/36908] Elapsed 395m 12s (remain 0m 0s) Loss: 0.0008(0.0071) Grad: 9477.3945  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 11m 38s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 24s (remain 4m 24s) Loss: 0.0007(0.0058) \n","EVAL: [200/1192] Elapsed 0m 48s (remain 3m 57s) Loss: 0.0001(0.0071) \n","EVAL: [300/1192] Elapsed 1m 12s (remain 3m 33s) Loss: 0.0014(0.0108) \n","EVAL: [400/1192] Elapsed 1m 36s (remain 3m 10s) Loss: 0.0258(0.0110) \n","EVAL: [500/1192] Elapsed 2m 0s (remain 2m 45s) Loss: 0.0361(0.0102) \n","EVAL: [600/1192] Elapsed 2m 23s (remain 2m 21s) Loss: 0.1338(0.0102) \n","EVAL: [700/1192] Elapsed 2m 47s (remain 1m 57s) Loss: 0.0055(0.0115) \n","EVAL: [800/1192] Elapsed 3m 11s (remain 1m 33s) Loss: 0.0173(0.0111) \n","EVAL: [900/1192] Elapsed 3m 34s (remain 1m 9s) Loss: 0.0002(0.0107) \n","EVAL: [1000/1192] Elapsed 3m 58s (remain 0m 45s) Loss: 0.0000(0.0104) \n","EVAL: [1100/1192] Elapsed 4m 22s (remain 0m 21s) Loss: 0.0068(0.0100) \n","EVAL: [1191/1192] Elapsed 4m 43s (remain 0m 0s) Loss: 0.0106(0.0094) \n","Epoch 1 - avg_train_loss: 0.0071  avg_val_loss: 0.0094  time: 23998s\n","Epoch 1 - Score: 0.8818\n","Epoch 1 - Save Best Score: 0.8818 Model\n","best_thres: 0.48  score: 0.88221\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n","load weights from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp086/fold1_best.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5142db37f4b54a7aa8bf53084dcd22f5"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception ignored in: <function _ConnectionBase.__del__ at 0x7f1c33db5950>\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 132, in __del__\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n"]}],"source":["if __name__ == \"__main__\":\n","    main()"],"id":"proprietary-civilian"},{"cell_type":"code","execution_count":35,"metadata":{"id":"N5kZWfSSfJMf","executionInfo":{"status":"ok","timestamp":1650225400707,"user_tz":-540,"elapsed":11,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":[""],"id":"N5kZWfSSfJMf"}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"name":"nbme-exp086.ipynb","provenance":[{"file_id":"1z5-LBslyd-6MGCBnC92CrbNun5FS-a0p","timestamp":1650034002077},{"file_id":"1EAQeYx7ZDlc73HHqsOI51H0I6B4rJ9nW","timestamp":1649764812129},{"file_id":"1Un3kcPOvK7CdpO-7Pd7r0upTnePSOWdv","timestamp":1649491695188},{"file_id":"13PUc1BK1XquiZCrLMEB3RguGfJtqCZgm","timestamp":1647850264218},{"file_id":"1wqr1Y1MTpmofNqOtVD8cCBYlPZoPt3XQ","timestamp":1647823100264}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"papermill":{"default_parameters":{},"duration":641.572862,"end_time":"2022-02-27T11:39:50.972497","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-02-27T11:29:09.399635","version":"2.3.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ac860605be6d4ccb80defbef465af2cf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d738411dbae4413fabbe9e58623623d2","IPY_MODEL_1022bca3b4c14cb59584db471d6b6567","IPY_MODEL_dff17845069e4dbe981186360646257e"],"layout":"IPY_MODEL_a7297ea57e944ef299a9e4191b7eedb9"}},"d738411dbae4413fabbe9e58623623d2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1fc04917e4cf4ea39aa153271a9175ca","placeholder":"​","style":"IPY_MODEL_6bd95d5cb94243bca44933ca75ef76cf","value":"Downloading: 100%"}},"1022bca3b4c14cb59584db471d6b6567":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_474304937e824ef4ac313a3add81b9d4","max":52,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7c6b58242f81489184b58ea972d49af3","value":52}},"dff17845069e4dbe981186360646257e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9bc718c584934caab95653139f5a4f1c","placeholder":"​","style":"IPY_MODEL_59949601219640d496830cdde9dd7105","value":" 52.0/52.0 [00:00&lt;00:00, 1.92kB/s]"}},"a7297ea57e944ef299a9e4191b7eedb9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fc04917e4cf4ea39aa153271a9175ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bd95d5cb94243bca44933ca75ef76cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"474304937e824ef4ac313a3add81b9d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c6b58242f81489184b58ea972d49af3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9bc718c584934caab95653139f5a4f1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59949601219640d496830cdde9dd7105":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2835be046e6743349b6a6aa83242dced":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fce3cb20a54a4b748046031c5f23228d","IPY_MODEL_be4044df99114b5796d225d8b93086fd","IPY_MODEL_4dc5f7fd6b8548b6b473267bb677155a"],"layout":"IPY_MODEL_2efb2ba8d65540cba144906bfced60a0"}},"fce3cb20a54a4b748046031c5f23228d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e29e07a7200645c8ace1fdd8123f42b9","placeholder":"​","style":"IPY_MODEL_9636559332e841969e13834a4e12ec9b","value":"Downloading: 100%"}},"be4044df99114b5796d225d8b93086fd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7bb76f0b82e4f3cb8e167b8de7be09e","max":2464616,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0334e068282947d19fabff46286630ec","value":2464616}},"4dc5f7fd6b8548b6b473267bb677155a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ced6fc3b543c41da9159f653d6d6a8bb","placeholder":"​","style":"IPY_MODEL_ce81d8df9c104dffa1f371b33ebe3b1c","value":" 2.35M/2.35M [00:00&lt;00:00, 30.8MB/s]"}},"2efb2ba8d65540cba144906bfced60a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e29e07a7200645c8ace1fdd8123f42b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9636559332e841969e13834a4e12ec9b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7bb76f0b82e4f3cb8e167b8de7be09e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0334e068282947d19fabff46286630ec":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ced6fc3b543c41da9159f653d6d6a8bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce81d8df9c104dffa1f371b33ebe3b1c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e52eb5ac96df49b5ad5e368afb1b9769":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_849895b02b56453e8e88640459ec9fc9","IPY_MODEL_02fc1fa8472a4f2082543ea4fd5b5dcd","IPY_MODEL_29b2a8b227e94a0d9eb0e38d1c316118"],"layout":"IPY_MODEL_ee54527400a74635a3c41f36de397a4e"}},"849895b02b56453e8e88640459ec9fc9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4df97bc057c490890ac72f16d42e1f3","placeholder":"​","style":"IPY_MODEL_33052307133d47adb3d59e7de79e3b8e","value":"Downloading: 100%"}},"02fc1fa8472a4f2082543ea4fd5b5dcd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4adc6c819b404afe95d4a52401a7156f","max":580,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2de3b3b5335c42938db7a9f7243143f8","value":580}},"29b2a8b227e94a0d9eb0e38d1c316118":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49a1e9b98f6d46a6a4a3fc4717703300","placeholder":"​","style":"IPY_MODEL_9ed0ff37e0cf4afe98f6b4333548eb0e","value":" 580/580 [00:00&lt;00:00, 17.4kB/s]"}},"ee54527400a74635a3c41f36de397a4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4df97bc057c490890ac72f16d42e1f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33052307133d47adb3d59e7de79e3b8e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4adc6c819b404afe95d4a52401a7156f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2de3b3b5335c42938db7a9f7243143f8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"49a1e9b98f6d46a6a4a3fc4717703300":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ed0ff37e0cf4afe98f6b4333548eb0e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a93876fd9244b8594dd5472b0abf0b6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1b5187839977435ba3cbf35c80fb1c8a","IPY_MODEL_e3c4faf26d2a41cf93fa0995e40d3dcd","IPY_MODEL_1175cce67a6942728349a9a228a2ba5c"],"layout":"IPY_MODEL_f14079b0364a4a3d82f175682eb42e9d"}},"1b5187839977435ba3cbf35c80fb1c8a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e74a8c230b2247a0bc83fe351933d04f","placeholder":"​","style":"IPY_MODEL_18cf8bc82c4f4d888f8c800839e4f1b7","value":"100%"}},"e3c4faf26d2a41cf93fa0995e40d3dcd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5978915477c24dd9961d340739f874ce","max":42146,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ac54243a971746938f04112d635652d0","value":42146}},"1175cce67a6942728349a9a228a2ba5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8a80376aaff4e798d9ce9783856943b","placeholder":"​","style":"IPY_MODEL_6bc98b8d28a044928883739d73ee2ee7","value":" 42146/42146 [00:22&lt;00:00, 2072.82it/s]"}},"f14079b0364a4a3d82f175682eb42e9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e74a8c230b2247a0bc83fe351933d04f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18cf8bc82c4f4d888f8c800839e4f1b7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5978915477c24dd9961d340739f874ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac54243a971746938f04112d635652d0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f8a80376aaff4e798d9ce9783856943b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bc98b8d28a044928883739d73ee2ee7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c6da57033d304960ad821287817f0144":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e967f7ec4b96484997f9efe328653ddb","IPY_MODEL_c0f663a411ba4349bfa0cd1c1f33582d","IPY_MODEL_a20b4ace20f04bcd882b27be2b958769"],"layout":"IPY_MODEL_fd74c58f0d9b4d05aa10e196962eadd4"}},"e967f7ec4b96484997f9efe328653ddb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63ff4c660b0b4198b45e469abde02303","placeholder":"​","style":"IPY_MODEL_7ea04854b5924b8ca46a9db186c164f9","value":"100%"}},"c0f663a411ba4349bfa0cd1c1f33582d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5dd4d2689564936b6a6a7e04e554af1","max":143,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e65218f1de5243d8a074129e8943c08b","value":143}},"a20b4ace20f04bcd882b27be2b958769":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d6b152d5ea54929933ead2cad6054a2","placeholder":"​","style":"IPY_MODEL_c5989a8f87ba442d9af7069b66e0c3ab","value":" 143/143 [00:00&lt;00:00, 3319.47it/s]"}},"fd74c58f0d9b4d05aa10e196962eadd4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63ff4c660b0b4198b45e469abde02303":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ea04854b5924b8ca46a9db186c164f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5dd4d2689564936b6a6a7e04e554af1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e65218f1de5243d8a074129e8943c08b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4d6b152d5ea54929933ead2cad6054a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5989a8f87ba442d9af7069b66e0c3ab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"50cc101e9e704b23acd7b8bc85c6ab70":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_19b460e2f8f6424aac3b60b32717a95f","IPY_MODEL_9ca2e0ebb76a4a14ae5f55d7860fb96b","IPY_MODEL_77b223f37a9e440db5cea798d0b95ec6"],"layout":"IPY_MODEL_282ad1b60a2b4638a19618382d4e2745"}},"19b460e2f8f6424aac3b60b32717a95f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_668dedf9daf942e28b04c411d21c7a0b","placeholder":"​","style":"IPY_MODEL_fab427187a1641048544394ab2d9eb76","value":"100%"}},"9ca2e0ebb76a4a14ae5f55d7860fb96b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_36d9c157700545db86375e7b1dc7a493","max":42146,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ab04f8ecac424a00966604bf9c5c9d16","value":42146}},"77b223f37a9e440db5cea798d0b95ec6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e6e58633efb4e5ca3dc5d68e058fd7f","placeholder":"​","style":"IPY_MODEL_bed8714f95f54ba7a7ac9e28bb8bd328","value":" 42146/42146 [00:00&lt;00:00, 769227.68it/s]"}},"282ad1b60a2b4638a19618382d4e2745":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"668dedf9daf942e28b04c411d21c7a0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fab427187a1641048544394ab2d9eb76":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"36d9c157700545db86375e7b1dc7a493":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab04f8ecac424a00966604bf9c5c9d16":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4e6e58633efb4e5ca3dc5d68e058fd7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bed8714f95f54ba7a7ac9e28bb8bd328":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3df9e2c24b36418ca041546a69d9e042":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d06147a3f80d4ca2b2981d8b150f3dbe","IPY_MODEL_7d737ce3eb724847bb8f6556c4558df3","IPY_MODEL_4546dfab613e4840ad69c15f9ff25693"],"layout":"IPY_MODEL_55345548bbea4594aeaad94ee870b4b3"}},"d06147a3f80d4ca2b2981d8b150f3dbe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_07c2347c4a8c42f58e9fc68966dcc4ba","placeholder":"​","style":"IPY_MODEL_8d5ddeaccd7b47f8bac23b0217724a42","value":"Downloading: 100%"}},"7d737ce3eb724847bb8f6556c4558df3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_50631c1b50cc48c3870fd13da88ccd55","max":873673253,"min":0,"orientation":"horizontal","style":"IPY_MODEL_46535d3b8fca42acba3beeafeb3e7144","value":873673253}},"4546dfab613e4840ad69c15f9ff25693":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_13f10d90402d416891351c70d25cf14b","placeholder":"​","style":"IPY_MODEL_1cfbd8fd3a144d71b0f262386a12de95","value":" 833M/833M [00:14&lt;00:00, 58.9MB/s]"}},"55345548bbea4594aeaad94ee870b4b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07c2347c4a8c42f58e9fc68966dcc4ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d5ddeaccd7b47f8bac23b0217724a42":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"50631c1b50cc48c3870fd13da88ccd55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46535d3b8fca42acba3beeafeb3e7144":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"13f10d90402d416891351c70d25cf14b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cfbd8fd3a144d71b0f262386a12de95":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5142db37f4b54a7aa8bf53084dcd22f5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e1aa543165e94b87ae4a5579fb7db0d4","IPY_MODEL_00324af6587e4680824128aa79aebbb2","IPY_MODEL_279f57e6702b4d31a3488b4dc692efbf"],"layout":"IPY_MODEL_574c8435ee3247898628a316f18c0bbd"}},"e1aa543165e94b87ae4a5579fb7db0d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8612a466f63946799cc212e97e574d34","placeholder":"​","style":"IPY_MODEL_a5d2624879c04586ad09b7746dc5508f","value":"100%"}},"00324af6587e4680824128aa79aebbb2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_82f75bd114f4445f9bc466ba81c63914","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6886c89be6774feab01f9538112fd57b","value":2}},"279f57e6702b4d31a3488b4dc692efbf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c200ea7980574b8aa7dc067fb735a043","placeholder":"​","style":"IPY_MODEL_5b36660022d4468ba9e2c0e90a9445dd","value":" 2/2 [00:01&lt;00:00,  2.09it/s]"}},"574c8435ee3247898628a316f18c0bbd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8612a466f63946799cc212e97e574d34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5d2624879c04586ad09b7746dc5508f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"82f75bd114f4445f9bc466ba81c63914":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6886c89be6774feab01f9538112fd57b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c200ea7980574b8aa7dc067fb735a043":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b36660022d4468ba9e2c0e90a9445dd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}