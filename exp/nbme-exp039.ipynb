{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "advance-arabic",
   "metadata": {
    "id": "aa1f8e80"
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-ballot",
   "metadata": {
    "id": "c0138fac"
   },
   "source": [
    "- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-slide",
   "metadata": {
    "id": "cf1dfda9"
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "descending-module",
   "metadata": {
    "id": "a7a78d25"
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"nbme-exp039\"\n",
    "ENV = \"local\"\n",
    "DEBUG_MODE = False\n",
    "SUBMISSION_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "proper-juvenile",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.16.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "controlling-speed",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sentencepiece' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-883fa73f6a6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msentencepiece\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'sentencepiece' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import sentencepiece\n",
    "sentencepiece.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "organizational-memphis",
   "metadata": {
    "id": "4ecc4e4d"
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    env=ENV\n",
    "    exp_name=EXP_NAME\n",
    "    debug=DEBUG_MODE\n",
    "    submission=SUBMISSION_MODE\n",
    "    apex=True\n",
    "    input_dir=None\n",
    "    output_dir=None\n",
    "    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n",
    "    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n",
    "    competition_name=\"nbme-score-clinical-patient-notes\"\n",
    "    id_col=\"id\"\n",
    "    target_col=\"location\"\n",
    "    pretrained_model_name=\"roberta-large\"\n",
    "    tokenizer=None\n",
    "    max_len=None\n",
    "    output_dim=1\n",
    "    dropout=0.2\n",
    "    num_workers=4\n",
    "    batch_size=4\n",
    "    lr=2e-5\n",
    "    betas=(0.9, 0.98)\n",
    "    weight_decay=0.1\n",
    "    num_warmup_steps_rate=0.1\n",
    "    batch_scheduler=True\n",
    "    epochs=5\n",
    "    n_fold=5\n",
    "    train_fold=[0, 1, 2, 3, 4]\n",
    "    seed=71\n",
    "    gradient_accumulation_steps=2\n",
    "    max_grad_norm=1000\n",
    "    print_freq=100\n",
    "    train=True\n",
    "    inference=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mechanical-watch",
   "metadata": {
    "id": "3894c88b"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.train_fold = [0, 1]\n",
    "\n",
    "if CFG.submission:\n",
    "    CFG.train = False\n",
    "    CFG.inference = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-concentrate",
   "metadata": {
    "id": "31768c85"
   },
   "source": [
    "## Directory Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tribal-camel",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4693,
     "status": "ok",
     "timestamp": 1646023773081,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "00e7d967",
    "outputId": "d56a483d-9171-44e6-856a-a90dfe8e0ac3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "print(CFG.env)\n",
    "if CFG.env == \"colab\":\n",
    "    # colab環境\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "    # install packages\n",
    "    !pip install transformers\n",
    "\n",
    "elif CFG.env == \"local\":\n",
    "    # ローカルサーバ\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "\n",
    "elif CFG.env == \"kaggle\":\n",
    "    # kaggle環境\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "antique-southeast",
   "metadata": {
    "id": "d726b7d9"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import BartModel,BertModel,BertTokenizer\n",
    "from transformers import DebertaModel,DebertaTokenizer\n",
    "from transformers import RobertaModel,RobertaTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-phenomenon",
   "metadata": {
    "id": "b6d82f71"
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "vertical-interview",
   "metadata": {
    "id": "95abbe2c"
   },
   "outputs": [],
   "source": [
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adequate-thong",
   "metadata": {
    "id": "832ee36d"
   },
   "outputs": [],
   "source": [
    "def create_labels_for_scoring(df):\n",
    "    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n",
    "    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, \"location\"]\n",
    "        if lst:\n",
    "            new_lst = \";\".join(lst)\n",
    "            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n",
    "\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df[\"location_for_create_labels\"].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, token_probs, tokenizer):\n",
    "    res = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n",
    "        encoded = tokenizer(\n",
    "            text=text,\n",
    "            max_length=CFG.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n",
    "            start, end = offset_mapping\n",
    "            res[i][start:end] = pred\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_predicted_location_str(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def scoring(df, th=0.5):\n",
    "    labels = create_labels_for_scoring(df)\n",
    "\n",
    "    token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n",
    "    char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n",
    "    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n",
    "    preds = get_predictions(predicted_location_str)\n",
    "\n",
    "    score = get_score(labels, preds)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_best_thres(oof_df):\n",
    "    def f1_opt(x):\n",
    "        return -1 * scoring(oof_df, th=x)\n",
    "\n",
    "    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n",
    "    return best_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "accessory-compact",
   "metadata": {
    "id": "918828a7"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "organizational-potential",
   "metadata": {
    "id": "d02a78e1"
   },
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-memorial",
   "metadata": {
    "id": "47266f39"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "turned-storage",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 815,
     "status": "ok",
     "timestamp": 1646023777557,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "20fed6da",
    "outputId": "64d3e7ad-0986-4799-f9df-f0242c1977a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 6), (143, 3), (42146, 3), (5, 4))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(CFG.input_dir / \"train.csv\")\n",
    "features = pd.read_csv(CFG.input_dir / \"features.csv\")\n",
    "patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n",
    "test = pd.read_csv(CFG.input_dir / \"test.csv\")\n",
    "\n",
    "train.shape, features.shape, patient_notes.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "little-waterproof",
   "metadata": {
    "id": "e67d0132"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n",
    "    print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-nicholas",
   "metadata": {
    "id": "47bca11a"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "distributed-incident",
   "metadata": {
    "id": "d9c8e9ba"
   },
   "outputs": [],
   "source": [
    "def preprocess_features(features):\n",
    "    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "\n",
    "\n",
    "features = preprocess_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "official-prediction",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1646023777558,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "7ef41e18",
    "outputId": "31edaa7d-c088-495a-95ee-f0d56f97074c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 8), (5, 6))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "shaped-swing",
   "metadata": {
    "id": "8233df16"
   },
   "outputs": [],
   "source": [
    "train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n",
    "train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "stone-riding",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1646023778018,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "e9143e61",
    "outputId": "cf45e2d7-5f66-4d96-c6e2-da79c888bcc8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4399\n",
       "1    8181\n",
       "2    1296\n",
       "3     287\n",
       "4      99\n",
       "5      27\n",
       "6       9\n",
       "7       1\n",
       "8       1\n",
       "Name: annotation_length, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n",
    "display(train['annotation_length'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-travel",
   "metadata": {
    "id": "6bdc7949"
   },
   "source": [
    "## CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ruled-wound",
   "metadata": {
    "id": "c4acf61d"
   },
   "outputs": [],
   "source": [
    "def get_groupkfold(df, group_name):\n",
    "    groups = df[group_name].unique()\n",
    "\n",
    "    kf = KFold(\n",
    "        n_splits=CFG.n_fold,\n",
    "        shuffle=True,\n",
    "        random_state=CFG.seed,\n",
    "    )\n",
    "    folds_ids = []\n",
    "    for i_fold, (_, val_group_idx) in enumerate(kf.split(groups)):\n",
    "        val_group = groups[val_group_idx]\n",
    "        is_val = df[group_name].isin(val_group)\n",
    "        val_idx = df[is_val].index\n",
    "        df.loc[val_idx, \"fold\"] = int(i_fold)\n",
    "\n",
    "    df[\"fold\"] = df[\"fold\"].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "union-bloom",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1646023778018,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "2ca0c08e",
    "outputId": "cfc9c06e-e30c-4cb5-a072-d0cfcfa5fdc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    2902\n",
       "1    2894\n",
       "2    2813\n",
       "3    2791\n",
       "4    2900\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = get_groupkfold(train, \"pn_num\")\n",
    "display(train.groupby(\"fold\").size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-radiation",
   "metadata": {
    "id": "a8560070"
   },
   "source": [
    "## Setup tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ideal-surveillance",
   "metadata": {
    "id": "c316b13f"
   },
   "outputs": [],
   "source": [
    "if CFG.submission:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\", trim_offsets=False)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG.pretrained_model_name, trim_offsets=False)\n",
    "    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n",
    "\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "binary-seeking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'', 0, 0\n",
      "'dad', 0, 3\n",
      "' with', 3, 8\n",
      "' recent', 8, 15\n",
      "' heart', 15, 21\n",
      "' attack', 21, 28\n",
      "'', 0, 0\n",
      "ans\n",
      "\n",
      "'', 0, 0\n",
      "'dad', 0, 3\n",
      "' with', 3, 8\n",
      "' recent', 8, 15\n",
      "' heart', 15, 21\n",
      "' attack', 21, 28\n",
      "'', 0, 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp = 'dad with recent heart attack'\n",
    "encode = tokenizer(tmp, return_offsets_mapping=True)\n",
    "for (start,end) in encode['offset_mapping']:\n",
    "    print(f\"'{tmp[start:end]}', {start}, {end}\")\n",
    "\n",
    "print(\"ans\")\n",
    "print(\"\"\"\n",
    "'', 0, 0\n",
    "'dad', 0, 3\n",
    "' with', 3, 8\n",
    "' recent', 8, 15\n",
    "' heart', 15, 21\n",
    "' attack', 21, 28\n",
    "'', 0, 0\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-ministry",
   "metadata": {
    "id": "e689a7fc"
   },
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "likely-breed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "a31c60ff4dab48e08d2ef9293d85df6d",
      "c40d970496ff447a8c0b80d787b07a4d",
      "40e6583408c447199ff5b94d23601936",
      "1141ae38bc6f473aab89db14fa4eeacf",
      "47b8a7f3d0544d79b30ad02e4222082e",
      "0260998578564385a0b5b9425a0a5ca1",
      "428ca357bd284d199e2558b1f577d79a",
      "2e32fee744ef42e0aaa89a7b03e82427",
      "d51d3aa414db4aa8b0ccae896e671152",
      "ad49cbf6b6e84ccaab873458182f22a1",
      "5375de82ce3a41a8b5550e0a6b4316c1"
     ]
    },
    "executionInfo": {
     "elapsed": 37449,
     "status": "ok",
     "timestamp": 1646023819498,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "df31758e",
    "outputId": "e3ee6910-2896-413b-9bb7-1e1dd630166c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d807c0c7b44e12866162762db4e461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 433\n"
     ]
    }
   ],
   "source": [
    "pn_history_lengths = []\n",
    "tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    pn_history_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(pn_history_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "approximate-admission",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "8a503d1abd884514a1e23101e03c6781",
      "e06e5e9eb0414b6fad63bdc99b44a313",
      "6b04b019813e458080f02bc9111433a6",
      "2e3818222bab4603a896be5976cb8409",
      "eeb468dbb94943fcb30219d4dd98fcab",
      "5e66444e9c714134bd2765cb3b6d1f15",
      "220f78b6119042af8729543465e1234e",
      "7f1d7796e2174485a0d1b1e9a71d7ade",
      "e72cad76f875451a8e2479e2df237575",
      "9754a5f1e61d49c8972df40ee9290375",
      "25bf78e432e641e0a435dc3626c3ee8a"
     ]
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1646023819500,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "3caff24a",
    "outputId": "09841871-9f3a-4e70-a528-07122a0ebba2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d1ec6815a640c7986e90351a28f5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 30\n"
     ]
    }
   ],
   "source": [
    "feature_text_lengths = []\n",
    "tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    feature_text_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(feature_text_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "lesser-sewing",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1646023819500,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "756d83ff",
    "outputId": "02d1e748-4ce8-4d68-f2a2-175f08316e9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 466\n"
     ]
    }
   ],
   "source": [
    "CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n",
    "\n",
    "print(\"max length:\", CFG.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "british-throat",
   "metadata": {
    "id": "054b899a"
   },
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "        self.annotation_lengths = self.df[\"annotation_length\"].values\n",
    "        self.locations = self.df[\"location\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def _create_label(self, pn_history, annotation_length, location_list):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        offset_mapping = encoded[\"offset_mapping\"]\n",
    "        ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n",
    "        label = np.zeros(len(offset_mapping))\n",
    "        label[ignore_idxes] = -1\n",
    "\n",
    "        if annotation_length > 0:\n",
    "            for location in location_list:\n",
    "                for loc in [s.split() for s in location.split(\";\")]:\n",
    "                    start, end = int(loc[0]), int(loc[1])\n",
    "                    start_idx = -1\n",
    "                    end_idx = -1\n",
    "                    for idx in range(len(offset_mapping)):\n",
    "                        if (start_idx == -1) & (start < offset_mapping[idx][0]):\n",
    "                            start_idx = idx - 1\n",
    "                        if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n",
    "                            end_idx = idx + 1\n",
    "                    if start_idx == -1:\n",
    "                        start_idx = end_idx\n",
    "                    if (start_idx != -1) & (end_idx != -1):\n",
    "                        label[start_idx:end_idx] = 1\n",
    "\n",
    "        return torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n",
    "        return input_, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecological-contemporary",
   "metadata": {
    "id": "1d58367c"
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        return input_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-league",
   "metadata": {
    "id": "8c57abef"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "demographic-stanford",
   "metadata": {
    "id": "54f92d89"
   },
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, model_config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if model_config_path is None:\n",
    "            self.model_config = AutoConfig.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "        else:\n",
    "            self.model_config = torch.load(model_config_path)\n",
    "\n",
    "        if pretrained:\n",
    "            self.backbone = AutoModel.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                config=self.model_config,\n",
    "            )\n",
    "            print(f\"Load weight from pretrained\")\n",
    "        else:\n",
    "            #self.backbone = AutoModel.from_config(self.model_config)\n",
    "            itpt = AutoModelForMaskedLM.from_config(self.model_config)\n",
    "            #path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp009/checkpoint-129000/pytorch_model.bin\")\n",
    "            path = \"../output/nbme-score-clinical-patient-notes/nbme-exp033/checkpoint-130170/pytorch_model.bin\"\n",
    "            state_dict = torch.load(path)\n",
    "            itpt.load_state_dict(state_dict, strict=False)\n",
    "            self.backbone = itpt.roberta\n",
    "            print(f\"Load weight from {path}\")\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(self.cfg.dropout),\n",
    "            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h = self.backbone(**inputs)[\"last_hidden_state\"]\n",
    "        output = self.fc(h)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-patio",
   "metadata": {
    "id": "91401041"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "subsequent-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/65938\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "intermediate-island",
   "metadata": {
    "id": "eda8175d"
   },
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    train_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    epoch,\n",
    "    scheduler,\n",
    "    device,\n",
    "):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels) in enumerate(train_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            output = model(inputs)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "\n",
    "        pos_nums = (labels == 1).sum(axis=1)\n",
    "        pos_nums = torch.tensor([[pos_num] * CFG.max_len for pos_num in pos_nums]).view(-1, 1).to(device)\n",
    "        pos_nums = torch.masked_select(pos_nums, labels.view(-1, 1) != -1)\n",
    "        weight = []\n",
    "        for pos_num in pos_nums:\n",
    "            if pos_num == 0:\n",
    "                weight.append(3.0)\n",
    "            else:\n",
    "                weight.append(1.0)\n",
    "        weight = torch.tensor(weight).to(device)\n",
    "        loss = loss * weight\n",
    "\n",
    "        loss = loss.mean()\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if CFG.batch_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n",
    "            print(\n",
    "                \"Epoch: [{0}][{1}/{2}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                \"Grad: {grad_norm:.4f}  \"\n",
    "                \"LR: {lr:.6f}  \"\n",
    "                .format(\n",
    "                    epoch+1,\n",
    "                    step,\n",
    "                    len(train_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n",
    "                    loss=losses,\n",
    "                     grad_norm=grad_norm,\n",
    "                     lr=scheduler.get_lr()[0],\n",
    "                )\n",
    "            )\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "occasional-replacement",
   "metadata": {
    "id": "c44b63a7"
   },
   "outputs": [],
   "source": [
    "def valid_fn(\n",
    "    val_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    device,\n",
    "):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels) in enumerate(val_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "\n",
    "        pos_nums = (labels == 1).sum(axis=1)\n",
    "        pos_nums = torch.tensor([[pos_num] * CFG.max_len for pos_num in pos_nums]).view(-1, 1).to(device)\n",
    "        pos_nums = torch.masked_select(pos_nums, labels.view(-1, 1) != -1)\n",
    "        weight = []\n",
    "        for pos_num in pos_nums:\n",
    "            if pos_num == 0:\n",
    "                weight.append(3.0)\n",
    "            else:\n",
    "                weight.append(1.0)\n",
    "        weight = torch.tensor(weight).to(device)\n",
    "        loss = loss * weight\n",
    "\n",
    "        loss = loss.mean()\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n",
    "            print(\n",
    "                \"EVAL: [{0}/{1}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                .format(\n",
    "                    step, len(val_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n",
    "                    loss=losses,\n",
    "                )\n",
    "            )\n",
    "    preds = np.concatenate(preds)\n",
    "    return losses.avg, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "changed-strip",
   "metadata": {
    "id": "4219ac38"
   },
   "outputs": [],
   "source": [
    "def inference_fn(test_dataloader, model, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preds = []\n",
    "    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "raising-startup",
   "metadata": {
    "id": "014a76b7"
   },
   "outputs": [],
   "source": [
    "def train_loop(df, i_fold, device):\n",
    "    print(f\"========== fold: {i_fold} training ==========\")\n",
    "    train_idx = df[df[\"fold\"] != i_fold].index\n",
    "    val_idx = df[df[\"fold\"] == i_fold].index\n",
    "\n",
    "    train_folds = df.loc[train_idx].reset_index(drop=True)\n",
    "    val_folds = df.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = TrainingDataset(CFG, train_folds)\n",
    "    val_dataset = TrainingDataset(CFG, val_folds)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "    model = CustomModel(CFG, model_config_path=None, pretrained=False)   # itptを使うため\n",
    "    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n",
    "    model.to(device)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in param_optimizer if not any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n",
    "        {\"params\": [p for n, p in param_optimizer if any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=CFG.lr,\n",
    "        betas=CFG.betas,\n",
    "        weight_decay=CFG.weight_decay,\n",
    "    )\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n",
    "    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_train_optimization_steps,\n",
    "    )\n",
    "\n",
    "    #criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    criterion = FocalLoss(reduce=False)\n",
    "    best_score = -1 * np.inf\n",
    "\n",
    "    _ = train_fn(\n",
    "        train_dataloader,\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        0,\n",
    "        scheduler,\n",
    "        device,\n",
    "    )\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_train_optimization_steps,\n",
    "    )\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "        avg_loss = train_fn(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            scheduler,\n",
    "            device,\n",
    "        )\n",
    "        avg_val_loss, val_preds = valid_fn(\n",
    "            val_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "\n",
    "        # scoring\n",
    "        val_folds[[str(i) for i in range(CFG.max_len)]] = val_preds\n",
    "        score = scoring(val_folds, th=0.5)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n",
    "        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"predictions\": val_preds,\n",
    "                },\n",
    "                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "            )\n",
    "\n",
    "    predictions = torch.load(\n",
    "        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )[\"predictions\"]\n",
    "    val_folds[[str(i) for i in range(CFG.max_len)]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return val_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-apache",
   "metadata": {
    "id": "c38fb834"
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "secondary-fruit",
   "metadata": {
    "id": "62d677cd"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for i_fold in range(CFG.n_fold):\n",
    "            if i_fold in CFG.train_fold:\n",
    "                _oof_df = train_loop(train, i_fold, device)\n",
    "                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n",
    "        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    if CFG.submission:\n",
    "        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n",
    "    else:\n",
    "        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    score = scoring(oof_df, th=0.5)\n",
    "    print(f\"Best thres: 0.5, Score: {score:.4f}\")\n",
    "    best_thres = get_best_thres(oof_df)\n",
    "    score = scoring(oof_df, th=best_thres)\n",
    "    print(f\"Best thres: {best_thres}, Score: {score:.4f}\")\n",
    "\n",
    "    if CFG.inference:\n",
    "        test_dataset = TestDataset(CFG, test)\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=CFG.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=CFG.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        predictions = []\n",
    "        for i_fold in CFG.train_fold:\n",
    "            if CFG.submission:\n",
    "                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n",
    "                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n",
    "            else:\n",
    "                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n",
    "\n",
    "            state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "            model.load_state_dict(state[\"model\"])\n",
    "            test_token_probs = inference_fn(test_dataloader, model, device)\n",
    "            test[[f\"fold{i_fold}_{i}\" for i in range(CFG.max_len)]] = test_token_probs\n",
    "            test_char_probs = get_char_probs(test[\"pn_history\"].values, test_token_probs, CFG.tokenizer)\n",
    "            predictions.append(test_char_probs)\n",
    "\n",
    "            del state, test_token_probs, model; gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        predictions = np.mean(predictions, axis=0)\n",
    "        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n",
    "        test[CFG.target_col] = predicted_location_str\n",
    "        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n",
    "        test[[CFG.id_col, CFG.target_col]].to_csv(\n",
    "            CFG.output_dir / \"submission.csv\", index=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "interracial-catering",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "5a00641beddc46eb8da430f2d9999490",
      "04966868d2974cb8b3215a50572c2c94",
      "5a5c41748cba4234a6a6f9aabddfa861",
      "72044a7dbe7d4b5f839f38f6e827ec63",
      "bed6a691643a46d5bd25e03cdc5b73f7",
      "b4d0bd0dea5341a9b03f0092fe3cba39"
     ]
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1646034258180,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "1d4fcf7c",
    "outputId": "1362d223-3d70-4ba7-daa5-14b7300eef5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp033/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2849] Elapsed 0m 0s (remain 45m 45s) Loss: 0.2091(0.2091) Grad: inf  LR: 0.000000  \n",
      "Epoch: [1][100/2849] Elapsed 0m 15s (remain 6m 58s) Loss: 0.1293(0.2120) Grad: 17346.2402  LR: 0.000001  \n",
      "Epoch: [1][200/2849] Elapsed 0m 29s (remain 6m 29s) Loss: 0.2586(0.2160) Grad: 36258.4766  LR: 0.000003  \n",
      "Epoch: [1][300/2849] Elapsed 0m 44s (remain 6m 12s) Loss: 0.1833(0.2119) Grad: 25465.0664  LR: 0.000004  \n",
      "Epoch: [1][400/2849] Elapsed 0m 58s (remain 5m 58s) Loss: 0.1558(0.2081) Grad: 22325.2168  LR: 0.000006  \n",
      "Epoch: [1][500/2849] Elapsed 1m 13s (remain 5m 45s) Loss: 0.1605(0.2022) Grad: 23219.4766  LR: 0.000007  \n",
      "Epoch: [1][600/2849] Elapsed 1m 28s (remain 5m 30s) Loss: 0.1742(0.1967) Grad: 26613.6660  LR: 0.000008  \n",
      "Epoch: [1][700/2849] Elapsed 1m 42s (remain 5m 14s) Loss: 0.1473(0.1898) Grad: 23131.1016  LR: 0.000010  \n",
      "Epoch: [1][800/2849] Elapsed 1m 57s (remain 4m 59s) Loss: 0.1215(0.1831) Grad: 18741.5117  LR: 0.000011  \n",
      "Epoch: [1][900/2849] Elapsed 2m 11s (remain 4m 45s) Loss: 0.1227(0.1759) Grad: 19936.8105  LR: 0.000013  \n",
      "Epoch: [1][1000/2849] Elapsed 2m 26s (remain 4m 30s) Loss: 0.0555(0.1679) Grad: 8931.5713  LR: 0.000014  \n",
      "Epoch: [1][1100/2849] Elapsed 2m 40s (remain 4m 15s) Loss: 0.0706(0.1593) Grad: 11473.8398  LR: 0.000015  \n",
      "Epoch: [1][1200/2849] Elapsed 2m 55s (remain 4m 0s) Loss: 0.0371(0.1512) Grad: 5193.4395  LR: 0.000017  \n",
      "Epoch: [1][1300/2849] Elapsed 3m 9s (remain 3m 45s) Loss: 0.0469(0.1432) Grad: 8750.5967  LR: 0.000018  \n",
      "Epoch: [1][1400/2849] Elapsed 3m 24s (remain 3m 31s) Loss: 0.0375(0.1355) Grad: 6010.2808  LR: 0.000020  \n",
      "Epoch: [1][1500/2849] Elapsed 3m 39s (remain 3m 16s) Loss: 0.0354(0.1284) Grad: 2180.7495  LR: 0.000020  \n",
      "Epoch: [1][1600/2849] Elapsed 3m 53s (remain 3m 2s) Loss: 0.0256(0.1218) Grad: 1270.4016  LR: 0.000020  \n",
      "Epoch: [1][1700/2849] Elapsed 4m 8s (remain 2m 47s) Loss: 0.0222(0.1158) Grad: 2036.8910  LR: 0.000020  \n",
      "Epoch: [1][1800/2849] Elapsed 4m 23s (remain 2m 33s) Loss: 0.0191(0.1103) Grad: 908.3152  LR: 0.000019  \n",
      "Epoch: [1][1900/2849] Elapsed 4m 37s (remain 2m 18s) Loss: 0.0137(0.1053) Grad: 1028.4760  LR: 0.000019  \n",
      "Epoch: [1][2000/2849] Elapsed 4m 52s (remain 2m 3s) Loss: 0.0126(0.1008) Grad: 298.2325  LR: 0.000019  \n",
      "Epoch: [1][2100/2849] Elapsed 5m 6s (remain 1m 49s) Loss: 0.0098(0.0967) Grad: 933.0394  LR: 0.000019  \n",
      "Epoch: [1][2200/2849] Elapsed 5m 21s (remain 1m 34s) Loss: 0.0128(0.0929) Grad: 650.2720  LR: 0.000019  \n",
      "Epoch: [1][2300/2849] Elapsed 5m 36s (remain 1m 20s) Loss: 0.0190(0.0896) Grad: 448.0785  LR: 0.000019  \n",
      "Epoch: [1][2400/2849] Elapsed 5m 52s (remain 1m 5s) Loss: 0.0166(0.0864) Grad: 373.2717  LR: 0.000018  \n",
      "Epoch: [1][2500/2849] Elapsed 6m 6s (remain 0m 51s) Loss: 0.0054(0.0835) Grad: 989.1645  LR: 0.000018  \n",
      "Epoch: [1][2600/2849] Elapsed 6m 21s (remain 0m 36s) Loss: 0.0101(0.0808) Grad: 198.1237  LR: 0.000018  \n",
      "Epoch: [1][2700/2849] Elapsed 6m 36s (remain 0m 21s) Loss: 0.0156(0.0784) Grad: 381.3239  LR: 0.000018  \n",
      "Epoch: [1][2800/2849] Elapsed 6m 51s (remain 0m 7s) Loss: 0.0172(0.0761) Grad: 794.4169  LR: 0.000018  \n",
      "Epoch: [1][2848/2849] Elapsed 6m 58s (remain 0m 0s) Loss: 0.0155(0.0750) Grad: 393.3481  LR: 0.000018  \n",
      "Epoch: [1][0/2849] Elapsed 0m 0s (remain 25m 17s) Loss: 0.0100(0.0100) Grad: 14220.4180  LR: 0.000000  \n",
      "Epoch: [1][100/2849] Elapsed 0m 33s (remain 15m 1s) Loss: 0.0129(0.0133) Grad: 12341.8340  LR: 0.000001  \n",
      "Epoch: [1][200/2849] Elapsed 1m 6s (remain 14m 36s) Loss: 0.0059(0.0128) Grad: 11545.4541  LR: 0.000003  \n",
      "Epoch: [1][300/2849] Elapsed 1m 40s (remain 14m 6s) Loss: 0.0117(0.0135) Grad: 7468.5571  LR: 0.000004  \n",
      "Epoch: [1][400/2849] Elapsed 2m 12s (remain 13m 31s) Loss: 0.0163(0.0134) Grad: 16577.1934  LR: 0.000006  \n",
      "Epoch: [1][500/2849] Elapsed 2m 45s (remain 12m 56s) Loss: 0.0119(0.0133) Grad: 8252.6055  LR: 0.000007  \n",
      "Epoch: [1][600/2849] Elapsed 3m 18s (remain 12m 22s) Loss: 0.0071(0.0133) Grad: 5093.6250  LR: 0.000008  \n",
      "Epoch: [1][700/2849] Elapsed 3m 51s (remain 11m 48s) Loss: 0.0117(0.0132) Grad: 10845.9541  LR: 0.000010  \n",
      "Epoch: [1][800/2849] Elapsed 4m 24s (remain 11m 17s) Loss: 0.0180(0.0131) Grad: 16385.2832  LR: 0.000011  \n",
      "Epoch: [1][900/2849] Elapsed 4m 58s (remain 10m 46s) Loss: 0.0136(0.0130) Grad: 15122.9941  LR: 0.000013  \n",
      "Epoch: [1][1000/2849] Elapsed 5m 31s (remain 10m 11s) Loss: 0.0065(0.0129) Grad: 9017.9277  LR: 0.000014  \n",
      "Epoch: [1][1100/2849] Elapsed 6m 4s (remain 9m 38s) Loss: 0.0055(0.0126) Grad: 15787.5107  LR: 0.000015  \n",
      "Epoch: [1][1200/2849] Elapsed 6m 38s (remain 9m 6s) Loss: 0.0106(0.0123) Grad: 43162.9883  LR: 0.000017  \n",
      "Epoch: [1][1300/2849] Elapsed 7m 11s (remain 8m 33s) Loss: 0.0039(0.0119) Grad: 21321.8887  LR: 0.000018  \n",
      "Epoch: [1][1400/2849] Elapsed 7m 44s (remain 8m 0s) Loss: 0.0033(0.0115) Grad: 19110.0391  LR: 0.000020  \n",
      "Epoch: [1][1500/2849] Elapsed 8m 17s (remain 7m 26s) Loss: 0.0188(0.0112) Grad: 61268.4023  LR: 0.000020  \n",
      "Epoch: [1][1600/2849] Elapsed 8m 50s (remain 6m 53s) Loss: 0.0058(0.0108) Grad: 76187.0234  LR: 0.000020  \n",
      "Epoch: [1][1700/2849] Elapsed 9m 23s (remain 6m 20s) Loss: 0.0041(0.0105) Grad: 27263.2949  LR: 0.000020  \n",
      "Epoch: [1][1800/2849] Elapsed 9m 57s (remain 5m 47s) Loss: 0.0053(0.0102) Grad: 29282.7090  LR: 0.000019  \n",
      "Epoch: [1][1900/2849] Elapsed 10m 31s (remain 5m 14s) Loss: 0.0115(0.0099) Grad: 33553.0586  LR: 0.000019  \n",
      "Epoch: [1][2000/2849] Elapsed 11m 4s (remain 4m 41s) Loss: 0.0015(0.0097) Grad: 6913.7910  LR: 0.000019  \n",
      "Epoch: [1][2100/2849] Elapsed 11m 38s (remain 4m 8s) Loss: 0.0010(0.0094) Grad: 5837.6650  LR: 0.000019  \n",
      "Epoch: [1][2200/2849] Elapsed 12m 12s (remain 3m 35s) Loss: 0.0061(0.0092) Grad: 8415.5156  LR: 0.000019  \n",
      "Epoch: [1][2300/2849] Elapsed 12m 45s (remain 3m 2s) Loss: 0.0009(0.0090) Grad: 2719.6052  LR: 0.000019  \n",
      "Epoch: [1][2400/2849] Elapsed 13m 18s (remain 2m 29s) Loss: 0.0010(0.0088) Grad: 29490.5391  LR: 0.000018  \n",
      "Epoch: [1][2500/2849] Elapsed 13m 51s (remain 1m 55s) Loss: 0.0064(0.0086) Grad: 34894.8828  LR: 0.000018  \n",
      "Epoch: [1][2600/2849] Elapsed 14m 24s (remain 1m 22s) Loss: 0.0025(0.0084) Grad: 3860.9004  LR: 0.000018  \n",
      "Epoch: [1][2700/2849] Elapsed 14m 57s (remain 0m 49s) Loss: 0.0096(0.0082) Grad: 13673.4307  LR: 0.000018  \n",
      "Epoch: [1][2800/2849] Elapsed 15m 31s (remain 0m 15s) Loss: 0.0014(0.0081) Grad: 4974.9741  LR: 0.000018  \n",
      "Epoch: [1][2848/2849] Elapsed 15m 48s (remain 0m 0s) Loss: 0.0034(0.0080) Grad: 42644.2188  LR: 0.000018  \n",
      "EVAL: [0/726] Elapsed 0m 0s (remain 5m 3s) Loss: 0.0008(0.0008) \n",
      "EVAL: [100/726] Elapsed 0m 20s (remain 2m 7s) Loss: 0.0003(0.0026) \n",
      "EVAL: [200/726] Elapsed 0m 41s (remain 1m 48s) Loss: 0.0002(0.0028) \n",
      "EVAL: [300/726] Elapsed 1m 1s (remain 1m 27s) Loss: 0.0004(0.0028) \n",
      "EVAL: [400/726] Elapsed 1m 21s (remain 1m 6s) Loss: 0.0050(0.0035) \n",
      "EVAL: [500/726] Elapsed 1m 42s (remain 0m 45s) Loss: 0.0039(0.0034) \n",
      "EVAL: [600/726] Elapsed 2m 2s (remain 0m 25s) Loss: 0.0009(0.0033) \n",
      "EVAL: [700/726] Elapsed 2m 24s (remain 0m 5s) Loss: 0.0008(0.0032) \n",
      "EVAL: [725/726] Elapsed 2m 29s (remain 0m 0s) Loss: 0.0003(0.0031) \n",
      "Epoch 1 - avg_train_loss: 0.0080  avg_val_loss: 0.0031  time: 1103s\n",
      "Epoch 1 - Score: 0.7916\n",
      "Epoch 1 - Save Best Score: 0.7916 Model\n",
      "Epoch: [2][0/2849] Elapsed 0m 0s (remain 28m 16s) Loss: 0.0007(0.0007) Grad: 3484.9587  LR: 0.000018  \n",
      "Epoch: [2][100/2849] Elapsed 0m 33s (remain 15m 18s) Loss: 0.0022(0.0032) Grad: 11473.3730  LR: 0.000018  \n",
      "Epoch: [2][200/2849] Elapsed 1m 8s (remain 14m 58s) Loss: 0.0001(0.0029) Grad: 1987.4763  LR: 0.000017  \n",
      "Epoch: [2][300/2849] Elapsed 1m 41s (remain 14m 15s) Loss: 0.0141(0.0032) Grad: 39254.2305  LR: 0.000017  \n",
      "Epoch: [2][400/2849] Elapsed 2m 13s (remain 13m 36s) Loss: 0.0001(0.0031) Grad: 889.2900  LR: 0.000017  \n",
      "Epoch: [2][500/2849] Elapsed 2m 46s (remain 13m 2s) Loss: 0.0007(0.0032) Grad: 7037.2134  LR: 0.000017  \n",
      "Epoch: [2][600/2849] Elapsed 3m 20s (remain 12m 29s) Loss: 0.0004(0.0031) Grad: 3863.2805  LR: 0.000017  \n",
      "Epoch: [2][700/2849] Elapsed 3m 53s (remain 11m 55s) Loss: 0.0012(0.0031) Grad: 34552.2344  LR: 0.000017  \n",
      "Epoch: [2][800/2849] Elapsed 4m 25s (remain 11m 19s) Loss: 0.0003(0.0031) Grad: 1903.2451  LR: 0.000017  \n",
      "Epoch: [2][900/2849] Elapsed 4m 58s (remain 10m 45s) Loss: 0.0041(0.0031) Grad: 8213.5957  LR: 0.000016  \n",
      "Epoch: [2][1000/2849] Elapsed 5m 30s (remain 10m 10s) Loss: 0.0006(0.0031) Grad: 3611.7515  LR: 0.000016  \n",
      "Epoch: [2][1100/2849] Elapsed 6m 3s (remain 9m 37s) Loss: 0.0012(0.0030) Grad: 2836.9648  LR: 0.000016  \n",
      "Epoch: [2][1200/2849] Elapsed 6m 36s (remain 9m 4s) Loss: 0.0022(0.0030) Grad: 7392.4600  LR: 0.000016  \n",
      "Epoch: [2][1300/2849] Elapsed 7m 9s (remain 8m 31s) Loss: 0.0064(0.0030) Grad: 12117.3135  LR: 0.000016  \n",
      "Epoch: [2][1400/2849] Elapsed 7m 43s (remain 7m 58s) Loss: 0.0040(0.0030) Grad: 20740.8691  LR: 0.000016  \n",
      "Epoch: [2][1500/2849] Elapsed 8m 15s (remain 7m 25s) Loss: 0.0006(0.0029) Grad: 831.3705  LR: 0.000015  \n",
      "Epoch: [2][1600/2849] Elapsed 8m 48s (remain 6m 52s) Loss: 0.0028(0.0030) Grad: 2818.7793  LR: 0.000015  \n",
      "Epoch: [2][1700/2849] Elapsed 9m 22s (remain 6m 19s) Loss: 0.0143(0.0030) Grad: 17474.3730  LR: 0.000015  \n",
      "Epoch: [2][1800/2849] Elapsed 9m 55s (remain 5m 46s) Loss: 0.0018(0.0029) Grad: 4848.7964  LR: 0.000015  \n",
      "Epoch: [2][1900/2849] Elapsed 10m 28s (remain 5m 13s) Loss: 0.0043(0.0029) Grad: 8555.4424  LR: 0.000015  \n",
      "Epoch: [2][2000/2849] Elapsed 11m 1s (remain 4m 40s) Loss: 0.0007(0.0029) Grad: 2086.5017  LR: 0.000015  \n",
      "Epoch: [2][2100/2849] Elapsed 11m 33s (remain 4m 7s) Loss: 0.0009(0.0029) Grad: 1847.4839  LR: 0.000014  \n",
      "Epoch: [2][2200/2849] Elapsed 12m 6s (remain 3m 33s) Loss: 0.0001(0.0029) Grad: 367.4472  LR: 0.000014  \n",
      "Epoch: [2][2300/2849] Elapsed 12m 39s (remain 3m 0s) Loss: 0.0134(0.0028) Grad: 13723.3311  LR: 0.000014  \n",
      "Epoch: [2][2400/2849] Elapsed 13m 12s (remain 2m 27s) Loss: 0.0070(0.0028) Grad: 14453.7324  LR: 0.000014  \n",
      "Epoch: [2][2500/2849] Elapsed 13m 45s (remain 1m 54s) Loss: 0.0038(0.0028) Grad: 8420.7803  LR: 0.000014  \n",
      "Epoch: [2][2600/2849] Elapsed 14m 17s (remain 1m 21s) Loss: 0.0043(0.0028) Grad: 10139.4287  LR: 0.000014  \n",
      "Epoch: [2][2700/2849] Elapsed 14m 50s (remain 0m 48s) Loss: 0.0005(0.0028) Grad: 2346.7446  LR: 0.000014  \n",
      "Epoch: [2][2800/2849] Elapsed 15m 23s (remain 0m 15s) Loss: 0.0012(0.0028) Grad: 5752.5718  LR: 0.000013  \n",
      "Epoch: [2][2848/2849] Elapsed 15m 40s (remain 0m 0s) Loss: 0.0012(0.0027) Grad: 1911.1180  LR: 0.000013  \n",
      "EVAL: [0/726] Elapsed 0m 0s (remain 5m 23s) Loss: 0.0003(0.0003) \n",
      "EVAL: [100/726] Elapsed 0m 20s (remain 2m 7s) Loss: 0.0005(0.0023) \n",
      "EVAL: [200/726] Elapsed 0m 41s (remain 1m 47s) Loss: 0.0002(0.0025) \n",
      "EVAL: [300/726] Elapsed 1m 1s (remain 1m 26s) Loss: 0.0000(0.0025) \n",
      "EVAL: [400/726] Elapsed 1m 21s (remain 1m 6s) Loss: 0.0046(0.0030) \n",
      "EVAL: [500/726] Elapsed 1m 42s (remain 0m 45s) Loss: 0.0059(0.0030) \n",
      "EVAL: [600/726] Elapsed 2m 2s (remain 0m 25s) Loss: 0.0013(0.0028) \n",
      "EVAL: [700/726] Elapsed 2m 22s (remain 0m 5s) Loss: 0.0005(0.0027) \n",
      "EVAL: [725/726] Elapsed 2m 27s (remain 0m 0s) Loss: 0.0001(0.0026) \n",
      "Epoch 2 - avg_train_loss: 0.0027  avg_val_loss: 0.0026  time: 1092s\n",
      "Epoch 2 - Score: 0.8431\n",
      "Epoch 2 - Save Best Score: 0.8431 Model\n",
      "Epoch: [3][0/2849] Elapsed 0m 0s (remain 27m 42s) Loss: 0.0004(0.0004) Grad: 3173.0750  LR: 0.000013  \n",
      "Epoch: [3][100/2849] Elapsed 0m 33s (remain 15m 5s) Loss: 0.0007(0.0022) Grad: 3579.1206  LR: 0.000013  \n",
      "Epoch: [3][200/2849] Elapsed 1m 6s (remain 14m 33s) Loss: 0.0072(0.0020) Grad: 24993.4902  LR: 0.000013  \n",
      "Epoch: [3][300/2849] Elapsed 1m 39s (remain 14m 6s) Loss: 0.0036(0.0022) Grad: 29919.5449  LR: 0.000013  \n",
      "Epoch: [3][400/2849] Elapsed 2m 12s (remain 13m 30s) Loss: 0.0017(0.0023) Grad: 7680.6108  LR: 0.000013  \n",
      "Epoch: [3][500/2849] Elapsed 2m 45s (remain 12m 55s) Loss: 0.0008(0.0022) Grad: 3812.7563  LR: 0.000013  \n",
      "Epoch: [3][600/2849] Elapsed 3m 18s (remain 12m 21s) Loss: 0.0012(0.0021) Grad: 10752.8174  LR: 0.000012  \n",
      "Epoch: [3][700/2849] Elapsed 3m 51s (remain 11m 50s) Loss: 0.0026(0.0021) Grad: 10066.0000  LR: 0.000012  \n",
      "Epoch: [3][800/2849] Elapsed 4m 25s (remain 11m 18s) Loss: 0.0060(0.0022) Grad: 33577.2070  LR: 0.000012  \n",
      "Epoch: [3][900/2849] Elapsed 4m 58s (remain 10m 45s) Loss: 0.0022(0.0021) Grad: 8074.1152  LR: 0.000012  \n",
      "Epoch: [3][1000/2849] Elapsed 5m 31s (remain 10m 11s) Loss: 0.0023(0.0021) Grad: 8902.0361  LR: 0.000012  \n",
      "Epoch: [3][1100/2849] Elapsed 6m 3s (remain 9m 37s) Loss: 0.0000(0.0021) Grad: 89.7848  LR: 0.000012  \n",
      "Epoch: [3][1200/2849] Elapsed 6m 36s (remain 9m 4s) Loss: 0.0004(0.0020) Grad: 2638.3105  LR: 0.000011  \n",
      "Epoch: [3][1300/2849] Elapsed 7m 10s (remain 8m 32s) Loss: 0.0005(0.0020) Grad: 2604.0286  LR: 0.000011  \n",
      "Epoch: [3][1400/2849] Elapsed 7m 43s (remain 7m 58s) Loss: 0.0045(0.0020) Grad: 24322.6230  LR: 0.000011  \n",
      "Epoch: [3][1500/2849] Elapsed 8m 15s (remain 7m 25s) Loss: 0.0001(0.0021) Grad: 551.1130  LR: 0.000011  \n",
      "Epoch: [3][1600/2849] Elapsed 8m 48s (remain 6m 51s) Loss: 0.0001(0.0021) Grad: 1394.3601  LR: 0.000011  \n",
      "Epoch: [3][1700/2849] Elapsed 9m 21s (remain 6m 18s) Loss: 0.0013(0.0020) Grad: 5042.5400  LR: 0.000011  \n",
      "Epoch: [3][1800/2849] Elapsed 9m 54s (remain 5m 45s) Loss: 0.0005(0.0020) Grad: 3943.8247  LR: 0.000011  \n",
      "Epoch: [3][1900/2849] Elapsed 10m 27s (remain 5m 13s) Loss: 0.0002(0.0020) Grad: 1131.6327  LR: 0.000010  \n",
      "Epoch: [3][2000/2849] Elapsed 11m 0s (remain 4m 40s) Loss: 0.0007(0.0020) Grad: 4891.0444  LR: 0.000010  \n",
      "Epoch: [3][2100/2849] Elapsed 11m 33s (remain 4m 6s) Loss: 0.0007(0.0020) Grad: 2828.9126  LR: 0.000010  \n",
      "Epoch: [3][2200/2849] Elapsed 12m 5s (remain 3m 33s) Loss: 0.0025(0.0020) Grad: 17543.0449  LR: 0.000010  \n",
      "Epoch: [3][2300/2849] Elapsed 12m 38s (remain 3m 0s) Loss: 0.0005(0.0020) Grad: 3404.0974  LR: 0.000010  \n",
      "Epoch: [3][2400/2849] Elapsed 13m 11s (remain 2m 27s) Loss: 0.0001(0.0020) Grad: 1375.2155  LR: 0.000010  \n",
      "Epoch: [3][2500/2849] Elapsed 13m 47s (remain 1m 55s) Loss: 0.0025(0.0020) Grad: 13674.8086  LR: 0.000009  \n",
      "Epoch: [3][2600/2849] Elapsed 14m 21s (remain 1m 22s) Loss: 0.0041(0.0020) Grad: 35516.1133  LR: 0.000009  \n",
      "Epoch: [3][2700/2849] Elapsed 14m 54s (remain 0m 49s) Loss: 0.0001(0.0020) Grad: 3166.4119  LR: 0.000009  \n",
      "Epoch: [3][2800/2849] Elapsed 15m 27s (remain 0m 15s) Loss: 0.0007(0.0020) Grad: 4098.6714  LR: 0.000009  \n",
      "Epoch: [3][2848/2849] Elapsed 15m 43s (remain 0m 0s) Loss: 0.0014(0.0021) Grad: 9025.4941  LR: 0.000009  \n",
      "EVAL: [0/726] Elapsed 0m 0s (remain 5m 7s) Loss: 0.0003(0.0003) \n",
      "EVAL: [100/726] Elapsed 0m 20s (remain 2m 5s) Loss: 0.0005(0.0023) \n",
      "EVAL: [200/726] Elapsed 0m 40s (remain 1m 46s) Loss: 0.0001(0.0023) \n",
      "EVAL: [300/726] Elapsed 1m 1s (remain 1m 26s) Loss: 0.0000(0.0023) \n",
      "EVAL: [400/726] Elapsed 1m 21s (remain 1m 6s) Loss: 0.0024(0.0028) \n",
      "EVAL: [500/726] Elapsed 1m 42s (remain 0m 46s) Loss: 0.0045(0.0027) \n",
      "EVAL: [600/726] Elapsed 2m 3s (remain 0m 25s) Loss: 0.0008(0.0025) \n",
      "EVAL: [700/726] Elapsed 2m 23s (remain 0m 5s) Loss: 0.0002(0.0025) \n",
      "EVAL: [725/726] Elapsed 2m 28s (remain 0m 0s) Loss: 0.0000(0.0024) \n",
      "Epoch 3 - avg_train_loss: 0.0021  avg_val_loss: 0.0024  time: 1097s\n",
      "Epoch 3 - Score: 0.8571\n",
      "Epoch 3 - Save Best Score: 0.8571 Model\n",
      "Epoch: [4][0/2849] Elapsed 0m 0s (remain 28m 11s) Loss: 0.0007(0.0007) Grad: 3343.1909  LR: 0.000009  \n",
      "Epoch: [4][100/2849] Elapsed 0m 33s (remain 15m 11s) Loss: 0.0000(0.0016) Grad: 775.6733  LR: 0.000009  \n",
      "Epoch: [4][200/2849] Elapsed 1m 7s (remain 14m 45s) Loss: 0.0058(0.0017) Grad: 52885.0312  LR: 0.000009  \n",
      "Epoch: [4][300/2849] Elapsed 1m 41s (remain 14m 15s) Loss: 0.0002(0.0016) Grad: 1369.8494  LR: 0.000008  \n",
      "Epoch: [4][400/2849] Elapsed 2m 13s (remain 13m 36s) Loss: 0.0000(0.0017) Grad: 263.7307  LR: 0.000008  \n",
      "Epoch: [4][500/2849] Elapsed 2m 46s (remain 12m 59s) Loss: 0.0002(0.0016) Grad: 1450.6449  LR: 0.000008  \n",
      "Epoch: [4][600/2849] Elapsed 3m 19s (remain 12m 25s) Loss: 0.0009(0.0017) Grad: 5064.3193  LR: 0.000008  \n",
      "Epoch: [4][700/2849] Elapsed 3m 53s (remain 11m 54s) Loss: 0.0000(0.0017) Grad: 211.8033  LR: 0.000008  \n",
      "Epoch: [4][800/2849] Elapsed 4m 27s (remain 11m 24s) Loss: 0.0000(0.0017) Grad: 169.1738  LR: 0.000008  \n",
      "Epoch: [4][900/2849] Elapsed 5m 0s (remain 10m 50s) Loss: 0.0008(0.0017) Grad: 5266.6714  LR: 0.000007  \n",
      "Epoch: [4][1000/2849] Elapsed 5m 33s (remain 10m 15s) Loss: 0.0025(0.0017) Grad: 13633.8164  LR: 0.000007  \n",
      "Epoch: [4][1100/2849] Elapsed 6m 6s (remain 9m 41s) Loss: 0.0000(0.0017) Grad: 528.0441  LR: 0.000007  \n",
      "Epoch: [4][1200/2849] Elapsed 6m 39s (remain 9m 7s) Loss: 0.0026(0.0018) Grad: 12510.4062  LR: 0.000007  \n",
      "Epoch: [4][1300/2849] Elapsed 7m 12s (remain 8m 34s) Loss: 0.0024(0.0017) Grad: 7065.9106  LR: 0.000007  \n",
      "Epoch: [4][1400/2849] Elapsed 7m 45s (remain 8m 0s) Loss: 0.0010(0.0018) Grad: 4924.3184  LR: 0.000007  \n",
      "Epoch: [4][1500/2849] Elapsed 8m 20s (remain 7m 29s) Loss: 0.0000(0.0017) Grad: 741.0204  LR: 0.000007  \n",
      "Epoch: [4][1600/2849] Elapsed 8m 53s (remain 6m 55s) Loss: 0.0005(0.0017) Grad: 5740.4019  LR: 0.000006  \n",
      "Epoch: [4][1700/2849] Elapsed 9m 26s (remain 6m 22s) Loss: 0.0000(0.0017) Grad: 108.6798  LR: 0.000006  \n",
      "Epoch: [4][1800/2849] Elapsed 9m 59s (remain 5m 48s) Loss: 0.0000(0.0017) Grad: 68.5248  LR: 0.000006  \n",
      "Epoch: [4][1900/2849] Elapsed 10m 33s (remain 5m 15s) Loss: 0.0011(0.0017) Grad: 17375.0762  LR: 0.000006  \n",
      "Epoch: [4][2000/2849] Elapsed 11m 6s (remain 4m 42s) Loss: 0.0001(0.0017) Grad: 449.7803  LR: 0.000006  \n",
      "Epoch: [4][2100/2849] Elapsed 11m 39s (remain 4m 8s) Loss: 0.0001(0.0017) Grad: 972.6385  LR: 0.000006  \n",
      "Epoch: [4][2200/2849] Elapsed 12m 11s (remain 3m 35s) Loss: 0.0053(0.0017) Grad: 19163.5547  LR: 0.000005  \n",
      "Epoch: [4][2300/2849] Elapsed 12m 44s (remain 3m 2s) Loss: 0.0001(0.0017) Grad: 1518.2288  LR: 0.000005  \n",
      "Epoch: [4][2400/2849] Elapsed 13m 18s (remain 2m 28s) Loss: 0.0001(0.0017) Grad: 551.3469  LR: 0.000005  \n",
      "Epoch: [4][2500/2849] Elapsed 13m 52s (remain 1m 55s) Loss: 0.0001(0.0017) Grad: 456.5598  LR: 0.000005  \n",
      "Epoch: [4][2600/2849] Elapsed 14m 24s (remain 1m 22s) Loss: 0.0001(0.0017) Grad: 1708.3494  LR: 0.000005  \n",
      "Epoch: [4][2700/2849] Elapsed 14m 57s (remain 0m 49s) Loss: 0.0000(0.0017) Grad: 515.5148  LR: 0.000005  \n",
      "Epoch: [4][2800/2849] Elapsed 15m 30s (remain 0m 15s) Loss: 0.0005(0.0017) Grad: 4813.2871  LR: 0.000005  \n",
      "Epoch: [4][2848/2849] Elapsed 15m 46s (remain 0m 0s) Loss: 0.0011(0.0017) Grad: 4475.6113  LR: 0.000004  \n",
      "EVAL: [0/726] Elapsed 0m 0s (remain 5m 13s) Loss: 0.0001(0.0001) \n",
      "EVAL: [100/726] Elapsed 0m 21s (remain 2m 13s) Loss: 0.0008(0.0026) \n",
      "EVAL: [200/726] Elapsed 0m 42s (remain 1m 51s) Loss: 0.0000(0.0027) \n",
      "EVAL: [300/726] Elapsed 1m 2s (remain 1m 28s) Loss: 0.0000(0.0027) \n",
      "EVAL: [400/726] Elapsed 1m 23s (remain 1m 7s) Loss: 0.0025(0.0032) \n",
      "EVAL: [500/726] Elapsed 1m 43s (remain 0m 46s) Loss: 0.0054(0.0031) \n",
      "EVAL: [600/726] Elapsed 2m 3s (remain 0m 25s) Loss: 0.0014(0.0029) \n",
      "EVAL: [700/726] Elapsed 2m 23s (remain 0m 5s) Loss: 0.0002(0.0028) \n",
      "EVAL: [725/726] Elapsed 2m 28s (remain 0m 0s) Loss: 0.0000(0.0027) \n",
      "Epoch 4 - avg_train_loss: 0.0017  avg_val_loss: 0.0027  time: 1100s\n",
      "Epoch 4 - Score: 0.8622\n",
      "Epoch 4 - Save Best Score: 0.8622 Model\n",
      "Epoch: [5][0/2849] Elapsed 0m 0s (remain 28m 3s) Loss: 0.0000(0.0000) Grad: 1039.1906  LR: 0.000004  \n",
      "Epoch: [5][100/2849] Elapsed 0m 33s (remain 15m 12s) Loss: 0.0004(0.0015) Grad: 4774.4600  LR: 0.000004  \n",
      "Epoch: [5][200/2849] Elapsed 1m 6s (remain 14m 31s) Loss: 0.0048(0.0013) Grad: 16494.3027  LR: 0.000004  \n",
      "Epoch: [5][300/2849] Elapsed 1m 38s (remain 13m 57s) Loss: 0.0004(0.0014) Grad: 2343.8374  LR: 0.000004  \n",
      "Epoch: [5][400/2849] Elapsed 2m 12s (remain 13m 26s) Loss: 0.0010(0.0014) Grad: 10993.7324  LR: 0.000004  \n",
      "Epoch: [5][500/2849] Elapsed 2m 44s (remain 12m 52s) Loss: 0.0000(0.0014) Grad: 220.6957  LR: 0.000004  \n",
      "Epoch: [5][600/2849] Elapsed 3m 17s (remain 12m 18s) Loss: 0.0003(0.0013) Grad: 6439.9458  LR: 0.000004  \n",
      "Epoch: [5][700/2849] Elapsed 3m 50s (remain 11m 45s) Loss: 0.0001(0.0013) Grad: 1480.8450  LR: 0.000003  \n",
      "Epoch: [5][800/2849] Elapsed 4m 24s (remain 11m 16s) Loss: 0.0000(0.0013) Grad: 467.0424  LR: 0.000003  \n",
      "Epoch: [5][900/2849] Elapsed 4m 58s (remain 10m 44s) Loss: 0.0001(0.0013) Grad: 785.9028  LR: 0.000003  \n",
      "Epoch: [5][1000/2849] Elapsed 5m 30s (remain 10m 10s) Loss: 0.0011(0.0013) Grad: 5858.3794  LR: 0.000003  \n",
      "Epoch: [5][1100/2849] Elapsed 6m 3s (remain 9m 37s) Loss: 0.0004(0.0013) Grad: 3563.8098  LR: 0.000003  \n",
      "Epoch: [5][1200/2849] Elapsed 6m 37s (remain 9m 5s) Loss: 0.0001(0.0014) Grad: 930.2574  LR: 0.000003  \n",
      "Epoch: [5][1300/2849] Elapsed 7m 10s (remain 8m 32s) Loss: 0.0000(0.0014) Grad: 60.2487  LR: 0.000002  \n",
      "Epoch: [5][1400/2849] Elapsed 7m 43s (remain 7m 59s) Loss: 0.0000(0.0014) Grad: 220.8981  LR: 0.000002  \n",
      "Epoch: [5][1500/2849] Elapsed 8m 16s (remain 7m 25s) Loss: 0.0000(0.0014) Grad: 571.1452  LR: 0.000002  \n",
      "Epoch: [5][1600/2849] Elapsed 8m 48s (remain 6m 52s) Loss: 0.0008(0.0014) Grad: 3874.2029  LR: 0.000002  \n",
      "Epoch: [5][1700/2849] Elapsed 9m 21s (remain 6m 18s) Loss: 0.0102(0.0014) Grad: 142463.6719  LR: 0.000002  \n",
      "Epoch: [5][1800/2849] Elapsed 9m 53s (remain 5m 45s) Loss: 0.0096(0.0014) Grad: 67261.1094  LR: 0.000002  \n",
      "Epoch: [5][1900/2849] Elapsed 10m 27s (remain 5m 12s) Loss: 0.0002(0.0014) Grad: 11067.7197  LR: 0.000001  \n",
      "Epoch: [5][2000/2849] Elapsed 11m 0s (remain 4m 40s) Loss: 0.0006(0.0014) Grad: 4078.6497  LR: 0.000001  \n",
      "Epoch: [5][2100/2849] Elapsed 11m 33s (remain 4m 6s) Loss: 0.0003(0.0014) Grad: 2554.4084  LR: 0.000001  \n",
      "Epoch: [5][2200/2849] Elapsed 12m 7s (remain 3m 34s) Loss: 0.0002(0.0014) Grad: 1690.4727  LR: 0.000001  \n",
      "Epoch: [5][2300/2849] Elapsed 12m 41s (remain 3m 1s) Loss: 0.0000(0.0014) Grad: 75.0918  LR: 0.000001  \n",
      "Epoch: [5][2400/2849] Elapsed 13m 13s (remain 2m 28s) Loss: 0.0001(0.0014) Grad: 1127.4287  LR: 0.000001  \n",
      "Epoch: [5][2500/2849] Elapsed 13m 46s (remain 1m 54s) Loss: 0.0017(0.0014) Grad: 7645.0171  LR: 0.000001  \n",
      "Epoch: [5][2600/2849] Elapsed 14m 20s (remain 1m 22s) Loss: 0.0010(0.0014) Grad: 8824.9971  LR: 0.000000  \n",
      "Epoch: [5][2700/2849] Elapsed 14m 54s (remain 0m 48s) Loss: 0.0005(0.0014) Grad: 6811.1562  LR: 0.000000  \n",
      "Epoch: [5][2800/2849] Elapsed 15m 27s (remain 0m 15s) Loss: 0.0004(0.0014) Grad: 10469.9111  LR: 0.000000  \n",
      "Epoch: [5][2848/2849] Elapsed 15m 42s (remain 0m 0s) Loss: 0.0001(0.0014) Grad: 1846.5460  LR: 0.000000  \n",
      "EVAL: [0/726] Elapsed 0m 0s (remain 5m 4s) Loss: 0.0001(0.0001) \n",
      "EVAL: [100/726] Elapsed 0m 20s (remain 2m 7s) Loss: 0.0011(0.0032) \n",
      "EVAL: [200/726] Elapsed 0m 41s (remain 1m 48s) Loss: 0.0000(0.0031) \n",
      "EVAL: [300/726] Elapsed 1m 1s (remain 1m 27s) Loss: 0.0000(0.0030) \n",
      "EVAL: [400/726] Elapsed 1m 22s (remain 1m 6s) Loss: 0.0021(0.0036) \n",
      "EVAL: [500/726] Elapsed 1m 42s (remain 0m 45s) Loss: 0.0057(0.0035) \n",
      "EVAL: [600/726] Elapsed 2m 2s (remain 0m 25s) Loss: 0.0022(0.0032) \n",
      "EVAL: [700/726] Elapsed 2m 24s (remain 0m 5s) Loss: 0.0003(0.0031) \n",
      "EVAL: [725/726] Elapsed 2m 29s (remain 0m 0s) Loss: 0.0000(0.0031) \n",
      "Epoch 5 - avg_train_loss: 0.0014  avg_val_loss: 0.0031  time: 1097s\n",
      "Epoch 5 - Score: 0.8650\n",
      "Epoch 5 - Save Best Score: 0.8650 Model\n",
      "========== fold: 1 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp033/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2851] Elapsed 0m 0s (remain 20m 5s) Loss: 0.0539(0.0539) Grad: inf  LR: 0.000000  \n",
      "Epoch: [1][100/2851] Elapsed 0m 15s (remain 6m 51s) Loss: 0.0603(0.0580) Grad: 33111.2773  LR: 0.000001  \n",
      "Epoch: [1][200/2851] Elapsed 0m 29s (remain 6m 33s) Loss: 0.0533(0.0581) Grad: 25936.6758  LR: 0.000003  \n",
      "Epoch: [1][300/2851] Elapsed 0m 44s (remain 6m 16s) Loss: 0.0515(0.0584) Grad: 25273.0742  LR: 0.000004  \n",
      "Epoch: [1][400/2851] Elapsed 0m 58s (remain 5m 59s) Loss: 0.0531(0.0570) Grad: 28671.3359  LR: 0.000006  \n",
      "Epoch: [1][500/2851] Elapsed 1m 13s (remain 5m 45s) Loss: 0.0390(0.0554) Grad: 21067.0996  LR: 0.000007  \n",
      "Epoch: [1][600/2851] Elapsed 1m 28s (remain 5m 31s) Loss: 0.0471(0.0535) Grad: 26358.4980  LR: 0.000008  \n",
      "Epoch: [1][700/2851] Elapsed 1m 43s (remain 5m 16s) Loss: 0.0490(0.0515) Grad: 26108.9512  LR: 0.000010  \n",
      "Epoch: [1][800/2851] Elapsed 1m 57s (remain 5m 1s) Loss: 0.0300(0.0495) Grad: 16150.5908  LR: 0.000011  \n",
      "Epoch: [1][900/2851] Elapsed 2m 12s (remain 4m 47s) Loss: 0.0201(0.0474) Grad: 5331.8916  LR: 0.000013  \n",
      "Epoch: [1][1000/2851] Elapsed 2m 28s (remain 4m 33s) Loss: 0.0211(0.0453) Grad: 7126.0762  LR: 0.000014  \n",
      "Epoch: [1][1100/2851] Elapsed 2m 42s (remain 4m 18s) Loss: 0.0194(0.0433) Grad: 7256.3101  LR: 0.000015  \n",
      "Epoch: [1][1200/2851] Elapsed 2m 57s (remain 4m 3s) Loss: 0.0203(0.0414) Grad: 1289.0084  LR: 0.000017  \n",
      "Epoch: [1][1300/2851] Elapsed 3m 12s (remain 3m 49s) Loss: 0.0140(0.0396) Grad: 3060.6963  LR: 0.000018  \n",
      "Epoch: [1][1400/2851] Elapsed 3m 26s (remain 3m 34s) Loss: 0.0159(0.0380) Grad: 1103.7283  LR: 0.000020  \n",
      "Epoch: [1][1500/2851] Elapsed 3m 41s (remain 3m 18s) Loss: 0.0115(0.0366) Grad: 1810.6222  LR: 0.000020  \n",
      "Epoch: [1][1600/2851] Elapsed 3m 55s (remain 3m 3s) Loss: 0.0216(0.0353) Grad: 1976.2819  LR: 0.000020  \n",
      "Epoch: [1][1700/2851] Elapsed 4m 11s (remain 2m 49s) Loss: 0.0228(0.0342) Grad: 2697.8315  LR: 0.000020  \n",
      "Epoch: [1][1800/2851] Elapsed 4m 26s (remain 2m 35s) Loss: 0.0172(0.0332) Grad: 1668.7218  LR: 0.000019  \n",
      "Epoch: [1][1900/2851] Elapsed 4m 41s (remain 2m 20s) Loss: 0.0170(0.0322) Grad: 1297.8127  LR: 0.000019  \n",
      "Epoch: [1][2000/2851] Elapsed 4m 55s (remain 2m 5s) Loss: 0.0156(0.0314) Grad: 1427.6947  LR: 0.000019  \n",
      "Epoch: [1][2100/2851] Elapsed 5m 10s (remain 1m 50s) Loss: 0.0078(0.0306) Grad: 2335.8657  LR: 0.000019  \n",
      "Epoch: [1][2200/2851] Elapsed 5m 24s (remain 1m 35s) Loss: 0.0199(0.0299) Grad: 3269.6504  LR: 0.000019  \n",
      "Epoch: [1][2300/2851] Elapsed 5m 39s (remain 1m 21s) Loss: 0.0152(0.0292) Grad: 1910.0684  LR: 0.000019  \n",
      "Epoch: [1][2400/2851] Elapsed 5m 53s (remain 1m 6s) Loss: 0.0218(0.0286) Grad: 3168.6033  LR: 0.000018  \n",
      "Epoch: [1][2500/2851] Elapsed 6m 8s (remain 0m 51s) Loss: 0.0098(0.0280) Grad: 789.6588  LR: 0.000018  \n",
      "Epoch: [1][2600/2851] Elapsed 6m 24s (remain 0m 36s) Loss: 0.0226(0.0275) Grad: 2480.0583  LR: 0.000018  \n",
      "Epoch: [1][2700/2851] Elapsed 6m 38s (remain 0m 22s) Loss: 0.0175(0.0271) Grad: 3260.6316  LR: 0.000018  \n",
      "Epoch: [1][2800/2851] Elapsed 6m 53s (remain 0m 7s) Loss: 0.0080(0.0266) Grad: 2340.5986  LR: 0.000018  \n",
      "Epoch: [1][2850/2851] Elapsed 7m 0s (remain 0m 0s) Loss: 0.0131(0.0264) Grad: 1010.7001  LR: 0.000018  \n",
      "Epoch: [1][0/2851] Elapsed 0m 0s (remain 27m 14s) Loss: 0.0082(0.0082) Grad: 9386.5918  LR: 0.000000  \n",
      "Epoch: [1][100/2851] Elapsed 0m 34s (remain 15m 38s) Loss: 0.0072(0.0140) Grad: 6680.1406  LR: 0.000001  \n",
      "Epoch: [1][200/2851] Elapsed 1m 7s (remain 14m 43s) Loss: 0.0080(0.0138) Grad: 6702.7603  LR: 0.000003  \n",
      "Epoch: [1][300/2851] Elapsed 1m 39s (remain 14m 5s) Loss: 0.0163(0.0133) Grad: 16207.4600  LR: 0.000004  \n",
      "Epoch: [1][400/2851] Elapsed 2m 12s (remain 13m 28s) Loss: 0.0087(0.0133) Grad: 13182.3809  LR: 0.000006  \n",
      "Epoch: [1][500/2851] Elapsed 2m 45s (remain 12m 54s) Loss: 0.0239(0.0133) Grad: 24373.1543  LR: 0.000007  \n",
      "Epoch: [1][600/2851] Elapsed 3m 18s (remain 12m 23s) Loss: 0.0136(0.0133) Grad: 6536.1606  LR: 0.000008  \n",
      "Epoch: [1][700/2851] Elapsed 3m 52s (remain 11m 51s) Loss: 0.0056(0.0132) Grad: 2510.9197  LR: 0.000010  \n",
      "Epoch: [1][800/2851] Elapsed 4m 24s (remain 11m 18s) Loss: 0.0055(0.0132) Grad: 2571.8645  LR: 0.000011  \n",
      "Epoch: [1][900/2851] Elapsed 4m 58s (remain 10m 46s) Loss: 0.0161(0.0131) Grad: 10099.8096  LR: 0.000013  \n",
      "Epoch: [1][1000/2851] Elapsed 5m 34s (remain 10m 17s) Loss: 0.0044(0.0128) Grad: 4024.2551  LR: 0.000014  \n",
      "Epoch: [1][1100/2851] Elapsed 6m 7s (remain 9m 43s) Loss: 0.0136(0.0126) Grad: 20576.9121  LR: 0.000015  \n",
      "Epoch: [1][1200/2851] Elapsed 6m 40s (remain 9m 9s) Loss: 0.0066(0.0122) Grad: 9970.1572  LR: 0.000017  \n",
      "Epoch: [1][1300/2851] Elapsed 7m 12s (remain 8m 35s) Loss: 0.0063(0.0118) Grad: 7277.2026  LR: 0.000018  \n",
      "Epoch: [1][1400/2851] Elapsed 7m 46s (remain 8m 3s) Loss: 0.0056(0.0114) Grad: 11274.4717  LR: 0.000020  \n",
      "Epoch: [1][1500/2851] Elapsed 8m 20s (remain 7m 30s) Loss: 0.0019(0.0110) Grad: 11050.8760  LR: 0.000020  \n",
      "Epoch: [1][1600/2851] Elapsed 8m 53s (remain 6m 56s) Loss: 0.0045(0.0106) Grad: 14963.4697  LR: 0.000020  \n",
      "Epoch: [1][1700/2851] Elapsed 9m 26s (remain 6m 22s) Loss: 0.0021(0.0103) Grad: 4502.4849  LR: 0.000020  \n",
      "Epoch: [1][1800/2851] Elapsed 10m 1s (remain 5m 50s) Loss: 0.0032(0.0100) Grad: 7942.0122  LR: 0.000019  \n",
      "Epoch: [1][1900/2851] Elapsed 10m 34s (remain 5m 17s) Loss: 0.0047(0.0097) Grad: 37220.9609  LR: 0.000019  \n",
      "Epoch: [1][2000/2851] Elapsed 11m 7s (remain 4m 43s) Loss: 0.0028(0.0094) Grad: 14256.7744  LR: 0.000019  \n",
      "Epoch: [1][2100/2851] Elapsed 11m 41s (remain 4m 10s) Loss: 0.0036(0.0092) Grad: 6068.7793  LR: 0.000019  \n",
      "Epoch: [1][2200/2851] Elapsed 12m 14s (remain 3m 36s) Loss: 0.0018(0.0089) Grad: 5644.7495  LR: 0.000019  \n",
      "Epoch: [1][2300/2851] Elapsed 12m 47s (remain 3m 3s) Loss: 0.0002(0.0087) Grad: 981.4661  LR: 0.000019  \n",
      "Epoch: [1][2400/2851] Elapsed 13m 20s (remain 2m 30s) Loss: 0.0054(0.0085) Grad: 13424.1230  LR: 0.000018  \n",
      "Epoch: [1][2500/2851] Elapsed 13m 54s (remain 1m 56s) Loss: 0.0005(0.0083) Grad: 1760.4207  LR: 0.000018  \n",
      "Epoch: [1][2600/2851] Elapsed 14m 27s (remain 1m 23s) Loss: 0.0028(0.0081) Grad: 1855.7186  LR: 0.000018  \n",
      "Epoch: [1][2700/2851] Elapsed 15m 0s (remain 0m 49s) Loss: 0.0008(0.0079) Grad: 1410.7820  LR: 0.000018  \n",
      "Epoch: [1][2800/2851] Elapsed 15m 32s (remain 0m 16s) Loss: 0.0049(0.0077) Grad: 4441.2188  LR: 0.000018  \n",
      "Epoch: [1][2850/2851] Elapsed 15m 48s (remain 0m 0s) Loss: 0.0030(0.0077) Grad: 4617.2852  LR: 0.000018  \n",
      "EVAL: [0/724] Elapsed 0m 0s (remain 5m 15s) Loss: 0.0019(0.0019) \n",
      "EVAL: [100/724] Elapsed 0m 21s (remain 2m 11s) Loss: 0.0012(0.0026) \n",
      "EVAL: [200/724] Elapsed 0m 43s (remain 1m 51s) Loss: 0.0001(0.0031) \n",
      "EVAL: [300/724] Elapsed 1m 3s (remain 1m 29s) Loss: 0.0005(0.0031) \n",
      "EVAL: [400/724] Elapsed 1m 23s (remain 1m 7s) Loss: 0.0001(0.0031) \n",
      "EVAL: [500/724] Elapsed 1m 43s (remain 0m 46s) Loss: 0.0039(0.0035) \n",
      "EVAL: [600/724] Elapsed 2m 4s (remain 0m 25s) Loss: 0.0014(0.0034) \n",
      "EVAL: [700/724] Elapsed 2m 24s (remain 0m 4s) Loss: 0.0001(0.0032) \n",
      "EVAL: [723/724] Elapsed 2m 28s (remain 0m 0s) Loss: 0.0005(0.0032) \n",
      "Epoch 1 - avg_train_loss: 0.0077  avg_val_loss: 0.0032  time: 1103s\n",
      "Epoch 1 - Score: 0.7962\n",
      "Epoch 1 - Save Best Score: 0.7962 Model\n",
      "Epoch: [2][0/2851] Elapsed 0m 0s (remain 28m 48s) Loss: 0.0014(0.0014) Grad: 5361.2988  LR: 0.000018  \n",
      "Epoch: [2][100/2851] Elapsed 0m 33s (remain 15m 7s) Loss: 0.0014(0.0025) Grad: 3363.6243  LR: 0.000018  \n",
      "Epoch: [2][200/2851] Elapsed 1m 6s (remain 14m 35s) Loss: 0.0018(0.0027) Grad: 4533.0903  LR: 0.000017  \n",
      "Epoch: [2][300/2851] Elapsed 1m 39s (remain 14m 0s) Loss: 0.0016(0.0028) Grad: 23676.6895  LR: 0.000017  \n",
      "Epoch: [2][400/2851] Elapsed 2m 12s (remain 13m 29s) Loss: 0.0009(0.0028) Grad: 3589.5444  LR: 0.000017  \n",
      "Epoch: [2][500/2851] Elapsed 2m 46s (remain 12m 58s) Loss: 0.0030(0.0028) Grad: 16187.6416  LR: 0.000017  \n",
      "Epoch: [2][600/2851] Elapsed 3m 19s (remain 12m 25s) Loss: 0.0028(0.0028) Grad: 15459.8623  LR: 0.000017  \n",
      "Epoch: [2][700/2851] Elapsed 3m 52s (remain 11m 51s) Loss: 0.0050(0.0028) Grad: 6807.1704  LR: 0.000017  \n",
      "Epoch: [2][800/2851] Elapsed 4m 25s (remain 11m 18s) Loss: 0.0028(0.0028) Grad: 6386.1362  LR: 0.000017  \n",
      "Epoch: [2][900/2851] Elapsed 4m 58s (remain 10m 46s) Loss: 0.0009(0.0027) Grad: 1990.7589  LR: 0.000016  \n",
      "Epoch: [2][1000/2851] Elapsed 5m 31s (remain 10m 11s) Loss: 0.0010(0.0027) Grad: 5744.7173  LR: 0.000016  \n",
      "Epoch: [2][1100/2851] Elapsed 6m 3s (remain 9m 37s) Loss: 0.0003(0.0027) Grad: 1360.7518  LR: 0.000016  \n",
      "Epoch: [2][1200/2851] Elapsed 6m 36s (remain 9m 4s) Loss: 0.0008(0.0027) Grad: 2409.9995  LR: 0.000016  \n",
      "Epoch: [2][1300/2851] Elapsed 7m 9s (remain 8m 32s) Loss: 0.0001(0.0027) Grad: 339.2306  LR: 0.000016  \n",
      "Epoch: [2][1400/2851] Elapsed 7m 43s (remain 7m 59s) Loss: 0.0027(0.0027) Grad: 6543.2412  LR: 0.000016  \n",
      "Epoch: [2][1500/2851] Elapsed 8m 18s (remain 7m 27s) Loss: 0.0008(0.0027) Grad: 4124.8462  LR: 0.000015  \n",
      "Epoch: [2][1600/2851] Elapsed 8m 52s (remain 6m 55s) Loss: 0.0042(0.0027) Grad: 8504.4619  LR: 0.000015  \n",
      "Epoch: [2][1700/2851] Elapsed 9m 25s (remain 6m 22s) Loss: 0.0007(0.0027) Grad: 3211.6094  LR: 0.000015  \n",
      "Epoch: [2][1800/2851] Elapsed 10m 0s (remain 5m 50s) Loss: 0.0001(0.0026) Grad: 584.6099  LR: 0.000015  \n",
      "Epoch: [2][1900/2851] Elapsed 10m 33s (remain 5m 16s) Loss: 0.0011(0.0026) Grad: 7533.5464  LR: 0.000015  \n",
      "Epoch: [2][2000/2851] Elapsed 11m 5s (remain 4m 42s) Loss: 0.0002(0.0026) Grad: 1036.0000  LR: 0.000015  \n",
      "Epoch: [2][2100/2851] Elapsed 11m 39s (remain 4m 9s) Loss: 0.0000(0.0026) Grad: 129.4127  LR: 0.000015  \n",
      "Epoch: [2][2200/2851] Elapsed 12m 14s (remain 3m 36s) Loss: 0.0034(0.0026) Grad: 11797.5742  LR: 0.000014  \n",
      "Epoch: [2][2300/2851] Elapsed 12m 47s (remain 3m 3s) Loss: 0.0253(0.0026) Grad: 16104.8477  LR: 0.000014  \n",
      "Epoch: [2][2400/2851] Elapsed 13m 20s (remain 2m 29s) Loss: 0.0022(0.0026) Grad: 3365.6914  LR: 0.000014  \n",
      "Epoch: [2][2500/2851] Elapsed 13m 52s (remain 1m 56s) Loss: 0.0037(0.0026) Grad: 6556.3823  LR: 0.000014  \n",
      "Epoch: [2][2600/2851] Elapsed 14m 25s (remain 1m 23s) Loss: 0.0103(0.0026) Grad: 30619.6953  LR: 0.000014  \n",
      "Epoch: [2][2700/2851] Elapsed 14m 57s (remain 0m 49s) Loss: 0.0248(0.0026) Grad: 30639.3340  LR: 0.000014  \n",
      "Epoch: [2][2800/2851] Elapsed 15m 33s (remain 0m 16s) Loss: 0.0102(0.0026) Grad: 11861.7734  LR: 0.000013  \n",
      "Epoch: [2][2850/2851] Elapsed 15m 51s (remain 0m 0s) Loss: 0.0002(0.0026) Grad: 481.5109  LR: 0.000013  \n",
      "EVAL: [0/724] Elapsed 0m 0s (remain 5m 25s) Loss: 0.0002(0.0002) \n",
      "EVAL: [100/724] Elapsed 0m 20s (remain 2m 8s) Loss: 0.0006(0.0020) \n",
      "EVAL: [200/724] Elapsed 0m 41s (remain 1m 47s) Loss: 0.0000(0.0024) \n",
      "EVAL: [300/724] Elapsed 1m 1s (remain 1m 26s) Loss: 0.0001(0.0023) \n",
      "EVAL: [400/724] Elapsed 1m 23s (remain 1m 7s) Loss: 0.0000(0.0023) \n",
      "EVAL: [500/724] Elapsed 1m 44s (remain 0m 46s) Loss: 0.0021(0.0027) \n",
      "EVAL: [600/724] Elapsed 2m 5s (remain 0m 25s) Loss: 0.0011(0.0026) \n",
      "EVAL: [700/724] Elapsed 2m 24s (remain 0m 4s) Loss: 0.0000(0.0023) \n",
      "EVAL: [723/724] Elapsed 2m 28s (remain 0m 0s) Loss: 0.0002(0.0023) \n",
      "Epoch 2 - avg_train_loss: 0.0026  avg_val_loss: 0.0023  time: 1105s\n",
      "Epoch 2 - Score: 0.8574\n",
      "Epoch 2 - Save Best Score: 0.8574 Model\n",
      "Epoch: [3][0/2851] Elapsed 0m 0s (remain 28m 27s) Loss: 0.0003(0.0003) Grad: 1825.2106  LR: 0.000013  \n",
      "Epoch: [3][100/2851] Elapsed 0m 33s (remain 15m 2s) Loss: 0.0020(0.0020) Grad: 10273.7383  LR: 0.000013  \n",
      "Epoch: [3][200/2851] Elapsed 1m 5s (remain 14m 27s) Loss: 0.0085(0.0018) Grad: 64076.7969  LR: 0.000013  \n",
      "Epoch: [3][300/2851] Elapsed 1m 39s (remain 14m 2s) Loss: 0.0046(0.0019) Grad: 44967.2461  LR: 0.000013  \n",
      "Epoch: [3][400/2851] Elapsed 2m 14s (remain 13m 39s) Loss: 0.0002(0.0020) Grad: 642.2647  LR: 0.000013  \n",
      "Epoch: [3][500/2851] Elapsed 2m 47s (remain 13m 4s) Loss: 0.0013(0.0020) Grad: 10836.6455  LR: 0.000013  \n",
      "Epoch: [3][600/2851] Elapsed 3m 20s (remain 12m 30s) Loss: 0.0009(0.0019) Grad: 7340.0430  LR: 0.000012  \n",
      "Epoch: [3][700/2851] Elapsed 3m 53s (remain 11m 57s) Loss: 0.0002(0.0020) Grad: 2326.3257  LR: 0.000012  \n",
      "Epoch: [3][800/2851] Elapsed 4m 27s (remain 11m 23s) Loss: 0.0002(0.0019) Grad: 1180.9961  LR: 0.000012  \n",
      "Epoch: [3][900/2851] Elapsed 4m 59s (remain 10m 49s) Loss: 0.0031(0.0019) Grad: 14808.6699  LR: 0.000012  \n",
      "Epoch: [3][1000/2851] Elapsed 5m 33s (remain 10m 16s) Loss: 0.0013(0.0019) Grad: 6600.2354  LR: 0.000012  \n",
      "Epoch: [3][1100/2851] Elapsed 6m 6s (remain 9m 42s) Loss: 0.0001(0.0019) Grad: 877.8375  LR: 0.000012  \n",
      "Epoch: [3][1200/2851] Elapsed 6m 40s (remain 9m 9s) Loss: 0.0006(0.0019) Grad: 2590.1104  LR: 0.000011  \n",
      "Epoch: [3][1300/2851] Elapsed 7m 13s (remain 8m 36s) Loss: 0.0210(0.0019) Grad: 82056.2109  LR: 0.000011  \n",
      "Epoch: [3][1400/2851] Elapsed 7m 46s (remain 8m 2s) Loss: 0.0013(0.0020) Grad: 4653.6504  LR: 0.000011  \n",
      "Epoch: [3][1500/2851] Elapsed 8m 18s (remain 7m 28s) Loss: 0.0002(0.0020) Grad: 1193.7158  LR: 0.000011  \n",
      "Epoch: [3][1600/2851] Elapsed 8m 51s (remain 6m 54s) Loss: 0.0004(0.0019) Grad: 1645.3784  LR: 0.000011  \n",
      "Epoch: [3][1700/2851] Elapsed 9m 24s (remain 6m 21s) Loss: 0.0002(0.0019) Grad: 678.0950  LR: 0.000011  \n",
      "Epoch: [3][1800/2851] Elapsed 9m 58s (remain 5m 48s) Loss: 0.0002(0.0019) Grad: 1722.4258  LR: 0.000011  \n",
      "Epoch: [3][1900/2851] Elapsed 10m 31s (remain 5m 15s) Loss: 0.0025(0.0019) Grad: 5702.8696  LR: 0.000010  \n",
      "Epoch: [3][2000/2851] Elapsed 11m 4s (remain 4m 42s) Loss: 0.0014(0.0019) Grad: 10228.1035  LR: 0.000010  \n",
      "Epoch: [3][2100/2851] Elapsed 11m 38s (remain 4m 9s) Loss: 0.0010(0.0019) Grad: 12063.8486  LR: 0.000010  \n",
      "Epoch: [3][2200/2851] Elapsed 12m 11s (remain 3m 35s) Loss: 0.0032(0.0019) Grad: 24773.0430  LR: 0.000010  \n",
      "Epoch: [3][2300/2851] Elapsed 12m 44s (remain 3m 2s) Loss: 0.0003(0.0019) Grad: 2160.5232  LR: 0.000010  \n",
      "Epoch: [3][2400/2851] Elapsed 13m 17s (remain 2m 29s) Loss: 0.0005(0.0019) Grad: 4649.6152  LR: 0.000010  \n",
      "Epoch: [3][2500/2851] Elapsed 13m 50s (remain 1m 56s) Loss: 0.0015(0.0019) Grad: 17143.0469  LR: 0.000009  \n",
      "Epoch: [3][2600/2851] Elapsed 14m 24s (remain 1m 23s) Loss: 0.0010(0.0019) Grad: 6702.1631  LR: 0.000009  \n",
      "Epoch: [3][2700/2851] Elapsed 14m 56s (remain 0m 49s) Loss: 0.0001(0.0019) Grad: 399.0394  LR: 0.000009  \n",
      "Epoch: [3][2800/2851] Elapsed 15m 29s (remain 0m 16s) Loss: 0.0019(0.0019) Grad: 6436.8154  LR: 0.000009  \n",
      "Epoch: [3][2850/2851] Elapsed 15m 45s (remain 0m 0s) Loss: 0.0084(0.0019) Grad: 27749.7148  LR: 0.000009  \n",
      "EVAL: [0/724] Elapsed 0m 0s (remain 5m 7s) Loss: 0.0001(0.0001) \n",
      "EVAL: [100/724] Elapsed 0m 20s (remain 2m 8s) Loss: 0.0014(0.0027) \n",
      "EVAL: [200/724] Elapsed 0m 41s (remain 1m 47s) Loss: 0.0000(0.0031) \n",
      "EVAL: [300/724] Elapsed 1m 2s (remain 1m 27s) Loss: 0.0001(0.0030) \n",
      "EVAL: [400/724] Elapsed 1m 23s (remain 1m 6s) Loss: 0.0000(0.0029) \n",
      "EVAL: [500/724] Elapsed 1m 43s (remain 0m 45s) Loss: 0.0047(0.0035) \n",
      "EVAL: [600/724] Elapsed 2m 3s (remain 0m 25s) Loss: 0.0035(0.0034) \n",
      "EVAL: [700/724] Elapsed 2m 23s (remain 0m 4s) Loss: 0.0000(0.0031) \n",
      "EVAL: [723/724] Elapsed 2m 27s (remain 0m 0s) Loss: 0.0001(0.0030) \n",
      "Epoch 3 - avg_train_loss: 0.0019  avg_val_loss: 0.0030  time: 1099s\n",
      "Epoch 3 - Score: 0.8705\n",
      "Epoch 3 - Save Best Score: 0.8705 Model\n",
      "Epoch: [4][0/2851] Elapsed 0m 0s (remain 26m 2s) Loss: 0.0000(0.0000) Grad: 993.8397  LR: 0.000009  \n",
      "Epoch: [4][100/2851] Elapsed 0m 33s (remain 15m 14s) Loss: 0.0000(0.0015) Grad: 72.5202  LR: 0.000009  \n",
      "Epoch: [4][200/2851] Elapsed 1m 6s (remain 14m 39s) Loss: 0.0004(0.0016) Grad: 5798.6772  LR: 0.000009  \n",
      "Epoch: [4][300/2851] Elapsed 1m 39s (remain 14m 4s) Loss: 0.0003(0.0016) Grad: 3336.0977  LR: 0.000008  \n",
      "Epoch: [4][400/2851] Elapsed 2m 13s (remain 13m 33s) Loss: 0.0026(0.0016) Grad: 17608.6172  LR: 0.000008  \n",
      "Epoch: [4][500/2851] Elapsed 2m 47s (remain 13m 4s) Loss: 0.0007(0.0016) Grad: 1677.5276  LR: 0.000008  \n",
      "Epoch: [4][600/2851] Elapsed 3m 20s (remain 12m 31s) Loss: 0.0009(0.0016) Grad: 4537.9487  LR: 0.000008  \n",
      "Epoch: [4][700/2851] Elapsed 3m 53s (remain 11m 55s) Loss: 0.0048(0.0016) Grad: 13593.6055  LR: 0.000008  \n",
      "Epoch: [4][800/2851] Elapsed 4m 25s (remain 11m 20s) Loss: 0.0002(0.0016) Grad: 511.2380  LR: 0.000008  \n",
      "Epoch: [4][900/2851] Elapsed 4m 58s (remain 10m 45s) Loss: 0.0003(0.0016) Grad: 1266.9935  LR: 0.000007  \n",
      "Epoch: [4][1000/2851] Elapsed 5m 32s (remain 10m 13s) Loss: 0.0003(0.0016) Grad: 1220.7924  LR: 0.000007  \n",
      "Epoch: [4][1100/2851] Elapsed 6m 5s (remain 9m 40s) Loss: 0.0012(0.0016) Grad: 2774.1570  LR: 0.000007  \n",
      "Epoch: [4][1200/2851] Elapsed 6m 39s (remain 9m 8s) Loss: 0.0002(0.0016) Grad: 782.4399  LR: 0.000007  \n",
      "Epoch: [4][1300/2851] Elapsed 7m 12s (remain 8m 34s) Loss: 0.0017(0.0016) Grad: 29889.9551  LR: 0.000007  \n",
      "Epoch: [4][1400/2851] Elapsed 7m 44s (remain 8m 0s) Loss: 0.0000(0.0015) Grad: 96.9396  LR: 0.000007  \n",
      "Epoch: [4][1500/2851] Elapsed 8m 17s (remain 7m 27s) Loss: 0.0016(0.0016) Grad: 4247.8232  LR: 0.000007  \n",
      "Epoch: [4][1600/2851] Elapsed 8m 51s (remain 6m 54s) Loss: 0.0004(0.0016) Grad: 3841.3787  LR: 0.000006  \n",
      "Epoch: [4][1700/2851] Elapsed 9m 24s (remain 6m 21s) Loss: 0.0006(0.0016) Grad: 3853.9546  LR: 0.000006  \n",
      "Epoch: [4][1800/2851] Elapsed 9m 56s (remain 5m 47s) Loss: 0.0066(0.0016) Grad: 8335.5352  LR: 0.000006  \n",
      "Epoch: [4][1900/2851] Elapsed 10m 29s (remain 5m 14s) Loss: 0.0006(0.0016) Grad: 3910.1140  LR: 0.000006  \n",
      "Epoch: [4][2000/2851] Elapsed 11m 1s (remain 4m 41s) Loss: 0.0097(0.0016) Grad: 20363.4473  LR: 0.000006  \n",
      "Epoch: [4][2100/2851] Elapsed 11m 34s (remain 4m 7s) Loss: 0.0004(0.0016) Grad: 1297.2307  LR: 0.000006  \n",
      "Epoch: [4][2200/2851] Elapsed 12m 6s (remain 3m 34s) Loss: 0.0009(0.0016) Grad: 1667.5333  LR: 0.000005  \n",
      "Epoch: [4][2300/2851] Elapsed 12m 39s (remain 3m 1s) Loss: 0.0047(0.0016) Grad: 19383.3828  LR: 0.000005  \n",
      "Epoch: [4][2400/2851] Elapsed 13m 13s (remain 2m 28s) Loss: 0.0003(0.0016) Grad: 1476.4607  LR: 0.000005  \n",
      "Epoch: [4][2500/2851] Elapsed 13m 45s (remain 1m 55s) Loss: 0.0003(0.0016) Grad: 1905.7982  LR: 0.000005  \n",
      "Epoch: [4][2600/2851] Elapsed 14m 18s (remain 1m 22s) Loss: 0.0000(0.0016) Grad: 48.1373  LR: 0.000005  \n",
      "Epoch: [4][2700/2851] Elapsed 14m 50s (remain 0m 49s) Loss: 0.0053(0.0016) Grad: 4276.3496  LR: 0.000005  \n",
      "Epoch: [4][2800/2851] Elapsed 15m 23s (remain 0m 16s) Loss: 0.0007(0.0016) Grad: 2467.1838  LR: 0.000005  \n",
      "Epoch: [4][2850/2851] Elapsed 15m 39s (remain 0m 0s) Loss: 0.0009(0.0016) Grad: 2881.7585  LR: 0.000004  \n",
      "EVAL: [0/724] Elapsed 0m 0s (remain 5m 11s) Loss: 0.0002(0.0002) \n",
      "EVAL: [100/724] Elapsed 0m 21s (remain 2m 12s) Loss: 0.0009(0.0023) \n",
      "EVAL: [200/724] Elapsed 0m 42s (remain 1m 49s) Loss: 0.0000(0.0026) \n",
      "EVAL: [300/724] Elapsed 1m 2s (remain 1m 28s) Loss: 0.0002(0.0025) \n",
      "EVAL: [400/724] Elapsed 1m 22s (remain 1m 6s) Loss: 0.0000(0.0025) \n",
      "EVAL: [500/724] Elapsed 1m 42s (remain 0m 45s) Loss: 0.0050(0.0030) \n",
      "EVAL: [600/724] Elapsed 2m 3s (remain 0m 25s) Loss: 0.0034(0.0029) \n",
      "EVAL: [700/724] Elapsed 2m 24s (remain 0m 4s) Loss: 0.0000(0.0026) \n",
      "EVAL: [723/724] Elapsed 2m 28s (remain 0m 0s) Loss: 0.0000(0.0026) \n",
      "Epoch 4 - avg_train_loss: 0.0016  avg_val_loss: 0.0026  time: 1094s\n",
      "Epoch 4 - Score: 0.8729\n",
      "Epoch 4 - Save Best Score: 0.8729 Model\n",
      "Epoch: [5][0/2851] Elapsed 0m 0s (remain 29m 8s) Loss: 0.0011(0.0011) Grad: 4694.5508  LR: 0.000004  \n",
      "Epoch: [5][100/2851] Elapsed 0m 33s (remain 15m 6s) Loss: 0.0004(0.0012) Grad: 1910.2010  LR: 0.000004  \n",
      "Epoch: [5][200/2851] Elapsed 1m 5s (remain 14m 29s) Loss: 0.0001(0.0011) Grad: 457.7699  LR: 0.000004  \n",
      "Epoch: [5][300/2851] Elapsed 1m 38s (remain 13m 54s) Loss: 0.0007(0.0012) Grad: 17506.2695  LR: 0.000004  \n",
      "Epoch: [5][400/2851] Elapsed 2m 10s (remain 13m 19s) Loss: 0.0000(0.0013) Grad: 506.1570  LR: 0.000004  \n",
      "Epoch: [5][500/2851] Elapsed 2m 43s (remain 12m 46s) Loss: 0.0000(0.0012) Grad: 54.9850  LR: 0.000004  \n",
      "Epoch: [5][600/2851] Elapsed 3m 16s (remain 12m 15s) Loss: 0.0004(0.0012) Grad: 6287.3555  LR: 0.000004  \n",
      "Epoch: [5][700/2851] Elapsed 3m 49s (remain 11m 44s) Loss: 0.0000(0.0013) Grad: 276.0277  LR: 0.000003  \n",
      "Epoch: [5][800/2851] Elapsed 4m 22s (remain 11m 11s) Loss: 0.0000(0.0013) Grad: 760.5616  LR: 0.000003  \n",
      "Epoch: [5][900/2851] Elapsed 4m 55s (remain 10m 38s) Loss: 0.0011(0.0013) Grad: 8038.3408  LR: 0.000003  \n",
      "Epoch: [5][1000/2851] Elapsed 5m 28s (remain 10m 6s) Loss: 0.0018(0.0013) Grad: 22564.6543  LR: 0.000003  \n",
      "Epoch: [5][1100/2851] Elapsed 6m 1s (remain 9m 34s) Loss: 0.0021(0.0013) Grad: 9591.8545  LR: 0.000003  \n",
      "Epoch: [5][1200/2851] Elapsed 6m 33s (remain 9m 1s) Loss: 0.0002(0.0013) Grad: 1404.3110  LR: 0.000003  \n",
      "Epoch: [5][1300/2851] Elapsed 7m 6s (remain 8m 27s) Loss: 0.0005(0.0013) Grad: 8411.0488  LR: 0.000002  \n",
      "Epoch: [5][1400/2851] Elapsed 7m 38s (remain 7m 54s) Loss: 0.0002(0.0013) Grad: 2183.8293  LR: 0.000002  \n",
      "Epoch: [5][1500/2851] Elapsed 8m 12s (remain 7m 23s) Loss: 0.0001(0.0013) Grad: 943.9504  LR: 0.000002  \n",
      "Epoch: [5][1600/2851] Elapsed 8m 46s (remain 6m 50s) Loss: 0.0005(0.0013) Grad: 8267.1055  LR: 0.000002  \n",
      "Epoch: [5][1700/2851] Elapsed 9m 18s (remain 6m 17s) Loss: 0.0000(0.0013) Grad: 743.9416  LR: 0.000002  \n",
      "Epoch: [5][1800/2851] Elapsed 9m 51s (remain 5m 44s) Loss: 0.0000(0.0014) Grad: 338.3136  LR: 0.000002  \n",
      "Epoch: [5][1900/2851] Elapsed 10m 24s (remain 5m 12s) Loss: 0.0001(0.0013) Grad: 3564.1704  LR: 0.000001  \n",
      "Epoch: [5][2000/2851] Elapsed 10m 57s (remain 4m 39s) Loss: 0.0000(0.0013) Grad: 280.9848  LR: 0.000001  \n",
      "Epoch: [5][2100/2851] Elapsed 11m 30s (remain 4m 6s) Loss: 0.0004(0.0013) Grad: 4736.5957  LR: 0.000001  \n",
      "Epoch: [5][2200/2851] Elapsed 12m 3s (remain 3m 33s) Loss: 0.0019(0.0013) Grad: 14499.2695  LR: 0.000001  \n",
      "Epoch: [5][2300/2851] Elapsed 12m 36s (remain 3m 0s) Loss: 0.0005(0.0013) Grad: 5185.4009  LR: 0.000001  \n",
      "Epoch: [5][2400/2851] Elapsed 13m 9s (remain 2m 27s) Loss: 0.0006(0.0013) Grad: 2752.1938  LR: 0.000001  \n",
      "Epoch: [5][2500/2851] Elapsed 13m 42s (remain 1m 55s) Loss: 0.0025(0.0013) Grad: 12099.6260  LR: 0.000001  \n",
      "Epoch: [5][2600/2851] Elapsed 14m 14s (remain 1m 22s) Loss: 0.0000(0.0013) Grad: 105.2842  LR: 0.000000  \n",
      "Epoch: [5][2700/2851] Elapsed 14m 47s (remain 0m 49s) Loss: 0.0001(0.0013) Grad: 2256.9749  LR: 0.000000  \n",
      "Epoch: [5][2800/2851] Elapsed 15m 20s (remain 0m 16s) Loss: 0.0000(0.0013) Grad: 365.4594  LR: 0.000000  \n",
      "Epoch: [5][2850/2851] Elapsed 15m 37s (remain 0m 0s) Loss: 0.0011(0.0013) Grad: 8188.9917  LR: 0.000000  \n",
      "EVAL: [0/724] Elapsed 0m 0s (remain 5m 11s) Loss: 0.0002(0.0002) \n",
      "EVAL: [100/724] Elapsed 0m 20s (remain 2m 8s) Loss: 0.0009(0.0026) \n",
      "EVAL: [200/724] Elapsed 0m 41s (remain 1m 47s) Loss: 0.0000(0.0031) \n",
      "EVAL: [300/724] Elapsed 1m 1s (remain 1m 26s) Loss: 0.0002(0.0030) \n",
      "EVAL: [400/724] Elapsed 1m 21s (remain 1m 5s) Loss: 0.0000(0.0029) \n",
      "EVAL: [500/724] Elapsed 1m 42s (remain 0m 45s) Loss: 0.0064(0.0037) \n",
      "EVAL: [600/724] Elapsed 2m 2s (remain 0m 25s) Loss: 0.0032(0.0035) \n",
      "EVAL: [700/724] Elapsed 2m 22s (remain 0m 4s) Loss: 0.0000(0.0032) \n",
      "EVAL: [723/724] Elapsed 2m 26s (remain 0m 0s) Loss: 0.0000(0.0031) \n",
      "Epoch 5 - avg_train_loss: 0.0013  avg_val_loss: 0.0031  time: 1089s\n",
      "Epoch 5 - Score: 0.8750\n",
      "Epoch 5 - Save Best Score: 0.8750 Model\n",
      "========== fold: 2 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp033/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2871] Elapsed 0m 0s (remain 19m 1s) Loss: 0.0910(0.0910) Grad: inf  LR: 0.000000  \n",
      "Epoch: [1][100/2871] Elapsed 0m 14s (remain 6m 47s) Loss: 0.2536(0.1491) Grad: 36413.7422  LR: 0.000001  \n",
      "Epoch: [1][200/2871] Elapsed 0m 29s (remain 6m 29s) Loss: 0.0897(0.1485) Grad: 11995.5439  LR: 0.000003  \n",
      "Epoch: [1][300/2871] Elapsed 0m 44s (remain 6m 16s) Loss: 0.0853(0.1456) Grad: 12116.4590  LR: 0.000004  \n",
      "Epoch: [1][400/2871] Elapsed 0m 59s (remain 6m 4s) Loss: 0.1125(0.1417) Grad: 16417.2988  LR: 0.000006  \n",
      "Epoch: [1][500/2871] Elapsed 1m 13s (remain 5m 48s) Loss: 0.0964(0.1388) Grad: 14823.5879  LR: 0.000007  \n",
      "Epoch: [1][600/2871] Elapsed 1m 28s (remain 5m 33s) Loss: 0.1248(0.1331) Grad: 18700.3086  LR: 0.000008  \n",
      "Epoch: [1][700/2871] Elapsed 1m 43s (remain 5m 19s) Loss: 0.1174(0.1281) Grad: 18263.8750  LR: 0.000010  \n",
      "Epoch: [1][800/2871] Elapsed 1m 57s (remain 5m 4s) Loss: 0.1034(0.1225) Grad: 15959.7314  LR: 0.000011  \n",
      "Epoch: [1][900/2871] Elapsed 2m 12s (remain 4m 49s) Loss: 0.0670(0.1169) Grad: 10682.3154  LR: 0.000013  \n",
      "Epoch: [1][1000/2871] Elapsed 2m 26s (remain 4m 34s) Loss: 0.0573(0.1113) Grad: 8364.7412  LR: 0.000014  \n",
      "Epoch: [1][1100/2871] Elapsed 2m 41s (remain 4m 19s) Loss: 0.0399(0.1058) Grad: 5951.7915  LR: 0.000015  \n",
      "Epoch: [1][1200/2871] Elapsed 2m 55s (remain 4m 4s) Loss: 0.0381(0.1003) Grad: 4652.2637  LR: 0.000017  \n",
      "Epoch: [1][1300/2871] Elapsed 3m 10s (remain 3m 49s) Loss: 0.0274(0.0951) Grad: 1250.4078  LR: 0.000018  \n",
      "Epoch: [1][1400/2871] Elapsed 3m 24s (remain 3m 34s) Loss: 0.0250(0.0902) Grad: 3579.3730  LR: 0.000020  \n",
      "Epoch: [1][1500/2871] Elapsed 3m 39s (remain 3m 20s) Loss: 0.0196(0.0857) Grad: 2852.1489  LR: 0.000020  \n",
      "Epoch: [1][1600/2871] Elapsed 3m 54s (remain 3m 6s) Loss: 0.0109(0.0814) Grad: 864.1859  LR: 0.000020  \n",
      "Epoch: [1][1700/2871] Elapsed 4m 9s (remain 2m 51s) Loss: 0.0131(0.0777) Grad: 1754.1053  LR: 0.000020  \n",
      "Epoch: [1][1800/2871] Elapsed 4m 23s (remain 2m 36s) Loss: 0.0186(0.0742) Grad: 373.3982  LR: 0.000019  \n",
      "Epoch: [1][1900/2871] Elapsed 4m 38s (remain 2m 22s) Loss: 0.0082(0.0711) Grad: 640.5457  LR: 0.000019  \n",
      "Epoch: [1][2000/2871] Elapsed 4m 53s (remain 2m 7s) Loss: 0.0246(0.0683) Grad: 501.5725  LR: 0.000019  \n",
      "Epoch: [1][2100/2871] Elapsed 5m 7s (remain 1m 52s) Loss: 0.0321(0.0658) Grad: 480.4072  LR: 0.000019  \n",
      "Epoch: [1][2200/2871] Elapsed 5m 22s (remain 1m 38s) Loss: 0.0117(0.0635) Grad: 489.2443  LR: 0.000019  \n",
      "Epoch: [1][2300/2871] Elapsed 5m 37s (remain 1m 23s) Loss: 0.0221(0.0614) Grad: 877.1016  LR: 0.000019  \n",
      "Epoch: [1][2400/2871] Elapsed 5m 52s (remain 1m 8s) Loss: 0.0147(0.0594) Grad: 251.4205  LR: 0.000019  \n",
      "Epoch: [1][2500/2871] Elapsed 6m 6s (remain 0m 54s) Loss: 0.0260(0.0576) Grad: 769.1055  LR: 0.000018  \n",
      "Epoch: [1][2600/2871] Elapsed 6m 21s (remain 0m 39s) Loss: 0.0111(0.0560) Grad: 221.1639  LR: 0.000018  \n",
      "Epoch: [1][2700/2871] Elapsed 6m 35s (remain 0m 24s) Loss: 0.0294(0.0545) Grad: 1511.1241  LR: 0.000018  \n",
      "Epoch: [1][2800/2871] Elapsed 6m 50s (remain 0m 10s) Loss: 0.0167(0.0530) Grad: 694.6198  LR: 0.000018  \n",
      "Epoch: [1][2870/2871] Elapsed 7m 1s (remain 0m 0s) Loss: 0.0145(0.0521) Grad: 224.5637  LR: 0.000018  \n",
      "Epoch: [1][0/2871] Elapsed 0m 0s (remain 26m 22s) Loss: 0.0063(0.0063) Grad: 11736.0078  LR: 0.000000  \n",
      "Epoch: [1][100/2871] Elapsed 0m 33s (remain 15m 17s) Loss: 0.0124(0.0146) Grad: 7874.4238  LR: 0.000001  \n",
      "Epoch: [1][200/2871] Elapsed 1m 6s (remain 14m 41s) Loss: 0.0102(0.0145) Grad: 7300.0376  LR: 0.000003  \n",
      "Epoch: [1][300/2871] Elapsed 1m 39s (remain 14m 5s) Loss: 0.0110(0.0143) Grad: 8347.1035  LR: 0.000004  \n",
      "Epoch: [1][400/2871] Elapsed 2m 13s (remain 13m 43s) Loss: 0.0132(0.0140) Grad: 8859.5918  LR: 0.000006  \n",
      "Epoch: [1][500/2871] Elapsed 2m 46s (remain 13m 8s) Loss: 0.0150(0.0139) Grad: 17554.0117  LR: 0.000007  \n",
      "Epoch: [1][600/2871] Elapsed 3m 19s (remain 12m 33s) Loss: 0.0163(0.0137) Grad: 15771.4385  LR: 0.000008  \n",
      "Epoch: [1][700/2871] Elapsed 3m 52s (remain 11m 59s) Loss: 0.0143(0.0136) Grad: 11251.2393  LR: 0.000010  \n",
      "Epoch: [1][800/2871] Elapsed 4m 26s (remain 11m 28s) Loss: 0.0100(0.0134) Grad: 7178.9111  LR: 0.000011  \n",
      "Epoch: [1][900/2871] Elapsed 5m 0s (remain 10m 57s) Loss: 0.0250(0.0134) Grad: 32808.6211  LR: 0.000013  \n",
      "Epoch: [1][1000/2871] Elapsed 5m 33s (remain 10m 23s) Loss: 0.0155(0.0133) Grad: 16473.7930  LR: 0.000014  \n",
      "Epoch: [1][1100/2871] Elapsed 6m 6s (remain 9m 49s) Loss: 0.0106(0.0132) Grad: 11646.0283  LR: 0.000015  \n",
      "Epoch: [1][1200/2871] Elapsed 6m 41s (remain 9m 18s) Loss: 0.0083(0.0131) Grad: 6765.3687  LR: 0.000017  \n",
      "Epoch: [1][1300/2871] Elapsed 7m 14s (remain 8m 44s) Loss: 0.0205(0.0131) Grad: 27607.7441  LR: 0.000018  \n",
      "Epoch: [1][1400/2871] Elapsed 7m 47s (remain 8m 10s) Loss: 0.0113(0.0130) Grad: 7538.2827  LR: 0.000020  \n",
      "Epoch: [1][1500/2871] Elapsed 8m 20s (remain 7m 36s) Loss: 0.0064(0.0129) Grad: 28499.2832  LR: 0.000020  \n",
      "Epoch: [1][1600/2871] Elapsed 8m 52s (remain 7m 2s) Loss: 0.0057(0.0127) Grad: 12825.9814  LR: 0.000020  \n",
      "Epoch: [1][1700/2871] Elapsed 9m 25s (remain 6m 29s) Loss: 0.0038(0.0123) Grad: 12812.9248  LR: 0.000020  \n",
      "Epoch: [1][1800/2871] Elapsed 9m 59s (remain 5m 56s) Loss: 0.0021(0.0120) Grad: 4706.3926  LR: 0.000019  \n",
      "Epoch: [1][1900/2871] Elapsed 10m 32s (remain 5m 22s) Loss: 0.0011(0.0117) Grad: 2361.3364  LR: 0.000019  \n",
      "Epoch: [1][2000/2871] Elapsed 11m 5s (remain 4m 49s) Loss: 0.0056(0.0113) Grad: 15018.7988  LR: 0.000019  \n",
      "Epoch: [1][2100/2871] Elapsed 11m 38s (remain 4m 16s) Loss: 0.0022(0.0110) Grad: 6581.2964  LR: 0.000019  \n",
      "Epoch: [1][2200/2871] Elapsed 12m 11s (remain 3m 42s) Loss: 0.0026(0.0108) Grad: 10162.7090  LR: 0.000019  \n",
      "Epoch: [1][2300/2871] Elapsed 12m 44s (remain 3m 9s) Loss: 0.0008(0.0105) Grad: 2248.8147  LR: 0.000019  \n",
      "Epoch: [1][2400/2871] Elapsed 13m 17s (remain 2m 36s) Loss: 0.0023(0.0102) Grad: 9344.1416  LR: 0.000019  \n",
      "Epoch: [1][2500/2871] Elapsed 13m 52s (remain 2m 3s) Loss: 0.0019(0.0100) Grad: 2855.3716  LR: 0.000018  \n",
      "Epoch: [1][2600/2871] Elapsed 14m 25s (remain 1m 29s) Loss: 0.0004(0.0098) Grad: 1537.1519  LR: 0.000018  \n",
      "Epoch: [1][2700/2871] Elapsed 14m 58s (remain 0m 56s) Loss: 0.0015(0.0095) Grad: 4431.2598  LR: 0.000018  \n",
      "Epoch: [1][2800/2871] Elapsed 15m 31s (remain 0m 23s) Loss: 0.0025(0.0093) Grad: 4087.7754  LR: 0.000018  \n",
      "Epoch: [1][2870/2871] Elapsed 15m 54s (remain 0m 0s) Loss: 0.0044(0.0092) Grad: 71353.5156  LR: 0.000018  \n",
      "EVAL: [0/704] Elapsed 0m 0s (remain 5m 13s) Loss: 0.0015(0.0015) \n",
      "EVAL: [100/704] Elapsed 0m 21s (remain 2m 5s) Loss: 0.0028(0.0029) \n",
      "EVAL: [200/704] Elapsed 0m 41s (remain 1m 43s) Loss: 0.0000(0.0029) \n",
      "EVAL: [300/704] Elapsed 1m 1s (remain 1m 22s) Loss: 0.0003(0.0028) \n",
      "EVAL: [400/704] Elapsed 1m 21s (remain 1m 1s) Loss: 0.0024(0.0031) \n",
      "EVAL: [500/704] Elapsed 1m 42s (remain 0m 41s) Loss: 0.0035(0.0033) \n",
      "EVAL: [600/704] Elapsed 2m 2s (remain 0m 20s) Loss: 0.0002(0.0033) \n",
      "EVAL: [700/704] Elapsed 2m 23s (remain 0m 0s) Loss: 0.0006(0.0032) \n",
      "EVAL: [703/704] Elapsed 2m 23s (remain 0m 0s) Loss: 0.0000(0.0032) \n",
      "Epoch 1 - avg_train_loss: 0.0092  avg_val_loss: 0.0032  time: 1104s\n",
      "Epoch 1 - Score: 0.7701\n",
      "Epoch 1 - Save Best Score: 0.7701 Model\n",
      "Epoch: [2][0/2871] Elapsed 0m 0s (remain 26m 56s) Loss: 0.0026(0.0026) Grad: 15817.6680  LR: 0.000018  \n",
      "Epoch: [2][100/2871] Elapsed 0m 33s (remain 15m 13s) Loss: 0.0011(0.0027) Grad: 7909.4194  LR: 0.000018  \n",
      "Epoch: [2][200/2871] Elapsed 1m 6s (remain 14m 39s) Loss: 0.0008(0.0028) Grad: 4650.7847  LR: 0.000017  \n",
      "Epoch: [2][300/2871] Elapsed 1m 39s (remain 14m 12s) Loss: 0.0016(0.0030) Grad: 7544.1050  LR: 0.000017  \n",
      "Epoch: [2][400/2871] Elapsed 2m 12s (remain 13m 37s) Loss: 0.0001(0.0030) Grad: 654.0992  LR: 0.000017  \n",
      "Epoch: [2][500/2871] Elapsed 2m 45s (remain 13m 3s) Loss: 0.0034(0.0031) Grad: 9048.7881  LR: 0.000017  \n",
      "Epoch: [2][600/2871] Elapsed 3m 18s (remain 12m 30s) Loss: 0.0013(0.0031) Grad: 5765.5962  LR: 0.000017  \n",
      "Epoch: [2][700/2871] Elapsed 3m 51s (remain 11m 56s) Loss: 0.0004(0.0031) Grad: 4905.2505  LR: 0.000017  \n",
      "Epoch: [2][800/2871] Elapsed 4m 24s (remain 11m 23s) Loss: 0.0010(0.0030) Grad: 6735.1064  LR: 0.000017  \n",
      "Epoch: [2][900/2871] Elapsed 4m 58s (remain 10m 53s) Loss: 0.0287(0.0030) Grad: 68580.4844  LR: 0.000016  \n",
      "Epoch: [2][1000/2871] Elapsed 5m 31s (remain 10m 19s) Loss: 0.0022(0.0030) Grad: 8351.9980  LR: 0.000016  \n",
      "Epoch: [2][1100/2871] Elapsed 6m 4s (remain 9m 45s) Loss: 0.0002(0.0029) Grad: 2024.4407  LR: 0.000016  \n",
      "Epoch: [2][1200/2871] Elapsed 6m 37s (remain 9m 12s) Loss: 0.0008(0.0029) Grad: 14041.2344  LR: 0.000016  \n",
      "Epoch: [2][1300/2871] Elapsed 7m 10s (remain 8m 38s) Loss: 0.0026(0.0029) Grad: 9871.3447  LR: 0.000016  \n",
      "Epoch: [2][1400/2871] Elapsed 7m 42s (remain 8m 5s) Loss: 0.0007(0.0029) Grad: 6428.3462  LR: 0.000016  \n",
      "Epoch: [2][1500/2871] Elapsed 8m 15s (remain 7m 32s) Loss: 0.0004(0.0029) Grad: 11510.4492  LR: 0.000015  \n",
      "Epoch: [2][1600/2871] Elapsed 8m 49s (remain 7m 0s) Loss: 0.0011(0.0029) Grad: 7720.6470  LR: 0.000015  \n",
      "Epoch: [2][1700/2871] Elapsed 9m 23s (remain 6m 27s) Loss: 0.0034(0.0029) Grad: 23137.3496  LR: 0.000015  \n",
      "Epoch: [2][1800/2871] Elapsed 9m 56s (remain 5m 54s) Loss: 0.0012(0.0029) Grad: 5029.9204  LR: 0.000015  \n",
      "Epoch: [2][1900/2871] Elapsed 10m 29s (remain 5m 21s) Loss: 0.0066(0.0029) Grad: 26513.4395  LR: 0.000015  \n",
      "Epoch: [2][2000/2871] Elapsed 11m 1s (remain 4m 47s) Loss: 0.0001(0.0029) Grad: 806.4185  LR: 0.000015  \n",
      "Epoch: [2][2100/2871] Elapsed 11m 35s (remain 4m 14s) Loss: 0.0015(0.0029) Grad: 11334.1455  LR: 0.000015  \n",
      "Epoch: [2][2200/2871] Elapsed 12m 10s (remain 3m 42s) Loss: 0.0010(0.0028) Grad: 7046.8340  LR: 0.000014  \n",
      "Epoch: [2][2300/2871] Elapsed 12m 43s (remain 3m 9s) Loss: 0.0029(0.0028) Grad: 11501.6455  LR: 0.000014  \n",
      "Epoch: [2][2400/2871] Elapsed 13m 16s (remain 2m 35s) Loss: 0.0013(0.0028) Grad: 13911.0732  LR: 0.000014  \n",
      "Epoch: [2][2500/2871] Elapsed 13m 49s (remain 2m 2s) Loss: 0.0009(0.0028) Grad: 7834.0386  LR: 0.000014  \n",
      "Epoch: [2][2600/2871] Elapsed 14m 21s (remain 1m 29s) Loss: 0.0013(0.0028) Grad: 6597.8706  LR: 0.000014  \n",
      "Epoch: [2][2700/2871] Elapsed 14m 54s (remain 0m 56s) Loss: 0.0014(0.0028) Grad: 15405.9834  LR: 0.000014  \n",
      "Epoch: [2][2800/2871] Elapsed 15m 26s (remain 0m 23s) Loss: 0.0014(0.0028) Grad: 11698.1094  LR: 0.000013  \n",
      "Epoch: [2][2870/2871] Elapsed 15m 50s (remain 0m 0s) Loss: 0.0000(0.0028) Grad: 3767.1125  LR: 0.000013  \n",
      "EVAL: [0/704] Elapsed 0m 0s (remain 5m 19s) Loss: 0.0005(0.0005) \n",
      "EVAL: [100/704] Elapsed 0m 20s (remain 2m 2s) Loss: 0.0007(0.0031) \n",
      "EVAL: [200/704] Elapsed 0m 40s (remain 1m 42s) Loss: 0.0000(0.0027) \n",
      "EVAL: [300/704] Elapsed 1m 1s (remain 1m 21s) Loss: 0.0001(0.0024) \n",
      "EVAL: [400/704] Elapsed 1m 21s (remain 1m 1s) Loss: 0.0066(0.0028) \n",
      "EVAL: [500/704] Elapsed 1m 41s (remain 0m 41s) Loss: 0.0021(0.0030) \n",
      "EVAL: [600/704] Elapsed 2m 1s (remain 0m 20s) Loss: 0.0000(0.0031) \n",
      "EVAL: [700/704] Elapsed 2m 22s (remain 0m 0s) Loss: 0.0006(0.0029) \n",
      "EVAL: [703/704] Elapsed 2m 22s (remain 0m 0s) Loss: 0.0000(0.0029) \n",
      "Epoch 2 - avg_train_loss: 0.0028  avg_val_loss: 0.0029  time: 1098s\n",
      "Epoch 2 - Score: 0.8358\n",
      "Epoch 2 - Save Best Score: 0.8358 Model\n",
      "Epoch: [3][0/2871] Elapsed 0m 0s (remain 28m 44s) Loss: 0.0001(0.0001) Grad: 1582.0146  LR: 0.000013  \n",
      "Epoch: [3][100/2871] Elapsed 0m 33s (remain 15m 13s) Loss: 0.0079(0.0022) Grad: 28669.6387  LR: 0.000013  \n",
      "Epoch: [3][200/2871] Elapsed 1m 5s (remain 14m 35s) Loss: 0.0006(0.0024) Grad: 4966.6738  LR: 0.000013  \n",
      "Epoch: [3][300/2871] Elapsed 1m 39s (remain 14m 5s) Loss: 0.0022(0.0022) Grad: 18127.2812  LR: 0.000013  \n",
      "Epoch: [3][400/2871] Elapsed 2m 12s (remain 13m 35s) Loss: 0.0009(0.0022) Grad: 7943.6582  LR: 0.000013  \n",
      "Epoch: [3][500/2871] Elapsed 2m 46s (remain 13m 8s) Loss: 0.0006(0.0021) Grad: 5007.8979  LR: 0.000013  \n",
      "Epoch: [3][600/2871] Elapsed 3m 19s (remain 12m 35s) Loss: 0.0001(0.0021) Grad: 2806.9355  LR: 0.000012  \n",
      "Epoch: [3][700/2871] Elapsed 3m 54s (remain 12m 7s) Loss: 0.0026(0.0021) Grad: 62790.2344  LR: 0.000012  \n",
      "Epoch: [3][800/2871] Elapsed 4m 29s (remain 11m 35s) Loss: 0.0053(0.0021) Grad: 49672.7969  LR: 0.000012  \n",
      "Epoch: [3][900/2871] Elapsed 5m 1s (remain 10m 59s) Loss: 0.0000(0.0021) Grad: 188.2460  LR: 0.000012  \n",
      "Epoch: [3][1000/2871] Elapsed 5m 34s (remain 10m 24s) Loss: 0.0012(0.0022) Grad: 3759.4993  LR: 0.000012  \n",
      "Epoch: [3][1100/2871] Elapsed 6m 7s (remain 9m 51s) Loss: 0.0002(0.0022) Grad: 1090.6832  LR: 0.000012  \n",
      "Epoch: [3][1200/2871] Elapsed 6m 40s (remain 9m 17s) Loss: 0.0013(0.0022) Grad: 51622.5703  LR: 0.000011  \n",
      "Epoch: [3][1300/2871] Elapsed 7m 13s (remain 8m 43s) Loss: 0.0005(0.0022) Grad: 1968.4160  LR: 0.000011  \n",
      "Epoch: [3][1400/2871] Elapsed 7m 46s (remain 8m 9s) Loss: 0.0019(0.0022) Grad: 13520.7129  LR: 0.000011  \n",
      "Epoch: [3][1500/2871] Elapsed 8m 19s (remain 7m 35s) Loss: 0.0001(0.0022) Grad: 795.3008  LR: 0.000011  \n",
      "Epoch: [3][1600/2871] Elapsed 8m 52s (remain 7m 2s) Loss: 0.0009(0.0021) Grad: 4063.7292  LR: 0.000011  \n",
      "Epoch: [3][1700/2871] Elapsed 9m 25s (remain 6m 28s) Loss: 0.0011(0.0022) Grad: 7362.6660  LR: 0.000011  \n",
      "Epoch: [3][1800/2871] Elapsed 9m 58s (remain 5m 55s) Loss: 0.0002(0.0021) Grad: 28941.0820  LR: 0.000011  \n",
      "Epoch: [3][1900/2871] Elapsed 10m 31s (remain 5m 22s) Loss: 0.0001(0.0021) Grad: 1543.4194  LR: 0.000010  \n",
      "Epoch: [3][2000/2871] Elapsed 11m 5s (remain 4m 49s) Loss: 0.0002(0.0021) Grad: 2026.7618  LR: 0.000010  \n",
      "Epoch: [3][2100/2871] Elapsed 11m 38s (remain 4m 16s) Loss: 0.0001(0.0021) Grad: 1348.6763  LR: 0.000010  \n",
      "Epoch: [3][2200/2871] Elapsed 12m 12s (remain 3m 42s) Loss: 0.0001(0.0021) Grad: 1135.3807  LR: 0.000010  \n",
      "Epoch: [3][2300/2871] Elapsed 12m 45s (remain 3m 9s) Loss: 0.0013(0.0021) Grad: 31950.6914  LR: 0.000010  \n",
      "Epoch: [3][2400/2871] Elapsed 13m 18s (remain 2m 36s) Loss: 0.0000(0.0022) Grad: 240.4460  LR: 0.000010  \n",
      "Epoch: [3][2500/2871] Elapsed 13m 51s (remain 2m 3s) Loss: 0.0089(0.0022) Grad: 68197.6250  LR: 0.000009  \n",
      "Epoch: [3][2600/2871] Elapsed 14m 25s (remain 1m 29s) Loss: 0.0003(0.0022) Grad: 1810.4705  LR: 0.000009  \n",
      "Epoch: [3][2700/2871] Elapsed 14m 58s (remain 0m 56s) Loss: 0.0060(0.0022) Grad: 20197.4648  LR: 0.000009  \n",
      "Epoch: [3][2800/2871] Elapsed 15m 31s (remain 0m 23s) Loss: 0.0118(0.0022) Grad: 20756.3105  LR: 0.000009  \n",
      "Epoch: [3][2870/2871] Elapsed 15m 54s (remain 0m 0s) Loss: 0.0007(0.0022) Grad: 4398.1011  LR: 0.000009  \n",
      "EVAL: [0/704] Elapsed 0m 0s (remain 5m 11s) Loss: 0.0006(0.0006) \n",
      "EVAL: [100/704] Elapsed 0m 20s (remain 2m 5s) Loss: 0.0007(0.0034) \n",
      "EVAL: [200/704] Elapsed 0m 41s (remain 1m 44s) Loss: 0.0000(0.0027) \n",
      "EVAL: [300/704] Elapsed 1m 2s (remain 1m 23s) Loss: 0.0001(0.0024) \n",
      "EVAL: [400/704] Elapsed 1m 22s (remain 1m 2s) Loss: 0.0069(0.0028) \n",
      "EVAL: [500/704] Elapsed 1m 43s (remain 0m 41s) Loss: 0.0016(0.0030) \n",
      "EVAL: [600/704] Elapsed 2m 3s (remain 0m 21s) Loss: 0.0000(0.0031) \n",
      "EVAL: [700/704] Elapsed 2m 23s (remain 0m 0s) Loss: 0.0001(0.0029) \n",
      "EVAL: [703/704] Elapsed 2m 24s (remain 0m 0s) Loss: 0.0000(0.0029) \n",
      "Epoch 3 - avg_train_loss: 0.0022  avg_val_loss: 0.0029  time: 1104s\n",
      "Epoch 3 - Score: 0.8563\n",
      "Epoch 3 - Save Best Score: 0.8563 Model\n",
      "Epoch: [4][0/2871] Elapsed 0m 0s (remain 29m 57s) Loss: 0.0009(0.0009) Grad: 5172.1011  LR: 0.000009  \n",
      "Epoch: [4][100/2871] Elapsed 0m 33s (remain 15m 26s) Loss: 0.0036(0.0017) Grad: 35175.0547  LR: 0.000009  \n",
      "Epoch: [4][200/2871] Elapsed 1m 6s (remain 14m 46s) Loss: 0.0005(0.0017) Grad: 6687.5239  LR: 0.000009  \n",
      "Epoch: [4][300/2871] Elapsed 1m 39s (remain 14m 10s) Loss: 0.0008(0.0016) Grad: 2781.3542  LR: 0.000008  \n",
      "Epoch: [4][400/2871] Elapsed 2m 13s (remain 13m 39s) Loss: 0.0000(0.0017) Grad: 459.8134  LR: 0.000008  \n",
      "Epoch: [4][500/2871] Elapsed 2m 46s (remain 13m 8s) Loss: 0.0028(0.0016) Grad: 7668.7158  LR: 0.000008  \n",
      "Epoch: [4][600/2871] Elapsed 3m 20s (remain 12m 36s) Loss: 0.0001(0.0016) Grad: 605.9741  LR: 0.000008  \n",
      "Epoch: [4][700/2871] Elapsed 3m 53s (remain 12m 1s) Loss: 0.0345(0.0017) Grad: 141686.9688  LR: 0.000008  \n",
      "Epoch: [4][800/2871] Elapsed 4m 26s (remain 11m 27s) Loss: 0.0001(0.0017) Grad: 926.4115  LR: 0.000008  \n",
      "Epoch: [4][900/2871] Elapsed 4m 59s (remain 10m 54s) Loss: 0.0102(0.0018) Grad: 26233.4336  LR: 0.000007  \n",
      "Epoch: [4][1000/2871] Elapsed 5m 32s (remain 10m 20s) Loss: 0.0006(0.0018) Grad: 10393.4326  LR: 0.000007  \n",
      "Epoch: [4][1100/2871] Elapsed 6m 5s (remain 9m 47s) Loss: 0.0036(0.0018) Grad: 13190.1426  LR: 0.000007  \n",
      "Epoch: [4][1200/2871] Elapsed 6m 38s (remain 9m 13s) Loss: 0.0018(0.0019) Grad: 13558.1641  LR: 0.000007  \n",
      "Epoch: [4][1300/2871] Elapsed 7m 10s (remain 8m 40s) Loss: 0.0004(0.0019) Grad: 3080.1074  LR: 0.000007  \n",
      "Epoch: [4][1400/2871] Elapsed 7m 43s (remain 8m 6s) Loss: 0.0012(0.0019) Grad: 10097.1924  LR: 0.000007  \n",
      "Epoch: [4][1500/2871] Elapsed 8m 17s (remain 7m 33s) Loss: 0.0001(0.0019) Grad: 1112.9978  LR: 0.000007  \n",
      "Epoch: [4][1600/2871] Elapsed 8m 51s (remain 7m 1s) Loss: 0.0057(0.0019) Grad: 20313.7676  LR: 0.000006  \n",
      "Epoch: [4][1700/2871] Elapsed 9m 25s (remain 6m 28s) Loss: 0.0015(0.0019) Grad: 6723.3101  LR: 0.000006  \n",
      "Epoch: [4][1800/2871] Elapsed 9m 58s (remain 5m 55s) Loss: 0.0106(0.0019) Grad: 109190.8125  LR: 0.000006  \n",
      "Epoch: [4][1900/2871] Elapsed 10m 30s (remain 5m 21s) Loss: 0.0041(0.0019) Grad: 53439.6367  LR: 0.000006  \n",
      "Epoch: [4][2000/2871] Elapsed 11m 3s (remain 4m 48s) Loss: 0.0000(0.0018) Grad: 8.4816  LR: 0.000006  \n",
      "Epoch: [4][2100/2871] Elapsed 11m 36s (remain 4m 15s) Loss: 0.0010(0.0018) Grad: 8027.2534  LR: 0.000006  \n",
      "Epoch: [4][2200/2871] Elapsed 12m 8s (remain 3m 41s) Loss: 0.0014(0.0018) Grad: 3158.7297  LR: 0.000005  \n",
      "Epoch: [4][2300/2871] Elapsed 12m 42s (remain 3m 8s) Loss: 0.0007(0.0018) Grad: 21059.4707  LR: 0.000005  \n",
      "Epoch: [4][2400/2871] Elapsed 13m 15s (remain 2m 35s) Loss: 0.0022(0.0018) Grad: 5822.4722  LR: 0.000005  \n",
      "Epoch: [4][2500/2871] Elapsed 13m 48s (remain 2m 2s) Loss: 0.0044(0.0018) Grad: 18465.8555  LR: 0.000005  \n",
      "Epoch: [4][2600/2871] Elapsed 14m 22s (remain 1m 29s) Loss: 0.0013(0.0018) Grad: 7293.7358  LR: 0.000005  \n",
      "Epoch: [4][2700/2871] Elapsed 14m 54s (remain 0m 56s) Loss: 0.0043(0.0018) Grad: 18656.1855  LR: 0.000005  \n",
      "Epoch: [4][2800/2871] Elapsed 15m 27s (remain 0m 23s) Loss: 0.0002(0.0018) Grad: 2379.9329  LR: 0.000005  \n",
      "Epoch: [4][2870/2871] Elapsed 15m 50s (remain 0m 0s) Loss: 0.0018(0.0018) Grad: 8475.6191  LR: 0.000004  \n",
      "EVAL: [0/704] Elapsed 0m 0s (remain 5m 20s) Loss: 0.0003(0.0003) \n",
      "EVAL: [100/704] Elapsed 0m 21s (remain 2m 5s) Loss: 0.0004(0.0028) \n",
      "EVAL: [200/704] Elapsed 0m 41s (remain 1m 43s) Loss: 0.0000(0.0025) \n",
      "EVAL: [300/704] Elapsed 1m 1s (remain 1m 22s) Loss: 0.0001(0.0022) \n",
      "EVAL: [400/704] Elapsed 1m 21s (remain 1m 1s) Loss: 0.0067(0.0026) \n",
      "EVAL: [500/704] Elapsed 1m 42s (remain 0m 41s) Loss: 0.0020(0.0028) \n",
      "EVAL: [600/704] Elapsed 2m 2s (remain 0m 20s) Loss: 0.0000(0.0030) \n",
      "EVAL: [700/704] Elapsed 2m 22s (remain 0m 0s) Loss: 0.0000(0.0027) \n",
      "EVAL: [703/704] Elapsed 2m 23s (remain 0m 0s) Loss: 0.0000(0.0027) \n",
      "Epoch 4 - avg_train_loss: 0.0018  avg_val_loss: 0.0027  time: 1098s\n",
      "Epoch 4 - Score: 0.8620\n",
      "Epoch 4 - Save Best Score: 0.8620 Model\n",
      "Epoch: [5][0/2871] Elapsed 0m 0s (remain 31m 28s) Loss: 0.0001(0.0001) Grad: 1264.2341  LR: 0.000004  \n",
      "Epoch: [5][100/2871] Elapsed 0m 35s (remain 16m 17s) Loss: 0.0002(0.0012) Grad: 2018.8688  LR: 0.000004  \n",
      "Epoch: [5][200/2871] Elapsed 1m 8s (remain 15m 5s) Loss: 0.0122(0.0015) Grad: 54325.9297  LR: 0.000004  \n",
      "Epoch: [5][300/2871] Elapsed 1m 41s (remain 14m 24s) Loss: 0.0000(0.0016) Grad: 419.9316  LR: 0.000004  \n",
      "Epoch: [5][400/2871] Elapsed 2m 15s (remain 13m 56s) Loss: 0.0000(0.0015) Grad: 19.6232  LR: 0.000004  \n",
      "Epoch: [5][500/2871] Elapsed 2m 48s (remain 13m 17s) Loss: 0.0003(0.0015) Grad: 2985.1155  LR: 0.000004  \n",
      "Epoch: [5][600/2871] Elapsed 3m 21s (remain 12m 42s) Loss: 0.0001(0.0015) Grad: 3741.8750  LR: 0.000004  \n",
      "Epoch: [5][700/2871] Elapsed 3m 55s (remain 12m 7s) Loss: 0.0000(0.0016) Grad: 132.4870  LR: 0.000003  \n",
      "Epoch: [5][800/2871] Elapsed 4m 28s (remain 11m 33s) Loss: 0.0000(0.0016) Grad: 695.2478  LR: 0.000003  \n",
      "Epoch: [5][900/2871] Elapsed 5m 1s (remain 10m 58s) Loss: 0.0009(0.0015) Grad: 6421.8438  LR: 0.000003  \n",
      "Epoch: [5][1000/2871] Elapsed 5m 33s (remain 10m 23s) Loss: 0.0010(0.0016) Grad: 9080.0898  LR: 0.000003  \n",
      "Epoch: [5][1100/2871] Elapsed 6m 6s (remain 9m 49s) Loss: 0.0005(0.0015) Grad: 5373.3110  LR: 0.000003  \n",
      "Epoch: [5][1200/2871] Elapsed 6m 42s (remain 9m 19s) Loss: 0.0019(0.0015) Grad: 11006.6709  LR: 0.000003  \n",
      "Epoch: [5][1300/2871] Elapsed 7m 16s (remain 8m 46s) Loss: 0.0005(0.0015) Grad: 4874.3301  LR: 0.000002  \n",
      "Epoch: [5][1400/2871] Elapsed 7m 48s (remain 8m 11s) Loss: 0.0000(0.0015) Grad: 335.1268  LR: 0.000002  \n",
      "Epoch: [5][1500/2871] Elapsed 8m 21s (remain 7m 37s) Loss: 0.0007(0.0015) Grad: 10093.8936  LR: 0.000002  \n",
      "Epoch: [5][1600/2871] Elapsed 8m 53s (remain 7m 3s) Loss: 0.0003(0.0015) Grad: 8455.3154  LR: 0.000002  \n",
      "Epoch: [5][1700/2871] Elapsed 9m 26s (remain 6m 29s) Loss: 0.0034(0.0015) Grad: 27705.7832  LR: 0.000002  \n",
      "Epoch: [5][1800/2871] Elapsed 9m 59s (remain 5m 56s) Loss: 0.0019(0.0015) Grad: 10765.9766  LR: 0.000002  \n",
      "Epoch: [5][1900/2871] Elapsed 10m 33s (remain 5m 23s) Loss: 0.0001(0.0014) Grad: 1210.0258  LR: 0.000002  \n",
      "Epoch: [5][2000/2871] Elapsed 11m 6s (remain 4m 49s) Loss: 0.0003(0.0014) Grad: 4611.5547  LR: 0.000001  \n",
      "Epoch: [5][2100/2871] Elapsed 11m 38s (remain 4m 16s) Loss: 0.0139(0.0015) Grad: 150873.6719  LR: 0.000001  \n",
      "Epoch: [5][2200/2871] Elapsed 12m 11s (remain 3m 42s) Loss: 0.0007(0.0015) Grad: 5604.5093  LR: 0.000001  \n",
      "Epoch: [5][2300/2871] Elapsed 12m 44s (remain 3m 9s) Loss: 0.0002(0.0015) Grad: 2517.2471  LR: 0.000001  \n",
      "Epoch: [5][2400/2871] Elapsed 13m 17s (remain 2m 36s) Loss: 0.0003(0.0015) Grad: 4572.6440  LR: 0.000001  \n",
      "Epoch: [5][2500/2871] Elapsed 13m 51s (remain 2m 2s) Loss: 0.0005(0.0015) Grad: 3358.6431  LR: 0.000001  \n",
      "Epoch: [5][2600/2871] Elapsed 14m 24s (remain 1m 29s) Loss: 0.0003(0.0015) Grad: 5705.6196  LR: 0.000000  \n",
      "Epoch: [5][2700/2871] Elapsed 14m 56s (remain 0m 56s) Loss: 0.0001(0.0015) Grad: 899.4096  LR: 0.000000  \n",
      "Epoch: [5][2800/2871] Elapsed 15m 29s (remain 0m 23s) Loss: 0.0006(0.0015) Grad: 4961.7070  LR: 0.000000  \n",
      "Epoch: [5][2870/2871] Elapsed 15m 54s (remain 0m 0s) Loss: 0.0001(0.0015) Grad: 1596.0830  LR: 0.000000  \n",
      "EVAL: [0/704] Elapsed 0m 0s (remain 5m 36s) Loss: 0.0003(0.0003) \n",
      "EVAL: [100/704] Elapsed 0m 20s (remain 2m 3s) Loss: 0.0005(0.0035) \n",
      "EVAL: [200/704] Elapsed 0m 41s (remain 1m 44s) Loss: 0.0000(0.0029) \n",
      "EVAL: [300/704] Elapsed 1m 3s (remain 1m 24s) Loss: 0.0000(0.0026) \n",
      "EVAL: [400/704] Elapsed 1m 23s (remain 1m 2s) Loss: 0.0084(0.0030) \n",
      "EVAL: [500/704] Elapsed 1m 43s (remain 0m 41s) Loss: 0.0024(0.0033) \n",
      "EVAL: [600/704] Elapsed 2m 3s (remain 0m 21s) Loss: 0.0000(0.0034) \n",
      "EVAL: [700/704] Elapsed 2m 24s (remain 0m 0s) Loss: 0.0000(0.0032) \n",
      "EVAL: [703/704] Elapsed 2m 24s (remain 0m 0s) Loss: 0.0000(0.0032) \n",
      "Epoch 5 - avg_train_loss: 0.0015  avg_val_loss: 0.0032  time: 1104s\n",
      "Epoch 5 - Score: 0.8635\n",
      "Epoch 5 - Save Best Score: 0.8635 Model\n",
      "========== fold: 3 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp033/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2877] Elapsed 0m 0s (remain 18m 29s) Loss: 0.1285(0.1285) Grad: inf  LR: 0.000000  \n",
      "Epoch: [1][100/2877] Elapsed 0m 16s (remain 7m 30s) Loss: 0.1224(0.1429) Grad: 28457.3438  LR: 0.000001  \n",
      "Epoch: [1][200/2877] Elapsed 0m 31s (remain 6m 52s) Loss: 0.1912(0.1409) Grad: 43959.2539  LR: 0.000003  \n",
      "Epoch: [1][300/2877] Elapsed 0m 45s (remain 6m 29s) Loss: 0.1493(0.1400) Grad: 35573.8281  LR: 0.000004  \n",
      "Epoch: [1][400/2877] Elapsed 1m 0s (remain 6m 10s) Loss: 0.1100(0.1364) Grad: 25111.9609  LR: 0.000006  \n",
      "Epoch: [1][500/2877] Elapsed 1m 14s (remain 5m 54s) Loss: 0.1232(0.1329) Grad: 29216.9922  LR: 0.000007  \n",
      "Epoch: [1][600/2877] Elapsed 1m 29s (remain 5m 39s) Loss: 0.1141(0.1294) Grad: 29568.9355  LR: 0.000008  \n",
      "Epoch: [1][700/2877] Elapsed 1m 44s (remain 5m 24s) Loss: 0.0833(0.1242) Grad: 17687.6152  LR: 0.000010  \n",
      "Epoch: [1][800/2877] Elapsed 1m 59s (remain 5m 9s) Loss: 0.0880(0.1192) Grad: 22304.5117  LR: 0.000011  \n",
      "Epoch: [1][900/2877] Elapsed 2m 13s (remain 4m 53s) Loss: 0.0536(0.1141) Grad: 12062.4258  LR: 0.000013  \n",
      "Epoch: [1][1000/2877] Elapsed 2m 28s (remain 4m 38s) Loss: 0.0653(0.1087) Grad: 15806.1592  LR: 0.000014  \n",
      "Epoch: [1][1100/2877] Elapsed 2m 43s (remain 4m 24s) Loss: 0.0494(0.1032) Grad: 12086.0088  LR: 0.000015  \n",
      "Epoch: [1][1200/2877] Elapsed 2m 58s (remain 4m 8s) Loss: 0.0267(0.0979) Grad: 3692.7747  LR: 0.000017  \n",
      "Epoch: [1][1300/2877] Elapsed 3m 12s (remain 3m 53s) Loss: 0.0264(0.0926) Grad: 1457.7909  LR: 0.000018  \n",
      "Epoch: [1][1400/2877] Elapsed 3m 27s (remain 3m 38s) Loss: 0.0258(0.0878) Grad: 1952.1132  LR: 0.000019  \n",
      "Epoch: [1][1500/2877] Elapsed 3m 42s (remain 3m 24s) Loss: 0.0176(0.0833) Grad: 3488.0405  LR: 0.000020  \n",
      "Epoch: [1][1600/2877] Elapsed 3m 57s (remain 3m 9s) Loss: 0.0153(0.0793) Grad: 1579.0227  LR: 0.000020  \n",
      "Epoch: [1][1700/2877] Elapsed 4m 11s (remain 2m 54s) Loss: 0.0141(0.0756) Grad: 852.6095  LR: 0.000020  \n",
      "Epoch: [1][1800/2877] Elapsed 4m 27s (remain 2m 39s) Loss: 0.0121(0.0724) Grad: 492.2967  LR: 0.000019  \n",
      "Epoch: [1][1900/2877] Elapsed 4m 41s (remain 2m 24s) Loss: 0.0332(0.0695) Grad: 3625.7881  LR: 0.000019  \n",
      "Epoch: [1][2000/2877] Elapsed 4m 56s (remain 2m 9s) Loss: 0.0102(0.0668) Grad: 667.2455  LR: 0.000019  \n",
      "Epoch: [1][2100/2877] Elapsed 5m 11s (remain 1m 55s) Loss: 0.0128(0.0644) Grad: 467.5359  LR: 0.000019  \n",
      "Epoch: [1][2200/2877] Elapsed 5m 26s (remain 1m 40s) Loss: 0.0244(0.0622) Grad: 1865.1188  LR: 0.000019  \n",
      "Epoch: [1][2300/2877] Elapsed 5m 40s (remain 1m 25s) Loss: 0.0293(0.0601) Grad: 2451.4717  LR: 0.000019  \n",
      "Epoch: [1][2400/2877] Elapsed 5m 55s (remain 1m 10s) Loss: 0.0071(0.0582) Grad: 818.0804  LR: 0.000019  \n",
      "Epoch: [1][2500/2877] Elapsed 6m 9s (remain 0m 55s) Loss: 0.0119(0.0565) Grad: 494.2281  LR: 0.000018  \n",
      "Epoch: [1][2600/2877] Elapsed 6m 24s (remain 0m 40s) Loss: 0.0235(0.0549) Grad: 2401.3357  LR: 0.000018  \n",
      "Epoch: [1][2700/2877] Elapsed 6m 38s (remain 0m 25s) Loss: 0.0104(0.0534) Grad: 472.0377  LR: 0.000018  \n",
      "Epoch: [1][2800/2877] Elapsed 6m 53s (remain 0m 11s) Loss: 0.0181(0.0521) Grad: 1475.5756  LR: 0.000018  \n",
      "Epoch: [1][2876/2877] Elapsed 7m 5s (remain 0m 0s) Loss: 0.0097(0.0511) Grad: 486.2014  LR: 0.000018  \n",
      "Epoch: [1][0/2877] Elapsed 0m 0s (remain 25m 39s) Loss: 0.0128(0.0128) Grad: 11947.4795  LR: 0.000000  \n",
      "Epoch: [1][100/2877] Elapsed 0m 33s (remain 15m 21s) Loss: 0.0115(0.0150) Grad: 8426.1064  LR: 0.000001  \n",
      "Epoch: [1][200/2877] Elapsed 1m 9s (remain 15m 26s) Loss: 0.0089(0.0142) Grad: 15082.8291  LR: 0.000003  \n",
      "Epoch: [1][300/2877] Elapsed 1m 45s (remain 15m 4s) Loss: 0.0144(0.0143) Grad: 9193.6729  LR: 0.000004  \n",
      "Epoch: [1][400/2877] Elapsed 2m 21s (remain 14m 34s) Loss: 0.0154(0.0142) Grad: 16881.3223  LR: 0.000006  \n",
      "Epoch: [1][500/2877] Elapsed 2m 55s (remain 13m 52s) Loss: 0.0097(0.0139) Grad: 10363.4395  LR: 0.000007  \n",
      "Epoch: [1][600/2877] Elapsed 3m 28s (remain 13m 11s) Loss: 0.0117(0.0140) Grad: 8475.1475  LR: 0.000008  \n",
      "Epoch: [1][700/2877] Elapsed 4m 4s (remain 12m 40s) Loss: 0.0122(0.0137) Grad: 9307.7451  LR: 0.000010  \n",
      "Epoch: [1][800/2877] Elapsed 4m 40s (remain 12m 7s) Loss: 0.0135(0.0137) Grad: 13017.0068  LR: 0.000011  \n",
      "Epoch: [1][900/2877] Elapsed 5m 14s (remain 11m 29s) Loss: 0.0067(0.0135) Grad: 12296.7373  LR: 0.000013  \n",
      "Epoch: [1][1000/2877] Elapsed 5m 47s (remain 10m 50s) Loss: 0.0082(0.0133) Grad: 11409.6514  LR: 0.000014  \n",
      "Epoch: [1][1100/2877] Elapsed 6m 19s (remain 10m 12s) Loss: 0.0062(0.0131) Grad: 13800.3359  LR: 0.000015  \n",
      "Epoch: [1][1200/2877] Elapsed 6m 52s (remain 9m 35s) Loss: 0.0087(0.0129) Grad: 12278.5801  LR: 0.000017  \n",
      "Epoch: [1][1300/2877] Elapsed 7m 25s (remain 8m 59s) Loss: 0.0056(0.0125) Grad: 14653.1699  LR: 0.000018  \n",
      "Epoch: [1][1400/2877] Elapsed 7m 57s (remain 8m 23s) Loss: 0.0043(0.0121) Grad: 10089.1309  LR: 0.000019  \n",
      "Epoch: [1][1500/2877] Elapsed 8m 30s (remain 7m 48s) Loss: 0.0044(0.0116) Grad: 8369.8057  LR: 0.000020  \n",
      "Epoch: [1][1600/2877] Elapsed 9m 4s (remain 7m 14s) Loss: 0.0018(0.0113) Grad: 3931.7405  LR: 0.000020  \n",
      "Epoch: [1][1700/2877] Elapsed 9m 38s (remain 6m 39s) Loss: 0.0055(0.0109) Grad: 8089.0405  LR: 0.000020  \n",
      "Epoch: [1][1800/2877] Elapsed 10m 12s (remain 6m 5s) Loss: 0.0008(0.0106) Grad: 2603.8040  LR: 0.000019  \n",
      "Epoch: [1][1900/2877] Elapsed 10m 45s (remain 5m 31s) Loss: 0.0006(0.0103) Grad: 1048.5925  LR: 0.000019  \n",
      "Epoch: [1][2000/2877] Elapsed 11m 18s (remain 4m 56s) Loss: 0.0016(0.0100) Grad: 3238.5251  LR: 0.000019  \n",
      "Epoch: [1][2100/2877] Elapsed 11m 51s (remain 4m 22s) Loss: 0.0124(0.0097) Grad: 21840.2402  LR: 0.000019  \n",
      "Epoch: [1][2200/2877] Elapsed 12m 25s (remain 3m 48s) Loss: 0.0020(0.0094) Grad: 6253.4219  LR: 0.000019  \n",
      "Epoch: [1][2300/2877] Elapsed 12m 57s (remain 3m 14s) Loss: 0.0005(0.0092) Grad: 2192.9285  LR: 0.000019  \n",
      "Epoch: [1][2400/2877] Elapsed 13m 30s (remain 2m 40s) Loss: 0.0002(0.0089) Grad: 667.7156  LR: 0.000019  \n",
      "Epoch: [1][2500/2877] Elapsed 14m 3s (remain 2m 6s) Loss: 0.0023(0.0087) Grad: 3716.0518  LR: 0.000018  \n",
      "Epoch: [1][2600/2877] Elapsed 14m 36s (remain 1m 33s) Loss: 0.0034(0.0085) Grad: 5447.0171  LR: 0.000018  \n",
      "Epoch: [1][2700/2877] Elapsed 15m 9s (remain 0m 59s) Loss: 0.0033(0.0083) Grad: 6429.3286  LR: 0.000018  \n",
      "Epoch: [1][2800/2877] Elapsed 15m 42s (remain 0m 25s) Loss: 0.0006(0.0082) Grad: 2080.6670  LR: 0.000018  \n",
      "Epoch: [1][2876/2877] Elapsed 16m 7s (remain 0m 0s) Loss: 0.0019(0.0081) Grad: 4941.7988  LR: 0.000018  \n",
      "EVAL: [0/698] Elapsed 0m 0s (remain 5m 45s) Loss: 0.0016(0.0016) \n",
      "EVAL: [100/698] Elapsed 0m 21s (remain 2m 5s) Loss: 0.0004(0.0018) \n",
      "EVAL: [200/698] Elapsed 0m 42s (remain 1m 46s) Loss: 0.0024(0.0023) \n",
      "EVAL: [300/698] Elapsed 1m 3s (remain 1m 23s) Loss: 0.0004(0.0021) \n",
      "EVAL: [400/698] Elapsed 1m 23s (remain 1m 1s) Loss: 0.0056(0.0026) \n",
      "EVAL: [500/698] Elapsed 1m 43s (remain 0m 40s) Loss: 0.0026(0.0027) \n",
      "EVAL: [600/698] Elapsed 2m 4s (remain 0m 20s) Loss: 0.0022(0.0026) \n",
      "EVAL: [697/698] Elapsed 2m 24s (remain 0m 0s) Loss: 0.0000(0.0025) \n",
      "Epoch 1 - avg_train_loss: 0.0081  avg_val_loss: 0.0025  time: 1117s\n",
      "Epoch 1 - Score: 0.7890\n",
      "Epoch 1 - Save Best Score: 0.7890 Model\n",
      "Epoch: [2][0/2877] Elapsed 0m 0s (remain 27m 45s) Loss: 0.0005(0.0005) Grad: 3433.4521  LR: 0.000018  \n",
      "Epoch: [2][100/2877] Elapsed 0m 33s (remain 15m 14s) Loss: 0.0004(0.0025) Grad: 4595.7510  LR: 0.000018  \n",
      "Epoch: [2][200/2877] Elapsed 1m 7s (remain 15m 1s) Loss: 0.0008(0.0024) Grad: 20270.1406  LR: 0.000017  \n",
      "Epoch: [2][300/2877] Elapsed 1m 42s (remain 14m 41s) Loss: 0.0019(0.0026) Grad: 9728.6826  LR: 0.000017  \n",
      "Epoch: [2][400/2877] Elapsed 2m 16s (remain 14m 1s) Loss: 0.0027(0.0028) Grad: 52183.5664  LR: 0.000017  \n",
      "Epoch: [2][500/2877] Elapsed 2m 49s (remain 13m 23s) Loss: 0.0026(0.0029) Grad: 14076.1172  LR: 0.000017  \n",
      "Epoch: [2][600/2877] Elapsed 3m 22s (remain 12m 46s) Loss: 0.0015(0.0028) Grad: 6986.2236  LR: 0.000017  \n",
      "Epoch: [2][700/2877] Elapsed 3m 55s (remain 12m 11s) Loss: 0.0004(0.0029) Grad: 2429.5310  LR: 0.000017  \n",
      "Epoch: [2][800/2877] Elapsed 4m 29s (remain 11m 39s) Loss: 0.0056(0.0028) Grad: 17755.1191  LR: 0.000017  \n",
      "Epoch: [2][900/2877] Elapsed 5m 2s (remain 11m 4s) Loss: 0.0039(0.0027) Grad: 7260.0688  LR: 0.000016  \n",
      "Epoch: [2][1000/2877] Elapsed 5m 35s (remain 10m 29s) Loss: 0.0021(0.0028) Grad: 11027.8184  LR: 0.000016  \n",
      "Epoch: [2][1100/2877] Elapsed 6m 9s (remain 9m 55s) Loss: 0.0009(0.0027) Grad: 6965.9004  LR: 0.000016  \n",
      "Epoch: [2][1200/2877] Elapsed 6m 41s (remain 9m 20s) Loss: 0.0002(0.0027) Grad: 6416.5273  LR: 0.000016  \n",
      "Epoch: [2][1300/2877] Elapsed 7m 14s (remain 8m 46s) Loss: 0.0001(0.0027) Grad: 1242.7656  LR: 0.000016  \n",
      "Epoch: [2][1400/2877] Elapsed 7m 47s (remain 8m 12s) Loss: 0.0007(0.0026) Grad: 5204.5283  LR: 0.000016  \n",
      "Epoch: [2][1500/2877] Elapsed 8m 21s (remain 7m 39s) Loss: 0.0003(0.0027) Grad: 3413.3689  LR: 0.000015  \n",
      "Epoch: [2][1600/2877] Elapsed 8m 54s (remain 7m 6s) Loss: 0.0003(0.0027) Grad: 1737.7120  LR: 0.000015  \n",
      "Epoch: [2][1700/2877] Elapsed 9m 28s (remain 6m 33s) Loss: 0.0020(0.0027) Grad: 9591.2002  LR: 0.000015  \n",
      "Epoch: [2][1800/2877] Elapsed 10m 1s (remain 5m 59s) Loss: 0.0092(0.0027) Grad: 22926.1484  LR: 0.000015  \n",
      "Epoch: [2][1900/2877] Elapsed 10m 33s (remain 5m 25s) Loss: 0.0025(0.0027) Grad: 19826.5332  LR: 0.000015  \n",
      "Epoch: [2][2000/2877] Elapsed 11m 6s (remain 4m 51s) Loss: 0.0001(0.0027) Grad: 1167.0740  LR: 0.000015  \n",
      "Epoch: [2][2100/2877] Elapsed 11m 39s (remain 4m 18s) Loss: 0.0008(0.0027) Grad: 3926.8303  LR: 0.000015  \n",
      "Epoch: [2][2200/2877] Elapsed 12m 12s (remain 3m 44s) Loss: 0.0005(0.0026) Grad: 5806.6787  LR: 0.000014  \n",
      "Epoch: [2][2300/2877] Elapsed 12m 45s (remain 3m 11s) Loss: 0.0006(0.0027) Grad: 3534.2646  LR: 0.000014  \n",
      "Epoch: [2][2400/2877] Elapsed 13m 18s (remain 2m 38s) Loss: 0.0003(0.0027) Grad: 1963.7853  LR: 0.000014  \n",
      "Epoch: [2][2500/2877] Elapsed 13m 51s (remain 2m 5s) Loss: 0.0007(0.0026) Grad: 4986.1035  LR: 0.000014  \n",
      "Epoch: [2][2600/2877] Elapsed 14m 25s (remain 1m 31s) Loss: 0.0057(0.0026) Grad: 22590.1074  LR: 0.000014  \n",
      "Epoch: [2][2700/2877] Elapsed 14m 59s (remain 0m 58s) Loss: 0.0093(0.0026) Grad: 25840.8301  LR: 0.000014  \n",
      "Epoch: [2][2800/2877] Elapsed 15m 32s (remain 0m 25s) Loss: 0.0022(0.0026) Grad: 52231.6758  LR: 0.000013  \n",
      "Epoch: [2][2876/2877] Elapsed 15m 57s (remain 0m 0s) Loss: 0.0003(0.0026) Grad: 3136.7969  LR: 0.000013  \n",
      "EVAL: [0/698] Elapsed 0m 0s (remain 5m 34s) Loss: 0.0007(0.0007) \n",
      "EVAL: [100/698] Elapsed 0m 20s (remain 2m 3s) Loss: 0.0002(0.0018) \n",
      "EVAL: [200/698] Elapsed 0m 41s (remain 1m 41s) Loss: 0.0047(0.0025) \n",
      "EVAL: [300/698] Elapsed 1m 1s (remain 1m 20s) Loss: 0.0002(0.0023) \n",
      "EVAL: [400/698] Elapsed 1m 21s (remain 1m 0s) Loss: 0.0094(0.0026) \n",
      "EVAL: [500/698] Elapsed 1m 42s (remain 0m 40s) Loss: 0.0041(0.0027) \n",
      "EVAL: [600/698] Elapsed 2m 2s (remain 0m 19s) Loss: 0.0030(0.0026) \n",
      "EVAL: [697/698] Elapsed 2m 22s (remain 0m 0s) Loss: 0.0000(0.0026) \n",
      "Epoch 2 - avg_train_loss: 0.0026  avg_val_loss: 0.0026  time: 1105s\n",
      "Epoch 2 - Score: 0.8441\n",
      "Epoch 2 - Save Best Score: 0.8441 Model\n",
      "Epoch: [3][0/2877] Elapsed 0m 0s (remain 28m 9s) Loss: 0.0028(0.0028) Grad: 16425.0449  LR: 0.000013  \n",
      "Epoch: [3][100/2877] Elapsed 0m 35s (remain 16m 3s) Loss: 0.0084(0.0019) Grad: 21988.9551  LR: 0.000013  \n",
      "Epoch: [3][200/2877] Elapsed 1m 7s (remain 15m 0s) Loss: 0.0005(0.0020) Grad: 3499.7280  LR: 0.000013  \n",
      "Epoch: [3][300/2877] Elapsed 1m 40s (remain 14m 19s) Loss: 0.0002(0.0019) Grad: 870.1995  LR: 0.000013  \n",
      "Epoch: [3][400/2877] Elapsed 2m 12s (remain 13m 40s) Loss: 0.0010(0.0020) Grad: 8446.3467  LR: 0.000013  \n",
      "Epoch: [3][500/2877] Elapsed 2m 46s (remain 13m 7s) Loss: 0.0038(0.0020) Grad: 20910.9160  LR: 0.000013  \n",
      "Epoch: [3][600/2877] Elapsed 3m 20s (remain 12m 38s) Loss: 0.0006(0.0020) Grad: 6794.7080  LR: 0.000012  \n",
      "Epoch: [3][700/2877] Elapsed 3m 52s (remain 12m 3s) Loss: 0.0074(0.0021) Grad: 26634.2559  LR: 0.000012  \n",
      "Epoch: [3][800/2877] Elapsed 4m 26s (remain 11m 30s) Loss: 0.0003(0.0020) Grad: 2049.8345  LR: 0.000012  \n",
      "Epoch: [3][900/2877] Elapsed 4m 59s (remain 10m 55s) Loss: 0.0027(0.0020) Grad: 12200.0859  LR: 0.000012  \n",
      "Epoch: [3][1000/2877] Elapsed 5m 31s (remain 10m 21s) Loss: 0.0001(0.0020) Grad: 727.3350  LR: 0.000012  \n",
      "Epoch: [3][1100/2877] Elapsed 6m 5s (remain 9m 49s) Loss: 0.0004(0.0020) Grad: 2007.9370  LR: 0.000012  \n",
      "Epoch: [3][1200/2877] Elapsed 6m 38s (remain 9m 15s) Loss: 0.0007(0.0020) Grad: 5194.9053  LR: 0.000011  \n",
      "Epoch: [3][1300/2877] Elapsed 7m 10s (remain 8m 41s) Loss: 0.0016(0.0020) Grad: 45606.5938  LR: 0.000011  \n",
      "Epoch: [3][1400/2877] Elapsed 7m 44s (remain 8m 9s) Loss: 0.0021(0.0020) Grad: 8570.7510  LR: 0.000011  \n",
      "Epoch: [3][1500/2877] Elapsed 8m 16s (remain 7m 35s) Loss: 0.0002(0.0020) Grad: 1565.5100  LR: 0.000011  \n",
      "Epoch: [3][1600/2877] Elapsed 8m 49s (remain 7m 2s) Loss: 0.0003(0.0020) Grad: 3184.8440  LR: 0.000011  \n",
      "Epoch: [3][1700/2877] Elapsed 9m 22s (remain 6m 28s) Loss: 0.0020(0.0021) Grad: 22733.1621  LR: 0.000011  \n",
      "Epoch: [3][1800/2877] Elapsed 9m 56s (remain 5m 56s) Loss: 0.0001(0.0020) Grad: 426.9193  LR: 0.000011  \n",
      "Epoch: [3][1900/2877] Elapsed 10m 29s (remain 5m 23s) Loss: 0.0002(0.0021) Grad: 2780.6770  LR: 0.000010  \n",
      "Epoch: [3][2000/2877] Elapsed 11m 2s (remain 4m 49s) Loss: 0.0004(0.0021) Grad: 2552.4346  LR: 0.000010  \n",
      "Epoch: [3][2100/2877] Elapsed 11m 34s (remain 4m 16s) Loss: 0.0003(0.0021) Grad: 2247.0310  LR: 0.000010  \n",
      "Epoch: [3][2200/2877] Elapsed 12m 8s (remain 3m 43s) Loss: 0.0000(0.0021) Grad: 333.0923  LR: 0.000010  \n",
      "Epoch: [3][2300/2877] Elapsed 12m 42s (remain 3m 10s) Loss: 0.0003(0.0021) Grad: 1763.1111  LR: 0.000010  \n",
      "Epoch: [3][2400/2877] Elapsed 13m 16s (remain 2m 37s) Loss: 0.0002(0.0021) Grad: 1088.1118  LR: 0.000010  \n",
      "Epoch: [3][2500/2877] Elapsed 13m 49s (remain 2m 4s) Loss: 0.0009(0.0021) Grad: 6017.3662  LR: 0.000009  \n",
      "Epoch: [3][2600/2877] Elapsed 14m 21s (remain 1m 31s) Loss: 0.0029(0.0021) Grad: 8091.8467  LR: 0.000009  \n",
      "Epoch: [3][2700/2877] Elapsed 14m 54s (remain 0m 58s) Loss: 0.0015(0.0021) Grad: 3964.1270  LR: 0.000009  \n",
      "Epoch: [3][2800/2877] Elapsed 15m 27s (remain 0m 25s) Loss: 0.0004(0.0021) Grad: 1280.4921  LR: 0.000009  \n",
      "Epoch: [3][2876/2877] Elapsed 15m 54s (remain 0m 0s) Loss: 0.0000(0.0021) Grad: 135.9048  LR: 0.000009  \n",
      "EVAL: [0/698] Elapsed 0m 0s (remain 5m 22s) Loss: 0.0004(0.0004) \n",
      "EVAL: [100/698] Elapsed 0m 20s (remain 2m 3s) Loss: 0.0003(0.0016) \n",
      "EVAL: [200/698] Elapsed 0m 41s (remain 1m 41s) Loss: 0.0022(0.0023) \n",
      "EVAL: [300/698] Elapsed 1m 2s (remain 1m 23s) Loss: 0.0005(0.0022) \n",
      "EVAL: [400/698] Elapsed 1m 24s (remain 1m 2s) Loss: 0.0138(0.0025) \n",
      "EVAL: [500/698] Elapsed 1m 45s (remain 0m 41s) Loss: 0.0021(0.0024) \n",
      "EVAL: [600/698] Elapsed 2m 5s (remain 0m 20s) Loss: 0.0017(0.0024) \n",
      "EVAL: [697/698] Elapsed 2m 24s (remain 0m 0s) Loss: 0.0000(0.0023) \n",
      "Epoch 3 - avg_train_loss: 0.0021  avg_val_loss: 0.0023  time: 1105s\n",
      "Epoch 3 - Score: 0.8588\n",
      "Epoch 3 - Save Best Score: 0.8588 Model\n",
      "Epoch: [4][0/2877] Elapsed 0m 0s (remain 31m 2s) Loss: 0.0001(0.0001) Grad: 1615.6011  LR: 0.000009  \n",
      "Epoch: [4][100/2877] Elapsed 0m 34s (remain 15m 49s) Loss: 0.0006(0.0011) Grad: 2491.3137  LR: 0.000009  \n",
      "Epoch: [4][200/2877] Elapsed 1m 7s (remain 14m 57s) Loss: 0.0009(0.0015) Grad: 6391.6660  LR: 0.000009  \n",
      "Epoch: [4][300/2877] Elapsed 1m 40s (remain 14m 18s) Loss: 0.0003(0.0016) Grad: 1988.8792  LR: 0.000008  \n",
      "Epoch: [4][400/2877] Elapsed 2m 14s (remain 13m 50s) Loss: 0.0003(0.0017) Grad: 2740.6191  LR: 0.000008  \n",
      "Epoch: [4][500/2877] Elapsed 2m 49s (remain 13m 22s) Loss: 0.0026(0.0016) Grad: 7260.6626  LR: 0.000008  \n",
      "Epoch: [4][600/2877] Elapsed 3m 22s (remain 12m 46s) Loss: 0.0020(0.0017) Grad: 8354.7920  LR: 0.000008  \n",
      "Epoch: [4][700/2877] Elapsed 3m 55s (remain 12m 10s) Loss: 0.0006(0.0016) Grad: 6520.0981  LR: 0.000008  \n",
      "Epoch: [4][800/2877] Elapsed 4m 28s (remain 11m 35s) Loss: 0.0003(0.0016) Grad: 2593.8645  LR: 0.000008  \n",
      "Epoch: [4][900/2877] Elapsed 5m 1s (remain 11m 1s) Loss: 0.0000(0.0016) Grad: 52.4443  LR: 0.000007  \n",
      "Epoch: [4][1000/2877] Elapsed 5m 34s (remain 10m 26s) Loss: 0.0001(0.0016) Grad: 3628.8811  LR: 0.000007  \n",
      "Epoch: [4][1100/2877] Elapsed 6m 7s (remain 9m 52s) Loss: 0.0000(0.0017) Grad: 1166.4489  LR: 0.000007  \n",
      "Epoch: [4][1200/2877] Elapsed 6m 39s (remain 9m 18s) Loss: 0.0007(0.0017) Grad: 4164.3062  LR: 0.000007  \n",
      "Epoch: [4][1300/2877] Elapsed 7m 12s (remain 8m 43s) Loss: 0.0006(0.0017) Grad: 30067.8574  LR: 0.000007  \n",
      "Epoch: [4][1400/2877] Elapsed 7m 45s (remain 8m 10s) Loss: 0.0103(0.0018) Grad: 49271.9102  LR: 0.000007  \n",
      "Epoch: [4][1500/2877] Elapsed 8m 18s (remain 7m 37s) Loss: 0.0007(0.0018) Grad: 15911.7227  LR: 0.000007  \n",
      "Epoch: [4][1600/2877] Elapsed 8m 52s (remain 7m 4s) Loss: 0.0000(0.0018) Grad: 58.9073  LR: 0.000006  \n",
      "Epoch: [4][1700/2877] Elapsed 9m 25s (remain 6m 30s) Loss: 0.0002(0.0018) Grad: 580.2839  LR: 0.000006  \n",
      "Epoch: [4][1800/2877] Elapsed 9m 57s (remain 5m 57s) Loss: 0.0003(0.0018) Grad: 1726.1753  LR: 0.000006  \n",
      "Epoch: [4][1900/2877] Elapsed 10m 30s (remain 5m 23s) Loss: 0.0012(0.0018) Grad: 5047.7109  LR: 0.000006  \n",
      "Epoch: [4][2000/2877] Elapsed 11m 3s (remain 4m 50s) Loss: 0.0000(0.0018) Grad: 83.8068  LR: 0.000006  \n",
      "Epoch: [4][2100/2877] Elapsed 11m 36s (remain 4m 17s) Loss: 0.0042(0.0017) Grad: 28918.9668  LR: 0.000006  \n",
      "Epoch: [4][2200/2877] Elapsed 12m 9s (remain 3m 43s) Loss: 0.0019(0.0017) Grad: 4799.1274  LR: 0.000005  \n",
      "Epoch: [4][2300/2877] Elapsed 12m 41s (remain 3m 10s) Loss: 0.0056(0.0017) Grad: 13286.4463  LR: 0.000005  \n",
      "Epoch: [4][2400/2877] Elapsed 13m 15s (remain 2m 37s) Loss: 0.0003(0.0017) Grad: 1448.9064  LR: 0.000005  \n",
      "Epoch: [4][2500/2877] Elapsed 13m 48s (remain 2m 4s) Loss: 0.0002(0.0017) Grad: 1356.1454  LR: 0.000005  \n",
      "Epoch: [4][2600/2877] Elapsed 14m 20s (remain 1m 31s) Loss: 0.0006(0.0017) Grad: 5412.7432  LR: 0.000005  \n",
      "Epoch: [4][2700/2877] Elapsed 14m 53s (remain 0m 58s) Loss: 0.0029(0.0017) Grad: 29834.7871  LR: 0.000005  \n",
      "Epoch: [4][2800/2877] Elapsed 15m 25s (remain 0m 25s) Loss: 0.0000(0.0017) Grad: 32.3720  LR: 0.000005  \n",
      "Epoch: [4][2876/2877] Elapsed 15m 50s (remain 0m 0s) Loss: 0.0007(0.0017) Grad: 1661.1268  LR: 0.000004  \n",
      "EVAL: [0/698] Elapsed 0m 0s (remain 5m 19s) Loss: 0.0004(0.0004) \n",
      "EVAL: [100/698] Elapsed 0m 21s (remain 2m 5s) Loss: 0.0003(0.0017) \n",
      "EVAL: [200/698] Elapsed 0m 42s (remain 1m 44s) Loss: 0.0040(0.0025) \n",
      "EVAL: [300/698] Elapsed 1m 2s (remain 1m 22s) Loss: 0.0004(0.0023) \n",
      "EVAL: [400/698] Elapsed 1m 23s (remain 1m 1s) Loss: 0.0104(0.0026) \n",
      "EVAL: [500/698] Elapsed 1m 44s (remain 0m 40s) Loss: 0.0020(0.0026) \n",
      "EVAL: [600/698] Elapsed 2m 4s (remain 0m 20s) Loss: 0.0011(0.0026) \n",
      "EVAL: [697/698] Elapsed 2m 24s (remain 0m 0s) Loss: 0.0000(0.0025) \n",
      "Epoch 4 - avg_train_loss: 0.0017  avg_val_loss: 0.0025  time: 1101s\n",
      "Epoch 4 - Score: 0.8653\n",
      "Epoch 4 - Save Best Score: 0.8653 Model\n",
      "Epoch: [5][0/2877] Elapsed 0m 0s (remain 28m 16s) Loss: 0.0001(0.0001) Grad: 1401.2549  LR: 0.000004  \n",
      "Epoch: [5][100/2877] Elapsed 0m 33s (remain 15m 17s) Loss: 0.0070(0.0014) Grad: 39900.5547  LR: 0.000004  \n",
      "Epoch: [5][200/2877] Elapsed 1m 6s (remain 14m 49s) Loss: 0.0042(0.0013) Grad: 23642.7500  LR: 0.000004  \n",
      "Epoch: [5][300/2877] Elapsed 1m 40s (remain 14m 17s) Loss: 0.0203(0.0013) Grad: 49790.5742  LR: 0.000004  \n",
      "Epoch: [5][400/2877] Elapsed 2m 16s (remain 14m 0s) Loss: 0.0031(0.0013) Grad: 157767.2031  LR: 0.000004  \n",
      "Epoch: [5][500/2877] Elapsed 2m 49s (remain 13m 25s) Loss: 0.0005(0.0013) Grad: 12632.1445  LR: 0.000004  \n",
      "Epoch: [5][600/2877] Elapsed 3m 22s (remain 12m 48s) Loss: 0.0040(0.0013) Grad: 12856.6318  LR: 0.000004  \n",
      "Epoch: [5][700/2877] Elapsed 3m 55s (remain 12m 11s) Loss: 0.0001(0.0012) Grad: 795.8591  LR: 0.000003  \n",
      "Epoch: [5][800/2877] Elapsed 4m 28s (remain 11m 36s) Loss: 0.0000(0.0013) Grad: 123.9385  LR: 0.000003  \n",
      "Epoch: [5][900/2877] Elapsed 5m 1s (remain 11m 1s) Loss: 0.0014(0.0013) Grad: 15453.0713  LR: 0.000003  \n",
      "Epoch: [5][1000/2877] Elapsed 5m 34s (remain 10m 26s) Loss: 0.0003(0.0013) Grad: 3786.2957  LR: 0.000003  \n",
      "Epoch: [5][1100/2877] Elapsed 6m 6s (remain 9m 51s) Loss: 0.0001(0.0013) Grad: 919.3534  LR: 0.000003  \n",
      "Epoch: [5][1200/2877] Elapsed 6m 39s (remain 9m 17s) Loss: 0.0067(0.0014) Grad: 96538.0781  LR: 0.000003  \n",
      "Epoch: [5][1300/2877] Elapsed 7m 12s (remain 8m 43s) Loss: 0.0019(0.0014) Grad: 42835.5156  LR: 0.000002  \n",
      "Epoch: [5][1400/2877] Elapsed 7m 45s (remain 8m 10s) Loss: 0.0005(0.0014) Grad: 4902.3354  LR: 0.000002  \n",
      "Epoch: [5][1500/2877] Elapsed 8m 18s (remain 7m 37s) Loss: 0.0042(0.0014) Grad: 24463.4883  LR: 0.000002  \n",
      "Epoch: [5][1600/2877] Elapsed 8m 51s (remain 7m 3s) Loss: 0.0001(0.0014) Grad: 1274.1703  LR: 0.000002  \n",
      "Epoch: [5][1700/2877] Elapsed 9m 23s (remain 6m 29s) Loss: 0.0006(0.0014) Grad: 3294.5840  LR: 0.000002  \n",
      "Epoch: [5][1800/2877] Elapsed 9m 56s (remain 5m 56s) Loss: 0.0000(0.0014) Grad: 659.3658  LR: 0.000002  \n",
      "Epoch: [5][1900/2877] Elapsed 10m 29s (remain 5m 23s) Loss: 0.0000(0.0014) Grad: 119.2055  LR: 0.000002  \n",
      "Epoch: [5][2000/2877] Elapsed 11m 4s (remain 4m 50s) Loss: 0.0001(0.0014) Grad: 3868.0347  LR: 0.000001  \n",
      "Epoch: [5][2100/2877] Elapsed 11m 37s (remain 4m 17s) Loss: 0.0000(0.0014) Grad: 369.8702  LR: 0.000001  \n",
      "Epoch: [5][2200/2877] Elapsed 12m 10s (remain 3m 44s) Loss: 0.0000(0.0014) Grad: 124.6085  LR: 0.000001  \n",
      "Epoch: [5][2300/2877] Elapsed 12m 42s (remain 3m 10s) Loss: 0.0031(0.0014) Grad: 14848.5967  LR: 0.000001  \n",
      "Epoch: [5][2400/2877] Elapsed 13m 16s (remain 2m 37s) Loss: 0.0001(0.0014) Grad: 1005.4357  LR: 0.000001  \n",
      "Epoch: [5][2500/2877] Elapsed 13m 49s (remain 2m 4s) Loss: 0.0014(0.0014) Grad: 8306.1182  LR: 0.000001  \n",
      "Epoch: [5][2600/2877] Elapsed 14m 22s (remain 1m 31s) Loss: 0.0002(0.0014) Grad: 1197.9476  LR: 0.000000  \n",
      "Epoch: [5][2700/2877] Elapsed 14m 55s (remain 0m 58s) Loss: 0.0000(0.0014) Grad: 35.5444  LR: 0.000000  \n",
      "Epoch: [5][2800/2877] Elapsed 15m 28s (remain 0m 25s) Loss: 0.0000(0.0014) Grad: 36.3951  LR: 0.000000  \n",
      "Epoch: [5][2876/2877] Elapsed 15m 53s (remain 0m 0s) Loss: 0.0000(0.0014) Grad: 47.2945  LR: 0.000000  \n",
      "EVAL: [0/698] Elapsed 0m 0s (remain 5m 34s) Loss: 0.0003(0.0003) \n",
      "EVAL: [100/698] Elapsed 0m 21s (remain 2m 5s) Loss: 0.0008(0.0022) \n",
      "EVAL: [200/698] Elapsed 0m 42s (remain 1m 44s) Loss: 0.0036(0.0031) \n",
      "EVAL: [300/698] Elapsed 1m 2s (remain 1m 22s) Loss: 0.0009(0.0029) \n",
      "EVAL: [400/698] Elapsed 1m 22s (remain 1m 1s) Loss: 0.0142(0.0032) \n",
      "EVAL: [500/698] Elapsed 1m 42s (remain 0m 40s) Loss: 0.0015(0.0032) \n",
      "EVAL: [600/698] Elapsed 2m 3s (remain 0m 19s) Loss: 0.0015(0.0031) \n",
      "EVAL: [697/698] Elapsed 2m 23s (remain 0m 0s) Loss: 0.0000(0.0030) \n",
      "Epoch 5 - avg_train_loss: 0.0014  avg_val_loss: 0.0030  time: 1102s\n",
      "Epoch 5 - Score: 0.8695\n",
      "Epoch 5 - Save Best Score: 0.8695 Model\n",
      "========== fold: 4 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp033/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2850] Elapsed 0m 0s (remain 18m 16s) Loss: 0.3207(0.3207) Grad: inf  LR: 0.000000  \n",
      "Epoch: [1][100/2850] Elapsed 0m 14s (remain 6m 45s) Loss: 0.2054(0.3315) Grad: 13165.1836  LR: 0.000001  \n",
      "Epoch: [1][200/2850] Elapsed 0m 29s (remain 6m 26s) Loss: 0.3434(0.3310) Grad: 20928.9668  LR: 0.000003  \n",
      "Epoch: [1][300/2850] Elapsed 0m 43s (remain 6m 10s) Loss: 0.3810(0.3285) Grad: 25481.6797  LR: 0.000004  \n",
      "Epoch: [1][400/2850] Elapsed 0m 59s (remain 6m 0s) Loss: 0.3332(0.3236) Grad: 22779.5547  LR: 0.000006  \n",
      "Epoch: [1][500/2850] Elapsed 1m 13s (remain 5m 45s) Loss: 0.4294(0.3183) Grad: 29735.8770  LR: 0.000007  \n",
      "Epoch: [1][600/2850] Elapsed 1m 28s (remain 5m 30s) Loss: 0.2212(0.3085) Grad: 15588.4453  LR: 0.000008  \n",
      "Epoch: [1][700/2850] Elapsed 1m 42s (remain 5m 15s) Loss: 0.1539(0.2963) Grad: 10839.2529  LR: 0.000010  \n",
      "Epoch: [1][800/2850] Elapsed 1m 57s (remain 5m 0s) Loss: 0.2312(0.2855) Grad: 17214.5410  LR: 0.000011  \n",
      "Epoch: [1][900/2850] Elapsed 2m 11s (remain 4m 45s) Loss: 0.1422(0.2742) Grad: 11347.6855  LR: 0.000013  \n",
      "Epoch: [1][1000/2850] Elapsed 2m 26s (remain 4m 29s) Loss: 0.1390(0.2627) Grad: 11170.3174  LR: 0.000014  \n",
      "Epoch: [1][1100/2850] Elapsed 2m 40s (remain 4m 15s) Loss: 0.0744(0.2500) Grad: 5680.9497  LR: 0.000015  \n",
      "Epoch: [1][1200/2850] Elapsed 2m 55s (remain 4m 0s) Loss: 0.0791(0.2381) Grad: 7027.6348  LR: 0.000017  \n",
      "Epoch: [1][1300/2850] Elapsed 3m 9s (remain 3m 45s) Loss: 0.0731(0.2260) Grad: 4287.2661  LR: 0.000018  \n",
      "Epoch: [1][1400/2850] Elapsed 3m 24s (remain 3m 31s) Loss: 0.0457(0.2143) Grad: 3611.9131  LR: 0.000020  \n",
      "Epoch: [1][1500/2850] Elapsed 3m 38s (remain 3m 16s) Loss: 0.0320(0.2032) Grad: 1831.5950  LR: 0.000020  \n",
      "Epoch: [1][1600/2850] Elapsed 3m 53s (remain 3m 2s) Loss: 0.0321(0.1929) Grad: 2605.0071  LR: 0.000020  \n",
      "Epoch: [1][1700/2850] Elapsed 4m 9s (remain 2m 48s) Loss: 0.0213(0.1833) Grad: 1410.1650  LR: 0.000020  \n",
      "Epoch: [1][1800/2850] Elapsed 4m 25s (remain 2m 34s) Loss: 0.0256(0.1745) Grad: 1878.7314  LR: 0.000019  \n",
      "Epoch: [1][1900/2850] Elapsed 4m 40s (remain 2m 20s) Loss: 0.0206(0.1664) Grad: 1458.6606  LR: 0.000019  \n",
      "Epoch: [1][2000/2850] Elapsed 4m 55s (remain 2m 5s) Loss: 0.0199(0.1591) Grad: 1033.2550  LR: 0.000019  \n",
      "Epoch: [1][2100/2850] Elapsed 5m 9s (remain 1m 50s) Loss: 0.0175(0.1524) Grad: 368.5403  LR: 0.000019  \n",
      "Epoch: [1][2200/2850] Elapsed 5m 24s (remain 1m 35s) Loss: 0.0191(0.1462) Grad: 315.9297  LR: 0.000019  \n",
      "Epoch: [1][2300/2850] Elapsed 5m 39s (remain 1m 20s) Loss: 0.0115(0.1405) Grad: 359.9320  LR: 0.000019  \n",
      "Epoch: [1][2400/2850] Elapsed 5m 54s (remain 1m 6s) Loss: 0.0078(0.1353) Grad: 589.8525  LR: 0.000018  \n",
      "Epoch: [1][2500/2850] Elapsed 6m 8s (remain 0m 51s) Loss: 0.0212(0.1305) Grad: 236.4959  LR: 0.000018  \n",
      "Epoch: [1][2600/2850] Elapsed 6m 22s (remain 0m 36s) Loss: 0.0145(0.1260) Grad: 122.1953  LR: 0.000018  \n",
      "Epoch: [1][2700/2850] Elapsed 6m 37s (remain 0m 21s) Loss: 0.0074(0.1219) Grad: 743.3074  LR: 0.000018  \n",
      "Epoch: [1][2800/2850] Elapsed 6m 51s (remain 0m 7s) Loss: 0.0150(0.1181) Grad: 154.1855  LR: 0.000018  \n",
      "Epoch: [1][2849/2850] Elapsed 6m 59s (remain 0m 0s) Loss: 0.0156(0.1163) Grad: 642.1625  LR: 0.000018  \n",
      "Epoch: [1][0/2850] Elapsed 0m 0s (remain 26m 49s) Loss: 0.0431(0.0431) Grad: 59137.8047  LR: 0.000000  \n",
      "Epoch: [1][100/2850] Elapsed 0m 34s (remain 15m 27s) Loss: 0.0107(0.0149) Grad: 15822.5479  LR: 0.000001  \n",
      "Epoch: [1][200/2850] Elapsed 1m 7s (remain 14m 48s) Loss: 0.0057(0.0144) Grad: 21384.6016  LR: 0.000003  \n",
      "Epoch: [1][300/2850] Elapsed 1m 40s (remain 14m 10s) Loss: 0.0099(0.0142) Grad: 9265.5508  LR: 0.000004  \n",
      "Epoch: [1][400/2850] Elapsed 2m 13s (remain 13m 33s) Loss: 0.0133(0.0140) Grad: 15344.1924  LR: 0.000006  \n",
      "Epoch: [1][500/2850] Elapsed 2m 46s (remain 12m 58s) Loss: 0.0167(0.0138) Grad: 11164.7148  LR: 0.000007  \n",
      "Epoch: [1][600/2850] Elapsed 3m 19s (remain 12m 25s) Loss: 0.0218(0.0136) Grad: 28846.1621  LR: 0.000008  \n",
      "Epoch: [1][700/2850] Elapsed 3m 52s (remain 11m 52s) Loss: 0.0173(0.0136) Grad: 11726.4219  LR: 0.000010  \n",
      "Epoch: [1][800/2850] Elapsed 4m 25s (remain 11m 19s) Loss: 0.0160(0.0135) Grad: 15393.1641  LR: 0.000011  \n",
      "Epoch: [1][900/2850] Elapsed 4m 58s (remain 10m 45s) Loss: 0.0177(0.0134) Grad: 24714.8828  LR: 0.000013  \n",
      "Epoch: [1][1000/2850] Elapsed 5m 32s (remain 10m 13s) Loss: 0.0129(0.0133) Grad: 28923.8750  LR: 0.000014  \n",
      "Epoch: [1][1100/2850] Elapsed 6m 6s (remain 9m 41s) Loss: 0.0111(0.0132) Grad: 6321.4854  LR: 0.000015  \n",
      "Epoch: [1][1200/2850] Elapsed 6m 39s (remain 9m 8s) Loss: 0.0122(0.0131) Grad: 10479.1602  LR: 0.000017  \n",
      "Epoch: [1][1300/2850] Elapsed 7m 12s (remain 8m 34s) Loss: 0.0084(0.0130) Grad: 12194.4141  LR: 0.000018  \n",
      "Epoch: [1][1400/2850] Elapsed 7m 44s (remain 8m 0s) Loss: 0.0077(0.0128) Grad: 29100.2598  LR: 0.000020  \n",
      "Epoch: [1][1500/2850] Elapsed 8m 18s (remain 7m 28s) Loss: 0.0037(0.0124) Grad: 23807.2500  LR: 0.000020  \n",
      "Epoch: [1][1600/2850] Elapsed 8m 53s (remain 6m 55s) Loss: 0.0033(0.0121) Grad: 6466.9590  LR: 0.000020  \n",
      "Epoch: [1][1700/2850] Elapsed 9m 25s (remain 6m 22s) Loss: 0.0076(0.0117) Grad: 21381.4531  LR: 0.000020  \n",
      "Epoch: [1][1800/2850] Elapsed 9m 58s (remain 5m 48s) Loss: 0.0009(0.0114) Grad: 3753.1123  LR: 0.000019  \n",
      "Epoch: [1][1900/2850] Elapsed 10m 33s (remain 5m 16s) Loss: 0.0032(0.0111) Grad: 10259.9990  LR: 0.000019  \n",
      "Epoch: [1][2000/2850] Elapsed 11m 6s (remain 4m 42s) Loss: 0.0088(0.0108) Grad: 41346.8242  LR: 0.000019  \n",
      "Epoch: [1][2100/2850] Elapsed 11m 39s (remain 4m 9s) Loss: 0.0014(0.0105) Grad: 9168.1602  LR: 0.000019  \n",
      "Epoch: [1][2200/2850] Elapsed 12m 12s (remain 3m 36s) Loss: 0.0031(0.0102) Grad: 26368.7500  LR: 0.000019  \n",
      "Epoch: [1][2300/2850] Elapsed 12m 46s (remain 3m 2s) Loss: 0.0032(0.0100) Grad: 236405.3125  LR: 0.000019  \n",
      "Epoch: [1][2400/2850] Elapsed 13m 19s (remain 2m 29s) Loss: 0.0054(0.0097) Grad: 29591.3027  LR: 0.000018  \n",
      "Epoch: [1][2500/2850] Elapsed 13m 53s (remain 1m 56s) Loss: 0.0015(0.0095) Grad: 4173.3560  LR: 0.000018  \n",
      "Epoch: [1][2600/2850] Elapsed 14m 26s (remain 1m 22s) Loss: 0.0101(0.0093) Grad: 14502.4990  LR: 0.000018  \n",
      "Epoch: [1][2700/2850] Elapsed 14m 59s (remain 0m 49s) Loss: 0.0069(0.0091) Grad: 5866.8984  LR: 0.000018  \n",
      "Epoch: [1][2800/2850] Elapsed 15m 33s (remain 0m 16s) Loss: 0.0027(0.0089) Grad: 3135.5962  LR: 0.000018  \n",
      "Epoch: [1][2849/2850] Elapsed 15m 50s (remain 0m 0s) Loss: 0.0038(0.0088) Grad: 4101.9155  LR: 0.000018  \n",
      "EVAL: [0/725] Elapsed 0m 0s (remain 4m 49s) Loss: 0.0028(0.0028) \n",
      "EVAL: [100/725] Elapsed 0m 20s (remain 2m 6s) Loss: 0.0006(0.0026) \n",
      "EVAL: [200/725] Elapsed 0m 40s (remain 1m 46s) Loss: 0.0050(0.0031) \n",
      "EVAL: [300/725] Elapsed 1m 1s (remain 1m 26s) Loss: 0.0028(0.0028) \n",
      "EVAL: [400/725] Elapsed 1m 21s (remain 1m 5s) Loss: 0.0163(0.0030) \n",
      "EVAL: [500/725] Elapsed 1m 41s (remain 0m 45s) Loss: 0.0028(0.0031) \n",
      "EVAL: [600/725] Elapsed 2m 2s (remain 0m 25s) Loss: 0.0016(0.0032) \n",
      "EVAL: [700/725] Elapsed 2m 22s (remain 0m 4s) Loss: 0.0013(0.0030) \n",
      "EVAL: [724/725] Elapsed 2m 28s (remain 0m 0s) Loss: 0.0017(0.0030) \n",
      "Epoch 1 - avg_train_loss: 0.0088  avg_val_loss: 0.0030  time: 1104s\n",
      "Epoch 1 - Score: 0.7680\n",
      "Epoch 1 - Save Best Score: 0.7680 Model\n",
      "Epoch: [2][0/2850] Elapsed 0m 0s (remain 29m 42s) Loss: 0.0017(0.0017) Grad: 14231.9160  LR: 0.000018  \n",
      "Epoch: [2][100/2850] Elapsed 0m 33s (remain 15m 11s) Loss: 0.0009(0.0031) Grad: 6701.4385  LR: 0.000018  \n",
      "Epoch: [2][200/2850] Elapsed 1m 6s (remain 14m 36s) Loss: 0.0024(0.0033) Grad: 19672.2148  LR: 0.000017  \n",
      "Epoch: [2][300/2850] Elapsed 1m 40s (remain 14m 6s) Loss: 0.0013(0.0033) Grad: 11084.4004  LR: 0.000017  \n",
      "Epoch: [2][400/2850] Elapsed 2m 13s (remain 13m 35s) Loss: 0.0073(0.0032) Grad: 33603.8125  LR: 0.000017  \n",
      "Epoch: [2][500/2850] Elapsed 2m 46s (remain 13m 1s) Loss: 0.0017(0.0031) Grad: 7907.2573  LR: 0.000017  \n",
      "Epoch: [2][600/2850] Elapsed 3m 20s (remain 12m 31s) Loss: 0.0028(0.0030) Grad: 44760.5820  LR: 0.000017  \n",
      "Epoch: [2][700/2850] Elapsed 3m 57s (remain 12m 6s) Loss: 0.0004(0.0030) Grad: 3708.1931  LR: 0.000017  \n",
      "Epoch: [2][800/2850] Elapsed 4m 30s (remain 11m 32s) Loss: 0.0028(0.0031) Grad: 20737.2969  LR: 0.000017  \n",
      "Epoch: [2][900/2850] Elapsed 5m 3s (remain 10m 56s) Loss: 0.0059(0.0031) Grad: 25038.0312  LR: 0.000016  \n",
      "Epoch: [2][1000/2850] Elapsed 5m 36s (remain 10m 21s) Loss: 0.0005(0.0030) Grad: 3858.9858  LR: 0.000016  \n",
      "Epoch: [2][1100/2850] Elapsed 6m 9s (remain 9m 46s) Loss: 0.0065(0.0030) Grad: 31530.5391  LR: 0.000016  \n",
      "Epoch: [2][1200/2850] Elapsed 6m 42s (remain 9m 13s) Loss: 0.0008(0.0030) Grad: 7912.0889  LR: 0.000016  \n",
      "Epoch: [2][1300/2850] Elapsed 7m 17s (remain 8m 41s) Loss: 0.0001(0.0030) Grad: 1494.9897  LR: 0.000016  \n",
      "Epoch: [2][1400/2850] Elapsed 7m 51s (remain 8m 7s) Loss: 0.0049(0.0030) Grad: 26086.6992  LR: 0.000016  \n",
      "Epoch: [2][1500/2850] Elapsed 8m 24s (remain 7m 33s) Loss: 0.0016(0.0030) Grad: 14507.9561  LR: 0.000015  \n",
      "Epoch: [2][1600/2850] Elapsed 8m 57s (remain 6m 59s) Loss: 0.0006(0.0030) Grad: 3221.1443  LR: 0.000015  \n",
      "Epoch: [2][1700/2850] Elapsed 9m 30s (remain 6m 25s) Loss: 0.0061(0.0030) Grad: 44917.0781  LR: 0.000015  \n",
      "Epoch: [2][1800/2850] Elapsed 10m 6s (remain 5m 53s) Loss: 0.0021(0.0030) Grad: 12082.1074  LR: 0.000015  \n",
      "Epoch: [2][1900/2850] Elapsed 10m 39s (remain 5m 19s) Loss: 0.0001(0.0030) Grad: 1427.1917  LR: 0.000015  \n",
      "Epoch: [2][2000/2850] Elapsed 11m 12s (remain 4m 45s) Loss: 0.0008(0.0030) Grad: 2045.6272  LR: 0.000015  \n",
      "Epoch: [2][2100/2850] Elapsed 11m 45s (remain 4m 11s) Loss: 0.0015(0.0030) Grad: 9918.8027  LR: 0.000015  \n",
      "Epoch: [2][2200/2850] Elapsed 12m 19s (remain 3m 37s) Loss: 0.0005(0.0030) Grad: 4859.6016  LR: 0.000014  \n",
      "Epoch: [2][2300/2850] Elapsed 12m 52s (remain 3m 4s) Loss: 0.0045(0.0030) Grad: 17366.0918  LR: 0.000014  \n",
      "Epoch: [2][2400/2850] Elapsed 13m 28s (remain 2m 31s) Loss: 0.0018(0.0029) Grad: 3554.9883  LR: 0.000014  \n",
      "Epoch: [2][2500/2850] Elapsed 14m 1s (remain 1m 57s) Loss: 0.0007(0.0029) Grad: 1126.0641  LR: 0.000014  \n",
      "Epoch: [2][2600/2850] Elapsed 14m 33s (remain 1m 23s) Loss: 0.0037(0.0029) Grad: 5287.2480  LR: 0.000014  \n",
      "Epoch: [2][2700/2850] Elapsed 15m 6s (remain 0m 50s) Loss: 0.0039(0.0029) Grad: 8095.2134  LR: 0.000014  \n",
      "Epoch: [2][2800/2850] Elapsed 15m 39s (remain 0m 16s) Loss: 0.0117(0.0029) Grad: 37151.3828  LR: 0.000013  \n",
      "Epoch: [2][2849/2850] Elapsed 15m 55s (remain 0m 0s) Loss: 0.0039(0.0029) Grad: 11090.7109  LR: 0.000013  \n",
      "EVAL: [0/725] Elapsed 0m 0s (remain 5m 14s) Loss: 0.0025(0.0025) \n",
      "EVAL: [100/725] Elapsed 0m 20s (remain 2m 7s) Loss: 0.0004(0.0022) \n",
      "EVAL: [200/725] Elapsed 0m 40s (remain 1m 46s) Loss: 0.0023(0.0030) \n",
      "EVAL: [300/725] Elapsed 1m 1s (remain 1m 27s) Loss: 0.0025(0.0026) \n",
      "EVAL: [400/725] Elapsed 1m 23s (remain 1m 7s) Loss: 0.0209(0.0029) \n",
      "EVAL: [500/725] Elapsed 1m 44s (remain 0m 46s) Loss: 0.0018(0.0029) \n",
      "EVAL: [600/725] Elapsed 2m 5s (remain 0m 25s) Loss: 0.0007(0.0029) \n",
      "EVAL: [700/725] Elapsed 2m 25s (remain 0m 4s) Loss: 0.0001(0.0027) \n",
      "EVAL: [724/725] Elapsed 2m 30s (remain 0m 0s) Loss: 0.0062(0.0027) \n",
      "Epoch 2 - avg_train_loss: 0.0029  avg_val_loss: 0.0027  time: 1111s\n",
      "Epoch 2 - Score: 0.8342\n",
      "Epoch 2 - Save Best Score: 0.8342 Model\n",
      "Epoch: [3][0/2850] Elapsed 0m 0s (remain 27m 50s) Loss: 0.0063(0.0063) Grad: 47730.3672  LR: 0.000013  \n",
      "Epoch: [3][100/2850] Elapsed 0m 33s (remain 15m 6s) Loss: 0.0127(0.0023) Grad: 44562.3594  LR: 0.000013  \n",
      "Epoch: [3][200/2850] Elapsed 1m 6s (remain 14m 40s) Loss: 0.0002(0.0020) Grad: 1669.7839  LR: 0.000013  \n",
      "Epoch: [3][300/2850] Elapsed 1m 40s (remain 14m 7s) Loss: 0.0035(0.0021) Grad: 15320.7246  LR: 0.000013  \n",
      "Epoch: [3][400/2850] Elapsed 2m 12s (remain 13m 31s) Loss: 0.0024(0.0022) Grad: 13786.9395  LR: 0.000013  \n",
      "Epoch: [3][500/2850] Elapsed 2m 46s (remain 12m 59s) Loss: 0.0004(0.0024) Grad: 3090.4993  LR: 0.000013  \n",
      "Epoch: [3][600/2850] Elapsed 3m 19s (remain 12m 28s) Loss: 0.0005(0.0023) Grad: 3167.8813  LR: 0.000012  \n",
      "Epoch: [3][700/2850] Elapsed 3m 53s (remain 11m 55s) Loss: 0.0003(0.0023) Grad: 1810.8702  LR: 0.000012  \n",
      "Epoch: [3][800/2850] Elapsed 4m 26s (remain 11m 21s) Loss: 0.0246(0.0023) Grad: 70406.5391  LR: 0.000012  \n",
      "Epoch: [3][900/2850] Elapsed 4m 59s (remain 10m 47s) Loss: 0.0026(0.0023) Grad: 9866.4619  LR: 0.000012  \n",
      "Epoch: [3][1000/2850] Elapsed 5m 32s (remain 10m 14s) Loss: 0.0058(0.0022) Grad: 132801.8125  LR: 0.000012  \n",
      "Epoch: [3][1100/2850] Elapsed 6m 5s (remain 9m 41s) Loss: 0.0025(0.0022) Grad: 31431.7910  LR: 0.000012  \n",
      "Epoch: [3][1200/2850] Elapsed 6m 38s (remain 9m 7s) Loss: 0.0016(0.0022) Grad: 7376.1899  LR: 0.000011  \n",
      "Epoch: [3][1300/2850] Elapsed 7m 11s (remain 8m 33s) Loss: 0.0020(0.0022) Grad: 46404.1602  LR: 0.000011  \n",
      "Epoch: [3][1400/2850] Elapsed 7m 44s (remain 8m 0s) Loss: 0.0005(0.0022) Grad: 6768.2891  LR: 0.000011  \n",
      "Epoch: [3][1500/2850] Elapsed 8m 17s (remain 7m 27s) Loss: 0.0021(0.0022) Grad: 8963.3867  LR: 0.000011  \n",
      "Epoch: [3][1600/2850] Elapsed 8m 50s (remain 6m 53s) Loss: 0.0251(0.0022) Grad: 59234.0234  LR: 0.000011  \n",
      "Epoch: [3][1700/2850] Elapsed 9m 22s (remain 6m 20s) Loss: 0.0055(0.0022) Grad: 26098.8438  LR: 0.000011  \n",
      "Epoch: [3][1800/2850] Elapsed 9m 57s (remain 5m 48s) Loss: 0.0014(0.0022) Grad: 8285.1719  LR: 0.000011  \n",
      "Epoch: [3][1900/2850] Elapsed 10m 31s (remain 5m 15s) Loss: 0.0022(0.0022) Grad: 34675.7305  LR: 0.000010  \n",
      "Epoch: [3][2000/2850] Elapsed 11m 3s (remain 4m 41s) Loss: 0.0000(0.0022) Grad: 216.5417  LR: 0.000010  \n",
      "Epoch: [3][2100/2850] Elapsed 11m 36s (remain 4m 8s) Loss: 0.0017(0.0022) Grad: 19612.6680  LR: 0.000010  \n",
      "Epoch: [3][2200/2850] Elapsed 12m 9s (remain 3m 35s) Loss: 0.0002(0.0022) Grad: 1587.4215  LR: 0.000010  \n",
      "Epoch: [3][2300/2850] Elapsed 12m 42s (remain 3m 1s) Loss: 0.0002(0.0022) Grad: 1592.2037  LR: 0.000010  \n",
      "Epoch: [3][2400/2850] Elapsed 13m 18s (remain 2m 29s) Loss: 0.0000(0.0022) Grad: 74.7690  LR: 0.000010  \n",
      "Epoch: [3][2500/2850] Elapsed 13m 51s (remain 1m 56s) Loss: 0.0042(0.0022) Grad: 13781.6260  LR: 0.000009  \n",
      "Epoch: [3][2600/2850] Elapsed 14m 24s (remain 1m 22s) Loss: 0.0017(0.0022) Grad: 9842.5195  LR: 0.000009  \n",
      "Epoch: [3][2700/2850] Elapsed 14m 57s (remain 0m 49s) Loss: 0.0002(0.0022) Grad: 537.3154  LR: 0.000009  \n",
      "Epoch: [3][2800/2850] Elapsed 15m 31s (remain 0m 16s) Loss: 0.0002(0.0022) Grad: 707.6503  LR: 0.000009  \n",
      "Epoch: [3][2849/2850] Elapsed 15m 47s (remain 0m 0s) Loss: 0.0035(0.0022) Grad: 13873.2979  LR: 0.000009  \n",
      "EVAL: [0/725] Elapsed 0m 0s (remain 5m 14s) Loss: 0.0015(0.0015) \n",
      "EVAL: [100/725] Elapsed 0m 21s (remain 2m 13s) Loss: 0.0002(0.0021) \n",
      "EVAL: [200/725] Elapsed 0m 42s (remain 1m 51s) Loss: 0.0022(0.0028) \n",
      "EVAL: [300/725] Elapsed 1m 2s (remain 1m 28s) Loss: 0.0007(0.0024) \n",
      "EVAL: [400/725] Elapsed 1m 22s (remain 1m 7s) Loss: 0.0208(0.0026) \n",
      "EVAL: [500/725] Elapsed 1m 42s (remain 0m 46s) Loss: 0.0015(0.0026) \n",
      "EVAL: [600/725] Elapsed 2m 3s (remain 0m 25s) Loss: 0.0021(0.0026) \n",
      "EVAL: [700/725] Elapsed 2m 24s (remain 0m 4s) Loss: 0.0001(0.0024) \n",
      "EVAL: [724/725] Elapsed 2m 29s (remain 0m 0s) Loss: 0.0043(0.0024) \n",
      "Epoch 3 - avg_train_loss: 0.0022  avg_val_loss: 0.0024  time: 1103s\n",
      "Epoch 3 - Score: 0.8574\n",
      "Epoch 3 - Save Best Score: 0.8574 Model\n",
      "Epoch: [4][0/2850] Elapsed 0m 0s (remain 27m 4s) Loss: 0.0013(0.0013) Grad: 6406.1660  LR: 0.000009  \n",
      "Epoch: [4][100/2850] Elapsed 0m 33s (remain 15m 1s) Loss: 0.0002(0.0017) Grad: 976.7197  LR: 0.000009  \n",
      "Epoch: [4][200/2850] Elapsed 1m 5s (remain 14m 24s) Loss: 0.0003(0.0020) Grad: 2889.3889  LR: 0.000009  \n",
      "Epoch: [4][300/2850] Elapsed 1m 38s (remain 13m 55s) Loss: 0.0006(0.0019) Grad: 5070.2666  LR: 0.000008  \n",
      "Epoch: [4][400/2850] Elapsed 2m 11s (remain 13m 25s) Loss: 0.0007(0.0019) Grad: 3806.7888  LR: 0.000008  \n",
      "Epoch: [4][500/2850] Elapsed 2m 44s (remain 12m 52s) Loss: 0.0008(0.0019) Grad: 9314.5977  LR: 0.000008  \n",
      "Epoch: [4][600/2850] Elapsed 3m 17s (remain 12m 19s) Loss: 0.0221(0.0019) Grad: 47728.3789  LR: 0.000008  \n",
      "Epoch: [4][700/2850] Elapsed 3m 50s (remain 11m 46s) Loss: 0.0003(0.0019) Grad: 2702.3511  LR: 0.000008  \n",
      "Epoch: [4][800/2850] Elapsed 4m 25s (remain 11m 18s) Loss: 0.0114(0.0019) Grad: 47731.1406  LR: 0.000008  \n",
      "Epoch: [4][900/2850] Elapsed 4m 59s (remain 10m 47s) Loss: 0.0000(0.0019) Grad: 239.5679  LR: 0.000007  \n",
      "Epoch: [4][1000/2850] Elapsed 5m 31s (remain 10m 12s) Loss: 0.0006(0.0018) Grad: 4410.4775  LR: 0.000007  \n",
      "Epoch: [4][1100/2850] Elapsed 6m 4s (remain 9m 38s) Loss: 0.0000(0.0018) Grad: 726.8919  LR: 0.000007  \n",
      "Epoch: [4][1200/2850] Elapsed 6m 36s (remain 9m 5s) Loss: 0.0000(0.0018) Grad: 125.8970  LR: 0.000007  \n",
      "Epoch: [4][1300/2850] Elapsed 7m 10s (remain 8m 32s) Loss: 0.0001(0.0018) Grad: 1076.3176  LR: 0.000007  \n",
      "Epoch: [4][1400/2850] Elapsed 7m 43s (remain 7m 59s) Loss: 0.0058(0.0018) Grad: 18350.9512  LR: 0.000007  \n",
      "Epoch: [4][1500/2850] Elapsed 8m 18s (remain 7m 27s) Loss: 0.0001(0.0018) Grad: 697.5448  LR: 0.000007  \n",
      "Epoch: [4][1600/2850] Elapsed 8m 50s (remain 6m 54s) Loss: 0.0037(0.0019) Grad: 15462.4834  LR: 0.000006  \n",
      "Epoch: [4][1700/2850] Elapsed 9m 23s (remain 6m 20s) Loss: 0.0000(0.0018) Grad: 29.5189  LR: 0.000006  \n",
      "Epoch: [4][1800/2850] Elapsed 9m 56s (remain 5m 47s) Loss: 0.0001(0.0019) Grad: 1353.7490  LR: 0.000006  \n",
      "Epoch: [4][1900/2850] Elapsed 10m 30s (remain 5m 14s) Loss: 0.0042(0.0019) Grad: 43353.5664  LR: 0.000006  \n",
      "Epoch: [4][2000/2850] Elapsed 11m 2s (remain 4m 41s) Loss: 0.0019(0.0019) Grad: 39268.4727  LR: 0.000006  \n",
      "Epoch: [4][2100/2850] Elapsed 11m 37s (remain 4m 8s) Loss: 0.0012(0.0019) Grad: 10222.5156  LR: 0.000006  \n",
      "Epoch: [4][2200/2850] Elapsed 12m 10s (remain 3m 35s) Loss: 0.0002(0.0019) Grad: 3030.7693  LR: 0.000005  \n",
      "Epoch: [4][2300/2850] Elapsed 12m 42s (remain 3m 2s) Loss: 0.0007(0.0019) Grad: 7764.4351  LR: 0.000005  \n",
      "Epoch: [4][2400/2850] Elapsed 13m 16s (remain 2m 28s) Loss: 0.0043(0.0019) Grad: 39550.7070  LR: 0.000005  \n",
      "Epoch: [4][2500/2850] Elapsed 13m 50s (remain 1m 55s) Loss: 0.0023(0.0019) Grad: 9269.9434  LR: 0.000005  \n",
      "Epoch: [4][2600/2850] Elapsed 14m 23s (remain 1m 22s) Loss: 0.0001(0.0019) Grad: 780.8458  LR: 0.000005  \n",
      "Epoch: [4][2700/2850] Elapsed 14m 56s (remain 0m 49s) Loss: 0.0001(0.0019) Grad: 912.3071  LR: 0.000005  \n",
      "Epoch: [4][2800/2850] Elapsed 15m 28s (remain 0m 16s) Loss: 0.0004(0.0019) Grad: 4361.1289  LR: 0.000005  \n",
      "Epoch: [4][2849/2850] Elapsed 15m 45s (remain 0m 0s) Loss: 0.0021(0.0019) Grad: 15980.9561  LR: 0.000004  \n",
      "EVAL: [0/725] Elapsed 0m 0s (remain 4m 56s) Loss: 0.0033(0.0033) \n",
      "EVAL: [100/725] Elapsed 0m 20s (remain 2m 7s) Loss: 0.0005(0.0020) \n",
      "EVAL: [200/725] Elapsed 0m 41s (remain 1m 47s) Loss: 0.0016(0.0028) \n",
      "EVAL: [300/725] Elapsed 1m 1s (remain 1m 26s) Loss: 0.0004(0.0025) \n",
      "EVAL: [400/725] Elapsed 1m 21s (remain 1m 5s) Loss: 0.0189(0.0027) \n",
      "EVAL: [500/725] Elapsed 1m 41s (remain 0m 45s) Loss: 0.0020(0.0028) \n",
      "EVAL: [600/725] Elapsed 2m 2s (remain 0m 25s) Loss: 0.0019(0.0027) \n",
      "EVAL: [700/725] Elapsed 2m 23s (remain 0m 4s) Loss: 0.0001(0.0025) \n",
      "EVAL: [724/725] Elapsed 2m 28s (remain 0m 0s) Loss: 0.0060(0.0025) \n",
      "Epoch 4 - avg_train_loss: 0.0019  avg_val_loss: 0.0025  time: 1099s\n",
      "Epoch 4 - Score: 0.8699\n",
      "Epoch 4 - Save Best Score: 0.8699 Model\n",
      "Epoch: [5][0/2850] Elapsed 0m 0s (remain 28m 1s) Loss: 0.0078(0.0078) Grad: 29465.2656  LR: 0.000004  \n",
      "Epoch: [5][100/2850] Elapsed 0m 33s (remain 15m 5s) Loss: 0.0007(0.0010) Grad: 5120.3335  LR: 0.000004  \n",
      "Epoch: [5][200/2850] Elapsed 1m 6s (remain 14m 30s) Loss: 0.0000(0.0010) Grad: 166.8313  LR: 0.000004  \n",
      "Epoch: [5][300/2850] Elapsed 1m 39s (remain 14m 0s) Loss: 0.0002(0.0012) Grad: 2840.6875  LR: 0.000004  \n",
      "Epoch: [5][400/2850] Elapsed 2m 11s (remain 13m 25s) Loss: 0.0003(0.0013) Grad: 16120.1543  LR: 0.000004  \n",
      "Epoch: [5][500/2850] Elapsed 2m 44s (remain 12m 52s) Loss: 0.0030(0.0014) Grad: 17065.3730  LR: 0.000004  \n",
      "Epoch: [5][600/2850] Elapsed 3m 17s (remain 12m 20s) Loss: 0.0001(0.0015) Grad: 889.3694  LR: 0.000004  \n",
      "Epoch: [5][700/2850] Elapsed 3m 51s (remain 11m 48s) Loss: 0.0099(0.0015) Grad: 52054.0820  LR: 0.000003  \n",
      "Epoch: [5][800/2850] Elapsed 4m 27s (remain 11m 23s) Loss: 0.0004(0.0016) Grad: 4889.8389  LR: 0.000003  \n",
      "Epoch: [5][900/2850] Elapsed 5m 1s (remain 10m 51s) Loss: 0.0018(0.0015) Grad: 22051.2656  LR: 0.000003  \n",
      "Epoch: [5][1000/2850] Elapsed 5m 34s (remain 10m 18s) Loss: 0.0000(0.0016) Grad: 92.9286  LR: 0.000003  \n",
      "Epoch: [5][1100/2850] Elapsed 6m 8s (remain 9m 44s) Loss: 0.0001(0.0016) Grad: 1703.0925  LR: 0.000003  \n",
      "Epoch: [5][1200/2850] Elapsed 6m 41s (remain 9m 11s) Loss: 0.0001(0.0016) Grad: 1341.2368  LR: 0.000003  \n",
      "Epoch: [5][1300/2850] Elapsed 7m 14s (remain 8m 37s) Loss: 0.0011(0.0016) Grad: 5793.0820  LR: 0.000002  \n",
      "Epoch: [5][1400/2850] Elapsed 7m 47s (remain 8m 3s) Loss: 0.0000(0.0016) Grad: 360.7621  LR: 0.000002  \n",
      "Epoch: [5][1500/2850] Elapsed 8m 21s (remain 7m 30s) Loss: 0.0007(0.0016) Grad: 6538.2036  LR: 0.000002  \n",
      "Epoch: [5][1600/2850] Elapsed 8m 54s (remain 6m 57s) Loss: 0.0006(0.0016) Grad: 5919.2920  LR: 0.000002  \n",
      "Epoch: [5][1700/2850] Elapsed 9m 27s (remain 6m 23s) Loss: 0.0021(0.0016) Grad: 6726.2910  LR: 0.000002  \n",
      "Epoch: [5][1800/2850] Elapsed 9m 59s (remain 5m 49s) Loss: 0.0051(0.0016) Grad: 7406.3560  LR: 0.000002  \n",
      "Epoch: [5][1900/2850] Elapsed 10m 32s (remain 5m 15s) Loss: 0.0000(0.0016) Grad: 21.5130  LR: 0.000001  \n",
      "Epoch: [5][2000/2850] Elapsed 11m 5s (remain 4m 42s) Loss: 0.0003(0.0016) Grad: 1108.7101  LR: 0.000001  \n",
      "Epoch: [5][2100/2850] Elapsed 11m 38s (remain 4m 9s) Loss: 0.0000(0.0016) Grad: 97.8216  LR: 0.000001  \n",
      "Epoch: [5][2200/2850] Elapsed 12m 11s (remain 3m 35s) Loss: 0.0000(0.0016) Grad: 17.8569  LR: 0.000001  \n",
      "Epoch: [5][2300/2850] Elapsed 12m 43s (remain 3m 2s) Loss: 0.0026(0.0016) Grad: 3587.1050  LR: 0.000001  \n",
      "Epoch: [5][2400/2850] Elapsed 13m 16s (remain 2m 28s) Loss: 0.0102(0.0016) Grad: 26433.5078  LR: 0.000001  \n",
      "Epoch: [5][2500/2850] Elapsed 13m 48s (remain 1m 55s) Loss: 0.0030(0.0016) Grad: 5443.9849  LR: 0.000001  \n",
      "Epoch: [5][2600/2850] Elapsed 14m 21s (remain 1m 22s) Loss: 0.0000(0.0016) Grad: 29.7278  LR: 0.000000  \n",
      "Epoch: [5][2700/2850] Elapsed 14m 53s (remain 0m 49s) Loss: 0.0002(0.0016) Grad: 396.9910  LR: 0.000000  \n",
      "Epoch: [5][2800/2850] Elapsed 15m 26s (remain 0m 16s) Loss: 0.0007(0.0016) Grad: 1496.4186  LR: 0.000000  \n",
      "Epoch: [5][2849/2850] Elapsed 15m 42s (remain 0m 0s) Loss: 0.0001(0.0016) Grad: 819.2026  LR: 0.000000  \n",
      "EVAL: [0/725] Elapsed 0m 0s (remain 4m 57s) Loss: 0.0025(0.0025) \n",
      "EVAL: [100/725] Elapsed 0m 20s (remain 2m 9s) Loss: 0.0005(0.0020) \n",
      "EVAL: [200/725] Elapsed 0m 41s (remain 1m 47s) Loss: 0.0018(0.0028) \n",
      "EVAL: [300/725] Elapsed 1m 1s (remain 1m 26s) Loss: 0.0003(0.0025) \n",
      "EVAL: [400/725] Elapsed 1m 21s (remain 1m 6s) Loss: 0.0169(0.0027) \n",
      "EVAL: [500/725] Elapsed 1m 42s (remain 0m 45s) Loss: 0.0021(0.0027) \n",
      "EVAL: [600/725] Elapsed 2m 3s (remain 0m 25s) Loss: 0.0024(0.0027) \n",
      "EVAL: [700/725] Elapsed 2m 23s (remain 0m 4s) Loss: 0.0001(0.0025) \n",
      "EVAL: [724/725] Elapsed 2m 28s (remain 0m 0s) Loss: 0.0061(0.0025) \n",
      "Epoch 5 - avg_train_loss: 0.0016  avg_val_loss: 0.0025  time: 1096s\n",
      "Epoch 5 - Score: 0.8735\n",
      "Epoch 5 - Save Best Score: 0.8735 Model\n",
      "Best thres: 0.5, Score: 0.8693\n",
      "Best thres: 0.5023437500000001, Score: 0.8693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CustomModel:\n\tMissing key(s) in state_dict: \"backbone.pooler.dense.weight\", \"backbone.pooler.dense.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-0f687401855c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mtest_token_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"fold{i_fold}_{i}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_token_probs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1052\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CustomModel:\n\tMissing key(s) in state_dict: \"backbone.pooler.dense.weight\", \"backbone.pooler.dense.bias\". "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "name": "nbme-exp011.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 641.572862,
   "end_time": "2022-02-27T11:39:50.972497",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-27T11:29:09.399635",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0260998578564385a0b5b9425a0a5ca1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1141ae38bc6f473aab89db14fa4eeacf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad49cbf6b6e84ccaab873458182f22a1",
      "placeholder": "​",
      "style": "IPY_MODEL_5375de82ce3a41a8b5550e0a6b4316c1",
      "value": " 42146/42146 [00:36&lt;00:00, 2019.05it/s]"
     }
    },
    "220f78b6119042af8729543465e1234e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "25bf78e432e641e0a435dc3626c3ee8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2e32fee744ef42e0aaa89a7b03e82427": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e3818222bab4603a896be5976cb8409": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9754a5f1e61d49c8972df40ee9290375",
      "placeholder": "​",
      "style": "IPY_MODEL_25bf78e432e641e0a435dc3626c3ee8a",
      "value": " 143/143 [00:00&lt;00:00, 2166.88it/s]"
     }
    },
    "40e6583408c447199ff5b94d23601936": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e32fee744ef42e0aaa89a7b03e82427",
      "max": 42146,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d51d3aa414db4aa8b0ccae896e671152",
      "value": 42146
     }
    },
    "428ca357bd284d199e2558b1f577d79a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "47b8a7f3d0544d79b30ad02e4222082e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5375de82ce3a41a8b5550e0a6b4316c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e66444e9c714134bd2765cb3b6d1f15": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b04b019813e458080f02bc9111433a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f1d7796e2174485a0d1b1e9a71d7ade",
      "max": 143,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e72cad76f875451a8e2479e2df237575",
      "value": 143
     }
    },
    "7f1d7796e2174485a0d1b1e9a71d7ade": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a503d1abd884514a1e23101e03c6781": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e06e5e9eb0414b6fad63bdc99b44a313",
       "IPY_MODEL_6b04b019813e458080f02bc9111433a6",
       "IPY_MODEL_2e3818222bab4603a896be5976cb8409"
      ],
      "layout": "IPY_MODEL_eeb468dbb94943fcb30219d4dd98fcab"
     }
    },
    "9754a5f1e61d49c8972df40ee9290375": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a31c60ff4dab48e08d2ef9293d85df6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c40d970496ff447a8c0b80d787b07a4d",
       "IPY_MODEL_40e6583408c447199ff5b94d23601936",
       "IPY_MODEL_1141ae38bc6f473aab89db14fa4eeacf"
      ],
      "layout": "IPY_MODEL_47b8a7f3d0544d79b30ad02e4222082e"
     }
    },
    "ad49cbf6b6e84ccaab873458182f22a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c40d970496ff447a8c0b80d787b07a4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0260998578564385a0b5b9425a0a5ca1",
      "placeholder": "​",
      "style": "IPY_MODEL_428ca357bd284d199e2558b1f577d79a",
      "value": "100%"
     }
    },
    "d51d3aa414db4aa8b0ccae896e671152": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e06e5e9eb0414b6fad63bdc99b44a313": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e66444e9c714134bd2765cb3b6d1f15",
      "placeholder": "​",
      "style": "IPY_MODEL_220f78b6119042af8729543465e1234e",
      "value": "100%"
     }
    },
    "e72cad76f875451a8e2479e2df237575": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eeb468dbb94943fcb30219d4dd98fcab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
