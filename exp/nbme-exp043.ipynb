{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "colored-security",
   "metadata": {
    "id": "blind-kingdom"
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-operator",
   "metadata": {
    "id": "antique-glenn"
   },
   "source": [
    "- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-greek",
   "metadata": {
    "id": "bored-ministry"
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alive-granny",
   "metadata": {
    "id": "deadly-confidence"
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"nbme-exp043\"\n",
    "ENV = \"local\"\n",
    "DEBUG_MODE = False\n",
    "SUBMISSION_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "heavy-prophet",
   "metadata": {
    "id": "aware-worcester"
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    env=ENV\n",
    "    exp_name=EXP_NAME\n",
    "    debug=DEBUG_MODE\n",
    "    submission=SUBMISSION_MODE\n",
    "    apex=True\n",
    "    input_dir=None\n",
    "    output_dir=None\n",
    "    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n",
    "    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n",
    "    competition_name=\"nbme-score-clinical-patient-notes\"\n",
    "    id_col=\"id\"\n",
    "    target_col=\"location\"\n",
    "    pretrained_model_name=\"microsoft/deberta-large\"\n",
    "    tokenizer=None\n",
    "    max_len=None\n",
    "    output_dim=1\n",
    "    dropout=0.2\n",
    "    num_workers=4\n",
    "    batch_size=4\n",
    "    lr=2e-5\n",
    "    betas=(0.9, 0.98)\n",
    "    weight_decay=0.1\n",
    "    num_warmup_steps_rate=0.1\n",
    "    batch_scheduler=True\n",
    "    epochs=5\n",
    "    n_fold=4\n",
    "    train_fold=[0, 1, 2, 3]\n",
    "    seed=71\n",
    "    gradient_accumulation_steps=2\n",
    "    max_grad_norm=1000\n",
    "    print_freq=100\n",
    "    train=True\n",
    "    inference=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "vocational-coating",
   "metadata": {
    "id": "personalized-death"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.train_fold = [0, 1]\n",
    "\n",
    "if CFG.submission:\n",
    "    CFG.train = False\n",
    "    CFG.inference = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-moderator",
   "metadata": {
    "id": "cardiovascular-neutral"
   },
   "source": [
    "## Directory Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "married-tokyo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "checked-boards",
    "outputId": "cea3ff7c-73cb-479b-82dc-d4ad1b7d6719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "print(CFG.env)\n",
    "if CFG.env == \"colab\":\n",
    "    # colab環境\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "    # install packages\n",
    "    !pip install transformers\n",
    "\n",
    "elif CFG.env == \"local\":\n",
    "    # ローカルサーバ\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "\n",
    "elif CFG.env == \"kaggle\":\n",
    "    # kaggle環境\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "blank-pierre",
   "metadata": {
    "id": "vital-mexico"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import BartModel,BertModel,BertTokenizer\n",
    "from transformers import DebertaModel,DebertaTokenizer\n",
    "from transformers import RobertaModel,RobertaTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-still",
   "metadata": {
    "id": "economic-ladder"
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "surprised-commercial",
   "metadata": {
    "id": "desperate-keyboard"
   },
   "outputs": [],
   "source": [
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "interstate-accident",
   "metadata": {
    "id": "flexible-wednesday"
   },
   "outputs": [],
   "source": [
    "def create_labels_for_scoring(df):\n",
    "    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n",
    "    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, \"location\"]\n",
    "        if lst:\n",
    "            new_lst = \";\".join(lst)\n",
    "            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n",
    "\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df[\"location_for_create_labels\"].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, token_probs, tokenizer):\n",
    "    res = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n",
    "        encoded = tokenizer(\n",
    "            text=text,\n",
    "            max_length=CFG.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n",
    "            start, end = offset_mapping\n",
    "            res[i][start:end] = pred\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_predicted_location_str(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def scoring(df, th=0.5):\n",
    "    labels = create_labels_for_scoring(df)\n",
    "\n",
    "    token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n",
    "    char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n",
    "    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n",
    "    preds = get_predictions(predicted_location_str)\n",
    "\n",
    "    score = get_score(labels, preds)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_best_thres(oof_df):\n",
    "    def f1_opt(x):\n",
    "        return -1 * scoring(oof_df, th=x)\n",
    "\n",
    "    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n",
    "    return best_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "coated-pioneer",
   "metadata": {
    "id": "logical-chemistry"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "nervous-delaware",
   "metadata": {
    "id": "gorgeous-record"
   },
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-destruction",
   "metadata": {
    "id": "frozen-africa"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "global-monte",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "shaped-metallic",
    "outputId": "c2e09677-e2ae-4b51-b411-f1a0634026d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 6), (143, 3), (42146, 3), (5, 4))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(CFG.input_dir / \"train.csv\")\n",
    "features = pd.read_csv(CFG.input_dir / \"features.csv\")\n",
    "patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n",
    "test = pd.read_csv(CFG.input_dir / \"test.csv\")\n",
    "\n",
    "train.shape, features.shape, patient_notes.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "independent-airfare",
   "metadata": {
    "id": "visible-australia"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n",
    "    print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-locator",
   "metadata": {
    "id": "hydraulic-gibson"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unusual-fifty",
   "metadata": {
    "id": "interpreted-northeast"
   },
   "outputs": [],
   "source": [
    "def preprocess_features(features):\n",
    "    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "\n",
    "\n",
    "features = preprocess_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "decreased-mustang",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "martial-blind",
    "outputId": "5e7f2195-15c6-4e13-fc35-359b478f2af2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 8), (5, 6))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "boolean-trade",
   "metadata": {
    "id": "electoral-favor"
   },
   "outputs": [],
   "source": [
    "train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n",
    "train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "accomplished-dakota",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "reported-parade",
    "outputId": "bf29b91f-33b5-4aea-ce84-2e4fb8e9bf40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4399\n",
       "1    8181\n",
       "2    1296\n",
       "3     287\n",
       "4      99\n",
       "5      27\n",
       "6       9\n",
       "7       1\n",
       "8       1\n",
       "Name: annotation_length, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n",
    "display(train['annotation_length'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-elizabeth",
   "metadata": {
    "id": "enabling-relevance"
   },
   "source": [
    "## CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "unexpected-columbia",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "mature-coalition",
    "outputId": "ed39702e-822c-408e-ad92-eccd51f61c97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    3575\n",
       "1    3575\n",
       "2    3575\n",
       "3    3575\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Fold = GroupKFold(n_splits=CFG.n_fold)\n",
    "groups = train['pn_num'].values\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-archive",
   "metadata": {
    "id": "subjective-entrance"
   },
   "source": [
    "## Setup tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "broken-generator",
   "metadata": {
    "id": "dramatic-afghanistan"
   },
   "outputs": [],
   "source": [
    "if CFG.submission:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG.pretrained_model_name)\n",
    "    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n",
    "\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-lincoln",
   "metadata": {
    "id": "divided-arrow"
   },
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fluid-nancy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "8c7eb508782d4253af8cbff0a39e5d19",
      "33e4f48f7c8f40f48081c34bc992f215",
      "0c70395dca6341349fc883dc6b95090a",
      "0dfe4d3aa0354d95bb5d019420b510a9",
      "e6775e6fc2ad4b14b473b8a07e27426d",
      "40ae5d1c9e9f4feaa4207599ef17a1ec",
      "8cd3352e5e9342e4a814e9958ea6dc1d",
      "5a92171cb3d34fc8b1ed5119e32951ac",
      "a202501b76a748fe80180604dff32c7b",
      "c3d9ac191d9b4772ae4bca323a34ddd7",
      "9deae6afbd4740df8eb0289bc5f53ae7"
     ]
    },
    "id": "immune-campbell",
    "outputId": "dbda74a8-5a58-49cf-b86a-3e05c265f758"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10c87985d674348867e327520180f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 433\n"
     ]
    }
   ],
   "source": [
    "pn_history_lengths = []\n",
    "tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    pn_history_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(pn_history_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "posted-humidity",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "47e8d82f628f4bd7b8b0ef0aa96852a5",
      "cc9a4ba21d664dafb8ae32a4e0a1c5e0",
      "f4421d3b3a844dbcb003f11902ee1898",
      "d02f186539954463873bb560b775894e",
      "14efac00edd349898e9fa95a63a2773d",
      "83d1b90076dc431893e0ab1c87e35f9c",
      "0b503e832e57492291cbf6e9ae66e343",
      "625abc68d2fb4fd4b8556c7cc1ae514a",
      "d2581946e9fd4f9a8dc56b3453cc70bd",
      "1f6c8df95c7845818253189f2e365ba9",
      "5e29db6be6284caa978ee223047f23c5"
     ]
    },
    "id": "northern-branch",
    "outputId": "e2c62ebe-aae9-413d-d424-6850759224a2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b027a622289e4105b21af37d6f54dad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 30\n"
     ]
    }
   ],
   "source": [
    "feature_text_lengths = []\n",
    "tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    feature_text_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(feature_text_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "resistant-amount",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oriental-jacksonville",
    "outputId": "293b5e4d-a369-433b-a5af-fbfd35f411a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 466\n"
     ]
    }
   ],
   "source": [
    "CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n",
    "\n",
    "print(\"max length:\", CFG.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "august-equity",
   "metadata": {
    "id": "flexible-trainer"
   },
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "        self.annotation_lengths = self.df[\"annotation_length\"].values\n",
    "        self.locations = self.df[\"location\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def _create_label(self, pn_history, annotation_length, location_list):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        offset_mapping = encoded[\"offset_mapping\"]\n",
    "        ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n",
    "        label = np.zeros(len(offset_mapping))\n",
    "        label[ignore_idxes] = -1\n",
    "\n",
    "        if annotation_length > 0:\n",
    "            for location in location_list:\n",
    "                for loc in [s.split() for s in location.split(\";\")]:\n",
    "                    start, end = int(loc[0]), int(loc[1])\n",
    "                    start_idx = -1\n",
    "                    end_idx = -1\n",
    "                    for idx in range(len(offset_mapping)):\n",
    "                        if (start_idx == -1) & (start < offset_mapping[idx][0]):\n",
    "                            start_idx = idx - 1\n",
    "                        if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n",
    "                            end_idx = idx + 1\n",
    "                    if start_idx == -1:\n",
    "                        start_idx = end_idx\n",
    "                    if (start_idx != -1) & (end_idx != -1):\n",
    "                        label[start_idx:end_idx] = 1\n",
    "\n",
    "        return torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n",
    "        return input_, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "weird-interaction",
   "metadata": {
    "id": "stock-robertson"
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        return input_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-mobility",
   "metadata": {
    "id": "chemical-lucas"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "spanish-destruction",
   "metadata": {
    "id": "animated-array"
   },
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, model_config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if model_config_path is None:\n",
    "            self.model_config = AutoConfig.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "        else:\n",
    "            self.model_config = torch.load(model_config_path)\n",
    "\n",
    "        if pretrained:\n",
    "            self.backbone = AutoModel.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                config=self.model_config,\n",
    "            )\n",
    "            print(f\"Load weight from pretrained\")\n",
    "        else:\n",
    "            #self.backbone = AutoModel.from_config(self.model_config)\n",
    "            itpt = AutoModelForMaskedLM.from_config(self.model_config)\n",
    "            #path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp010/checkpoint-130170/pytorch_model.bin\")\n",
    "            path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n",
    "            state_dict = torch.load(path)\n",
    "            itpt.load_state_dict(state_dict)\n",
    "            self.backbone = itpt.deberta\n",
    "            print(f\"Load weight from {path}\")\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(self.cfg.dropout),\n",
    "            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h = self.backbone(**inputs)[\"last_hidden_state\"]\n",
    "        output = self.fc(h)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-bullet",
   "metadata": {
    "id": "thorough-bristol"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "biological-hunger",
   "metadata": {
    "id": "talented-quantity"
   },
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    train_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    epoch,\n",
    "    scheduler,\n",
    "    device,\n",
    "):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels) in enumerate(train_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            output = model(inputs)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "\n",
    "        pos_nums = (labels == 1).sum(axis=1)\n",
    "        pos_nums = torch.tensor([[pos_num] * CFG.max_len for pos_num in pos_nums]).view(-1, 1).to(device)\n",
    "        pos_nums = torch.masked_select(pos_nums, labels.view(-1, 1) != -1)\n",
    "        weight = []\n",
    "        for pos_num in pos_nums:\n",
    "            if pos_num == 0:\n",
    "                weight.append(3.0)\n",
    "            else:\n",
    "                weight.append(1.0)\n",
    "        weight = torch.tensor(weight).to(device)\n",
    "        loss = loss * weight\n",
    "\n",
    "        loss = loss.mean()\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if CFG.batch_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n",
    "            print(\n",
    "                \"Epoch: [{0}][{1}/{2}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                \"Grad: {grad_norm:.4f}  \"\n",
    "                \"LR: {lr:.6f}  \"\n",
    "                .format(\n",
    "                    epoch+1,\n",
    "                    step,\n",
    "                    len(train_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n",
    "                    loss=losses,\n",
    "                     grad_norm=grad_norm,\n",
    "                     lr=scheduler.get_lr()[0],\n",
    "                )\n",
    "            )\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "satisfied-sterling",
   "metadata": {
    "id": "figured-cooperative"
   },
   "outputs": [],
   "source": [
    "def valid_fn(\n",
    "    val_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    device,\n",
    "):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels) in enumerate(val_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "\n",
    "        pos_nums = (labels == 1).sum(axis=1)\n",
    "        pos_nums = torch.tensor([[pos_num] * CFG.max_len for pos_num in pos_nums]).view(-1, 1).to(device)\n",
    "        pos_nums = torch.masked_select(pos_nums, labels.view(-1, 1) != -1)\n",
    "        weight = []\n",
    "        for pos_num in pos_nums:\n",
    "            if pos_num == 0:\n",
    "                weight.append(3.0)\n",
    "            else:\n",
    "                weight.append(1.0)\n",
    "        weight = torch.tensor(weight).to(device)\n",
    "        loss = loss * weight\n",
    "\n",
    "        loss = loss.mean()\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n",
    "            print(\n",
    "                \"EVAL: [{0}/{1}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                .format(\n",
    "                    step, len(val_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n",
    "                    loss=losses,\n",
    "                )\n",
    "            )\n",
    "    preds = np.concatenate(preds)\n",
    "    return losses.avg, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "incorporate-viking",
   "metadata": {
    "id": "played-pointer"
   },
   "outputs": [],
   "source": [
    "def inference_fn(test_dataloader, model, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preds = []\n",
    "    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dental-sunset",
   "metadata": {
    "id": "brazilian-nigeria"
   },
   "outputs": [],
   "source": [
    "def train_loop(df, i_fold, device):\n",
    "    print(f\"========== fold: {i_fold} training ==========\")\n",
    "    train_idx = df[df[\"fold\"] != i_fold].index\n",
    "    val_idx = df[df[\"fold\"] == i_fold].index\n",
    "\n",
    "    train_folds = df.loc[train_idx].reset_index(drop=True)\n",
    "    val_folds = df.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = TrainingDataset(CFG, train_folds)\n",
    "    val_dataset = TrainingDataset(CFG, val_folds)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "    model = CustomModel(CFG, model_config_path=None, pretrained=False)   # itptを使うため\n",
    "    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n",
    "    model.to(device)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in param_optimizer if not any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n",
    "        {\"params\": [p for n, p in param_optimizer if any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=CFG.lr,\n",
    "        betas=CFG.betas,\n",
    "        weight_decay=CFG.weight_decay,\n",
    "    )\n",
    "    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n",
    "    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_train_optimization_steps,\n",
    "    )\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    best_score = -1 * np.inf\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "        avg_loss = train_fn(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            scheduler,\n",
    "            device,\n",
    "        )\n",
    "        avg_val_loss, val_preds = valid_fn(\n",
    "            val_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "\n",
    "        # scoring\n",
    "        val_folds[[str(i) for i in range(CFG.max_len)]] = val_preds\n",
    "        score = scoring(val_folds, th=0.5)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n",
    "        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"predictions\": val_preds,\n",
    "                },\n",
    "                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "            )\n",
    "\n",
    "    predictions = torch.load(\n",
    "        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )[\"predictions\"]\n",
    "    val_folds[[str(i) for i in range(CFG.max_len)]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return val_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-graphics",
   "metadata": {
    "id": "bearing-switch"
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "connected-protein",
   "metadata": {
    "id": "desperate-crime"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for i_fold in range(CFG.n_fold):\n",
    "            if i_fold in CFG.train_fold:\n",
    "                _oof_df = train_loop(train, i_fold, device)\n",
    "                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n",
    "        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    if CFG.submission:\n",
    "        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n",
    "    else:\n",
    "        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    score = scoring(oof_df, th=0.5)\n",
    "    print(f\"Best thres: 0.5, Score: {score:.4f}\")\n",
    "    best_thres = get_best_thres(oof_df)\n",
    "    score = scoring(oof_df, th=best_thres)\n",
    "    print(f\"Best thres: {best_thres}, Score: {score:.4f}\")\n",
    "\n",
    "    if CFG.inference:\n",
    "        test_dataset = TestDataset(CFG, test)\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=CFG.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=CFG.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        predictions = []\n",
    "        for i_fold in CFG.train_fold:\n",
    "            if CFG.submission:\n",
    "                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n",
    "                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n",
    "            else:\n",
    "                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n",
    "\n",
    "            state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "            model.load_state_dict(state[\"model\"])\n",
    "            test_token_probs = inference_fn(test_dataloader, model, device)\n",
    "            test[[f\"fold{i_fold}_{i}\" for i in range(CFG.max_len)]] = test_token_probs\n",
    "            test_char_probs = get_char_probs(test[\"pn_history\"].values, test_token_probs, CFG.tokenizer)\n",
    "            predictions.append(test_char_probs)\n",
    "\n",
    "            del state, test_token_probs, model; gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        predictions = np.mean(predictions, axis=0)\n",
    "        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n",
    "        test[CFG.target_col] = predicted_location_str\n",
    "        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n",
    "        test[[CFG.id_col, CFG.target_col]].to_csv(\n",
    "            CFG.output_dir / \"submission.csv\", index=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "serious-bunny",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "graduate-vision",
    "outputId": "90d56101-1cd3-4eac-8d44-5be254003857"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2681] Elapsed 0m 1s (remain 63m 41s) Loss: 0.7419(0.7419) Grad: inf  LR: 0.000000  \n",
      "Epoch: [1][100/2681] Elapsed 0m 51s (remain 21m 55s) Loss: 0.3288(0.4600) Grad: 27334.1523  LR: 0.000002  \n",
      "Epoch: [1][200/2681] Elapsed 1m 41s (remain 20m 52s) Loss: 0.0591(0.3237) Grad: 1913.1337  LR: 0.000003  \n",
      "Epoch: [1][300/2681] Elapsed 2m 31s (remain 19m 57s) Loss: 0.0310(0.2294) Grad: 499.2654  LR: 0.000004  \n",
      "Epoch: [1][400/2681] Elapsed 3m 21s (remain 19m 8s) Loss: 0.0603(0.1824) Grad: 986.2064  LR: 0.000006  \n",
      "Epoch: [1][500/2681] Elapsed 4m 11s (remain 18m 14s) Loss: 0.0248(0.1535) Grad: 780.0833  LR: 0.000007  \n",
      "Epoch: [1][600/2681] Elapsed 5m 1s (remain 17m 23s) Loss: 0.0124(0.1316) Grad: 2272.1345  LR: 0.000009  \n",
      "Epoch: [1][700/2681] Elapsed 5m 51s (remain 16m 32s) Loss: 0.0059(0.1153) Grad: 520.8360  LR: 0.000010  \n",
      "Epoch: [1][800/2681] Elapsed 6m 41s (remain 15m 43s) Loss: 0.0302(0.1030) Grad: 2121.1750  LR: 0.000012  \n",
      "Epoch: [1][900/2681] Elapsed 7m 32s (remain 14m 53s) Loss: 0.0065(0.0928) Grad: 2256.1858  LR: 0.000013  \n",
      "Epoch: [1][1000/2681] Elapsed 8m 22s (remain 14m 2s) Loss: 0.0250(0.0849) Grad: 4841.2637  LR: 0.000015  \n",
      "Epoch: [1][1100/2681] Elapsed 9m 12s (remain 13m 12s) Loss: 0.0278(0.0782) Grad: 1398.9274  LR: 0.000016  \n",
      "Epoch: [1][1200/2681] Elapsed 10m 2s (remain 12m 22s) Loss: 0.0274(0.0724) Grad: 3228.4167  LR: 0.000018  \n",
      "Epoch: [1][1300/2681] Elapsed 10m 53s (remain 11m 33s) Loss: 0.0354(0.0675) Grad: 1787.7808  LR: 0.000019  \n",
      "Epoch: [1][1400/2681] Elapsed 11m 43s (remain 10m 42s) Loss: 0.0273(0.0635) Grad: 3816.4460  LR: 0.000020  \n",
      "Epoch: [1][1500/2681] Elapsed 12m 33s (remain 9m 52s) Loss: 0.0376(0.0599) Grad: 5774.4375  LR: 0.000020  \n",
      "Epoch: [1][1600/2681] Elapsed 13m 22s (remain 9m 1s) Loss: 0.0074(0.0567) Grad: 3157.7700  LR: 0.000020  \n",
      "Epoch: [1][1700/2681] Elapsed 14m 12s (remain 8m 11s) Loss: 0.0015(0.0540) Grad: 174.1625  LR: 0.000019  \n",
      "Epoch: [1][1800/2681] Elapsed 15m 2s (remain 7m 21s) Loss: 0.0073(0.0515) Grad: 650.8094  LR: 0.000019  \n",
      "Epoch: [1][1900/2681] Elapsed 15m 52s (remain 6m 30s) Loss: 0.0124(0.0494) Grad: 2441.5632  LR: 0.000019  \n",
      "Epoch: [1][2000/2681] Elapsed 16m 42s (remain 5m 40s) Loss: 0.0100(0.0473) Grad: 2947.2295  LR: 0.000019  \n",
      "Epoch: [1][2100/2681] Elapsed 17m 32s (remain 4m 50s) Loss: 0.0066(0.0454) Grad: 898.0516  LR: 0.000019  \n",
      "Epoch: [1][2200/2681] Elapsed 18m 22s (remain 4m 0s) Loss: 0.0014(0.0437) Grad: 346.0220  LR: 0.000019  \n",
      "Epoch: [1][2300/2681] Elapsed 19m 13s (remain 3m 10s) Loss: 0.0036(0.0422) Grad: 338.5392  LR: 0.000018  \n",
      "Epoch: [1][2400/2681] Elapsed 20m 2s (remain 2m 20s) Loss: 0.0037(0.0407) Grad: 370.4250  LR: 0.000018  \n",
      "Epoch: [1][2500/2681] Elapsed 20m 52s (remain 1m 30s) Loss: 0.0034(0.0393) Grad: 997.7729  LR: 0.000018  \n",
      "Epoch: [1][2600/2681] Elapsed 21m 42s (remain 0m 40s) Loss: 0.0024(0.0381) Grad: 277.0777  LR: 0.000018  \n",
      "Epoch: [1][2680/2681] Elapsed 22m 22s (remain 0m 0s) Loss: 0.0038(0.0373) Grad: 404.5835  LR: 0.000018  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 9m 18s) Loss: 0.0052(0.0052) \n",
      "EVAL: [100/894] Elapsed 0m 28s (remain 3m 41s) Loss: 0.0087(0.0068) \n",
      "EVAL: [200/894] Elapsed 0m 55s (remain 3m 12s) Loss: 0.0050(0.0071) \n",
      "EVAL: [300/894] Elapsed 1m 23s (remain 2m 44s) Loss: 0.0027(0.0072) \n",
      "EVAL: [400/894] Elapsed 1m 51s (remain 2m 16s) Loss: 0.0103(0.0066) \n",
      "EVAL: [500/894] Elapsed 2m 18s (remain 1m 48s) Loss: 0.0030(0.0073) \n",
      "EVAL: [600/894] Elapsed 2m 46s (remain 1m 21s) Loss: 0.0034(0.0079) \n",
      "EVAL: [700/894] Elapsed 3m 13s (remain 0m 53s) Loss: 0.0030(0.0077) \n",
      "EVAL: [800/894] Elapsed 3m 41s (remain 0m 25s) Loss: 0.0007(0.0075) \n",
      "EVAL: [893/894] Elapsed 4m 6s (remain 0m 0s) Loss: 0.0012(0.0073) \n",
      "Epoch 1 - avg_train_loss: 0.0373  avg_val_loss: 0.0073  time: 1597s\n",
      "Epoch 1 - Score: 0.8402\n",
      "Epoch 1 - Save Best Score: 0.8402 Model\n",
      "Epoch: [2][0/2681] Elapsed 0m 0s (remain 37m 25s) Loss: 0.0014(0.0014) Grad: 3084.5923  LR: 0.000018  \n",
      "Epoch: [2][100/2681] Elapsed 0m 51s (remain 22m 4s) Loss: 0.0003(0.0060) Grad: 703.4495  LR: 0.000018  \n",
      "Epoch: [2][200/2681] Elapsed 1m 42s (remain 21m 10s) Loss: 0.0001(0.0059) Grad: 347.7051  LR: 0.000017  \n",
      "Epoch: [2][300/2681] Elapsed 2m 34s (remain 20m 18s) Loss: 0.0669(0.0061) Grad: 78229.1562  LR: 0.000017  \n",
      "Epoch: [2][400/2681] Elapsed 3m 24s (remain 19m 23s) Loss: 0.0004(0.0060) Grad: 3041.7859  LR: 0.000017  \n",
      "Epoch: [2][500/2681] Elapsed 4m 14s (remain 18m 29s) Loss: 0.0007(0.0060) Grad: 5104.7729  LR: 0.000017  \n",
      "Epoch: [2][600/2681] Elapsed 5m 5s (remain 17m 36s) Loss: 0.0025(0.0059) Grad: 9914.8467  LR: 0.000017  \n",
      "Epoch: [2][700/2681] Elapsed 5m 56s (remain 16m 46s) Loss: 0.0000(0.0058) Grad: 30.9770  LR: 0.000017  \n",
      "Epoch: [2][800/2681] Elapsed 6m 47s (remain 15m 56s) Loss: 0.0020(0.0058) Grad: 9749.1543  LR: 0.000016  \n",
      "Epoch: [2][900/2681] Elapsed 7m 38s (remain 15m 5s) Loss: 0.0202(0.0059) Grad: 52402.2930  LR: 0.000016  \n",
      "Epoch: [2][1000/2681] Elapsed 8m 28s (remain 14m 14s) Loss: 0.0038(0.0060) Grad: 2862.4888  LR: 0.000016  \n",
      "Epoch: [2][1100/2681] Elapsed 9m 19s (remain 13m 22s) Loss: 0.0205(0.0061) Grad: 54304.2422  LR: 0.000016  \n",
      "Epoch: [2][1200/2681] Elapsed 10m 9s (remain 12m 31s) Loss: 0.0015(0.0061) Grad: 4529.4478  LR: 0.000016  \n",
      "Epoch: [2][1300/2681] Elapsed 11m 0s (remain 11m 40s) Loss: 0.0001(0.0061) Grad: 449.9442  LR: 0.000016  \n",
      "Epoch: [2][1400/2681] Elapsed 11m 51s (remain 10m 49s) Loss: 0.0000(0.0061) Grad: 156.9430  LR: 0.000015  \n",
      "Epoch: [2][1500/2681] Elapsed 12m 42s (remain 9m 59s) Loss: 0.0015(0.0062) Grad: 3306.9482  LR: 0.000015  \n",
      "Epoch: [2][1600/2681] Elapsed 13m 32s (remain 9m 8s) Loss: 0.0000(0.0061) Grad: 131.8588  LR: 0.000015  \n",
      "Epoch: [2][1700/2681] Elapsed 14m 23s (remain 8m 17s) Loss: 0.0000(0.0062) Grad: 177.1926  LR: 0.000015  \n",
      "Epoch: [2][1800/2681] Elapsed 15m 13s (remain 7m 26s) Loss: 0.0032(0.0062) Grad: 7060.5073  LR: 0.000015  \n",
      "Epoch: [2][1900/2681] Elapsed 16m 4s (remain 6m 35s) Loss: 0.0002(0.0062) Grad: 1322.8917  LR: 0.000015  \n",
      "Epoch: [2][2000/2681] Elapsed 16m 55s (remain 5m 45s) Loss: 0.0035(0.0062) Grad: 9170.4473  LR: 0.000014  \n",
      "Epoch: [2][2100/2681] Elapsed 17m 47s (remain 4m 54s) Loss: 0.0011(0.0062) Grad: 3067.2102  LR: 0.000014  \n",
      "Epoch: [2][2200/2681] Elapsed 18m 37s (remain 4m 3s) Loss: 0.0112(0.0061) Grad: 7290.8936  LR: 0.000014  \n",
      "Epoch: [2][2300/2681] Elapsed 19m 28s (remain 3m 12s) Loss: 0.0024(0.0061) Grad: 4842.4331  LR: 0.000014  \n",
      "Epoch: [2][2400/2681] Elapsed 20m 18s (remain 2m 22s) Loss: 0.0008(0.0061) Grad: 4266.4043  LR: 0.000014  \n",
      "Epoch: [2][2500/2681] Elapsed 21m 9s (remain 1m 31s) Loss: 0.0230(0.0061) Grad: 35819.6094  LR: 0.000014  \n",
      "Epoch: [2][2600/2681] Elapsed 22m 0s (remain 0m 40s) Loss: 0.0062(0.0060) Grad: 28137.6055  LR: 0.000013  \n",
      "Epoch: [2][2680/2681] Elapsed 22m 41s (remain 0m 0s) Loss: 0.0056(0.0060) Grad: 17812.1543  LR: 0.000013  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 10m 8s) Loss: 0.0011(0.0011) \n",
      "EVAL: [100/894] Elapsed 0m 28s (remain 3m 41s) Loss: 0.0083(0.0065) \n",
      "EVAL: [200/894] Elapsed 0m 56s (remain 3m 14s) Loss: 0.0063(0.0063) \n",
      "EVAL: [300/894] Elapsed 1m 24s (remain 2m 46s) Loss: 0.0025(0.0065) \n",
      "EVAL: [400/894] Elapsed 1m 51s (remain 2m 17s) Loss: 0.0138(0.0059) \n",
      "EVAL: [500/894] Elapsed 2m 19s (remain 1m 49s) Loss: 0.0012(0.0073) \n",
      "EVAL: [600/894] Elapsed 2m 47s (remain 1m 21s) Loss: 0.0036(0.0087) \n",
      "EVAL: [700/894] Elapsed 3m 15s (remain 0m 53s) Loss: 0.0014(0.0085) \n",
      "EVAL: [800/894] Elapsed 3m 43s (remain 0m 25s) Loss: 0.0011(0.0082) \n",
      "EVAL: [893/894] Elapsed 4m 8s (remain 0m 0s) Loss: 0.0000(0.0080) \n",
      "Epoch 2 - avg_train_loss: 0.0060  avg_val_loss: 0.0080  time: 1617s\n",
      "Epoch 2 - Score: 0.8721\n",
      "Epoch 2 - Save Best Score: 0.8721 Model\n",
      "Epoch: [3][0/2681] Elapsed 0m 0s (remain 39m 53s) Loss: 0.0038(0.0038) Grad: 35330.2891  LR: 0.000013  \n",
      "Epoch: [3][100/2681] Elapsed 0m 52s (remain 22m 10s) Loss: 0.0005(0.0050) Grad: 2100.8284  LR: 0.000013  \n",
      "Epoch: [3][200/2681] Elapsed 1m 42s (remain 21m 6s) Loss: 0.0150(0.0048) Grad: 30449.8730  LR: 0.000013  \n",
      "Epoch: [3][300/2681] Elapsed 2m 33s (remain 20m 11s) Loss: 0.0087(0.0047) Grad: 8150.2847  LR: 0.000013  \n",
      "Epoch: [3][400/2681] Elapsed 3m 23s (remain 19m 17s) Loss: 0.0001(0.0045) Grad: 194.2586  LR: 0.000013  \n",
      "Epoch: [3][500/2681] Elapsed 4m 13s (remain 18m 23s) Loss: 0.0061(0.0045) Grad: 12820.5957  LR: 0.000013  \n",
      "Epoch: [3][600/2681] Elapsed 5m 4s (remain 17m 32s) Loss: 0.0004(0.0045) Grad: 3571.1997  LR: 0.000012  \n",
      "Epoch: [3][700/2681] Elapsed 5m 54s (remain 16m 40s) Loss: 0.0015(0.0044) Grad: 7326.8530  LR: 0.000012  \n",
      "Epoch: [3][800/2681] Elapsed 6m 44s (remain 15m 50s) Loss: 0.0020(0.0045) Grad: 8281.2627  LR: 0.000012  \n",
      "Epoch: [3][900/2681] Elapsed 7m 35s (remain 14m 59s) Loss: 0.0123(0.0046) Grad: 31358.4434  LR: 0.000012  \n",
      "Epoch: [3][1000/2681] Elapsed 8m 26s (remain 14m 10s) Loss: 0.0001(0.0046) Grad: 292.0884  LR: 0.000012  \n",
      "Epoch: [3][1100/2681] Elapsed 9m 17s (remain 13m 20s) Loss: 0.0355(0.0047) Grad: 69914.3516  LR: 0.000012  \n",
      "Epoch: [3][1200/2681] Elapsed 10m 7s (remain 12m 28s) Loss: 0.0001(0.0046) Grad: 416.8027  LR: 0.000011  \n",
      "Epoch: [3][1300/2681] Elapsed 10m 57s (remain 11m 37s) Loss: 0.0000(0.0046) Grad: 116.9987  LR: 0.000011  \n",
      "Epoch: [3][1400/2681] Elapsed 11m 48s (remain 10m 46s) Loss: 0.0000(0.0047) Grad: 50.4336  LR: 0.000011  \n",
      "Epoch: [3][1500/2681] Elapsed 12m 38s (remain 9m 56s) Loss: 0.0020(0.0046) Grad: 11334.8105  LR: 0.000011  \n",
      "Epoch: [3][1600/2681] Elapsed 13m 29s (remain 9m 5s) Loss: 0.0007(0.0046) Grad: 7988.5259  LR: 0.000011  \n",
      "Epoch: [3][1700/2681] Elapsed 14m 19s (remain 8m 15s) Loss: 0.0042(0.0046) Grad: 11873.8896  LR: 0.000011  \n",
      "Epoch: [3][1800/2681] Elapsed 15m 10s (remain 7m 24s) Loss: 0.0030(0.0047) Grad: 8711.3135  LR: 0.000010  \n",
      "Epoch: [3][1900/2681] Elapsed 16m 1s (remain 6m 34s) Loss: 0.0065(0.0046) Grad: 11885.8115  LR: 0.000010  \n",
      "Epoch: [3][2000/2681] Elapsed 16m 51s (remain 5m 43s) Loss: 0.0086(0.0046) Grad: 14259.8320  LR: 0.000010  \n",
      "Epoch: [3][2100/2681] Elapsed 17m 42s (remain 4m 53s) Loss: 0.0000(0.0045) Grad: 50.2688  LR: 0.000010  \n",
      "Epoch: [3][2200/2681] Elapsed 18m 33s (remain 4m 2s) Loss: 0.0000(0.0047) Grad: 11.3921  LR: 0.000010  \n",
      "Epoch: [3][2300/2681] Elapsed 19m 24s (remain 3m 12s) Loss: 0.0000(0.0047) Grad: 158.5123  LR: 0.000010  \n",
      "Epoch: [3][2400/2681] Elapsed 20m 15s (remain 2m 21s) Loss: 0.0022(0.0047) Grad: 4117.8311  LR: 0.000009  \n",
      "Epoch: [3][2500/2681] Elapsed 21m 6s (remain 1m 31s) Loss: 0.0078(0.0047) Grad: 40353.1289  LR: 0.000009  \n",
      "Epoch: [3][2600/2681] Elapsed 21m 57s (remain 0m 40s) Loss: 0.0023(0.0047) Grad: 9666.1104  LR: 0.000009  \n",
      "Epoch: [3][2680/2681] Elapsed 22m 37s (remain 0m 0s) Loss: 0.0001(0.0047) Grad: 218.7071  LR: 0.000009  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 8m 25s) Loss: 0.0019(0.0019) \n",
      "EVAL: [100/894] Elapsed 0m 28s (remain 3m 40s) Loss: 0.0019(0.0060) \n",
      "EVAL: [200/894] Elapsed 0m 55s (remain 3m 12s) Loss: 0.0067(0.0066) \n",
      "EVAL: [300/894] Elapsed 1m 23s (remain 2m 44s) Loss: 0.0029(0.0064) \n",
      "EVAL: [400/894] Elapsed 1m 51s (remain 2m 16s) Loss: 0.0103(0.0060) \n",
      "EVAL: [500/894] Elapsed 2m 18s (remain 1m 48s) Loss: 0.0019(0.0072) \n",
      "EVAL: [600/894] Elapsed 2m 46s (remain 1m 21s) Loss: 0.0022(0.0085) \n",
      "EVAL: [700/894] Elapsed 3m 14s (remain 0m 53s) Loss: 0.0002(0.0084) \n",
      "EVAL: [800/894] Elapsed 3m 41s (remain 0m 25s) Loss: 0.0011(0.0081) \n",
      "EVAL: [893/894] Elapsed 4m 6s (remain 0m 0s) Loss: 0.0000(0.0078) \n",
      "Epoch 3 - avg_train_loss: 0.0047  avg_val_loss: 0.0078  time: 1612s\n",
      "Epoch 3 - Score: 0.8736\n",
      "Epoch 3 - Save Best Score: 0.8736 Model\n",
      "Epoch: [4][0/2681] Elapsed 0m 0s (remain 38m 57s) Loss: 0.0447(0.0447) Grad: 86210.0078  LR: 0.000009  \n",
      "Epoch: [4][100/2681] Elapsed 0m 51s (remain 21m 46s) Loss: 0.0000(0.0040) Grad: 43.7592  LR: 0.000009  \n",
      "Epoch: [4][200/2681] Elapsed 1m 42s (remain 21m 0s) Loss: 0.0000(0.0046) Grad: 22.9878  LR: 0.000009  \n",
      "Epoch: [4][300/2681] Elapsed 2m 32s (remain 20m 3s) Loss: 0.0000(0.0043) Grad: 30.0150  LR: 0.000008  \n",
      "Epoch: [4][400/2681] Elapsed 3m 23s (remain 19m 15s) Loss: 0.0292(0.0039) Grad: 33313.0586  LR: 0.000008  \n",
      "Epoch: [4][500/2681] Elapsed 4m 13s (remain 18m 22s) Loss: 0.0052(0.0043) Grad: 13320.9414  LR: 0.000008  \n",
      "Epoch: [4][600/2681] Elapsed 5m 4s (remain 17m 34s) Loss: 0.0051(0.0043) Grad: 12307.6875  LR: 0.000008  \n",
      "Epoch: [4][700/2681] Elapsed 5m 55s (remain 16m 42s) Loss: 0.0001(0.0042) Grad: 624.5176  LR: 0.000008  \n",
      "Epoch: [4][800/2681] Elapsed 6m 45s (remain 15m 51s) Loss: 0.0004(0.0042) Grad: 8580.2520  LR: 0.000008  \n",
      "Epoch: [4][900/2681] Elapsed 7m 35s (remain 14m 59s) Loss: 0.0554(0.0041) Grad: 80158.7188  LR: 0.000007  \n",
      "Epoch: [4][1000/2681] Elapsed 8m 26s (remain 14m 10s) Loss: 0.0040(0.0041) Grad: 12118.2637  LR: 0.000007  \n",
      "Epoch: [4][1100/2681] Elapsed 9m 17s (remain 13m 20s) Loss: 0.0003(0.0040) Grad: 1528.2721  LR: 0.000007  \n",
      "Epoch: [4][1200/2681] Elapsed 10m 7s (remain 12m 28s) Loss: 0.0002(0.0041) Grad: 1758.7991  LR: 0.000007  \n",
      "Epoch: [4][1300/2681] Elapsed 10m 57s (remain 11m 37s) Loss: 0.0042(0.0040) Grad: 12013.5742  LR: 0.000007  \n",
      "Epoch: [4][1400/2681] Elapsed 11m 48s (remain 10m 47s) Loss: 0.0001(0.0040) Grad: 230.5863  LR: 0.000007  \n",
      "Epoch: [4][1500/2681] Elapsed 12m 39s (remain 9m 56s) Loss: 0.0059(0.0041) Grad: 15042.6133  LR: 0.000006  \n",
      "Epoch: [4][1600/2681] Elapsed 13m 29s (remain 9m 5s) Loss: 0.0035(0.0040) Grad: 10757.3252  LR: 0.000006  \n",
      "Epoch: [4][1700/2681] Elapsed 14m 19s (remain 8m 14s) Loss: 0.0000(0.0040) Grad: 238.8613  LR: 0.000006  \n",
      "Epoch: [4][1800/2681] Elapsed 15m 9s (remain 7m 24s) Loss: 0.0009(0.0039) Grad: 7936.8950  LR: 0.000006  \n",
      "Epoch: [4][1900/2681] Elapsed 15m 59s (remain 6m 33s) Loss: 0.0001(0.0039) Grad: 786.7731  LR: 0.000006  \n",
      "Epoch: [4][2000/2681] Elapsed 16m 49s (remain 5m 43s) Loss: 0.0003(0.0040) Grad: 4383.8452  LR: 0.000006  \n",
      "Epoch: [4][2100/2681] Elapsed 17m 40s (remain 4m 52s) Loss: 0.0041(0.0040) Grad: 8975.4824  LR: 0.000005  \n",
      "Epoch: [4][2200/2681] Elapsed 18m 31s (remain 4m 2s) Loss: 0.0349(0.0041) Grad: 62652.0586  LR: 0.000005  \n",
      "Epoch: [4][2300/2681] Elapsed 19m 23s (remain 3m 12s) Loss: 0.0050(0.0040) Grad: 12303.5879  LR: 0.000005  \n",
      "Epoch: [4][2400/2681] Elapsed 20m 13s (remain 2m 21s) Loss: 0.0029(0.0040) Grad: 5611.4531  LR: 0.000005  \n",
      "Epoch: [4][2500/2681] Elapsed 21m 3s (remain 1m 30s) Loss: 0.0162(0.0040) Grad: 115835.9219  LR: 0.000005  \n",
      "Epoch: [4][2600/2681] Elapsed 21m 53s (remain 0m 40s) Loss: 0.0086(0.0040) Grad: 26796.8086  LR: 0.000005  \n",
      "Epoch: [4][2680/2681] Elapsed 22m 33s (remain 0m 0s) Loss: 0.0060(0.0040) Grad: 11513.1768  LR: 0.000004  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 10m 52s) Loss: 0.0014(0.0014) \n",
      "EVAL: [100/894] Elapsed 0m 28s (remain 3m 47s) Loss: 0.0014(0.0080) \n",
      "EVAL: [200/894] Elapsed 0m 57s (remain 3m 16s) Loss: 0.0065(0.0080) \n",
      "EVAL: [300/894] Elapsed 1m 24s (remain 2m 47s) Loss: 0.0025(0.0075) \n",
      "EVAL: [400/894] Elapsed 1m 52s (remain 2m 18s) Loss: 0.0136(0.0069) \n",
      "EVAL: [500/894] Elapsed 2m 20s (remain 1m 49s) Loss: 0.0003(0.0084) \n",
      "EVAL: [600/894] Elapsed 2m 48s (remain 1m 21s) Loss: 0.0005(0.0099) \n",
      "EVAL: [700/894] Elapsed 3m 15s (remain 0m 53s) Loss: 0.0008(0.0098) \n",
      "EVAL: [800/894] Elapsed 3m 43s (remain 0m 25s) Loss: 0.0003(0.0095) \n",
      "EVAL: [893/894] Elapsed 4m 8s (remain 0m 0s) Loss: 0.0000(0.0092) \n",
      "Epoch 4 - avg_train_loss: 0.0040  avg_val_loss: 0.0092  time: 1609s\n",
      "Epoch 4 - Score: 0.8812\n",
      "Epoch 4 - Save Best Score: 0.8812 Model\n",
      "Epoch: [5][0/2681] Elapsed 0m 0s (remain 39m 46s) Loss: 0.0007(0.0007) Grad: 6985.2271  LR: 0.000004  \n",
      "Epoch: [5][100/2681] Elapsed 0m 51s (remain 21m 43s) Loss: 0.0000(0.0028) Grad: 352.8706  LR: 0.000004  \n",
      "Epoch: [5][200/2681] Elapsed 1m 43s (remain 21m 19s) Loss: 0.0032(0.0028) Grad: 33835.5156  LR: 0.000004  \n",
      "Epoch: [5][300/2681] Elapsed 2m 34s (remain 20m 25s) Loss: 0.0077(0.0028) Grad: 9142.7500  LR: 0.000004  \n",
      "Epoch: [5][400/2681] Elapsed 3m 24s (remain 19m 24s) Loss: 0.0005(0.0028) Grad: 5435.4443  LR: 0.000004  \n",
      "Epoch: [5][500/2681] Elapsed 4m 14s (remain 18m 28s) Loss: 0.0006(0.0027) Grad: 3809.5471  LR: 0.000004  \n",
      "Epoch: [5][600/2681] Elapsed 5m 4s (remain 17m 34s) Loss: 0.0005(0.0030) Grad: 4657.9546  LR: 0.000003  \n",
      "Epoch: [5][700/2681] Elapsed 5m 54s (remain 16m 41s) Loss: 0.0000(0.0034) Grad: 35.2428  LR: 0.000003  \n",
      "Epoch: [5][800/2681] Elapsed 6m 44s (remain 15m 50s) Loss: 0.0019(0.0035) Grad: 7515.4702  LR: 0.000003  \n",
      "Epoch: [5][900/2681] Elapsed 7m 35s (remain 14m 59s) Loss: 0.0000(0.0033) Grad: 33.7137  LR: 0.000003  \n",
      "Epoch: [5][1000/2681] Elapsed 8m 25s (remain 14m 8s) Loss: 0.0019(0.0033) Grad: 9222.5352  LR: 0.000003  \n",
      "Epoch: [5][1100/2681] Elapsed 9m 15s (remain 13m 16s) Loss: 0.0001(0.0032) Grad: 924.4022  LR: 0.000003  \n",
      "Epoch: [5][1200/2681] Elapsed 10m 5s (remain 12m 25s) Loss: 0.0002(0.0032) Grad: 937.4870  LR: 0.000002  \n",
      "Epoch: [5][1300/2681] Elapsed 10m 55s (remain 11m 34s) Loss: 0.0000(0.0032) Grad: 80.5448  LR: 0.000002  \n",
      "Epoch: [5][1400/2681] Elapsed 11m 45s (remain 10m 44s) Loss: 0.0000(0.0033) Grad: 348.9233  LR: 0.000002  \n",
      "Epoch: [5][1500/2681] Elapsed 12m 35s (remain 9m 53s) Loss: 0.0000(0.0032) Grad: 91.0205  LR: 0.000002  \n",
      "Epoch: [5][1600/2681] Elapsed 13m 25s (remain 9m 3s) Loss: 0.0001(0.0033) Grad: 755.3718  LR: 0.000002  \n",
      "Epoch: [5][1700/2681] Elapsed 14m 15s (remain 8m 12s) Loss: 0.0000(0.0033) Grad: 6.3610  LR: 0.000002  \n",
      "Epoch: [5][1800/2681] Elapsed 15m 5s (remain 7m 22s) Loss: 0.0000(0.0032) Grad: 35.2865  LR: 0.000001  \n",
      "Epoch: [5][1900/2681] Elapsed 15m 55s (remain 6m 32s) Loss: 0.0000(0.0033) Grad: 50.8027  LR: 0.000001  \n",
      "Epoch: [5][2000/2681] Elapsed 16m 45s (remain 5m 41s) Loss: 0.0000(0.0033) Grad: 261.4682  LR: 0.000001  \n",
      "Epoch: [5][2100/2681] Elapsed 17m 35s (remain 4m 51s) Loss: 0.0008(0.0032) Grad: 18460.4336  LR: 0.000001  \n",
      "Epoch: [5][2200/2681] Elapsed 18m 25s (remain 4m 1s) Loss: 0.0001(0.0033) Grad: 1261.8484  LR: 0.000001  \n",
      "Epoch: [5][2300/2681] Elapsed 19m 16s (remain 3m 10s) Loss: 0.0084(0.0033) Grad: 47557.3203  LR: 0.000001  \n",
      "Epoch: [5][2400/2681] Elapsed 20m 6s (remain 2m 20s) Loss: 0.0256(0.0033) Grad: 62279.3633  LR: 0.000000  \n",
      "Epoch: [5][2500/2681] Elapsed 20m 56s (remain 1m 30s) Loss: 0.0000(0.0033) Grad: 223.9692  LR: 0.000000  \n",
      "Epoch: [5][2600/2681] Elapsed 21m 46s (remain 0m 40s) Loss: 0.0000(0.0033) Grad: 6.6288  LR: 0.000000  \n",
      "Epoch: [5][2680/2681] Elapsed 22m 26s (remain 0m 0s) Loss: 0.0011(0.0032) Grad: 5661.6934  LR: 0.000000  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 9m 28s) Loss: 0.0009(0.0009) \n",
      "EVAL: [100/894] Elapsed 0m 28s (remain 3m 41s) Loss: 0.0001(0.0080) \n",
      "EVAL: [200/894] Elapsed 0m 56s (remain 3m 13s) Loss: 0.0066(0.0082) \n",
      "EVAL: [300/894] Elapsed 1m 24s (remain 2m 45s) Loss: 0.0039(0.0079) \n",
      "EVAL: [400/894] Elapsed 1m 51s (remain 2m 17s) Loss: 0.0126(0.0073) \n",
      "EVAL: [500/894] Elapsed 2m 19s (remain 1m 49s) Loss: 0.0011(0.0087) \n",
      "EVAL: [600/894] Elapsed 2m 46s (remain 1m 21s) Loss: 0.0006(0.0104) \n",
      "EVAL: [700/894] Elapsed 3m 14s (remain 0m 53s) Loss: 0.0037(0.0103) \n",
      "EVAL: [800/894] Elapsed 3m 42s (remain 0m 25s) Loss: 0.0005(0.0100) \n",
      "EVAL: [893/894] Elapsed 4m 7s (remain 0m 0s) Loss: 0.0000(0.0097) \n",
      "Epoch 5 - avg_train_loss: 0.0032  avg_val_loss: 0.0097  time: 1600s\n",
      "Epoch 5 - Score: 0.8824\n",
      "Epoch 5 - Save Best Score: 0.8824 Model\n",
      "========== fold: 1 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2681] Elapsed 0m 0s (remain 39m 18s) Loss: 0.5280(0.5280) Grad: inf  LR: 0.000000  \n",
      "Epoch: [1][100/2681] Elapsed 0m 51s (remain 21m 55s) Loss: 0.4349(0.5297) Grad: 64129.5195  LR: 0.000002  \n",
      "Epoch: [1][200/2681] Elapsed 1m 41s (remain 20m 55s) Loss: 0.1306(0.4186) Grad: 28878.7969  LR: 0.000003  \n",
      "Epoch: [1][300/2681] Elapsed 2m 32s (remain 20m 5s) Loss: 0.0487(0.3025) Grad: 3801.8169  LR: 0.000004  \n",
      "Epoch: [1][400/2681] Elapsed 3m 22s (remain 19m 13s) Loss: 0.0508(0.2368) Grad: 4050.5381  LR: 0.000006  \n",
      "Epoch: [1][500/2681] Elapsed 4m 13s (remain 18m 21s) Loss: 0.0657(0.1972) Grad: 4394.5146  LR: 0.000007  \n",
      "Epoch: [1][600/2681] Elapsed 5m 4s (remain 17m 33s) Loss: 0.0209(0.1699) Grad: 5160.8364  LR: 0.000009  \n",
      "Epoch: [1][700/2681] Elapsed 5m 55s (remain 16m 42s) Loss: 0.0232(0.1490) Grad: 9513.0488  LR: 0.000010  \n",
      "Epoch: [1][800/2681] Elapsed 6m 45s (remain 15m 51s) Loss: 0.0099(0.1328) Grad: 3851.2178  LR: 0.000012  \n",
      "Epoch: [1][900/2681] Elapsed 7m 36s (remain 15m 1s) Loss: 0.0169(0.1198) Grad: 10840.7744  LR: 0.000013  \n",
      "Epoch: [1][1000/2681] Elapsed 8m 27s (remain 14m 11s) Loss: 0.0223(0.1091) Grad: 9350.9404  LR: 0.000015  \n",
      "Epoch: [1][1100/2681] Elapsed 9m 17s (remain 13m 20s) Loss: 0.0198(0.1004) Grad: 35414.7500  LR: 0.000016  \n",
      "Epoch: [1][1200/2681] Elapsed 10m 9s (remain 12m 30s) Loss: 0.0038(0.0931) Grad: 1672.8190  LR: 0.000018  \n",
      "Epoch: [1][1300/2681] Elapsed 11m 0s (remain 11m 40s) Loss: 0.0106(0.0868) Grad: 5139.8013  LR: 0.000019  \n",
      "Epoch: [1][1400/2681] Elapsed 11m 50s (remain 10m 49s) Loss: 0.0065(0.0813) Grad: 4561.7637  LR: 0.000020  \n",
      "Epoch: [1][1500/2681] Elapsed 12m 41s (remain 9m 58s) Loss: 0.0071(0.0765) Grad: 5119.9990  LR: 0.000020  \n",
      "Epoch: [1][1600/2681] Elapsed 13m 33s (remain 9m 8s) Loss: 0.0219(0.0723) Grad: 11443.7246  LR: 0.000020  \n",
      "Epoch: [1][1700/2681] Elapsed 14m 23s (remain 8m 17s) Loss: 0.0106(0.0687) Grad: 3420.0889  LR: 0.000019  \n",
      "Epoch: [1][1800/2681] Elapsed 15m 13s (remain 7m 26s) Loss: 0.0034(0.0655) Grad: 1275.3977  LR: 0.000019  \n",
      "Epoch: [1][1900/2681] Elapsed 16m 4s (remain 6m 35s) Loss: 0.0028(0.0626) Grad: 1906.0823  LR: 0.000019  \n",
      "Epoch: [1][2000/2681] Elapsed 16m 55s (remain 5m 44s) Loss: 0.0023(0.0600) Grad: 3414.8030  LR: 0.000019  \n",
      "Epoch: [1][2100/2681] Elapsed 17m 45s (remain 4m 54s) Loss: 0.0025(0.0576) Grad: 1087.7394  LR: 0.000019  \n",
      "Epoch: [1][2200/2681] Elapsed 18m 36s (remain 4m 3s) Loss: 0.0127(0.0554) Grad: 5776.0728  LR: 0.000019  \n",
      "Epoch: [1][2300/2681] Elapsed 19m 26s (remain 3m 12s) Loss: 0.0053(0.0533) Grad: 1025.2042  LR: 0.000018  \n",
      "Epoch: [1][2400/2681] Elapsed 20m 16s (remain 2m 21s) Loss: 0.0036(0.0515) Grad: 900.2501  LR: 0.000018  \n",
      "Epoch: [1][2500/2681] Elapsed 21m 7s (remain 1m 31s) Loss: 0.0022(0.0499) Grad: 538.7151  LR: 0.000018  \n",
      "Epoch: [1][2600/2681] Elapsed 21m 57s (remain 0m 40s) Loss: 0.0016(0.0483) Grad: 473.5633  LR: 0.000018  \n",
      "Epoch: [1][2680/2681] Elapsed 22m 37s (remain 0m 0s) Loss: 0.0099(0.0471) Grad: 1059.0786  LR: 0.000018  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 8m 57s) Loss: 0.0071(0.0071) \n",
      "EVAL: [100/894] Elapsed 0m 28s (remain 3m 42s) Loss: 0.0034(0.0051) \n",
      "EVAL: [200/894] Elapsed 0m 56s (remain 3m 13s) Loss: 0.0097(0.0090) \n",
      "EVAL: [300/894] Elapsed 1m 23s (remain 2m 45s) Loss: 0.0062(0.0106) \n",
      "EVAL: [400/894] Elapsed 1m 51s (remain 2m 16s) Loss: 0.0017(0.0097) \n",
      "EVAL: [500/894] Elapsed 2m 19s (remain 1m 49s) Loss: 0.0210(0.0099) \n",
      "EVAL: [600/894] Elapsed 2m 47s (remain 1m 21s) Loss: 0.0053(0.0098) \n",
      "EVAL: [700/894] Elapsed 3m 14s (remain 0m 53s) Loss: 0.0004(0.0094) \n",
      "EVAL: [800/894] Elapsed 3m 42s (remain 0m 25s) Loss: 0.0022(0.0091) \n",
      "EVAL: [893/894] Elapsed 4m 7s (remain 0m 0s) Loss: 0.0279(0.0085) \n",
      "Epoch 1 - avg_train_loss: 0.0471  avg_val_loss: 0.0085  time: 1612s\n",
      "Epoch 1 - Score: 0.8338\n",
      "Epoch 1 - Save Best Score: 0.8338 Model\n",
      "Epoch: [2][0/2681] Elapsed 0m 1s (remain 47m 28s) Loss: 0.0004(0.0004) Grad: 1439.4802  LR: 0.000018  \n",
      "Epoch: [2][100/2681] Elapsed 0m 52s (remain 22m 17s) Loss: 0.0405(0.0081) Grad: 35860.6562  LR: 0.000018  \n",
      "Epoch: [2][200/2681] Elapsed 1m 44s (remain 21m 34s) Loss: 0.0006(0.0086) Grad: 5711.0386  LR: 0.000017  \n",
      "Epoch: [2][300/2681] Elapsed 2m 36s (remain 20m 35s) Loss: 0.0085(0.0078) Grad: 13413.4473  LR: 0.000017  \n",
      "Epoch: [2][400/2681] Elapsed 3m 26s (remain 19m 35s) Loss: 0.0001(0.0076) Grad: 775.1100  LR: 0.000017  \n",
      "Epoch: [2][500/2681] Elapsed 4m 17s (remain 18m 40s) Loss: 0.0015(0.0075) Grad: 4315.1401  LR: 0.000017  \n",
      "Epoch: [2][600/2681] Elapsed 5m 7s (remain 17m 45s) Loss: 0.0049(0.0078) Grad: 13675.5938  LR: 0.000017  \n",
      "Epoch: [2][700/2681] Elapsed 5m 58s (remain 16m 53s) Loss: 0.0081(0.0076) Grad: 15613.2100  LR: 0.000017  \n",
      "Epoch: [2][800/2681] Elapsed 6m 49s (remain 16m 1s) Loss: 0.0000(0.0075) Grad: 104.3288  LR: 0.000016  \n",
      "Epoch: [2][900/2681] Elapsed 7m 40s (remain 15m 9s) Loss: 0.0015(0.0076) Grad: 27108.4922  LR: 0.000016  \n",
      "Epoch: [2][1000/2681] Elapsed 8m 30s (remain 14m 17s) Loss: 0.0059(0.0075) Grad: 5531.6860  LR: 0.000016  \n",
      "Epoch: [2][1100/2681] Elapsed 9m 22s (remain 13m 27s) Loss: 0.0018(0.0073) Grad: 4159.6138  LR: 0.000016  \n",
      "Epoch: [2][1200/2681] Elapsed 10m 13s (remain 12m 36s) Loss: 0.0125(0.0073) Grad: 31515.9492  LR: 0.000016  \n",
      "Epoch: [2][1300/2681] Elapsed 11m 4s (remain 11m 44s) Loss: 0.0012(0.0073) Grad: 4085.4399  LR: 0.000016  \n",
      "Epoch: [2][1400/2681] Elapsed 11m 55s (remain 10m 53s) Loss: 0.0007(0.0071) Grad: 4488.4634  LR: 0.000015  \n",
      "Epoch: [2][1500/2681] Elapsed 12m 45s (remain 10m 2s) Loss: 0.0025(0.0072) Grad: 3023.3057  LR: 0.000015  \n",
      "Epoch: [2][1600/2681] Elapsed 13m 36s (remain 9m 10s) Loss: 0.0066(0.0072) Grad: 5336.1641  LR: 0.000015  \n",
      "Epoch: [2][1700/2681] Elapsed 14m 27s (remain 8m 19s) Loss: 0.0004(0.0070) Grad: 708.9611  LR: 0.000015  \n",
      "Epoch: [2][1800/2681] Elapsed 15m 17s (remain 7m 28s) Loss: 0.0010(0.0070) Grad: 1137.5549  LR: 0.000015  \n",
      "Epoch: [2][1900/2681] Elapsed 16m 8s (remain 6m 37s) Loss: 0.0061(0.0070) Grad: 1531.0361  LR: 0.000015  \n",
      "Epoch: [2][2000/2681] Elapsed 16m 59s (remain 5m 46s) Loss: 0.0000(0.0070) Grad: 36.4304  LR: 0.000014  \n",
      "Epoch: [2][2100/2681] Elapsed 17m 50s (remain 4m 55s) Loss: 0.0003(0.0069) Grad: 406.0031  LR: 0.000014  \n",
      "Epoch: [2][2200/2681] Elapsed 18m 40s (remain 4m 4s) Loss: 0.0044(0.0069) Grad: 3919.7014  LR: 0.000014  \n",
      "Epoch: [2][2300/2681] Elapsed 19m 30s (remain 3m 13s) Loss: 0.0009(0.0068) Grad: 489.7253  LR: 0.000014  \n",
      "Epoch: [2][2400/2681] Elapsed 20m 21s (remain 2m 22s) Loss: 0.0071(0.0068) Grad: 1237.3871  LR: 0.000014  \n",
      "Epoch: [2][2500/2681] Elapsed 21m 12s (remain 1m 31s) Loss: 0.0006(0.0067) Grad: 481.2046  LR: 0.000014  \n",
      "Epoch: [2][2600/2681] Elapsed 22m 3s (remain 0m 40s) Loss: 0.0001(0.0067) Grad: 79.4311  LR: 0.000013  \n",
      "Epoch: [2][2680/2681] Elapsed 22m 44s (remain 0m 0s) Loss: 0.0010(0.0067) Grad: 685.5683  LR: 0.000013  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 9m 40s) Loss: 0.0085(0.0085) \n",
      "EVAL: [100/894] Elapsed 0m 28s (remain 3m 41s) Loss: 0.0024(0.0059) \n",
      "EVAL: [200/894] Elapsed 0m 55s (remain 3m 12s) Loss: 0.0080(0.0088) \n",
      "EVAL: [300/894] Elapsed 1m 23s (remain 2m 44s) Loss: 0.0106(0.0092) \n",
      "EVAL: [400/894] Elapsed 1m 50s (remain 2m 15s) Loss: 0.0006(0.0087) \n",
      "EVAL: [500/894] Elapsed 2m 18s (remain 1m 48s) Loss: 0.0074(0.0094) \n",
      "EVAL: [600/894] Elapsed 2m 46s (remain 1m 20s) Loss: 0.0036(0.0097) \n",
      "EVAL: [700/894] Elapsed 3m 13s (remain 0m 53s) Loss: 0.0001(0.0095) \n",
      "EVAL: [800/894] Elapsed 3m 41s (remain 0m 25s) Loss: 0.0025(0.0091) \n",
      "EVAL: [893/894] Elapsed 4m 6s (remain 0m 0s) Loss: 0.0128(0.0085) \n",
      "Epoch 2 - avg_train_loss: 0.0067  avg_val_loss: 0.0085  time: 1618s\n",
      "Epoch 2 - Score: 0.8581\n",
      "Epoch 2 - Save Best Score: 0.8581 Model\n",
      "Epoch: [3][0/2681] Elapsed 0m 0s (remain 42m 16s) Loss: 0.0034(0.0034) Grad: 8408.6514  LR: 0.000013  \n",
      "Epoch: [3][100/2681] Elapsed 0m 52s (remain 22m 11s) Loss: 0.0002(0.0059) Grad: 1406.9491  LR: 0.000013  \n",
      "Epoch: [3][200/2681] Elapsed 1m 42s (remain 21m 7s) Loss: 0.0144(0.0050) Grad: 59048.3594  LR: 0.000013  \n",
      "Epoch: [3][300/2681] Elapsed 2m 33s (remain 20m 15s) Loss: 0.0003(0.0048) Grad: 1093.9376  LR: 0.000013  \n",
      "Epoch: [3][400/2681] Elapsed 3m 25s (remain 19m 31s) Loss: 0.0057(0.0050) Grad: 28520.4551  LR: 0.000013  \n",
      "Epoch: [3][500/2681] Elapsed 4m 16s (remain 18m 36s) Loss: 0.0001(0.0051) Grad: 688.7148  LR: 0.000013  \n",
      "Epoch: [3][600/2681] Elapsed 5m 7s (remain 17m 43s) Loss: 0.0030(0.0051) Grad: 3897.8818  LR: 0.000012  \n",
      "Epoch: [3][700/2681] Elapsed 5m 57s (remain 16m 50s) Loss: 0.0003(0.0053) Grad: 1951.6959  LR: 0.000012  \n",
      "Epoch: [3][800/2681] Elapsed 6m 49s (remain 16m 0s) Loss: 0.0010(0.0054) Grad: 4593.7451  LR: 0.000012  \n",
      "Epoch: [3][900/2681] Elapsed 7m 39s (remain 15m 8s) Loss: 0.0091(0.0053) Grad: 18392.3086  LR: 0.000012  \n",
      "Epoch: [3][1000/2681] Elapsed 8m 30s (remain 14m 16s) Loss: 0.0003(0.0053) Grad: 2029.2876  LR: 0.000012  \n",
      "Epoch: [3][1100/2681] Elapsed 9m 20s (remain 13m 24s) Loss: 0.0001(0.0053) Grad: 1428.7568  LR: 0.000012  \n",
      "Epoch: [3][1200/2681] Elapsed 10m 11s (remain 12m 33s) Loss: 0.0000(0.0054) Grad: 216.8273  LR: 0.000011  \n",
      "Epoch: [3][1300/2681] Elapsed 11m 2s (remain 11m 42s) Loss: 0.0095(0.0054) Grad: 15459.8770  LR: 0.000011  \n",
      "Epoch: [3][1400/2681] Elapsed 11m 53s (remain 10m 51s) Loss: 0.0001(0.0053) Grad: 219.6962  LR: 0.000011  \n",
      "Epoch: [3][1500/2681] Elapsed 12m 44s (remain 10m 1s) Loss: 0.0472(0.0052) Grad: 59265.7734  LR: 0.000011  \n",
      "Epoch: [3][1600/2681] Elapsed 13m 35s (remain 9m 10s) Loss: 0.0286(0.0053) Grad: 26995.9883  LR: 0.000011  \n",
      "Epoch: [3][1700/2681] Elapsed 14m 25s (remain 8m 18s) Loss: 0.0027(0.0052) Grad: 7072.1323  LR: 0.000011  \n",
      "Epoch: [3][1800/2681] Elapsed 15m 16s (remain 7m 27s) Loss: 0.0076(0.0052) Grad: 87292.5156  LR: 0.000010  \n",
      "Epoch: [3][1900/2681] Elapsed 16m 6s (remain 6m 36s) Loss: 0.0333(0.0052) Grad: 39385.7852  LR: 0.000010  \n",
      "Epoch: [3][2000/2681] Elapsed 16m 57s (remain 5m 45s) Loss: 0.0002(0.0051) Grad: 2999.6658  LR: 0.000010  \n",
      "Epoch: [3][2100/2681] Elapsed 17m 48s (remain 4m 54s) Loss: 0.0019(0.0050) Grad: 3378.1948  LR: 0.000010  \n",
      "Epoch: [3][2200/2681] Elapsed 18m 39s (remain 4m 4s) Loss: 0.0001(0.0051) Grad: 1888.1283  LR: 0.000010  \n",
      "Epoch: [3][2300/2681] Elapsed 19m 30s (remain 3m 13s) Loss: 0.0000(0.0050) Grad: 54.2678  LR: 0.000010  \n",
      "Epoch: [3][2400/2681] Elapsed 20m 21s (remain 2m 22s) Loss: 0.0004(0.0051) Grad: 1609.6509  LR: 0.000009  \n",
      "Epoch: [3][2500/2681] Elapsed 21m 12s (remain 1m 31s) Loss: 0.0000(0.0051) Grad: 106.5623  LR: 0.000009  \n",
      "Epoch: [3][2600/2681] Elapsed 22m 3s (remain 0m 40s) Loss: 0.0151(0.0051) Grad: 44933.0078  LR: 0.000009  \n",
      "Epoch: [3][2680/2681] Elapsed 22m 43s (remain 0m 0s) Loss: 0.0002(0.0051) Grad: 3282.6758  LR: 0.000009  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 11m 23s) Loss: 0.0117(0.0117) \n",
      "EVAL: [100/894] Elapsed 0m 28s (remain 3m 41s) Loss: 0.0013(0.0063) \n",
      "EVAL: [200/894] Elapsed 0m 56s (remain 3m 14s) Loss: 0.0179(0.0097) \n",
      "EVAL: [300/894] Elapsed 1m 24s (remain 2m 47s) Loss: 0.0135(0.0102) \n",
      "EVAL: [400/894] Elapsed 1m 53s (remain 2m 18s) Loss: 0.0010(0.0095) \n",
      "EVAL: [500/894] Elapsed 2m 21s (remain 1m 50s) Loss: 0.0062(0.0106) \n",
      "EVAL: [600/894] Elapsed 2m 48s (remain 1m 22s) Loss: 0.0105(0.0109) \n",
      "EVAL: [700/894] Elapsed 3m 16s (remain 0m 54s) Loss: 0.0000(0.0104) \n",
      "EVAL: [800/894] Elapsed 3m 43s (remain 0m 25s) Loss: 0.0017(0.0098) \n",
      "EVAL: [893/894] Elapsed 4m 9s (remain 0m 0s) Loss: 0.0168(0.0092) \n",
      "Epoch 3 - avg_train_loss: 0.0051  avg_val_loss: 0.0092  time: 1619s\n",
      "Epoch 3 - Score: 0.8732\n",
      "Epoch 3 - Save Best Score: 0.8732 Model\n",
      "Epoch: [4][0/2681] Elapsed 0m 0s (remain 42m 52s) Loss: 0.0150(0.0150) Grad: 25180.6660  LR: 0.000009  \n",
      "Epoch: [4][100/2681] Elapsed 0m 52s (remain 22m 33s) Loss: 0.0026(0.0036) Grad: 6600.5645  LR: 0.000009  \n",
      "Epoch: [4][200/2681] Elapsed 1m 43s (remain 21m 18s) Loss: 0.0000(0.0036) Grad: 13.9161  LR: 0.000009  \n",
      "Epoch: [4][300/2681] Elapsed 2m 35s (remain 20m 25s) Loss: 0.0013(0.0038) Grad: 8928.1836  LR: 0.000008  \n",
      "Epoch: [4][400/2681] Elapsed 3m 25s (remain 19m 27s) Loss: 0.0064(0.0038) Grad: 25029.5723  LR: 0.000008  \n",
      "Epoch: [4][500/2681] Elapsed 4m 16s (remain 18m 35s) Loss: 0.0039(0.0036) Grad: 10207.5117  LR: 0.000008  \n",
      "Epoch: [4][600/2681] Elapsed 5m 9s (remain 17m 52s) Loss: 0.0000(0.0038) Grad: 122.3607  LR: 0.000008  \n",
      "Epoch: [4][700/2681] Elapsed 6m 0s (remain 16m 57s) Loss: 0.0001(0.0036) Grad: 426.7279  LR: 0.000008  \n",
      "Epoch: [4][800/2681] Elapsed 6m 50s (remain 16m 4s) Loss: 0.0000(0.0039) Grad: 25.6735  LR: 0.000008  \n",
      "Epoch: [4][900/2681] Elapsed 7m 41s (remain 15m 12s) Loss: 0.0169(0.0038) Grad: 50603.5625  LR: 0.000007  \n",
      "Epoch: [4][1000/2681] Elapsed 8m 32s (remain 14m 20s) Loss: 0.0002(0.0040) Grad: 768.6313  LR: 0.000007  \n",
      "Epoch: [4][1100/2681] Elapsed 9m 23s (remain 13m 28s) Loss: 0.0042(0.0040) Grad: 6591.3530  LR: 0.000007  \n",
      "Epoch: [4][1200/2681] Elapsed 10m 13s (remain 12m 35s) Loss: 0.0105(0.0040) Grad: 24474.3008  LR: 0.000007  \n",
      "Epoch: [4][1300/2681] Elapsed 11m 3s (remain 11m 43s) Loss: 0.0031(0.0041) Grad: 10397.4355  LR: 0.000007  \n",
      "Epoch: [4][1400/2681] Elapsed 11m 53s (remain 10m 52s) Loss: 0.0001(0.0041) Grad: 475.6076  LR: 0.000007  \n",
      "Epoch: [4][1500/2681] Elapsed 12m 44s (remain 10m 0s) Loss: 0.0003(0.0042) Grad: 1443.7416  LR: 0.000006  \n",
      "Epoch: [4][1600/2681] Elapsed 13m 34s (remain 9m 9s) Loss: 0.0000(0.0042) Grad: 103.5581  LR: 0.000006  \n",
      "Epoch: [4][1700/2681] Elapsed 14m 25s (remain 8m 18s) Loss: 0.0024(0.0043) Grad: 8122.4917  LR: 0.000006  \n",
      "Epoch: [4][1800/2681] Elapsed 15m 16s (remain 7m 27s) Loss: 0.0021(0.0043) Grad: 6756.9004  LR: 0.000006  \n",
      "Epoch: [4][1900/2681] Elapsed 16m 6s (remain 6m 36s) Loss: 0.0000(0.0043) Grad: 127.0281  LR: 0.000006  \n",
      "Epoch: [4][2000/2681] Elapsed 16m 57s (remain 5m 45s) Loss: 0.0000(0.0042) Grad: 50.3847  LR: 0.000006  \n",
      "Epoch: [4][2100/2681] Elapsed 17m 48s (remain 4m 54s) Loss: 0.0004(0.0042) Grad: 4941.4844  LR: 0.000005  \n",
      "Epoch: [4][2200/2681] Elapsed 18m 38s (remain 4m 3s) Loss: 0.0007(0.0042) Grad: 5769.1899  LR: 0.000005  \n",
      "Epoch: [4][2300/2681] Elapsed 19m 29s (remain 3m 13s) Loss: 0.0003(0.0042) Grad: 2859.2463  LR: 0.000005  \n",
      "Epoch: [4][2400/2681] Elapsed 20m 19s (remain 2m 22s) Loss: 0.0000(0.0041) Grad: 145.8264  LR: 0.000005  \n",
      "Epoch: [4][2500/2681] Elapsed 21m 9s (remain 1m 31s) Loss: 0.0002(0.0041) Grad: 1564.1653  LR: 0.000005  \n",
      "Epoch: [4][2600/2681] Elapsed 21m 59s (remain 0m 40s) Loss: 0.0000(0.0041) Grad: 140.7668  LR: 0.000005  \n",
      "Epoch: [4][2680/2681] Elapsed 22m 39s (remain 0m 0s) Loss: 0.0000(0.0041) Grad: 145.2243  LR: 0.000004  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 9m 2s) Loss: 0.0122(0.0122) \n",
      "EVAL: [100/894] Elapsed 0m 28s (remain 3m 40s) Loss: 0.0006(0.0062) \n",
      "EVAL: [200/894] Elapsed 0m 56s (remain 3m 13s) Loss: 0.0162(0.0098) \n",
      "EVAL: [300/894] Elapsed 1m 23s (remain 2m 44s) Loss: 0.0163(0.0102) \n",
      "EVAL: [400/894] Elapsed 1m 50s (remain 2m 15s) Loss: 0.0013(0.0096) \n",
      "EVAL: [500/894] Elapsed 2m 18s (remain 1m 48s) Loss: 0.0100(0.0105) \n",
      "EVAL: [600/894] Elapsed 2m 45s (remain 1m 20s) Loss: 0.0140(0.0108) \n",
      "EVAL: [700/894] Elapsed 3m 13s (remain 0m 53s) Loss: 0.0000(0.0103) \n",
      "EVAL: [800/894] Elapsed 3m 41s (remain 0m 25s) Loss: 0.0008(0.0098) \n",
      "EVAL: [893/894] Elapsed 4m 6s (remain 0m 0s) Loss: 0.0158(0.0092) \n",
      "Epoch 4 - avg_train_loss: 0.0041  avg_val_loss: 0.0092  time: 1613s\n",
      "Epoch 4 - Score: 0.8755\n",
      "Epoch 4 - Save Best Score: 0.8755 Model\n",
      "Epoch: [5][0/2681] Elapsed 0m 0s (remain 42m 58s) Loss: 0.0003(0.0003) Grad: 3160.8787  LR: 0.000004  \n",
      "Epoch: [5][100/2681] Elapsed 0m 51s (remain 21m 57s) Loss: 0.0000(0.0025) Grad: 216.6166  LR: 0.000004  \n",
      "Epoch: [5][200/2681] Elapsed 1m 41s (remain 20m 57s) Loss: 0.0009(0.0027) Grad: 4093.8425  LR: 0.000004  \n",
      "Epoch: [5][300/2681] Elapsed 2m 32s (remain 20m 2s) Loss: 0.0010(0.0029) Grad: 5845.6533  LR: 0.000004  \n",
      "Epoch: [5][400/2681] Elapsed 3m 23s (remain 19m 15s) Loss: 0.0002(0.0029) Grad: 2261.6301  LR: 0.000004  \n",
      "Epoch: [5][500/2681] Elapsed 4m 13s (remain 18m 24s) Loss: 0.0035(0.0029) Grad: 5598.7256  LR: 0.000004  \n",
      "Epoch: [5][600/2681] Elapsed 5m 4s (remain 17m 32s) Loss: 0.0001(0.0029) Grad: 504.7846  LR: 0.000003  \n",
      "Epoch: [5][700/2681] Elapsed 5m 54s (remain 16m 41s) Loss: 0.0037(0.0031) Grad: 7450.4385  LR: 0.000003  \n",
      "Epoch: [5][800/2681] Elapsed 6m 44s (remain 15m 50s) Loss: 0.0029(0.0031) Grad: 16995.3301  LR: 0.000003  \n",
      "Epoch: [5][900/2681] Elapsed 7m 35s (remain 14m 59s) Loss: 0.0006(0.0031) Grad: 2405.3562  LR: 0.000003  \n",
      "Epoch: [5][1000/2681] Elapsed 8m 25s (remain 14m 8s) Loss: 0.0000(0.0031) Grad: 209.6266  LR: 0.000003  \n",
      "Epoch: [5][1100/2681] Elapsed 9m 16s (remain 13m 18s) Loss: 0.0000(0.0031) Grad: 22.3008  LR: 0.000003  \n",
      "Epoch: [5][1200/2681] Elapsed 10m 8s (remain 12m 29s) Loss: 0.0006(0.0032) Grad: 3788.7092  LR: 0.000002  \n",
      "Epoch: [5][1300/2681] Elapsed 10m 59s (remain 11m 39s) Loss: 0.0011(0.0032) Grad: 15526.5693  LR: 0.000002  \n",
      "Epoch: [5][1400/2681] Elapsed 11m 49s (remain 10m 48s) Loss: 0.0005(0.0032) Grad: 2266.7275  LR: 0.000002  \n",
      "Epoch: [5][1500/2681] Elapsed 12m 40s (remain 9m 57s) Loss: 0.0000(0.0033) Grad: 275.2299  LR: 0.000002  \n",
      "Epoch: [5][1600/2681] Elapsed 13m 30s (remain 9m 6s) Loss: 0.0051(0.0033) Grad: 40180.8047  LR: 0.000002  \n",
      "Epoch: [5][1700/2681] Elapsed 14m 21s (remain 8m 16s) Loss: 0.0043(0.0033) Grad: 12702.5410  LR: 0.000002  \n",
      "Epoch: [5][1800/2681] Elapsed 15m 12s (remain 7m 25s) Loss: 0.0017(0.0033) Grad: inf  LR: 0.000001  \n",
      "Epoch: [5][1900/2681] Elapsed 16m 3s (remain 6m 35s) Loss: 0.0002(0.0033) Grad: 1665.5736  LR: 0.000001  \n",
      "Epoch: [5][2000/2681] Elapsed 16m 54s (remain 5m 44s) Loss: 0.0000(0.0033) Grad: 6.1706  LR: 0.000001  \n",
      "Epoch: [5][2100/2681] Elapsed 17m 44s (remain 4m 53s) Loss: 0.0027(0.0033) Grad: 4137.9775  LR: 0.000001  \n",
      "Epoch: [5][2200/2681] Elapsed 18m 34s (remain 4m 3s) Loss: 0.0002(0.0033) Grad: 411.7551  LR: 0.000001  \n",
      "Epoch: [5][2300/2681] Elapsed 19m 24s (remain 3m 12s) Loss: 0.0000(0.0033) Grad: 14.6619  LR: 0.000001  \n",
      "Epoch: [5][2400/2681] Elapsed 20m 14s (remain 2m 21s) Loss: 0.0004(0.0033) Grad: 1140.2245  LR: 0.000000  \n",
      "Epoch: [5][2500/2681] Elapsed 21m 4s (remain 1m 31s) Loss: 0.0204(0.0033) Grad: 12823.8682  LR: 0.000000  \n",
      "Epoch: [5][2600/2681] Elapsed 21m 55s (remain 0m 40s) Loss: 0.0002(0.0033) Grad: 937.0837  LR: 0.000000  \n",
      "Epoch: [5][2680/2681] Elapsed 22m 36s (remain 0m 0s) Loss: 0.0057(0.0033) Grad: 8081.2871  LR: 0.000000  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 10m 22s) Loss: 0.0142(0.0142) \n",
      "EVAL: [100/894] Elapsed 0m 28s (remain 3m 43s) Loss: 0.0009(0.0066) \n",
      "EVAL: [200/894] Elapsed 0m 56s (remain 3m 14s) Loss: 0.0196(0.0108) \n",
      "EVAL: [300/894] Elapsed 1m 23s (remain 2m 45s) Loss: 0.0184(0.0114) \n",
      "EVAL: [400/894] Elapsed 1m 51s (remain 2m 16s) Loss: 0.0011(0.0106) \n",
      "EVAL: [500/894] Elapsed 2m 19s (remain 1m 49s) Loss: 0.0078(0.0117) \n",
      "EVAL: [600/894] Elapsed 2m 47s (remain 1m 21s) Loss: 0.0140(0.0119) \n",
      "EVAL: [700/894] Elapsed 3m 15s (remain 0m 53s) Loss: 0.0000(0.0112) \n",
      "EVAL: [800/894] Elapsed 3m 43s (remain 0m 25s) Loss: 0.0005(0.0107) \n",
      "EVAL: [893/894] Elapsed 4m 8s (remain 0m 0s) Loss: 0.0168(0.0100) \n",
      "Epoch 5 - avg_train_loss: 0.0033  avg_val_loss: 0.0100  time: 1612s\n",
      "Epoch 5 - Score: 0.8768\n",
      "Epoch 5 - Save Best Score: 0.8768 Model\n",
      "========== fold: 2 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2681] Elapsed 0m 0s (remain 42m 12s) Loss: 0.7083(0.7083) Grad: inf  LR: 0.000000  \n",
      "Epoch: [1][100/2681] Elapsed 0m 51s (remain 21m 51s) Loss: 0.3742(0.5466) Grad: 29342.8242  LR: 0.000002  \n",
      "Epoch: [1][200/2681] Elapsed 1m 42s (remain 20m 59s) Loss: 0.0732(0.4049) Grad: 2210.0725  LR: 0.000003  \n",
      "Epoch: [1][300/2681] Elapsed 2m 31s (remain 20m 0s) Loss: 0.0261(0.2847) Grad: 454.3371  LR: 0.000004  \n",
      "Epoch: [1][400/2681] Elapsed 3m 21s (remain 19m 5s) Loss: 0.0367(0.2239) Grad: 565.1055  LR: 0.000006  \n",
      "Epoch: [1][500/2681] Elapsed 4m 11s (remain 18m 15s) Loss: 0.0303(0.1867) Grad: 778.1650  LR: 0.000007  \n",
      "Epoch: [1][600/2681] Elapsed 5m 2s (remain 17m 28s) Loss: 0.0145(0.1611) Grad: 1131.9515  LR: 0.000009  \n",
      "Epoch: [1][700/2681] Elapsed 5m 53s (remain 16m 39s) Loss: 0.0357(0.1407) Grad: 2492.2798  LR: 0.000010  \n",
      "Epoch: [1][800/2681] Elapsed 6m 43s (remain 15m 47s) Loss: 0.0016(0.1253) Grad: 313.6262  LR: 0.000012  \n",
      "Epoch: [1][900/2681] Elapsed 7m 33s (remain 14m 56s) Loss: 0.0054(0.1128) Grad: 570.5558  LR: 0.000013  \n",
      "Epoch: [1][1000/2681] Elapsed 8m 24s (remain 14m 6s) Loss: 0.0103(0.1029) Grad: 2573.8523  LR: 0.000015  \n",
      "Epoch: [1][1100/2681] Elapsed 9m 14s (remain 13m 15s) Loss: 0.0067(0.0946) Grad: 996.8922  LR: 0.000016  \n",
      "Epoch: [1][1200/2681] Elapsed 10m 4s (remain 12m 24s) Loss: 0.0027(0.0875) Grad: 295.5681  LR: 0.000018  \n",
      "Epoch: [1][1300/2681] Elapsed 10m 54s (remain 11m 34s) Loss: 0.0626(0.0815) Grad: 3375.9570  LR: 0.000019  \n",
      "Epoch: [1][1400/2681] Elapsed 11m 44s (remain 10m 43s) Loss: 0.0039(0.0765) Grad: 501.4696  LR: 0.000020  \n",
      "Epoch: [1][1500/2681] Elapsed 12m 35s (remain 9m 53s) Loss: 0.0024(0.0720) Grad: 603.0493  LR: 0.000020  \n",
      "Epoch: [1][1600/2681] Elapsed 13m 25s (remain 9m 3s) Loss: 0.0002(0.0680) Grad: 36.7100  LR: 0.000020  \n",
      "Epoch: [1][1700/2681] Elapsed 14m 15s (remain 8m 12s) Loss: 0.0015(0.0645) Grad: 231.7474  LR: 0.000019  \n",
      "Epoch: [1][1800/2681] Elapsed 15m 5s (remain 7m 22s) Loss: 0.0008(0.0615) Grad: 124.3128  LR: 0.000019  \n",
      "Epoch: [1][1900/2681] Elapsed 15m 56s (remain 6m 32s) Loss: 0.0040(0.0587) Grad: 2892.2334  LR: 0.000019  \n",
      "Epoch: [1][2000/2681] Elapsed 16m 47s (remain 5m 42s) Loss: 0.0628(0.0561) Grad: 6782.6636  LR: 0.000019  \n",
      "Epoch: [1][2100/2681] Elapsed 17m 38s (remain 4m 52s) Loss: 0.0038(0.0538) Grad: 437.7999  LR: 0.000019  \n",
      "Epoch: [1][2200/2681] Elapsed 18m 28s (remain 4m 1s) Loss: 0.0051(0.0517) Grad: 960.4911  LR: 0.000019  \n",
      "Epoch: [1][2300/2681] Elapsed 19m 18s (remain 3m 11s) Loss: 0.0000(0.0498) Grad: 11.7272  LR: 0.000018  \n",
      "Epoch: [1][2400/2681] Elapsed 20m 10s (remain 2m 21s) Loss: 0.0023(0.0481) Grad: 255.2328  LR: 0.000018  \n",
      "Epoch: [1][2500/2681] Elapsed 21m 0s (remain 1m 30s) Loss: 0.0113(0.0465) Grad: 635.8961  LR: 0.000018  \n",
      "Epoch: [1][2600/2681] Elapsed 21m 50s (remain 0m 40s) Loss: 0.0120(0.0450) Grad: 1355.6097  LR: 0.000018  \n",
      "Epoch: [1][2680/2681] Elapsed 22m 30s (remain 0m 0s) Loss: 0.0081(0.0439) Grad: 2320.5144  LR: 0.000018  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 11m 36s) Loss: 0.0030(0.0030) \n",
      "EVAL: [100/894] Elapsed 0m 29s (remain 3m 48s) Loss: 0.0012(0.0066) \n",
      "EVAL: [200/894] Elapsed 0m 57s (remain 3m 18s) Loss: 0.0014(0.0062) \n",
      "EVAL: [300/894] Elapsed 1m 24s (remain 2m 47s) Loss: 0.0044(0.0069) \n",
      "EVAL: [400/894] Elapsed 1m 52s (remain 2m 18s) Loss: 0.0003(0.0063) \n",
      "EVAL: [500/894] Elapsed 2m 20s (remain 1m 50s) Loss: 0.0142(0.0070) \n",
      "EVAL: [600/894] Elapsed 2m 47s (remain 1m 21s) Loss: 0.0002(0.0074) \n",
      "EVAL: [700/894] Elapsed 3m 15s (remain 0m 53s) Loss: 0.0223(0.0075) \n",
      "EVAL: [800/894] Elapsed 3m 43s (remain 0m 25s) Loss: 0.0020(0.0071) \n",
      "EVAL: [893/894] Elapsed 4m 9s (remain 0m 0s) Loss: 0.0009(0.0068) \n",
      "Epoch 1 - avg_train_loss: 0.0439  avg_val_loss: 0.0068  time: 1607s\n",
      "Epoch 1 - Score: 0.8326\n",
      "Epoch 1 - Save Best Score: 0.8326 Model\n",
      "Epoch: [2][0/2681] Elapsed 0m 0s (remain 41m 1s) Loss: 0.0051(0.0051) Grad: 7017.1235  LR: 0.000018  \n",
      "Epoch: [2][100/2681] Elapsed 0m 51s (remain 22m 0s) Loss: 0.0003(0.0049) Grad: 1148.7946  LR: 0.000018  \n",
      "Epoch: [2][200/2681] Elapsed 1m 42s (remain 20m 59s) Loss: 0.0064(0.0053) Grad: 14743.6660  LR: 0.000017  \n",
      "Epoch: [2][300/2681] Elapsed 2m 32s (remain 20m 5s) Loss: 0.0021(0.0050) Grad: 11846.5039  LR: 0.000017  \n",
      "Epoch: [2][400/2681] Elapsed 3m 24s (remain 19m 20s) Loss: 0.0001(0.0051) Grad: 161.9371  LR: 0.000017  \n",
      "Epoch: [2][500/2681] Elapsed 4m 14s (remain 18m 26s) Loss: 0.0027(0.0053) Grad: 8865.9648  LR: 0.000017  \n",
      "Epoch: [2][600/2681] Elapsed 5m 4s (remain 17m 34s) Loss: 0.0000(0.0054) Grad: 173.6189  LR: 0.000017  \n",
      "Epoch: [2][700/2681] Elapsed 5m 56s (remain 16m 46s) Loss: 0.0071(0.0055) Grad: 18750.7871  LR: 0.000017  \n",
      "Epoch: [2][800/2681] Elapsed 6m 47s (remain 15m 55s) Loss: 0.0000(0.0057) Grad: 103.2402  LR: 0.000016  \n",
      "Epoch: [2][900/2681] Elapsed 7m 37s (remain 15m 4s) Loss: 0.0063(0.0056) Grad: 19777.8613  LR: 0.000016  \n",
      "Epoch: [2][1000/2681] Elapsed 8m 28s (remain 14m 13s) Loss: 0.0001(0.0057) Grad: 376.9829  LR: 0.000016  \n",
      "Epoch: [2][1100/2681] Elapsed 9m 19s (remain 13m 23s) Loss: 0.0006(0.0057) Grad: 4548.2178  LR: 0.000016  \n",
      "Epoch: [2][1200/2681] Elapsed 10m 11s (remain 12m 34s) Loss: 0.0003(0.0057) Grad: 2737.2212  LR: 0.000016  \n",
      "Epoch: [2][1300/2681] Elapsed 11m 5s (remain 11m 45s) Loss: 0.0018(0.0057) Grad: 5254.3594  LR: 0.000016  \n",
      "Epoch: [2][1400/2681] Elapsed 11m 56s (remain 10m 54s) Loss: 0.0002(0.0057) Grad: 1567.1774  LR: 0.000015  \n",
      "Epoch: [2][1500/2681] Elapsed 12m 46s (remain 10m 2s) Loss: 0.0021(0.0057) Grad: 12716.7559  LR: 0.000015  \n",
      "Epoch: [2][1600/2681] Elapsed 13m 36s (remain 9m 11s) Loss: 0.0011(0.0058) Grad: 4703.7891  LR: 0.000015  \n",
      "Epoch: [2][1700/2681] Elapsed 14m 27s (remain 8m 19s) Loss: 0.0177(0.0058) Grad: 17304.6055  LR: 0.000015  \n",
      "Epoch: [2][1800/2681] Elapsed 15m 20s (remain 7m 29s) Loss: 0.0037(0.0058) Grad: 8759.8516  LR: 0.000015  \n",
      "Epoch: [2][1900/2681] Elapsed 16m 13s (remain 6m 39s) Loss: 0.0001(0.0058) Grad: 207.6444  LR: 0.000015  \n",
      "Epoch: [2][2000/2681] Elapsed 17m 5s (remain 5m 48s) Loss: 0.0002(0.0057) Grad: 629.2953  LR: 0.000014  \n",
      "Epoch: [2][2100/2681] Elapsed 17m 56s (remain 4m 57s) Loss: 0.0147(0.0057) Grad: 15047.6094  LR: 0.000014  \n",
      "Epoch: [2][2200/2681] Elapsed 18m 46s (remain 4m 5s) Loss: 0.0006(0.0058) Grad: 11772.5820  LR: 0.000014  \n",
      "Epoch: [2][2300/2681] Elapsed 19m 38s (remain 3m 14s) Loss: 0.0000(0.0058) Grad: 30.7578  LR: 0.000014  \n",
      "Epoch: [2][2400/2681] Elapsed 20m 29s (remain 2m 23s) Loss: 0.0015(0.0058) Grad: 3880.5242  LR: 0.000014  \n",
      "Epoch: [2][2500/2681] Elapsed 21m 20s (remain 1m 32s) Loss: 0.0039(0.0058) Grad: 14006.0723  LR: 0.000014  \n",
      "Epoch: [2][2600/2681] Elapsed 22m 11s (remain 0m 40s) Loss: 0.0052(0.0058) Grad: 13452.3281  LR: 0.000013  \n",
      "Epoch: [2][2680/2681] Elapsed 22m 52s (remain 0m 0s) Loss: 0.0167(0.0058) Grad: 24296.3691  LR: 0.000013  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 11m 7s) Loss: 0.0019(0.0019) \n",
      "EVAL: [100/894] Elapsed 0m 28s (remain 3m 42s) Loss: 0.0025(0.0072) \n",
      "EVAL: [200/894] Elapsed 0m 56s (remain 3m 13s) Loss: 0.0002(0.0067) \n",
      "EVAL: [300/894] Elapsed 1m 23s (remain 2m 44s) Loss: 0.0007(0.0077) \n",
      "EVAL: [400/894] Elapsed 1m 51s (remain 2m 16s) Loss: 0.0004(0.0069) \n",
      "EVAL: [500/894] Elapsed 2m 18s (remain 1m 48s) Loss: 0.0018(0.0076) \n",
      "EVAL: [600/894] Elapsed 2m 46s (remain 1m 21s) Loss: 0.0000(0.0082) \n",
      "EVAL: [700/894] Elapsed 3m 14s (remain 0m 53s) Loss: 0.0236(0.0081) \n",
      "EVAL: [800/894] Elapsed 3m 42s (remain 0m 25s) Loss: 0.0019(0.0076) \n",
      "EVAL: [893/894] Elapsed 4m 7s (remain 0m 0s) Loss: 0.0001(0.0074) \n",
      "Epoch 2 - avg_train_loss: 0.0058  avg_val_loss: 0.0074  time: 1627s\n",
      "Epoch 2 - Score: 0.8828\n",
      "Epoch 2 - Save Best Score: 0.8828 Model\n",
      "Epoch: [3][0/2681] Elapsed 0m 0s (remain 42m 34s) Loss: 0.0008(0.0008) Grad: 2666.5442  LR: 0.000013  \n",
      "Epoch: [3][100/2681] Elapsed 0m 51s (remain 21m 52s) Loss: 0.0000(0.0037) Grad: 18.2811  LR: 0.000013  \n",
      "Epoch: [3][200/2681] Elapsed 1m 41s (remain 20m 53s) Loss: 0.0025(0.0045) Grad: 4732.7261  LR: 0.000013  \n",
      "Epoch: [3][300/2681] Elapsed 2m 32s (remain 20m 8s) Loss: 0.0000(0.0050) Grad: 57.5758  LR: 0.000013  \n",
      "Epoch: [3][400/2681] Elapsed 3m 23s (remain 19m 17s) Loss: 0.0132(0.0047) Grad: 13272.3760  LR: 0.000013  \n",
      "Epoch: [3][500/2681] Elapsed 4m 13s (remain 18m 24s) Loss: 0.0085(0.0048) Grad: 27794.9062  LR: 0.000013  \n",
      "Epoch: [3][600/2681] Elapsed 5m 4s (remain 17m 33s) Loss: 0.0005(0.0048) Grad: 3106.4475  LR: 0.000012  \n",
      "Epoch: [3][700/2681] Elapsed 5m 55s (remain 16m 44s) Loss: 0.0030(0.0046) Grad: 34434.5234  LR: 0.000012  \n",
      "Epoch: [3][800/2681] Elapsed 6m 46s (remain 15m 53s) Loss: 0.0002(0.0048) Grad: 1402.6224  LR: 0.000012  \n",
      "Epoch: [3][900/2681] Elapsed 7m 36s (remain 15m 2s) Loss: 0.0004(0.0048) Grad: 2227.3877  LR: 0.000012  \n",
      "Epoch: [3][1000/2681] Elapsed 8m 27s (remain 14m 11s) Loss: 0.0004(0.0047) Grad: 1482.3363  LR: 0.000012  \n",
      "Epoch: [3][1100/2681] Elapsed 9m 17s (remain 13m 20s) Loss: 0.0024(0.0047) Grad: 38465.7695  LR: 0.000012  \n",
      "Epoch: [3][1200/2681] Elapsed 10m 7s (remain 12m 29s) Loss: 0.0001(0.0047) Grad: 385.5105  LR: 0.000011  \n",
      "Epoch: [3][1300/2681] Elapsed 10m 58s (remain 11m 38s) Loss: 0.0085(0.0048) Grad: 13978.3467  LR: 0.000011  \n",
      "Epoch: [3][1400/2681] Elapsed 11m 50s (remain 10m 49s) Loss: 0.0000(0.0048) Grad: 78.5014  LR: 0.000011  \n",
      "Epoch: [3][1500/2681] Elapsed 12m 42s (remain 9m 59s) Loss: 0.0849(0.0049) Grad: 28727.2539  LR: 0.000011  \n",
      "Epoch: [3][1600/2681] Elapsed 13m 32s (remain 9m 8s) Loss: 0.0019(0.0048) Grad: 17626.7754  LR: 0.000011  \n",
      "Epoch: [3][1700/2681] Elapsed 14m 23s (remain 8m 17s) Loss: 0.0000(0.0049) Grad: 137.9294  LR: 0.000011  \n",
      "Epoch: [3][1800/2681] Elapsed 15m 14s (remain 7m 26s) Loss: 0.0040(0.0048) Grad: 11546.8027  LR: 0.000010  \n",
      "Epoch: [3][1900/2681] Elapsed 16m 5s (remain 6m 36s) Loss: 0.0000(0.0048) Grad: 58.4585  LR: 0.000010  \n",
      "Epoch: [3][2000/2681] Elapsed 16m 56s (remain 5m 45s) Loss: 0.0024(0.0048) Grad: 8178.7744  LR: 0.000010  \n",
      "Epoch: [3][2100/2681] Elapsed 17m 47s (remain 4m 54s) Loss: 0.0064(0.0048) Grad: 12637.1299  LR: 0.000010  \n",
      "Epoch: [3][2200/2681] Elapsed 18m 39s (remain 4m 4s) Loss: 0.0000(0.0048) Grad: 111.8471  LR: 0.000010  \n",
      "Epoch: [3][2300/2681] Elapsed 19m 30s (remain 3m 13s) Loss: 0.0069(0.0048) Grad: 15569.5781  LR: 0.000010  \n",
      "Epoch: [3][2400/2681] Elapsed 20m 20s (remain 2m 22s) Loss: 0.0001(0.0048) Grad: 469.5208  LR: 0.000009  \n",
      "Epoch: [3][2500/2681] Elapsed 21m 11s (remain 1m 31s) Loss: 0.0052(0.0049) Grad: 16524.7871  LR: 0.000009  \n",
      "Epoch: [3][2600/2681] Elapsed 22m 2s (remain 0m 40s) Loss: 0.0156(0.0049) Grad: 19649.1270  LR: 0.000009  \n",
      "Epoch: [3][2680/2681] Elapsed 22m 43s (remain 0m 0s) Loss: 0.0009(0.0049) Grad: 2197.0940  LR: 0.000009  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 10m 55s) Loss: 0.0007(0.0007) \n",
      "EVAL: [100/894] Elapsed 0m 28s (remain 3m 41s) Loss: 0.0002(0.0079) \n",
      "EVAL: [200/894] Elapsed 0m 56s (remain 3m 15s) Loss: 0.0002(0.0074) \n",
      "EVAL: [300/894] Elapsed 1m 24s (remain 2m 46s) Loss: 0.0003(0.0080) \n",
      "EVAL: [400/894] Elapsed 1m 52s (remain 2m 17s) Loss: 0.0001(0.0072) \n",
      "EVAL: [500/894] Elapsed 2m 19s (remain 1m 49s) Loss: 0.0012(0.0076) \n",
      "EVAL: [600/894] Elapsed 2m 47s (remain 1m 21s) Loss: 0.0000(0.0081) \n",
      "EVAL: [700/894] Elapsed 3m 15s (remain 0m 53s) Loss: 0.0257(0.0082) \n",
      "EVAL: [800/894] Elapsed 3m 43s (remain 0m 25s) Loss: 0.0007(0.0078) \n",
      "EVAL: [893/894] Elapsed 4m 8s (remain 0m 0s) Loss: 0.0000(0.0075) \n",
      "Epoch 3 - avg_train_loss: 0.0049  avg_val_loss: 0.0075  time: 1619s\n",
      "Epoch 3 - Score: 0.8870\n",
      "Epoch 3 - Save Best Score: 0.8870 Model\n",
      "Epoch: [4][0/2681] Elapsed 0m 0s (remain 43m 27s) Loss: 0.0020(0.0020) Grad: 5275.6772  LR: 0.000009  \n",
      "Epoch: [4][100/2681] Elapsed 0m 51s (remain 21m 56s) Loss: 0.0012(0.0028) Grad: 2292.1279  LR: 0.000009  \n",
      "Epoch: [4][200/2681] Elapsed 1m 41s (remain 20m 58s) Loss: 0.0021(0.0033) Grad: 13082.0605  LR: 0.000009  \n",
      "Epoch: [4][300/2681] Elapsed 2m 32s (remain 20m 6s) Loss: 0.0004(0.0033) Grad: 1348.5155  LR: 0.000008  \n",
      "Epoch: [4][400/2681] Elapsed 3m 24s (remain 19m 24s) Loss: 0.0002(0.0037) Grad: 4730.1670  LR: 0.000008  \n",
      "Epoch: [4][500/2681] Elapsed 4m 15s (remain 18m 32s) Loss: 0.0004(0.0037) Grad: 1539.5159  LR: 0.000008  \n",
      "Epoch: [4][600/2681] Elapsed 5m 6s (remain 17m 39s) Loss: 0.0002(0.0039) Grad: 1988.3324  LR: 0.000008  \n",
      "Epoch: [4][700/2681] Elapsed 5m 56s (remain 16m 47s) Loss: 0.0002(0.0038) Grad: 1792.7992  LR: 0.000008  \n",
      "Epoch: [4][800/2681] Elapsed 6m 46s (remain 15m 53s) Loss: 0.0000(0.0036) Grad: 74.8793  LR: 0.000008  \n",
      "Epoch: [4][900/2681] Elapsed 7m 36s (remain 15m 1s) Loss: 0.0311(0.0036) Grad: 41160.2773  LR: 0.000007  \n",
      "Epoch: [4][1000/2681] Elapsed 8m 27s (remain 14m 11s) Loss: 0.0075(0.0036) Grad: 11390.8096  LR: 0.000007  \n",
      "Epoch: [4][1100/2681] Elapsed 9m 17s (remain 13m 19s) Loss: 0.0003(0.0036) Grad: 3908.3218  LR: 0.000007  \n",
      "Epoch: [4][1200/2681] Elapsed 10m 7s (remain 12m 28s) Loss: 0.0009(0.0037) Grad: 5169.7231  LR: 0.000007  \n",
      "Epoch: [4][1300/2681] Elapsed 10m 57s (remain 11m 37s) Loss: 0.0006(0.0036) Grad: 5191.9331  LR: 0.000007  \n",
      "Epoch: [4][1400/2681] Elapsed 11m 47s (remain 10m 46s) Loss: 0.0002(0.0037) Grad: 961.6656  LR: 0.000007  \n",
      "Epoch: [4][1500/2681] Elapsed 12m 37s (remain 9m 55s) Loss: 0.0039(0.0038) Grad: 8290.6943  LR: 0.000006  \n",
      "Epoch: [4][1600/2681] Elapsed 13m 28s (remain 9m 5s) Loss: 0.0046(0.0037) Grad: 13925.7559  LR: 0.000006  \n",
      "Epoch: [4][1700/2681] Elapsed 14m 18s (remain 8m 14s) Loss: 0.0223(0.0037) Grad: 29986.2598  LR: 0.000006  \n",
      "Epoch: [4][1800/2681] Elapsed 15m 9s (remain 7m 24s) Loss: 0.0002(0.0037) Grad: 662.7544  LR: 0.000006  \n",
      "Epoch: [4][1900/2681] Elapsed 16m 0s (remain 6m 33s) Loss: 0.0230(0.0037) Grad: 53687.0117  LR: 0.000006  \n",
      "Epoch: [4][2000/2681] Elapsed 16m 50s (remain 5m 43s) Loss: 0.0038(0.0037) Grad: 12691.0410  LR: 0.000006  \n",
      "Epoch: [4][2100/2681] Elapsed 17m 40s (remain 4m 52s) Loss: 0.0000(0.0037) Grad: 12.4173  LR: 0.000005  \n",
      "Epoch: [4][2200/2681] Elapsed 18m 30s (remain 4m 2s) Loss: 0.0011(0.0037) Grad: 4008.1118  LR: 0.000005  \n",
      "Epoch: [4][2300/2681] Elapsed 19m 20s (remain 3m 11s) Loss: 0.0163(0.0037) Grad: 262481.7812  LR: 0.000005  \n",
      "Epoch: [4][2400/2681] Elapsed 20m 10s (remain 2m 21s) Loss: 0.0008(0.0037) Grad: 8044.2583  LR: 0.000005  \n",
      "Epoch: [4][2500/2681] Elapsed 21m 1s (remain 1m 30s) Loss: 0.0000(0.0037) Grad: 48.7566  LR: 0.000005  \n",
      "Epoch: [4][2600/2681] Elapsed 21m 51s (remain 0m 40s) Loss: 0.0013(0.0038) Grad: 13651.4346  LR: 0.000005  \n",
      "Epoch: [4][2680/2681] Elapsed 22m 31s (remain 0m 0s) Loss: 0.0002(0.0039) Grad: 1287.9067  LR: 0.000004  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 10m 29s) Loss: 0.0005(0.0005) \n",
      "EVAL: [100/894] Elapsed 0m 28s (remain 3m 42s) Loss: 0.0008(0.0095) \n",
      "EVAL: [200/894] Elapsed 0m 56s (remain 3m 13s) Loss: 0.0010(0.0091) \n",
      "EVAL: [300/894] Elapsed 1m 23s (remain 2m 44s) Loss: 0.0002(0.0094) \n",
      "EVAL: [400/894] Elapsed 1m 51s (remain 2m 17s) Loss: 0.0000(0.0084) \n",
      "EVAL: [500/894] Elapsed 2m 19s (remain 1m 49s) Loss: 0.0006(0.0089) \n",
      "EVAL: [600/894] Elapsed 2m 46s (remain 1m 21s) Loss: 0.0000(0.0095) \n",
      "EVAL: [700/894] Elapsed 3m 14s (remain 0m 53s) Loss: 0.0326(0.0096) \n",
      "EVAL: [800/894] Elapsed 3m 42s (remain 0m 25s) Loss: 0.0004(0.0090) \n",
      "EVAL: [893/894] Elapsed 4m 7s (remain 0m 0s) Loss: 0.0000(0.0087) \n",
      "Epoch 4 - avg_train_loss: 0.0039  avg_val_loss: 0.0087  time: 1606s\n",
      "Epoch 4 - Score: 0.8895\n",
      "Epoch 4 - Save Best Score: 0.8895 Model\n",
      "Epoch: [5][0/2681] Elapsed 0m 0s (remain 41m 4s) Loss: 0.0001(0.0001) Grad: 1247.3749  LR: 0.000004  \n",
      "Epoch: [5][100/2681] Elapsed 0m 50s (remain 21m 38s) Loss: 0.0006(0.0027) Grad: 8302.8389  LR: 0.000004  \n",
      "Epoch: [5][200/2681] Elapsed 1m 41s (remain 20m 47s) Loss: 0.0001(0.0028) Grad: 484.9234  LR: 0.000004  \n",
      "Epoch: [5][300/2681] Elapsed 2m 31s (remain 19m 59s) Loss: 0.0001(0.0024) Grad: 283.0042  LR: 0.000004  \n",
      "Epoch: [5][400/2681] Elapsed 3m 22s (remain 19m 10s) Loss: 0.0000(0.0028) Grad: 94.7038  LR: 0.000004  \n",
      "Epoch: [5][500/2681] Elapsed 4m 12s (remain 18m 19s) Loss: 0.0005(0.0028) Grad: 2561.6763  LR: 0.000004  \n",
      "Epoch: [5][600/2681] Elapsed 5m 3s (remain 17m 31s) Loss: 0.0000(0.0028) Grad: 56.0889  LR: 0.000003  \n",
      "Epoch: [5][700/2681] Elapsed 5m 54s (remain 16m 41s) Loss: 0.0002(0.0030) Grad: 2605.9014  LR: 0.000003  \n",
      "Epoch: [5][800/2681] Elapsed 6m 46s (remain 15m 53s) Loss: 0.0000(0.0029) Grad: 96.5931  LR: 0.000003  \n",
      "Epoch: [5][900/2681] Elapsed 7m 36s (remain 15m 2s) Loss: 0.0012(0.0033) Grad: 7928.1489  LR: 0.000003  \n",
      "Epoch: [5][1000/2681] Elapsed 8m 27s (remain 14m 11s) Loss: 0.0000(0.0032) Grad: 228.3996  LR: 0.000003  \n",
      "Epoch: [5][1100/2681] Elapsed 9m 18s (remain 13m 20s) Loss: 0.0002(0.0031) Grad: 6613.7461  LR: 0.000003  \n",
      "Epoch: [5][1200/2681] Elapsed 10m 10s (remain 12m 32s) Loss: 0.0024(0.0030) Grad: 60373.3242  LR: 0.000002  \n",
      "Epoch: [5][1300/2681] Elapsed 11m 0s (remain 11m 40s) Loss: 0.0000(0.0030) Grad: 169.5942  LR: 0.000002  \n",
      "Epoch: [5][1400/2681] Elapsed 11m 51s (remain 10m 49s) Loss: 0.0001(0.0030) Grad: 378.4330  LR: 0.000002  \n",
      "Epoch: [5][1500/2681] Elapsed 12m 41s (remain 9m 58s) Loss: 0.0000(0.0030) Grad: 248.3854  LR: 0.000002  \n",
      "Epoch: [5][1600/2681] Elapsed 13m 31s (remain 9m 7s) Loss: 0.0000(0.0030) Grad: 60.9114  LR: 0.000002  \n",
      "Epoch: [5][1700/2681] Elapsed 14m 21s (remain 8m 16s) Loss: 0.0030(0.0030) Grad: 13699.5508  LR: 0.000002  \n",
      "Epoch: [5][1800/2681] Elapsed 15m 11s (remain 7m 25s) Loss: 0.0140(0.0032) Grad: 54780.8281  LR: 0.000001  \n",
      "Epoch: [5][1900/2681] Elapsed 16m 2s (remain 6m 35s) Loss: 0.0002(0.0031) Grad: 1049.9384  LR: 0.000001  \n",
      "Epoch: [5][2000/2681] Elapsed 16m 53s (remain 5m 44s) Loss: 0.0026(0.0031) Grad: 8480.2666  LR: 0.000001  \n",
      "Epoch: [5][2100/2681] Elapsed 17m 44s (remain 4m 53s) Loss: 0.0001(0.0032) Grad: 619.5171  LR: 0.000001  \n",
      "Epoch: [5][2200/2681] Elapsed 18m 34s (remain 4m 3s) Loss: 0.0021(0.0032) Grad: 12431.2129  LR: 0.000001  \n",
      "Epoch: [5][2300/2681] Elapsed 19m 24s (remain 3m 12s) Loss: 0.0169(0.0032) Grad: 53409.8008  LR: 0.000001  \n",
      "Epoch: [5][2400/2681] Elapsed 20m 15s (remain 2m 21s) Loss: 0.0000(0.0032) Grad: 393.8716  LR: 0.000000  \n",
      "Epoch: [5][2500/2681] Elapsed 21m 5s (remain 1m 31s) Loss: 0.0000(0.0032) Grad: 161.5070  LR: 0.000000  \n",
      "Epoch: [5][2600/2681] Elapsed 21m 56s (remain 0m 40s) Loss: 0.0000(0.0032) Grad: 49.2957  LR: 0.000000  \n",
      "Epoch: [5][2680/2681] Elapsed 22m 36s (remain 0m 0s) Loss: 0.0000(0.0032) Grad: 18.7612  LR: 0.000000  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 10m 18s) Loss: 0.0003(0.0003) \n",
      "EVAL: [100/894] Elapsed 0m 28s (remain 3m 45s) Loss: 0.0003(0.0105) \n",
      "EVAL: [200/894] Elapsed 0m 56s (remain 3m 16s) Loss: 0.0011(0.0096) \n",
      "EVAL: [300/894] Elapsed 1m 24s (remain 2m 46s) Loss: 0.0001(0.0099) \n",
      "EVAL: [400/894] Elapsed 1m 52s (remain 2m 18s) Loss: 0.0001(0.0090) \n",
      "EVAL: [500/894] Elapsed 2m 20s (remain 1m 49s) Loss: 0.0004(0.0095) \n",
      "EVAL: [600/894] Elapsed 2m 47s (remain 1m 21s) Loss: 0.0000(0.0102) \n",
      "EVAL: [700/894] Elapsed 3m 15s (remain 0m 53s) Loss: 0.0305(0.0102) \n",
      "EVAL: [800/894] Elapsed 3m 44s (remain 0m 26s) Loss: 0.0005(0.0096) \n",
      "EVAL: [893/894] Elapsed 4m 9s (remain 0m 0s) Loss: 0.0000(0.0093) \n",
      "Epoch 5 - avg_train_loss: 0.0032  avg_val_loss: 0.0093  time: 1613s\n",
      "Epoch 5 - Score: 0.8875\n",
      "========== fold: 3 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2681] Elapsed 0m 0s (remain 44m 4s) Loss: 0.4698(0.4698) Grad: inf  LR: 0.000000  \n",
      "Epoch: [1][100/2681] Elapsed 0m 52s (remain 22m 20s) Loss: 0.2717(0.4976) Grad: 39937.7148  LR: 0.000002  \n",
      "Epoch: [1][200/2681] Elapsed 1m 45s (remain 21m 46s) Loss: 0.1517(0.3856) Grad: 14143.9912  LR: 0.000003  \n",
      "Epoch: [1][300/2681] Elapsed 2m 36s (remain 20m 37s) Loss: 0.0590(0.2744) Grad: 2096.6816  LR: 0.000004  \n",
      "Epoch: [1][400/2681] Elapsed 3m 26s (remain 19m 36s) Loss: 0.0315(0.2160) Grad: 1011.4329  LR: 0.000006  \n",
      "Epoch: [1][500/2681] Elapsed 4m 17s (remain 18m 41s) Loss: 0.0200(0.1803) Grad: 682.9472  LR: 0.000007  \n",
      "Epoch: [1][600/2681] Elapsed 5m 8s (remain 17m 47s) Loss: 0.0239(0.1560) Grad: 4044.9622  LR: 0.000009  \n",
      "Epoch: [1][700/2681] Elapsed 5m 59s (remain 16m 54s) Loss: 0.0218(0.1368) Grad: 4565.7471  LR: 0.000010  \n",
      "Epoch: [1][800/2681] Elapsed 6m 50s (remain 16m 3s) Loss: 0.0197(0.1217) Grad: 5539.9751  LR: 0.000012  \n",
      "Epoch: [1][900/2681] Elapsed 7m 42s (remain 15m 12s) Loss: 0.0237(0.1099) Grad: 8547.3486  LR: 0.000013  \n",
      "Epoch: [1][1000/2681] Elapsed 8m 33s (remain 14m 21s) Loss: 0.0045(0.1003) Grad: 1465.2113  LR: 0.000015  \n",
      "Epoch: [1][1100/2681] Elapsed 9m 23s (remain 13m 29s) Loss: 0.0216(0.0922) Grad: 2179.7393  LR: 0.000016  \n",
      "Epoch: [1][1200/2681] Elapsed 10m 14s (remain 12m 37s) Loss: 0.0067(0.0853) Grad: 3954.2312  LR: 0.000018  \n",
      "Epoch: [1][1300/2681] Elapsed 11m 5s (remain 11m 46s) Loss: 0.0241(0.0796) Grad: 2983.0808  LR: 0.000019  \n",
      "Epoch: [1][1400/2681] Elapsed 11m 56s (remain 10m 54s) Loss: 0.0072(0.0747) Grad: 1957.5247  LR: 0.000020  \n",
      "Epoch: [1][1500/2681] Elapsed 12m 47s (remain 10m 3s) Loss: 0.0046(0.0704) Grad: 1044.3856  LR: 0.000020  \n",
      "Epoch: [1][1600/2681] Elapsed 13m 37s (remain 9m 11s) Loss: 0.0083(0.0667) Grad: 2041.2430  LR: 0.000020  \n",
      "Epoch: [1][1700/2681] Elapsed 14m 28s (remain 8m 20s) Loss: 0.0015(0.0633) Grad: 1167.2681  LR: 0.000019  \n",
      "Epoch: [1][1800/2681] Elapsed 15m 19s (remain 7m 29s) Loss: 0.0044(0.0604) Grad: 1618.0437  LR: 0.000019  \n",
      "Epoch: [1][1900/2681] Elapsed 16m 11s (remain 6m 38s) Loss: 0.0057(0.0577) Grad: 1408.8380  LR: 0.000019  \n",
      "Epoch: [1][2000/2681] Elapsed 17m 2s (remain 5m 47s) Loss: 0.0018(0.0553) Grad: 505.4734  LR: 0.000019  \n",
      "Epoch: [1][2100/2681] Elapsed 17m 52s (remain 4m 56s) Loss: 0.0004(0.0530) Grad: 106.7782  LR: 0.000019  \n",
      "Epoch: [1][2200/2681] Elapsed 18m 42s (remain 4m 4s) Loss: 0.0105(0.0510) Grad: 4113.0112  LR: 0.000019  \n",
      "Epoch: [1][2300/2681] Elapsed 19m 33s (remain 3m 13s) Loss: 0.0001(0.0492) Grad: 34.6120  LR: 0.000018  \n",
      "Epoch: [1][2400/2681] Elapsed 20m 25s (remain 2m 22s) Loss: 0.0070(0.0475) Grad: 930.2937  LR: 0.000018  \n",
      "Epoch: [1][2500/2681] Elapsed 21m 16s (remain 1m 31s) Loss: 0.0034(0.0458) Grad: 741.8656  LR: 0.000018  \n",
      "Epoch: [1][2600/2681] Elapsed 22m 6s (remain 0m 40s) Loss: 0.0055(0.0444) Grad: 801.6642  LR: 0.000018  \n",
      "Epoch: [1][2680/2681] Elapsed 22m 46s (remain 0m 0s) Loss: 0.0055(0.0434) Grad: 1472.0880  LR: 0.000018  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 11m 12s) Loss: 0.0018(0.0018) \n",
      "EVAL: [100/894] Elapsed 0m 28s (remain 3m 43s) Loss: 0.0006(0.0051) \n",
      "EVAL: [200/894] Elapsed 0m 57s (remain 3m 16s) Loss: 0.0082(0.0059) \n",
      "EVAL: [300/894] Elapsed 1m 25s (remain 2m 48s) Loss: 0.0029(0.0064) \n",
      "EVAL: [400/894] Elapsed 1m 52s (remain 2m 18s) Loss: 0.0045(0.0059) \n",
      "EVAL: [500/894] Elapsed 2m 20s (remain 1m 50s) Loss: 0.0040(0.0066) \n",
      "EVAL: [600/894] Elapsed 2m 48s (remain 1m 22s) Loss: 0.0045(0.0069) \n",
      "EVAL: [700/894] Elapsed 3m 16s (remain 0m 54s) Loss: 0.0149(0.0069) \n",
      "EVAL: [800/894] Elapsed 3m 44s (remain 0m 26s) Loss: 0.0024(0.0067) \n",
      "EVAL: [893/894] Elapsed 4m 9s (remain 0m 0s) Loss: 0.0004(0.0065) \n",
      "Epoch 1 - avg_train_loss: 0.0434  avg_val_loss: 0.0065  time: 1623s\n",
      "Epoch 1 - Score: 0.8682\n",
      "Epoch 1 - Save Best Score: 0.8682 Model\n",
      "Epoch: [2][0/2681] Elapsed 0m 0s (remain 41m 35s) Loss: 0.0021(0.0021) Grad: 5088.1323  LR: 0.000018  \n",
      "Epoch: [2][100/2681] Elapsed 0m 52s (remain 22m 9s) Loss: 0.0136(0.0058) Grad: 7452.9087  LR: 0.000018  \n",
      "Epoch: [2][200/2681] Elapsed 1m 43s (remain 21m 10s) Loss: 0.0003(0.0054) Grad: 819.3502  LR: 0.000017  \n",
      "Epoch: [2][300/2681] Elapsed 2m 33s (remain 20m 15s) Loss: 0.0057(0.0057) Grad: 38406.8047  LR: 0.000017  \n",
      "Epoch: [2][400/2681] Elapsed 3m 24s (remain 19m 20s) Loss: 0.0135(0.0058) Grad: 25581.9102  LR: 0.000017  \n",
      "Epoch: [2][500/2681] Elapsed 4m 14s (remain 18m 27s) Loss: 0.0000(0.0059) Grad: 52.9044  LR: 0.000017  \n",
      "Epoch: [2][600/2681] Elapsed 5m 5s (remain 17m 37s) Loss: 0.0059(0.0069) Grad: 6864.8071  LR: 0.000017  \n",
      "Epoch: [2][700/2681] Elapsed 5m 56s (remain 16m 46s) Loss: 0.0001(0.0069) Grad: 222.6636  LR: 0.000017  \n",
      "Epoch: [2][800/2681] Elapsed 6m 46s (remain 15m 55s) Loss: 0.0021(0.0069) Grad: 7483.9043  LR: 0.000016  \n",
      "Epoch: [2][900/2681] Elapsed 7m 37s (remain 15m 3s) Loss: 0.0007(0.0068) Grad: 2104.5115  LR: 0.000016  \n",
      "Epoch: [2][1000/2681] Elapsed 8m 28s (remain 14m 12s) Loss: 0.0001(0.0067) Grad: 891.2007  LR: 0.000016  \n",
      "Epoch: [2][1100/2681] Elapsed 9m 19s (remain 13m 22s) Loss: 0.0004(0.0066) Grad: 1925.4276  LR: 0.000016  \n",
      "Epoch: [2][1200/2681] Elapsed 10m 11s (remain 12m 32s) Loss: 0.0252(0.0066) Grad: 23521.2109  LR: 0.000016  \n",
      "Epoch: [2][1300/2681] Elapsed 11m 1s (remain 11m 41s) Loss: 0.0006(0.0066) Grad: 2331.3970  LR: 0.000016  \n",
      "Epoch: [2][1400/2681] Elapsed 11m 53s (remain 10m 51s) Loss: 0.0003(0.0065) Grad: 1823.5320  LR: 0.000015  \n",
      "Epoch: [2][1500/2681] Elapsed 12m 44s (remain 10m 0s) Loss: 0.0045(0.0066) Grad: 24926.7480  LR: 0.000015  \n",
      "Epoch: [2][1600/2681] Elapsed 13m 35s (remain 9m 9s) Loss: 0.0035(0.0066) Grad: 13600.5166  LR: 0.000015  \n",
      "Epoch: [2][1700/2681] Elapsed 14m 25s (remain 8m 18s) Loss: 0.0042(0.0066) Grad: 28017.7207  LR: 0.000015  \n",
      "Epoch: [2][1800/2681] Elapsed 15m 16s (remain 7m 27s) Loss: 0.0011(0.0066) Grad: 4544.3843  LR: 0.000015  \n",
      "Epoch: [2][1900/2681] Elapsed 16m 7s (remain 6m 36s) Loss: 0.0038(0.0066) Grad: 9339.4209  LR: 0.000015  \n",
      "Epoch: [2][2000/2681] Elapsed 16m 57s (remain 5m 45s) Loss: 0.0006(0.0066) Grad: 8177.2651  LR: 0.000014  \n",
      "Epoch: [2][2100/2681] Elapsed 17m 48s (remain 4m 54s) Loss: 0.0002(0.0066) Grad: 933.7856  LR: 0.000014  \n",
      "Epoch: [2][2200/2681] Elapsed 18m 38s (remain 4m 4s) Loss: 0.0033(0.0066) Grad: 7352.4155  LR: 0.000014  \n",
      "Epoch: [2][2300/2681] Elapsed 19m 29s (remain 3m 13s) Loss: 0.0033(0.0066) Grad: 5177.7202  LR: 0.000014  \n",
      "Epoch: [2][2400/2681] Elapsed 20m 21s (remain 2m 22s) Loss: 0.0107(0.0067) Grad: 20427.1719  LR: 0.000014  \n",
      "Epoch: [2][2500/2681] Elapsed 21m 12s (remain 1m 31s) Loss: 0.0016(0.0066) Grad: 8565.7764  LR: 0.000014  \n",
      "Epoch: [2][2600/2681] Elapsed 22m 2s (remain 0m 40s) Loss: 0.0158(0.0066) Grad: 24463.4238  LR: 0.000013  \n",
      "Epoch: [2][2680/2681] Elapsed 22m 44s (remain 0m 0s) Loss: 0.0017(0.0066) Grad: 5779.4434  LR: 0.000013  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 10m 19s) Loss: 0.0011(0.0011) \n",
      "EVAL: [100/894] Elapsed 0m 28s (remain 3m 45s) Loss: 0.0003(0.0063) \n",
      "EVAL: [200/894] Elapsed 0m 56s (remain 3m 15s) Loss: 0.0110(0.0066) \n",
      "EVAL: [300/894] Elapsed 1m 24s (remain 2m 46s) Loss: 0.0026(0.0068) \n",
      "EVAL: [400/894] Elapsed 1m 52s (remain 2m 17s) Loss: 0.0039(0.0065) \n",
      "EVAL: [500/894] Elapsed 2m 19s (remain 1m 49s) Loss: 0.0076(0.0073) \n",
      "EVAL: [600/894] Elapsed 2m 46s (remain 1m 21s) Loss: 0.0063(0.0080) \n",
      "EVAL: [700/894] Elapsed 3m 14s (remain 0m 53s) Loss: 0.0275(0.0079) \n",
      "EVAL: [800/894] Elapsed 3m 42s (remain 0m 25s) Loss: 0.0015(0.0077) \n",
      "EVAL: [893/894] Elapsed 4m 7s (remain 0m 0s) Loss: 0.0000(0.0075) \n",
      "Epoch 2 - avg_train_loss: 0.0066  avg_val_loss: 0.0075  time: 1619s\n",
      "Epoch 2 - Score: 0.8777\n",
      "Epoch 2 - Save Best Score: 0.8777 Model\n",
      "Epoch: [3][0/2681] Elapsed 0m 0s (remain 44m 1s) Loss: 0.0031(0.0031) Grad: 7310.0654  LR: 0.000013  \n",
      "Epoch: [3][100/2681] Elapsed 0m 52s (remain 22m 14s) Loss: 0.0000(0.0070) Grad: 35.6969  LR: 0.000013  \n",
      "Epoch: [3][200/2681] Elapsed 1m 43s (remain 21m 18s) Loss: 0.0038(0.0051) Grad: 15455.2109  LR: 0.000013  \n",
      "Epoch: [3][300/2681] Elapsed 2m 34s (remain 20m 21s) Loss: 0.0056(0.0050) Grad: 8264.6221  LR: 0.000013  \n",
      "Epoch: [3][400/2681] Elapsed 3m 25s (remain 19m 27s) Loss: 0.0027(0.0048) Grad: 7908.7329  LR: 0.000013  \n",
      "Epoch: [3][500/2681] Elapsed 4m 16s (remain 18m 36s) Loss: 0.0030(0.0050) Grad: 7317.1602  LR: 0.000013  \n",
      "Epoch: [3][600/2681] Elapsed 5m 8s (remain 17m 47s) Loss: 0.0088(0.0053) Grad: 27713.0039  LR: 0.000012  \n",
      "Epoch: [3][700/2681] Elapsed 5m 59s (remain 16m 54s) Loss: 0.0013(0.0055) Grad: 7212.5322  LR: 0.000012  \n",
      "Epoch: [3][800/2681] Elapsed 6m 49s (remain 16m 1s) Loss: 0.0061(0.0054) Grad: 10628.9209  LR: 0.000012  \n",
      "Epoch: [3][900/2681] Elapsed 7m 39s (remain 15m 8s) Loss: 0.0020(0.0054) Grad: 7061.5820  LR: 0.000012  \n",
      "Epoch: [3][1000/2681] Elapsed 8m 30s (remain 14m 16s) Loss: 0.0092(0.0053) Grad: 18009.9570  LR: 0.000012  \n",
      "Epoch: [3][1100/2681] Elapsed 9m 20s (remain 13m 24s) Loss: 0.0060(0.0052) Grad: 27027.6406  LR: 0.000012  \n",
      "Epoch: [3][1200/2681] Elapsed 10m 11s (remain 12m 33s) Loss: 0.0345(0.0051) Grad: 96874.2891  LR: 0.000011  \n",
      "Epoch: [3][1300/2681] Elapsed 11m 1s (remain 11m 41s) Loss: 0.0010(0.0052) Grad: 6669.9692  LR: 0.000011  \n",
      "Epoch: [3][1400/2681] Elapsed 11m 52s (remain 10m 50s) Loss: 0.0013(0.0052) Grad: 3763.0659  LR: 0.000011  \n",
      "Epoch: [3][1500/2681] Elapsed 12m 42s (remain 9m 59s) Loss: 0.0000(0.0051) Grad: 384.4715  LR: 0.000011  \n",
      "Epoch: [3][1600/2681] Elapsed 13m 32s (remain 9m 8s) Loss: 0.0007(0.0050) Grad: 5565.1943  LR: 0.000011  \n",
      "Epoch: [3][1700/2681] Elapsed 14m 22s (remain 8m 17s) Loss: 0.0006(0.0050) Grad: 22722.2402  LR: 0.000011  \n",
      "Epoch: [3][1800/2681] Elapsed 15m 13s (remain 7m 26s) Loss: 0.0009(0.0050) Grad: 6776.1797  LR: 0.000010  \n",
      "Epoch: [3][1900/2681] Elapsed 16m 4s (remain 6m 35s) Loss: 0.0141(0.0052) Grad: 118142.5000  LR: 0.000010  \n",
      "Epoch: [3][2000/2681] Elapsed 16m 54s (remain 5m 44s) Loss: 0.0001(0.0051) Grad: 356.3899  LR: 0.000010  \n",
      "Epoch: [3][2100/2681] Elapsed 17m 45s (remain 4m 54s) Loss: 0.0026(0.0051) Grad: 13235.3613  LR: 0.000010  \n",
      "Epoch: [3][2200/2681] Elapsed 18m 35s (remain 4m 3s) Loss: 0.0005(0.0051) Grad: 2635.9604  LR: 0.000010  \n",
      "Epoch: [3][2300/2681] Elapsed 19m 25s (remain 3m 12s) Loss: 0.0001(0.0051) Grad: 443.9483  LR: 0.000010  \n",
      "Epoch: [3][2400/2681] Elapsed 20m 16s (remain 2m 21s) Loss: 0.0029(0.0051) Grad: 8449.6602  LR: 0.000009  \n",
      "Epoch: [3][2500/2681] Elapsed 21m 7s (remain 1m 31s) Loss: 0.0043(0.0052) Grad: 50979.9961  LR: 0.000009  \n",
      "Epoch: [3][2600/2681] Elapsed 21m 57s (remain 0m 40s) Loss: 0.0009(0.0053) Grad: 2091.4868  LR: 0.000009  \n",
      "Epoch: [3][2680/2681] Elapsed 22m 38s (remain 0m 0s) Loss: 0.0011(0.0052) Grad: 6803.6973  LR: 0.000009  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 9m 12s) Loss: 0.0005(0.0005) \n",
      "EVAL: [100/894] Elapsed 0m 28s (remain 3m 42s) Loss: 0.0002(0.0059) \n",
      "EVAL: [200/894] Elapsed 0m 56s (remain 3m 13s) Loss: 0.0093(0.0066) \n",
      "EVAL: [300/894] Elapsed 1m 24s (remain 2m 45s) Loss: 0.0038(0.0068) \n",
      "EVAL: [400/894] Elapsed 1m 51s (remain 2m 17s) Loss: 0.0032(0.0064) \n",
      "EVAL: [500/894] Elapsed 2m 18s (remain 1m 48s) Loss: 0.0067(0.0072) \n",
      "EVAL: [600/894] Elapsed 2m 46s (remain 1m 21s) Loss: 0.0050(0.0079) \n",
      "EVAL: [700/894] Elapsed 3m 14s (remain 0m 53s) Loss: 0.0024(0.0078) \n",
      "EVAL: [800/894] Elapsed 3m 42s (remain 0m 25s) Loss: 0.0014(0.0076) \n",
      "EVAL: [893/894] Elapsed 4m 7s (remain 0m 0s) Loss: 0.0000(0.0075) \n",
      "Epoch 3 - avg_train_loss: 0.0052  avg_val_loss: 0.0075  time: 1613s\n",
      "Epoch 3 - Score: 0.8816\n",
      "Epoch 3 - Save Best Score: 0.8816 Model\n",
      "Epoch: [4][0/2681] Elapsed 0m 0s (remain 42m 8s) Loss: 0.0050(0.0050) Grad: 11241.0068  LR: 0.000009  \n",
      "Epoch: [4][100/2681] Elapsed 0m 51s (remain 21m 56s) Loss: 0.0027(0.0047) Grad: 4687.1348  LR: 0.000009  \n",
      "Epoch: [4][200/2681] Elapsed 1m 42s (remain 21m 4s) Loss: 0.0437(0.0047) Grad: 34782.7656  LR: 0.000009  \n",
      "Epoch: [4][300/2681] Elapsed 2m 33s (remain 20m 14s) Loss: 0.0000(0.0044) Grad: 145.5444  LR: 0.000008  \n",
      "Epoch: [4][400/2681] Elapsed 3m 24s (remain 19m 24s) Loss: 0.0000(0.0040) Grad: 16.7278  LR: 0.000008  \n",
      "Epoch: [4][500/2681] Elapsed 4m 15s (remain 18m 31s) Loss: 0.0001(0.0040) Grad: 529.6622  LR: 0.000008  \n",
      "Epoch: [4][600/2681] Elapsed 5m 5s (remain 17m 37s) Loss: 0.0065(0.0038) Grad: 9970.2314  LR: 0.000008  \n",
      "Epoch: [4][700/2681] Elapsed 5m 56s (remain 16m 45s) Loss: 0.0000(0.0038) Grad: 95.2449  LR: 0.000008  \n",
      "Epoch: [4][800/2681] Elapsed 6m 46s (remain 15m 53s) Loss: 0.0012(0.0041) Grad: 9438.9619  LR: 0.000008  \n",
      "Epoch: [4][900/2681] Elapsed 7m 36s (remain 15m 2s) Loss: 0.0004(0.0042) Grad: 3278.8684  LR: 0.000007  \n",
      "Epoch: [4][1000/2681] Elapsed 8m 27s (remain 14m 12s) Loss: 0.0002(0.0042) Grad: 916.0433  LR: 0.000007  \n",
      "Epoch: [4][1100/2681] Elapsed 9m 20s (remain 13m 24s) Loss: 0.0070(0.0040) Grad: 14011.9629  LR: 0.000007  \n",
      "Epoch: [4][1200/2681] Elapsed 10m 10s (remain 12m 32s) Loss: 0.0000(0.0041) Grad: 99.0741  LR: 0.000007  \n",
      "Epoch: [4][1300/2681] Elapsed 11m 1s (remain 11m 41s) Loss: 0.0015(0.0041) Grad: 4463.0908  LR: 0.000007  \n",
      "Epoch: [4][1400/2681] Elapsed 11m 51s (remain 10m 49s) Loss: 0.0008(0.0040) Grad: 6878.1313  LR: 0.000007  \n",
      "Epoch: [4][1500/2681] Elapsed 12m 41s (remain 9m 58s) Loss: 0.0000(0.0041) Grad: 59.5502  LR: 0.000006  \n",
      "Epoch: [4][1600/2681] Elapsed 13m 32s (remain 9m 8s) Loss: 0.0005(0.0040) Grad: 4505.2715  LR: 0.000006  \n",
      "Epoch: [4][1700/2681] Elapsed 14m 22s (remain 8m 17s) Loss: 0.0000(0.0041) Grad: 70.7590  LR: 0.000006  \n",
      "Epoch: [4][1800/2681] Elapsed 15m 13s (remain 7m 26s) Loss: 0.0067(0.0041) Grad: 9327.9443  LR: 0.000006  \n",
      "Epoch: [4][1900/2681] Elapsed 16m 4s (remain 6m 35s) Loss: 0.0000(0.0041) Grad: 147.4801  LR: 0.000006  \n",
      "Epoch: [4][2000/2681] Elapsed 16m 56s (remain 5m 45s) Loss: 0.0011(0.0042) Grad: 3567.5090  LR: 0.000006  \n",
      "Epoch: [4][2100/2681] Elapsed 17m 46s (remain 4m 54s) Loss: 0.0000(0.0042) Grad: 187.1664  LR: 0.000005  \n",
      "Epoch: [4][2200/2681] Elapsed 18m 37s (remain 4m 3s) Loss: 0.0119(0.0043) Grad: 50480.5039  LR: 0.000005  \n",
      "Epoch: [4][2300/2681] Elapsed 19m 28s (remain 3m 13s) Loss: 0.0010(0.0043) Grad: 3744.1821  LR: 0.000005  \n",
      "Epoch: [4][2400/2681] Elapsed 20m 19s (remain 2m 22s) Loss: 0.0000(0.0043) Grad: 80.1469  LR: 0.000005  \n",
      "Epoch: [4][2500/2681] Elapsed 21m 9s (remain 1m 31s) Loss: 0.0028(0.0043) Grad: 7110.9238  LR: 0.000005  \n",
      "Epoch: [4][2600/2681] Elapsed 22m 0s (remain 0m 40s) Loss: 0.0034(0.0043) Grad: 10006.4277  LR: 0.000005  \n",
      "Epoch: [4][2680/2681] Elapsed 22m 40s (remain 0m 0s) Loss: 0.0014(0.0043) Grad: 6040.2163  LR: 0.000004  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 11m 3s) Loss: 0.0009(0.0009) \n",
      "EVAL: [100/894] Elapsed 0m 28s (remain 3m 42s) Loss: 0.0002(0.0059) \n",
      "EVAL: [200/894] Elapsed 0m 56s (remain 3m 13s) Loss: 0.0134(0.0067) \n",
      "EVAL: [300/894] Elapsed 1m 23s (remain 2m 45s) Loss: 0.0051(0.0070) \n",
      "EVAL: [400/894] Elapsed 1m 51s (remain 2m 17s) Loss: 0.0037(0.0067) \n",
      "EVAL: [500/894] Elapsed 2m 19s (remain 1m 49s) Loss: 0.0074(0.0076) \n",
      "EVAL: [600/894] Elapsed 2m 46s (remain 1m 21s) Loss: 0.0070(0.0084) \n",
      "EVAL: [700/894] Elapsed 3m 14s (remain 0m 53s) Loss: 0.0043(0.0084) \n",
      "EVAL: [800/894] Elapsed 3m 42s (remain 0m 25s) Loss: 0.0022(0.0083) \n",
      "EVAL: [893/894] Elapsed 4m 7s (remain 0m 0s) Loss: 0.0000(0.0081) \n",
      "Epoch 4 - avg_train_loss: 0.0043  avg_val_loss: 0.0081  time: 1615s\n",
      "Epoch 4 - Score: 0.8855\n",
      "Epoch 4 - Save Best Score: 0.8855 Model\n",
      "Epoch: [5][0/2681] Elapsed 0m 0s (remain 42m 42s) Loss: 0.0004(0.0004) Grad: 4865.4946  LR: 0.000004  \n",
      "Epoch: [5][100/2681] Elapsed 0m 52s (remain 22m 12s) Loss: 0.0012(0.0024) Grad: 22102.9980  LR: 0.000004  \n",
      "Epoch: [5][200/2681] Elapsed 1m 43s (remain 21m 13s) Loss: 0.0050(0.0023) Grad: 30421.2148  LR: 0.000004  \n",
      "Epoch: [5][300/2681] Elapsed 2m 34s (remain 20m 20s) Loss: 0.0000(0.0024) Grad: 109.6933  LR: 0.000004  \n",
      "Epoch: [5][400/2681] Elapsed 3m 24s (remain 19m 24s) Loss: 0.0203(0.0025) Grad: 47302.2656  LR: 0.000004  \n",
      "Epoch: [5][500/2681] Elapsed 4m 15s (remain 18m 33s) Loss: 0.0011(0.0027) Grad: 8060.6187  LR: 0.000004  \n",
      "Epoch: [5][600/2681] Elapsed 5m 5s (remain 17m 38s) Loss: 0.0004(0.0036) Grad: 1685.4335  LR: 0.000003  \n",
      "Epoch: [5][700/2681] Elapsed 5m 56s (remain 16m 45s) Loss: 0.0000(0.0036) Grad: 70.7518  LR: 0.000003  \n",
      "Epoch: [5][800/2681] Elapsed 6m 46s (remain 15m 53s) Loss: 0.0001(0.0035) Grad: 440.1819  LR: 0.000003  \n",
      "Epoch: [5][900/2681] Elapsed 7m 36s (remain 15m 1s) Loss: 0.0001(0.0035) Grad: 888.6223  LR: 0.000003  \n",
      "Epoch: [5][1000/2681] Elapsed 8m 26s (remain 14m 10s) Loss: 0.0060(0.0034) Grad: 8948.9043  LR: 0.000003  \n",
      "Epoch: [5][1100/2681] Elapsed 9m 17s (remain 13m 19s) Loss: 0.0000(0.0034) Grad: 16.4578  LR: 0.000003  \n",
      "Epoch: [5][1200/2681] Elapsed 10m 6s (remain 12m 27s) Loss: 0.0003(0.0033) Grad: 6442.3794  LR: 0.000002  \n",
      "Epoch: [5][1300/2681] Elapsed 10m 57s (remain 11m 37s) Loss: 0.0015(0.0034) Grad: 6096.5156  LR: 0.000002  \n",
      "Epoch: [5][1400/2681] Elapsed 11m 47s (remain 10m 46s) Loss: 0.0004(0.0034) Grad: 9288.2871  LR: 0.000002  \n",
      "Epoch: [5][1500/2681] Elapsed 12m 37s (remain 9m 55s) Loss: 0.0000(0.0033) Grad: 434.9988  LR: 0.000002  \n",
      "Epoch: [5][1600/2681] Elapsed 13m 27s (remain 9m 4s) Loss: 0.0012(0.0033) Grad: 12081.7041  LR: 0.000002  \n",
      "Epoch: [5][1700/2681] Elapsed 14m 18s (remain 8m 14s) Loss: 0.0001(0.0034) Grad: 3012.5110  LR: 0.000002  \n",
      "Epoch: [5][1800/2681] Elapsed 15m 9s (remain 7m 24s) Loss: 0.0000(0.0033) Grad: 69.3666  LR: 0.000001  \n",
      "Epoch: [5][1900/2681] Elapsed 15m 59s (remain 6m 33s) Loss: 0.0053(0.0034) Grad: 8959.7549  LR: 0.000001  \n",
      "Epoch: [5][2000/2681] Elapsed 16m 49s (remain 5m 43s) Loss: 0.0000(0.0034) Grad: 206.8429  LR: 0.000001  \n",
      "Epoch: [5][2100/2681] Elapsed 17m 40s (remain 4m 52s) Loss: 0.0037(0.0035) Grad: 27328.4961  LR: 0.000001  \n",
      "Epoch: [5][2200/2681] Elapsed 18m 31s (remain 4m 2s) Loss: 0.0006(0.0035) Grad: 4455.1484  LR: 0.000001  \n",
      "Epoch: [5][2300/2681] Elapsed 19m 21s (remain 3m 11s) Loss: 0.0001(0.0036) Grad: 2127.9409  LR: 0.000001  \n",
      "Epoch: [5][2400/2681] Elapsed 20m 12s (remain 2m 21s) Loss: 0.0000(0.0036) Grad: 24.2639  LR: 0.000000  \n",
      "Epoch: [5][2500/2681] Elapsed 21m 2s (remain 1m 30s) Loss: 0.0006(0.0036) Grad: 4314.8081  LR: 0.000000  \n",
      "Epoch: [5][2600/2681] Elapsed 21m 52s (remain 0m 40s) Loss: 0.0098(0.0036) Grad: 13963.0439  LR: 0.000000  \n",
      "Epoch: [5][2680/2681] Elapsed 22m 32s (remain 0m 0s) Loss: 0.0024(0.0037) Grad: 12966.5537  LR: 0.000000  \n",
      "EVAL: [0/894] Elapsed 0m 0s (remain 9m 28s) Loss: 0.0005(0.0005) \n",
      "EVAL: [100/894] Elapsed 0m 28s (remain 3m 42s) Loss: 0.0001(0.0065) \n",
      "EVAL: [200/894] Elapsed 0m 56s (remain 3m 13s) Loss: 0.0140(0.0073) \n",
      "EVAL: [300/894] Elapsed 1m 24s (remain 2m 45s) Loss: 0.0040(0.0075) \n",
      "EVAL: [400/894] Elapsed 1m 51s (remain 2m 16s) Loss: 0.0027(0.0072) \n",
      "EVAL: [500/894] Elapsed 2m 18s (remain 1m 48s) Loss: 0.0090(0.0082) \n",
      "EVAL: [600/894] Elapsed 2m 46s (remain 1m 21s) Loss: 0.0077(0.0091) \n",
      "EVAL: [700/894] Elapsed 3m 14s (remain 0m 53s) Loss: 0.0052(0.0091) \n",
      "EVAL: [800/894] Elapsed 3m 42s (remain 0m 25s) Loss: 0.0021(0.0089) \n",
      "EVAL: [893/894] Elapsed 4m 7s (remain 0m 0s) Loss: 0.0000(0.0087) \n",
      "Epoch 5 - avg_train_loss: 0.0037  avg_val_loss: 0.0087  time: 1607s\n",
      "Epoch 5 - Score: 0.8850\n",
      "Best thres: 0.5, Score: 0.8836\n",
      "Best thres: 0.5296875, Score: 0.8837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2931d50984614b8985c906871ffa8032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c6816aaf4f4188a8c4e92da445358f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53f713f428f4c0ca1d37e8132544762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694503861964458c82975fd0cd41012f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "name": "nbme-exp043.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 641.572862,
   "end_time": "2022-02-27T11:39:50.972497",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-27T11:29:09.399635",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0b503e832e57492291cbf6e9ae66e343": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0c70395dca6341349fc883dc6b95090a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a92171cb3d34fc8b1ed5119e32951ac",
      "max": 42146,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a202501b76a748fe80180604dff32c7b",
      "value": 42146
     }
    },
    "0dfe4d3aa0354d95bb5d019420b510a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3d9ac191d9b4772ae4bca323a34ddd7",
      "placeholder": "​",
      "style": "IPY_MODEL_9deae6afbd4740df8eb0289bc5f53ae7",
      "value": " 42146/42146 [00:35&lt;00:00, 2027.81it/s]"
     }
    },
    "14efac00edd349898e9fa95a63a2773d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f6c8df95c7845818253189f2e365ba9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33e4f48f7c8f40f48081c34bc992f215": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40ae5d1c9e9f4feaa4207599ef17a1ec",
      "placeholder": "​",
      "style": "IPY_MODEL_8cd3352e5e9342e4a814e9958ea6dc1d",
      "value": "100%"
     }
    },
    "40ae5d1c9e9f4feaa4207599ef17a1ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "47e8d82f628f4bd7b8b0ef0aa96852a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cc9a4ba21d664dafb8ae32a4e0a1c5e0",
       "IPY_MODEL_f4421d3b3a844dbcb003f11902ee1898",
       "IPY_MODEL_d02f186539954463873bb560b775894e"
      ],
      "layout": "IPY_MODEL_14efac00edd349898e9fa95a63a2773d"
     }
    },
    "5a92171cb3d34fc8b1ed5119e32951ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e29db6be6284caa978ee223047f23c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "625abc68d2fb4fd4b8556c7cc1ae514a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83d1b90076dc431893e0ab1c87e35f9c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c7eb508782d4253af8cbff0a39e5d19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_33e4f48f7c8f40f48081c34bc992f215",
       "IPY_MODEL_0c70395dca6341349fc883dc6b95090a",
       "IPY_MODEL_0dfe4d3aa0354d95bb5d019420b510a9"
      ],
      "layout": "IPY_MODEL_e6775e6fc2ad4b14b473b8a07e27426d"
     }
    },
    "8cd3352e5e9342e4a814e9958ea6dc1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9deae6afbd4740df8eb0289bc5f53ae7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a202501b76a748fe80180604dff32c7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c3d9ac191d9b4772ae4bca323a34ddd7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc9a4ba21d664dafb8ae32a4e0a1c5e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83d1b90076dc431893e0ab1c87e35f9c",
      "placeholder": "​",
      "style": "IPY_MODEL_0b503e832e57492291cbf6e9ae66e343",
      "value": "100%"
     }
    },
    "d02f186539954463873bb560b775894e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f6c8df95c7845818253189f2e365ba9",
      "placeholder": "​",
      "style": "IPY_MODEL_5e29db6be6284caa978ee223047f23c5",
      "value": " 143/143 [00:00&lt;00:00, 2361.05it/s]"
     }
    },
    "d2581946e9fd4f9a8dc56b3453cc70bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e6775e6fc2ad4b14b473b8a07e27426d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4421d3b3a844dbcb003f11902ee1898": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_625abc68d2fb4fd4b8556c7cc1ae514a",
      "max": 143,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d2581946e9fd4f9a8dc56b3453cc70bd",
      "value": 143
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
