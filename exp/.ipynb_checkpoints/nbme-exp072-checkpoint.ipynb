{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fitted-cooking",
   "metadata": {
    "id": "colored-security"
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-spoke",
   "metadata": {
    "id": "educational-operator"
   },
   "source": [
    "- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-enhancement",
   "metadata": {
    "id": "incorrect-greek"
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sealed-scroll",
   "metadata": {
    "id": "alive-granny"
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"nbme-exp072\"\n",
    "ENV = \"local\"\n",
    "DEBUG_MODE = False\n",
    "SUBMISSION_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "strange-organ",
   "metadata": {
    "id": "heavy-prophet"
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    env=ENV\n",
    "    exp_name=EXP_NAME\n",
    "    debug=DEBUG_MODE\n",
    "    submission=SUBMISSION_MODE\n",
    "    apex=True\n",
    "    input_dir=None\n",
    "    output_dir=None\n",
    "    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n",
    "    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n",
    "    competition_name=\"nbme-score-clinical-patient-notes\"\n",
    "    id_col=\"id\"\n",
    "    target_col=\"location\"\n",
    "    pretrained_model_name=\"microsoft/deberta-large\"\n",
    "    tokenizer=None\n",
    "    max_len=None\n",
    "    max_char_len=None\n",
    "    pseudo_plain_path='../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl'\n",
    "    n_pseudo_labels=100000\n",
    "    output_dim=1\n",
    "    dropout=0.2\n",
    "    num_workers=4\n",
    "    batch_size=3\n",
    "    lr=2e-5\n",
    "    betas=(0.9, 0.98)\n",
    "    weight_decay=0.1\n",
    "    num_warmup_steps_rate=0.1\n",
    "    batch_scheduler=True\n",
    "    epochs=1\n",
    "    n_fold=4\n",
    "    train_fold=[0, 1, 2, 3]  # [0, 1, 2, 3]\n",
    "    seed=71\n",
    "    gradient_accumulation_steps=2\n",
    "    max_grad_norm=1000\n",
    "    print_freq=100\n",
    "    train=True\n",
    "    inference=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "designed-orleans",
   "metadata": {
    "id": "vocational-coating"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.train_fold = [0, 1]\n",
    "\n",
    "if CFG.submission:\n",
    "    CFG.train = False\n",
    "    CFG.inference = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-processor",
   "metadata": {
    "id": "private-moderator"
   },
   "source": [
    "## Directory Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "detected-session",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "married-tokyo",
    "outputId": "9c0fba66-759b-4354-898f-1afb47256d96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "print(CFG.env)\n",
    "if CFG.env == \"colab\":\n",
    "    # colab環境\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "    # install packages\n",
    "    !pip install transformers==4.16.2\n",
    "\n",
    "elif CFG.env == \"local\":\n",
    "    # ローカルサーバ\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "\n",
    "elif CFG.env == \"kaggle\":\n",
    "    # kaggle環境\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dutch-harrison",
   "metadata": {
    "id": "blank-pierre"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import BartModel,BertModel,BertTokenizer\n",
    "from transformers import DebertaModel,DebertaTokenizer\n",
    "from transformers import RobertaModel,RobertaTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-italy",
   "metadata": {
    "id": "sound-still"
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "positive-blogger",
   "metadata": {
    "id": "surprised-commercial"
   },
   "outputs": [],
   "source": [
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "opposite-broadcasting",
   "metadata": {
    "id": "interstate-accident"
   },
   "outputs": [],
   "source": [
    "def create_labels_for_scoring(df):\n",
    "    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n",
    "    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, \"location\"]\n",
    "        if lst:\n",
    "            new_lst = \";\".join(lst)\n",
    "            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n",
    "\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df[\"location_for_create_labels\"].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, token_probs, tokenizer):\n",
    "    res = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n",
    "        encoded = tokenizer(\n",
    "            text=text,\n",
    "            max_length=CFG.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n",
    "            start, end = offset_mapping\n",
    "            res[i][start:end] = pred\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_predicted_location_str(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        # result = np.where(char_prob >= th)[0] + 1\n",
    "        result = np.where(char_prob >= th)[0]\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        # result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = [f\"{min(r)} {max(r) + 1}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def scoring(df, th=0.5, use_token_prob=True):\n",
    "    labels = create_labels_for_scoring(df)\n",
    "\n",
    "    if use_token_prob:\n",
    "        token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n",
    "        char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n",
    "    else:\n",
    "        char_probs = df[[str(i) for i in range(CFG.max_char_len)]].values\n",
    "        char_probs = [char_probs[i] for i in range(len(char_probs))]\n",
    "\n",
    "    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n",
    "    preds = get_predictions(predicted_location_str)\n",
    "\n",
    "    score = get_score(labels, preds)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_best_thres(oof_df):\n",
    "    def f1_opt(x):\n",
    "        return -1 * scoring(oof_df, th=x)\n",
    "\n",
    "    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n",
    "    return best_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "noted-silver",
   "metadata": {
    "id": "coated-pioneer"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "rubber-liberty",
   "metadata": {
    "id": "nervous-delaware"
   },
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-qualification",
   "metadata": {
    "id": "functioning-destruction"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "generic-surgeon",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "global-monte",
    "outputId": "77675ef9-9cb4-44a4-ebeb-ae9e74d7ebd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 6), (143, 3), (42146, 3), (5, 4))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(CFG.input_dir / \"train.csv\")\n",
    "features = pd.read_csv(CFG.input_dir / \"features.csv\")\n",
    "patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n",
    "test = pd.read_csv(CFG.input_dir / \"test.csv\")\n",
    "\n",
    "train.shape, features.shape, patient_notes.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "tired-bicycle",
   "metadata": {
    "id": "independent-airfare"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n",
    "    print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-taste",
   "metadata": {
    "id": "silent-locator"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ahead-contribution",
   "metadata": {
    "id": "unusual-fifty"
   },
   "outputs": [],
   "source": [
    "def preprocess_features(features):\n",
    "    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "\n",
    "\n",
    "features = preprocess_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "important-simpson",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "decreased-mustang",
    "outputId": "61c7d744-fde7-4d3c-e0c2-8393e24159b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 8), (5, 6))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "electrical-cooler",
   "metadata": {
    "id": "boolean-trade"
   },
   "outputs": [],
   "source": [
    "train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n",
    "train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "specific-football",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "accomplished-dakota",
    "outputId": "0c1b9ec8-3c61-4a44-bafe-fb9ea649d668"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4399\n",
       "1    8181\n",
       "2    1296\n",
       "3     287\n",
       "4      99\n",
       "5      27\n",
       "6       9\n",
       "7       1\n",
       "8       1\n",
       "Name: annotation_length, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n",
    "display(train['annotation_length'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-impact",
   "metadata": {
    "id": "funded-elizabeth"
   },
   "source": [
    "## CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "existing-operator",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "unexpected-columbia",
    "outputId": "e4b2bd12-a470-45e9-89f8-18f01bfb8836"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    3575\n",
       "1    3575\n",
       "2    3575\n",
       "3    3575\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Fold = GroupKFold(n_splits=CFG.n_fold)\n",
    "groups = train['pn_num'].values\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-invitation",
   "metadata": {
    "id": "critical-archive"
   },
   "source": [
    "## Setup tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "polar-basketball",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "9826121100004ec49f1cd7ed26023d9f",
      "cfc527e5b7e84c45af32ba7e49275790",
      "33bbf6ff0a9847fc9900a16ea8247120",
      "1c64feffa14d4a56b3bf4c25f6d05c51",
      "b42a1221467e43648126dc9432be0b28",
      "1c619fc144a44ae8ba6d091c236a20f6",
      "f3dcb10079874c84a85896aae581b1dc",
      "61d290b99d78422494bd99ba7f4ce0b4",
      "511fe7d1f0db48f5848e3539ddb29fff",
      "1fc34ac85840429e8d5d62785817dd03",
      "c7dbd264bb804aabbb9a3a3922cdcc00",
      "a2fc47910e5a4b158eddf7faa455d683",
      "808441188c1a4b5788ae84fa3edf27db",
      "638cf831bc0443658b4b455200f9b990",
      "c5ad324c87914e26ad665efcc418f354",
      "8254bfa8a6fa47dba57db5ba29dccaeb",
      "3d505cf1fb134928953da8d79d68665a",
      "81714296902f4d26a32aa05da8890693",
      "8f0a30d22d004fd6866556696346fb74",
      "5504bc5bee8c4c189614fd398d1b7fef",
      "5a47b531ee814b1eba201f0aa79212db",
      "7c1442db3949418184bfb68266910d91",
      "f61172e130474199999fe10c8470b1e8",
      "530069c0165e4b3d8e077e9c6a4a1d21",
      "323239c2e16449c088afd6eb5eeaa73f",
      "263650ffbf3145048f331827b49ef11b",
      "7ebb3f8b10154ef4badb2d6856140d18",
      "2c0468cceac144b890d30b510202deba",
      "b4c22ff3f7064b5c82b501058fa367a6",
      "b5f8f50bdaf843f1a938f6129848cd83",
      "0f0864ef340b49aaa9b4596e032163a7",
      "886eee1754fd42e185a7548aa44871f1",
      "ec232b9c334c4987b35fde837fac77da",
      "0c748174ece64ca6992b434a2d92e1e4",
      "827b5e8189d242c9b62bb8d6a084de72",
      "0f910e441d334b10bc666e6ef47de941",
      "09da6c904a3344ad9d7d0f17c646c8b1",
      "d6ba1f9a002c49268799fc4a17f793ee",
      "dc036c5ffc0a4a0fb2d0f504cf4264f3",
      "80be6bc8b1c8454187599fe6f05cdcba",
      "760cf6427cdc4b05affb72378d92a0f3",
      "9a660863781b487392504ab3302a5f93",
      "66daca2dd30d44efbcd88adcbb1c2725",
      "02633c7de1ea4b7ca2fd899ae0c6d209"
     ]
    },
    "id": "broken-generator",
    "outputId": "9ed5f1df-2a6b-4b0d-bbfb-dd49474e17ea"
   },
   "outputs": [],
   "source": [
    "if CFG.submission:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG.pretrained_model_name)\n",
    "    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n",
    "\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-glossary",
   "metadata": {
    "id": "compatible-lincoln"
   },
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "periodic-charlotte",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "8e1aca2f17c6476a855cbe11a1d661a9",
      "aa1104924af9458eb0d5443fd617cf29",
      "b94dd392dc874911b1deeb39f77b342f",
      "be8e2f5fc8eb4029a4aece3d1776054d",
      "032d2a594c45472e9681d2d7557ab93d",
      "cd01049905654fe6bee6c4c602b89ed9",
      "66a3685d05e1493c987e6cb1d9ed00a1",
      "13133c559d1b4a14ba19c157c169b532",
      "7e01c08e3af148f580fcdc6ef6ebf5b4",
      "e464d1ba21a1489183fda5c01bbc0cca",
      "5bb3e7fc97c64d59a20a7ebb29a39800"
     ]
    },
    "id": "fluid-nancy",
    "outputId": "9b844ea4-2568-4dea-dd39-867dcb402cfe"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1547cd1e40c24d77ba76d6438716a209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 433\n"
     ]
    }
   ],
   "source": [
    "pn_history_lengths = []\n",
    "tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    pn_history_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(pn_history_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "negative-slovenia",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "6639cae9d2844ea3b7d9a777b43b2181",
      "6e5be1d244794198afee6c8fefd3a191",
      "2df59efc32cf4642a4cfff7db709db5a",
      "72d201f9470a4d04b1e89f7d0018c0d7",
      "cebcd629be2340bfa4ca1780d70dd364",
      "da9bce59ac4845cda340d840b79be311",
      "323befd254f741a7b041d124d4073817",
      "eb191fc8e771447ab389bbb57fe22d2f",
      "ced0845b868342c29d4e2d830a48f203",
      "13873604cb644df0b4b5e8e9ea57d645",
      "f129267b3ae6422e8d345c28b73f5516"
     ]
    },
    "id": "posted-humidity",
    "outputId": "c23cdcb5-4093-47d7-d95c-a35eaf72ea9d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f03cef80e90476591d7fae0714a6388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 30\n"
     ]
    }
   ],
   "source": [
    "feature_text_lengths = []\n",
    "tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    feature_text_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(feature_text_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "developed-volunteer",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "resistant-amount",
    "outputId": "8bc2659a-d25a-40a1-f85d-37d57b1f3fc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 466\n"
     ]
    }
   ],
   "source": [
    "CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n",
    "\n",
    "print(\"max length:\", CFG.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ranking-hayes",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "4c276b71af2f4301b4048e4f8dad36d3",
      "563519b534874ec8a145d292258b2dad",
      "535a8156a6da43928fd0d7a33c624540",
      "ef3dd8c9fb4a46bfa6c30489d2d75b02",
      "5d985ee5c7d84bf9a135972d46830ad0",
      "5bc25dd928614ba999cefcde5efa9197",
      "5ccb74cb082845d7ade9962f741a2910",
      "8fa83fd7255f43ff9126aa633ed1b663",
      "9febd109d3a24bef886c8d24808347ba",
      "811a7f7f9bc34108b113d084a7d488f4",
      "ecfdf18519e34e2d9a0b112b0815cba5"
     ]
    },
    "id": "be6XpsR0aIWS",
    "outputId": "1af87d7f-23bc-4035-d4c2-08d1f778e070"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00588c1ba6ac4b0499bc1d779395744d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 950\n"
     ]
    }
   ],
   "source": [
    "pn_history_lengths = []\n",
    "tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n",
    "for text in tk0:\n",
    "    length = len(text)\n",
    "    pn_history_lengths.append(length)\n",
    "\n",
    "CFG.max_char_len = max(pn_history_lengths)\n",
    "\n",
    "print(\"max length:\", CFG.max_char_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "disciplinary-workstation",
   "metadata": {
    "id": "fIzpppqiaMRn"
   },
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, cfg, df, pseudo_label=None):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.max_char_len = self.cfg.max_char_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "        self.annotation_lengths = self.df[\"annotation_length\"].values\n",
    "        self.locations = self.df[\"location\"].values\n",
    "        if \"pseudo_idx\" in df.columns:\n",
    "            self.pseudo_idx = self.df[\"pseudo_idx\"].values\n",
    "            self.pseudo_label = pseudo_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def _create_mapping_from_token_to_char(self, pn_history):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        mapping_from_token_to_char = np.zeros(self.max_char_len)\n",
    "        offset_mapping = encoded[\"offset_mapping\"]\n",
    "        for i, offset in enumerate(offset_mapping):\n",
    "            start_idx, end_idx = offset\n",
    "            mapping_from_token_to_char[start_idx:end_idx] = i\n",
    "        return torch.tensor(mapping_from_token_to_char, dtype=torch.long)\n",
    "\n",
    "    def _create_label(self, pn_history, annotation_length, location_list):\n",
    "        label = np.zeros(self.max_char_len)\n",
    "        label[len(pn_history):] = -1\n",
    "        if annotation_length > 0:\n",
    "            for location in location_list:\n",
    "                for loc in [s.split() for s in location.split(\";\")]:\n",
    "                    start, end = int(loc[0]), int(loc[1])\n",
    "                    label[start:end] = 1\n",
    "        return torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        if not np.isnan(self.annotation_lengths[idx]):\n",
    "            label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n",
    "        else:\n",
    "            p_idx = int(self.pseudo_idx[idx])\n",
    "            label = torch.tensor(self.pseudo_label[p_idx], dtype=torch.float)\n",
    "        mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n",
    "        return input_, label, mapping_from_token_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "coordinate-facility",
   "metadata": {
    "id": "weird-interaction"
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.max_char_len = self.cfg.max_char_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def _create_mapping_from_token_to_char(self, pn_history):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        mapping_from_token_to_char = np.zeros(self.max_char_len)\n",
    "        offset_mapping = encoded[\"offset_mapping\"]\n",
    "        for i, offset in enumerate(offset_mapping):\n",
    "            start_idx, end_idx = offset\n",
    "            mapping_from_token_to_char[start_idx:end_idx] = i\n",
    "        return torch.tensor(mapping_from_token_to_char, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n",
    "        return input_, mapping_from_token_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-going",
   "metadata": {
    "id": "upper-mobility"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "virgin-fighter",
   "metadata": {
    "id": "spanish-destruction"
   },
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, model_config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if model_config_path is None:\n",
    "            self.model_config = AutoConfig.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "        else:\n",
    "            self.model_config = torch.load(model_config_path)\n",
    "\n",
    "        if pretrained:\n",
    "            self.backbone = AutoModel.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                config=self.model_config,\n",
    "            )\n",
    "            print(f\"Load weight from pretrained\")\n",
    "        else:\n",
    "            #self.backbone = AutoModel.from_config(self.model_config)\n",
    "            itpt = AutoModelForMaskedLM.from_config(self.model_config)\n",
    "            # path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp010/checkpoint-130170/pytorch_model.bin\")\n",
    "            path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n",
    "            state_dict = torch.load(path)\n",
    "            itpt.load_state_dict(state_dict)\n",
    "            self.backbone = itpt.deberta\n",
    "            print(f\"Load weight from {path}\")\n",
    "\n",
    "        self.lstm = nn.GRU(\n",
    "            input_size=self.model_config.hidden_size,\n",
    "            bidirectional=True,\n",
    "            hidden_size=self.model_config.hidden_size // 2,\n",
    "            num_layers=4,\n",
    "            dropout=self.cfg.dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(self.cfg.dropout),\n",
    "            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, mappings_from_token_to_char):\n",
    "        h = self.backbone(**inputs)[\"last_hidden_state\"]  # [batch, seq_len, d_model]\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.unsqueeze(2).expand(-1, -1, self.model_config.hidden_size)\n",
    "        h = torch.gather(h, 1, mappings_from_token_to_char)    # [batch, seq_len, d_model]\n",
    "        h, _ = self.lstm(h)\n",
    "        output = self.fc(h)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-monte",
   "metadata": {
    "id": "chronic-bullet"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "personalized-wages",
   "metadata": {
    "id": "biological-hunger"
   },
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    train_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    epoch,\n",
    "    scheduler,\n",
    "    device,\n",
    "):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels, mappings_from_token_to_char) in enumerate(train_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device) \n",
    "        batch_size = labels.size(0)\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            output = model(inputs, mappings_from_token_to_char)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if CFG.batch_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n",
    "            print(\n",
    "                \"Epoch: [{0}][{1}/{2}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                \"Grad: {grad_norm:.4f}  \"\n",
    "                \"LR: {lr:.6f}  \"\n",
    "                .format(\n",
    "                    epoch+1,\n",
    "                    step,\n",
    "                    len(train_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n",
    "                    loss=losses,\n",
    "                     grad_norm=grad_norm,\n",
    "                     lr=scheduler.get_lr()[0],\n",
    "                )\n",
    "            )\n",
    "    del output, loss, inputs, labels, mappings_from_token_to_char, scaler, grad_norm; gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "processed-scholarship",
   "metadata": {
    "id": "satisfied-sterling"
   },
   "outputs": [],
   "source": [
    "def valid_fn(\n",
    "    val_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    device,\n",
    "):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels, mappings_from_token_to_char) in enumerate(val_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device) \n",
    "        batch_size = labels.size(0)\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            output = model(inputs, mappings_from_token_to_char)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "        loss = loss.mean()\n",
    "    \n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n",
    "            print(\n",
    "                \"EVAL: [{0}/{1}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                .format(\n",
    "                    step, len(val_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n",
    "                    loss=losses,\n",
    "                )\n",
    "            )\n",
    "    preds = np.concatenate(preds)\n",
    "    return losses.avg, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "indoor-halloween",
   "metadata": {
    "id": "incorporate-viking"
   },
   "outputs": [],
   "source": [
    "def inference_fn(test_dataloader, model, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preds = []\n",
    "    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n",
    "    for (inputs, mappings_from_token_to_char) in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs, mappings_from_token_to_char)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "married-norfolk",
   "metadata": {
    "id": "dental-sunset"
   },
   "outputs": [],
   "source": [
    "def train_loop(df, i_fold, device):\n",
    "    print(f\"========== fold: {i_fold} training ==========\")\n",
    "    train_idx = df[df[\"fold\"] != i_fold].index\n",
    "    val_idx = df[df[\"fold\"] == i_fold].index\n",
    "\n",
    "    train_folds = df.loc[train_idx].reset_index(drop=True)\n",
    "    val_folds = df.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    if CFG.pseudo_plain_path is not None:\n",
    "        pseudo_plain = pd.read_pickle(CFG.pseudo_plain_path)\n",
    "        print(f\"get pseudo plain from {CFG.pseudo_plain_path}\")\n",
    "        pseudo_label_list = []\n",
    "        for exp_name in [\"nbme-exp060\", \"nbme-exp067\"]:\n",
    "            pseudo_label_path = f'../output/nbme-score-clinical-patient-notes/{exp_name}/pseudo_labels_{i_fold}.npy'\n",
    "            pseudo_label = np.load(pseudo_label_path)\n",
    "            print(f\"get pseudo labels from {pseudo_label_path}\")\n",
    "            pseudo_label_list.append(pseudo_label)\n",
    "    \n",
    "        pseudo_label = 0.5 * pseudo_label_list[0] + 0.5 * pseudo_label_list[1]\n",
    "        print(pseudo_plain.shape, pseudo_label.shape)\n",
    "\n",
    "        pseudo_plain[\"pseudo_idx\"] = np.arange(len(pseudo_plain))\n",
    "        pseudo_plain = pseudo_plain.sample(n=CFG.n_pseudo_labels, random_state=i_fold+100)\n",
    "        print(pseudo_plain.shape)\n",
    "        train_folds = pd.concat([train_folds, pseudo_plain], axis=0, ignore_index=True)\n",
    "        print(train_folds.shape)\n",
    "\n",
    "    train_dataset = TrainingDataset(CFG, train_folds, pseudo_label)\n",
    "    val_dataset = TrainingDataset(CFG, val_folds)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "    # model = CustomModel(CFG, model_config_path=None, pretrained=False)   # itptを使うため\n",
    "    model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "    path = f\"../output/nbme-score-clinical-patient-notes/nbme-exp070/fold{i_fold}_best.pth\"\n",
    "    state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n",
    "    model.to(device)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in param_optimizer if not any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n",
    "        {\"params\": [p for n, p in param_optimizer if any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=CFG.lr,\n",
    "        betas=CFG.betas,\n",
    "        weight_decay=CFG.weight_decay,\n",
    "    )\n",
    "    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n",
    "    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_train_optimization_steps,\n",
    "    )\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    best_score = -1 * np.inf\n",
    "    \n",
    "    if i_fold == 3:\n",
    "        for epoch in range(CFG.epochs):\n",
    "            start_time = time.time()\n",
    "            avg_loss = train_fn(\n",
    "                train_dataloader,\n",
    "                model,\n",
    "                criterion,\n",
    "                optimizer,\n",
    "                epoch,\n",
    "                scheduler,\n",
    "                device,\n",
    "            )\n",
    "            avg_val_loss, val_preds = valid_fn(\n",
    "                val_dataloader,\n",
    "                model,\n",
    "                criterion,\n",
    "                device,\n",
    "            )\n",
    "\n",
    "            if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
    "                scheduler.step()\n",
    "\n",
    "            # scoring\n",
    "            val_folds[[str(i) for i in range(CFG.max_char_len)]] = val_preds\n",
    "            score = scoring(val_folds, th=0.5, use_token_prob=False)\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n",
    "            print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n",
    "                torch.save({\n",
    "                    \"model\": model.state_dict(),\n",
    "                    \"predictions\": val_preds,\n",
    "                    },\n",
    "                    CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "                )\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    predictions = torch.load(\n",
    "        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )[\"predictions\"]\n",
    "    val_folds[[str(i) for i in range(CFG.max_char_len)]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return val_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-hamburg",
   "metadata": {
    "id": "brazilian-graphics"
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "sufficient-pension",
   "metadata": {
    "id": "connected-protein"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for i_fold in range(CFG.n_fold):\n",
    "            if i_fold in CFG.train_fold:\n",
    "                _oof_df = train_loop(train, i_fold, device)\n",
    "                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n",
    "        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    if CFG.submission:\n",
    "        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n",
    "    else:\n",
    "        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    best_thres = 0.5\n",
    "    best_score = 0.\n",
    "    for th in np.arange(0.45, 0.55, 0.01):\n",
    "        th = np.round(th, 2)\n",
    "        score = scoring(oof_df, th=th, use_token_prob=False)\n",
    "        if best_score < score:\n",
    "            best_thres = th\n",
    "            best_score = score\n",
    "    print(f\"best_thres: {best_thres}  score: {best_score:.5f}\")\n",
    "\n",
    "    if CFG.inference:\n",
    "        test_dataset = TestDataset(CFG, test)\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=CFG.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=CFG.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        predictions = []\n",
    "        for i_fold in CFG.train_fold:\n",
    "            if CFG.submission:\n",
    "                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n",
    "                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n",
    "            else:\n",
    "                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n",
    "\n",
    "            state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "            model.load_state_dict(state[\"model\"])\n",
    "            print(f\"load weights from {path}\")\n",
    "            test_char_probs = inference_fn(test_dataloader, model, device)\n",
    "            predictions.append(test_char_probs)\n",
    "\n",
    "            del state, test_char_probs, model; gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        predictions = np.mean(predictions, axis=0)\n",
    "        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n",
    "        test[CFG.target_col] = predicted_location_str\n",
    "        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n",
    "        test[[CFG.id_col, CFG.target_col]].to_csv(\n",
    "            CFG.output_dir / \"submission.csv\", index=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "homeless-verification",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "64961f62c5f94c3991e2b9f09c7c4782",
      "8cc41478e6584c039ec440aa3c314e6d",
      "5a5fdc30f42646058b9162b0e0974e3d",
      "c74b2e0635904f98920a922b26b6541d"
     ]
    },
    "id": "serious-bunny",
    "outputId": "be850b3e-328a-47cf-b797-2d51d289e13e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "get pseudo plain from ../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_0.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_0.npy\n",
      "(612602, 6) (612602, 950)\n",
      "(100000, 7)\n",
      "(110725, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n",
      "Epoch: [1][0/36908] Elapsed 0m 1s (remain 840m 16s) Loss: 0.0001(0.0001) Grad: 54.7416  LR: 0.000000  \n",
      "Epoch: [1][100/36908] Elapsed 1m 24s (remain 512m 17s) Loss: 0.0019(0.0023) Grad: 210.8963  LR: 0.000001  \n",
      "Epoch: [1][200/36908] Elapsed 2m 47s (remain 508m 22s) Loss: 0.0014(0.0022) Grad: 848.2419  LR: 0.000001  \n",
      "Epoch: [1][300/36908] Elapsed 4m 10s (remain 508m 3s) Loss: 0.0007(0.0022) Grad: 248.7964  LR: 0.000002  \n",
      "Epoch: [1][400/36908] Elapsed 5m 34s (remain 506m 55s) Loss: 0.0028(0.0022) Grad: 2311.0500  LR: 0.000002  \n",
      "Epoch: [1][500/36908] Elapsed 6m 56s (remain 504m 10s) Loss: 0.0002(0.0024) Grad: 522.1076  LR: 0.000003  \n",
      "Epoch: [1][600/36908] Elapsed 8m 17s (remain 501m 1s) Loss: 0.0002(0.0025) Grad: 169.6007  LR: 0.000003  \n",
      "Epoch: [1][700/36908] Elapsed 9m 39s (remain 499m 10s) Loss: 0.0015(0.0025) Grad: 648.2403  LR: 0.000004  \n",
      "Epoch: [1][800/36908] Elapsed 11m 2s (remain 497m 50s) Loss: 0.0000(0.0025) Grad: 36.3798  LR: 0.000004  \n",
      "Epoch: [1][900/36908] Elapsed 12m 25s (remain 496m 14s) Loss: 0.0000(0.0024) Grad: 8.4279  LR: 0.000005  \n",
      "Epoch: [1][1000/36908] Elapsed 13m 46s (remain 493m 58s) Loss: 0.0003(0.0024) Grad: 347.7033  LR: 0.000005  \n",
      "Epoch: [1][1100/36908] Elapsed 15m 8s (remain 492m 24s) Loss: 0.0018(0.0024) Grad: 3689.1074  LR: 0.000006  \n",
      "Epoch: [1][1200/36908] Elapsed 16m 31s (remain 491m 7s) Loss: 0.0004(0.0024) Grad: 266.1300  LR: 0.000007  \n",
      "Epoch: [1][1300/36908] Elapsed 17m 53s (remain 489m 44s) Loss: 0.0043(0.0024) Grad: 5696.7510  LR: 0.000007  \n",
      "Epoch: [1][1400/36908] Elapsed 19m 15s (remain 488m 9s) Loss: 0.0022(0.0024) Grad: 1622.5575  LR: 0.000008  \n",
      "Epoch: [1][1500/36908] Elapsed 20m 37s (remain 486m 21s) Loss: 0.0015(0.0024) Grad: 1579.6578  LR: 0.000008  \n",
      "Epoch: [1][1600/36908] Elapsed 21m 59s (remain 484m 55s) Loss: 0.0032(0.0024) Grad: 3964.3105  LR: 0.000009  \n",
      "Epoch: [1][1700/36908] Elapsed 23m 21s (remain 483m 32s) Loss: 0.0073(0.0023) Grad: 2004.2932  LR: 0.000009  \n",
      "Epoch: [1][1800/36908] Elapsed 24m 43s (remain 481m 48s) Loss: 0.0051(0.0024) Grad: 8535.6426  LR: 0.000010  \n",
      "Epoch: [1][1900/36908] Elapsed 26m 4s (remain 480m 11s) Loss: 0.0042(0.0024) Grad: 7751.4038  LR: 0.000010  \n",
      "Epoch: [1][2000/36908] Elapsed 27m 26s (remain 478m 47s) Loss: 0.0053(0.0024) Grad: 5542.6870  LR: 0.000011  \n",
      "Epoch: [1][2100/36908] Elapsed 28m 47s (remain 477m 6s) Loss: 0.0003(0.0024) Grad: 412.1695  LR: 0.000011  \n",
      "Epoch: [1][2200/36908] Elapsed 30m 9s (remain 475m 37s) Loss: 0.0014(0.0024) Grad: 3036.1492  LR: 0.000012  \n",
      "Epoch: [1][2300/36908] Elapsed 31m 32s (remain 474m 24s) Loss: 0.0014(0.0024) Grad: 4705.4771  LR: 0.000012  \n",
      "Epoch: [1][2400/36908] Elapsed 32m 54s (remain 472m 56s) Loss: 0.0005(0.0024) Grad: 1122.8033  LR: 0.000013  \n",
      "Epoch: [1][2500/36908] Elapsed 34m 17s (remain 471m 42s) Loss: 0.0002(0.0024) Grad: 150.8568  LR: 0.000014  \n",
      "Epoch: [1][2600/36908] Elapsed 35m 40s (remain 470m 27s) Loss: 0.0135(0.0024) Grad: 17589.5254  LR: 0.000014  \n",
      "Epoch: [1][2700/36908] Elapsed 37m 1s (remain 469m 0s) Loss: 0.0002(0.0024) Grad: 135.3704  LR: 0.000015  \n",
      "Epoch: [1][2800/36908] Elapsed 38m 24s (remain 467m 38s) Loss: 0.0004(0.0024) Grad: 624.9351  LR: 0.000015  \n",
      "Epoch: [1][2900/36908] Elapsed 39m 46s (remain 466m 21s) Loss: 0.0003(0.0024) Grad: 185.9897  LR: 0.000016  \n",
      "Epoch: [1][3000/36908] Elapsed 41m 9s (remain 464m 56s) Loss: 0.0001(0.0024) Grad: 14.3862  LR: 0.000016  \n",
      "Epoch: [1][3100/36908] Elapsed 42m 30s (remain 463m 21s) Loss: 0.0005(0.0024) Grad: 249.5693  LR: 0.000017  \n",
      "Epoch: [1][3200/36908] Elapsed 43m 52s (remain 461m 58s) Loss: 0.0005(0.0024) Grad: 2603.7510  LR: 0.000017  \n",
      "Epoch: [1][3300/36908] Elapsed 45m 14s (remain 460m 31s) Loss: 0.0004(0.0024) Grad: 849.5840  LR: 0.000018  \n",
      "Epoch: [1][3400/36908] Elapsed 46m 36s (remain 459m 6s) Loss: 0.0001(0.0024) Grad: 10.7502  LR: 0.000018  \n",
      "Epoch: [1][3500/36908] Elapsed 47m 59s (remain 457m 55s) Loss: 0.0033(0.0024) Grad: 1596.6625  LR: 0.000019  \n",
      "Epoch: [1][3600/36908] Elapsed 49m 22s (remain 456m 43s) Loss: 0.0002(0.0024) Grad: 124.5800  LR: 0.000020  \n",
      "Epoch: [1][3700/36908] Elapsed 50m 45s (remain 455m 24s) Loss: 0.0002(0.0024) Grad: 62.7941  LR: 0.000020  \n",
      "Epoch: [1][3800/36908] Elapsed 52m 7s (remain 453m 59s) Loss: 0.0040(0.0024) Grad: 3628.1157  LR: 0.000020  \n",
      "Epoch: [1][3900/36908] Elapsed 53m 29s (remain 452m 34s) Loss: 0.0010(0.0024) Grad: 1737.9995  LR: 0.000020  \n",
      "Epoch: [1][4000/36908] Elapsed 54m 52s (remain 451m 21s) Loss: 0.0023(0.0024) Grad: 13429.0898  LR: 0.000020  \n",
      "Epoch: [1][4100/36908] Elapsed 56m 15s (remain 450m 2s) Loss: 0.0001(0.0025) Grad: 25.3290  LR: 0.000020  \n",
      "Epoch: [1][4200/36908] Elapsed 57m 37s (remain 448m 37s) Loss: 0.0014(0.0025) Grad: 2662.6709  LR: 0.000020  \n",
      "Epoch: [1][4300/36908] Elapsed 59m 1s (remain 447m 28s) Loss: 0.0005(0.0025) Grad: 6041.7290  LR: 0.000020  \n",
      "Epoch: [1][4400/36908] Elapsed 60m 23s (remain 446m 6s) Loss: 0.0040(0.0025) Grad: 10583.2441  LR: 0.000020  \n",
      "Epoch: [1][4500/36908] Elapsed 61m 45s (remain 444m 41s) Loss: 0.0081(0.0025) Grad: 6111.8306  LR: 0.000020  \n",
      "Epoch: [1][4600/36908] Elapsed 63m 7s (remain 443m 18s) Loss: 0.0055(0.0025) Grad: 20688.8223  LR: 0.000019  \n",
      "Epoch: [1][4700/36908] Elapsed 64m 30s (remain 441m 54s) Loss: 0.0021(0.0025) Grad: 5685.1304  LR: 0.000019  \n",
      "Epoch: [1][4800/36908] Elapsed 65m 53s (remain 440m 38s) Loss: 0.0019(0.0025) Grad: 4419.6875  LR: 0.000019  \n",
      "Epoch: [1][4900/36908] Elapsed 67m 16s (remain 439m 23s) Loss: 0.0006(0.0025) Grad: 1973.3181  LR: 0.000019  \n",
      "Epoch: [1][5000/36908] Elapsed 68m 38s (remain 437m 58s) Loss: 0.0130(0.0025) Grad: 38429.1133  LR: 0.000019  \n",
      "Epoch: [1][5100/36908] Elapsed 70m 0s (remain 436m 32s) Loss: 0.0000(0.0025) Grad: 15.8000  LR: 0.000019  \n",
      "Epoch: [1][5200/36908] Elapsed 71m 22s (remain 435m 5s) Loss: 0.0002(0.0025) Grad: 104.7062  LR: 0.000019  \n",
      "Epoch: [1][5300/36908] Elapsed 72m 43s (remain 433m 39s) Loss: 0.0013(0.0025) Grad: 2482.9358  LR: 0.000019  \n",
      "Epoch: [1][5400/36908] Elapsed 74m 7s (remain 432m 21s) Loss: 0.0006(0.0026) Grad: 1347.7284  LR: 0.000019  \n",
      "Epoch: [1][5500/36908] Elapsed 75m 29s (remain 431m 2s) Loss: 0.0000(0.0025) Grad: 82.9474  LR: 0.000019  \n",
      "Epoch: [1][5600/36908] Elapsed 76m 51s (remain 429m 38s) Loss: 0.0001(0.0025) Grad: 32.1549  LR: 0.000019  \n",
      "Epoch: [1][5700/36908] Elapsed 78m 14s (remain 428m 15s) Loss: 0.0009(0.0026) Grad: 1052.5852  LR: 0.000019  \n",
      "Epoch: [1][5800/36908] Elapsed 79m 36s (remain 426m 55s) Loss: 0.0004(0.0026) Grad: 845.0722  LR: 0.000019  \n",
      "Epoch: [1][5900/36908] Elapsed 80m 58s (remain 425m 29s) Loss: 0.0008(0.0026) Grad: 440.1176  LR: 0.000019  \n",
      "Epoch: [1][6000/36908] Elapsed 82m 20s (remain 424m 6s) Loss: 0.0007(0.0026) Grad: 1551.3368  LR: 0.000019  \n",
      "Epoch: [1][6100/36908] Elapsed 83m 44s (remain 422m 49s) Loss: 0.0058(0.0026) Grad: 18673.7598  LR: 0.000019  \n",
      "Epoch: [1][6200/36908] Elapsed 85m 6s (remain 421m 26s) Loss: 0.0008(0.0026) Grad: 1606.3478  LR: 0.000018  \n",
      "Epoch: [1][6300/36908] Elapsed 86m 28s (remain 420m 3s) Loss: 0.0002(0.0026) Grad: 590.6849  LR: 0.000018  \n",
      "Epoch: [1][6400/36908] Elapsed 87m 50s (remain 418m 39s) Loss: 0.0000(0.0026) Grad: 22.7992  LR: 0.000018  \n",
      "Epoch: [1][6500/36908] Elapsed 89m 12s (remain 417m 17s) Loss: 0.0002(0.0026) Grad: 66.9808  LR: 0.000018  \n",
      "Epoch: [1][6600/36908] Elapsed 90m 34s (remain 415m 52s) Loss: 0.0048(0.0026) Grad: 13869.9561  LR: 0.000018  \n",
      "Epoch: [1][6700/36908] Elapsed 91m 57s (remain 414m 33s) Loss: 0.0037(0.0026) Grad: 12352.8223  LR: 0.000018  \n",
      "Epoch: [1][6800/36908] Elapsed 93m 19s (remain 413m 8s) Loss: 0.0039(0.0026) Grad: 33454.1875  LR: 0.000018  \n",
      "Epoch: [1][6900/36908] Elapsed 94m 40s (remain 411m 40s) Loss: 0.0004(0.0026) Grad: 2349.2827  LR: 0.000018  \n",
      "Epoch: [1][7000/36908] Elapsed 96m 3s (remain 410m 22s) Loss: 0.0003(0.0026) Grad: 117.9711  LR: 0.000018  \n",
      "Epoch: [1][7100/36908] Elapsed 97m 25s (remain 408m 57s) Loss: 0.0006(0.0026) Grad: 5590.8081  LR: 0.000018  \n",
      "Epoch: [1][7200/36908] Elapsed 98m 47s (remain 407m 34s) Loss: 0.0013(0.0026) Grad: 1348.7415  LR: 0.000018  \n",
      "Epoch: [1][7300/36908] Elapsed 100m 11s (remain 406m 17s) Loss: 0.0059(0.0026) Grad: 46226.0977  LR: 0.000018  \n",
      "Epoch: [1][7400/36908] Elapsed 101m 34s (remain 404m 59s) Loss: 0.0014(0.0026) Grad: 2595.0679  LR: 0.000018  \n",
      "Epoch: [1][7500/36908] Elapsed 102m 57s (remain 403m 37s) Loss: 0.0003(0.0026) Grad: 525.6440  LR: 0.000018  \n",
      "Epoch: [1][7600/36908] Elapsed 104m 20s (remain 402m 18s) Loss: 0.0072(0.0026) Grad: 22044.3867  LR: 0.000018  \n",
      "Epoch: [1][7700/36908] Elapsed 105m 43s (remain 400m 59s) Loss: 0.0171(0.0026) Grad: 31323.5059  LR: 0.000018  \n",
      "Epoch: [1][7800/36908] Elapsed 107m 7s (remain 399m 41s) Loss: 0.0039(0.0026) Grad: 9795.9268  LR: 0.000018  \n",
      "Epoch: [1][7900/36908] Elapsed 108m 29s (remain 398m 18s) Loss: 0.0003(0.0026) Grad: 244.2830  LR: 0.000017  \n",
      "Epoch: [1][8000/36908] Elapsed 109m 51s (remain 396m 54s) Loss: 0.0001(0.0026) Grad: 406.8490  LR: 0.000017  \n",
      "Epoch: [1][8100/36908] Elapsed 111m 14s (remain 395m 35s) Loss: 0.0108(0.0026) Grad: 49812.6289  LR: 0.000017  \n",
      "Epoch: [1][8200/36908] Elapsed 112m 38s (remain 394m 16s) Loss: 0.0267(0.0026) Grad: 167567.9844  LR: 0.000017  \n",
      "Epoch: [1][8300/36908] Elapsed 114m 0s (remain 392m 52s) Loss: 0.0001(0.0026) Grad: 86.3065  LR: 0.000017  \n",
      "Epoch: [1][8400/36908] Elapsed 115m 23s (remain 391m 32s) Loss: 0.0110(0.0026) Grad: 34981.4336  LR: 0.000017  \n",
      "Epoch: [1][8500/36908] Elapsed 116m 46s (remain 390m 13s) Loss: 0.0015(0.0026) Grad: 508.8409  LR: 0.000017  \n",
      "Epoch: [1][8600/36908] Elapsed 118m 9s (remain 388m 51s) Loss: 0.0001(0.0026) Grad: 1343.7668  LR: 0.000017  \n",
      "Epoch: [1][8700/36908] Elapsed 119m 32s (remain 387m 32s) Loss: 0.0116(0.0026) Grad: 20961.2656  LR: 0.000017  \n",
      "Epoch: [1][8800/36908] Elapsed 120m 55s (remain 386m 11s) Loss: 0.0017(0.0026) Grad: 15685.5986  LR: 0.000017  \n",
      "Epoch: [1][8900/36908] Elapsed 122m 18s (remain 384m 49s) Loss: 0.0011(0.0026) Grad: 1857.7972  LR: 0.000017  \n",
      "Epoch: [1][9000/36908] Elapsed 123m 41s (remain 383m 28s) Loss: 0.0026(0.0026) Grad: 5094.7031  LR: 0.000017  \n",
      "Epoch: [1][9100/36908] Elapsed 125m 4s (remain 382m 10s) Loss: 0.0003(0.0026) Grad: 1964.6594  LR: 0.000017  \n",
      "Epoch: [1][9200/36908] Elapsed 126m 27s (remain 380m 46s) Loss: 0.0043(0.0026) Grad: 10162.7666  LR: 0.000017  \n",
      "Epoch: [1][9300/36908] Elapsed 127m 49s (remain 379m 23s) Loss: 0.0065(0.0026) Grad: 14319.8154  LR: 0.000017  \n",
      "Epoch: [1][9400/36908] Elapsed 129m 11s (remain 377m 59s) Loss: 0.0111(0.0026) Grad: 67836.0391  LR: 0.000017  \n",
      "Epoch: [1][9500/36908] Elapsed 130m 34s (remain 376m 38s) Loss: 0.0088(0.0026) Grad: 68043.8672  LR: 0.000017  \n",
      "Epoch: [1][9600/36908] Elapsed 131m 56s (remain 375m 15s) Loss: 0.0009(0.0026) Grad: 8197.8174  LR: 0.000016  \n",
      "Epoch: [1][9700/36908] Elapsed 133m 18s (remain 373m 51s) Loss: 0.0003(0.0026) Grad: 424.9144  LR: 0.000016  \n",
      "Epoch: [1][9800/36908] Elapsed 134m 41s (remain 372m 30s) Loss: 0.0001(0.0026) Grad: 43.5975  LR: 0.000016  \n",
      "Epoch: [1][9900/36908] Elapsed 136m 3s (remain 371m 7s) Loss: 0.0067(0.0026) Grad: 23284.6172  LR: 0.000016  \n",
      "Epoch: [1][10000/36908] Elapsed 137m 25s (remain 369m 43s) Loss: 0.0034(0.0026) Grad: 14227.3281  LR: 0.000016  \n",
      "Epoch: [1][10100/36908] Elapsed 138m 48s (remain 368m 23s) Loss: 0.0001(0.0026) Grad: 28.9608  LR: 0.000016  \n",
      "Epoch: [1][10200/36908] Elapsed 140m 11s (remain 367m 3s) Loss: 0.0114(0.0026) Grad: 85756.6641  LR: 0.000016  \n",
      "Epoch: [1][10300/36908] Elapsed 141m 33s (remain 365m 39s) Loss: 0.0006(0.0026) Grad: 2348.5068  LR: 0.000016  \n",
      "Epoch: [1][10400/36908] Elapsed 142m 56s (remain 364m 17s) Loss: 0.0054(0.0026) Grad: 31732.0176  LR: 0.000016  \n",
      "Epoch: [1][10500/36908] Elapsed 144m 19s (remain 362m 55s) Loss: 0.0024(0.0026) Grad: 835.7827  LR: 0.000016  \n",
      "Epoch: [1][10600/36908] Elapsed 145m 41s (remain 361m 32s) Loss: 0.0006(0.0026) Grad: 5911.2896  LR: 0.000016  \n",
      "Epoch: [1][10700/36908] Elapsed 147m 4s (remain 360m 10s) Loss: 0.0015(0.0026) Grad: 5001.0762  LR: 0.000016  \n",
      "Epoch: [1][10800/36908] Elapsed 148m 27s (remain 358m 49s) Loss: 0.0038(0.0026) Grad: 2980.7981  LR: 0.000016  \n",
      "Epoch: [1][10900/36908] Elapsed 149m 48s (remain 357m 25s) Loss: 0.0002(0.0026) Grad: 38.5522  LR: 0.000016  \n",
      "Epoch: [1][11000/36908] Elapsed 151m 12s (remain 356m 6s) Loss: 0.0011(0.0026) Grad: 3151.9065  LR: 0.000016  \n",
      "Epoch: [1][11100/36908] Elapsed 152m 36s (remain 354m 45s) Loss: 0.0002(0.0026) Grad: 136.8540  LR: 0.000016  \n",
      "Epoch: [1][11200/36908] Elapsed 154m 0s (remain 353m 26s) Loss: 0.0028(0.0026) Grad: 22778.9922  LR: 0.000015  \n",
      "Epoch: [1][11300/36908] Elapsed 155m 22s (remain 352m 4s) Loss: 0.0056(0.0026) Grad: 24197.1484  LR: 0.000015  \n",
      "Epoch: [1][11400/36908] Elapsed 156m 45s (remain 350m 42s) Loss: 0.0005(0.0026) Grad: 3231.0793  LR: 0.000015  \n",
      "Epoch: [1][11500/36908] Elapsed 158m 8s (remain 349m 20s) Loss: 0.0035(0.0026) Grad: 10248.2314  LR: 0.000015  \n",
      "Epoch: [1][11600/36908] Elapsed 159m 30s (remain 347m 57s) Loss: 0.0002(0.0026) Grad: 1709.0739  LR: 0.000015  \n",
      "Epoch: [1][11700/36908] Elapsed 160m 52s (remain 346m 33s) Loss: 0.0001(0.0026) Grad: 246.4478  LR: 0.000015  \n",
      "Epoch: [1][11800/36908] Elapsed 162m 15s (remain 345m 12s) Loss: 0.0001(0.0026) Grad: 49.7263  LR: 0.000015  \n",
      "Epoch: [1][11900/36908] Elapsed 163m 39s (remain 343m 52s) Loss: 0.0177(0.0026) Grad: 62419.9180  LR: 0.000015  \n",
      "Epoch: [1][12000/36908] Elapsed 165m 2s (remain 342m 31s) Loss: 0.0025(0.0026) Grad: 16257.9912  LR: 0.000015  \n",
      "Epoch: [1][12100/36908] Elapsed 166m 24s (remain 341m 8s) Loss: 0.0023(0.0026) Grad: 44776.4062  LR: 0.000015  \n",
      "Epoch: [1][12200/36908] Elapsed 167m 48s (remain 339m 49s) Loss: 0.0006(0.0026) Grad: 2342.4451  LR: 0.000015  \n",
      "Epoch: [1][12300/36908] Elapsed 169m 12s (remain 338m 29s) Loss: 0.0023(0.0026) Grad: 17802.1152  LR: 0.000015  \n",
      "Epoch: [1][12400/36908] Elapsed 170m 35s (remain 337m 7s) Loss: 0.0060(0.0026) Grad: 259033.2812  LR: 0.000015  \n",
      "Epoch: [1][12500/36908] Elapsed 172m 0s (remain 335m 49s) Loss: 0.0035(0.0026) Grad: 49295.1133  LR: 0.000015  \n",
      "Epoch: [1][12600/36908] Elapsed 173m 24s (remain 334m 30s) Loss: 0.0021(0.0026) Grad: 19401.2363  LR: 0.000015  \n",
      "Epoch: [1][12700/36908] Elapsed 174m 47s (remain 333m 7s) Loss: 0.0093(0.0026) Grad: 41808.7812  LR: 0.000015  \n",
      "Epoch: [1][12800/36908] Elapsed 176m 8s (remain 331m 43s) Loss: 0.0006(0.0026) Grad: 18245.4902  LR: 0.000015  \n",
      "Epoch: [1][12900/36908] Elapsed 177m 32s (remain 330m 22s) Loss: 0.0010(0.0026) Grad: 25113.9414  LR: 0.000014  \n",
      "Epoch: [1][13000/36908] Elapsed 178m 54s (remain 328m 59s) Loss: 0.0005(0.0026) Grad: 3998.2144  LR: 0.000014  \n",
      "Epoch: [1][13100/36908] Elapsed 180m 17s (remain 327m 36s) Loss: 0.0012(0.0026) Grad: 8706.5762  LR: 0.000014  \n",
      "Epoch: [1][13200/36908] Elapsed 181m 39s (remain 326m 13s) Loss: 0.0000(0.0026) Grad: 104.3465  LR: 0.000014  \n",
      "Epoch: [1][13300/36908] Elapsed 183m 2s (remain 324m 51s) Loss: 0.0001(0.0026) Grad: 179.2473  LR: 0.000014  \n",
      "Epoch: [1][13400/36908] Elapsed 184m 24s (remain 323m 28s) Loss: 0.0008(0.0026) Grad: 17259.7812  LR: 0.000014  \n",
      "Epoch: [1][13500/36908] Elapsed 185m 47s (remain 322m 6s) Loss: 0.0015(0.0026) Grad: 32905.6953  LR: 0.000014  \n",
      "Epoch: [1][13600/36908] Elapsed 187m 9s (remain 320m 43s) Loss: 0.0000(0.0026) Grad: 9.0978  LR: 0.000014  \n",
      "Epoch: [1][13700/36908] Elapsed 188m 31s (remain 319m 20s) Loss: 0.0003(0.0026) Grad: 4133.7744  LR: 0.000014  \n",
      "Epoch: [1][13800/36908] Elapsed 189m 55s (remain 317m 59s) Loss: 0.0040(0.0026) Grad: 70502.8594  LR: 0.000014  \n",
      "Epoch: [1][13900/36908] Elapsed 191m 18s (remain 316m 37s) Loss: 0.0078(0.0026) Grad: 121822.0391  LR: 0.000014  \n",
      "Epoch: [1][14000/36908] Elapsed 192m 41s (remain 315m 16s) Loss: 0.0004(0.0026) Grad: 13517.0342  LR: 0.000014  \n",
      "Epoch: [1][14100/36908] Elapsed 194m 5s (remain 313m 55s) Loss: 0.0015(0.0026) Grad: 130562.3828  LR: 0.000014  \n",
      "Epoch: [1][14200/36908] Elapsed 195m 28s (remain 312m 33s) Loss: 0.0009(0.0026) Grad: 6814.7725  LR: 0.000014  \n",
      "Epoch: [1][14300/36908] Elapsed 196m 51s (remain 311m 11s) Loss: 0.0000(0.0026) Grad: 38.5285  LR: 0.000014  \n",
      "Epoch: [1][14400/36908] Elapsed 198m 14s (remain 309m 49s) Loss: 0.0001(0.0026) Grad: 126.1412  LR: 0.000014  \n",
      "Epoch: [1][14500/36908] Elapsed 199m 37s (remain 308m 28s) Loss: 0.0477(0.0026) Grad: 603957.8750  LR: 0.000013  \n",
      "Epoch: [1][14600/36908] Elapsed 201m 0s (remain 307m 6s) Loss: 0.0018(0.0026) Grad: 18056.4629  LR: 0.000013  \n",
      "Epoch: [1][14700/36908] Elapsed 202m 22s (remain 305m 42s) Loss: 0.0097(0.0026) Grad: 22816.8535  LR: 0.000013  \n",
      "Epoch: [1][14800/36908] Elapsed 203m 45s (remain 304m 19s) Loss: 0.0001(0.0026) Grad: 107.3025  LR: 0.000013  \n",
      "Epoch: [1][14900/36908] Elapsed 205m 8s (remain 302m 57s) Loss: 0.0069(0.0026) Grad: 94722.5000  LR: 0.000013  \n",
      "Epoch: [1][15000/36908] Elapsed 206m 30s (remain 301m 34s) Loss: 0.0001(0.0026) Grad: 398.8186  LR: 0.000013  \n",
      "Epoch: [1][15100/36908] Elapsed 207m 52s (remain 300m 10s) Loss: 0.0014(0.0026) Grad: 20562.3809  LR: 0.000013  \n",
      "Epoch: [1][15200/36908] Elapsed 209m 15s (remain 298m 48s) Loss: 0.0020(0.0026) Grad: 15335.3877  LR: 0.000013  \n",
      "Epoch: [1][15300/36908] Elapsed 210m 37s (remain 297m 25s) Loss: 0.0006(0.0026) Grad: 388.6465  LR: 0.000013  \n",
      "Epoch: [1][15400/36908] Elapsed 211m 59s (remain 296m 2s) Loss: 0.0034(0.0026) Grad: 62528.8008  LR: 0.000013  \n",
      "Epoch: [1][15500/36908] Elapsed 213m 21s (remain 294m 39s) Loss: 0.0010(0.0026) Grad: 5481.8525  LR: 0.000013  \n",
      "Epoch: [1][15600/36908] Elapsed 214m 44s (remain 293m 17s) Loss: 0.0000(0.0026) Grad: 102.7709  LR: 0.000013  \n",
      "Epoch: [1][15700/36908] Elapsed 216m 6s (remain 291m 53s) Loss: 0.0012(0.0026) Grad: 22060.7910  LR: 0.000013  \n",
      "Epoch: [1][15800/36908] Elapsed 217m 28s (remain 290m 30s) Loss: 0.0044(0.0026) Grad: 55476.0664  LR: 0.000013  \n",
      "Epoch: [1][15900/36908] Elapsed 218m 50s (remain 289m 7s) Loss: 0.0008(0.0025) Grad: 4934.1646  LR: 0.000013  \n",
      "Epoch: [1][16000/36908] Elapsed 220m 11s (remain 287m 42s) Loss: 0.0002(0.0025) Grad: 545.5034  LR: 0.000013  \n",
      "Epoch: [1][16100/36908] Elapsed 221m 35s (remain 286m 21s) Loss: 0.0018(0.0025) Grad: 175603.2031  LR: 0.000013  \n",
      "Epoch: [1][16200/36908] Elapsed 222m 59s (remain 285m 0s) Loss: 0.0237(0.0025) Grad: 122230.2031  LR: 0.000012  \n",
      "Epoch: [1][16300/36908] Elapsed 224m 23s (remain 283m 39s) Loss: 0.0009(0.0026) Grad: 23383.2578  LR: 0.000012  \n",
      "Epoch: [1][16400/36908] Elapsed 225m 46s (remain 282m 17s) Loss: 0.0001(0.0026) Grad: 52.8877  LR: 0.000012  \n",
      "Epoch: [1][16500/36908] Elapsed 227m 8s (remain 280m 54s) Loss: 0.0000(0.0026) Grad: 19.8776  LR: 0.000012  \n",
      "Epoch: [1][16600/36908] Elapsed 228m 31s (remain 279m 32s) Loss: 0.0111(0.0026) Grad: 236429.0625  LR: 0.000012  \n",
      "Epoch: [1][16700/36908] Elapsed 229m 55s (remain 278m 11s) Loss: 0.0025(0.0026) Grad: 70446.1016  LR: 0.000012  \n",
      "Epoch: [1][16800/36908] Elapsed 231m 17s (remain 276m 48s) Loss: 0.0005(0.0026) Grad: 10642.5664  LR: 0.000012  \n",
      "Epoch: [1][16900/36908] Elapsed 232m 40s (remain 275m 26s) Loss: 0.0006(0.0025) Grad: 4592.6162  LR: 0.000012  \n",
      "Epoch: [1][17000/36908] Elapsed 234m 3s (remain 274m 4s) Loss: 0.0003(0.0025) Grad: 3648.7302  LR: 0.000012  \n",
      "Epoch: [1][17100/36908] Elapsed 235m 26s (remain 272m 42s) Loss: 0.0000(0.0025) Grad: 89.7127  LR: 0.000012  \n",
      "Epoch: [1][17200/36908] Elapsed 236m 49s (remain 271m 19s) Loss: 0.0005(0.0025) Grad: 1814.3201  LR: 0.000012  \n",
      "Epoch: [1][17300/36908] Elapsed 238m 12s (remain 269m 57s) Loss: 0.0002(0.0025) Grad: 10294.7422  LR: 0.000012  \n",
      "Epoch: [1][17400/36908] Elapsed 239m 35s (remain 268m 35s) Loss: 0.0000(0.0025) Grad: 31.9897  LR: 0.000012  \n",
      "Epoch: [1][17500/36908] Elapsed 240m 57s (remain 267m 12s) Loss: 0.0055(0.0025) Grad: 97465.4844  LR: 0.000012  \n",
      "Epoch: [1][17600/36908] Elapsed 242m 19s (remain 265m 49s) Loss: 0.0043(0.0025) Grad: 58924.4141  LR: 0.000012  \n",
      "Epoch: [1][17700/36908] Elapsed 243m 42s (remain 264m 26s) Loss: 0.0064(0.0025) Grad: 20821.7812  LR: 0.000012  \n",
      "Epoch: [1][17800/36908] Elapsed 245m 5s (remain 263m 4s) Loss: 0.0059(0.0025) Grad: 74454.9062  LR: 0.000012  \n",
      "Epoch: [1][17900/36908] Elapsed 246m 27s (remain 261m 40s) Loss: 0.0005(0.0025) Grad: 23365.8945  LR: 0.000011  \n",
      "Epoch: [1][18000/36908] Elapsed 247m 50s (remain 260m 18s) Loss: 0.0000(0.0025) Grad: 31.9037  LR: 0.000011  \n",
      "Epoch: [1][18100/36908] Elapsed 249m 12s (remain 258m 55s) Loss: 0.0001(0.0025) Grad: 35.9160  LR: 0.000011  \n",
      "Epoch: [1][18200/36908] Elapsed 250m 36s (remain 257m 34s) Loss: 0.0002(0.0025) Grad: 457.7985  LR: 0.000011  \n",
      "Epoch: [1][18300/36908] Elapsed 251m 59s (remain 256m 12s) Loss: 0.0429(0.0025) Grad: 748585.8125  LR: 0.000011  \n",
      "Epoch: [1][18400/36908] Elapsed 253m 21s (remain 254m 49s) Loss: 0.0057(0.0025) Grad: 108582.8359  LR: 0.000011  \n",
      "Epoch: [1][18500/36908] Elapsed 254m 44s (remain 253m 26s) Loss: 0.0001(0.0025) Grad: 788.0269  LR: 0.000011  \n",
      "Epoch: [1][18600/36908] Elapsed 256m 5s (remain 252m 2s) Loss: 0.0001(0.0025) Grad: 618.6260  LR: 0.000011  \n",
      "Epoch: [1][18700/36908] Elapsed 257m 29s (remain 250m 41s) Loss: 0.0071(0.0025) Grad: 53500.9961  LR: 0.000011  \n",
      "Epoch: [1][18800/36908] Elapsed 258m 52s (remain 249m 19s) Loss: 0.0028(0.0025) Grad: 45412.1953  LR: 0.000011  \n",
      "Epoch: [1][18900/36908] Elapsed 260m 14s (remain 247m 56s) Loss: 0.0012(0.0025) Grad: 6666.2485  LR: 0.000011  \n",
      "Epoch: [1][19000/36908] Elapsed 261m 38s (remain 246m 34s) Loss: 0.0000(0.0025) Grad: 1770.1711  LR: 0.000011  \n",
      "Epoch: [1][19100/36908] Elapsed 263m 0s (remain 245m 11s) Loss: 0.0015(0.0025) Grad: 15856.1719  LR: 0.000011  \n",
      "Epoch: [1][19200/36908] Elapsed 264m 22s (remain 243m 48s) Loss: 0.0001(0.0025) Grad: 153.3741  LR: 0.000011  \n",
      "Epoch: [1][19300/36908] Elapsed 265m 45s (remain 242m 26s) Loss: 0.0001(0.0025) Grad: 264.4914  LR: 0.000011  \n",
      "Epoch: [1][19400/36908] Elapsed 267m 9s (remain 241m 4s) Loss: 0.0049(0.0025) Grad: 53599.0039  LR: 0.000011  \n",
      "Epoch: [1][19500/36908] Elapsed 268m 31s (remain 239m 41s) Loss: 0.0003(0.0025) Grad: 1086.7158  LR: 0.000010  \n",
      "Epoch: [1][19600/36908] Elapsed 269m 53s (remain 238m 18s) Loss: 0.0002(0.0025) Grad: 420.1626  LR: 0.000010  \n",
      "Epoch: [1][19700/36908] Elapsed 271m 15s (remain 236m 55s) Loss: 0.0000(0.0025) Grad: 2.8915  LR: 0.000010  \n",
      "Epoch: [1][19800/36908] Elapsed 272m 38s (remain 235m 33s) Loss: 0.0061(0.0025) Grad: 44606.6055  LR: 0.000010  \n",
      "Epoch: [1][19900/36908] Elapsed 274m 1s (remain 234m 10s) Loss: 0.0076(0.0025) Grad: 67435.1719  LR: 0.000010  \n",
      "Epoch: [1][20000/36908] Elapsed 275m 23s (remain 232m 47s) Loss: 0.0015(0.0025) Grad: 2381.5088  LR: 0.000010  \n",
      "Epoch: [1][20100/36908] Elapsed 276m 46s (remain 231m 24s) Loss: 0.0024(0.0025) Grad: 41296.3359  LR: 0.000010  \n",
      "Epoch: [1][20200/36908] Elapsed 278m 8s (remain 230m 2s) Loss: 0.0001(0.0025) Grad: 416.0886  LR: 0.000010  \n",
      "Epoch: [1][20300/36908] Elapsed 279m 32s (remain 228m 40s) Loss: 0.0055(0.0025) Grad: 7028.9355  LR: 0.000010  \n",
      "Epoch: [1][20400/36908] Elapsed 280m 55s (remain 227m 18s) Loss: 0.0008(0.0025) Grad: 6365.0093  LR: 0.000010  \n",
      "Epoch: [1][20500/36908] Elapsed 282m 17s (remain 225m 55s) Loss: 0.0019(0.0025) Grad: 14537.0791  LR: 0.000010  \n",
      "Epoch: [1][20600/36908] Elapsed 283m 39s (remain 224m 31s) Loss: 0.0010(0.0025) Grad: 958.8262  LR: 0.000010  \n",
      "Epoch: [1][20700/36908] Elapsed 285m 3s (remain 223m 10s) Loss: 0.0040(0.0025) Grad: 28118.2383  LR: 0.000010  \n",
      "Epoch: [1][20800/36908] Elapsed 286m 26s (remain 221m 48s) Loss: 0.0037(0.0025) Grad: 25251.8398  LR: 0.000010  \n",
      "Epoch: [1][20900/36908] Elapsed 287m 49s (remain 220m 25s) Loss: 0.0051(0.0025) Grad: 21781.5449  LR: 0.000010  \n",
      "Epoch: [1][21000/36908] Elapsed 289m 13s (remain 219m 4s) Loss: 0.0016(0.0025) Grad: 5926.5308  LR: 0.000010  \n",
      "Epoch: [1][21100/36908] Elapsed 290m 35s (remain 217m 41s) Loss: 0.0007(0.0025) Grad: 10673.3516  LR: 0.000010  \n",
      "Epoch: [1][21200/36908] Elapsed 291m 58s (remain 216m 18s) Loss: 0.0013(0.0025) Grad: 11641.5928  LR: 0.000009  \n",
      "Epoch: [1][21300/36908] Elapsed 293m 20s (remain 214m 55s) Loss: 0.0047(0.0025) Grad: 89110.7422  LR: 0.000009  \n",
      "Epoch: [1][21400/36908] Elapsed 294m 44s (remain 213m 34s) Loss: 0.0018(0.0025) Grad: 11345.5742  LR: 0.000009  \n",
      "Epoch: [1][21500/36908] Elapsed 296m 7s (remain 212m 11s) Loss: 0.0001(0.0025) Grad: 82.8487  LR: 0.000009  \n",
      "Epoch: [1][21600/36908] Elapsed 297m 30s (remain 210m 49s) Loss: 0.0008(0.0025) Grad: 2259.2512  LR: 0.000009  \n",
      "Epoch: [1][21700/36908] Elapsed 298m 53s (remain 209m 26s) Loss: 0.0026(0.0025) Grad: 31177.7715  LR: 0.000009  \n",
      "Epoch: [1][21800/36908] Elapsed 300m 16s (remain 208m 4s) Loss: 0.0001(0.0025) Grad: 21.6593  LR: 0.000009  \n",
      "Epoch: [1][21900/36908] Elapsed 301m 39s (remain 206m 42s) Loss: 0.0001(0.0025) Grad: 42.3983  LR: 0.000009  \n",
      "Epoch: [1][22000/36908] Elapsed 303m 2s (remain 205m 19s) Loss: 0.0001(0.0025) Grad: 54.9935  LR: 0.000009  \n",
      "Epoch: [1][22100/36908] Elapsed 304m 25s (remain 203m 57s) Loss: 0.0005(0.0025) Grad: 8668.5791  LR: 0.000009  \n",
      "Epoch: [1][22200/36908] Elapsed 305m 47s (remain 202m 34s) Loss: 0.0017(0.0025) Grad: 2092.8108  LR: 0.000009  \n",
      "Epoch: [1][22300/36908] Elapsed 307m 9s (remain 201m 11s) Loss: 0.0011(0.0025) Grad: 1841.8604  LR: 0.000009  \n",
      "Epoch: [1][22400/36908] Elapsed 308m 32s (remain 199m 48s) Loss: 0.0000(0.0025) Grad: 29.4725  LR: 0.000009  \n",
      "Epoch: [1][22500/36908] Elapsed 309m 54s (remain 198m 26s) Loss: 0.0103(0.0025) Grad: 23976.9668  LR: 0.000009  \n",
      "Epoch: [1][22600/36908] Elapsed 311m 16s (remain 197m 2s) Loss: 0.0041(0.0025) Grad: 24981.2246  LR: 0.000009  \n",
      "Epoch: [1][22700/36908] Elapsed 312m 38s (remain 195m 39s) Loss: 0.0007(0.0025) Grad: 707.6427  LR: 0.000009  \n",
      "Epoch: [1][22800/36908] Elapsed 314m 1s (remain 194m 17s) Loss: 0.0000(0.0025) Grad: 69.1500  LR: 0.000008  \n",
      "Epoch: [1][22900/36908] Elapsed 315m 23s (remain 192m 54s) Loss: 0.0044(0.0025) Grad: 6209.8354  LR: 0.000008  \n",
      "Epoch: [1][23000/36908] Elapsed 316m 46s (remain 191m 31s) Loss: 0.0119(0.0025) Grad: 166522.3125  LR: 0.000008  \n",
      "Epoch: [1][23100/36908] Elapsed 318m 8s (remain 190m 8s) Loss: 0.0008(0.0025) Grad: 1062.2054  LR: 0.000008  \n",
      "Epoch: [1][23200/36908] Elapsed 319m 31s (remain 188m 46s) Loss: 0.0011(0.0025) Grad: 1293.1685  LR: 0.000008  \n",
      "Epoch: [1][23300/36908] Elapsed 320m 53s (remain 187m 23s) Loss: 0.0001(0.0025) Grad: 67.4701  LR: 0.000008  \n",
      "Epoch: [1][23400/36908] Elapsed 322m 15s (remain 186m 0s) Loss: 0.0059(0.0025) Grad: 29040.8906  LR: 0.000008  \n",
      "Epoch: [1][23500/36908] Elapsed 323m 38s (remain 184m 37s) Loss: 0.0020(0.0025) Grad: 5346.6548  LR: 0.000008  \n",
      "Epoch: [1][23600/36908] Elapsed 325m 1s (remain 183m 15s) Loss: 0.0001(0.0025) Grad: 66.4639  LR: 0.000008  \n",
      "Epoch: [1][23700/36908] Elapsed 326m 24s (remain 181m 53s) Loss: 0.0134(0.0025) Grad: 39994.7969  LR: 0.000008  \n",
      "Epoch: [1][23800/36908] Elapsed 327m 46s (remain 180m 30s) Loss: 0.0001(0.0025) Grad: 18.5326  LR: 0.000008  \n",
      "Epoch: [1][23900/36908] Elapsed 329m 8s (remain 179m 7s) Loss: 0.0021(0.0025) Grad: 6532.5283  LR: 0.000008  \n",
      "Epoch: [1][24000/36908] Elapsed 330m 30s (remain 177m 43s) Loss: 0.0039(0.0025) Grad: 7078.3369  LR: 0.000008  \n",
      "Epoch: [1][24100/36908] Elapsed 331m 53s (remain 176m 21s) Loss: 0.0011(0.0025) Grad: 7865.1353  LR: 0.000008  \n",
      "Epoch: [1][24200/36908] Elapsed 333m 14s (remain 174m 58s) Loss: 0.0001(0.0025) Grad: 34.7432  LR: 0.000008  \n",
      "Epoch: [1][24300/36908] Elapsed 334m 36s (remain 173m 35s) Loss: 0.0021(0.0025) Grad: 6642.2827  LR: 0.000008  \n",
      "Epoch: [1][24400/36908] Elapsed 335m 59s (remain 172m 12s) Loss: 0.0021(0.0025) Grad: 719.1815  LR: 0.000008  \n",
      "Epoch: [1][24500/36908] Elapsed 337m 22s (remain 170m 50s) Loss: 0.0004(0.0025) Grad: 810.1463  LR: 0.000007  \n",
      "Epoch: [1][24600/36908] Elapsed 338m 45s (remain 169m 28s) Loss: 0.0031(0.0025) Grad: 1233.6058  LR: 0.000007  \n",
      "Epoch: [1][24700/36908] Elapsed 340m 7s (remain 168m 5s) Loss: 0.0009(0.0025) Grad: 802.4449  LR: 0.000007  \n",
      "Epoch: [1][24800/36908] Elapsed 341m 30s (remain 166m 42s) Loss: 0.0005(0.0025) Grad: 417.9850  LR: 0.000007  \n",
      "Epoch: [1][24900/36908] Elapsed 342m 54s (remain 165m 20s) Loss: 0.0011(0.0025) Grad: 2777.3210  LR: 0.000007  \n",
      "Epoch: [1][25000/36908] Elapsed 344m 17s (remain 163m 58s) Loss: 0.0025(0.0025) Grad: 18028.6797  LR: 0.000007  \n",
      "Epoch: [1][25100/36908] Elapsed 345m 39s (remain 162m 35s) Loss: 0.0019(0.0025) Grad: 37903.5625  LR: 0.000007  \n",
      "Epoch: [1][25200/36908] Elapsed 347m 0s (remain 161m 12s) Loss: 0.0045(0.0025) Grad: 32662.3945  LR: 0.000007  \n",
      "Epoch: [1][25300/36908] Elapsed 348m 23s (remain 159m 49s) Loss: 0.0001(0.0025) Grad: 19.5903  LR: 0.000007  \n",
      "Epoch: [1][25400/36908] Elapsed 349m 46s (remain 158m 27s) Loss: 0.0000(0.0025) Grad: 64.4328  LR: 0.000007  \n",
      "Epoch: [1][25500/36908] Elapsed 351m 8s (remain 157m 4s) Loss: 0.0001(0.0025) Grad: 26.7834  LR: 0.000007  \n",
      "Epoch: [1][25600/36908] Elapsed 352m 31s (remain 155m 41s) Loss: 0.0003(0.0025) Grad: 687.7691  LR: 0.000007  \n",
      "Epoch: [1][25700/36908] Elapsed 353m 52s (remain 154m 18s) Loss: 0.0001(0.0025) Grad: 241.9621  LR: 0.000007  \n",
      "Epoch: [1][25800/36908] Elapsed 355m 14s (remain 152m 55s) Loss: 0.0005(0.0025) Grad: 1473.7051  LR: 0.000007  \n",
      "Epoch: [1][25900/36908] Elapsed 356m 37s (remain 151m 33s) Loss: 0.0002(0.0025) Grad: 389.4502  LR: 0.000007  \n",
      "Epoch: [1][26000/36908] Elapsed 358m 0s (remain 150m 10s) Loss: 0.0018(0.0025) Grad: 14149.1914  LR: 0.000007  \n",
      "Epoch: [1][26100/36908] Elapsed 359m 24s (remain 148m 48s) Loss: 0.0001(0.0025) Grad: 321.0214  LR: 0.000007  \n",
      "Epoch: [1][26200/36908] Elapsed 360m 46s (remain 147m 25s) Loss: 0.0023(0.0025) Grad: 14942.4854  LR: 0.000006  \n",
      "Epoch: [1][26300/36908] Elapsed 362m 9s (remain 146m 3s) Loss: 0.0001(0.0025) Grad: 534.4901  LR: 0.000006  \n",
      "Epoch: [1][26400/36908] Elapsed 363m 32s (remain 144m 40s) Loss: 0.0010(0.0025) Grad: 12246.0107  LR: 0.000006  \n",
      "Epoch: [1][26500/36908] Elapsed 364m 55s (remain 143m 18s) Loss: 0.0010(0.0025) Grad: 9006.7949  LR: 0.000006  \n",
      "Epoch: [1][26600/36908] Elapsed 366m 18s (remain 141m 56s) Loss: 0.0001(0.0025) Grad: 72.8699  LR: 0.000006  \n",
      "Epoch: [1][26700/36908] Elapsed 367m 41s (remain 140m 33s) Loss: 0.0046(0.0025) Grad: 7919.6279  LR: 0.000006  \n",
      "Epoch: [1][26800/36908] Elapsed 369m 4s (remain 139m 11s) Loss: 0.0007(0.0025) Grad: 2883.3606  LR: 0.000006  \n",
      "Epoch: [1][26900/36908] Elapsed 370m 26s (remain 137m 48s) Loss: 0.0000(0.0025) Grad: 126.0520  LR: 0.000006  \n",
      "Epoch: [1][27000/36908] Elapsed 371m 48s (remain 136m 25s) Loss: 0.0010(0.0025) Grad: 12484.0859  LR: 0.000006  \n",
      "Epoch: [1][27100/36908] Elapsed 373m 12s (remain 135m 3s) Loss: 0.0007(0.0025) Grad: 11256.6787  LR: 0.000006  \n",
      "Epoch: [1][27200/36908] Elapsed 374m 35s (remain 133m 40s) Loss: 0.0001(0.0025) Grad: 232.1637  LR: 0.000006  \n",
      "Epoch: [1][27300/36908] Elapsed 375m 57s (remain 132m 17s) Loss: 0.0053(0.0025) Grad: 9707.3955  LR: 0.000006  \n",
      "Epoch: [1][27400/36908] Elapsed 377m 20s (remain 130m 55s) Loss: 0.0039(0.0025) Grad: 17680.3438  LR: 0.000006  \n",
      "Epoch: [1][27500/36908] Elapsed 378m 43s (remain 129m 32s) Loss: 0.0099(0.0025) Grad: 23521.5781  LR: 0.000006  \n",
      "Epoch: [1][27600/36908] Elapsed 380m 5s (remain 128m 10s) Loss: 0.0001(0.0025) Grad: 10.1220  LR: 0.000006  \n",
      "Epoch: [1][27700/36908] Elapsed 381m 29s (remain 126m 47s) Loss: 0.0000(0.0025) Grad: 652.9844  LR: 0.000006  \n",
      "Epoch: [1][27800/36908] Elapsed 382m 52s (remain 125m 25s) Loss: 0.0101(0.0025) Grad: 23009.5273  LR: 0.000005  \n",
      "Epoch: [1][27900/36908] Elapsed 384m 14s (remain 124m 2s) Loss: 0.0003(0.0025) Grad: 3684.8259  LR: 0.000005  \n",
      "Epoch: [1][28000/36908] Elapsed 385m 36s (remain 122m 39s) Loss: 0.0010(0.0025) Grad: 893.1104  LR: 0.000005  \n",
      "Epoch: [1][28100/36908] Elapsed 386m 59s (remain 121m 17s) Loss: 0.0061(0.0025) Grad: 32737.3750  LR: 0.000005  \n",
      "Epoch: [1][28200/36908] Elapsed 388m 21s (remain 119m 54s) Loss: 0.0001(0.0025) Grad: 10.3959  LR: 0.000005  \n",
      "Epoch: [1][28300/36908] Elapsed 389m 45s (remain 118m 32s) Loss: 0.0000(0.0025) Grad: 6.5910  LR: 0.000005  \n",
      "Epoch: [1][28400/36908] Elapsed 391m 8s (remain 117m 9s) Loss: 0.0090(0.0025) Grad: 956.4827  LR: 0.000005  \n",
      "Epoch: [1][28500/36908] Elapsed 392m 31s (remain 115m 46s) Loss: 0.0004(0.0025) Grad: 504.2680  LR: 0.000005  \n",
      "Epoch: [1][28600/36908] Elapsed 393m 53s (remain 114m 24s) Loss: 0.0063(0.0025) Grad: 2268.8650  LR: 0.000005  \n",
      "Epoch: [1][28700/36908] Elapsed 395m 16s (remain 113m 1s) Loss: 0.0009(0.0025) Grad: 1727.7013  LR: 0.000005  \n",
      "Epoch: [1][28800/36908] Elapsed 396m 39s (remain 111m 39s) Loss: 0.0007(0.0025) Grad: 299.7131  LR: 0.000005  \n",
      "Epoch: [1][28900/36908] Elapsed 398m 2s (remain 110m 16s) Loss: 0.0038(0.0025) Grad: 6869.1294  LR: 0.000005  \n",
      "Epoch: [1][29000/36908] Elapsed 399m 25s (remain 108m 54s) Loss: 0.0000(0.0025) Grad: 9.5052  LR: 0.000005  \n",
      "Epoch: [1][29100/36908] Elapsed 400m 49s (remain 107m 31s) Loss: 0.0048(0.0025) Grad: 6338.8667  LR: 0.000005  \n",
      "Epoch: [1][29200/36908] Elapsed 402m 11s (remain 106m 9s) Loss: 0.0004(0.0025) Grad: 456.3660  LR: 0.000005  \n",
      "Epoch: [1][29300/36908] Elapsed 403m 35s (remain 104m 46s) Loss: 0.0001(0.0025) Grad: 136.8174  LR: 0.000005  \n",
      "Epoch: [1][29400/36908] Elapsed 404m 58s (remain 103m 24s) Loss: 0.0008(0.0025) Grad: 1480.2621  LR: 0.000005  \n",
      "Epoch: [1][29500/36908] Elapsed 406m 20s (remain 102m 1s) Loss: 0.0005(0.0025) Grad: 390.7149  LR: 0.000004  \n",
      "Epoch: [1][29600/36908] Elapsed 407m 44s (remain 100m 39s) Loss: 0.0075(0.0025) Grad: 5554.8608  LR: 0.000004  \n",
      "Epoch: [1][29700/36908] Elapsed 409m 7s (remain 99m 16s) Loss: 0.0010(0.0025) Grad: 7720.2021  LR: 0.000004  \n",
      "Epoch: [1][29800/36908] Elapsed 410m 29s (remain 97m 53s) Loss: 0.0006(0.0025) Grad: 7797.2720  LR: 0.000004  \n",
      "Epoch: [1][29900/36908] Elapsed 411m 51s (remain 96m 30s) Loss: 0.0030(0.0025) Grad: 9684.7852  LR: 0.000004  \n",
      "Epoch: [1][30000/36908] Elapsed 413m 13s (remain 95m 8s) Loss: 0.0003(0.0025) Grad: 2006.1033  LR: 0.000004  \n",
      "Epoch: [1][30100/36908] Elapsed 414m 35s (remain 93m 45s) Loss: 0.0051(0.0025) Grad: 9957.1416  LR: 0.000004  \n",
      "Epoch: [1][30200/36908] Elapsed 415m 57s (remain 92m 22s) Loss: 0.0001(0.0025) Grad: 37.0416  LR: 0.000004  \n",
      "Epoch: [1][30300/36908] Elapsed 417m 19s (remain 90m 59s) Loss: 0.0021(0.0025) Grad: 3951.3093  LR: 0.000004  \n",
      "Epoch: [1][30400/36908] Elapsed 418m 42s (remain 89m 37s) Loss: 0.0016(0.0025) Grad: 9068.7861  LR: 0.000004  \n",
      "Epoch: [1][30500/36908] Elapsed 420m 5s (remain 88m 14s) Loss: 0.0003(0.0025) Grad: 500.0144  LR: 0.000004  \n",
      "Epoch: [1][30600/36908] Elapsed 421m 27s (remain 86m 51s) Loss: 0.0008(0.0025) Grad: 558.9172  LR: 0.000004  \n",
      "Epoch: [1][30700/36908] Elapsed 422m 50s (remain 85m 29s) Loss: 0.0008(0.0025) Grad: 823.4493  LR: 0.000004  \n",
      "Epoch: [1][30800/36908] Elapsed 424m 13s (remain 84m 6s) Loss: 0.0002(0.0025) Grad: 631.2302  LR: 0.000004  \n",
      "Epoch: [1][30900/36908] Elapsed 425m 36s (remain 82m 44s) Loss: 0.0002(0.0025) Grad: 825.6547  LR: 0.000004  \n",
      "Epoch: [1][31000/36908] Elapsed 426m 59s (remain 81m 21s) Loss: 0.0001(0.0025) Grad: 36.9660  LR: 0.000004  \n",
      "Epoch: [1][31100/36908] Elapsed 428m 20s (remain 79m 58s) Loss: 0.0001(0.0025) Grad: 93.1466  LR: 0.000003  \n",
      "Epoch: [1][31200/36908] Elapsed 429m 43s (remain 78m 36s) Loss: 0.0003(0.0025) Grad: 1066.5969  LR: 0.000003  \n",
      "Epoch: [1][31300/36908] Elapsed 431m 6s (remain 77m 13s) Loss: 0.0002(0.0025) Grad: 425.6101  LR: 0.000003  \n",
      "Epoch: [1][31400/36908] Elapsed 432m 27s (remain 75m 50s) Loss: 0.0003(0.0025) Grad: 4713.3037  LR: 0.000003  \n",
      "Epoch: [1][31500/36908] Elapsed 433m 50s (remain 74m 27s) Loss: 0.0001(0.0025) Grad: 95.5564  LR: 0.000003  \n",
      "Epoch: [1][31600/36908] Elapsed 435m 12s (remain 73m 5s) Loss: 0.0063(0.0025) Grad: 18840.3340  LR: 0.000003  \n",
      "Epoch: [1][31700/36908] Elapsed 436m 34s (remain 71m 42s) Loss: 0.0008(0.0025) Grad: 25068.8555  LR: 0.000003  \n",
      "Epoch: [1][31800/36908] Elapsed 437m 57s (remain 70m 20s) Loss: 0.0052(0.0025) Grad: 9008.1436  LR: 0.000003  \n",
      "Epoch: [1][31900/36908] Elapsed 439m 20s (remain 68m 57s) Loss: 0.0099(0.0025) Grad: 63398.1797  LR: 0.000003  \n",
      "Epoch: [1][32000/36908] Elapsed 440m 43s (remain 67m 34s) Loss: 0.0059(0.0025) Grad: 64255.8125  LR: 0.000003  \n",
      "Epoch: [1][32100/36908] Elapsed 442m 7s (remain 66m 12s) Loss: 0.0001(0.0025) Grad: 120.6039  LR: 0.000003  \n",
      "Epoch: [1][32200/36908] Elapsed 443m 30s (remain 64m 49s) Loss: 0.0001(0.0025) Grad: 30.4375  LR: 0.000003  \n",
      "Epoch: [1][32300/36908] Elapsed 444m 53s (remain 63m 27s) Loss: 0.0016(0.0025) Grad: 16094.3613  LR: 0.000003  \n",
      "Epoch: [1][32400/36908] Elapsed 446m 15s (remain 62m 4s) Loss: 0.0027(0.0025) Grad: 6990.5337  LR: 0.000003  \n",
      "Epoch: [1][32500/36908] Elapsed 447m 38s (remain 60m 41s) Loss: 0.0001(0.0025) Grad: 38.0672  LR: 0.000003  \n",
      "Epoch: [1][32600/36908] Elapsed 449m 0s (remain 59m 19s) Loss: 0.0001(0.0025) Grad: 253.2528  LR: 0.000003  \n",
      "Epoch: [1][32700/36908] Elapsed 450m 24s (remain 57m 56s) Loss: 0.0099(0.0025) Grad: 17988.0156  LR: 0.000003  \n",
      "Epoch: [1][32800/36908] Elapsed 451m 46s (remain 56m 33s) Loss: 0.0060(0.0025) Grad: 73942.2031  LR: 0.000002  \n",
      "Epoch: [1][32900/36908] Elapsed 453m 8s (remain 55m 11s) Loss: 0.0008(0.0025) Grad: 580.9717  LR: 0.000002  \n",
      "Epoch: [1][33000/36908] Elapsed 454m 29s (remain 53m 48s) Loss: 0.0001(0.0025) Grad: 421.6321  LR: 0.000002  \n",
      "Epoch: [1][33100/36908] Elapsed 455m 53s (remain 52m 25s) Loss: 0.0055(0.0025) Grad: 15710.8887  LR: 0.000002  \n",
      "Epoch: [1][33200/36908] Elapsed 457m 15s (remain 51m 3s) Loss: 0.0007(0.0025) Grad: 1464.9521  LR: 0.000002  \n",
      "Epoch: [1][33300/36908] Elapsed 458m 38s (remain 49m 40s) Loss: 0.0007(0.0025) Grad: 1894.5175  LR: 0.000002  \n",
      "Epoch: [1][33400/36908] Elapsed 460m 2s (remain 48m 18s) Loss: 0.0031(0.0025) Grad: 16818.9844  LR: 0.000002  \n",
      "Epoch: [1][33500/36908] Elapsed 461m 26s (remain 46m 55s) Loss: 0.0038(0.0025) Grad: 22167.2188  LR: 0.000002  \n",
      "Epoch: [1][33600/36908] Elapsed 462m 49s (remain 45m 33s) Loss: 0.0003(0.0025) Grad: 4435.5596  LR: 0.000002  \n",
      "Epoch: [1][33700/36908] Elapsed 464m 12s (remain 44m 10s) Loss: 0.0083(0.0025) Grad: 56622.9844  LR: 0.000002  \n",
      "Epoch: [1][33800/36908] Elapsed 465m 34s (remain 42m 47s) Loss: 0.0089(0.0025) Grad: 31635.3867  LR: 0.000002  \n",
      "Epoch: [1][33900/36908] Elapsed 466m 56s (remain 41m 25s) Loss: 0.0022(0.0025) Grad: 2567.9531  LR: 0.000002  \n",
      "Epoch: [1][34000/36908] Elapsed 468m 18s (remain 40m 2s) Loss: 0.0001(0.0025) Grad: 16.8235  LR: 0.000002  \n",
      "Epoch: [1][34100/36908] Elapsed 469m 42s (remain 38m 39s) Loss: 0.0001(0.0025) Grad: 19.6066  LR: 0.000002  \n",
      "Epoch: [1][34200/36908] Elapsed 471m 5s (remain 37m 17s) Loss: 0.0016(0.0025) Grad: 7211.0977  LR: 0.000002  \n",
      "Epoch: [1][34300/36908] Elapsed 472m 27s (remain 35m 54s) Loss: 0.0042(0.0025) Grad: 5811.9155  LR: 0.000002  \n",
      "Epoch: [1][34400/36908] Elapsed 473m 51s (remain 34m 31s) Loss: 0.0117(0.0025) Grad: 80591.8125  LR: 0.000002  \n",
      "Epoch: [1][34500/36908] Elapsed 475m 15s (remain 33m 9s) Loss: 0.0001(0.0025) Grad: 32.7410  LR: 0.000001  \n",
      "Epoch: [1][34600/36908] Elapsed 476m 39s (remain 31m 46s) Loss: 0.0001(0.0025) Grad: 925.6030  LR: 0.000001  \n",
      "Epoch: [1][34700/36908] Elapsed 478m 1s (remain 30m 24s) Loss: 0.0031(0.0025) Grad: 19207.0801  LR: 0.000001  \n",
      "Epoch: [1][34800/36908] Elapsed 479m 25s (remain 29m 1s) Loss: 0.0036(0.0025) Grad: 16839.8672  LR: 0.000001  \n",
      "Epoch: [1][34900/36908] Elapsed 480m 48s (remain 27m 38s) Loss: 0.0029(0.0025) Grad: 30400.5059  LR: 0.000001  \n",
      "Epoch: [1][35000/36908] Elapsed 482m 10s (remain 26m 16s) Loss: 0.0002(0.0025) Grad: 391.2926  LR: 0.000001  \n",
      "Epoch: [1][35100/36908] Elapsed 483m 31s (remain 24m 53s) Loss: 0.0018(0.0025) Grad: 12244.8438  LR: 0.000001  \n",
      "Epoch: [1][35200/36908] Elapsed 484m 54s (remain 23m 30s) Loss: 0.0016(0.0025) Grad: 2233.6821  LR: 0.000001  \n",
      "Epoch: [1][35300/36908] Elapsed 486m 18s (remain 22m 8s) Loss: 0.0001(0.0025) Grad: 93.1612  LR: 0.000001  \n",
      "Epoch: [1][35400/36908] Elapsed 487m 41s (remain 20m 45s) Loss: 0.0014(0.0025) Grad: 9327.5781  LR: 0.000001  \n",
      "Epoch: [1][35500/36908] Elapsed 489m 3s (remain 19m 22s) Loss: 0.0004(0.0025) Grad: 4162.2441  LR: 0.000001  \n",
      "Epoch: [1][35600/36908] Elapsed 490m 27s (remain 18m 0s) Loss: 0.0017(0.0025) Grad: 4450.1050  LR: 0.000001  \n",
      "Epoch: [1][35700/36908] Elapsed 491m 50s (remain 16m 37s) Loss: 0.0043(0.0025) Grad: 25337.1504  LR: 0.000001  \n",
      "Epoch: [1][35800/36908] Elapsed 493m 12s (remain 15m 15s) Loss: 0.0000(0.0025) Grad: 4.9440  LR: 0.000001  \n",
      "Epoch: [1][35900/36908] Elapsed 494m 35s (remain 13m 52s) Loss: 0.0001(0.0025) Grad: 69.4608  LR: 0.000001  \n",
      "Epoch: [1][36000/36908] Elapsed 495m 57s (remain 12m 29s) Loss: 0.0034(0.0025) Grad: 12944.7588  LR: 0.000001  \n",
      "Epoch: [1][36100/36908] Elapsed 497m 20s (remain 11m 7s) Loss: 0.0028(0.0025) Grad: 5573.7637  LR: 0.000000  \n",
      "Epoch: [1][36200/36908] Elapsed 498m 42s (remain 9m 44s) Loss: 0.0027(0.0025) Grad: 11585.2676  LR: 0.000000  \n",
      "Epoch: [1][36300/36908] Elapsed 500m 6s (remain 8m 21s) Loss: 0.0012(0.0025) Grad: 6179.9189  LR: 0.000000  \n",
      "Epoch: [1][36400/36908] Elapsed 501m 28s (remain 6m 59s) Loss: 0.0000(0.0025) Grad: 20.0516  LR: 0.000000  \n",
      "Epoch: [1][36500/36908] Elapsed 502m 52s (remain 5m 36s) Loss: 0.0112(0.0025) Grad: 17098.8809  LR: 0.000000  \n",
      "Epoch: [1][36600/36908] Elapsed 504m 15s (remain 4m 13s) Loss: 0.0003(0.0025) Grad: 959.1400  LR: 0.000000  \n",
      "Epoch: [1][36700/36908] Elapsed 505m 38s (remain 2m 51s) Loss: 0.0106(0.0025) Grad: 17159.5508  LR: 0.000000  \n",
      "Epoch: [1][36800/36908] Elapsed 507m 2s (remain 1m 28s) Loss: 0.0025(0.0025) Grad: 33243.9102  LR: 0.000000  \n",
      "Epoch: [1][36900/36908] Elapsed 508m 24s (remain 0m 5s) Loss: 0.0003(0.0025) Grad: 1464.9219  LR: 0.000000  \n",
      "Epoch: [1][36907/36908] Elapsed 508m 29s (remain 0m 0s) Loss: 0.0062(0.0025) Grad: 36571.6875  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 14m 54s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 30s (remain 5m 26s) Loss: 0.0107(0.0054) \n",
      "EVAL: [200/1192] Elapsed 1m 0s (remain 4m 57s) Loss: 0.0035(0.0064) \n",
      "EVAL: [300/1192] Elapsed 1m 29s (remain 4m 24s) Loss: 0.0062(0.0068) \n",
      "EVAL: [400/1192] Elapsed 1m 58s (remain 3m 54s) Loss: 0.0077(0.0071) \n",
      "EVAL: [500/1192] Elapsed 2m 27s (remain 3m 24s) Loss: 0.0120(0.0065) \n",
      "EVAL: [600/1192] Elapsed 2m 56s (remain 2m 53s) Loss: 0.0009(0.0068) \n",
      "EVAL: [700/1192] Elapsed 3m 26s (remain 2m 24s) Loss: 0.1069(0.0083) \n",
      "EVAL: [800/1192] Elapsed 3m 55s (remain 1m 55s) Loss: 0.0025(0.0086) \n",
      "EVAL: [900/1192] Elapsed 4m 24s (remain 1m 25s) Loss: 0.0014(0.0085) \n",
      "EVAL: [1000/1192] Elapsed 4m 54s (remain 0m 56s) Loss: 0.0000(0.0084) \n",
      "EVAL: [1100/1192] Elapsed 5m 24s (remain 0m 26s) Loss: 0.0019(0.0080) \n",
      "EVAL: [1191/1192] Elapsed 5m 51s (remain 0m 0s) Loss: 0.0000(0.0078) \n",
      "Epoch 1 - avg_train_loss: 0.0025  avg_val_loss: 0.0078  time: 30865s\n",
      "Epoch 1 - Score: 0.8884\n",
      "Epoch 1 - Save Best Score: 0.8884 Model\n",
      "========== fold: 1 training ==========\n",
      "get pseudo plain from ../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_1.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_1.npy\n",
      "(612602, 6) (612602, 950)\n",
      "(100000, 7)\n",
      "(110725, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n",
      "Epoch: [1][0/36908] Elapsed 0m 1s (remain 793m 35s) Loss: 0.0035(0.0035) Grad: 2510.8281  LR: 0.000000  \n",
      "Epoch: [1][100/36908] Elapsed 1m 23s (remain 508m 50s) Loss: 0.0007(0.0028) Grad: 306.1113  LR: 0.000001  \n",
      "Epoch: [1][200/36908] Elapsed 2m 45s (remain 502m 28s) Loss: 0.0001(0.0029) Grad: 40.0067  LR: 0.000001  \n",
      "Epoch: [1][300/36908] Elapsed 4m 6s (remain 499m 8s) Loss: 0.0006(0.0030) Grad: 75.9404  LR: 0.000002  \n",
      "Epoch: [1][400/36908] Elapsed 5m 29s (remain 499m 25s) Loss: 0.0099(0.0030) Grad: 10893.7949  LR: 0.000002  \n",
      "Epoch: [1][500/36908] Elapsed 6m 50s (remain 497m 9s) Loss: 0.0011(0.0029) Grad: 1661.6960  LR: 0.000003  \n",
      "Epoch: [1][600/36908] Elapsed 8m 11s (remain 495m 9s) Loss: 0.0003(0.0029) Grad: 963.6631  LR: 0.000003  \n",
      "Epoch: [1][700/36908] Elapsed 9m 33s (remain 493m 47s) Loss: 0.0019(0.0028) Grad: 2418.4263  LR: 0.000004  \n",
      "Epoch: [1][800/36908] Elapsed 10m 57s (remain 493m 50s) Loss: 0.0013(0.0028) Grad: 195.3564  LR: 0.000004  \n",
      "Epoch: [1][900/36908] Elapsed 12m 20s (remain 492m 55s) Loss: 0.0000(0.0028) Grad: 86.3262  LR: 0.000005  \n",
      "Epoch: [1][1000/36908] Elapsed 13m 41s (remain 490m 52s) Loss: 0.0019(0.0028) Grad: 1789.7905  LR: 0.000005  \n",
      "Epoch: [1][1100/36908] Elapsed 15m 3s (remain 489m 41s) Loss: 0.0007(0.0028) Grad: 236.9086  LR: 0.000006  \n",
      "Epoch: [1][1200/36908] Elapsed 16m 26s (remain 488m 46s) Loss: 0.0091(0.0029) Grad: 22507.1230  LR: 0.000007  \n",
      "Epoch: [1][1300/36908] Elapsed 17m 49s (remain 487m 38s) Loss: 0.0001(0.0029) Grad: 15.6688  LR: 0.000007  \n",
      "Epoch: [1][1400/36908] Elapsed 19m 11s (remain 486m 14s) Loss: 0.0001(0.0029) Grad: 12.7347  LR: 0.000008  \n",
      "Epoch: [1][1500/36908] Elapsed 20m 32s (remain 484m 34s) Loss: 0.0018(0.0028) Grad: 1564.7692  LR: 0.000008  \n",
      "Epoch: [1][1600/36908] Elapsed 21m 52s (remain 482m 33s) Loss: 0.0023(0.0029) Grad: 3579.6924  LR: 0.000009  \n",
      "Epoch: [1][1700/36908] Elapsed 23m 14s (remain 480m 58s) Loss: 0.0005(0.0028) Grad: 24.8690  LR: 0.000009  \n",
      "Epoch: [1][1800/36908] Elapsed 24m 36s (remain 479m 47s) Loss: 0.0061(0.0029) Grad: 13160.6787  LR: 0.000010  \n",
      "Epoch: [1][1900/36908] Elapsed 25m 58s (remain 478m 11s) Loss: 0.0001(0.0029) Grad: 25.5348  LR: 0.000010  \n",
      "Epoch: [1][2000/36908] Elapsed 27m 19s (remain 476m 40s) Loss: 0.0016(0.0029) Grad: 338.4099  LR: 0.000011  \n",
      "Epoch: [1][2100/36908] Elapsed 28m 42s (remain 475m 34s) Loss: 0.0018(0.0029) Grad: 2674.9260  LR: 0.000011  \n",
      "Epoch: [1][2200/36908] Elapsed 30m 5s (remain 474m 35s) Loss: 0.0043(0.0029) Grad: 11584.8428  LR: 0.000012  \n",
      "Epoch: [1][2300/36908] Elapsed 31m 27s (remain 473m 11s) Loss: 0.0023(0.0029) Grad: 317.8724  LR: 0.000012  \n",
      "Epoch: [1][2400/36908] Elapsed 32m 49s (remain 471m 42s) Loss: 0.0047(0.0029) Grad: 8090.9463  LR: 0.000013  \n",
      "Epoch: [1][2500/36908] Elapsed 34m 11s (remain 470m 20s) Loss: 0.0003(0.0029) Grad: 29.2017  LR: 0.000014  \n",
      "Epoch: [1][2600/36908] Elapsed 35m 32s (remain 468m 49s) Loss: 0.0043(0.0029) Grad: 3453.7437  LR: 0.000014  \n",
      "Epoch: [1][2700/36908] Elapsed 36m 54s (remain 467m 19s) Loss: 0.0011(0.0029) Grad: 2841.5845  LR: 0.000015  \n",
      "Epoch: [1][2800/36908] Elapsed 38m 17s (remain 466m 17s) Loss: 0.0042(0.0029) Grad: 2486.8718  LR: 0.000015  \n",
      "Epoch: [1][2900/36908] Elapsed 39m 40s (remain 465m 6s) Loss: 0.0002(0.0029) Grad: 18.4905  LR: 0.000016  \n",
      "Epoch: [1][3000/36908] Elapsed 41m 2s (remain 463m 40s) Loss: 0.0088(0.0029) Grad: 2708.3955  LR: 0.000016  \n",
      "Epoch: [1][3100/36908] Elapsed 42m 23s (remain 462m 11s) Loss: 0.0028(0.0029) Grad: 868.0437  LR: 0.000017  \n",
      "Epoch: [1][3200/36908] Elapsed 43m 45s (remain 460m 42s) Loss: 0.0013(0.0029) Grad: 2160.6643  LR: 0.000017  \n",
      "Epoch: [1][3300/36908] Elapsed 45m 7s (remain 459m 28s) Loss: 0.0008(0.0029) Grad: 250.8688  LR: 0.000018  \n",
      "Epoch: [1][3400/36908] Elapsed 46m 30s (remain 458m 15s) Loss: 0.0001(0.0029) Grad: 31.1966  LR: 0.000018  \n",
      "Epoch: [1][3500/36908] Elapsed 47m 53s (remain 456m 59s) Loss: 0.0009(0.0029) Grad: 133.6616  LR: 0.000019  \n",
      "Epoch: [1][3600/36908] Elapsed 49m 14s (remain 455m 30s) Loss: 0.0001(0.0029) Grad: 253.8179  LR: 0.000020  \n",
      "Epoch: [1][3700/36908] Elapsed 50m 37s (remain 454m 13s) Loss: 0.0008(0.0029) Grad: 818.6832  LR: 0.000020  \n",
      "Epoch: [1][3800/36908] Elapsed 51m 59s (remain 452m 53s) Loss: 0.0025(0.0029) Grad: 2699.5110  LR: 0.000020  \n",
      "Epoch: [1][3900/36908] Elapsed 53m 21s (remain 451m 25s) Loss: 0.0051(0.0029) Grad: 7996.5088  LR: 0.000020  \n",
      "Epoch: [1][4000/36908] Elapsed 54m 43s (remain 450m 4s) Loss: 0.0026(0.0029) Grad: 12833.7686  LR: 0.000020  \n",
      "Epoch: [1][4100/36908] Elapsed 56m 6s (remain 448m 52s) Loss: 0.0001(0.0029) Grad: 77.1046  LR: 0.000020  \n",
      "Epoch: [1][4200/36908] Elapsed 57m 29s (remain 447m 37s) Loss: 0.0007(0.0029) Grad: 183.6036  LR: 0.000020  \n",
      "Epoch: [1][4300/36908] Elapsed 58m 51s (remain 446m 13s) Loss: 0.0002(0.0029) Grad: 112.5747  LR: 0.000020  \n",
      "Epoch: [1][4400/36908] Elapsed 60m 13s (remain 444m 47s) Loss: 0.0016(0.0029) Grad: 2579.1760  LR: 0.000020  \n",
      "Epoch: [1][4500/36908] Elapsed 61m 35s (remain 443m 29s) Loss: 0.0022(0.0029) Grad: 1830.8765  LR: 0.000020  \n",
      "Epoch: [1][4600/36908] Elapsed 62m 57s (remain 442m 2s) Loss: 0.0019(0.0029) Grad: 7531.0142  LR: 0.000019  \n",
      "Epoch: [1][4700/36908] Elapsed 64m 19s (remain 440m 42s) Loss: 0.0002(0.0030) Grad: 483.4743  LR: 0.000019  \n",
      "Epoch: [1][4800/36908] Elapsed 65m 40s (remain 439m 13s) Loss: 0.0053(0.0029) Grad: 1997.3799  LR: 0.000019  \n",
      "Epoch: [1][4900/36908] Elapsed 67m 2s (remain 437m 49s) Loss: 0.0007(0.0029) Grad: 793.4878  LR: 0.000019  \n",
      "Epoch: [1][5000/36908] Elapsed 68m 24s (remain 436m 24s) Loss: 0.0009(0.0029) Grad: 441.0104  LR: 0.000019  \n",
      "Epoch: [1][5100/36908] Elapsed 69m 47s (remain 435m 10s) Loss: 0.0137(0.0029) Grad: 15547.9424  LR: 0.000019  \n",
      "Epoch: [1][5200/36908] Elapsed 71m 9s (remain 433m 50s) Loss: 0.0019(0.0029) Grad: 1862.1552  LR: 0.000019  \n",
      "Epoch: [1][5300/36908] Elapsed 72m 31s (remain 432m 28s) Loss: 0.0007(0.0029) Grad: 734.6863  LR: 0.000019  \n",
      "Epoch: [1][5400/36908] Elapsed 73m 54s (remain 431m 10s) Loss: 0.0160(0.0029) Grad: 43172.9727  LR: 0.000019  \n",
      "Epoch: [1][5500/36908] Elapsed 75m 16s (remain 429m 48s) Loss: 0.0036(0.0029) Grad: 5795.2993  LR: 0.000019  \n",
      "Epoch: [1][5600/36908] Elapsed 76m 40s (remain 428m 35s) Loss: 0.0010(0.0029) Grad: 3144.0330  LR: 0.000019  \n",
      "Epoch: [1][5700/36908] Elapsed 78m 3s (remain 427m 15s) Loss: 0.0001(0.0029) Grad: 207.9727  LR: 0.000019  \n",
      "Epoch: [1][5800/36908] Elapsed 79m 24s (remain 425m 50s) Loss: 0.0035(0.0029) Grad: 10349.3486  LR: 0.000019  \n",
      "Epoch: [1][5900/36908] Elapsed 80m 46s (remain 424m 26s) Loss: 0.0001(0.0029) Grad: 165.2507  LR: 0.000019  \n",
      "Epoch: [1][6000/36908] Elapsed 82m 8s (remain 423m 2s) Loss: 0.0020(0.0029) Grad: 1354.2313  LR: 0.000019  \n",
      "Epoch: [1][6100/36908] Elapsed 83m 30s (remain 421m 41s) Loss: 0.0034(0.0029) Grad: 5337.4751  LR: 0.000019  \n",
      "Epoch: [1][6200/36908] Elapsed 84m 53s (remain 420m 20s) Loss: 0.0015(0.0029) Grad: 782.9998  LR: 0.000018  \n",
      "Epoch: [1][6300/36908] Elapsed 86m 15s (remain 419m 2s) Loss: 0.0159(0.0029) Grad: 34191.1289  LR: 0.000018  \n",
      "Epoch: [1][6400/36908] Elapsed 87m 39s (remain 417m 47s) Loss: 0.0131(0.0029) Grad: 49637.5430  LR: 0.000018  \n",
      "Epoch: [1][6500/36908] Elapsed 89m 2s (remain 416m 29s) Loss: 0.0036(0.0029) Grad: 3130.9355  LR: 0.000018  \n",
      "Epoch: [1][6600/36908] Elapsed 90m 24s (remain 415m 4s) Loss: 0.0007(0.0029) Grad: 281.7780  LR: 0.000018  \n",
      "Epoch: [1][6700/36908] Elapsed 91m 45s (remain 413m 37s) Loss: 0.0039(0.0029) Grad: 7629.6338  LR: 0.000018  \n",
      "Epoch: [1][6800/36908] Elapsed 93m 6s (remain 412m 10s) Loss: 0.0125(0.0029) Grad: 36521.0352  LR: 0.000018  \n",
      "Epoch: [1][6900/36908] Elapsed 94m 28s (remain 410m 47s) Loss: 0.0127(0.0029) Grad: 18952.9785  LR: 0.000018  \n",
      "Epoch: [1][7000/36908] Elapsed 95m 50s (remain 409m 26s) Loss: 0.0003(0.0029) Grad: 164.2140  LR: 0.000018  \n",
      "Epoch: [1][7100/36908] Elapsed 97m 12s (remain 408m 0s) Loss: 0.0003(0.0029) Grad: 230.4902  LR: 0.000018  \n",
      "Epoch: [1][7200/36908] Elapsed 98m 33s (remain 406m 35s) Loss: 0.0017(0.0029) Grad: 14825.2764  LR: 0.000018  \n",
      "Epoch: [1][7300/36908] Elapsed 99m 56s (remain 405m 15s) Loss: 0.0007(0.0029) Grad: 516.6241  LR: 0.000018  \n",
      "Epoch: [1][7400/36908] Elapsed 101m 18s (remain 403m 54s) Loss: 0.0027(0.0029) Grad: 14204.9209  LR: 0.000018  \n",
      "Epoch: [1][7500/36908] Elapsed 102m 40s (remain 402m 31s) Loss: 0.0008(0.0029) Grad: 40.5148  LR: 0.000018  \n",
      "Epoch: [1][7600/36908] Elapsed 104m 3s (remain 401m 13s) Loss: 0.0016(0.0029) Grad: 3250.4817  LR: 0.000018  \n",
      "Epoch: [1][7700/36908] Elapsed 105m 26s (remain 399m 55s) Loss: 0.0001(0.0029) Grad: 45.5406  LR: 0.000018  \n",
      "Epoch: [1][7800/36908] Elapsed 106m 50s (remain 398m 37s) Loss: 0.0005(0.0029) Grad: 145.0161  LR: 0.000018  \n",
      "Epoch: [1][7900/36908] Elapsed 108m 13s (remain 397m 17s) Loss: 0.0001(0.0029) Grad: 217.3318  LR: 0.000017  \n",
      "Epoch: [1][8000/36908] Elapsed 109m 34s (remain 395m 54s) Loss: 0.0000(0.0029) Grad: 864.7245  LR: 0.000017  \n",
      "Epoch: [1][8100/36908] Elapsed 110m 56s (remain 394m 30s) Loss: 0.0058(0.0029) Grad: 40923.7539  LR: 0.000017  \n",
      "Epoch: [1][8200/36908] Elapsed 112m 17s (remain 393m 3s) Loss: 0.0063(0.0029) Grad: 40003.5312  LR: 0.000017  \n",
      "Epoch: [1][8300/36908] Elapsed 113m 39s (remain 391m 40s) Loss: 0.0254(0.0029) Grad: 173652.7344  LR: 0.000017  \n",
      "Epoch: [1][8400/36908] Elapsed 115m 1s (remain 390m 18s) Loss: 0.0080(0.0029) Grad: 78901.4375  LR: 0.000017  \n",
      "Epoch: [1][8500/36908] Elapsed 116m 22s (remain 388m 52s) Loss: 0.0025(0.0029) Grad: 14732.6416  LR: 0.000017  \n",
      "Epoch: [1][8600/36908] Elapsed 117m 45s (remain 387m 34s) Loss: 0.0029(0.0029) Grad: 20364.7891  LR: 0.000017  \n",
      "Epoch: [1][8700/36908] Elapsed 119m 7s (remain 386m 11s) Loss: 0.0002(0.0029) Grad: 946.8276  LR: 0.000017  \n",
      "Epoch: [1][8800/36908] Elapsed 120m 29s (remain 384m 47s) Loss: 0.0015(0.0029) Grad: 3030.1997  LR: 0.000017  \n",
      "Epoch: [1][8900/36908] Elapsed 121m 52s (remain 383m 27s) Loss: 0.0008(0.0029) Grad: 478.8713  LR: 0.000017  \n",
      "Epoch: [1][9000/36908] Elapsed 123m 13s (remain 382m 3s) Loss: 0.0025(0.0029) Grad: 19438.0918  LR: 0.000017  \n",
      "Epoch: [1][9100/36908] Elapsed 124m 35s (remain 380m 39s) Loss: 0.0163(0.0029) Grad: 100167.1484  LR: 0.000017  \n",
      "Epoch: [1][9200/36908] Elapsed 125m 59s (remain 379m 22s) Loss: 0.0013(0.0029) Grad: 6561.6045  LR: 0.000017  \n",
      "Epoch: [1][9300/36908] Elapsed 127m 22s (remain 378m 3s) Loss: 0.0005(0.0029) Grad: 11156.7627  LR: 0.000017  \n",
      "Epoch: [1][9400/36908] Elapsed 128m 44s (remain 376m 41s) Loss: 0.0001(0.0029) Grad: 56.2854  LR: 0.000017  \n",
      "Epoch: [1][9500/36908] Elapsed 130m 6s (remain 375m 19s) Loss: 0.0009(0.0029) Grad: 3356.2502  LR: 0.000017  \n",
      "Epoch: [1][9600/36908] Elapsed 131m 28s (remain 373m 56s) Loss: 0.0052(0.0029) Grad: 80612.6094  LR: 0.000016  \n",
      "Epoch: [1][9700/36908] Elapsed 132m 52s (remain 372m 38s) Loss: 0.0177(0.0029) Grad: 35471.1055  LR: 0.000016  \n",
      "Epoch: [1][9800/36908] Elapsed 134m 15s (remain 371m 19s) Loss: 0.0008(0.0029) Grad: 4989.0669  LR: 0.000016  \n",
      "Epoch: [1][9900/36908] Elapsed 135m 37s (remain 369m 56s) Loss: 0.0003(0.0029) Grad: 232.8517  LR: 0.000016  \n",
      "Epoch: [1][10000/36908] Elapsed 136m 59s (remain 368m 34s) Loss: 0.0066(0.0029) Grad: 18933.0762  LR: 0.000016  \n",
      "Epoch: [1][10100/36908] Elapsed 138m 21s (remain 367m 10s) Loss: 0.0032(0.0029) Grad: 1846.5026  LR: 0.000016  \n",
      "Epoch: [1][10200/36908] Elapsed 139m 44s (remain 365m 50s) Loss: 0.0081(0.0029) Grad: 15204.0654  LR: 0.000016  \n",
      "Epoch: [1][10300/36908] Elapsed 141m 6s (remain 364m 28s) Loss: 0.0025(0.0029) Grad: 3146.5417  LR: 0.000016  \n",
      "Epoch: [1][10400/36908] Elapsed 142m 29s (remain 363m 7s) Loss: 0.0018(0.0029) Grad: 8847.7129  LR: 0.000016  \n",
      "Epoch: [1][10500/36908] Elapsed 143m 52s (remain 361m 48s) Loss: 0.0006(0.0029) Grad: 892.6591  LR: 0.000016  \n",
      "Epoch: [1][10600/36908] Elapsed 145m 16s (remain 360m 30s) Loss: 0.0041(0.0029) Grad: 13934.5488  LR: 0.000016  \n",
      "Epoch: [1][10700/36908] Elapsed 146m 38s (remain 359m 8s) Loss: 0.0007(0.0029) Grad: 458.7637  LR: 0.000016  \n",
      "Epoch: [1][10800/36908] Elapsed 148m 1s (remain 357m 47s) Loss: 0.0008(0.0029) Grad: 2976.8037  LR: 0.000016  \n",
      "Epoch: [1][10900/36908] Elapsed 149m 23s (remain 356m 25s) Loss: 0.0052(0.0029) Grad: 988.1898  LR: 0.000016  \n",
      "Epoch: [1][11000/36908] Elapsed 150m 45s (remain 355m 1s) Loss: 0.0004(0.0029) Grad: 157.0146  LR: 0.000016  \n",
      "Epoch: [1][11100/36908] Elapsed 152m 7s (remain 353m 40s) Loss: 0.0054(0.0029) Grad: 4798.2476  LR: 0.000016  \n",
      "Epoch: [1][11200/36908] Elapsed 153m 31s (remain 352m 20s) Loss: 0.0011(0.0029) Grad: 1045.1552  LR: 0.000015  \n",
      "Epoch: [1][11300/36908] Elapsed 154m 54s (remain 350m 59s) Loss: 0.0011(0.0029) Grad: 2880.1216  LR: 0.000015  \n",
      "Epoch: [1][11400/36908] Elapsed 156m 16s (remain 349m 37s) Loss: 0.0003(0.0029) Grad: 426.8030  LR: 0.000015  \n",
      "Epoch: [1][11500/36908] Elapsed 157m 38s (remain 348m 15s) Loss: 0.0002(0.0029) Grad: 2780.7402  LR: 0.000015  \n",
      "Epoch: [1][11600/36908] Elapsed 159m 2s (remain 346m 56s) Loss: 0.0049(0.0029) Grad: 1778.2842  LR: 0.000015  \n",
      "Epoch: [1][11700/36908] Elapsed 160m 25s (remain 345m 35s) Loss: 0.0006(0.0029) Grad: 333.0749  LR: 0.000015  \n",
      "Epoch: [1][11800/36908] Elapsed 161m 46s (remain 344m 11s) Loss: 0.0008(0.0029) Grad: 3617.0769  LR: 0.000015  \n",
      "Epoch: [1][11900/36908] Elapsed 163m 7s (remain 342m 46s) Loss: 0.0005(0.0029) Grad: 9522.7432  LR: 0.000015  \n",
      "Epoch: [1][12000/36908] Elapsed 164m 29s (remain 341m 23s) Loss: 0.0083(0.0029) Grad: 76055.8516  LR: 0.000015  \n",
      "Epoch: [1][12100/36908] Elapsed 165m 52s (remain 340m 2s) Loss: 0.0012(0.0029) Grad: 14416.9219  LR: 0.000015  \n",
      "Epoch: [1][12200/36908] Elapsed 167m 14s (remain 338m 39s) Loss: 0.0006(0.0029) Grad: 7888.8486  LR: 0.000015  \n",
      "Epoch: [1][12300/36908] Elapsed 168m 35s (remain 337m 15s) Loss: 0.0056(0.0029) Grad: 73084.3047  LR: 0.000015  \n",
      "Epoch: [1][12400/36908] Elapsed 169m 58s (remain 335m 54s) Loss: 0.0068(0.0029) Grad: 46386.8594  LR: 0.000015  \n",
      "Epoch: [1][12500/36908] Elapsed 171m 20s (remain 334m 30s) Loss: 0.0001(0.0029) Grad: 95.7708  LR: 0.000015  \n",
      "Epoch: [1][12600/36908] Elapsed 172m 42s (remain 333m 8s) Loss: 0.0030(0.0029) Grad: 20322.8418  LR: 0.000015  \n",
      "Epoch: [1][12700/36908] Elapsed 174m 4s (remain 331m 46s) Loss: 0.0007(0.0029) Grad: 2035.3412  LR: 0.000015  \n",
      "Epoch: [1][12800/36908] Elapsed 175m 26s (remain 330m 23s) Loss: 0.0034(0.0029) Grad: 122714.0469  LR: 0.000015  \n",
      "Epoch: [1][12900/36908] Elapsed 176m 48s (remain 329m 1s) Loss: 0.0013(0.0029) Grad: 377.7363  LR: 0.000014  \n",
      "Epoch: [1][13000/36908] Elapsed 178m 11s (remain 327m 39s) Loss: 0.0039(0.0029) Grad: 37416.1289  LR: 0.000014  \n",
      "Epoch: [1][13100/36908] Elapsed 179m 34s (remain 326m 19s) Loss: 0.0022(0.0029) Grad: 27914.4609  LR: 0.000014  \n",
      "Epoch: [1][13200/36908] Elapsed 180m 57s (remain 324m 59s) Loss: 0.0003(0.0029) Grad: 710.6883  LR: 0.000014  \n",
      "Epoch: [1][13300/36908] Elapsed 182m 20s (remain 323m 37s) Loss: 0.0080(0.0029) Grad: 29956.9648  LR: 0.000014  \n",
      "Epoch: [1][13400/36908] Elapsed 183m 42s (remain 322m 15s) Loss: 0.0018(0.0029) Grad: 21016.0605  LR: 0.000014  \n",
      "Epoch: [1][13500/36908] Elapsed 185m 4s (remain 320m 52s) Loss: 0.0017(0.0029) Grad: 6512.3672  LR: 0.000014  \n",
      "Epoch: [1][13600/36908] Elapsed 186m 27s (remain 319m 31s) Loss: 0.0071(0.0029) Grad: 33045.9102  LR: 0.000014  \n",
      "Epoch: [1][13700/36908] Elapsed 187m 50s (remain 318m 9s) Loss: 0.0017(0.0029) Grad: 24176.4746  LR: 0.000014  \n",
      "Epoch: [1][13800/36908] Elapsed 189m 12s (remain 316m 47s) Loss: 0.0039(0.0029) Grad: 45061.7305  LR: 0.000014  \n",
      "Epoch: [1][13900/36908] Elapsed 190m 35s (remain 315m 25s) Loss: 0.0022(0.0029) Grad: 35886.5508  LR: 0.000014  \n",
      "Epoch: [1][14000/36908] Elapsed 191m 58s (remain 314m 5s) Loss: 0.0013(0.0029) Grad: 757.8044  LR: 0.000014  \n",
      "Epoch: [1][14100/36908] Elapsed 193m 20s (remain 312m 43s) Loss: 0.0003(0.0029) Grad: 420.8990  LR: 0.000014  \n",
      "Epoch: [1][14200/36908] Elapsed 194m 42s (remain 311m 20s) Loss: 0.0046(0.0029) Grad: 103474.6719  LR: 0.000014  \n",
      "Epoch: [1][14300/36908] Elapsed 196m 4s (remain 309m 58s) Loss: 0.0004(0.0029) Grad: 1882.4545  LR: 0.000014  \n",
      "Epoch: [1][14400/36908] Elapsed 197m 26s (remain 308m 34s) Loss: 0.0001(0.0029) Grad: 62.8150  LR: 0.000014  \n",
      "Epoch: [1][14500/36908] Elapsed 198m 48s (remain 307m 11s) Loss: 0.0153(0.0029) Grad: 164904.9375  LR: 0.000013  \n",
      "Epoch: [1][14600/36908] Elapsed 200m 9s (remain 305m 48s) Loss: 0.0009(0.0029) Grad: 2664.7676  LR: 0.000013  \n",
      "Epoch: [1][14700/36908] Elapsed 201m 33s (remain 304m 27s) Loss: 0.0001(0.0029) Grad: 147.1867  LR: 0.000013  \n",
      "Epoch: [1][14800/36908] Elapsed 202m 54s (remain 303m 4s) Loss: 0.0006(0.0029) Grad: 831.4230  LR: 0.000013  \n",
      "Epoch: [1][14900/36908] Elapsed 204m 16s (remain 301m 41s) Loss: 0.0016(0.0029) Grad: 3950.4033  LR: 0.000013  \n",
      "Epoch: [1][15000/36908] Elapsed 205m 39s (remain 300m 19s) Loss: 0.0063(0.0029) Grad: 9104.6191  LR: 0.000013  \n",
      "Epoch: [1][15100/36908] Elapsed 207m 0s (remain 298m 56s) Loss: 0.0046(0.0029) Grad: 238969.8281  LR: 0.000013  \n",
      "Epoch: [1][15200/36908] Elapsed 208m 23s (remain 297m 34s) Loss: 0.0080(0.0029) Grad: 97544.4531  LR: 0.000013  \n",
      "Epoch: [1][15300/36908] Elapsed 209m 45s (remain 296m 12s) Loss: 0.0012(0.0029) Grad: 2009.3860  LR: 0.000013  \n",
      "Epoch: [1][15400/36908] Elapsed 211m 7s (remain 294m 50s) Loss: 0.0006(0.0029) Grad: 1191.3956  LR: 0.000013  \n",
      "Epoch: [1][15500/36908] Elapsed 212m 30s (remain 293m 28s) Loss: 0.0002(0.0029) Grad: 2125.5300  LR: 0.000013  \n",
      "Epoch: [1][15600/36908] Elapsed 213m 52s (remain 292m 6s) Loss: 0.0022(0.0029) Grad: 865.1763  LR: 0.000013  \n",
      "Epoch: [1][15700/36908] Elapsed 215m 15s (remain 290m 44s) Loss: 0.0038(0.0029) Grad: 10093.7295  LR: 0.000013  \n",
      "Epoch: [1][15800/36908] Elapsed 216m 36s (remain 289m 21s) Loss: 0.0080(0.0029) Grad: 10976.9443  LR: 0.000013  \n",
      "Epoch: [1][15900/36908] Elapsed 217m 59s (remain 287m 59s) Loss: 0.0002(0.0029) Grad: 145.2643  LR: 0.000013  \n",
      "Epoch: [1][16000/36908] Elapsed 219m 21s (remain 286m 37s) Loss: 0.0107(0.0029) Grad: 114396.7500  LR: 0.000013  \n",
      "Epoch: [1][16100/36908] Elapsed 220m 43s (remain 285m 14s) Loss: 0.0027(0.0029) Grad: 52465.1797  LR: 0.000013  \n",
      "Epoch: [1][16200/36908] Elapsed 222m 6s (remain 283m 52s) Loss: 0.0055(0.0029) Grad: 149883.8906  LR: 0.000012  \n",
      "Epoch: [1][16300/36908] Elapsed 223m 29s (remain 282m 31s) Loss: 0.0144(0.0029) Grad: 61592.1914  LR: 0.000012  \n",
      "Epoch: [1][16400/36908] Elapsed 224m 53s (remain 281m 11s) Loss: 0.0058(0.0029) Grad: 215998.8906  LR: 0.000012  \n",
      "Epoch: [1][16500/36908] Elapsed 226m 16s (remain 279m 49s) Loss: 0.0004(0.0029) Grad: 21142.7363  LR: 0.000012  \n",
      "Epoch: [1][16600/36908] Elapsed 227m 39s (remain 278m 28s) Loss: 0.0082(0.0029) Grad: 9250.3555  LR: 0.000012  \n",
      "Epoch: [1][16700/36908] Elapsed 229m 1s (remain 277m 6s) Loss: 0.0034(0.0029) Grad: 75922.2344  LR: 0.000012  \n",
      "Epoch: [1][16800/36908] Elapsed 230m 24s (remain 275m 44s) Loss: 0.0002(0.0029) Grad: 1499.7960  LR: 0.000012  \n",
      "Epoch: [1][16900/36908] Elapsed 231m 46s (remain 274m 21s) Loss: 0.0007(0.0029) Grad: 1143.0924  LR: 0.000012  \n",
      "Epoch: [1][17000/36908] Elapsed 233m 8s (remain 272m 59s) Loss: 0.0142(0.0029) Grad: 291168.5938  LR: 0.000012  \n",
      "Epoch: [1][17100/36908] Elapsed 234m 30s (remain 271m 36s) Loss: 0.0010(0.0029) Grad: 6482.4824  LR: 0.000012  \n",
      "Epoch: [1][17200/36908] Elapsed 235m 53s (remain 270m 15s) Loss: 0.0056(0.0029) Grad: 75030.6719  LR: 0.000012  \n",
      "Epoch: [1][17300/36908] Elapsed 237m 16s (remain 268m 53s) Loss: 0.0028(0.0029) Grad: 24993.9336  LR: 0.000012  \n",
      "Epoch: [1][17400/36908] Elapsed 238m 38s (remain 267m 31s) Loss: 0.0003(0.0029) Grad: 2919.1726  LR: 0.000012  \n",
      "Epoch: [1][17500/36908] Elapsed 240m 1s (remain 266m 9s) Loss: 0.0000(0.0029) Grad: 150.1925  LR: 0.000012  \n",
      "Epoch: [1][17600/36908] Elapsed 241m 23s (remain 264m 47s) Loss: 0.0003(0.0029) Grad: 1431.9968  LR: 0.000012  \n",
      "Epoch: [1][17700/36908] Elapsed 242m 45s (remain 263m 25s) Loss: 0.0001(0.0029) Grad: 305.0363  LR: 0.000012  \n",
      "Epoch: [1][17800/36908] Elapsed 244m 6s (remain 262m 1s) Loss: 0.0053(0.0029) Grad: 24236.6680  LR: 0.000012  \n",
      "Epoch: [1][17900/36908] Elapsed 245m 28s (remain 260m 38s) Loss: 0.0016(0.0029) Grad: 5138.4458  LR: 0.000011  \n",
      "Epoch: [1][18000/36908] Elapsed 246m 51s (remain 259m 17s) Loss: 0.0002(0.0029) Grad: 489.6537  LR: 0.000011  \n",
      "Epoch: [1][18100/36908] Elapsed 248m 15s (remain 257m 56s) Loss: 0.0018(0.0029) Grad: 57250.5039  LR: 0.000011  \n",
      "Epoch: [1][18200/36908] Elapsed 249m 38s (remain 256m 34s) Loss: 0.0061(0.0029) Grad: 41301.0859  LR: 0.000011  \n",
      "Epoch: [1][18300/36908] Elapsed 250m 59s (remain 255m 11s) Loss: 0.0004(0.0029) Grad: 1608.7297  LR: 0.000011  \n",
      "Epoch: [1][18400/36908] Elapsed 252m 21s (remain 253m 49s) Loss: 0.0001(0.0029) Grad: 149.1405  LR: 0.000011  \n",
      "Epoch: [1][18500/36908] Elapsed 253m 44s (remain 252m 26s) Loss: 0.0015(0.0029) Grad: 2612.6340  LR: 0.000011  \n",
      "Epoch: [1][18600/36908] Elapsed 255m 5s (remain 251m 4s) Loss: 0.0081(0.0029) Grad: 44867.9766  LR: 0.000011  \n",
      "Epoch: [1][18700/36908] Elapsed 256m 28s (remain 249m 41s) Loss: 0.0005(0.0029) Grad: 5703.5991  LR: 0.000011  \n",
      "Epoch: [1][18800/36908] Elapsed 257m 50s (remain 248m 19s) Loss: 0.0067(0.0029) Grad: 18853.1562  LR: 0.000011  \n",
      "Epoch: [1][18900/36908] Elapsed 259m 12s (remain 246m 57s) Loss: 0.0033(0.0029) Grad: 297879.5625  LR: 0.000011  \n",
      "Epoch: [1][19000/36908] Elapsed 260m 35s (remain 245m 34s) Loss: 0.0014(0.0029) Grad: 2164.9167  LR: 0.000011  \n",
      "Epoch: [1][19100/36908] Elapsed 261m 58s (remain 244m 13s) Loss: 0.0062(0.0029) Grad: 78838.1016  LR: 0.000011  \n",
      "Epoch: [1][19200/36908] Elapsed 263m 20s (remain 242m 51s) Loss: 0.0000(0.0029) Grad: 443.4982  LR: 0.000011  \n",
      "Epoch: [1][19300/36908] Elapsed 264m 42s (remain 241m 28s) Loss: 0.0092(0.0029) Grad: 125986.7578  LR: 0.000011  \n",
      "Epoch: [1][19400/36908] Elapsed 266m 6s (remain 240m 7s) Loss: 0.0012(0.0029) Grad: 9526.4248  LR: 0.000011  \n",
      "Epoch: [1][19500/36908] Elapsed 267m 27s (remain 238m 44s) Loss: 0.0019(0.0029) Grad: 2292.4497  LR: 0.000010  \n",
      "Epoch: [1][19600/36908] Elapsed 268m 50s (remain 237m 22s) Loss: 0.0034(0.0029) Grad: 68081.2109  LR: 0.000010  \n",
      "Epoch: [1][19700/36908] Elapsed 270m 13s (remain 236m 0s) Loss: 0.0032(0.0029) Grad: 2122.1545  LR: 0.000010  \n",
      "Epoch: [1][19800/36908] Elapsed 271m 34s (remain 234m 37s) Loss: 0.0078(0.0029) Grad: 182466.7656  LR: 0.000010  \n",
      "Epoch: [1][19900/36908] Elapsed 272m 56s (remain 233m 14s) Loss: 0.0118(0.0029) Grad: 344134.1250  LR: 0.000010  \n",
      "Epoch: [1][20000/36908] Elapsed 274m 17s (remain 231m 52s) Loss: 0.0044(0.0029) Grad: 62251.9375  LR: 0.000010  \n",
      "Epoch: [1][20100/36908] Elapsed 275m 40s (remain 230m 29s) Loss: 0.0094(0.0029) Grad: 119415.5625  LR: 0.000010  \n",
      "Epoch: [1][20200/36908] Elapsed 277m 1s (remain 229m 6s) Loss: 0.0052(0.0029) Grad: 551490.3750  LR: 0.000010  \n",
      "Epoch: [1][20300/36908] Elapsed 278m 24s (remain 227m 45s) Loss: 0.0034(0.0029) Grad: 61432.6055  LR: 0.000010  \n",
      "Epoch: [1][20400/36908] Elapsed 279m 46s (remain 226m 22s) Loss: 0.0038(0.0029) Grad: 33151.3398  LR: 0.000010  \n",
      "Epoch: [1][20500/36908] Elapsed 281m 7s (remain 224m 58s) Loss: 0.0023(0.0029) Grad: 96561.8359  LR: 0.000010  \n",
      "Epoch: [1][20600/36908] Elapsed 282m 28s (remain 223m 36s) Loss: 0.0001(0.0029) Grad: 323.8990  LR: 0.000010  \n",
      "Epoch: [1][20700/36908] Elapsed 283m 50s (remain 222m 13s) Loss: 0.0020(0.0029) Grad: 29963.3848  LR: 0.000010  \n",
      "Epoch: [1][20800/36908] Elapsed 285m 12s (remain 220m 51s) Loss: 0.0007(0.0029) Grad: 8251.2178  LR: 0.000010  \n",
      "Epoch: [1][20900/36908] Elapsed 286m 35s (remain 219m 29s) Loss: 0.0008(0.0029) Grad: 36099.2891  LR: 0.000010  \n",
      "Epoch: [1][21000/36908] Elapsed 287m 59s (remain 218m 8s) Loss: 0.0045(0.0029) Grad: 105422.0859  LR: 0.000010  \n",
      "Epoch: [1][21100/36908] Elapsed 289m 22s (remain 216m 46s) Loss: 0.0038(0.0029) Grad: 296381.5000  LR: 0.000010  \n",
      "Epoch: [1][21200/36908] Elapsed 290m 44s (remain 215m 24s) Loss: 0.0014(0.0029) Grad: 6902.7388  LR: 0.000009  \n",
      "Epoch: [1][21300/36908] Elapsed 292m 7s (remain 214m 2s) Loss: 0.0026(0.0029) Grad: 20913.8594  LR: 0.000009  \n",
      "Epoch: [1][21400/36908] Elapsed 293m 30s (remain 212m 40s) Loss: 0.0002(0.0029) Grad: 221.0778  LR: 0.000009  \n",
      "Epoch: [1][21500/36908] Elapsed 294m 52s (remain 211m 17s) Loss: 0.0039(0.0029) Grad: 76302.2031  LR: 0.000009  \n",
      "Epoch: [1][21600/36908] Elapsed 296m 13s (remain 209m 54s) Loss: 0.0001(0.0029) Grad: 970.9820  LR: 0.000009  \n",
      "Epoch: [1][21700/36908] Elapsed 297m 35s (remain 208m 32s) Loss: 0.0038(0.0029) Grad: 17562.6777  LR: 0.000009  \n",
      "Epoch: [1][21800/36908] Elapsed 298m 58s (remain 207m 10s) Loss: 0.0001(0.0029) Grad: 453.0461  LR: 0.000009  \n",
      "Epoch: [1][21900/36908] Elapsed 300m 20s (remain 205m 48s) Loss: 0.0035(0.0029) Grad: 56064.9492  LR: 0.000009  \n",
      "Epoch: [1][22000/36908] Elapsed 301m 43s (remain 204m 26s) Loss: 0.0124(0.0029) Grad: 48997.2109  LR: 0.000009  \n",
      "Epoch: [1][22100/36908] Elapsed 303m 5s (remain 203m 3s) Loss: 0.0017(0.0029) Grad: 17603.4805  LR: 0.000009  \n",
      "Epoch: [1][22200/36908] Elapsed 304m 26s (remain 201m 40s) Loss: 0.0017(0.0029) Grad: 11896.7422  LR: 0.000009  \n",
      "Epoch: [1][22300/36908] Elapsed 305m 49s (remain 200m 18s) Loss: 0.0025(0.0028) Grad: 28172.2500  LR: 0.000009  \n",
      "Epoch: [1][22400/36908] Elapsed 307m 11s (remain 198m 56s) Loss: 0.0003(0.0028) Grad: 2829.9016  LR: 0.000009  \n",
      "Epoch: [1][22500/36908] Elapsed 308m 34s (remain 197m 34s) Loss: 0.0012(0.0029) Grad: 22282.5449  LR: 0.000009  \n",
      "Epoch: [1][22600/36908] Elapsed 309m 56s (remain 196m 11s) Loss: 0.0002(0.0029) Grad: 942.2769  LR: 0.000009  \n",
      "Epoch: [1][22700/36908] Elapsed 311m 17s (remain 194m 49s) Loss: 0.0014(0.0029) Grad: 4761.4766  LR: 0.000009  \n",
      "Epoch: [1][22800/36908] Elapsed 312m 40s (remain 193m 27s) Loss: 0.0002(0.0029) Grad: 1269.2944  LR: 0.000008  \n",
      "Epoch: [1][22900/36908] Elapsed 314m 4s (remain 192m 5s) Loss: 0.0018(0.0029) Grad: 17210.7754  LR: 0.000008  \n",
      "Epoch: [1][23000/36908] Elapsed 315m 27s (remain 190m 43s) Loss: 0.0057(0.0029) Grad: 69800.3516  LR: 0.000008  \n",
      "Epoch: [1][23100/36908] Elapsed 316m 48s (remain 189m 21s) Loss: 0.0021(0.0029) Grad: 34233.2578  LR: 0.000008  \n",
      "Epoch: [1][23200/36908] Elapsed 318m 10s (remain 187m 58s) Loss: 0.0033(0.0029) Grad: 12292.6660  LR: 0.000008  \n",
      "Epoch: [1][23300/36908] Elapsed 319m 32s (remain 186m 36s) Loss: 0.0056(0.0029) Grad: 262584.0938  LR: 0.000008  \n",
      "Epoch: [1][23400/36908] Elapsed 320m 53s (remain 185m 13s) Loss: 0.0024(0.0029) Grad: 28557.3047  LR: 0.000008  \n",
      "Epoch: [1][23500/36908] Elapsed 322m 15s (remain 183m 50s) Loss: 0.0002(0.0029) Grad: 1175.0883  LR: 0.000008  \n",
      "Epoch: [1][23600/36908] Elapsed 323m 38s (remain 182m 28s) Loss: 0.0083(0.0029) Grad: 190486.9219  LR: 0.000008  \n",
      "Epoch: [1][23700/36908] Elapsed 325m 0s (remain 181m 6s) Loss: 0.0067(0.0029) Grad: 313654.8438  LR: 0.000008  \n",
      "Epoch: [1][23800/36908] Elapsed 326m 22s (remain 179m 43s) Loss: 0.0005(0.0029) Grad: 20833.3828  LR: 0.000008  \n",
      "Epoch: [1][23900/36908] Elapsed 327m 45s (remain 178m 22s) Loss: 0.0051(0.0028) Grad: 2240.2871  LR: 0.000008  \n",
      "Epoch: [1][24000/36908] Elapsed 329m 8s (remain 177m 0s) Loss: 0.0002(0.0029) Grad: 352.9453  LR: 0.000008  \n",
      "Epoch: [1][24100/36908] Elapsed 330m 31s (remain 175m 38s) Loss: 0.0047(0.0029) Grad: 156161.8125  LR: 0.000008  \n",
      "Epoch: [1][24200/36908] Elapsed 331m 53s (remain 174m 15s) Loss: 0.0001(0.0028) Grad: 412.5362  LR: 0.000008  \n",
      "Epoch: [1][24300/36908] Elapsed 333m 17s (remain 172m 54s) Loss: 0.0052(0.0028) Grad: 198262.8281  LR: 0.000008  \n",
      "Epoch: [1][24400/36908] Elapsed 334m 39s (remain 171m 32s) Loss: 0.0028(0.0028) Grad: 54584.3750  LR: 0.000008  \n",
      "Epoch: [1][24500/36908] Elapsed 336m 1s (remain 170m 9s) Loss: 0.0026(0.0029) Grad: 7018.6602  LR: 0.000007  \n",
      "Epoch: [1][24600/36908] Elapsed 337m 23s (remain 168m 46s) Loss: 0.0014(0.0028) Grad: 39367.7539  LR: 0.000007  \n",
      "Epoch: [1][24700/36908] Elapsed 338m 46s (remain 167m 25s) Loss: 0.0010(0.0028) Grad: 13985.6719  LR: 0.000007  \n",
      "Epoch: [1][24800/36908] Elapsed 340m 8s (remain 166m 2s) Loss: 0.0003(0.0028) Grad: 27454.4668  LR: 0.000007  \n",
      "Epoch: [1][24900/36908] Elapsed 341m 29s (remain 164m 40s) Loss: 0.0002(0.0028) Grad: 690.2364  LR: 0.000007  \n",
      "Epoch: [1][25000/36908] Elapsed 342m 52s (remain 163m 17s) Loss: 0.0002(0.0028) Grad: 3114.8901  LR: 0.000007  \n",
      "Epoch: [1][25100/36908] Elapsed 344m 15s (remain 161m 55s) Loss: 0.0022(0.0028) Grad: 74214.7109  LR: 0.000007  \n",
      "Epoch: [1][25200/36908] Elapsed 345m 37s (remain 160m 33s) Loss: 0.0023(0.0028) Grad: 36012.6094  LR: 0.000007  \n",
      "Epoch: [1][25300/36908] Elapsed 346m 59s (remain 159m 11s) Loss: 0.0066(0.0028) Grad: 227286.4219  LR: 0.000007  \n",
      "Epoch: [1][25400/36908] Elapsed 348m 22s (remain 157m 49s) Loss: 0.0004(0.0028) Grad: 1364.5908  LR: 0.000007  \n",
      "Epoch: [1][25500/36908] Elapsed 349m 45s (remain 156m 27s) Loss: 0.0002(0.0028) Grad: 278.6108  LR: 0.000007  \n",
      "Epoch: [1][25600/36908] Elapsed 351m 7s (remain 155m 4s) Loss: 0.0033(0.0028) Grad: 62671.4883  LR: 0.000007  \n",
      "Epoch: [1][25700/36908] Elapsed 352m 29s (remain 153m 42s) Loss: 0.0014(0.0028) Grad: 114660.5469  LR: 0.000007  \n",
      "Epoch: [1][25800/36908] Elapsed 353m 52s (remain 152m 20s) Loss: 0.0005(0.0028) Grad: 1733.1888  LR: 0.000007  \n",
      "Epoch: [1][25900/36908] Elapsed 355m 15s (remain 150m 58s) Loss: 0.0027(0.0028) Grad: 14386.8086  LR: 0.000007  \n",
      "Epoch: [1][26000/36908] Elapsed 356m 36s (remain 149m 35s) Loss: 0.0066(0.0028) Grad: 48728.1055  LR: 0.000007  \n",
      "Epoch: [1][26100/36908] Elapsed 357m 58s (remain 148m 12s) Loss: 0.0002(0.0028) Grad: 9668.0898  LR: 0.000007  \n",
      "Epoch: [1][26200/36908] Elapsed 359m 20s (remain 146m 50s) Loss: 0.0029(0.0028) Grad: 15199.3691  LR: 0.000006  \n",
      "Epoch: [1][26300/36908] Elapsed 360m 43s (remain 145m 28s) Loss: 0.0002(0.0028) Grad: 487.5619  LR: 0.000006  \n",
      "Epoch: [1][26400/36908] Elapsed 362m 4s (remain 144m 5s) Loss: 0.0014(0.0028) Grad: 5171.6050  LR: 0.000006  \n",
      "Epoch: [1][26500/36908] Elapsed 363m 26s (remain 142m 43s) Loss: 0.0021(0.0028) Grad: 21704.9766  LR: 0.000006  \n",
      "Epoch: [1][26600/36908] Elapsed 364m 48s (remain 141m 21s) Loss: 0.0011(0.0028) Grad: 3997.2581  LR: 0.000006  \n",
      "Epoch: [1][26700/36908] Elapsed 366m 10s (remain 139m 58s) Loss: 0.0026(0.0028) Grad: 21442.4395  LR: 0.000006  \n",
      "Epoch: [1][26800/36908] Elapsed 367m 31s (remain 138m 36s) Loss: 0.0021(0.0028) Grad: 21637.6406  LR: 0.000006  \n",
      "Epoch: [1][26900/36908] Elapsed 368m 53s (remain 137m 13s) Loss: 0.0043(0.0028) Grad: 138953.1406  LR: 0.000006  \n",
      "Epoch: [1][27000/36908] Elapsed 370m 16s (remain 135m 51s) Loss: 0.0022(0.0028) Grad: 13512.0137  LR: 0.000006  \n",
      "Epoch: [1][27100/36908] Elapsed 371m 38s (remain 134m 29s) Loss: 0.0059(0.0028) Grad: 26183.1270  LR: 0.000006  \n",
      "Epoch: [1][27200/36908] Elapsed 373m 0s (remain 133m 6s) Loss: 0.0040(0.0028) Grad: 86946.5469  LR: 0.000006  \n",
      "Epoch: [1][27300/36908] Elapsed 374m 23s (remain 131m 44s) Loss: 0.0016(0.0028) Grad: 12462.8916  LR: 0.000006  \n",
      "Epoch: [1][27400/36908] Elapsed 375m 46s (remain 130m 22s) Loss: 0.0029(0.0028) Grad: 5559.8252  LR: 0.000006  \n",
      "Epoch: [1][27500/36908] Elapsed 377m 9s (remain 129m 0s) Loss: 0.0003(0.0028) Grad: 6160.7808  LR: 0.000006  \n",
      "Epoch: [1][27600/36908] Elapsed 378m 32s (remain 127m 38s) Loss: 0.0043(0.0028) Grad: 9962.8086  LR: 0.000006  \n",
      "Epoch: [1][27700/36908] Elapsed 379m 53s (remain 126m 16s) Loss: 0.0001(0.0028) Grad: 145.0595  LR: 0.000006  \n",
      "Epoch: [1][27800/36908] Elapsed 381m 16s (remain 124m 53s) Loss: 0.0004(0.0028) Grad: 513.4441  LR: 0.000005  \n",
      "Epoch: [1][27900/36908] Elapsed 382m 39s (remain 123m 31s) Loss: 0.0081(0.0028) Grad: 198881.3906  LR: 0.000005  \n",
      "Epoch: [1][28000/36908] Elapsed 384m 2s (remain 122m 9s) Loss: 0.0017(0.0028) Grad: 9567.4961  LR: 0.000005  \n",
      "Epoch: [1][28100/36908] Elapsed 385m 24s (remain 120m 47s) Loss: 0.0007(0.0028) Grad: 1897.4595  LR: 0.000005  \n",
      "Epoch: [1][28200/36908] Elapsed 386m 45s (remain 119m 24s) Loss: 0.0042(0.0028) Grad: 5706.1680  LR: 0.000005  \n",
      "Epoch: [1][28300/36908] Elapsed 388m 7s (remain 118m 2s) Loss: 0.0019(0.0028) Grad: 3691.3413  LR: 0.000005  \n",
      "Epoch: [1][28400/36908] Elapsed 389m 29s (remain 116m 40s) Loss: 0.0011(0.0028) Grad: 3321.3726  LR: 0.000005  \n",
      "Epoch: [1][28500/36908] Elapsed 390m 52s (remain 115m 17s) Loss: 0.0020(0.0028) Grad: 7896.1909  LR: 0.000005  \n",
      "Epoch: [1][28600/36908] Elapsed 392m 13s (remain 113m 55s) Loss: 0.0062(0.0028) Grad: 37380.1641  LR: 0.000005  \n",
      "Epoch: [1][28700/36908] Elapsed 393m 35s (remain 112m 32s) Loss: 0.0023(0.0028) Grad: 6027.6631  LR: 0.000005  \n",
      "Epoch: [1][28800/36908] Elapsed 394m 58s (remain 111m 10s) Loss: 0.0006(0.0028) Grad: 729.3273  LR: 0.000005  \n",
      "Epoch: [1][28900/36908] Elapsed 396m 21s (remain 109m 48s) Loss: 0.0002(0.0028) Grad: 86.2831  LR: 0.000005  \n",
      "Epoch: [1][29000/36908] Elapsed 397m 43s (remain 108m 26s) Loss: 0.0009(0.0028) Grad: 1192.1740  LR: 0.000005  \n",
      "Epoch: [1][29100/36908] Elapsed 399m 6s (remain 107m 4s) Loss: 0.0094(0.0028) Grad: 11588.5811  LR: 0.000005  \n",
      "Epoch: [1][29200/36908] Elapsed 400m 28s (remain 105m 41s) Loss: 0.0013(0.0028) Grad: 20314.1035  LR: 0.000005  \n",
      "Epoch: [1][29300/36908] Elapsed 401m 50s (remain 104m 19s) Loss: 0.0017(0.0028) Grad: 2098.8159  LR: 0.000005  \n",
      "Epoch: [1][29400/36908] Elapsed 403m 12s (remain 102m 57s) Loss: 0.0049(0.0028) Grad: 1230.9675  LR: 0.000005  \n",
      "Epoch: [1][29500/36908] Elapsed 404m 35s (remain 101m 34s) Loss: 0.0015(0.0028) Grad: 20013.8750  LR: 0.000004  \n",
      "Epoch: [1][29600/36908] Elapsed 405m 58s (remain 100m 12s) Loss: 0.0014(0.0028) Grad: 3495.4036  LR: 0.000004  \n",
      "Epoch: [1][29700/36908] Elapsed 407m 21s (remain 98m 50s) Loss: 0.0001(0.0028) Grad: 84.8359  LR: 0.000004  \n",
      "Epoch: [1][29800/36908] Elapsed 408m 43s (remain 97m 28s) Loss: 0.0018(0.0028) Grad: 10541.9180  LR: 0.000004  \n",
      "Epoch: [1][29900/36908] Elapsed 410m 5s (remain 96m 6s) Loss: 0.0003(0.0028) Grad: 963.0945  LR: 0.000004  \n",
      "Epoch: [1][30000/36908] Elapsed 411m 28s (remain 94m 43s) Loss: 0.0024(0.0028) Grad: 57752.0977  LR: 0.000004  \n",
      "Epoch: [1][30100/36908] Elapsed 412m 49s (remain 93m 21s) Loss: 0.0006(0.0028) Grad: 831.2231  LR: 0.000004  \n",
      "Epoch: [1][30200/36908] Elapsed 414m 12s (remain 91m 59s) Loss: 0.0026(0.0028) Grad: 1514.4463  LR: 0.000004  \n",
      "Epoch: [1][30300/36908] Elapsed 415m 35s (remain 90m 37s) Loss: 0.0003(0.0028) Grad: 54.4431  LR: 0.000004  \n",
      "Epoch: [1][30400/36908] Elapsed 416m 58s (remain 89m 14s) Loss: 0.0054(0.0028) Grad: 50603.4297  LR: 0.000004  \n",
      "Epoch: [1][30500/36908] Elapsed 418m 19s (remain 87m 52s) Loss: 0.0002(0.0028) Grad: 695.0504  LR: 0.000004  \n",
      "Epoch: [1][30600/36908] Elapsed 419m 41s (remain 86m 29s) Loss: 0.0018(0.0028) Grad: 2229.8623  LR: 0.000004  \n",
      "Epoch: [1][30700/36908] Elapsed 421m 3s (remain 85m 7s) Loss: 0.0001(0.0028) Grad: 134.6167  LR: 0.000004  \n",
      "Epoch: [1][30800/36908] Elapsed 422m 24s (remain 83m 45s) Loss: 0.0004(0.0028) Grad: 3011.4399  LR: 0.000004  \n",
      "Epoch: [1][30900/36908] Elapsed 423m 46s (remain 82m 22s) Loss: 0.0008(0.0028) Grad: 492.8088  LR: 0.000004  \n",
      "Epoch: [1][31000/36908] Elapsed 425m 8s (remain 81m 0s) Loss: 0.0047(0.0028) Grad: 9164.4121  LR: 0.000004  \n",
      "Epoch: [1][31100/36908] Elapsed 426m 31s (remain 79m 38s) Loss: 0.0049(0.0028) Grad: 61791.4453  LR: 0.000003  \n",
      "Epoch: [1][31200/36908] Elapsed 427m 53s (remain 78m 15s) Loss: 0.0003(0.0028) Grad: 279.1754  LR: 0.000003  \n",
      "Epoch: [1][31300/36908] Elapsed 429m 14s (remain 76m 53s) Loss: 0.0022(0.0028) Grad: 1375.8220  LR: 0.000003  \n",
      "Epoch: [1][31400/36908] Elapsed 430m 36s (remain 75m 31s) Loss: 0.0018(0.0028) Grad: 4448.1885  LR: 0.000003  \n",
      "Epoch: [1][31500/36908] Elapsed 431m 57s (remain 74m 8s) Loss: 0.0018(0.0028) Grad: 2687.1118  LR: 0.000003  \n",
      "Epoch: [1][31600/36908] Elapsed 433m 19s (remain 72m 46s) Loss: 0.0006(0.0028) Grad: 2335.4233  LR: 0.000003  \n",
      "Epoch: [1][31700/36908] Elapsed 434m 41s (remain 71m 24s) Loss: 0.0022(0.0028) Grad: 74382.2344  LR: 0.000003  \n",
      "Epoch: [1][31800/36908] Elapsed 436m 4s (remain 70m 1s) Loss: 0.0024(0.0028) Grad: 25390.8828  LR: 0.000003  \n",
      "Epoch: [1][31900/36908] Elapsed 437m 26s (remain 68m 39s) Loss: 0.0003(0.0028) Grad: 419.0694  LR: 0.000003  \n",
      "Epoch: [1][32000/36908] Elapsed 438m 47s (remain 67m 17s) Loss: 0.0033(0.0028) Grad: 26047.5449  LR: 0.000003  \n",
      "Epoch: [1][32100/36908] Elapsed 440m 8s (remain 65m 54s) Loss: 0.0008(0.0028) Grad: 37931.0039  LR: 0.000003  \n",
      "Epoch: [1][32200/36908] Elapsed 441m 31s (remain 64m 32s) Loss: 0.0004(0.0028) Grad: 204.2848  LR: 0.000003  \n",
      "Epoch: [1][32300/36908] Elapsed 442m 55s (remain 63m 10s) Loss: 0.0035(0.0028) Grad: 19115.8555  LR: 0.000003  \n",
      "Epoch: [1][32400/36908] Elapsed 444m 17s (remain 61m 48s) Loss: 0.0084(0.0028) Grad: 221837.7969  LR: 0.000003  \n",
      "Epoch: [1][32500/36908] Elapsed 445m 40s (remain 60m 25s) Loss: 0.0017(0.0028) Grad: 3415.7480  LR: 0.000003  \n",
      "Epoch: [1][32600/36908] Elapsed 447m 3s (remain 59m 3s) Loss: 0.0027(0.0028) Grad: 30664.8379  LR: 0.000003  \n",
      "Epoch: [1][32700/36908] Elapsed 448m 26s (remain 57m 41s) Loss: 0.0082(0.0028) Grad: 92659.1719  LR: 0.000003  \n",
      "Epoch: [1][32800/36908] Elapsed 449m 48s (remain 56m 19s) Loss: 0.0017(0.0028) Grad: 6667.9785  LR: 0.000002  \n",
      "Epoch: [1][32900/36908] Elapsed 451m 10s (remain 54m 56s) Loss: 0.0002(0.0028) Grad: 201.5986  LR: 0.000002  \n",
      "Epoch: [1][33000/36908] Elapsed 452m 33s (remain 53m 34s) Loss: 0.0009(0.0028) Grad: 8042.1816  LR: 0.000002  \n",
      "Epoch: [1][33100/36908] Elapsed 453m 56s (remain 52m 12s) Loss: 0.0004(0.0028) Grad: 1728.1155  LR: 0.000002  \n",
      "Epoch: [1][33200/36908] Elapsed 455m 19s (remain 50m 50s) Loss: 0.0003(0.0028) Grad: 126.0207  LR: 0.000002  \n",
      "Epoch: [1][33300/36908] Elapsed 456m 42s (remain 49m 28s) Loss: 0.0023(0.0028) Grad: 6250.9316  LR: 0.000002  \n",
      "Epoch: [1][33400/36908] Elapsed 458m 4s (remain 48m 5s) Loss: 0.0030(0.0028) Grad: 20361.0137  LR: 0.000002  \n",
      "Epoch: [1][33500/36908] Elapsed 459m 26s (remain 46m 43s) Loss: 0.0025(0.0028) Grad: 16423.5098  LR: 0.000002  \n",
      "Epoch: [1][33600/36908] Elapsed 460m 49s (remain 45m 21s) Loss: 0.0039(0.0028) Grad: 20326.7051  LR: 0.000002  \n",
      "Epoch: [1][33700/36908] Elapsed 462m 13s (remain 43m 59s) Loss: 0.0054(0.0028) Grad: 27761.7988  LR: 0.000002  \n",
      "Epoch: [1][33800/36908] Elapsed 463m 35s (remain 42m 36s) Loss: 0.0010(0.0028) Grad: 16607.4102  LR: 0.000002  \n",
      "Epoch: [1][33900/36908] Elapsed 464m 58s (remain 41m 14s) Loss: 0.0007(0.0028) Grad: 77290.9531  LR: 0.000002  \n",
      "Epoch: [1][34000/36908] Elapsed 466m 20s (remain 39m 52s) Loss: 0.0003(0.0028) Grad: 783.5861  LR: 0.000002  \n",
      "Epoch: [1][34100/36908] Elapsed 467m 42s (remain 38m 29s) Loss: 0.0034(0.0028) Grad: 27227.9355  LR: 0.000002  \n",
      "Epoch: [1][34200/36908] Elapsed 469m 4s (remain 37m 7s) Loss: 0.0007(0.0028) Grad: 900.8408  LR: 0.000002  \n",
      "Epoch: [1][34300/36908] Elapsed 470m 26s (remain 35m 45s) Loss: 0.0016(0.0028) Grad: 6059.4746  LR: 0.000002  \n",
      "Epoch: [1][34400/36908] Elapsed 471m 49s (remain 34m 23s) Loss: 0.0013(0.0028) Grad: 2605.1135  LR: 0.000002  \n",
      "Epoch: [1][34500/36908] Elapsed 473m 11s (remain 33m 0s) Loss: 0.0000(0.0028) Grad: 440.1516  LR: 0.000001  \n",
      "Epoch: [1][34600/36908] Elapsed 474m 33s (remain 31m 38s) Loss: 0.0005(0.0028) Grad: 1234.0032  LR: 0.000001  \n",
      "Epoch: [1][34700/36908] Elapsed 475m 55s (remain 30m 16s) Loss: 0.0059(0.0028) Grad: 46583.2070  LR: 0.000001  \n",
      "Epoch: [1][34800/36908] Elapsed 477m 19s (remain 28m 53s) Loss: 0.0032(0.0028) Grad: 76205.3594  LR: 0.000001  \n",
      "Epoch: [1][34900/36908] Elapsed 478m 42s (remain 27m 31s) Loss: 0.0003(0.0028) Grad: 4680.9478  LR: 0.000001  \n",
      "Epoch: [1][35000/36908] Elapsed 480m 5s (remain 26m 9s) Loss: 0.0017(0.0028) Grad: 5022.3848  LR: 0.000001  \n",
      "Epoch: [1][35100/36908] Elapsed 481m 27s (remain 24m 47s) Loss: 0.0097(0.0028) Grad: 16364.7236  LR: 0.000001  \n",
      "Epoch: [1][35200/36908] Elapsed 482m 50s (remain 23m 24s) Loss: 0.0021(0.0028) Grad: 25612.0781  LR: 0.000001  \n",
      "Epoch: [1][35300/36908] Elapsed 484m 13s (remain 22m 2s) Loss: 0.0001(0.0028) Grad: 87.9605  LR: 0.000001  \n",
      "Epoch: [1][35400/36908] Elapsed 485m 36s (remain 20m 40s) Loss: 0.0014(0.0028) Grad: 2111.0032  LR: 0.000001  \n",
      "Epoch: [1][35500/36908] Elapsed 486m 57s (remain 19m 17s) Loss: 0.0020(0.0028) Grad: 30978.8652  LR: 0.000001  \n",
      "Epoch: [1][35600/36908] Elapsed 488m 19s (remain 17m 55s) Loss: 0.0026(0.0028) Grad: 90971.8047  LR: 0.000001  \n",
      "Epoch: [1][35700/36908] Elapsed 489m 42s (remain 16m 33s) Loss: 0.0010(0.0028) Grad: 3035.0986  LR: 0.000001  \n",
      "Epoch: [1][35800/36908] Elapsed 491m 4s (remain 15m 11s) Loss: 0.0018(0.0028) Grad: 4069.1738  LR: 0.000001  \n",
      "Epoch: [1][35900/36908] Elapsed 492m 26s (remain 13m 48s) Loss: 0.0005(0.0028) Grad: 3342.3538  LR: 0.000001  \n",
      "Epoch: [1][36000/36908] Elapsed 493m 49s (remain 12m 26s) Loss: 0.0155(0.0028) Grad: 35866.5703  LR: 0.000001  \n",
      "Epoch: [1][36100/36908] Elapsed 495m 11s (remain 11m 4s) Loss: 0.0002(0.0028) Grad: 1944.1235  LR: 0.000000  \n",
      "Epoch: [1][36200/36908] Elapsed 496m 35s (remain 9m 41s) Loss: 0.0026(0.0028) Grad: 9425.0137  LR: 0.000000  \n",
      "Epoch: [1][36300/36908] Elapsed 497m 58s (remain 8m 19s) Loss: 0.0006(0.0028) Grad: 2074.0044  LR: 0.000000  \n",
      "Epoch: [1][36400/36908] Elapsed 499m 21s (remain 6m 57s) Loss: 0.0044(0.0028) Grad: 73008.1328  LR: 0.000000  \n",
      "Epoch: [1][36500/36908] Elapsed 500m 45s (remain 5m 35s) Loss: 0.0003(0.0028) Grad: 7060.5352  LR: 0.000000  \n",
      "Epoch: [1][36600/36908] Elapsed 502m 8s (remain 4m 12s) Loss: 0.0008(0.0028) Grad: 8928.3809  LR: 0.000000  \n",
      "Epoch: [1][36700/36908] Elapsed 503m 31s (remain 2m 50s) Loss: 0.0002(0.0028) Grad: 219.6928  LR: 0.000000  \n",
      "Epoch: [1][36800/36908] Elapsed 504m 55s (remain 1m 28s) Loss: 0.0008(0.0028) Grad: 17016.3848  LR: 0.000000  \n",
      "Epoch: [1][36900/36908] Elapsed 506m 16s (remain 0m 5s) Loss: 0.0024(0.0028) Grad: 30017.1270  LR: 0.000000  \n",
      "Epoch: [1][36907/36908] Elapsed 506m 22s (remain 0m 0s) Loss: 0.0003(0.0028) Grad: 36989.8633  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 17m 58s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 30s (remain 5m 24s) Loss: 0.0002(0.0050) \n",
      "EVAL: [200/1192] Elapsed 0m 58s (remain 4m 50s) Loss: 0.0000(0.0062) \n",
      "EVAL: [300/1192] Elapsed 1m 28s (remain 4m 21s) Loss: 0.0013(0.0100) \n",
      "EVAL: [400/1192] Elapsed 1m 57s (remain 3m 51s) Loss: 0.0308(0.0099) \n",
      "EVAL: [500/1192] Elapsed 2m 26s (remain 3m 21s) Loss: 0.0338(0.0089) \n",
      "EVAL: [600/1192] Elapsed 2m 55s (remain 2m 52s) Loss: 0.1326(0.0088) \n",
      "EVAL: [700/1192] Elapsed 3m 24s (remain 2m 23s) Loss: 0.0051(0.0101) \n",
      "EVAL: [800/1192] Elapsed 3m 54s (remain 1m 54s) Loss: 0.0118(0.0099) \n",
      "EVAL: [900/1192] Elapsed 4m 23s (remain 1m 25s) Loss: 0.0010(0.0095) \n",
      "EVAL: [1000/1192] Elapsed 4m 53s (remain 0m 55s) Loss: 0.0000(0.0093) \n",
      "EVAL: [1100/1192] Elapsed 5m 22s (remain 0m 26s) Loss: 0.0072(0.0088) \n",
      "EVAL: [1191/1192] Elapsed 5m 49s (remain 0m 0s) Loss: 0.0107(0.0083) \n",
      "Epoch 1 - avg_train_loss: 0.0028  avg_val_loss: 0.0083  time: 30736s\n",
      "Epoch 1 - Score: 0.8868\n",
      "Epoch 1 - Save Best Score: 0.8868 Model\n",
      "========== fold: 2 training ==========\n",
      "get pseudo plain from ../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_2.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_2.npy\n",
      "(612602, 6) (612602, 950)\n",
      "(100000, 7)\n",
      "(110725, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n",
      "Epoch: [1][0/36908] Elapsed 0m 1s (remain 783m 10s) Loss: 0.0002(0.0002) Grad: 19.6740  LR: 0.000000  \n",
      "Epoch: [1][100/36908] Elapsed 1m 24s (remain 515m 0s) Loss: 0.0001(0.0028) Grad: 28.1174  LR: 0.000001  \n",
      "Epoch: [1][200/36908] Elapsed 2m 47s (remain 509m 6s) Loss: 0.0069(0.0024) Grad: 7743.5640  LR: 0.000001  \n",
      "Epoch: [1][300/36908] Elapsed 4m 9s (remain 505m 36s) Loss: 0.0136(0.0025) Grad: 8701.6162  LR: 0.000002  \n",
      "Epoch: [1][400/36908] Elapsed 5m 31s (remain 503m 15s) Loss: 0.0001(0.0025) Grad: 50.5880  LR: 0.000002  \n",
      "Epoch: [1][500/36908] Elapsed 6m 54s (remain 502m 2s) Loss: 0.0023(0.0024) Grad: 2482.4392  LR: 0.000003  \n",
      "Epoch: [1][600/36908] Elapsed 8m 18s (remain 501m 32s) Loss: 0.0105(0.0023) Grad: 8342.9160  LR: 0.000003  \n",
      "Epoch: [1][700/36908] Elapsed 9m 41s (remain 500m 9s) Loss: 0.0017(0.0023) Grad: 3982.5906  LR: 0.000004  \n",
      "Epoch: [1][800/36908] Elapsed 11m 3s (remain 498m 24s) Loss: 0.0019(0.0023) Grad: 1128.5894  LR: 0.000004  \n",
      "Epoch: [1][900/36908] Elapsed 12m 27s (remain 497m 45s) Loss: 0.0155(0.0024) Grad: 31859.8047  LR: 0.000005  \n",
      "Epoch: [1][1000/36908] Elapsed 13m 49s (remain 496m 2s) Loss: 0.0032(0.0023) Grad: 3440.7766  LR: 0.000005  \n",
      "Epoch: [1][1100/36908] Elapsed 15m 13s (remain 494m 55s) Loss: 0.0004(0.0024) Grad: 842.6754  LR: 0.000006  \n",
      "Epoch: [1][1200/36908] Elapsed 16m 35s (remain 493m 18s) Loss: 0.0012(0.0023) Grad: 3226.4685  LR: 0.000007  \n",
      "Epoch: [1][1300/36908] Elapsed 17m 57s (remain 491m 23s) Loss: 0.0056(0.0023) Grad: 8417.3809  LR: 0.000007  \n",
      "Epoch: [1][1400/36908] Elapsed 19m 20s (remain 490m 14s) Loss: 0.0000(0.0023) Grad: 5.7621  LR: 0.000008  \n",
      "Epoch: [1][1500/36908] Elapsed 20m 43s (remain 488m 58s) Loss: 0.0038(0.0023) Grad: 3161.4951  LR: 0.000008  \n",
      "Epoch: [1][1600/36908] Elapsed 22m 5s (remain 487m 15s) Loss: 0.0086(0.0023) Grad: 6182.4385  LR: 0.000009  \n",
      "Epoch: [1][1700/36908] Elapsed 23m 27s (remain 485m 28s) Loss: 0.0014(0.0023) Grad: 2749.1177  LR: 0.000009  \n",
      "Epoch: [1][1800/36908] Elapsed 24m 49s (remain 484m 0s) Loss: 0.0091(0.0024) Grad: 5154.0469  LR: 0.000010  \n",
      "Epoch: [1][1900/36908] Elapsed 26m 13s (remain 483m 1s) Loss: 0.0001(0.0023) Grad: 52.2044  LR: 0.000010  \n",
      "Epoch: [1][2000/36908] Elapsed 27m 36s (remain 481m 35s) Loss: 0.0001(0.0023) Grad: 15.0474  LR: 0.000011  \n",
      "Epoch: [1][2100/36908] Elapsed 28m 58s (remain 479m 56s) Loss: 0.0001(0.0023) Grad: 53.4568  LR: 0.000011  \n",
      "Epoch: [1][2200/36908] Elapsed 30m 20s (remain 478m 26s) Loss: 0.0056(0.0023) Grad: 12386.8730  LR: 0.000012  \n",
      "Epoch: [1][2300/36908] Elapsed 31m 41s (remain 476m 38s) Loss: 0.0035(0.0023) Grad: 7553.5981  LR: 0.000012  \n",
      "Epoch: [1][2400/36908] Elapsed 33m 4s (remain 475m 16s) Loss: 0.0001(0.0023) Grad: 2.6318  LR: 0.000013  \n",
      "Epoch: [1][2500/36908] Elapsed 34m 27s (remain 473m 59s) Loss: 0.0003(0.0023) Grad: 111.8809  LR: 0.000014  \n",
      "Epoch: [1][2600/36908] Elapsed 35m 49s (remain 472m 28s) Loss: 0.0005(0.0023) Grad: 950.6010  LR: 0.000014  \n",
      "Epoch: [1][2700/36908] Elapsed 37m 11s (remain 470m 55s) Loss: 0.0002(0.0023) Grad: 159.3261  LR: 0.000015  \n",
      "Epoch: [1][2800/36908] Elapsed 38m 34s (remain 469m 39s) Loss: 0.0009(0.0023) Grad: 6357.7104  LR: 0.000015  \n",
      "Epoch: [1][2900/36908] Elapsed 39m 56s (remain 468m 7s) Loss: 0.0037(0.0023) Grad: 2273.1099  LR: 0.000016  \n",
      "Epoch: [1][3000/36908] Elapsed 41m 18s (remain 466m 43s) Loss: 0.0057(0.0023) Grad: 9738.7646  LR: 0.000016  \n",
      "Epoch: [1][3100/36908] Elapsed 42m 41s (remain 465m 22s) Loss: 0.0002(0.0023) Grad: 17.8717  LR: 0.000017  \n",
      "Epoch: [1][3200/36908] Elapsed 44m 4s (remain 464m 7s) Loss: 0.0001(0.0023) Grad: 37.4480  LR: 0.000017  \n",
      "Epoch: [1][3300/36908] Elapsed 45m 27s (remain 462m 45s) Loss: 0.0009(0.0023) Grad: 517.3558  LR: 0.000018  \n",
      "Epoch: [1][3400/36908] Elapsed 46m 50s (remain 461m 28s) Loss: 0.0001(0.0023) Grad: 195.1239  LR: 0.000018  \n",
      "Epoch: [1][3500/36908] Elapsed 48m 13s (remain 460m 11s) Loss: 0.0008(0.0023) Grad: 276.3388  LR: 0.000019  \n",
      "Epoch: [1][3600/36908] Elapsed 49m 36s (remain 458m 48s) Loss: 0.0071(0.0023) Grad: 13516.0146  LR: 0.000020  \n",
      "Epoch: [1][3700/36908] Elapsed 50m 58s (remain 457m 26s) Loss: 0.0001(0.0023) Grad: 55.0784  LR: 0.000020  \n",
      "Epoch: [1][3800/36908] Elapsed 52m 22s (remain 456m 9s) Loss: 0.0032(0.0024) Grad: 11752.3955  LR: 0.000020  \n",
      "Epoch: [1][3900/36908] Elapsed 53m 45s (remain 454m 50s) Loss: 0.0202(0.0024) Grad: 39109.4727  LR: 0.000020  \n",
      "Epoch: [1][4000/36908] Elapsed 55m 7s (remain 453m 22s) Loss: 0.0030(0.0024) Grad: 15570.9375  LR: 0.000020  \n",
      "Epoch: [1][4100/36908] Elapsed 56m 31s (remain 452m 10s) Loss: 0.0007(0.0024) Grad: 746.1572  LR: 0.000020  \n",
      "Epoch: [1][4200/36908] Elapsed 57m 54s (remain 450m 51s) Loss: 0.0005(0.0024) Grad: 2737.0039  LR: 0.000020  \n",
      "Epoch: [1][4300/36908] Elapsed 59m 18s (remain 449m 37s) Loss: 0.0002(0.0024) Grad: 66.1554  LR: 0.000020  \n",
      "Epoch: [1][4400/36908] Elapsed 60m 42s (remain 448m 25s) Loss: 0.0001(0.0024) Grad: 158.6642  LR: 0.000020  \n",
      "Epoch: [1][4500/36908] Elapsed 62m 5s (remain 447m 6s) Loss: 0.0086(0.0024) Grad: 36924.5938  LR: 0.000020  \n",
      "Epoch: [1][4600/36908] Elapsed 63m 28s (remain 445m 40s) Loss: 0.0000(0.0024) Grad: 90.3864  LR: 0.000019  \n",
      "Epoch: [1][4700/36908] Elapsed 64m 50s (remain 444m 13s) Loss: 0.0049(0.0024) Grad: 13882.5820  LR: 0.000019  \n",
      "Epoch: [1][4800/36908] Elapsed 66m 13s (remain 442m 55s) Loss: 0.0006(0.0024) Grad: 1765.9236  LR: 0.000019  \n",
      "Epoch: [1][4900/36908] Elapsed 67m 37s (remain 441m 38s) Loss: 0.0003(0.0024) Grad: 1233.7288  LR: 0.000019  \n",
      "Epoch: [1][5000/36908] Elapsed 68m 59s (remain 440m 13s) Loss: 0.0010(0.0024) Grad: 1785.3130  LR: 0.000019  \n",
      "Epoch: [1][5100/36908] Elapsed 70m 22s (remain 438m 49s) Loss: 0.0011(0.0024) Grad: 1991.1967  LR: 0.000019  \n",
      "Epoch: [1][5200/36908] Elapsed 71m 44s (remain 437m 22s) Loss: 0.0002(0.0024) Grad: 92.8757  LR: 0.000019  \n",
      "Epoch: [1][5300/36908] Elapsed 73m 8s (remain 436m 6s) Loss: 0.0000(0.0024) Grad: 11.6740  LR: 0.000019  \n",
      "Epoch: [1][5400/36908] Elapsed 74m 32s (remain 434m 49s) Loss: 0.0010(0.0025) Grad: 170.8797  LR: 0.000019  \n",
      "Epoch: [1][5500/36908] Elapsed 75m 55s (remain 433m 27s) Loss: 0.0063(0.0025) Grad: 9155.5527  LR: 0.000019  \n",
      "Epoch: [1][5600/36908] Elapsed 77m 17s (remain 431m 59s) Loss: 0.0002(0.0025) Grad: 358.6415  LR: 0.000019  \n",
      "Epoch: [1][5700/36908] Elapsed 78m 40s (remain 430m 40s) Loss: 0.0002(0.0025) Grad: 3106.9941  LR: 0.000019  \n",
      "Epoch: [1][5800/36908] Elapsed 80m 4s (remain 429m 21s) Loss: 0.0037(0.0025) Grad: 6845.0029  LR: 0.000019  \n",
      "Epoch: [1][5900/36908] Elapsed 81m 27s (remain 428m 1s) Loss: 0.0054(0.0025) Grad: 3523.8037  LR: 0.000019  \n",
      "Epoch: [1][6000/36908] Elapsed 82m 49s (remain 426m 35s) Loss: 0.0002(0.0025) Grad: 62.3061  LR: 0.000019  \n",
      "Epoch: [1][6100/36908] Elapsed 84m 11s (remain 425m 5s) Loss: 0.0001(0.0025) Grad: 137.8102  LR: 0.000019  \n",
      "Epoch: [1][6200/36908] Elapsed 85m 34s (remain 423m 46s) Loss: 0.0001(0.0025) Grad: 115.4707  LR: 0.000018  \n",
      "Epoch: [1][6300/36908] Elapsed 86m 57s (remain 422m 22s) Loss: 0.0053(0.0025) Grad: 45173.7695  LR: 0.000018  \n",
      "Epoch: [1][6400/36908] Elapsed 88m 20s (remain 421m 0s) Loss: 0.0046(0.0025) Grad: 49110.5938  LR: 0.000018  \n",
      "Epoch: [1][6500/36908] Elapsed 89m 42s (remain 419m 35s) Loss: 0.0002(0.0025) Grad: 91.8528  LR: 0.000018  \n",
      "Epoch: [1][6600/36908] Elapsed 91m 4s (remain 418m 10s) Loss: 0.0046(0.0025) Grad: 23354.5977  LR: 0.000018  \n",
      "Epoch: [1][6700/36908] Elapsed 92m 28s (remain 416m 53s) Loss: 0.0002(0.0025) Grad: 180.5311  LR: 0.000018  \n",
      "Epoch: [1][6800/36908] Elapsed 93m 52s (remain 415m 33s) Loss: 0.0053(0.0025) Grad: 10533.6523  LR: 0.000018  \n",
      "Epoch: [1][6900/36908] Elapsed 95m 14s (remain 414m 9s) Loss: 0.0002(0.0025) Grad: 2628.8999  LR: 0.000018  \n",
      "Epoch: [1][7000/36908] Elapsed 96m 38s (remain 412m 50s) Loss: 0.0041(0.0025) Grad: 15589.8984  LR: 0.000018  \n",
      "Epoch: [1][7100/36908] Elapsed 98m 1s (remain 411m 28s) Loss: 0.0026(0.0025) Grad: 6397.2520  LR: 0.000018  \n",
      "Epoch: [1][7200/36908] Elapsed 99m 24s (remain 410m 4s) Loss: 0.0025(0.0025) Grad: 1159.8101  LR: 0.000018  \n",
      "Epoch: [1][7300/36908] Elapsed 100m 48s (remain 408m 47s) Loss: 0.0101(0.0025) Grad: 26306.4590  LR: 0.000018  \n",
      "Epoch: [1][7400/36908] Elapsed 102m 12s (remain 407m 28s) Loss: 0.0005(0.0025) Grad: 5884.3091  LR: 0.000018  \n",
      "Epoch: [1][7500/36908] Elapsed 103m 34s (remain 406m 4s) Loss: 0.0036(0.0025) Grad: 7972.5181  LR: 0.000018  \n",
      "Epoch: [1][7600/36908] Elapsed 104m 57s (remain 404m 42s) Loss: 0.0000(0.0025) Grad: 105.7975  LR: 0.000018  \n",
      "Epoch: [1][7700/36908] Elapsed 106m 20s (remain 403m 20s) Loss: 0.0000(0.0025) Grad: 13.5719  LR: 0.000018  \n",
      "Epoch: [1][7800/36908] Elapsed 107m 44s (remain 402m 1s) Loss: 0.0004(0.0025) Grad: 3384.7058  LR: 0.000018  \n",
      "Epoch: [1][7900/36908] Elapsed 109m 7s (remain 400m 37s) Loss: 0.0015(0.0025) Grad: 4191.0537  LR: 0.000017  \n",
      "Epoch: [1][8000/36908] Elapsed 110m 30s (remain 399m 14s) Loss: 0.0003(0.0025) Grad: 293.1423  LR: 0.000017  \n",
      "Epoch: [1][8100/36908] Elapsed 111m 52s (remain 397m 49s) Loss: 0.0001(0.0025) Grad: 68.7592  LR: 0.000017  \n",
      "Epoch: [1][8200/36908] Elapsed 113m 15s (remain 396m 26s) Loss: 0.0001(0.0025) Grad: 606.5825  LR: 0.000017  \n",
      "Epoch: [1][8300/36908] Elapsed 114m 38s (remain 395m 6s) Loss: 0.0001(0.0025) Grad: 225.7075  LR: 0.000017  \n",
      "Epoch: [1][8400/36908] Elapsed 116m 2s (remain 393m 47s) Loss: 0.0067(0.0025) Grad: 136503.9688  LR: 0.000017  \n",
      "Epoch: [1][8500/36908] Elapsed 117m 25s (remain 392m 24s) Loss: 0.0028(0.0025) Grad: 44362.1289  LR: 0.000017  \n",
      "Epoch: [1][8600/36908] Elapsed 118m 50s (remain 391m 6s) Loss: 0.0003(0.0025) Grad: 154.5871  LR: 0.000017  \n",
      "Epoch: [1][8700/36908] Elapsed 120m 14s (remain 389m 48s) Loss: 0.0008(0.0025) Grad: 5574.2588  LR: 0.000017  \n",
      "Epoch: [1][8800/36908] Elapsed 121m 37s (remain 388m 23s) Loss: 0.0108(0.0025) Grad: 102485.3828  LR: 0.000017  \n",
      "Epoch: [1][8900/36908] Elapsed 122m 59s (remain 386m 58s) Loss: 0.0026(0.0025) Grad: 16073.7939  LR: 0.000017  \n",
      "Epoch: [1][9000/36908] Elapsed 124m 22s (remain 385m 37s) Loss: 0.0006(0.0025) Grad: 5895.9541  LR: 0.000017  \n",
      "Epoch: [1][9100/36908] Elapsed 125m 45s (remain 384m 14s) Loss: 0.0058(0.0025) Grad: 26383.7461  LR: 0.000017  \n",
      "Epoch: [1][9200/36908] Elapsed 127m 8s (remain 382m 51s) Loss: 0.0001(0.0025) Grad: 78.8685  LR: 0.000017  \n",
      "Epoch: [1][9300/36908] Elapsed 128m 30s (remain 381m 25s) Loss: 0.0076(0.0025) Grad: 79539.8359  LR: 0.000017  \n",
      "Epoch: [1][9400/36908] Elapsed 129m 52s (remain 380m 0s) Loss: 0.0001(0.0025) Grad: 318.8655  LR: 0.000017  \n",
      "Epoch: [1][9500/36908] Elapsed 131m 14s (remain 378m 35s) Loss: 0.0000(0.0025) Grad: 36.2119  LR: 0.000017  \n",
      "Epoch: [1][9600/36908] Elapsed 132m 37s (remain 377m 12s) Loss: 0.0001(0.0025) Grad: 67.2321  LR: 0.000016  \n",
      "Epoch: [1][9700/36908] Elapsed 133m 59s (remain 375m 47s) Loss: 0.0002(0.0025) Grad: 403.0579  LR: 0.000016  \n",
      "Epoch: [1][9800/36908] Elapsed 135m 22s (remain 374m 24s) Loss: 0.0001(0.0025) Grad: 186.5737  LR: 0.000016  \n",
      "Epoch: [1][9900/36908] Elapsed 136m 45s (remain 373m 2s) Loss: 0.0003(0.0025) Grad: 317.6779  LR: 0.000016  \n",
      "Epoch: [1][10000/36908] Elapsed 138m 8s (remain 371m 40s) Loss: 0.0003(0.0024) Grad: 812.3311  LR: 0.000016  \n",
      "Epoch: [1][10100/36908] Elapsed 139m 31s (remain 370m 17s) Loss: 0.0033(0.0025) Grad: 23743.4414  LR: 0.000016  \n",
      "Epoch: [1][10200/36908] Elapsed 140m 53s (remain 368m 52s) Loss: 0.0028(0.0025) Grad: 9683.2275  LR: 0.000016  \n",
      "Epoch: [1][10300/36908] Elapsed 142m 15s (remain 367m 27s) Loss: 0.0011(0.0025) Grad: 16607.9434  LR: 0.000016  \n",
      "Epoch: [1][10400/36908] Elapsed 143m 37s (remain 366m 2s) Loss: 0.0001(0.0025) Grad: 142.1337  LR: 0.000016  \n",
      "Epoch: [1][10500/36908] Elapsed 145m 0s (remain 364m 40s) Loss: 0.0105(0.0025) Grad: 39928.8242  LR: 0.000016  \n",
      "Epoch: [1][10600/36908] Elapsed 146m 24s (remain 363m 19s) Loss: 0.0000(0.0025) Grad: 87.0491  LR: 0.000016  \n",
      "Epoch: [1][10700/36908] Elapsed 147m 47s (remain 361m 57s) Loss: 0.0001(0.0025) Grad: 39.6926  LR: 0.000016  \n",
      "Epoch: [1][10800/36908] Elapsed 149m 9s (remain 360m 32s) Loss: 0.0001(0.0025) Grad: 109.8482  LR: 0.000016  \n",
      "Epoch: [1][10900/36908] Elapsed 150m 33s (remain 359m 10s) Loss: 0.0001(0.0025) Grad: 25.9765  LR: 0.000016  \n",
      "Epoch: [1][11000/36908] Elapsed 151m 56s (remain 357m 48s) Loss: 0.0010(0.0025) Grad: 11531.8682  LR: 0.000016  \n",
      "Epoch: [1][11100/36908] Elapsed 153m 18s (remain 356m 25s) Loss: 0.0011(0.0025) Grad: 15877.2002  LR: 0.000016  \n",
      "Epoch: [1][11200/36908] Elapsed 154m 43s (remain 355m 5s) Loss: 0.0007(0.0025) Grad: 589.6818  LR: 0.000015  \n",
      "Epoch: [1][11300/36908] Elapsed 156m 6s (remain 353m 44s) Loss: 0.0082(0.0025) Grad: 15259.0488  LR: 0.000015  \n",
      "Epoch: [1][11400/36908] Elapsed 157m 29s (remain 352m 20s) Loss: 0.0006(0.0025) Grad: 6776.7285  LR: 0.000015  \n",
      "Epoch: [1][11500/36908] Elapsed 158m 51s (remain 350m 56s) Loss: 0.0009(0.0025) Grad: 14233.3408  LR: 0.000015  \n",
      "Epoch: [1][11600/36908] Elapsed 160m 14s (remain 349m 34s) Loss: 0.0026(0.0025) Grad: 5481.6865  LR: 0.000015  \n",
      "Epoch: [1][11700/36908] Elapsed 161m 37s (remain 348m 9s) Loss: 0.0005(0.0025) Grad: 2633.7456  LR: 0.000015  \n",
      "Epoch: [1][11800/36908] Elapsed 162m 58s (remain 346m 44s) Loss: 0.0066(0.0025) Grad: 40010.5195  LR: 0.000015  \n",
      "Epoch: [1][11900/36908] Elapsed 164m 22s (remain 345m 23s) Loss: 0.0015(0.0025) Grad: 1295.7001  LR: 0.000015  \n",
      "Epoch: [1][12000/36908] Elapsed 165m 45s (remain 344m 1s) Loss: 0.0003(0.0025) Grad: 2625.7166  LR: 0.000015  \n",
      "Epoch: [1][12100/36908] Elapsed 167m 8s (remain 342m 39s) Loss: 0.0001(0.0024) Grad: 72.6366  LR: 0.000015  \n",
      "Epoch: [1][12200/36908] Elapsed 168m 32s (remain 341m 17s) Loss: 0.0001(0.0024) Grad: 87.7067  LR: 0.000015  \n",
      "Epoch: [1][12300/36908] Elapsed 169m 55s (remain 339m 56s) Loss: 0.0013(0.0024) Grad: 38328.6328  LR: 0.000015  \n",
      "Epoch: [1][12400/36908] Elapsed 171m 18s (remain 338m 32s) Loss: 0.0007(0.0025) Grad: 4249.6582  LR: 0.000015  \n",
      "Epoch: [1][12500/36908] Elapsed 172m 42s (remain 337m 11s) Loss: 0.0040(0.0025) Grad: 44630.3555  LR: 0.000015  \n",
      "Epoch: [1][12600/36908] Elapsed 174m 5s (remain 335m 48s) Loss: 0.0001(0.0025) Grad: 30.0242  LR: 0.000015  \n",
      "Epoch: [1][12700/36908] Elapsed 175m 27s (remain 334m 23s) Loss: 0.0001(0.0025) Grad: 252.2734  LR: 0.000015  \n",
      "Epoch: [1][12800/36908] Elapsed 176m 51s (remain 333m 3s) Loss: 0.0001(0.0025) Grad: 648.6207  LR: 0.000015  \n",
      "Epoch: [1][12900/36908] Elapsed 178m 14s (remain 331m 40s) Loss: 0.0037(0.0025) Grad: 169949.7812  LR: 0.000014  \n",
      "Epoch: [1][13000/36908] Elapsed 179m 38s (remain 330m 19s) Loss: 0.0001(0.0025) Grad: 1204.7924  LR: 0.000014  \n",
      "Epoch: [1][13100/36908] Elapsed 181m 1s (remain 328m 58s) Loss: 0.0010(0.0024) Grad: 8263.3066  LR: 0.000014  \n",
      "Epoch: [1][13200/36908] Elapsed 182m 24s (remain 327m 35s) Loss: 0.0012(0.0024) Grad: 1643.7562  LR: 0.000014  \n",
      "Epoch: [1][13300/36908] Elapsed 183m 47s (remain 326m 12s) Loss: 0.0010(0.0024) Grad: 3604.5085  LR: 0.000014  \n",
      "Epoch: [1][13400/36908] Elapsed 185m 10s (remain 324m 49s) Loss: 0.0024(0.0024) Grad: 22383.1270  LR: 0.000014  \n",
      "Epoch: [1][13500/36908] Elapsed 186m 33s (remain 323m 26s) Loss: 0.0001(0.0024) Grad: 513.8457  LR: 0.000014  \n",
      "Epoch: [1][13600/36908] Elapsed 187m 55s (remain 322m 2s) Loss: 0.0042(0.0024) Grad: 23018.5352  LR: 0.000014  \n",
      "Epoch: [1][13700/36908] Elapsed 189m 17s (remain 320m 37s) Loss: 0.0001(0.0024) Grad: 266.9345  LR: 0.000014  \n",
      "Epoch: [1][13800/36908] Elapsed 190m 41s (remain 319m 15s) Loss: 0.0057(0.0024) Grad: 13669.9648  LR: 0.000014  \n",
      "Epoch: [1][13900/36908] Elapsed 192m 3s (remain 317m 52s) Loss: 0.0001(0.0024) Grad: 119.6455  LR: 0.000014  \n",
      "Epoch: [1][14000/36908] Elapsed 193m 26s (remain 316m 29s) Loss: 0.0039(0.0024) Grad: 9477.4482  LR: 0.000014  \n",
      "Epoch: [1][14100/36908] Elapsed 194m 50s (remain 315m 8s) Loss: 0.0082(0.0024) Grad: 112451.5547  LR: 0.000014  \n",
      "Epoch: [1][14200/36908] Elapsed 196m 14s (remain 313m 46s) Loss: 0.0008(0.0024) Grad: 1355.6669  LR: 0.000014  \n",
      "Epoch: [1][14300/36908] Elapsed 197m 38s (remain 312m 25s) Loss: 0.0001(0.0024) Grad: 56.5439  LR: 0.000014  \n",
      "Epoch: [1][14400/36908] Elapsed 199m 2s (remain 311m 4s) Loss: 0.0026(0.0024) Grad: 37149.2383  LR: 0.000014  \n",
      "Epoch: [1][14500/36908] Elapsed 200m 24s (remain 309m 40s) Loss: 0.0027(0.0025) Grad: 15007.2061  LR: 0.000013  \n",
      "Epoch: [1][14600/36908] Elapsed 201m 46s (remain 308m 16s) Loss: 0.0028(0.0024) Grad: 43145.6406  LR: 0.000013  \n",
      "Epoch: [1][14700/36908] Elapsed 203m 9s (remain 306m 53s) Loss: 0.0012(0.0025) Grad: 6203.5791  LR: 0.000013  \n",
      "Epoch: [1][14800/36908] Elapsed 204m 33s (remain 305m 31s) Loss: 0.0034(0.0025) Grad: 36427.5938  LR: 0.000013  \n",
      "Epoch: [1][14900/36908] Elapsed 205m 56s (remain 304m 8s) Loss: 0.0000(0.0025) Grad: 33.8929  LR: 0.000013  \n",
      "Epoch: [1][15000/36908] Elapsed 207m 19s (remain 302m 46s) Loss: 0.0030(0.0025) Grad: 43783.2422  LR: 0.000013  \n",
      "Epoch: [1][15100/36908] Elapsed 208m 42s (remain 301m 23s) Loss: 0.0010(0.0025) Grad: 2071.7800  LR: 0.000013  \n",
      "Epoch: [1][15200/36908] Elapsed 210m 5s (remain 300m 0s) Loss: 0.0019(0.0025) Grad: 8366.6562  LR: 0.000013  \n",
      "Epoch: [1][15300/36908] Elapsed 211m 27s (remain 298m 36s) Loss: 0.0032(0.0025) Grad: 54663.2695  LR: 0.000013  \n",
      "Epoch: [1][15400/36908] Elapsed 212m 50s (remain 297m 13s) Loss: 0.0001(0.0025) Grad: 40.3113  LR: 0.000013  \n",
      "Epoch: [1][15500/36908] Elapsed 214m 14s (remain 295m 51s) Loss: 0.0022(0.0025) Grad: 30601.7812  LR: 0.000013  \n",
      "Epoch: [1][15600/36908] Elapsed 215m 37s (remain 294m 29s) Loss: 0.0001(0.0025) Grad: 2141.5183  LR: 0.000013  \n",
      "Epoch: [1][15700/36908] Elapsed 217m 0s (remain 293m 6s) Loss: 0.0062(0.0025) Grad: 144590.1406  LR: 0.000013  \n",
      "Epoch: [1][15800/36908] Elapsed 218m 23s (remain 291m 44s) Loss: 0.0142(0.0025) Grad: 148887.8750  LR: 0.000013  \n",
      "Epoch: [1][15900/36908] Elapsed 219m 46s (remain 290m 21s) Loss: 0.0001(0.0025) Grad: 70.5373  LR: 0.000013  \n",
      "Epoch: [1][16000/36908] Elapsed 221m 10s (remain 288m 58s) Loss: 0.0002(0.0025) Grad: 680.0704  LR: 0.000013  \n",
      "Epoch: [1][16100/36908] Elapsed 222m 33s (remain 287m 36s) Loss: 0.0003(0.0024) Grad: 1202.8928  LR: 0.000013  \n",
      "Epoch: [1][16200/36908] Elapsed 223m 55s (remain 286m 12s) Loss: 0.0001(0.0024) Grad: 83.8120  LR: 0.000012  \n",
      "Epoch: [1][16300/36908] Elapsed 225m 18s (remain 284m 48s) Loss: 0.0000(0.0025) Grad: 382.2428  LR: 0.000012  \n",
      "Epoch: [1][16400/36908] Elapsed 226m 41s (remain 283m 26s) Loss: 0.0000(0.0025) Grad: 81.5501  LR: 0.000012  \n",
      "Epoch: [1][16500/36908] Elapsed 228m 5s (remain 282m 4s) Loss: 0.0252(0.0025) Grad: 791809.8125  LR: 0.000012  \n",
      "Epoch: [1][16600/36908] Elapsed 229m 28s (remain 280m 42s) Loss: 0.0005(0.0025) Grad: 8861.2070  LR: 0.000012  \n",
      "Epoch: [1][16700/36908] Elapsed 230m 51s (remain 279m 18s) Loss: 0.0001(0.0025) Grad: 157.0825  LR: 0.000012  \n",
      "Epoch: [1][16800/36908] Elapsed 232m 13s (remain 277m 55s) Loss: 0.0015(0.0025) Grad: 24955.0996  LR: 0.000012  \n",
      "Epoch: [1][16900/36908] Elapsed 233m 36s (remain 276m 31s) Loss: 0.0003(0.0025) Grad: 1950.7489  LR: 0.000012  \n",
      "Epoch: [1][17000/36908] Elapsed 234m 58s (remain 275m 8s) Loss: 0.0001(0.0024) Grad: 1074.7661  LR: 0.000012  \n",
      "Epoch: [1][17100/36908] Elapsed 236m 22s (remain 273m 47s) Loss: 0.0001(0.0024) Grad: 85.2285  LR: 0.000012  \n",
      "Epoch: [1][17200/36908] Elapsed 237m 46s (remain 272m 25s) Loss: 0.0018(0.0024) Grad: 23290.8242  LR: 0.000012  \n",
      "Epoch: [1][17300/36908] Elapsed 239m 10s (remain 271m 3s) Loss: 0.0009(0.0025) Grad: 9753.3887  LR: 0.000012  \n",
      "Epoch: [1][17400/36908] Elapsed 240m 34s (remain 269m 40s) Loss: 0.0035(0.0024) Grad: 98147.9922  LR: 0.000012  \n",
      "Epoch: [1][17500/36908] Elapsed 241m 58s (remain 268m 19s) Loss: 0.0008(0.0024) Grad: 29044.1152  LR: 0.000012  \n",
      "Epoch: [1][17600/36908] Elapsed 243m 21s (remain 266m 56s) Loss: 0.0060(0.0025) Grad: 53824.7070  LR: 0.000012  \n",
      "Epoch: [1][17700/36908] Elapsed 244m 44s (remain 265m 33s) Loss: 0.0007(0.0025) Grad: 12073.1455  LR: 0.000012  \n",
      "Epoch: [1][17800/36908] Elapsed 246m 8s (remain 264m 12s) Loss: 0.0036(0.0024) Grad: 459879.5312  LR: 0.000012  \n",
      "Epoch: [1][17900/36908] Elapsed 247m 30s (remain 262m 48s) Loss: 0.0003(0.0025) Grad: 6766.1440  LR: 0.000011  \n",
      "Epoch: [1][18000/36908] Elapsed 248m 52s (remain 261m 24s) Loss: 0.0003(0.0025) Grad: 2235.4021  LR: 0.000011  \n",
      "Epoch: [1][18100/36908] Elapsed 250m 15s (remain 260m 0s) Loss: 0.0005(0.0025) Grad: 444.5626  LR: 0.000011  \n",
      "Epoch: [1][18200/36908] Elapsed 251m 37s (remain 258m 37s) Loss: 0.0002(0.0025) Grad: 465.0382  LR: 0.000011  \n",
      "Epoch: [1][18300/36908] Elapsed 253m 1s (remain 257m 14s) Loss: 0.0006(0.0025) Grad: 506.0794  LR: 0.000011  \n",
      "Epoch: [1][18400/36908] Elapsed 254m 23s (remain 255m 50s) Loss: 0.0167(0.0025) Grad: 174604.3906  LR: 0.000011  \n",
      "Epoch: [1][18500/36908] Elapsed 255m 45s (remain 254m 27s) Loss: 0.0004(0.0025) Grad: 2309.0557  LR: 0.000011  \n",
      "Epoch: [1][18600/36908] Elapsed 257m 7s (remain 253m 3s) Loss: 0.0002(0.0025) Grad: 320.9932  LR: 0.000011  \n",
      "Epoch: [1][18700/36908] Elapsed 258m 29s (remain 251m 40s) Loss: 0.0001(0.0025) Grad: 1884.7766  LR: 0.000011  \n",
      "Epoch: [1][18800/36908] Elapsed 259m 53s (remain 250m 18s) Loss: 0.0055(0.0025) Grad: 97504.6953  LR: 0.000011  \n",
      "Epoch: [1][18900/36908] Elapsed 261m 17s (remain 248m 55s) Loss: 0.0020(0.0025) Grad: 21953.6641  LR: 0.000011  \n",
      "Epoch: [1][19000/36908] Elapsed 262m 40s (remain 247m 33s) Loss: 0.0002(0.0025) Grad: 337.0187  LR: 0.000011  \n",
      "Epoch: [1][19100/36908] Elapsed 264m 3s (remain 246m 9s) Loss: 0.0014(0.0025) Grad: 21594.1328  LR: 0.000011  \n",
      "Epoch: [1][19200/36908] Elapsed 265m 25s (remain 244m 46s) Loss: 0.0000(0.0025) Grad: 54.0896  LR: 0.000011  \n",
      "Epoch: [1][19300/36908] Elapsed 266m 48s (remain 243m 23s) Loss: 0.0007(0.0025) Grad: 9464.1719  LR: 0.000011  \n",
      "Epoch: [1][19400/36908] Elapsed 268m 12s (remain 242m 1s) Loss: 0.0001(0.0025) Grad: 644.0185  LR: 0.000011  \n",
      "Epoch: [1][19500/36908] Elapsed 269m 34s (remain 240m 37s) Loss: 0.0007(0.0025) Grad: 2284.0667  LR: 0.000010  \n",
      "Epoch: [1][19600/36908] Elapsed 270m 57s (remain 239m 15s) Loss: 0.0008(0.0025) Grad: 2922.1628  LR: 0.000010  \n",
      "Epoch: [1][19700/36908] Elapsed 272m 22s (remain 237m 53s) Loss: 0.0044(0.0025) Grad: 44514.0742  LR: 0.000010  \n",
      "Epoch: [1][19800/36908] Elapsed 273m 46s (remain 236m 31s) Loss: 0.0076(0.0025) Grad: 135158.9062  LR: 0.000010  \n",
      "Epoch: [1][19900/36908] Elapsed 275m 9s (remain 235m 8s) Loss: 0.0003(0.0025) Grad: 2294.5693  LR: 0.000010  \n",
      "Epoch: [1][20000/36908] Elapsed 276m 32s (remain 233m 45s) Loss: 0.0046(0.0025) Grad: 92659.7422  LR: 0.000010  \n",
      "Epoch: [1][20100/36908] Elapsed 277m 55s (remain 232m 22s) Loss: 0.0053(0.0025) Grad: 29115.1113  LR: 0.000010  \n",
      "Epoch: [1][20200/36908] Elapsed 279m 18s (remain 230m 59s) Loss: 0.0005(0.0025) Grad: 1715.1083  LR: 0.000010  \n",
      "Epoch: [1][20300/36908] Elapsed 280m 41s (remain 229m 37s) Loss: 0.0019(0.0025) Grad: 28108.1504  LR: 0.000010  \n",
      "Epoch: [1][20400/36908] Elapsed 282m 4s (remain 228m 14s) Loss: 0.0001(0.0025) Grad: 72.7294  LR: 0.000010  \n",
      "Epoch: [1][20500/36908] Elapsed 283m 28s (remain 226m 51s) Loss: 0.0028(0.0024) Grad: 17841.9219  LR: 0.000010  \n",
      "Epoch: [1][20600/36908] Elapsed 284m 50s (remain 225m 28s) Loss: 0.0006(0.0024) Grad: 64415.1445  LR: 0.000010  \n",
      "Epoch: [1][20700/36908] Elapsed 286m 12s (remain 224m 4s) Loss: 0.0084(0.0024) Grad: 40955.2539  LR: 0.000010  \n",
      "Epoch: [1][20800/36908] Elapsed 287m 35s (remain 222m 41s) Loss: 0.0000(0.0024) Grad: 31.4489  LR: 0.000010  \n",
      "Epoch: [1][20900/36908] Elapsed 288m 59s (remain 221m 19s) Loss: 0.0002(0.0024) Grad: 4238.2104  LR: 0.000010  \n",
      "Epoch: [1][21000/36908] Elapsed 290m 22s (remain 219m 56s) Loss: 0.0142(0.0024) Grad: 227485.8125  LR: 0.000010  \n",
      "Epoch: [1][21100/36908] Elapsed 291m 44s (remain 218m 32s) Loss: 0.0021(0.0024) Grad: 17073.4473  LR: 0.000010  \n",
      "Epoch: [1][21200/36908] Elapsed 293m 6s (remain 217m 8s) Loss: 0.0118(0.0024) Grad: 96667.9141  LR: 0.000009  \n",
      "Epoch: [1][21300/36908] Elapsed 294m 27s (remain 215m 45s) Loss: 0.0002(0.0024) Grad: 4556.5977  LR: 0.000009  \n",
      "Epoch: [1][21400/36908] Elapsed 295m 51s (remain 214m 22s) Loss: 0.0054(0.0024) Grad: 9865.4141  LR: 0.000009  \n",
      "Epoch: [1][21500/36908] Elapsed 297m 14s (remain 212m 59s) Loss: 0.0013(0.0024) Grad: 11005.6689  LR: 0.000009  \n",
      "Epoch: [1][21600/36908] Elapsed 298m 37s (remain 211m 36s) Loss: 0.0017(0.0024) Grad: 26783.9648  LR: 0.000009  \n",
      "Epoch: [1][21700/36908] Elapsed 300m 0s (remain 210m 13s) Loss: 0.0010(0.0024) Grad: 3588.1331  LR: 0.000009  \n",
      "Epoch: [1][21800/36908] Elapsed 301m 23s (remain 208m 50s) Loss: 0.0000(0.0024) Grad: 50.4038  LR: 0.000009  \n",
      "Epoch: [1][21900/36908] Elapsed 302m 46s (remain 207m 27s) Loss: 0.0002(0.0024) Grad: 204.2492  LR: 0.000009  \n",
      "Epoch: [1][22000/36908] Elapsed 304m 9s (remain 206m 5s) Loss: 0.0001(0.0024) Grad: 419.0639  LR: 0.000009  \n",
      "Epoch: [1][22100/36908] Elapsed 305m 33s (remain 204m 43s) Loss: 0.0014(0.0024) Grad: 103776.0469  LR: 0.000009  \n",
      "Epoch: [1][22200/36908] Elapsed 306m 58s (remain 203m 21s) Loss: 0.0011(0.0024) Grad: 14640.0605  LR: 0.000009  \n",
      "Epoch: [1][22300/36908] Elapsed 308m 21s (remain 201m 58s) Loss: 0.0007(0.0024) Grad: 8340.0762  LR: 0.000009  \n",
      "Epoch: [1][22400/36908] Elapsed 309m 44s (remain 200m 35s) Loss: 0.0001(0.0024) Grad: 251.0073  LR: 0.000009  \n",
      "Epoch: [1][22500/36908] Elapsed 311m 8s (remain 199m 12s) Loss: 0.0006(0.0024) Grad: 823.7212  LR: 0.000009  \n",
      "Epoch: [1][22600/36908] Elapsed 312m 32s (remain 197m 50s) Loss: 0.0007(0.0024) Grad: 3655.5415  LR: 0.000009  \n",
      "Epoch: [1][22700/36908] Elapsed 313m 54s (remain 196m 27s) Loss: 0.0029(0.0024) Grad: 41710.0352  LR: 0.000009  \n",
      "Epoch: [1][22800/36908] Elapsed 315m 18s (remain 195m 4s) Loss: 0.0002(0.0024) Grad: 847.2313  LR: 0.000008  \n",
      "Epoch: [1][22900/36908] Elapsed 316m 40s (remain 193m 41s) Loss: 0.0013(0.0024) Grad: 21389.9492  LR: 0.000008  \n",
      "Epoch: [1][23000/36908] Elapsed 318m 2s (remain 192m 17s) Loss: 0.0004(0.0024) Grad: 9636.6768  LR: 0.000008  \n",
      "Epoch: [1][23100/36908] Elapsed 319m 26s (remain 190m 55s) Loss: 0.0006(0.0024) Grad: 1553.4268  LR: 0.000008  \n",
      "Epoch: [1][23200/36908] Elapsed 320m 48s (remain 189m 31s) Loss: 0.0118(0.0024) Grad: 366380.1250  LR: 0.000008  \n",
      "Epoch: [1][23300/36908] Elapsed 322m 11s (remain 188m 8s) Loss: 0.0006(0.0024) Grad: 3700.0696  LR: 0.000008  \n",
      "Epoch: [1][23400/36908] Elapsed 323m 34s (remain 186m 46s) Loss: 0.0000(0.0024) Grad: 35.8250  LR: 0.000008  \n",
      "Epoch: [1][23500/36908] Elapsed 324m 58s (remain 185m 23s) Loss: 0.0049(0.0024) Grad: 146738.5781  LR: 0.000008  \n",
      "Epoch: [1][23600/36908] Elapsed 326m 21s (remain 184m 0s) Loss: 0.0028(0.0024) Grad: 48052.5430  LR: 0.000008  \n",
      "Epoch: [1][23700/36908] Elapsed 327m 44s (remain 182m 37s) Loss: 0.0003(0.0024) Grad: 1027.4575  LR: 0.000008  \n",
      "Epoch: [1][23800/36908] Elapsed 329m 7s (remain 181m 14s) Loss: 0.0204(0.0024) Grad: 163650.6562  LR: 0.000008  \n",
      "Epoch: [1][23900/36908] Elapsed 330m 30s (remain 179m 52s) Loss: 0.0043(0.0024) Grad: 28066.4531  LR: 0.000008  \n",
      "Epoch: [1][24000/36908] Elapsed 331m 53s (remain 178m 28s) Loss: 0.0040(0.0024) Grad: 51804.3633  LR: 0.000008  \n",
      "Epoch: [1][24100/36908] Elapsed 333m 15s (remain 177m 5s) Loss: 0.0004(0.0024) Grad: 22013.5059  LR: 0.000008  \n",
      "Epoch: [1][24200/36908] Elapsed 334m 37s (remain 175m 41s) Loss: 0.0007(0.0024) Grad: 6563.3677  LR: 0.000008  \n",
      "Epoch: [1][24300/36908] Elapsed 336m 0s (remain 174m 18s) Loss: 0.0126(0.0024) Grad: 919342.8125  LR: 0.000008  \n",
      "Epoch: [1][24400/36908] Elapsed 337m 23s (remain 172m 55s) Loss: 0.0000(0.0024) Grad: 2643.1943  LR: 0.000008  \n",
      "Epoch: [1][24500/36908] Elapsed 338m 47s (remain 171m 33s) Loss: 0.0004(0.0024) Grad: 1653.2625  LR: 0.000007  \n",
      "Epoch: [1][24600/36908] Elapsed 340m 10s (remain 170m 10s) Loss: 0.0001(0.0024) Grad: 379.1945  LR: 0.000007  \n",
      "Epoch: [1][24700/36908] Elapsed 341m 33s (remain 168m 47s) Loss: 0.0071(0.0024) Grad: 151713.8438  LR: 0.000007  \n",
      "Epoch: [1][24800/36908] Elapsed 342m 55s (remain 167m 24s) Loss: 0.0001(0.0024) Grad: 270.9292  LR: 0.000007  \n",
      "Epoch: [1][24900/36908] Elapsed 344m 18s (remain 166m 1s) Loss: 0.0007(0.0024) Grad: 21735.1504  LR: 0.000007  \n",
      "Epoch: [1][25000/36908] Elapsed 345m 42s (remain 164m 39s) Loss: 0.0003(0.0024) Grad: 6029.1758  LR: 0.000007  \n",
      "Epoch: [1][25100/36908] Elapsed 347m 7s (remain 163m 16s) Loss: 0.0002(0.0024) Grad: 15956.1777  LR: 0.000007  \n",
      "Epoch: [1][25200/36908] Elapsed 348m 30s (remain 161m 53s) Loss: 0.0003(0.0024) Grad: 1620.5125  LR: 0.000007  \n",
      "Epoch: [1][25300/36908] Elapsed 349m 53s (remain 160m 31s) Loss: 0.0000(0.0024) Grad: 68.2038  LR: 0.000007  \n",
      "Epoch: [1][25400/36908] Elapsed 351m 17s (remain 159m 8s) Loss: 0.0002(0.0024) Grad: 5060.9448  LR: 0.000007  \n",
      "Epoch: [1][25500/36908] Elapsed 352m 40s (remain 157m 45s) Loss: 0.0003(0.0024) Grad: 2580.0664  LR: 0.000007  \n",
      "Epoch: [1][25600/36908] Elapsed 354m 3s (remain 156m 22s) Loss: 0.0021(0.0024) Grad: 11756.6299  LR: 0.000007  \n",
      "Epoch: [1][25700/36908] Elapsed 355m 25s (remain 154m 59s) Loss: 0.0004(0.0024) Grad: 5922.3398  LR: 0.000007  \n",
      "Epoch: [1][25800/36908] Elapsed 356m 49s (remain 153m 36s) Loss: 0.0070(0.0024) Grad: 146536.6250  LR: 0.000007  \n",
      "Epoch: [1][25900/36908] Elapsed 358m 13s (remain 152m 13s) Loss: 0.0007(0.0024) Grad: 15164.7539  LR: 0.000007  \n",
      "Epoch: [1][26000/36908] Elapsed 359m 36s (remain 150m 51s) Loss: 0.0002(0.0024) Grad: 2241.8828  LR: 0.000007  \n",
      "Epoch: [1][26100/36908] Elapsed 360m 59s (remain 149m 28s) Loss: 0.0002(0.0024) Grad: 2529.7971  LR: 0.000007  \n",
      "Epoch: [1][26200/36908] Elapsed 362m 24s (remain 148m 5s) Loss: 0.0011(0.0024) Grad: 35756.2109  LR: 0.000006  \n",
      "Epoch: [1][26300/36908] Elapsed 363m 46s (remain 146m 42s) Loss: 0.0014(0.0024) Grad: 56344.5781  LR: 0.000006  \n",
      "Epoch: [1][26400/36908] Elapsed 365m 9s (remain 145m 19s) Loss: 0.0008(0.0024) Grad: 28559.7246  LR: 0.000006  \n",
      "Epoch: [1][26500/36908] Elapsed 366m 32s (remain 143m 56s) Loss: 0.0000(0.0024) Grad: 153.6759  LR: 0.000006  \n",
      "Epoch: [1][26600/36908] Elapsed 367m 56s (remain 142m 33s) Loss: 0.0038(0.0024) Grad: 57723.4648  LR: 0.000006  \n",
      "Epoch: [1][26700/36908] Elapsed 369m 18s (remain 141m 10s) Loss: 0.0093(0.0024) Grad: 66994.5859  LR: 0.000006  \n",
      "Epoch: [1][26800/36908] Elapsed 370m 40s (remain 139m 47s) Loss: 0.0008(0.0024) Grad: 22912.9512  LR: 0.000006  \n",
      "Epoch: [1][26900/36908] Elapsed 372m 3s (remain 138m 24s) Loss: 0.0012(0.0024) Grad: 5194.6899  LR: 0.000006  \n",
      "Epoch: [1][27000/36908] Elapsed 373m 26s (remain 137m 1s) Loss: 0.0005(0.0024) Grad: 513.9266  LR: 0.000006  \n",
      "Epoch: [1][27100/36908] Elapsed 374m 48s (remain 135m 37s) Loss: 0.0003(0.0024) Grad: 2596.9719  LR: 0.000006  \n",
      "Epoch: [1][27200/36908] Elapsed 376m 11s (remain 134m 15s) Loss: 0.0022(0.0024) Grad: 28895.0371  LR: 0.000006  \n",
      "Epoch: [1][27300/36908] Elapsed 377m 34s (remain 132m 52s) Loss: 0.0010(0.0024) Grad: 734.2217  LR: 0.000006  \n",
      "Epoch: [1][27400/36908] Elapsed 378m 57s (remain 131m 29s) Loss: 0.0047(0.0024) Grad: 49699.8398  LR: 0.000006  \n",
      "Epoch: [1][27500/36908] Elapsed 380m 20s (remain 130m 5s) Loss: 0.0001(0.0024) Grad: 61.5818  LR: 0.000006  \n",
      "Epoch: [1][27600/36908] Elapsed 381m 42s (remain 128m 42s) Loss: 0.0000(0.0024) Grad: 27.9794  LR: 0.000006  \n",
      "Epoch: [1][27700/36908] Elapsed 383m 4s (remain 127m 19s) Loss: 0.0011(0.0024) Grad: 6914.9258  LR: 0.000006  \n",
      "Epoch: [1][27800/36908] Elapsed 384m 26s (remain 125m 56s) Loss: 0.0002(0.0024) Grad: 757.3768  LR: 0.000005  \n",
      "Epoch: [1][27900/36908] Elapsed 385m 48s (remain 124m 32s) Loss: 0.0001(0.0024) Grad: 36.7716  LR: 0.000005  \n",
      "Epoch: [1][28000/36908] Elapsed 387m 10s (remain 123m 9s) Loss: 0.0023(0.0024) Grad: 4735.6577  LR: 0.000005  \n",
      "Epoch: [1][28100/36908] Elapsed 388m 32s (remain 121m 46s) Loss: 0.0002(0.0024) Grad: 52.2405  LR: 0.000005  \n",
      "Epoch: [1][28200/36908] Elapsed 389m 55s (remain 120m 23s) Loss: 0.0108(0.0024) Grad: 41034.1094  LR: 0.000005  \n",
      "Epoch: [1][28300/36908] Elapsed 391m 17s (remain 119m 0s) Loss: 0.0015(0.0024) Grad: 32679.0527  LR: 0.000005  \n",
      "Epoch: [1][28400/36908] Elapsed 392m 41s (remain 117m 37s) Loss: 0.0105(0.0024) Grad: 126554.7188  LR: 0.000005  \n",
      "Epoch: [1][28500/36908] Elapsed 394m 3s (remain 116m 14s) Loss: 0.0018(0.0024) Grad: 2502.6438  LR: 0.000005  \n",
      "Epoch: [1][28600/36908] Elapsed 395m 25s (remain 114m 50s) Loss: 0.0043(0.0024) Grad: 5333.8730  LR: 0.000005  \n",
      "Epoch: [1][28700/36908] Elapsed 396m 48s (remain 113m 28s) Loss: 0.0002(0.0024) Grad: 4310.9478  LR: 0.000005  \n",
      "Epoch: [1][28800/36908] Elapsed 398m 11s (remain 112m 4s) Loss: 0.0034(0.0024) Grad: 22713.3047  LR: 0.000005  \n",
      "Epoch: [1][28900/36908] Elapsed 399m 33s (remain 110m 41s) Loss: 0.0003(0.0024) Grad: 560.9738  LR: 0.000005  \n",
      "Epoch: [1][29000/36908] Elapsed 400m 55s (remain 109m 18s) Loss: 0.0017(0.0024) Grad: 1481.8868  LR: 0.000005  \n",
      "Epoch: [1][29100/36908] Elapsed 402m 18s (remain 107m 55s) Loss: 0.0001(0.0024) Grad: 171.3105  LR: 0.000005  \n",
      "Epoch: [1][29200/36908] Elapsed 403m 40s (remain 106m 32s) Loss: 0.0002(0.0024) Grad: 339.3456  LR: 0.000005  \n",
      "Epoch: [1][29300/36908] Elapsed 405m 3s (remain 105m 9s) Loss: 0.0210(0.0024) Grad: 111116.1875  LR: 0.000005  \n",
      "Epoch: [1][29400/36908] Elapsed 406m 25s (remain 103m 46s) Loss: 0.0003(0.0024) Grad: 62.8974  LR: 0.000005  \n",
      "Epoch: [1][29500/36908] Elapsed 407m 47s (remain 102m 23s) Loss: 0.0013(0.0024) Grad: 6551.8809  LR: 0.000004  \n",
      "Epoch: [1][29600/36908] Elapsed 409m 11s (remain 101m 0s) Loss: 0.0003(0.0024) Grad: 358.1863  LR: 0.000004  \n",
      "Epoch: [1][29700/36908] Elapsed 410m 34s (remain 99m 37s) Loss: 0.0001(0.0024) Grad: 2517.3921  LR: 0.000004  \n",
      "Epoch: [1][29800/36908] Elapsed 411m 58s (remain 98m 14s) Loss: 0.0009(0.0024) Grad: 1887.6458  LR: 0.000004  \n",
      "Epoch: [1][29900/36908] Elapsed 413m 22s (remain 96m 52s) Loss: 0.0020(0.0024) Grad: 2985.3059  LR: 0.000004  \n",
      "Epoch: [1][30000/36908] Elapsed 414m 45s (remain 95m 29s) Loss: 0.0026(0.0024) Grad: 2122.3645  LR: 0.000004  \n",
      "Epoch: [1][30100/36908] Elapsed 416m 9s (remain 94m 6s) Loss: 0.0005(0.0024) Grad: 17366.6602  LR: 0.000004  \n",
      "Epoch: [1][30200/36908] Elapsed 417m 32s (remain 92m 43s) Loss: 0.0020(0.0024) Grad: 5002.5688  LR: 0.000004  \n",
      "Epoch: [1][30300/36908] Elapsed 418m 55s (remain 91m 20s) Loss: 0.0000(0.0024) Grad: 49.9262  LR: 0.000004  \n",
      "Epoch: [1][30400/36908] Elapsed 420m 18s (remain 89m 57s) Loss: 0.0012(0.0024) Grad: 4850.2417  LR: 0.000004  \n",
      "Epoch: [1][30500/36908] Elapsed 421m 42s (remain 88m 35s) Loss: 0.0008(0.0024) Grad: 5507.0728  LR: 0.000004  \n",
      "Epoch: [1][30600/36908] Elapsed 423m 5s (remain 87m 12s) Loss: 0.0009(0.0024) Grad: 19146.7363  LR: 0.000004  \n",
      "Epoch: [1][30700/36908] Elapsed 424m 28s (remain 85m 49s) Loss: 0.0001(0.0024) Grad: 215.1310  LR: 0.000004  \n",
      "Epoch: [1][30800/36908] Elapsed 425m 51s (remain 84m 26s) Loss: 0.0026(0.0024) Grad: 8443.5547  LR: 0.000004  \n",
      "Epoch: [1][30900/36908] Elapsed 427m 14s (remain 83m 3s) Loss: 0.0042(0.0024) Grad: 38873.8906  LR: 0.000004  \n",
      "Epoch: [1][31000/36908] Elapsed 428m 37s (remain 81m 40s) Loss: 0.0008(0.0024) Grad: 3276.1052  LR: 0.000004  \n",
      "Epoch: [1][31100/36908] Elapsed 430m 1s (remain 80m 17s) Loss: 0.0005(0.0024) Grad: 3078.1448  LR: 0.000003  \n",
      "Epoch: [1][31200/36908] Elapsed 431m 25s (remain 78m 54s) Loss: 0.0058(0.0024) Grad: 20367.4277  LR: 0.000003  \n",
      "Epoch: [1][31300/36908] Elapsed 432m 47s (remain 77m 31s) Loss: 0.0000(0.0024) Grad: 152.9090  LR: 0.000003  \n",
      "Epoch: [1][31400/36908] Elapsed 434m 10s (remain 76m 8s) Loss: 0.0005(0.0024) Grad: 3924.5164  LR: 0.000003  \n",
      "Epoch: [1][31500/36908] Elapsed 435m 34s (remain 74m 45s) Loss: 0.0017(0.0024) Grad: 1478.9657  LR: 0.000003  \n",
      "Epoch: [1][31600/36908] Elapsed 436m 57s (remain 73m 22s) Loss: 0.0008(0.0024) Grad: 4319.9561  LR: 0.000003  \n",
      "Epoch: [1][31700/36908] Elapsed 438m 20s (remain 71m 59s) Loss: 0.0001(0.0024) Grad: 166.3735  LR: 0.000003  \n",
      "Epoch: [1][31800/36908] Elapsed 439m 43s (remain 70m 36s) Loss: 0.0002(0.0024) Grad: 377.4418  LR: 0.000003  \n",
      "Epoch: [1][31900/36908] Elapsed 441m 6s (remain 69m 13s) Loss: 0.0045(0.0024) Grad: 20737.8691  LR: 0.000003  \n",
      "Epoch: [1][32000/36908] Elapsed 442m 30s (remain 67m 51s) Loss: 0.0002(0.0024) Grad: 693.1087  LR: 0.000003  \n",
      "Epoch: [1][32100/36908] Elapsed 443m 54s (remain 66m 28s) Loss: 0.0002(0.0024) Grad: 721.1572  LR: 0.000003  \n",
      "Epoch: [1][32200/36908] Elapsed 445m 18s (remain 65m 5s) Loss: 0.0011(0.0024) Grad: 9835.8682  LR: 0.000003  \n",
      "Epoch: [1][32300/36908] Elapsed 446m 40s (remain 63m 42s) Loss: 0.0031(0.0024) Grad: 26798.3887  LR: 0.000003  \n",
      "Epoch: [1][32400/36908] Elapsed 448m 4s (remain 62m 19s) Loss: 0.0002(0.0024) Grad: 129.6703  LR: 0.000003  \n",
      "Epoch: [1][32500/36908] Elapsed 449m 28s (remain 60m 56s) Loss: 0.0000(0.0024) Grad: 25.3015  LR: 0.000003  \n",
      "Epoch: [1][32600/36908] Elapsed 450m 51s (remain 59m 33s) Loss: 0.0056(0.0024) Grad: 124114.7812  LR: 0.000003  \n",
      "Epoch: [1][32700/36908] Elapsed 452m 13s (remain 58m 10s) Loss: 0.0004(0.0024) Grad: 2252.9841  LR: 0.000003  \n",
      "Epoch: [1][32800/36908] Elapsed 453m 37s (remain 56m 47s) Loss: 0.0039(0.0024) Grad: 81130.4609  LR: 0.000002  \n",
      "Epoch: [1][32900/36908] Elapsed 455m 0s (remain 55m 24s) Loss: 0.0096(0.0024) Grad: 82145.3828  LR: 0.000002  \n",
      "Epoch: [1][33000/36908] Elapsed 456m 23s (remain 54m 1s) Loss: 0.0010(0.0024) Grad: 11565.4990  LR: 0.000002  \n",
      "Epoch: [1][33100/36908] Elapsed 457m 47s (remain 52m 39s) Loss: 0.0007(0.0024) Grad: 659.1988  LR: 0.000002  \n",
      "Epoch: [1][33200/36908] Elapsed 459m 10s (remain 51m 16s) Loss: 0.0021(0.0024) Grad: 70907.4766  LR: 0.000002  \n",
      "Epoch: [1][33300/36908] Elapsed 460m 33s (remain 49m 53s) Loss: 0.0134(0.0024) Grad: 446722.3750  LR: 0.000002  \n",
      "Epoch: [1][33400/36908] Elapsed 461m 56s (remain 48m 30s) Loss: 0.0050(0.0024) Grad: 12930.5723  LR: 0.000002  \n",
      "Epoch: [1][33500/36908] Elapsed 463m 20s (remain 47m 7s) Loss: 0.0021(0.0024) Grad: 37955.2344  LR: 0.000002  \n",
      "Epoch: [1][33600/36908] Elapsed 464m 43s (remain 45m 44s) Loss: 0.0027(0.0024) Grad: 10467.6543  LR: 0.000002  \n",
      "Epoch: [1][33700/36908] Elapsed 466m 5s (remain 44m 21s) Loss: 0.0318(0.0024) Grad: 40665.0820  LR: 0.000002  \n",
      "Epoch: [1][33800/36908] Elapsed 467m 28s (remain 42m 58s) Loss: 0.0007(0.0024) Grad: 1056.9191  LR: 0.000002  \n",
      "Epoch: [1][33900/36908] Elapsed 468m 50s (remain 41m 35s) Loss: 0.0001(0.0024) Grad: 31.1965  LR: 0.000002  \n",
      "Epoch: [1][34000/36908] Elapsed 470m 12s (remain 40m 12s) Loss: 0.0002(0.0024) Grad: 411.0872  LR: 0.000002  \n",
      "Epoch: [1][34100/36908] Elapsed 471m 34s (remain 38m 49s) Loss: 0.0001(0.0024) Grad: 36.1299  LR: 0.000002  \n",
      "Epoch: [1][34200/36908] Elapsed 472m 56s (remain 37m 26s) Loss: 0.0321(0.0024) Grad: 87340.9062  LR: 0.000002  \n",
      "Epoch: [1][34300/36908] Elapsed 474m 19s (remain 36m 3s) Loss: 0.0005(0.0024) Grad: 1121.9321  LR: 0.000002  \n",
      "Epoch: [1][34400/36908] Elapsed 475m 42s (remain 34m 40s) Loss: 0.0001(0.0024) Grad: 86.2847  LR: 0.000002  \n",
      "Epoch: [1][34500/36908] Elapsed 477m 5s (remain 33m 17s) Loss: 0.0006(0.0024) Grad: 1756.6648  LR: 0.000001  \n",
      "Epoch: [1][34600/36908] Elapsed 478m 28s (remain 31m 54s) Loss: 0.0002(0.0024) Grad: 250.3620  LR: 0.000001  \n",
      "Epoch: [1][34700/36908] Elapsed 479m 52s (remain 30m 31s) Loss: 0.0002(0.0024) Grad: 6005.7773  LR: 0.000001  \n",
      "Epoch: [1][34800/36908] Elapsed 481m 15s (remain 29m 8s) Loss: 0.0013(0.0024) Grad: 11024.4453  LR: 0.000001  \n",
      "Epoch: [1][34900/36908] Elapsed 482m 38s (remain 27m 45s) Loss: 0.0010(0.0024) Grad: 6518.7485  LR: 0.000001  \n",
      "Epoch: [1][35000/36908] Elapsed 484m 1s (remain 26m 22s) Loss: 0.0001(0.0024) Grad: 67.5565  LR: 0.000001  \n",
      "Epoch: [1][35100/36908] Elapsed 485m 23s (remain 24m 59s) Loss: 0.0138(0.0024) Grad: 212827.0000  LR: 0.000001  \n",
      "Epoch: [1][35200/36908] Elapsed 486m 46s (remain 23m 36s) Loss: 0.0000(0.0024) Grad: 128.6563  LR: 0.000001  \n",
      "Epoch: [1][35300/36908] Elapsed 488m 7s (remain 22m 13s) Loss: 0.0001(0.0024) Grad: 171.7842  LR: 0.000001  \n",
      "Epoch: [1][35400/36908] Elapsed 489m 30s (remain 20m 50s) Loss: 0.0002(0.0024) Grad: 429.2451  LR: 0.000001  \n",
      "Epoch: [1][35500/36908] Elapsed 490m 53s (remain 19m 27s) Loss: 0.0001(0.0024) Grad: 1385.0314  LR: 0.000001  \n",
      "Epoch: [1][35600/36908] Elapsed 492m 16s (remain 18m 4s) Loss: 0.0003(0.0024) Grad: 551.1167  LR: 0.000001  \n",
      "Epoch: [1][35700/36908] Elapsed 493m 40s (remain 16m 41s) Loss: 0.0017(0.0024) Grad: 12752.2695  LR: 0.000001  \n",
      "Epoch: [1][35800/36908] Elapsed 495m 3s (remain 15m 18s) Loss: 0.0001(0.0024) Grad: 3229.0544  LR: 0.000001  \n",
      "Epoch: [1][35900/36908] Elapsed 496m 26s (remain 13m 55s) Loss: 0.0018(0.0024) Grad: 9564.5967  LR: 0.000001  \n",
      "Epoch: [1][36000/36908] Elapsed 497m 48s (remain 12m 32s) Loss: 0.0002(0.0024) Grad: 290.1873  LR: 0.000001  \n",
      "Epoch: [1][36100/36908] Elapsed 499m 11s (remain 11m 9s) Loss: 0.0001(0.0024) Grad: 739.3777  LR: 0.000000  \n",
      "Epoch: [1][36200/36908] Elapsed 500m 33s (remain 9m 46s) Loss: 0.0024(0.0024) Grad: 131731.7656  LR: 0.000000  \n",
      "Epoch: [1][36300/36908] Elapsed 501m 57s (remain 8m 23s) Loss: 0.0002(0.0024) Grad: 2047.4629  LR: 0.000000  \n",
      "Epoch: [1][36400/36908] Elapsed 503m 20s (remain 7m 0s) Loss: 0.0008(0.0024) Grad: 10061.8457  LR: 0.000000  \n",
      "Epoch: [1][36500/36908] Elapsed 504m 42s (remain 5m 37s) Loss: 0.0008(0.0024) Grad: 131.9548  LR: 0.000000  \n",
      "Epoch: [1][36600/36908] Elapsed 506m 5s (remain 4m 14s) Loss: 0.0026(0.0024) Grad: 10727.6660  LR: 0.000000  \n",
      "Epoch: [1][36700/36908] Elapsed 507m 29s (remain 2m 51s) Loss: 0.0002(0.0024) Grad: 13503.9102  LR: 0.000000  \n",
      "Epoch: [1][36800/36908] Elapsed 508m 53s (remain 1m 28s) Loss: 0.0032(0.0024) Grad: 74967.8281  LR: 0.000000  \n",
      "Epoch: [1][36900/36908] Elapsed 510m 16s (remain 0m 5s) Loss: 0.0040(0.0024) Grad: 15890.6562  LR: 0.000000  \n",
      "Epoch: [1][36907/36908] Elapsed 510m 22s (remain 0m 0s) Loss: 0.0012(0.0024) Grad: 17439.8008  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 17m 48s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 31s (remain 5m 37s) Loss: 0.0386(0.0077) \n",
      "EVAL: [200/1192] Elapsed 1m 1s (remain 5m 1s) Loss: 0.0052(0.0072) \n",
      "EVAL: [300/1192] Elapsed 1m 30s (remain 4m 28s) Loss: 0.0063(0.0070) \n",
      "EVAL: [400/1192] Elapsed 1m 59s (remain 3m 56s) Loss: 0.0000(0.0074) \n",
      "EVAL: [500/1192] Elapsed 2m 29s (remain 3m 25s) Loss: 0.0000(0.0069) \n",
      "EVAL: [600/1192] Elapsed 2m 59s (remain 2m 56s) Loss: 0.0124(0.0069) \n",
      "EVAL: [700/1192] Elapsed 3m 28s (remain 2m 26s) Loss: 0.0069(0.0076) \n",
      "EVAL: [800/1192] Elapsed 3m 57s (remain 1m 56s) Loss: 0.0000(0.0078) \n",
      "EVAL: [900/1192] Elapsed 4m 26s (remain 1m 26s) Loss: 0.0154(0.0079) \n",
      "EVAL: [1000/1192] Elapsed 4m 55s (remain 0m 56s) Loss: 0.0007(0.0078) \n",
      "EVAL: [1100/1192] Elapsed 5m 25s (remain 0m 26s) Loss: 0.0311(0.0074) \n",
      "EVAL: [1191/1192] Elapsed 5m 52s (remain 0m 0s) Loss: 0.0000(0.0071) \n",
      "Epoch 1 - avg_train_loss: 0.0024  avg_val_loss: 0.0071  time: 30979s\n",
      "Epoch 1 - Score: 0.8919\n",
      "Epoch 1 - Save Best Score: 0.8919 Model\n",
      "========== fold: 3 training ==========\n",
      "get pseudo plain from ../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_3.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_3.npy\n",
      "(612602, 6) (612602, 950)\n",
      "(100000, 7)\n",
      "(110725, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n",
      "Epoch: [1][0/36908] Elapsed 0m 1s (remain 809m 4s) Loss: 0.0002(0.0002) Grad: 24.2172  LR: 0.000000  \n",
      "Epoch: [1][100/36908] Elapsed 1m 25s (remain 518m 27s) Loss: 0.0004(0.0023) Grad: 14.0259  LR: 0.000001  \n",
      "Epoch: [1][200/36908] Elapsed 2m 48s (remain 512m 31s) Loss: 0.0144(0.0024) Grad: 51157.0977  LR: 0.000001  \n",
      "Epoch: [1][300/36908] Elapsed 4m 10s (remain 507m 50s) Loss: 0.0000(0.0022) Grad: 5.3426  LR: 0.000002  \n",
      "Epoch: [1][400/36908] Elapsed 5m 34s (remain 507m 16s) Loss: 0.0003(0.0022) Grad: 39.2877  LR: 0.000002  \n",
      "Epoch: [1][500/36908] Elapsed 6m 58s (remain 506m 23s) Loss: 0.0002(0.0024) Grad: 86.2823  LR: 0.000003  \n",
      "Epoch: [1][600/36908] Elapsed 8m 20s (remain 504m 18s) Loss: 0.0131(0.0026) Grad: 20692.9746  LR: 0.000003  \n",
      "Epoch: [1][700/36908] Elapsed 9m 42s (remain 501m 48s) Loss: 0.0001(0.0026) Grad: 18.9470  LR: 0.000004  \n",
      "Epoch: [1][800/36908] Elapsed 11m 5s (remain 500m 21s) Loss: 0.0006(0.0026) Grad: 214.4708  LR: 0.000004  \n",
      "Epoch: [1][900/36908] Elapsed 12m 28s (remain 498m 27s) Loss: 0.0001(0.0025) Grad: 209.4192  LR: 0.000005  \n",
      "Epoch: [1][1000/36908] Elapsed 13m 50s (remain 496m 27s) Loss: 0.0051(0.0026) Grad: 12929.3477  LR: 0.000005  \n",
      "Epoch: [1][1100/36908] Elapsed 15m 13s (remain 495m 7s) Loss: 0.0027(0.0026) Grad: 788.7114  LR: 0.000006  \n",
      "Epoch: [1][1200/36908] Elapsed 16m 35s (remain 493m 7s) Loss: 0.0003(0.0026) Grad: 64.4065  LR: 0.000007  \n",
      "Epoch: [1][1300/36908] Elapsed 17m 56s (remain 491m 5s) Loss: 0.0001(0.0026) Grad: 69.5255  LR: 0.000007  \n",
      "Epoch: [1][1400/36908] Elapsed 19m 18s (remain 489m 14s) Loss: 0.0004(0.0025) Grad: 42.4321  LR: 0.000008  \n",
      "Epoch: [1][1500/36908] Elapsed 20m 39s (remain 487m 24s) Loss: 0.0059(0.0025) Grad: 4221.1274  LR: 0.000008  \n",
      "Epoch: [1][1600/36908] Elapsed 22m 3s (remain 486m 17s) Loss: 0.0001(0.0026) Grad: 20.9653  LR: 0.000009  \n",
      "Epoch: [1][1700/36908] Elapsed 23m 25s (remain 484m 52s) Loss: 0.0024(0.0025) Grad: 8781.1748  LR: 0.000009  \n",
      "Epoch: [1][1800/36908] Elapsed 24m 47s (remain 483m 14s) Loss: 0.0002(0.0026) Grad: 103.1551  LR: 0.000010  \n",
      "Epoch: [1][1900/36908] Elapsed 26m 8s (remain 481m 22s) Loss: 0.0067(0.0025) Grad: 3377.8567  LR: 0.000010  \n",
      "Epoch: [1][2000/36908] Elapsed 27m 29s (remain 479m 39s) Loss: 0.0049(0.0026) Grad: 2554.2100  LR: 0.000011  \n",
      "Epoch: [1][2100/36908] Elapsed 28m 51s (remain 478m 9s) Loss: 0.0002(0.0026) Grad: 185.4352  LR: 0.000011  \n",
      "Epoch: [1][2200/36908] Elapsed 30m 14s (remain 476m 59s) Loss: 0.0000(0.0026) Grad: 9.7950  LR: 0.000012  \n",
      "Epoch: [1][2300/36908] Elapsed 31m 37s (remain 475m 43s) Loss: 0.0022(0.0025) Grad: 1418.6010  LR: 0.000012  \n",
      "Epoch: [1][2400/36908] Elapsed 32m 59s (remain 474m 15s) Loss: 0.0002(0.0025) Grad: 334.6927  LR: 0.000013  \n",
      "Epoch: [1][2500/36908] Elapsed 34m 22s (remain 472m 53s) Loss: 0.0001(0.0025) Grad: 51.1846  LR: 0.000014  \n",
      "Epoch: [1][2600/36908] Elapsed 35m 43s (remain 471m 18s) Loss: 0.0062(0.0025) Grad: 20514.0840  LR: 0.000014  \n",
      "Epoch: [1][2700/36908] Elapsed 37m 6s (remain 469m 51s) Loss: 0.0009(0.0025) Grad: 318.5032  LR: 0.000015  \n",
      "Epoch: [1][2800/36908] Elapsed 38m 27s (remain 468m 23s) Loss: 0.0096(0.0025) Grad: 17439.7637  LR: 0.000015  \n",
      "Epoch: [1][2900/36908] Elapsed 39m 50s (remain 467m 2s) Loss: 0.0000(0.0025) Grad: 18.5856  LR: 0.000016  \n",
      "Epoch: [1][3000/36908] Elapsed 41m 13s (remain 465m 45s) Loss: 0.0044(0.0025) Grad: 3860.7856  LR: 0.000016  \n",
      "Epoch: [1][3100/36908] Elapsed 42m 36s (remain 464m 27s) Loss: 0.0321(0.0025) Grad: 21136.1172  LR: 0.000017  \n",
      "Epoch: [1][3200/36908] Elapsed 43m 59s (remain 463m 19s) Loss: 0.0004(0.0025) Grad: 1570.8439  LR: 0.000017  \n",
      "Epoch: [1][3300/36908] Elapsed 45m 22s (remain 461m 58s) Loss: 0.0009(0.0025) Grad: 2399.4214  LR: 0.000018  \n",
      "Epoch: [1][3400/36908] Elapsed 46m 44s (remain 460m 30s) Loss: 0.0003(0.0025) Grad: 197.4157  LR: 0.000018  \n",
      "Epoch: [1][3500/36908] Elapsed 48m 7s (remain 459m 9s) Loss: 0.0001(0.0025) Grad: 21.1213  LR: 0.000019  \n",
      "Epoch: [1][3600/36908] Elapsed 49m 31s (remain 458m 0s) Loss: 0.0002(0.0025) Grad: 32.2974  LR: 0.000020  \n",
      "Epoch: [1][3700/36908] Elapsed 50m 53s (remain 456m 40s) Loss: 0.0029(0.0025) Grad: 8055.5015  LR: 0.000020  \n",
      "Epoch: [1][3800/36908] Elapsed 52m 16s (remain 455m 16s) Loss: 0.0049(0.0025) Grad: 5651.9414  LR: 0.000020  \n",
      "Epoch: [1][3900/36908] Elapsed 53m 38s (remain 453m 51s) Loss: 0.0056(0.0025) Grad: 6215.2588  LR: 0.000020  \n",
      "Epoch: [1][4000/36908] Elapsed 55m 1s (remain 452m 32s) Loss: 0.0013(0.0025) Grad: 1416.7539  LR: 0.000020  \n",
      "Epoch: [1][4100/36908] Elapsed 56m 23s (remain 451m 8s) Loss: 0.0009(0.0025) Grad: 3805.9348  LR: 0.000020  \n",
      "Epoch: [1][4200/36908] Elapsed 57m 46s (remain 449m 48s) Loss: 0.0162(0.0026) Grad: 43035.9258  LR: 0.000020  \n",
      "Epoch: [1][4300/36908] Elapsed 59m 8s (remain 448m 23s) Loss: 0.0006(0.0025) Grad: 5102.3984  LR: 0.000020  \n",
      "Epoch: [1][4400/36908] Elapsed 60m 30s (remain 446m 58s) Loss: 0.0034(0.0025) Grad: 36135.3438  LR: 0.000020  \n",
      "Epoch: [1][4500/36908] Elapsed 61m 54s (remain 445m 43s) Loss: 0.0002(0.0025) Grad: 437.8672  LR: 0.000020  \n",
      "Epoch: [1][4600/36908] Elapsed 63m 17s (remain 444m 24s) Loss: 0.0001(0.0025) Grad: 69.9033  LR: 0.000019  \n",
      "Epoch: [1][4700/36908] Elapsed 64m 39s (remain 442m 58s) Loss: 0.0015(0.0025) Grad: 2451.7988  LR: 0.000019  \n",
      "Epoch: [1][4800/36908] Elapsed 66m 2s (remain 441m 40s) Loss: 0.0006(0.0025) Grad: 358.2274  LR: 0.000019  \n",
      "Epoch: [1][4900/36908] Elapsed 67m 25s (remain 440m 17s) Loss: 0.0000(0.0025) Grad: 91.8469  LR: 0.000019  \n",
      "Epoch: [1][5000/36908] Elapsed 68m 47s (remain 438m 52s) Loss: 0.0036(0.0025) Grad: 9204.2920  LR: 0.000019  \n",
      "Epoch: [1][5100/36908] Elapsed 70m 8s (remain 437m 23s) Loss: 0.0004(0.0025) Grad: 1944.9741  LR: 0.000019  \n",
      "Epoch: [1][5200/36908] Elapsed 71m 32s (remain 436m 6s) Loss: 0.0091(0.0025) Grad: 41285.5195  LR: 0.000019  \n",
      "Epoch: [1][5300/36908] Elapsed 72m 53s (remain 434m 38s) Loss: 0.0003(0.0025) Grad: 488.4559  LR: 0.000019  \n",
      "Epoch: [1][5400/36908] Elapsed 74m 15s (remain 433m 12s) Loss: 0.0002(0.0025) Grad: 113.6985  LR: 0.000019  \n",
      "Epoch: [1][5500/36908] Elapsed 75m 39s (remain 431m 59s) Loss: 0.0058(0.0025) Grad: 14379.4609  LR: 0.000019  \n",
      "Epoch: [1][5600/36908] Elapsed 77m 3s (remain 430m 45s) Loss: 0.0014(0.0025) Grad: 4488.4346  LR: 0.000019  \n",
      "Epoch: [1][5700/36908] Elapsed 78m 27s (remain 429m 31s) Loss: 0.0010(0.0025) Grad: 7246.9844  LR: 0.000019  \n",
      "Epoch: [1][5800/36908] Elapsed 79m 50s (remain 428m 6s) Loss: 0.0033(0.0025) Grad: 35808.9805  LR: 0.000019  \n",
      "Epoch: [1][5900/36908] Elapsed 81m 13s (remain 426m 47s) Loss: 0.0015(0.0026) Grad: 636.3181  LR: 0.000019  \n",
      "Epoch: [1][6000/36908] Elapsed 82m 36s (remain 425m 26s) Loss: 0.0001(0.0026) Grad: 111.7929  LR: 0.000019  \n",
      "Epoch: [1][6100/36908] Elapsed 83m 58s (remain 424m 0s) Loss: 0.0000(0.0026) Grad: 45.7761  LR: 0.000019  \n",
      "Epoch: [1][6200/36908] Elapsed 85m 19s (remain 422m 33s) Loss: 0.0002(0.0026) Grad: 477.5493  LR: 0.000018  \n",
      "Epoch: [1][6300/36908] Elapsed 86m 41s (remain 421m 6s) Loss: 0.0001(0.0026) Grad: 851.9688  LR: 0.000018  \n",
      "Epoch: [1][6400/36908] Elapsed 88m 5s (remain 419m 50s) Loss: 0.0135(0.0026) Grad: 32817.5273  LR: 0.000018  \n",
      "Epoch: [1][6500/36908] Elapsed 89m 27s (remain 418m 25s) Loss: 0.0059(0.0026) Grad: 17892.6426  LR: 0.000018  \n",
      "Epoch: [1][6600/36908] Elapsed 90m 50s (remain 417m 3s) Loss: 0.0121(0.0025) Grad: 41781.4141  LR: 0.000018  \n",
      "Epoch: [1][6700/36908] Elapsed 92m 13s (remain 415m 44s) Loss: 0.0002(0.0026) Grad: 319.2743  LR: 0.000018  \n",
      "Epoch: [1][6800/36908] Elapsed 93m 36s (remain 414m 22s) Loss: 0.0003(0.0026) Grad: 96.2476  LR: 0.000018  \n",
      "Epoch: [1][6900/36908] Elapsed 94m 58s (remain 412m 56s) Loss: 0.0000(0.0026) Grad: 19.6676  LR: 0.000018  \n",
      "Epoch: [1][7000/36908] Elapsed 96m 19s (remain 411m 27s) Loss: 0.0009(0.0026) Grad: 1301.6243  LR: 0.000018  \n",
      "Epoch: [1][7100/36908] Elapsed 97m 42s (remain 410m 8s) Loss: 0.0005(0.0026) Grad: 5714.3804  LR: 0.000018  \n",
      "Epoch: [1][7200/36908] Elapsed 99m 4s (remain 408m 44s) Loss: 0.0016(0.0026) Grad: 1691.7089  LR: 0.000018  \n",
      "Epoch: [1][7300/36908] Elapsed 100m 26s (remain 407m 19s) Loss: 0.0037(0.0026) Grad: 13541.9180  LR: 0.000018  \n",
      "Epoch: [1][7400/36908] Elapsed 101m 49s (remain 405m 58s) Loss: 0.0019(0.0026) Grad: 10992.1084  LR: 0.000018  \n",
      "Epoch: [1][7500/36908] Elapsed 103m 11s (remain 404m 33s) Loss: 0.0001(0.0026) Grad: 308.0025  LR: 0.000018  \n",
      "Epoch: [1][7600/36908] Elapsed 104m 34s (remain 403m 12s) Loss: 0.0008(0.0026) Grad: 1853.3915  LR: 0.000018  \n",
      "Epoch: [1][7700/36908] Elapsed 105m 56s (remain 401m 49s) Loss: 0.0012(0.0026) Grad: 1863.5917  LR: 0.000018  \n",
      "Epoch: [1][7800/36908] Elapsed 107m 21s (remain 400m 33s) Loss: 0.0001(0.0026) Grad: 249.0692  LR: 0.000018  \n",
      "Epoch: [1][7900/36908] Elapsed 108m 44s (remain 399m 14s) Loss: 0.0013(0.0026) Grad: 4687.9385  LR: 0.000017  \n",
      "Epoch: [1][8000/36908] Elapsed 110m 7s (remain 397m 51s) Loss: 0.0163(0.0026) Grad: 115601.5547  LR: 0.000017  \n",
      "Epoch: [1][8100/36908] Elapsed 111m 30s (remain 396m 30s) Loss: 0.0003(0.0026) Grad: 53.1608  LR: 0.000017  \n",
      "Epoch: [1][8200/36908] Elapsed 112m 53s (remain 395m 11s) Loss: 0.0008(0.0026) Grad: 4317.6709  LR: 0.000017  \n",
      "Epoch: [1][8300/36908] Elapsed 114m 15s (remain 393m 45s) Loss: 0.0001(0.0026) Grad: 970.4922  LR: 0.000017  \n",
      "Epoch: [1][8400/36908] Elapsed 115m 38s (remain 392m 25s) Loss: 0.0007(0.0026) Grad: 2770.7532  LR: 0.000017  \n",
      "Epoch: [1][8500/36908] Elapsed 117m 1s (remain 391m 4s) Loss: 0.0055(0.0026) Grad: 3426.0322  LR: 0.000017  \n",
      "Epoch: [1][8600/36908] Elapsed 118m 25s (remain 389m 44s) Loss: 0.0047(0.0026) Grad: 1608.9093  LR: 0.000017  \n",
      "Epoch: [1][8700/36908] Elapsed 119m 48s (remain 388m 23s) Loss: 0.0004(0.0026) Grad: 140.3726  LR: 0.000017  \n",
      "Epoch: [1][8800/36908] Elapsed 121m 10s (remain 386m 58s) Loss: 0.0001(0.0026) Grad: 732.3488  LR: 0.000017  \n",
      "Epoch: [1][8900/36908] Elapsed 122m 31s (remain 385m 32s) Loss: 0.0000(0.0026) Grad: 19.8699  LR: 0.000017  \n",
      "Epoch: [1][9000/36908] Elapsed 123m 54s (remain 384m 11s) Loss: 0.0027(0.0026) Grad: 5119.2031  LR: 0.000017  \n",
      "Epoch: [1][9100/36908] Elapsed 125m 17s (remain 382m 49s) Loss: 0.0006(0.0026) Grad: 862.9128  LR: 0.000017  \n",
      "Epoch: [1][9200/36908] Elapsed 126m 40s (remain 381m 27s) Loss: 0.0014(0.0026) Grad: 4779.7402  LR: 0.000017  \n",
      "Epoch: [1][9300/36908] Elapsed 128m 2s (remain 380m 3s) Loss: 0.0070(0.0026) Grad: 14423.9580  LR: 0.000017  \n",
      "Epoch: [1][9400/36908] Elapsed 129m 26s (remain 378m 43s) Loss: 0.0034(0.0026) Grad: 7088.4678  LR: 0.000017  \n",
      "Epoch: [1][9500/36908] Elapsed 130m 49s (remain 377m 21s) Loss: 0.0048(0.0026) Grad: 34157.0117  LR: 0.000017  \n",
      "Epoch: [1][9600/36908] Elapsed 132m 11s (remain 375m 59s) Loss: 0.0001(0.0026) Grad: 44.0341  LR: 0.000016  \n",
      "Epoch: [1][9700/36908] Elapsed 133m 33s (remain 374m 33s) Loss: 0.0019(0.0026) Grad: 11356.1182  LR: 0.000016  \n",
      "Epoch: [1][9800/36908] Elapsed 134m 55s (remain 373m 9s) Loss: 0.0098(0.0026) Grad: 13296.4150  LR: 0.000016  \n",
      "Epoch: [1][9900/36908] Elapsed 136m 18s (remain 371m 47s) Loss: 0.0064(0.0026) Grad: 7744.0679  LR: 0.000016  \n",
      "Epoch: [1][10000/36908] Elapsed 137m 39s (remain 370m 22s) Loss: 0.0029(0.0026) Grad: 3782.1968  LR: 0.000016  \n",
      "Epoch: [1][10100/36908] Elapsed 139m 1s (remain 368m 57s) Loss: 0.0017(0.0026) Grad: 5219.0020  LR: 0.000016  \n",
      "Epoch: [1][10200/36908] Elapsed 140m 23s (remain 367m 33s) Loss: 0.0068(0.0026) Grad: 11434.6816  LR: 0.000016  \n",
      "Epoch: [1][10300/36908] Elapsed 141m 46s (remain 366m 13s) Loss: 0.0001(0.0026) Grad: 15.1567  LR: 0.000016  \n",
      "Epoch: [1][10400/36908] Elapsed 143m 10s (remain 364m 52s) Loss: 0.0062(0.0026) Grad: 29709.3438  LR: 0.000016  \n",
      "Epoch: [1][10500/36908] Elapsed 144m 32s (remain 363m 28s) Loss: 0.0001(0.0026) Grad: 55.3991  LR: 0.000016  \n",
      "Epoch: [1][10600/36908] Elapsed 145m 55s (remain 362m 6s) Loss: 0.0116(0.0026) Grad: 29724.5410  LR: 0.000016  \n",
      "Epoch: [1][10700/36908] Elapsed 147m 17s (remain 360m 43s) Loss: 0.0017(0.0026) Grad: 1914.4055  LR: 0.000016  \n",
      "Epoch: [1][10800/36908] Elapsed 148m 39s (remain 359m 19s) Loss: 0.0047(0.0026) Grad: 12164.7334  LR: 0.000016  \n",
      "Epoch: [1][10900/36908] Elapsed 150m 2s (remain 357m 56s) Loss: 0.0011(0.0026) Grad: 7502.2344  LR: 0.000016  \n",
      "Epoch: [1][11000/36908] Elapsed 151m 23s (remain 356m 31s) Loss: 0.0017(0.0026) Grad: 5948.4814  LR: 0.000016  \n",
      "Epoch: [1][11100/36908] Elapsed 152m 46s (remain 355m 9s) Loss: 0.0029(0.0026) Grad: 11610.9932  LR: 0.000016  \n",
      "Epoch: [1][11200/36908] Elapsed 154m 8s (remain 353m 46s) Loss: 0.0008(0.0026) Grad: 598.4554  LR: 0.000015  \n",
      "Epoch: [1][11300/36908] Elapsed 155m 32s (remain 352m 25s) Loss: 0.0009(0.0026) Grad: 4686.9209  LR: 0.000015  \n",
      "Epoch: [1][11400/36908] Elapsed 156m 54s (remain 351m 1s) Loss: 0.0012(0.0026) Grad: 930.0923  LR: 0.000015  \n",
      "Epoch: [1][11500/36908] Elapsed 158m 15s (remain 349m 37s) Loss: 0.0076(0.0026) Grad: 51001.3242  LR: 0.000015  \n",
      "Epoch: [1][11600/36908] Elapsed 159m 38s (remain 348m 16s) Loss: 0.0002(0.0026) Grad: 1566.1484  LR: 0.000015  \n",
      "Epoch: [1][11700/36908] Elapsed 161m 1s (remain 346m 52s) Loss: 0.0001(0.0026) Grad: 52.5108  LR: 0.000015  \n",
      "Epoch: [1][11800/36908] Elapsed 162m 24s (remain 345m 31s) Loss: 0.0041(0.0026) Grad: 13489.1953  LR: 0.000015  \n",
      "Epoch: [1][11900/36908] Elapsed 163m 46s (remain 344m 8s) Loss: 0.0001(0.0026) Grad: 303.9745  LR: 0.000015  \n",
      "Epoch: [1][12000/36908] Elapsed 165m 9s (remain 342m 46s) Loss: 0.0041(0.0026) Grad: 13877.1475  LR: 0.000015  \n",
      "Epoch: [1][12100/36908] Elapsed 166m 31s (remain 341m 22s) Loss: 0.0000(0.0026) Grad: 60.6811  LR: 0.000015  \n",
      "Epoch: [1][12200/36908] Elapsed 167m 53s (remain 339m 57s) Loss: 0.0095(0.0026) Grad: 23166.8477  LR: 0.000015  \n",
      "Epoch: [1][12300/36908] Elapsed 169m 15s (remain 338m 34s) Loss: 0.0001(0.0026) Grad: 37.6815  LR: 0.000015  \n",
      "Epoch: [1][12400/36908] Elapsed 170m 37s (remain 337m 10s) Loss: 0.0019(0.0026) Grad: 9724.0361  LR: 0.000015  \n",
      "Epoch: [1][12500/36908] Elapsed 172m 0s (remain 335m 49s) Loss: 0.0025(0.0026) Grad: 23297.6230  LR: 0.000015  \n",
      "Epoch: [1][12600/36908] Elapsed 173m 23s (remain 334m 27s) Loss: 0.0004(0.0026) Grad: 237.2044  LR: 0.000015  \n",
      "Epoch: [1][12700/36908] Elapsed 174m 46s (remain 333m 5s) Loss: 0.0001(0.0026) Grad: 33.9214  LR: 0.000015  \n",
      "Epoch: [1][12800/36908] Elapsed 176m 8s (remain 331m 43s) Loss: 0.0001(0.0026) Grad: 195.3278  LR: 0.000015  \n",
      "Epoch: [1][12900/36908] Elapsed 177m 32s (remain 330m 22s) Loss: 0.0032(0.0026) Grad: 22814.2539  LR: 0.000014  \n",
      "Epoch: [1][13000/36908] Elapsed 178m 54s (remain 328m 59s) Loss: 0.0054(0.0026) Grad: 36907.7656  LR: 0.000014  \n",
      "Epoch: [1][13100/36908] Elapsed 180m 17s (remain 327m 37s) Loss: 0.0020(0.0026) Grad: 4869.8228  LR: 0.000014  \n",
      "Epoch: [1][13200/36908] Elapsed 181m 39s (remain 326m 14s) Loss: 0.0001(0.0026) Grad: 57.8372  LR: 0.000014  \n",
      "Epoch: [1][13300/36908] Elapsed 183m 2s (remain 324m 51s) Loss: 0.0001(0.0026) Grad: 270.7069  LR: 0.000014  \n",
      "Epoch: [1][13400/36908] Elapsed 184m 23s (remain 323m 27s) Loss: 0.0001(0.0026) Grad: 29.6052  LR: 0.000014  \n",
      "Epoch: [1][13500/36908] Elapsed 185m 45s (remain 322m 2s) Loss: 0.0000(0.0026) Grad: 4.7375  LR: 0.000014  \n",
      "Epoch: [1][13600/36908] Elapsed 187m 7s (remain 320m 39s) Loss: 0.0003(0.0026) Grad: 11609.1963  LR: 0.000014  \n",
      "Epoch: [1][13700/36908] Elapsed 188m 30s (remain 319m 17s) Loss: 0.0008(0.0026) Grad: 16936.5410  LR: 0.000014  \n",
      "Epoch: [1][13800/36908] Elapsed 189m 53s (remain 317m 55s) Loss: 0.0010(0.0026) Grad: 2888.0454  LR: 0.000014  \n",
      "Epoch: [1][13900/36908] Elapsed 191m 15s (remain 316m 32s) Loss: 0.0003(0.0026) Grad: 1420.2076  LR: 0.000014  \n",
      "Epoch: [1][14000/36908] Elapsed 192m 37s (remain 315m 9s) Loss: 0.0010(0.0026) Grad: 1883.9904  LR: 0.000014  \n",
      "Epoch: [1][14100/36908] Elapsed 193m 59s (remain 313m 46s) Loss: 0.0039(0.0026) Grad: 42086.1133  LR: 0.000014  \n",
      "Epoch: [1][14200/36908] Elapsed 195m 22s (remain 312m 23s) Loss: 0.0003(0.0026) Grad: 2578.4692  LR: 0.000014  \n",
      "Epoch: [1][14300/36908] Elapsed 196m 45s (remain 311m 1s) Loss: 0.0131(0.0026) Grad: 93278.0625  LR: 0.000014  \n",
      "Epoch: [1][14400/36908] Elapsed 198m 7s (remain 309m 38s) Loss: 0.0016(0.0026) Grad: 6579.2803  LR: 0.000014  \n",
      "Epoch: [1][14500/36908] Elapsed 199m 29s (remain 308m 15s) Loss: 0.0004(0.0026) Grad: 1306.7010  LR: 0.000013  \n",
      "Epoch: [1][14600/36908] Elapsed 200m 52s (remain 306m 54s) Loss: 0.0053(0.0026) Grad: 21750.2617  LR: 0.000013  \n",
      "Epoch: [1][14700/36908] Elapsed 202m 16s (remain 305m 32s) Loss: 0.0011(0.0026) Grad: 3222.9988  LR: 0.000013  \n",
      "Epoch: [1][14800/36908] Elapsed 203m 38s (remain 304m 9s) Loss: 0.0005(0.0026) Grad: 435.7305  LR: 0.000013  \n",
      "Epoch: [1][14900/36908] Elapsed 204m 59s (remain 302m 45s) Loss: 0.0045(0.0026) Grad: 20496.6426  LR: 0.000013  \n",
      "Epoch: [1][15000/36908] Elapsed 206m 22s (remain 301m 22s) Loss: 0.0001(0.0026) Grad: 60.0864  LR: 0.000013  \n",
      "Epoch: [1][15100/36908] Elapsed 207m 46s (remain 300m 2s) Loss: 0.0098(0.0026) Grad: 70883.8359  LR: 0.000013  \n",
      "Epoch: [1][15200/36908] Elapsed 209m 9s (remain 298m 40s) Loss: 0.0000(0.0026) Grad: 6.9465  LR: 0.000013  \n",
      "Epoch: [1][15300/36908] Elapsed 210m 31s (remain 297m 16s) Loss: 0.0001(0.0026) Grad: 90.1594  LR: 0.000013  \n",
      "Epoch: [1][15400/36908] Elapsed 211m 53s (remain 295m 53s) Loss: 0.0004(0.0026) Grad: 230.5520  LR: 0.000013  \n",
      "Epoch: [1][15500/36908] Elapsed 213m 15s (remain 294m 30s) Loss: 0.0012(0.0026) Grad: 15569.4268  LR: 0.000013  \n",
      "Epoch: [1][15600/36908] Elapsed 214m 37s (remain 293m 7s) Loss: 0.0000(0.0026) Grad: 24.1230  LR: 0.000013  \n",
      "Epoch: [1][15700/36908] Elapsed 216m 0s (remain 291m 44s) Loss: 0.0017(0.0026) Grad: 6665.1147  LR: 0.000013  \n",
      "Epoch: [1][15800/36908] Elapsed 217m 22s (remain 290m 22s) Loss: 0.0016(0.0026) Grad: 9818.8477  LR: 0.000013  \n",
      "Epoch: [1][15900/36908] Elapsed 218m 44s (remain 288m 59s) Loss: 0.0003(0.0026) Grad: 994.9827  LR: 0.000013  \n",
      "Epoch: [1][16000/36908] Elapsed 220m 6s (remain 287m 35s) Loss: 0.0001(0.0026) Grad: 47.4590  LR: 0.000013  \n",
      "Epoch: [1][16100/36908] Elapsed 221m 29s (remain 286m 14s) Loss: 0.0061(0.0026) Grad: 31688.0273  LR: 0.000013  \n",
      "Epoch: [1][16200/36908] Elapsed 222m 52s (remain 284m 51s) Loss: 0.0015(0.0026) Grad: 15750.4268  LR: 0.000012  \n",
      "Epoch: [1][16300/36908] Elapsed 224m 15s (remain 283m 29s) Loss: 0.0014(0.0026) Grad: 7202.8418  LR: 0.000012  \n",
      "Epoch: [1][16400/36908] Elapsed 225m 37s (remain 282m 7s) Loss: 0.0004(0.0026) Grad: 152.8703  LR: 0.000012  \n",
      "Epoch: [1][16500/36908] Elapsed 227m 0s (remain 280m 44s) Loss: 0.0002(0.0026) Grad: 280.9502  LR: 0.000012  \n",
      "Epoch: [1][16600/36908] Elapsed 228m 23s (remain 279m 22s) Loss: 0.0075(0.0026) Grad: 22727.3047  LR: 0.000012  \n",
      "Epoch: [1][16700/36908] Elapsed 229m 45s (remain 277m 59s) Loss: 0.0033(0.0026) Grad: 58339.0586  LR: 0.000012  \n",
      "Epoch: [1][16800/36908] Elapsed 231m 8s (remain 276m 37s) Loss: 0.0102(0.0026) Grad: 67031.8594  LR: 0.000012  \n",
      "Epoch: [1][16900/36908] Elapsed 232m 31s (remain 275m 15s) Loss: 0.0018(0.0026) Grad: 8959.1885  LR: 0.000012  \n",
      "Epoch: [1][17000/36908] Elapsed 233m 53s (remain 273m 51s) Loss: 0.0007(0.0026) Grad: 18515.6543  LR: 0.000012  \n",
      "Epoch: [1][17100/36908] Elapsed 235m 16s (remain 272m 29s) Loss: 0.0012(0.0026) Grad: 12066.5820  LR: 0.000012  \n",
      "Epoch: [1][17200/36908] Elapsed 236m 38s (remain 271m 7s) Loss: 0.0014(0.0026) Grad: 29421.3750  LR: 0.000012  \n",
      "Epoch: [1][17300/36908] Elapsed 238m 1s (remain 269m 44s) Loss: 0.0001(0.0026) Grad: 50.9493  LR: 0.000012  \n",
      "Epoch: [1][17400/36908] Elapsed 239m 24s (remain 268m 23s) Loss: 0.0007(0.0026) Grad: 9067.2490  LR: 0.000012  \n",
      "Epoch: [1][17500/36908] Elapsed 240m 47s (remain 267m 0s) Loss: 0.0022(0.0026) Grad: 25642.1602  LR: 0.000012  \n",
      "Epoch: [1][17600/36908] Elapsed 242m 9s (remain 265m 37s) Loss: 0.0004(0.0026) Grad: 2451.5884  LR: 0.000012  \n",
      "Epoch: [1][17700/36908] Elapsed 243m 31s (remain 264m 14s) Loss: 0.0026(0.0026) Grad: 37631.4141  LR: 0.000012  \n",
      "Epoch: [1][17800/36908] Elapsed 244m 54s (remain 262m 52s) Loss: 0.0021(0.0026) Grad: 15954.9199  LR: 0.000012  \n",
      "Epoch: [1][17900/36908] Elapsed 246m 17s (remain 261m 30s) Loss: 0.0003(0.0026) Grad: 616.4699  LR: 0.000011  \n",
      "Epoch: [1][18000/36908] Elapsed 247m 39s (remain 260m 7s) Loss: 0.0027(0.0026) Grad: 16748.6113  LR: 0.000011  \n",
      "Epoch: [1][18100/36908] Elapsed 249m 2s (remain 258m 44s) Loss: 0.0002(0.0026) Grad: 176.0329  LR: 0.000011  \n",
      "Epoch: [1][18200/36908] Elapsed 250m 24s (remain 257m 21s) Loss: 0.0026(0.0026) Grad: 7737.4448  LR: 0.000011  \n",
      "Epoch: [1][18300/36908] Elapsed 251m 46s (remain 255m 58s) Loss: 0.0045(0.0026) Grad: 16099.3721  LR: 0.000011  \n",
      "Epoch: [1][18400/36908] Elapsed 253m 8s (remain 254m 35s) Loss: 0.0002(0.0026) Grad: 3039.9417  LR: 0.000011  \n",
      "Epoch: [1][18500/36908] Elapsed 254m 32s (remain 253m 14s) Loss: 0.0000(0.0026) Grad: 11.7694  LR: 0.000011  \n",
      "Epoch: [1][18600/36908] Elapsed 255m 55s (remain 251m 53s) Loss: 0.0030(0.0026) Grad: 24868.3789  LR: 0.000011  \n",
      "Epoch: [1][18700/36908] Elapsed 257m 18s (remain 250m 30s) Loss: 0.0019(0.0026) Grad: 4319.9888  LR: 0.000011  \n",
      "Epoch: [1][18800/36908] Elapsed 258m 41s (remain 249m 8s) Loss: 0.0005(0.0026) Grad: 7911.9438  LR: 0.000011  \n",
      "Epoch: [1][18900/36908] Elapsed 260m 4s (remain 247m 46s) Loss: 0.0000(0.0026) Grad: 29.2250  LR: 0.000011  \n",
      "Epoch: [1][19000/36908] Elapsed 261m 27s (remain 246m 24s) Loss: 0.0004(0.0026) Grad: 1856.6613  LR: 0.000011  \n",
      "Epoch: [1][19100/36908] Elapsed 262m 49s (remain 245m 0s) Loss: 0.0032(0.0026) Grad: 12906.9365  LR: 0.000011  \n",
      "Epoch: [1][19200/36908] Elapsed 264m 10s (remain 243m 37s) Loss: 0.0039(0.0026) Grad: 43303.6250  LR: 0.000011  \n",
      "Epoch: [1][19300/36908] Elapsed 265m 34s (remain 242m 16s) Loss: 0.0041(0.0026) Grad: 27119.7148  LR: 0.000011  \n",
      "Epoch: [1][19400/36908] Elapsed 266m 57s (remain 240m 53s) Loss: 0.0009(0.0026) Grad: 3680.6470  LR: 0.000011  \n",
      "Epoch: [1][19500/36908] Elapsed 268m 20s (remain 239m 31s) Loss: 0.0000(0.0026) Grad: 9.4325  LR: 0.000010  \n",
      "Epoch: [1][19600/36908] Elapsed 269m 43s (remain 238m 9s) Loss: 0.0016(0.0026) Grad: 4705.6460  LR: 0.000010  \n",
      "Epoch: [1][19700/36908] Elapsed 271m 6s (remain 236m 47s) Loss: 0.0020(0.0026) Grad: 8930.4180  LR: 0.000010  \n",
      "Epoch: [1][19800/36908] Elapsed 272m 28s (remain 235m 24s) Loss: 0.0066(0.0026) Grad: 17971.6484  LR: 0.000010  \n",
      "Epoch: [1][19900/36908] Elapsed 273m 52s (remain 234m 3s) Loss: 0.0000(0.0026) Grad: 24.0444  LR: 0.000010  \n",
      "Epoch: [1][20000/36908] Elapsed 275m 16s (remain 232m 41s) Loss: 0.0009(0.0026) Grad: 905.4108  LR: 0.000010  \n",
      "Epoch: [1][20100/36908] Elapsed 276m 38s (remain 231m 18s) Loss: 0.0002(0.0026) Grad: 155.5676  LR: 0.000010  \n",
      "Epoch: [1][20200/36908] Elapsed 278m 1s (remain 229m 56s) Loss: 0.0035(0.0026) Grad: 18070.8223  LR: 0.000010  \n",
      "Epoch: [1][20300/36908] Elapsed 279m 24s (remain 228m 34s) Loss: 0.0011(0.0026) Grad: 1578.1283  LR: 0.000010  \n",
      "Epoch: [1][20400/36908] Elapsed 280m 46s (remain 227m 10s) Loss: 0.0590(0.0026) Grad: 249623.9844  LR: 0.000010  \n",
      "Epoch: [1][20500/36908] Elapsed 282m 8s (remain 225m 47s) Loss: 0.0031(0.0026) Grad: 17920.8555  LR: 0.000010  \n",
      "Epoch: [1][20600/36908] Elapsed 283m 32s (remain 224m 26s) Loss: 0.0020(0.0026) Grad: 26825.3828  LR: 0.000010  \n",
      "Epoch: [1][20700/36908] Elapsed 284m 55s (remain 223m 3s) Loss: 0.0004(0.0026) Grad: 693.2268  LR: 0.000010  \n",
      "Epoch: [1][20800/36908] Elapsed 286m 17s (remain 221m 41s) Loss: 0.0015(0.0026) Grad: 4326.5991  LR: 0.000010  \n",
      "Epoch: [1][20900/36908] Elapsed 287m 41s (remain 220m 19s) Loss: 0.0001(0.0026) Grad: 135.6313  LR: 0.000010  \n",
      "Epoch: [1][21000/36908] Elapsed 289m 4s (remain 218m 57s) Loss: 0.0079(0.0026) Grad: 39667.8594  LR: 0.000010  \n",
      "Epoch: [1][21100/36908] Elapsed 290m 26s (remain 217m 34s) Loss: 0.0011(0.0026) Grad: 14580.7217  LR: 0.000010  \n",
      "Epoch: [1][21200/36908] Elapsed 291m 49s (remain 216m 11s) Loss: 0.0001(0.0026) Grad: 111.6655  LR: 0.000009  \n",
      "Epoch: [1][21300/36908] Elapsed 293m 11s (remain 214m 48s) Loss: 0.0003(0.0026) Grad: 2300.6304  LR: 0.000009  \n",
      "Epoch: [1][21400/36908] Elapsed 294m 32s (remain 213m 25s) Loss: 0.0018(0.0026) Grad: 7307.1084  LR: 0.000009  \n",
      "Epoch: [1][21500/36908] Elapsed 295m 56s (remain 212m 3s) Loss: 0.0009(0.0026) Grad: 329.5803  LR: 0.000009  \n",
      "Epoch: [1][21600/36908] Elapsed 297m 19s (remain 210m 41s) Loss: 0.0018(0.0026) Grad: 9439.1943  LR: 0.000009  \n",
      "Epoch: [1][21700/36908] Elapsed 298m 40s (remain 209m 18s) Loss: 0.0087(0.0026) Grad: 75385.2656  LR: 0.000009  \n",
      "Epoch: [1][21800/36908] Elapsed 300m 3s (remain 207m 55s) Loss: 0.0000(0.0026) Grad: 23.3693  LR: 0.000009  \n",
      "Epoch: [1][21900/36908] Elapsed 301m 27s (remain 206m 33s) Loss: 0.0058(0.0026) Grad: 129748.8203  LR: 0.000009  \n",
      "Epoch: [1][22000/36908] Elapsed 302m 50s (remain 205m 11s) Loss: 0.0006(0.0026) Grad: 15421.1133  LR: 0.000009  \n",
      "Epoch: [1][22100/36908] Elapsed 304m 12s (remain 203m 48s) Loss: 0.0011(0.0026) Grad: 1724.6373  LR: 0.000009  \n",
      "Epoch: [1][22200/36908] Elapsed 305m 33s (remain 202m 25s) Loss: 0.0017(0.0026) Grad: 12567.4004  LR: 0.000009  \n",
      "Epoch: [1][22300/36908] Elapsed 306m 57s (remain 201m 3s) Loss: 0.0032(0.0026) Grad: 70210.1328  LR: 0.000009  \n",
      "Epoch: [1][22400/36908] Elapsed 308m 20s (remain 199m 40s) Loss: 0.0001(0.0026) Grad: 128.7573  LR: 0.000009  \n",
      "Epoch: [1][22500/36908] Elapsed 309m 42s (remain 198m 17s) Loss: 0.0028(0.0026) Grad: 37934.4336  LR: 0.000009  \n",
      "Epoch: [1][22600/36908] Elapsed 311m 4s (remain 196m 54s) Loss: 0.0027(0.0026) Grad: 24487.5996  LR: 0.000009  \n",
      "Epoch: [1][22700/36908] Elapsed 312m 26s (remain 195m 32s) Loss: 0.0023(0.0026) Grad: 59802.9023  LR: 0.000009  \n",
      "Epoch: [1][22800/36908] Elapsed 313m 49s (remain 194m 9s) Loss: 0.0018(0.0026) Grad: 15829.9375  LR: 0.000008  \n",
      "Epoch: [1][22900/36908] Elapsed 315m 12s (remain 192m 47s) Loss: 0.0002(0.0026) Grad: 206.0813  LR: 0.000008  \n",
      "Epoch: [1][23000/36908] Elapsed 316m 34s (remain 191m 24s) Loss: 0.0002(0.0026) Grad: 303.2154  LR: 0.000008  \n",
      "Epoch: [1][23100/36908] Elapsed 317m 57s (remain 190m 2s) Loss: 0.0001(0.0026) Grad: 98.4428  LR: 0.000008  \n",
      "Epoch: [1][23200/36908] Elapsed 319m 21s (remain 188m 40s) Loss: 0.0001(0.0026) Grad: 911.0356  LR: 0.000008  \n",
      "Epoch: [1][23300/36908] Elapsed 320m 44s (remain 187m 18s) Loss: 0.0016(0.0026) Grad: 19371.0820  LR: 0.000008  \n",
      "Epoch: [1][23400/36908] Elapsed 322m 6s (remain 185m 55s) Loss: 0.0001(0.0026) Grad: 34.9521  LR: 0.000008  \n",
      "Epoch: [1][23500/36908] Elapsed 323m 29s (remain 184m 32s) Loss: 0.0005(0.0026) Grad: 12409.0654  LR: 0.000008  \n",
      "Epoch: [1][23600/36908] Elapsed 324m 52s (remain 183m 10s) Loss: 0.0050(0.0026) Grad: 39534.1484  LR: 0.000008  \n",
      "Epoch: [1][23700/36908] Elapsed 326m 16s (remain 181m 48s) Loss: 0.0018(0.0026) Grad: 9414.3516  LR: 0.000008  \n",
      "Epoch: [1][23800/36908] Elapsed 327m 39s (remain 180m 26s) Loss: 0.0006(0.0025) Grad: 4866.2783  LR: 0.000008  \n",
      "Epoch: [1][23900/36908] Elapsed 329m 1s (remain 179m 3s) Loss: 0.0002(0.0025) Grad: 152.7169  LR: 0.000008  \n",
      "Epoch: [1][24000/36908] Elapsed 330m 25s (remain 177m 41s) Loss: 0.0001(0.0025) Grad: 63.9062  LR: 0.000008  \n",
      "Epoch: [1][24100/36908] Elapsed 331m 47s (remain 176m 18s) Loss: 0.0027(0.0025) Grad: 33195.0078  LR: 0.000008  \n",
      "Epoch: [1][24200/36908] Elapsed 333m 10s (remain 174m 56s) Loss: 0.0080(0.0025) Grad: 108766.7734  LR: 0.000008  \n",
      "Epoch: [1][24300/36908] Elapsed 334m 32s (remain 173m 33s) Loss: 0.0001(0.0025) Grad: 767.4568  LR: 0.000008  \n",
      "Epoch: [1][24400/36908] Elapsed 335m 54s (remain 172m 10s) Loss: 0.0005(0.0026) Grad: 3764.5254  LR: 0.000008  \n",
      "Epoch: [1][24500/36908] Elapsed 337m 16s (remain 170m 47s) Loss: 0.0001(0.0025) Grad: 339.9417  LR: 0.000007  \n",
      "Epoch: [1][24600/36908] Elapsed 338m 38s (remain 169m 24s) Loss: 0.0033(0.0025) Grad: 12569.4062  LR: 0.000007  \n",
      "Epoch: [1][24700/36908] Elapsed 340m 1s (remain 168m 2s) Loss: 0.0131(0.0025) Grad: 153239.2969  LR: 0.000007  \n",
      "Epoch: [1][24800/36908] Elapsed 341m 23s (remain 166m 39s) Loss: 0.0032(0.0025) Grad: 9284.6006  LR: 0.000007  \n",
      "Epoch: [1][24900/36908] Elapsed 342m 45s (remain 165m 16s) Loss: 0.0000(0.0026) Grad: 194.4125  LR: 0.000007  \n",
      "Epoch: [1][25000/36908] Elapsed 344m 8s (remain 163m 54s) Loss: 0.0000(0.0026) Grad: 667.7969  LR: 0.000007  \n",
      "Epoch: [1][25100/36908] Elapsed 345m 30s (remain 162m 31s) Loss: 0.0012(0.0026) Grad: 4388.9829  LR: 0.000007  \n",
      "Epoch: [1][25200/36908] Elapsed 346m 53s (remain 161m 8s) Loss: 0.0251(0.0026) Grad: 277673.5625  LR: 0.000007  \n",
      "Epoch: [1][25300/36908] Elapsed 348m 17s (remain 159m 47s) Loss: 0.0028(0.0026) Grad: 10969.4609  LR: 0.000007  \n",
      "Epoch: [1][25400/36908] Elapsed 349m 40s (remain 158m 24s) Loss: 0.0038(0.0026) Grad: 18441.7734  LR: 0.000007  \n",
      "Epoch: [1][25500/36908] Elapsed 351m 3s (remain 157m 2s) Loss: 0.0009(0.0026) Grad: 5852.1636  LR: 0.000007  \n",
      "Epoch: [1][25600/36908] Elapsed 352m 25s (remain 155m 39s) Loss: 0.0011(0.0026) Grad: 11854.7002  LR: 0.000007  \n",
      "Epoch: [1][25700/36908] Elapsed 353m 48s (remain 154m 16s) Loss: 0.0001(0.0025) Grad: 198.5795  LR: 0.000007  \n",
      "Epoch: [1][25800/36908] Elapsed 355m 11s (remain 152m 54s) Loss: 0.0005(0.0025) Grad: 1391.7690  LR: 0.000007  \n",
      "Epoch: [1][25900/36908] Elapsed 356m 36s (remain 151m 32s) Loss: 0.0004(0.0025) Grad: 14501.7236  LR: 0.000007  \n",
      "Epoch: [1][26000/36908] Elapsed 357m 59s (remain 150m 10s) Loss: 0.0013(0.0025) Grad: 54813.5859  LR: 0.000007  \n",
      "Epoch: [1][26100/36908] Elapsed 359m 21s (remain 148m 47s) Loss: 0.0015(0.0025) Grad: 48334.9492  LR: 0.000007  \n",
      "Epoch: [1][26200/36908] Elapsed 360m 44s (remain 147m 24s) Loss: 0.0001(0.0025) Grad: 358.8010  LR: 0.000006  \n",
      "Epoch: [1][26300/36908] Elapsed 362m 8s (remain 146m 2s) Loss: 0.0022(0.0025) Grad: 47123.6719  LR: 0.000006  \n",
      "Epoch: [1][26400/36908] Elapsed 363m 30s (remain 144m 39s) Loss: 0.0034(0.0025) Grad: 43028.9609  LR: 0.000006  \n",
      "Epoch: [1][26500/36908] Elapsed 364m 52s (remain 143m 17s) Loss: 0.0011(0.0025) Grad: 50780.3242  LR: 0.000006  \n",
      "Epoch: [1][26600/36908] Elapsed 366m 16s (remain 141m 55s) Loss: 0.0026(0.0025) Grad: 22747.3535  LR: 0.000006  \n",
      "Epoch: [1][26700/36908] Elapsed 367m 39s (remain 140m 32s) Loss: 0.0016(0.0025) Grad: 74966.9766  LR: 0.000006  \n",
      "Epoch: [1][26800/36908] Elapsed 369m 1s (remain 139m 9s) Loss: 0.0065(0.0025) Grad: 158157.0156  LR: 0.000006  \n",
      "Epoch: [1][26900/36908] Elapsed 370m 24s (remain 137m 47s) Loss: 0.0099(0.0025) Grad: 97476.1250  LR: 0.000006  \n",
      "Epoch: [1][27000/36908] Elapsed 371m 48s (remain 136m 25s) Loss: 0.0009(0.0025) Grad: 939.9651  LR: 0.000006  \n",
      "Epoch: [1][27100/36908] Elapsed 373m 11s (remain 135m 2s) Loss: 0.0006(0.0025) Grad: 4810.5396  LR: 0.000006  \n",
      "Epoch: [1][27200/36908] Elapsed 374m 33s (remain 133m 40s) Loss: 0.0062(0.0025) Grad: 88147.7188  LR: 0.000006  \n",
      "Epoch: [1][27300/36908] Elapsed 375m 57s (remain 132m 17s) Loss: 0.0001(0.0025) Grad: 8027.5605  LR: 0.000006  \n",
      "Epoch: [1][27400/36908] Elapsed 377m 19s (remain 130m 54s) Loss: 0.0084(0.0025) Grad: 375712.0000  LR: 0.000006  \n",
      "Epoch: [1][27500/36908] Elapsed 378m 41s (remain 129m 32s) Loss: 0.0004(0.0025) Grad: 14139.8047  LR: 0.000006  \n",
      "Epoch: [1][27600/36908] Elapsed 380m 4s (remain 128m 9s) Loss: 0.0322(0.0025) Grad: 388053.0312  LR: 0.000006  \n",
      "Epoch: [1][27700/36908] Elapsed 381m 27s (remain 126m 47s) Loss: 0.0117(0.0025) Grad: 198187.4844  LR: 0.000006  \n",
      "Epoch: [1][27800/36908] Elapsed 382m 49s (remain 125m 24s) Loss: 0.0014(0.0025) Grad: 56081.7500  LR: 0.000005  \n",
      "Epoch: [1][27900/36908] Elapsed 384m 12s (remain 124m 1s) Loss: 0.0005(0.0025) Grad: 14957.1582  LR: 0.000005  \n",
      "Epoch: [1][28000/36908] Elapsed 385m 34s (remain 122m 39s) Loss: 0.0003(0.0025) Grad: 2139.1838  LR: 0.000005  \n",
      "Epoch: [1][28100/36908] Elapsed 386m 56s (remain 121m 16s) Loss: 0.0005(0.0025) Grad: 5703.4282  LR: 0.000005  \n",
      "Epoch: [1][28200/36908] Elapsed 388m 18s (remain 119m 53s) Loss: 0.0046(0.0025) Grad: 87820.0156  LR: 0.000005  \n",
      "Epoch: [1][28300/36908] Elapsed 389m 42s (remain 118m 31s) Loss: 0.0089(0.0025) Grad: 215951.2656  LR: 0.000005  \n",
      "Epoch: [1][28400/36908] Elapsed 391m 6s (remain 117m 8s) Loss: 0.0013(0.0025) Grad: 55129.4688  LR: 0.000005  \n",
      "Epoch: [1][28500/36908] Elapsed 392m 28s (remain 115m 46s) Loss: 0.0003(0.0025) Grad: 231.3562  LR: 0.000005  \n",
      "Epoch: [1][28600/36908] Elapsed 393m 50s (remain 114m 23s) Loss: 0.0069(0.0025) Grad: 49088.6094  LR: 0.000005  \n",
      "Epoch: [1][28700/36908] Elapsed 395m 12s (remain 113m 0s) Loss: 0.0001(0.0025) Grad: 80.9071  LR: 0.000005  \n",
      "Epoch: [1][28800/36908] Elapsed 396m 34s (remain 111m 37s) Loss: 0.0107(0.0025) Grad: 230475.6406  LR: 0.000005  \n",
      "Epoch: [1][28900/36908] Elapsed 397m 56s (remain 110m 14s) Loss: 0.0019(0.0025) Grad: 8838.8105  LR: 0.000005  \n",
      "Epoch: [1][29000/36908] Elapsed 399m 19s (remain 108m 52s) Loss: 0.0038(0.0025) Grad: 58593.6094  LR: 0.000005  \n",
      "Epoch: [1][29100/36908] Elapsed 400m 42s (remain 107m 29s) Loss: 0.0006(0.0025) Grad: 4712.7842  LR: 0.000005  \n",
      "Epoch: [1][29200/36908] Elapsed 402m 4s (remain 106m 7s) Loss: 0.0002(0.0025) Grad: 678.6211  LR: 0.000005  \n",
      "Epoch: [1][29300/36908] Elapsed 403m 26s (remain 104m 44s) Loss: 0.0030(0.0025) Grad: 13665.7529  LR: 0.000005  \n",
      "Epoch: [1][29400/36908] Elapsed 404m 49s (remain 103m 21s) Loss: 0.0003(0.0025) Grad: 2787.7817  LR: 0.000005  \n",
      "Epoch: [1][29500/36908] Elapsed 406m 12s (remain 101m 59s) Loss: 0.0021(0.0025) Grad: 45765.2930  LR: 0.000004  \n",
      "Epoch: [1][29600/36908] Elapsed 407m 35s (remain 100m 36s) Loss: 0.0001(0.0025) Grad: 162.5413  LR: 0.000004  \n",
      "Epoch: [1][29700/36908] Elapsed 408m 59s (remain 99m 14s) Loss: 0.0019(0.0025) Grad: 49374.6953  LR: 0.000004  \n",
      "Epoch: [1][29800/36908] Elapsed 410m 21s (remain 97m 51s) Loss: 0.0008(0.0025) Grad: 10030.4102  LR: 0.000004  \n",
      "Epoch: [1][29900/36908] Elapsed 411m 43s (remain 96m 28s) Loss: 0.0050(0.0025) Grad: 297325.2500  LR: 0.000004  \n",
      "Epoch: [1][30000/36908] Elapsed 413m 5s (remain 95m 6s) Loss: 0.0006(0.0025) Grad: 14397.3594  LR: 0.000004  \n",
      "Epoch: [1][30100/36908] Elapsed 414m 28s (remain 93m 43s) Loss: 0.0000(0.0025) Grad: 52.1292  LR: 0.000004  \n",
      "Epoch: [1][30200/36908] Elapsed 415m 50s (remain 92m 21s) Loss: 0.0001(0.0025) Grad: 198.2260  LR: 0.000004  \n",
      "Epoch: [1][30300/36908] Elapsed 417m 12s (remain 90m 58s) Loss: 0.0009(0.0025) Grad: 1469.0193  LR: 0.000004  \n",
      "Epoch: [1][30400/36908] Elapsed 418m 35s (remain 89m 35s) Loss: 0.0063(0.0025) Grad: 89283.5234  LR: 0.000004  \n",
      "Epoch: [1][30500/36908] Elapsed 419m 58s (remain 88m 13s) Loss: 0.0001(0.0025) Grad: 617.1336  LR: 0.000004  \n",
      "Epoch: [1][30600/36908] Elapsed 421m 21s (remain 86m 50s) Loss: 0.0011(0.0025) Grad: 13391.4336  LR: 0.000004  \n",
      "Epoch: [1][30700/36908] Elapsed 422m 44s (remain 85m 28s) Loss: 0.0006(0.0025) Grad: 49124.3633  LR: 0.000004  \n",
      "Epoch: [1][30800/36908] Elapsed 424m 8s (remain 84m 5s) Loss: 0.0052(0.0025) Grad: 64728.3945  LR: 0.000004  \n",
      "Epoch: [1][30900/36908] Elapsed 425m 31s (remain 82m 43s) Loss: 0.0015(0.0025) Grad: 5376.6758  LR: 0.000004  \n",
      "Epoch: [1][31000/36908] Elapsed 426m 53s (remain 81m 20s) Loss: 0.0009(0.0025) Grad: 35021.6016  LR: 0.000004  \n",
      "Epoch: [1][31100/36908] Elapsed 428m 17s (remain 79m 58s) Loss: 0.0000(0.0025) Grad: 385.1226  LR: 0.000003  \n",
      "Epoch: [1][31200/36908] Elapsed 429m 39s (remain 78m 35s) Loss: 0.0002(0.0025) Grad: 15256.1953  LR: 0.000003  \n",
      "Epoch: [1][31300/36908] Elapsed 431m 1s (remain 77m 12s) Loss: 0.0011(0.0025) Grad: 28971.0801  LR: 0.000003  \n",
      "Epoch: [1][31400/36908] Elapsed 432m 23s (remain 75m 49s) Loss: 0.0068(0.0025) Grad: 128741.2188  LR: 0.000003  \n",
      "Epoch: [1][31500/36908] Elapsed 433m 45s (remain 74m 27s) Loss: 0.0025(0.0025) Grad: 14161.1328  LR: 0.000003  \n",
      "Epoch: [1][31600/36908] Elapsed 435m 9s (remain 73m 4s) Loss: 0.0001(0.0025) Grad: 237.6182  LR: 0.000003  \n",
      "Epoch: [1][31700/36908] Elapsed 436m 31s (remain 71m 42s) Loss: 0.0055(0.0025) Grad: 60129.1758  LR: 0.000003  \n",
      "Epoch: [1][31800/36908] Elapsed 437m 55s (remain 70m 19s) Loss: 0.0031(0.0025) Grad: 20573.1172  LR: 0.000003  \n",
      "Epoch: [1][31900/36908] Elapsed 439m 17s (remain 68m 56s) Loss: 0.0001(0.0025) Grad: 71.7318  LR: 0.000003  \n",
      "Epoch: [1][32000/36908] Elapsed 440m 40s (remain 67m 34s) Loss: 0.0080(0.0025) Grad: 139939.5156  LR: 0.000003  \n",
      "Epoch: [1][32100/36908] Elapsed 442m 3s (remain 66m 11s) Loss: 0.0000(0.0025) Grad: 13.3343  LR: 0.000003  \n",
      "Epoch: [1][32200/36908] Elapsed 443m 26s (remain 64m 49s) Loss: 0.0009(0.0025) Grad: 2820.2673  LR: 0.000003  \n",
      "Epoch: [1][32300/36908] Elapsed 444m 48s (remain 63m 26s) Loss: 0.0002(0.0025) Grad: 191.3262  LR: 0.000003  \n",
      "Epoch: [1][32400/36908] Elapsed 446m 13s (remain 62m 4s) Loss: 0.0004(0.0025) Grad: 476.1495  LR: 0.000003  \n",
      "Epoch: [1][32500/36908] Elapsed 447m 35s (remain 60m 41s) Loss: 0.0014(0.0025) Grad: 1405.0435  LR: 0.000003  \n",
      "Epoch: [1][32600/36908] Elapsed 448m 58s (remain 59m 18s) Loss: 0.0003(0.0025) Grad: 2101.8560  LR: 0.000003  \n",
      "Epoch: [1][32700/36908] Elapsed 450m 20s (remain 57m 56s) Loss: 0.0036(0.0025) Grad: 22214.0605  LR: 0.000003  \n",
      "Epoch: [1][32800/36908] Elapsed 451m 44s (remain 56m 33s) Loss: 0.0002(0.0025) Grad: 159.6043  LR: 0.000002  \n",
      "Epoch: [1][32900/36908] Elapsed 453m 8s (remain 55m 11s) Loss: 0.0009(0.0025) Grad: 4314.2466  LR: 0.000002  \n",
      "Epoch: [1][33000/36908] Elapsed 454m 31s (remain 53m 48s) Loss: 0.0009(0.0025) Grad: 31330.8262  LR: 0.000002  \n",
      "Epoch: [1][33100/36908] Elapsed 455m 55s (remain 52m 26s) Loss: 0.0014(0.0025) Grad: 6045.6201  LR: 0.000002  \n",
      "Epoch: [1][33200/36908] Elapsed 457m 18s (remain 51m 3s) Loss: 0.0001(0.0025) Grad: 91.4263  LR: 0.000002  \n",
      "Epoch: [1][33300/36908] Elapsed 458m 41s (remain 49m 40s) Loss: 0.0026(0.0025) Grad: 64551.4141  LR: 0.000002  \n",
      "Epoch: [1][33400/36908] Elapsed 460m 3s (remain 48m 18s) Loss: 0.0077(0.0025) Grad: 37015.6836  LR: 0.000002  \n",
      "Epoch: [1][33500/36908] Elapsed 461m 27s (remain 46m 55s) Loss: 0.0006(0.0025) Grad: 2216.3311  LR: 0.000002  \n",
      "Epoch: [1][33600/36908] Elapsed 462m 51s (remain 45m 33s) Loss: 0.0001(0.0025) Grad: 71.4090  LR: 0.000002  \n",
      "Epoch: [1][33700/36908] Elapsed 464m 13s (remain 44m 10s) Loss: 0.0024(0.0025) Grad: 12737.5264  LR: 0.000002  \n",
      "Epoch: [1][33800/36908] Elapsed 465m 35s (remain 42m 47s) Loss: 0.0001(0.0025) Grad: 495.6012  LR: 0.000002  \n",
      "Epoch: [1][33900/36908] Elapsed 466m 56s (remain 41m 25s) Loss: 0.0002(0.0025) Grad: 243.2214  LR: 0.000002  \n",
      "Epoch: [1][34000/36908] Elapsed 468m 20s (remain 40m 2s) Loss: 0.0004(0.0025) Grad: 7155.3145  LR: 0.000002  \n",
      "Epoch: [1][34100/36908] Elapsed 469m 43s (remain 38m 39s) Loss: 0.0005(0.0025) Grad: 2795.7656  LR: 0.000002  \n",
      "Epoch: [1][34200/36908] Elapsed 471m 5s (remain 37m 17s) Loss: 0.0003(0.0025) Grad: 53.1508  LR: 0.000002  \n",
      "Epoch: [1][34300/36908] Elapsed 472m 27s (remain 35m 54s) Loss: 0.0024(0.0025) Grad: 36318.1016  LR: 0.000002  \n",
      "Epoch: [1][34400/36908] Elapsed 473m 49s (remain 34m 31s) Loss: 0.0016(0.0025) Grad: 41290.3633  LR: 0.000002  \n",
      "Epoch: [1][34500/36908] Elapsed 475m 13s (remain 33m 9s) Loss: 0.0000(0.0025) Grad: 140.8035  LR: 0.000001  \n",
      "Epoch: [1][34600/36908] Elapsed 476m 35s (remain 31m 46s) Loss: 0.0032(0.0025) Grad: 15962.7490  LR: 0.000001  \n",
      "Epoch: [1][34700/36908] Elapsed 477m 57s (remain 30m 23s) Loss: 0.0006(0.0025) Grad: 8246.1846  LR: 0.000001  \n",
      "Epoch: [1][34800/36908] Elapsed 479m 21s (remain 29m 1s) Loss: 0.0001(0.0025) Grad: 1410.3035  LR: 0.000001  \n",
      "Epoch: [1][34900/36908] Elapsed 480m 45s (remain 27m 38s) Loss: 0.0008(0.0025) Grad: 2192.8240  LR: 0.000001  \n",
      "Epoch: [1][35000/36908] Elapsed 482m 8s (remain 26m 16s) Loss: 0.0001(0.0025) Grad: 57.5609  LR: 0.000001  \n",
      "Epoch: [1][35100/36908] Elapsed 483m 30s (remain 24m 53s) Loss: 0.0001(0.0025) Grad: 105.6406  LR: 0.000001  \n",
      "Epoch: [1][35200/36908] Elapsed 484m 53s (remain 23m 30s) Loss: 0.0010(0.0025) Grad: 34602.1016  LR: 0.000001  \n",
      "Epoch: [1][35300/36908] Elapsed 486m 15s (remain 22m 8s) Loss: 0.0001(0.0025) Grad: 137.3964  LR: 0.000001  \n",
      "Epoch: [1][35400/36908] Elapsed 487m 38s (remain 20m 45s) Loss: 0.0000(0.0025) Grad: 286.6506  LR: 0.000001  \n",
      "Epoch: [1][35500/36908] Elapsed 489m 0s (remain 19m 22s) Loss: 0.0001(0.0025) Grad: 245.1615  LR: 0.000001  \n",
      "Epoch: [1][35600/36908] Elapsed 490m 22s (remain 18m 0s) Loss: 0.0002(0.0025) Grad: 1859.3350  LR: 0.000001  \n",
      "Epoch: [1][35700/36908] Elapsed 491m 44s (remain 16m 37s) Loss: 0.0003(0.0025) Grad: 910.1190  LR: 0.000001  \n",
      "Epoch: [1][35800/36908] Elapsed 493m 8s (remain 15m 14s) Loss: 0.0000(0.0025) Grad: 49.7789  LR: 0.000001  \n",
      "Epoch: [1][35900/36908] Elapsed 494m 31s (remain 13m 52s) Loss: 0.0010(0.0025) Grad: 2674.7134  LR: 0.000001  \n",
      "Epoch: [1][36000/36908] Elapsed 495m 54s (remain 12m 29s) Loss: 0.0039(0.0025) Grad: 77995.0234  LR: 0.000001  \n",
      "Epoch: [1][36100/36908] Elapsed 497m 17s (remain 11m 6s) Loss: 0.0018(0.0025) Grad: 14606.8027  LR: 0.000000  \n",
      "Epoch: [1][36200/36908] Elapsed 498m 38s (remain 9m 44s) Loss: 0.0006(0.0025) Grad: 57740.0938  LR: 0.000000  \n",
      "Epoch: [1][36300/36908] Elapsed 500m 1s (remain 8m 21s) Loss: 0.0002(0.0025) Grad: 643.2618  LR: 0.000000  \n",
      "Epoch: [1][36400/36908] Elapsed 501m 25s (remain 6m 59s) Loss: 0.0054(0.0025) Grad: 153480.3125  LR: 0.000000  \n",
      "Epoch: [1][36500/36908] Elapsed 502m 49s (remain 5m 36s) Loss: 0.0064(0.0025) Grad: 397276.4375  LR: 0.000000  \n",
      "Epoch: [1][36600/36908] Elapsed 504m 11s (remain 4m 13s) Loss: 0.0001(0.0025) Grad: 548.0923  LR: 0.000000  \n",
      "Epoch: [1][36700/36908] Elapsed 505m 34s (remain 2m 51s) Loss: 0.0001(0.0025) Grad: 962.2994  LR: 0.000000  \n",
      "Epoch: [1][36800/36908] Elapsed 506m 57s (remain 1m 28s) Loss: 0.0005(0.0025) Grad: 473.1621  LR: 0.000000  \n",
      "Epoch: [1][36900/36908] Elapsed 508m 19s (remain 0m 5s) Loss: 0.0019(0.0025) Grad: 33308.4062  LR: 0.000000  \n",
      "Epoch: [1][36907/36908] Elapsed 508m 25s (remain 0m 0s) Loss: 0.0007(0.0025) Grad: 3611.7073  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 16m 57s) Loss: 0.0003(0.0003) \n",
      "EVAL: [100/1192] Elapsed 0m 30s (remain 5m 26s) Loss: 0.0499(0.0055) \n",
      "EVAL: [200/1192] Elapsed 0m 59s (remain 4m 53s) Loss: 0.0047(0.0056) \n",
      "EVAL: [300/1192] Elapsed 1m 28s (remain 4m 23s) Loss: 0.0110(0.0062) \n",
      "EVAL: [400/1192] Elapsed 1m 58s (remain 3m 53s) Loss: 0.0000(0.0059) \n",
      "EVAL: [500/1192] Elapsed 2m 27s (remain 3m 23s) Loss: 0.0704(0.0059) \n",
      "EVAL: [600/1192] Elapsed 2m 57s (remain 2m 54s) Loss: 0.0077(0.0061) \n",
      "EVAL: [700/1192] Elapsed 3m 27s (remain 2m 25s) Loss: 0.0048(0.0069) \n",
      "EVAL: [800/1192] Elapsed 3m 57s (remain 1m 55s) Loss: 0.0207(0.0071) \n",
      "EVAL: [900/1192] Elapsed 4m 26s (remain 1m 26s) Loss: 0.0099(0.0073) \n",
      "EVAL: [1000/1192] Elapsed 4m 56s (remain 0m 56s) Loss: 0.0000(0.0071) \n",
      "EVAL: [1100/1192] Elapsed 5m 26s (remain 0m 26s) Loss: 0.0211(0.0069) \n",
      "EVAL: [1191/1192] Elapsed 5m 52s (remain 0m 0s) Loss: 0.0000(0.0068) \n",
      "Epoch 1 - avg_train_loss: 0.0025  avg_val_loss: 0.0068  time: 30862s\n",
      "Epoch 1 - Score: 0.8985\n",
      "Epoch 1 - Save Best Score: 0.8985 Model\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:274] . unexpected pos 1694991168 vs 1694991056",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mbuf_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-e0ef3ac3ec0e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi_fold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi_fold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_fold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                 \u001b[0m_oof_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                 \u001b[0moof_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moof_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_oof_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moof_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"oof_df.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-757482180a6f>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(df, i_fold, device)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;34m\"predictions\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval_preds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 },\n\u001b[0;32m--> 118\u001b[0;31m                 \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"fold{i_fold}_best.pth\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             )\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:274] . unexpected pos 1694991168 vs 1694991056"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "name": "nbme-exp068.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 641.572862,
   "end_time": "2022-02-27T11:39:50.972497",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-27T11:29:09.399635",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02633c7de1ea4b7ca2fd899ae0c6d209": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "032d2a594c45472e9681d2d7557ab93d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "09da6c904a3344ad9d7d0f17c646c8b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_66daca2dd30d44efbcd88adcbb1c2725",
      "placeholder": "​",
      "style": "IPY_MODEL_02633c7de1ea4b7ca2fd899ae0c6d209",
      "value": " 446k/446k [00:00&lt;00:00, 4.84MB/s]"
     }
    },
    "0c748174ece64ca6992b434a2d92e1e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_827b5e8189d242c9b62bb8d6a084de72",
       "IPY_MODEL_0f910e441d334b10bc666e6ef47de941",
       "IPY_MODEL_09da6c904a3344ad9d7d0f17c646c8b1"
      ],
      "layout": "IPY_MODEL_d6ba1f9a002c49268799fc4a17f793ee"
     }
    },
    "0f0864ef340b49aaa9b4596e032163a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0f910e441d334b10bc666e6ef47de941": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_760cf6427cdc4b05affb72378d92a0f3",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9a660863781b487392504ab3302a5f93",
      "value": 456318
     }
    },
    "13133c559d1b4a14ba19c157c169b532": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13873604cb644df0b4b5e8e9ea57d645": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c619fc144a44ae8ba6d091c236a20f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c64feffa14d4a56b3bf4c25f6d05c51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1fc34ac85840429e8d5d62785817dd03",
      "placeholder": "​",
      "style": "IPY_MODEL_c7dbd264bb804aabbb9a3a3922cdcc00",
      "value": " 52.0/52.0 [00:00&lt;00:00, 2.12kB/s]"
     }
    },
    "1fc34ac85840429e8d5d62785817dd03": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "263650ffbf3145048f331827b49ef11b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_886eee1754fd42e185a7548aa44871f1",
      "placeholder": "​",
      "style": "IPY_MODEL_ec232b9c334c4987b35fde837fac77da",
      "value": " 878k/878k [00:00&lt;00:00, 3.84MB/s]"
     }
    },
    "2c0468cceac144b890d30b510202deba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2df59efc32cf4642a4cfff7db709db5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb191fc8e771447ab389bbb57fe22d2f",
      "max": 143,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ced0845b868342c29d4e2d830a48f203",
      "value": 143
     }
    },
    "323239c2e16449c088afd6eb5eeaa73f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b5f8f50bdaf843f1a938f6129848cd83",
      "max": 898825,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0f0864ef340b49aaa9b4596e032163a7",
      "value": 898825
     }
    },
    "323befd254f741a7b041d124d4073817": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "33bbf6ff0a9847fc9900a16ea8247120": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_61d290b99d78422494bd99ba7f4ce0b4",
      "max": 52,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_511fe7d1f0db48f5848e3539ddb29fff",
      "value": 52
     }
    },
    "3d505cf1fb134928953da8d79d68665a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c276b71af2f4301b4048e4f8dad36d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_563519b534874ec8a145d292258b2dad",
       "IPY_MODEL_535a8156a6da43928fd0d7a33c624540",
       "IPY_MODEL_ef3dd8c9fb4a46bfa6c30489d2d75b02"
      ],
      "layout": "IPY_MODEL_5d985ee5c7d84bf9a135972d46830ad0"
     }
    },
    "511fe7d1f0db48f5848e3539ddb29fff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "530069c0165e4b3d8e077e9c6a4a1d21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c0468cceac144b890d30b510202deba",
      "placeholder": "​",
      "style": "IPY_MODEL_b4c22ff3f7064b5c82b501058fa367a6",
      "value": "Downloading: 100%"
     }
    },
    "535a8156a6da43928fd0d7a33c624540": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8fa83fd7255f43ff9126aa633ed1b663",
      "max": 42146,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9febd109d3a24bef886c8d24808347ba",
      "value": 42146
     }
    },
    "5504bc5bee8c4c189614fd398d1b7fef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "563519b534874ec8a145d292258b2dad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5bc25dd928614ba999cefcde5efa9197",
      "placeholder": "​",
      "style": "IPY_MODEL_5ccb74cb082845d7ade9962f741a2910",
      "value": "100%"
     }
    },
    "5a47b531ee814b1eba201f0aa79212db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5bb3e7fc97c64d59a20a7ebb29a39800": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5bc25dd928614ba999cefcde5efa9197": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ccb74cb082845d7ade9962f741a2910": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5d985ee5c7d84bf9a135972d46830ad0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "61d290b99d78422494bd99ba7f4ce0b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "638cf831bc0443658b4b455200f9b990": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f0a30d22d004fd6866556696346fb74",
      "max": 475,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5504bc5bee8c4c189614fd398d1b7fef",
      "value": 475
     }
    },
    "6639cae9d2844ea3b7d9a777b43b2181": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6e5be1d244794198afee6c8fefd3a191",
       "IPY_MODEL_2df59efc32cf4642a4cfff7db709db5a",
       "IPY_MODEL_72d201f9470a4d04b1e89f7d0018c0d7"
      ],
      "layout": "IPY_MODEL_cebcd629be2340bfa4ca1780d70dd364"
     }
    },
    "66a3685d05e1493c987e6cb1d9ed00a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "66daca2dd30d44efbcd88adcbb1c2725": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e5be1d244794198afee6c8fefd3a191": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da9bce59ac4845cda340d840b79be311",
      "placeholder": "​",
      "style": "IPY_MODEL_323befd254f741a7b041d124d4073817",
      "value": "100%"
     }
    },
    "72d201f9470a4d04b1e89f7d0018c0d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13873604cb644df0b4b5e8e9ea57d645",
      "placeholder": "​",
      "style": "IPY_MODEL_f129267b3ae6422e8d345c28b73f5516",
      "value": " 143/143 [00:00&lt;00:00, 2848.65it/s]"
     }
    },
    "760cf6427cdc4b05affb72378d92a0f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c1442db3949418184bfb68266910d91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7e01c08e3af148f580fcdc6ef6ebf5b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7ebb3f8b10154ef4badb2d6856140d18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "808441188c1a4b5788ae84fa3edf27db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d505cf1fb134928953da8d79d68665a",
      "placeholder": "​",
      "style": "IPY_MODEL_81714296902f4d26a32aa05da8890693",
      "value": "Downloading: 100%"
     }
    },
    "80be6bc8b1c8454187599fe6f05cdcba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "811a7f7f9bc34108b113d084a7d488f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81714296902f4d26a32aa05da8890693": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8254bfa8a6fa47dba57db5ba29dccaeb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "827b5e8189d242c9b62bb8d6a084de72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc036c5ffc0a4a0fb2d0f504cf4264f3",
      "placeholder": "​",
      "style": "IPY_MODEL_80be6bc8b1c8454187599fe6f05cdcba",
      "value": "Downloading: 100%"
     }
    },
    "886eee1754fd42e185a7548aa44871f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e1aca2f17c6476a855cbe11a1d661a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aa1104924af9458eb0d5443fd617cf29",
       "IPY_MODEL_b94dd392dc874911b1deeb39f77b342f",
       "IPY_MODEL_be8e2f5fc8eb4029a4aece3d1776054d"
      ],
      "layout": "IPY_MODEL_032d2a594c45472e9681d2d7557ab93d"
     }
    },
    "8f0a30d22d004fd6866556696346fb74": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8fa83fd7255f43ff9126aa633ed1b663": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9826121100004ec49f1cd7ed26023d9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cfc527e5b7e84c45af32ba7e49275790",
       "IPY_MODEL_33bbf6ff0a9847fc9900a16ea8247120",
       "IPY_MODEL_1c64feffa14d4a56b3bf4c25f6d05c51"
      ],
      "layout": "IPY_MODEL_b42a1221467e43648126dc9432be0b28"
     }
    },
    "9a660863781b487392504ab3302a5f93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9febd109d3a24bef886c8d24808347ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a2fc47910e5a4b158eddf7faa455d683": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_808441188c1a4b5788ae84fa3edf27db",
       "IPY_MODEL_638cf831bc0443658b4b455200f9b990",
       "IPY_MODEL_c5ad324c87914e26ad665efcc418f354"
      ],
      "layout": "IPY_MODEL_8254bfa8a6fa47dba57db5ba29dccaeb"
     }
    },
    "aa1104924af9458eb0d5443fd617cf29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd01049905654fe6bee6c4c602b89ed9",
      "placeholder": "​",
      "style": "IPY_MODEL_66a3685d05e1493c987e6cb1d9ed00a1",
      "value": "100%"
     }
    },
    "b42a1221467e43648126dc9432be0b28": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4c22ff3f7064b5c82b501058fa367a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b5f8f50bdaf843f1a938f6129848cd83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b94dd392dc874911b1deeb39f77b342f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13133c559d1b4a14ba19c157c169b532",
      "max": 42146,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7e01c08e3af148f580fcdc6ef6ebf5b4",
      "value": 42146
     }
    },
    "be8e2f5fc8eb4029a4aece3d1776054d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e464d1ba21a1489183fda5c01bbc0cca",
      "placeholder": "​",
      "style": "IPY_MODEL_5bb3e7fc97c64d59a20a7ebb29a39800",
      "value": " 42146/42146 [00:22&lt;00:00, 2038.25it/s]"
     }
    },
    "c5ad324c87914e26ad665efcc418f354": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a47b531ee814b1eba201f0aa79212db",
      "placeholder": "​",
      "style": "IPY_MODEL_7c1442db3949418184bfb68266910d91",
      "value": " 475/475 [00:00&lt;00:00, 18.8kB/s]"
     }
    },
    "c7dbd264bb804aabbb9a3a3922cdcc00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd01049905654fe6bee6c4c602b89ed9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cebcd629be2340bfa4ca1780d70dd364": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ced0845b868342c29d4e2d830a48f203": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cfc527e5b7e84c45af32ba7e49275790": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c619fc144a44ae8ba6d091c236a20f6",
      "placeholder": "​",
      "style": "IPY_MODEL_f3dcb10079874c84a85896aae581b1dc",
      "value": "Downloading: 100%"
     }
    },
    "d6ba1f9a002c49268799fc4a17f793ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da9bce59ac4845cda340d840b79be311": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc036c5ffc0a4a0fb2d0f504cf4264f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e464d1ba21a1489183fda5c01bbc0cca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb191fc8e771447ab389bbb57fe22d2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec232b9c334c4987b35fde837fac77da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ecfdf18519e34e2d9a0b112b0815cba5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ef3dd8c9fb4a46bfa6c30489d2d75b02": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_811a7f7f9bc34108b113d084a7d488f4",
      "placeholder": "​",
      "style": "IPY_MODEL_ecfdf18519e34e2d9a0b112b0815cba5",
      "value": " 42146/42146 [00:00&lt;00:00, 687236.90it/s]"
     }
    },
    "f129267b3ae6422e8d345c28b73f5516": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f3dcb10079874c84a85896aae581b1dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f61172e130474199999fe10c8470b1e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_530069c0165e4b3d8e077e9c6a4a1d21",
       "IPY_MODEL_323239c2e16449c088afd6eb5eeaa73f",
       "IPY_MODEL_263650ffbf3145048f331827b49ef11b"
      ],
      "layout": "IPY_MODEL_7ebb3f8b10154ef4badb2d6856140d18"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
