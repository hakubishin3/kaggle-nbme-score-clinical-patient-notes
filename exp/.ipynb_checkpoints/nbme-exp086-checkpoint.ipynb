{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "featured-insurance",
   "metadata": {
    "id": "national-fancy"
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-legislation",
   "metadata": {
    "id": "copyrighted-centre"
   },
   "source": [
    "- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-oklahoma",
   "metadata": {
    "id": "imported-offset"
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "broke-london",
   "metadata": {
    "id": "complimentary-wyoming"
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"nbme-exp086\"\n",
    "ENV = \"local\"\n",
    "DEBUG_MODE = False\n",
    "SUBMISSION_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "suffering-candy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y03AHjwJAlGL",
    "outputId": "c33caf3f-c530-4628-bcbf-b9af87b252a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "educated-charlotte",
   "metadata": {
    "id": "allied-circuit"
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    env=ENV\n",
    "    exp_name=EXP_NAME\n",
    "    debug=DEBUG_MODE\n",
    "    submission=SUBMISSION_MODE\n",
    "    apex=True\n",
    "    input_dir=None\n",
    "    output_dir=None\n",
    "    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n",
    "    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n",
    "    competition_name=\"nbme-score-clinical-patient-notes\"\n",
    "    id_col=\"id\"\n",
    "    target_col=\"location\"\n",
    "    pretrained_model_name=\"microsoft/deberta-v3-large\"\n",
    "    tokenizer=None\n",
    "    max_len=None\n",
    "    pseudo_plain_path='../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl'\n",
    "    #pseudo_plain_path=\"./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\"\n",
    "    n_pseudo_labels=100000\n",
    "    output_dim=1\n",
    "    dropout=0.2\n",
    "    num_workers=4\n",
    "    batch_size=3\n",
    "    lr=2e-5\n",
    "    betas=(0.9, 0.98)\n",
    "    weight_decay=0.1\n",
    "    alpha=1\n",
    "    gamma=2\n",
    "    smoothing=0.0001\n",
    "    num_warmup_steps_rate=0.1\n",
    "    batch_scheduler=True\n",
    "    epochs=1\n",
    "    n_fold=4\n",
    "    train_fold=[0, 1, 2, 3]\n",
    "    seed=71\n",
    "    gradient_accumulation_steps=2\n",
    "    max_grad_norm=1000\n",
    "    print_freq=100\n",
    "    train=True\n",
    "    inference=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "inside-cologne",
   "metadata": {
    "id": "geographic-hindu"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.train_fold = [0, 1]\n",
    "\n",
    "if CFG.submission:\n",
    "    CFG.train = False\n",
    "    CFG.inference = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-georgia",
   "metadata": {
    "id": "confident-fifth"
   },
   "source": [
    "## Directory Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "extensive-montgomery",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "miniature-greeting",
    "outputId": "6a439b4d-636c-4026-8cfd-f86093855807"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "print(CFG.env)\n",
    "if CFG.env == \"colab\":\n",
    "    # colab環境\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "    # install packages\n",
    "    !pip install transformers==4.16.2\n",
    "    !pip install -q sentencepiece==0.1.96\n",
    "\n",
    "elif CFG.env == \"local\":\n",
    "    # ローカルサーバ\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "\n",
    "elif CFG.env == \"kaggle\":\n",
    "    # kaggle環境\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "union-photographer",
   "metadata": {
    "id": "nMFg9zv8YGcx"
   },
   "outputs": [],
   "source": [
    "# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n",
    "# This must be done before importing transformers\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "if CFG.env == \"colab\":\n",
    "    input_dir = Path(\"./drive/MyDrive/00.kaggle/input/deberta-v2-3-fast-tokenizer\")\n",
    "    transformers_path = Path(\"/usr/local/lib/python3.7/dist-packages/transformers\")\n",
    "else:\n",
    "    input_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n",
    "    transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n",
    "\n",
    "convert_file = input_dir / \"convert_slow_tokenizer.py\"\n",
    "conversion_path = transformers_path/convert_file.name\n",
    "\n",
    "if conversion_path.exists():\n",
    "    conversion_path.unlink()\n",
    "\n",
    "shutil.copy(convert_file, transformers_path)\n",
    "deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n",
    "\n",
    "for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n",
    "    filepath = deberta_v2_path/filename\n",
    "    if filepath.exists():\n",
    "        filepath.unlink()\n",
    "\n",
    "    shutil.copy(input_dir/filename, filepath)\n",
    "    \n",
    "    \n",
    "from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "greatest-exception",
   "metadata": {
    "id": "guilty-filename"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import BartModel,BertModel,BertTokenizer\n",
    "from transformers import DebertaModel,DebertaTokenizer\n",
    "from transformers import RobertaModel,RobertaTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-interpretation",
   "metadata": {
    "id": "cubic-designation"
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "alpine-montreal",
   "metadata": {
    "id": "opposite-plasma"
   },
   "outputs": [],
   "source": [
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "vulnerable-sport",
   "metadata": {
    "id": "multiple-poland"
   },
   "outputs": [],
   "source": [
    "def create_labels_for_scoring(df):\n",
    "    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n",
    "    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, \"location\"]\n",
    "        if lst:\n",
    "            new_lst = \";\".join(lst)\n",
    "            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n",
    "\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df[\"location_for_create_labels\"].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, token_probs, tokenizer):\n",
    "    res = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n",
    "        encoded = tokenizer(\n",
    "            text=text,\n",
    "            max_length=CFG.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n",
    "            start, end = offset_mapping\n",
    "            res[i][start:end] = pred\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_predicted_location_str(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        # result = np.where(char_prob >= th)[0] + 1\n",
    "        result = np.where(char_prob >= th)[0]\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        # result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = [f\"{min(r)} {max(r) + 1}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def scoring(df, th=0.5, use_token_prob=True):\n",
    "    labels = create_labels_for_scoring(df)\n",
    "\n",
    "    if use_token_prob:\n",
    "        token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n",
    "        char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n",
    "    else:\n",
    "        char_probs = df[[str(i) for i in range(CFG.max_char_len)]].values\n",
    "        char_probs = [char_probs[i] for i in range(len(char_probs))]\n",
    "\n",
    "    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n",
    "    preds = get_predictions(predicted_location_str)\n",
    "\n",
    "    score = get_score(labels, preds)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_best_thres(oof_df):\n",
    "    def f1_opt(x):\n",
    "        return -1 * scoring(oof_df, th=x)\n",
    "\n",
    "    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n",
    "    return best_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "noticed-parameter",
   "metadata": {
    "id": "seventh-fighter"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "vocational-anthropology",
   "metadata": {
    "id": "fifty-boundary"
   },
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-strike",
   "metadata": {
    "id": "unlimited-hotel"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "magnetic-story",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "classical-machine",
    "outputId": "918ddc66-5d2d-44dc-c637-0145350ded5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 6), (143, 3), (42146, 3), (5, 4))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(CFG.input_dir / \"train.csv\")\n",
    "features = pd.read_csv(CFG.input_dir / \"features.csv\")\n",
    "patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n",
    "test = pd.read_csv(CFG.input_dir / \"test.csv\")\n",
    "\n",
    "train.shape, features.shape, patient_notes.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "charged-prior",
   "metadata": {
    "id": "vanilla-iceland"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n",
    "    print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-grounds",
   "metadata": {
    "id": "convenient-plant"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "israeli-separation",
   "metadata": {
    "id": "convertible-thunder"
   },
   "outputs": [],
   "source": [
    "def preprocess_features(features):\n",
    "    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "\n",
    "\n",
    "features = preprocess_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "alert-morris",
   "metadata": {
    "id": "a7YBS_idYKtL"
   },
   "outputs": [],
   "source": [
    "features['feature_text'] = features['feature_text'].str.lower()\n",
    "patient_notes['pn_history'] = patient_notes['pn_history'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "spare-brief",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "charitable-memphis",
    "outputId": "fca9323a-7ebb-469c-e494-f3ea3aa5db24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 8), (5, 6))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "artificial-price",
   "metadata": {
    "id": "governing-election"
   },
   "outputs": [],
   "source": [
    "train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n",
    "train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "alleged-emergency",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "negative-provincial",
    "outputId": "a14cf34d-27c0-41c1-a97f-a02cb70b1482"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4399\n",
       "1    8181\n",
       "2    1296\n",
       "3     287\n",
       "4      99\n",
       "5      27\n",
       "6       9\n",
       "7       1\n",
       "8       1\n",
       "Name: annotation_length, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n",
    "display(train['annotation_length'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-rhythm",
   "metadata": {
    "id": "arbitrary-beatles"
   },
   "source": [
    "## CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "handy-screen",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "important-murray",
    "outputId": "0f77c675-4cc4-4be5-9d99-346e40c7135a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    3575\n",
       "1    3575\n",
       "2    3575\n",
       "3    3575\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Fold = GroupKFold(n_splits=CFG.n_fold)\n",
    "groups = train['pn_num'].values\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-workplace",
   "metadata": {
    "id": "configured-chemistry"
   },
   "source": [
    "## Setup tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "referenced-rwanda",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hindu-contest",
    "outputId": "93f3e77c-2a10-4a0e-8e67-65805e82e164"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "if CFG.submission:\n",
    "    tokenizer = DebertaV2TokenizerFast.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n",
    "else:\n",
    "    tokenizer = DebertaV2TokenizerFast.from_pretrained(CFG.pretrained_model_name)\n",
    "    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n",
    "\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-spectrum",
   "metadata": {
    "id": "alleged-protein"
   },
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adjustable-payroll",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "18b47303dd5b48bfb118053a80c44a40",
      "971945a52a6d4f06ba35e5363d264037",
      "2d7cfbb1d1a54c0590e59de0b280ab22",
      "054630edadaa453fb86d66a20e49030f",
      "8fce27d68c1c41ec9a3c58d439c2a5e7",
      "e9a3708f7e4c486da3a09e066b6936fb",
      "cd1144e40332427fa3e8d5bc7f57b924",
      "28c710503f154bdea731991801a85a47",
      "8c57bf78a80247db9521bd5b163502ef",
      "747b8a73ce544c569f4d063fc9d6d18a",
      "c7aa62f5eaca401dbbfbfd5f843d6a59"
     ]
    },
    "id": "composed-stroke",
    "outputId": "71ab3663-99ff-4577-ac6f-28425ca4c488"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f991b3407bc4d72853e2214abae059c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 284\n"
     ]
    }
   ],
   "source": [
    "pn_history_lengths = []\n",
    "tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    pn_history_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(pn_history_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "future-holder",
   "metadata": {
    "id": "emotional-region"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeed144e110a496491ab3026e73383c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 28\n"
     ]
    }
   ],
   "source": [
    "feature_text_lengths = []\n",
    "tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    feature_text_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(feature_text_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "descending-rover",
   "metadata": {
    "id": "wrong-leisure"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 315\n"
     ]
    }
   ],
   "source": [
    "CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n",
    "\n",
    "print(\"max length:\", CFG.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "physical-truth",
   "metadata": {
    "id": "convenient-gospel"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c504184dd44397b154c0d1c79bd41d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 950\n"
     ]
    }
   ],
   "source": [
    "pn_history_lengths = []\n",
    "tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n",
    "for text in tk0:\n",
    "    length = len(text)\n",
    "    pn_history_lengths.append(length)\n",
    "\n",
    "CFG.max_char_len = max(pn_history_lengths)\n",
    "\n",
    "print(\"max length:\", CFG.max_char_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "respective-forth",
   "metadata": {
    "id": "representative-contributor"
   },
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, cfg, df, pseudo_label=None):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.max_char_len = self.cfg.max_char_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "        self.annotation_lengths = self.df[\"annotation_length\"].values\n",
    "        self.locations = self.df[\"location\"].values\n",
    "        if \"pseudo_idx\" in df.columns:\n",
    "            self.pseudo_idx = self.df[\"pseudo_idx\"].values\n",
    "            self.pseudo_label = pseudo_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def _create_mapping_from_token_to_char(self, pn_history):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        mapping_from_token_to_char = np.zeros(self.max_char_len)\n",
    "        offset_mapping = encoded[\"offset_mapping\"]\n",
    "        for i, offset in enumerate(offset_mapping):\n",
    "            start_idx, end_idx = offset\n",
    "            mapping_from_token_to_char[start_idx:end_idx] = i\n",
    "        return torch.tensor(mapping_from_token_to_char, dtype=torch.long)\n",
    "\n",
    "    def _create_label(self, pn_history, annotation_length, location_list):\n",
    "        label = np.zeros(self.max_char_len)\n",
    "        label[len(pn_history):] = -1\n",
    "        if annotation_length > 0:\n",
    "            for location in location_list:\n",
    "                for loc in [s.split() for s in location.split(\";\")]:\n",
    "                    start, end = int(loc[0]), int(loc[1])\n",
    "                    label[start:end] = 1\n",
    "        return torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        if not np.isnan(self.annotation_lengths[idx]):\n",
    "            label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n",
    "        else:\n",
    "            p_idx = int(self.pseudo_idx[idx])\n",
    "            label = torch.tensor(self.pseudo_label[p_idx], dtype=torch.float)\n",
    "        mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n",
    "        return input_, label, mapping_from_token_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "confident-connection",
   "metadata": {
    "id": "decent-johnson"
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.max_char_len = self.cfg.max_char_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def _create_mapping_from_token_to_char(self, pn_history):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        mapping_from_token_to_char = np.zeros(self.max_char_len)\n",
    "        offset_mapping = encoded[\"offset_mapping\"]\n",
    "        for i, offset in enumerate(offset_mapping):\n",
    "            start_idx, end_idx = offset\n",
    "            mapping_from_token_to_char[start_idx:end_idx] = i\n",
    "        return torch.tensor(mapping_from_token_to_char, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n",
    "        return input_, mapping_from_token_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-palestine",
   "metadata": {
    "id": "arctic-joint"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "female-october",
   "metadata": {
    "id": "qTRu8eKOTlcX"
   },
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import MaskedLMOutput\n",
    "\n",
    "class MaskedModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(\n",
    "                cfg.pretrained_model_name,\n",
    "                output_hidden_states=False\n",
    "                )\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        \n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.pretrained_model_name, config=self.config)\n",
    "            self.lm_head = AutoModelForMaskedLM.from_pretrained(cfg.pretrained_model_name, config=self.config).cls # [cls, lm_head]\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "            self.lm_head = AutoModelForMaskedLM(self.config).cls # [cls, lm_head]\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            #position_ids=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None):\n",
    "        \n",
    "        outputs = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            #position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,)\n",
    "        \n",
    "        sequence_output = outputs[0]\n",
    "        prediction_scores = self.lm_head(sequence_output)\n",
    "\n",
    "        masked_lm_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        return MaskedLMOutput(loss=masked_lm_loss,\n",
    "                              logits=prediction_scores,\n",
    "                              hidden_states=outputs.hidden_states,\n",
    "                              attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "useful-prevention",
   "metadata": {
    "id": "OJt_cHeyTmDS"
   },
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, model_config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if model_config_path is None:\n",
    "            self.model_config = AutoConfig.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "        else:\n",
    "            self.model_config = torch.load(model_config_path)\n",
    "\n",
    "        if pretrained:\n",
    "            self.backbone = AutoModel.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                config=self.model_config,\n",
    "            )\n",
    "            print(f\"Load weight from pretrained\")\n",
    "        else:\n",
    "            #self.backbone = AutoModel.from_config(self.model_config)\n",
    "            # itpt = AutoModelForMaskedLM.from_config(self.model_config)\n",
    "            #path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp010/checkpoint-130170/pytorch_model.bin\")\n",
    "            # path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n",
    "            # state_dict = torch.load(path)\n",
    "            # itpt.load_state_dict(state_dict)\n",
    "            #path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\")\n",
    "            path = str(Path(\"../output\") / CFG.competition_name /  \"nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\")\n",
    "            masked_model = MaskedModel(CFG, config_path=None, pretrained=True)\n",
    "            state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "            masked_model.load_state_dict(state)\n",
    "            self.backbone = masked_model.model\n",
    "            print(f\"Load weight from {path}\")\n",
    "            del state, masked_model; gc.collect()\n",
    "\n",
    "        self.lstm = nn.GRU(\n",
    "            input_size=self.model_config.hidden_size,\n",
    "            bidirectional=True,\n",
    "            hidden_size=self.model_config.hidden_size // 2,\n",
    "            num_layers=4,\n",
    "            dropout=self.cfg.dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(self.cfg.dropout),\n",
    "            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, mappings_from_token_to_char):\n",
    "        h = self.backbone(**inputs)[\"last_hidden_state\"]  # [batch, seq_len, d_model]\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.unsqueeze(2).expand(-1, -1, self.model_config.hidden_size)\n",
    "        h = torch.gather(h, 1, mappings_from_token_to_char)    # [batch, seq_len, d_model]\n",
    "        h, _ = self.lstm(h)\n",
    "        output = self.fc(h)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-offense",
   "metadata": {
    "id": "therapeutic-assembly"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "informative-pleasure",
   "metadata": {
    "id": "going-conversion"
   },
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    train_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    epoch,\n",
    "    scheduler,\n",
    "    device,\n",
    "):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels, mappings_from_token_to_char) in enumerate(train_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device) \n",
    "        batch_size = labels.size(0)\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            output = model(inputs, mappings_from_token_to_char)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if CFG.batch_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n",
    "            print(\n",
    "                \"Epoch: [{0}][{1}/{2}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                \"Grad: {grad_norm:.4f}  \"\n",
    "                \"LR: {lr:.6f}  \"\n",
    "                .format(\n",
    "                    epoch+1,\n",
    "                    step,\n",
    "                    len(train_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n",
    "                    loss=losses,\n",
    "                     grad_norm=grad_norm,\n",
    "                     lr=scheduler.get_lr()[0],\n",
    "                )\n",
    "            )\n",
    "    del output, loss, inputs, labels, mappings_from_token_to_char, scaler, grad_norm; gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "minute-document",
   "metadata": {
    "id": "alleged-commonwealth"
   },
   "outputs": [],
   "source": [
    "def valid_fn(\n",
    "    val_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    device,\n",
    "):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels, mappings_from_token_to_char) in enumerate(val_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device) \n",
    "        batch_size = labels.size(0)\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            output = model(inputs, mappings_from_token_to_char)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "        loss = loss.mean()\n",
    "    \n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n",
    "            print(\n",
    "                \"EVAL: [{0}/{1}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                .format(\n",
    "                    step, len(val_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n",
    "                    loss=losses,\n",
    "                )\n",
    "            )\n",
    "    preds = np.concatenate(preds)\n",
    "    return losses.avg, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "color-elimination",
   "metadata": {
    "id": "middle-determination"
   },
   "outputs": [],
   "source": [
    "def inference_fn(test_dataloader, model, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preds = []\n",
    "    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n",
    "    for (inputs, mappings_from_token_to_char) in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs, mappings_from_token_to_char)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "connected-substance",
   "metadata": {
    "id": "familiar-participation"
   },
   "outputs": [],
   "source": [
    "def train_loop(df, i_fold, device):\n",
    "    print(f\"========== fold: {i_fold} training ==========\")\n",
    "    train_idx = df[df[\"fold\"] != i_fold].index\n",
    "    val_idx = df[df[\"fold\"] == i_fold].index\n",
    "\n",
    "    train_folds = df.loc[train_idx].reset_index(drop=True)\n",
    "    val_folds = df.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    if CFG.pseudo_plain_path is not None:\n",
    "        pseudo_plain = pd.read_pickle(CFG.pseudo_plain_path)\n",
    "        print(f\"get pseudo plain from {CFG.pseudo_plain_path}\")\n",
    "        pseudo_label_list = []\n",
    "        weights = [0.4433659049657008, 0.20859987143371844, 0.3480342236005807]\n",
    "        for exp_name in [\"nbme-exp060\", \"nbme-exp067\", \"nbme-exp083\"]:\n",
    "            #pseudo_label_path = f'./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/{exp_name}/pseudo_labels_{i_fold}.npy'\n",
    "            pseudo_label_path = f'../output/nbme-score-clinical-patient-notes/{exp_name}/pseudo_labels_{i_fold}.npy'\n",
    "            pseudo_label = np.load(pseudo_label_path)\n",
    "            print(f\"get pseudo labels from {pseudo_label_path}\")\n",
    "            pseudo_label_list.append(pseudo_label)\n",
    "\n",
    "        pseudo_label = weights[0] * pseudo_label_list[0] + weights[1] * pseudo_label_list[1] + weights[2] * pseudo_label_list[2]\n",
    "        print(pseudo_plain.shape, pseudo_label.shape)\n",
    "\n",
    "        pseudo_plain['feature_text'] = pseudo_plain['feature_text'].str.lower()\n",
    "        pseudo_plain['pn_history'] = pseudo_plain['pn_history'].str.lower()\n",
    "\n",
    "        pseudo_plain[\"pseudo_idx\"] = np.arange(len(pseudo_plain))\n",
    "        pseudo_plain = pseudo_plain.sample(n=CFG.n_pseudo_labels)\n",
    "        print(pseudo_plain.shape)\n",
    "        train_folds = pd.concat([train_folds, pseudo_plain], axis=0, ignore_index=True)\n",
    "        print(train_folds.shape)\n",
    "\n",
    "    train_dataset = TrainingDataset(CFG, train_folds, pseudo_label)\n",
    "    val_dataset = TrainingDataset(CFG, val_folds)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "    model = CustomModel(CFG, model_config_path=None, pretrained=False)   # itptを使うため\n",
    "    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n",
    "    model.to(device)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in param_optimizer if not any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n",
    "        {\"params\": [p for n, p in param_optimizer if any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=CFG.lr,\n",
    "        betas=CFG.betas,\n",
    "        weight_decay=CFG.weight_decay,\n",
    "    )\n",
    "    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n",
    "    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_train_optimization_steps,\n",
    "    )\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    best_score = -1 * np.inf\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "        avg_loss = train_fn(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            scheduler,\n",
    "            device,\n",
    "        )\n",
    "        avg_val_loss, val_preds = valid_fn(\n",
    "            val_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "\n",
    "        # scoring\n",
    "        val_folds[[str(i) for i in range(CFG.max_char_len)]] = val_preds\n",
    "        score = scoring(val_folds, th=0.5, use_token_prob=False)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n",
    "        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"predictions\": val_preds,\n",
    "                },\n",
    "                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "            )\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = torch.load(\n",
    "        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )[\"predictions\"]\n",
    "    val_folds[[str(i) for i in range(CFG.max_char_len)]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return val_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-danish",
   "metadata": {
    "id": "coated-cameroon"
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "living-palestine",
   "metadata": {
    "id": "quality-expansion"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for i_fold in range(CFG.n_fold):\n",
    "            if i_fold in CFG.train_fold:\n",
    "                _oof_df = train_loop(train, i_fold, device)\n",
    "                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n",
    "        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    if CFG.submission:\n",
    "        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n",
    "    else:\n",
    "        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    best_thres = 0.5\n",
    "    best_score = 0.\n",
    "    for th in np.arange(0.45, 0.55, 0.01):\n",
    "        th = np.round(th, 2)\n",
    "        score = scoring(oof_df, th=th, use_token_prob=False)\n",
    "        if best_score < score:\n",
    "            best_thres = th\n",
    "            best_score = score\n",
    "    print(f\"best_thres: {best_thres}  score: {best_score:.5f}\")\n",
    "\n",
    "    if CFG.inference:\n",
    "        test_dataset = TestDataset(CFG, test)\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=CFG.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=CFG.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        predictions = []\n",
    "        for i_fold in CFG.train_fold:\n",
    "            if CFG.submission:\n",
    "                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n",
    "                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n",
    "            else:\n",
    "                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n",
    "\n",
    "            state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "            model.load_state_dict(state[\"model\"])\n",
    "            print(f\"load weights from {path}\")\n",
    "            test_char_probs = inference_fn(test_dataloader, model, device)\n",
    "            predictions.append(test_char_probs)\n",
    "\n",
    "            del state, test_char_probs, model; gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        predictions = np.mean(predictions, axis=0)\n",
    "        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n",
    "        test[CFG.target_col] = predicted_location_str\n",
    "        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n",
    "        test[[CFG.id_col, CFG.target_col]].to_csv(\n",
    "            CFG.output_dir / \"submission.csv\", index=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "about-while",
   "metadata": {
    "id": "proprietary-civilian"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== fold: 2 training ==========\n",
      "get pseudo plain from ../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_2.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_2.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp083/pseudo_labels_2.npy\n",
      "(612602, 6) (612602, 950)\n",
      "(100000, 7)\n",
      "(110725, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\n",
      "Epoch: [1][0/36908] Elapsed 0m 1s (remain 734m 29s) Loss: 0.3459(0.3459) Grad: 125530.3047  LR: 0.000000  \n",
      "Epoch: [1][100/36908] Elapsed 1m 17s (remain 469m 0s) Loss: 0.3140(0.3326) Grad: 125897.2656  LR: 0.000001  \n",
      "Epoch: [1][200/36908] Elapsed 2m 35s (remain 474m 13s) Loss: 0.2318(0.3060) Grad: 103966.3281  LR: 0.000001  \n",
      "Epoch: [1][300/36908] Elapsed 3m 52s (remain 471m 32s) Loss: 0.1366(0.2662) Grad: 61193.6055  LR: 0.000002  \n",
      "Epoch: [1][400/36908] Elapsed 5m 9s (remain 469m 6s) Loss: 0.0532(0.2236) Grad: 24040.7852  LR: 0.000002  \n",
      "Epoch: [1][500/36908] Elapsed 6m 26s (remain 467m 32s) Loss: 0.0310(0.1884) Grad: 6525.8071  LR: 0.000003  \n",
      "Epoch: [1][600/36908] Elapsed 7m 42s (remain 465m 19s) Loss: 0.0530(0.1636) Grad: 5380.8789  LR: 0.000003  \n",
      "Epoch: [1][700/36908] Elapsed 8m 59s (remain 464m 0s) Loss: 0.0230(0.1453) Grad: 3777.6284  LR: 0.000004  \n",
      "Epoch: [1][800/36908] Elapsed 10m 15s (remain 462m 31s) Loss: 0.0703(0.1314) Grad: 11415.2764  LR: 0.000004  \n",
      "Epoch: [1][900/36908] Elapsed 11m 32s (remain 461m 5s) Loss: 0.0701(0.1208) Grad: 14512.3320  LR: 0.000005  \n",
      "Epoch: [1][1000/36908] Elapsed 12m 50s (remain 460m 55s) Loss: 0.0436(0.1123) Grad: 13014.7656  LR: 0.000005  \n",
      "Epoch: [1][1100/36908] Elapsed 14m 9s (remain 460m 13s) Loss: 0.0277(0.1049) Grad: 17170.3164  LR: 0.000006  \n",
      "Epoch: [1][1200/36908] Elapsed 15m 25s (remain 458m 47s) Loss: 0.0174(0.0984) Grad: 26674.0547  LR: 0.000007  \n",
      "Epoch: [1][1300/36908] Elapsed 16m 43s (remain 457m 50s) Loss: 0.0059(0.0926) Grad: 12821.4619  LR: 0.000007  \n",
      "Epoch: [1][1400/36908] Elapsed 18m 0s (remain 456m 28s) Loss: 0.0173(0.0870) Grad: 26712.3340  LR: 0.000008  \n",
      "Epoch: [1][1500/36908] Elapsed 19m 17s (remain 455m 5s) Loss: 0.0137(0.0825) Grad: 39820.6641  LR: 0.000008  \n",
      "Epoch: [1][1600/36908] Elapsed 20m 35s (remain 454m 6s) Loss: 0.0193(0.0782) Grad: 27109.6875  LR: 0.000009  \n",
      "Epoch: [1][1700/36908] Elapsed 21m 59s (remain 455m 8s) Loss: 0.0115(0.0744) Grad: 29779.9883  LR: 0.000009  \n",
      "Epoch: [1][1800/36908] Elapsed 23m 16s (remain 453m 40s) Loss: 0.0138(0.0711) Grad: 37429.9297  LR: 0.000010  \n",
      "Epoch: [1][1900/36908] Elapsed 24m 32s (remain 451m 55s) Loss: 0.0052(0.0679) Grad: 23569.7227  LR: 0.000010  \n",
      "Epoch: [1][2000/36908] Elapsed 25m 49s (remain 450m 24s) Loss: 0.0010(0.0650) Grad: 2044.1639  LR: 0.000011  \n",
      "Epoch: [1][2100/36908] Elapsed 27m 6s (remain 449m 8s) Loss: 0.0065(0.0624) Grad: 18814.8555  LR: 0.000011  \n",
      "Epoch: [1][2200/36908] Elapsed 28m 24s (remain 447m 57s) Loss: 0.0231(0.0601) Grad: 75474.1484  LR: 0.000012  \n",
      "Epoch: [1][2300/36908] Elapsed 29m 41s (remain 446m 34s) Loss: 0.0040(0.0580) Grad: 12033.5742  LR: 0.000012  \n",
      "Epoch: [1][2400/36908] Elapsed 30m 58s (remain 445m 13s) Loss: 0.0013(0.0560) Grad: 48552.2773  LR: 0.000013  \n",
      "Epoch: [1][2500/36908] Elapsed 32m 15s (remain 443m 52s) Loss: 0.0052(0.0542) Grad: 27502.0391  LR: 0.000014  \n",
      "Epoch: [1][2600/36908] Elapsed 33m 34s (remain 442m 56s) Loss: 0.0114(0.0524) Grad: 51880.2656  LR: 0.000014  \n",
      "Epoch: [1][2700/36908] Elapsed 34m 53s (remain 441m 51s) Loss: 0.0043(0.0508) Grad: 15473.7373  LR: 0.000015  \n",
      "Epoch: [1][2800/36908] Elapsed 36m 11s (remain 440m 35s) Loss: 0.0050(0.0493) Grad: 34949.8242  LR: 0.000015  \n",
      "Epoch: [1][2900/36908] Elapsed 37m 28s (remain 439m 19s) Loss: 0.0142(0.0480) Grad: 42737.8789  LR: 0.000016  \n",
      "Epoch: [1][3000/36908] Elapsed 38m 47s (remain 438m 14s) Loss: 0.0012(0.0466) Grad: 2012.1907  LR: 0.000016  \n",
      "Epoch: [1][3100/36908] Elapsed 40m 9s (remain 437m 44s) Loss: 0.0055(0.0454) Grad: 28296.0215  LR: 0.000017  \n",
      "Epoch: [1][3200/36908] Elapsed 41m 27s (remain 436m 33s) Loss: 0.0001(0.0443) Grad: 193.0004  LR: 0.000017  \n",
      "Epoch: [1][3300/36908] Elapsed 42m 45s (remain 435m 16s) Loss: 0.0033(0.0432) Grad: 9046.4678  LR: 0.000018  \n",
      "Epoch: [1][3400/36908] Elapsed 44m 2s (remain 433m 56s) Loss: 0.0048(0.0423) Grad: 17371.9863  LR: 0.000018  \n",
      "Epoch: [1][3500/36908] Elapsed 45m 22s (remain 432m 58s) Loss: 0.0008(0.0414) Grad: 637.5729  LR: 0.000019  \n",
      "Epoch: [1][3600/36908] Elapsed 46m 44s (remain 432m 16s) Loss: 0.0073(0.0404) Grad: 72326.6016  LR: 0.000020  \n",
      "Epoch: [1][3700/36908] Elapsed 48m 2s (remain 431m 0s) Loss: 0.0027(0.0395) Grad: 9244.7998  LR: 0.000020  \n",
      "Epoch: [1][3800/36908] Elapsed 49m 22s (remain 429m 59s) Loss: 0.0106(0.0387) Grad: 14815.3301  LR: 0.000020  \n",
      "Epoch: [1][3900/36908] Elapsed 50m 39s (remain 428m 39s) Loss: 0.0069(0.0380) Grad: 44604.6484  LR: 0.000020  \n",
      "Epoch: [1][4000/36908] Elapsed 51m 56s (remain 427m 16s) Loss: 0.0014(0.0372) Grad: 1757.1617  LR: 0.000020  \n",
      "Epoch: [1][4100/36908] Elapsed 53m 15s (remain 426m 0s) Loss: 0.0007(0.0365) Grad: 3244.3403  LR: 0.000020  \n",
      "Epoch: [1][4200/36908] Elapsed 54m 38s (remain 425m 24s) Loss: 0.0003(0.0358) Grad: 233.1146  LR: 0.000020  \n",
      "Epoch: [1][4300/36908] Elapsed 56m 1s (remain 424m 43s) Loss: 0.0027(0.0351) Grad: 18908.3203  LR: 0.000020  \n",
      "Epoch: [1][4400/36908] Elapsed 57m 21s (remain 423m 42s) Loss: 0.0199(0.0345) Grad: 30464.2266  LR: 0.000020  \n",
      "Epoch: [1][4500/36908] Elapsed 58m 39s (remain 422m 17s) Loss: 0.0000(0.0339) Grad: 59.6730  LR: 0.000020  \n",
      "Epoch: [1][4600/36908] Elapsed 59m 56s (remain 420m 53s) Loss: 0.0036(0.0333) Grad: 13434.4014  LR: 0.000019  \n",
      "Epoch: [1][4700/36908] Elapsed 61m 16s (remain 419m 45s) Loss: 0.0066(0.0328) Grad: 13303.4121  LR: 0.000019  \n",
      "Epoch: [1][4800/36908] Elapsed 62m 34s (remain 418m 28s) Loss: 0.0013(0.0322) Grad: 1116.3651  LR: 0.000019  \n",
      "Epoch: [1][4900/36908] Elapsed 63m 50s (remain 416m 58s) Loss: 0.0038(0.0318) Grad: 7981.3354  LR: 0.000019  \n",
      "Epoch: [1][5000/36908] Elapsed 65m 7s (remain 415m 28s) Loss: 0.0107(0.0313) Grad: 8532.9736  LR: 0.000019  \n",
      "Epoch: [1][5100/36908] Elapsed 66m 23s (remain 414m 1s) Loss: 0.0032(0.0308) Grad: 6254.3750  LR: 0.000019  \n",
      "Epoch: [1][5200/36908] Elapsed 67m 42s (remain 412m 44s) Loss: 0.0004(0.0303) Grad: 105.2970  LR: 0.000019  \n",
      "Epoch: [1][5300/36908] Elapsed 68m 58s (remain 411m 16s) Loss: 0.0009(0.0298) Grad: 248.4064  LR: 0.000019  \n",
      "Epoch: [1][5400/36908] Elapsed 70m 19s (remain 410m 13s) Loss: 0.0024(0.0294) Grad: 1943.6774  LR: 0.000019  \n",
      "Epoch: [1][5500/36908] Elapsed 71m 35s (remain 408m 44s) Loss: 0.0003(0.0289) Grad: 177.7474  LR: 0.000019  \n",
      "Epoch: [1][5600/36908] Elapsed 72m 51s (remain 407m 16s) Loss: 0.0046(0.0285) Grad: 5489.0054  LR: 0.000019  \n",
      "Epoch: [1][5700/36908] Elapsed 74m 9s (remain 405m 56s) Loss: 0.0040(0.0281) Grad: 17455.3262  LR: 0.000019  \n",
      "Epoch: [1][5800/36908] Elapsed 75m 30s (remain 404m 54s) Loss: 0.0118(0.0277) Grad: 23379.2461  LR: 0.000019  \n",
      "Epoch: [1][5900/36908] Elapsed 76m 47s (remain 403m 28s) Loss: 0.0012(0.0274) Grad: 8254.2646  LR: 0.000019  \n",
      "Epoch: [1][6000/36908] Elapsed 78m 3s (remain 401m 59s) Loss: 0.0003(0.0270) Grad: 34.4345  LR: 0.000019  \n",
      "Epoch: [1][6100/36908] Elapsed 79m 19s (remain 400m 32s) Loss: 0.0023(0.0267) Grad: 5025.5586  LR: 0.000019  \n",
      "Epoch: [1][6200/36908] Elapsed 80m 37s (remain 399m 13s) Loss: 0.0072(0.0263) Grad: 7562.3120  LR: 0.000018  \n",
      "Epoch: [1][6300/36908] Elapsed 81m 56s (remain 398m 0s) Loss: 0.0006(0.0260) Grad: 1480.1868  LR: 0.000018  \n",
      "Epoch: [1][6400/36908] Elapsed 83m 20s (remain 397m 10s) Loss: 0.0026(0.0256) Grad: 14328.0762  LR: 0.000018  \n",
      "Epoch: [1][6500/36908] Elapsed 84m 42s (remain 396m 14s) Loss: 0.0002(0.0253) Grad: 206.6962  LR: 0.000018  \n",
      "Epoch: [1][6600/36908] Elapsed 86m 5s (remain 395m 17s) Loss: 0.0006(0.0250) Grad: 154.7116  LR: 0.000018  \n",
      "Epoch: [1][6700/36908] Elapsed 87m 24s (remain 393m 59s) Loss: 0.0002(0.0247) Grad: 71.8204  LR: 0.000018  \n",
      "Epoch: [1][6800/36908] Elapsed 88m 41s (remain 392m 36s) Loss: 0.0003(0.0244) Grad: 360.6021  LR: 0.000018  \n",
      "Epoch: [1][6900/36908] Elapsed 89m 58s (remain 391m 14s) Loss: 0.0072(0.0242) Grad: 30416.4180  LR: 0.000018  \n",
      "Epoch: [1][7000/36908] Elapsed 91m 16s (remain 389m 52s) Loss: 0.0107(0.0239) Grad: 4591.0562  LR: 0.000018  \n",
      "Epoch: [1][7100/36908] Elapsed 92m 34s (remain 388m 36s) Loss: 0.0033(0.0236) Grad: 1605.5472  LR: 0.000018  \n",
      "Epoch: [1][7200/36908] Elapsed 93m 53s (remain 387m 20s) Loss: 0.0015(0.0233) Grad: 686.3055  LR: 0.000018  \n",
      "Epoch: [1][7300/36908] Elapsed 95m 10s (remain 385m 58s) Loss: 0.0036(0.0231) Grad: 869.2914  LR: 0.000018  \n",
      "Epoch: [1][7400/36908] Elapsed 96m 28s (remain 384m 36s) Loss: 0.0075(0.0228) Grad: 3569.0742  LR: 0.000018  \n",
      "Epoch: [1][7500/36908] Elapsed 97m 45s (remain 383m 16s) Loss: 0.0019(0.0226) Grad: 1077.4705  LR: 0.000018  \n",
      "Epoch: [1][7600/36908] Elapsed 99m 7s (remain 382m 9s) Loss: 0.0075(0.0223) Grad: 9895.9697  LR: 0.000018  \n",
      "Epoch: [1][7700/36908] Elapsed 100m 24s (remain 380m 49s) Loss: 0.0061(0.0221) Grad: 3869.7107  LR: 0.000018  \n",
      "Epoch: [1][7800/36908] Elapsed 101m 43s (remain 379m 33s) Loss: 0.0003(0.0219) Grad: 474.6482  LR: 0.000018  \n",
      "Epoch: [1][7900/36908] Elapsed 103m 3s (remain 378m 21s) Loss: 0.0005(0.0217) Grad: 548.0841  LR: 0.000017  \n",
      "Epoch: [1][8000/36908] Elapsed 104m 20s (remain 376m 59s) Loss: 0.0012(0.0214) Grad: 1315.3030  LR: 0.000017  \n",
      "Epoch: [1][8100/36908] Elapsed 105m 37s (remain 375m 37s) Loss: 0.0019(0.0212) Grad: 7946.0513  LR: 0.000017  \n",
      "Epoch: [1][8200/36908] Elapsed 106m 56s (remain 374m 21s) Loss: 0.0003(0.0210) Grad: 33.7172  LR: 0.000017  \n",
      "Epoch: [1][8300/36908] Elapsed 108m 14s (remain 373m 1s) Loss: 0.0034(0.0208) Grad: 1639.5781  LR: 0.000017  \n",
      "Epoch: [1][8400/36908] Elapsed 109m 30s (remain 371m 35s) Loss: 0.0019(0.0206) Grad: 3728.8167  LR: 0.000017  \n",
      "Epoch: [1][8500/36908] Elapsed 110m 47s (remain 370m 13s) Loss: 0.0043(0.0204) Grad: 11131.5986  LR: 0.000017  \n",
      "Epoch: [1][8600/36908] Elapsed 112m 5s (remain 368m 55s) Loss: 0.0007(0.0202) Grad: 372.8084  LR: 0.000017  \n",
      "Epoch: [1][8700/36908] Elapsed 113m 22s (remain 367m 33s) Loss: 0.0007(0.0200) Grad: 108.2102  LR: 0.000017  \n",
      "Epoch: [1][8800/36908] Elapsed 114m 42s (remain 366m 19s) Loss: 0.0007(0.0199) Grad: 344.7707  LR: 0.000017  \n",
      "Epoch: [1][8900/36908] Elapsed 115m 59s (remain 364m 57s) Loss: 0.0061(0.0197) Grad: 9361.9258  LR: 0.000017  \n",
      "Epoch: [1][9000/36908] Elapsed 117m 18s (remain 363m 42s) Loss: 0.0009(0.0195) Grad: 632.5416  LR: 0.000017  \n",
      "Epoch: [1][9100/36908] Elapsed 118m 35s (remain 362m 21s) Loss: 0.0002(0.0193) Grad: 26.3566  LR: 0.000017  \n",
      "Epoch: [1][9200/36908] Elapsed 119m 53s (remain 361m 2s) Loss: 0.0004(0.0192) Grad: 2615.6704  LR: 0.000017  \n",
      "Epoch: [1][9300/36908] Elapsed 121m 17s (remain 360m 1s) Loss: 0.0034(0.0190) Grad: 10734.3936  LR: 0.000017  \n",
      "Epoch: [1][9400/36908] Elapsed 122m 36s (remain 358m 43s) Loss: 0.0042(0.0188) Grad: 5779.5142  LR: 0.000017  \n",
      "Epoch: [1][9500/36908] Elapsed 123m 52s (remain 357m 19s) Loss: 0.0001(0.0187) Grad: 51.3476  LR: 0.000017  \n",
      "Epoch: [1][9600/36908] Elapsed 125m 9s (remain 355m 58s) Loss: 0.0111(0.0185) Grad: 10127.5723  LR: 0.000016  \n",
      "Epoch: [1][9700/36908] Elapsed 126m 28s (remain 354m 42s) Loss: 0.0024(0.0184) Grad: 9639.0020  LR: 0.000016  \n",
      "Epoch: [1][9800/36908] Elapsed 127m 45s (remain 353m 21s) Loss: 0.0002(0.0182) Grad: 31.7028  LR: 0.000016  \n",
      "Epoch: [1][9900/36908] Elapsed 129m 3s (remain 352m 1s) Loss: 0.0025(0.0181) Grad: 2059.1741  LR: 0.000016  \n",
      "Epoch: [1][10000/36908] Elapsed 130m 22s (remain 350m 46s) Loss: 0.0005(0.0180) Grad: 241.5994  LR: 0.000016  \n",
      "Epoch: [1][10100/36908] Elapsed 131m 39s (remain 349m 24s) Loss: 0.0009(0.0178) Grad: 513.0019  LR: 0.000016  \n",
      "Epoch: [1][10200/36908] Elapsed 132m 55s (remain 348m 1s) Loss: 0.0094(0.0177) Grad: 21020.0508  LR: 0.000016  \n",
      "Epoch: [1][10300/36908] Elapsed 134m 13s (remain 346m 41s) Loss: 0.0002(0.0175) Grad: 49.3305  LR: 0.000016  \n",
      "Epoch: [1][10400/36908] Elapsed 135m 30s (remain 345m 19s) Loss: 0.0083(0.0174) Grad: 5424.1787  LR: 0.000016  \n",
      "Epoch: [1][10500/36908] Elapsed 136m 46s (remain 343m 56s) Loss: 0.0087(0.0173) Grad: 33238.6172  LR: 0.000016  \n",
      "Epoch: [1][10600/36908] Elapsed 138m 4s (remain 342m 37s) Loss: 0.0006(0.0172) Grad: 827.6033  LR: 0.000016  \n",
      "Epoch: [1][10700/36908] Elapsed 139m 21s (remain 341m 17s) Loss: 0.0014(0.0170) Grad: 1640.8196  LR: 0.000016  \n",
      "Epoch: [1][10800/36908] Elapsed 140m 36s (remain 339m 51s) Loss: 0.0096(0.0169) Grad: 10282.9199  LR: 0.000016  \n",
      "Epoch: [1][10900/36908] Elapsed 141m 52s (remain 338m 27s) Loss: 0.0002(0.0168) Grad: 34.1434  LR: 0.000016  \n",
      "Epoch: [1][11000/36908] Elapsed 143m 9s (remain 337m 7s) Loss: 0.0036(0.0166) Grad: 6418.3247  LR: 0.000016  \n",
      "Epoch: [1][11100/36908] Elapsed 144m 26s (remain 335m 46s) Loss: 0.0006(0.0165) Grad: 6111.2925  LR: 0.000016  \n",
      "Epoch: [1][11200/36908] Elapsed 145m 43s (remain 334m 26s) Loss: 0.0004(0.0164) Grad: 163.3242  LR: 0.000015  \n",
      "Epoch: [1][11300/36908] Elapsed 147m 1s (remain 333m 9s) Loss: 0.0015(0.0163) Grad: 4617.4219  LR: 0.000015  \n",
      "Epoch: [1][11400/36908] Elapsed 148m 17s (remain 331m 47s) Loss: 0.0039(0.0162) Grad: 5115.8105  LR: 0.000015  \n",
      "Epoch: [1][11500/36908] Elapsed 149m 35s (remain 330m 27s) Loss: 0.0044(0.0161) Grad: 2992.5398  LR: 0.000015  \n",
      "Epoch: [1][11600/36908] Elapsed 150m 52s (remain 329m 8s) Loss: 0.0005(0.0159) Grad: 194.7642  LR: 0.000015  \n",
      "Epoch: [1][11700/36908] Elapsed 152m 8s (remain 327m 46s) Loss: 0.0011(0.0158) Grad: 553.6075  LR: 0.000015  \n",
      "Epoch: [1][11800/36908] Elapsed 153m 27s (remain 326m 28s) Loss: 0.0001(0.0157) Grad: 36.9600  LR: 0.000015  \n",
      "Epoch: [1][11900/36908] Elapsed 154m 45s (remain 325m 11s) Loss: 0.0053(0.0156) Grad: 7007.5298  LR: 0.000015  \n",
      "Epoch: [1][12000/36908] Elapsed 156m 1s (remain 323m 48s) Loss: 0.0007(0.0155) Grad: 92.8735  LR: 0.000015  \n",
      "Epoch: [1][12100/36908] Elapsed 157m 17s (remain 322m 26s) Loss: 0.0021(0.0154) Grad: 3539.8088  LR: 0.000015  \n",
      "Epoch: [1][12200/36908] Elapsed 158m 33s (remain 321m 5s) Loss: 0.0009(0.0153) Grad: 4428.3940  LR: 0.000015  \n",
      "Epoch: [1][12300/36908] Elapsed 159m 51s (remain 319m 47s) Loss: 0.0011(0.0152) Grad: 158.4152  LR: 0.000015  \n",
      "Epoch: [1][12400/36908] Elapsed 161m 9s (remain 318m 28s) Loss: 0.0004(0.0151) Grad: 30.6790  LR: 0.000015  \n",
      "Epoch: [1][12500/36908] Elapsed 162m 26s (remain 317m 9s) Loss: 0.0145(0.0151) Grad: 41504.6914  LR: 0.000015  \n",
      "Epoch: [1][12600/36908] Elapsed 163m 43s (remain 315m 49s) Loss: 0.0136(0.0150) Grad: 14838.6240  LR: 0.000015  \n",
      "Epoch: [1][12700/36908] Elapsed 165m 0s (remain 314m 30s) Loss: 0.0002(0.0149) Grad: 10546.6416  LR: 0.000015  \n",
      "Epoch: [1][12800/36908] Elapsed 166m 17s (remain 313m 8s) Loss: 0.0031(0.0148) Grad: 12353.2217  LR: 0.000015  \n",
      "Epoch: [1][12900/36908] Elapsed 167m 33s (remain 311m 48s) Loss: 0.0004(0.0147) Grad: 9773.3545  LR: 0.000014  \n",
      "Epoch: [1][13000/36908] Elapsed 168m 49s (remain 310m 27s) Loss: 0.0018(0.0146) Grad: 1575.2280  LR: 0.000014  \n",
      "Epoch: [1][13100/36908] Elapsed 170m 8s (remain 309m 11s) Loss: 0.0011(0.0145) Grad: 7348.1089  LR: 0.000014  \n",
      "Epoch: [1][13200/36908] Elapsed 171m 26s (remain 307m 52s) Loss: 0.0013(0.0144) Grad: 5911.4761  LR: 0.000014  \n",
      "Epoch: [1][13300/36908] Elapsed 172m 44s (remain 306m 35s) Loss: 0.0134(0.0144) Grad: 22926.9707  LR: 0.000014  \n",
      "Epoch: [1][13400/36908] Elapsed 174m 1s (remain 305m 16s) Loss: 0.0002(0.0143) Grad: 673.6468  LR: 0.000014  \n",
      "Epoch: [1][13500/36908] Elapsed 175m 19s (remain 303m 57s) Loss: 0.0377(0.0142) Grad: 198273.5156  LR: 0.000014  \n",
      "Epoch: [1][13600/36908] Elapsed 176m 39s (remain 302m 42s) Loss: 0.0004(0.0141) Grad: 1783.7242  LR: 0.000014  \n",
      "Epoch: [1][13700/36908] Elapsed 178m 2s (remain 301m 33s) Loss: 0.0001(0.0140) Grad: 44.0361  LR: 0.000014  \n",
      "Epoch: [1][13800/36908] Elapsed 179m 24s (remain 300m 23s) Loss: 0.0087(0.0140) Grad: 24293.0781  LR: 0.000014  \n",
      "Epoch: [1][13900/36908] Elapsed 180m 47s (remain 299m 13s) Loss: 0.0064(0.0139) Grad: 15156.1436  LR: 0.000014  \n",
      "Epoch: [1][14000/36908] Elapsed 182m 12s (remain 298m 6s) Loss: 0.0005(0.0138) Grad: 3279.1350  LR: 0.000014  \n",
      "Epoch: [1][14100/36908] Elapsed 183m 36s (remain 296m 57s) Loss: 0.0031(0.0137) Grad: 10725.0215  LR: 0.000014  \n",
      "Epoch: [1][14200/36908] Elapsed 184m 59s (remain 295m 48s) Loss: 0.0032(0.0136) Grad: 4322.7070  LR: 0.000014  \n",
      "Epoch: [1][14300/36908] Elapsed 186m 23s (remain 294m 38s) Loss: 0.0024(0.0136) Grad: 18972.1719  LR: 0.000014  \n",
      "Epoch: [1][14400/36908] Elapsed 187m 43s (remain 293m 23s) Loss: 0.0051(0.0135) Grad: 7427.3374  LR: 0.000014  \n",
      "Epoch: [1][14500/36908] Elapsed 189m 0s (remain 292m 3s) Loss: 0.0001(0.0134) Grad: 22.9311  LR: 0.000013  \n",
      "Epoch: [1][14600/36908] Elapsed 190m 16s (remain 290m 42s) Loss: 0.0071(0.0134) Grad: 25023.4316  LR: 0.000013  \n",
      "Epoch: [1][14700/36908] Elapsed 191m 39s (remain 289m 30s) Loss: 0.0008(0.0133) Grad: 3710.3970  LR: 0.000013  \n",
      "Epoch: [1][14800/36908] Elapsed 192m 56s (remain 288m 10s) Loss: 0.0088(0.0132) Grad: 10076.1982  LR: 0.000013  \n",
      "Epoch: [1][14900/36908] Elapsed 194m 12s (remain 286m 49s) Loss: 0.0002(0.0132) Grad: 121.5955  LR: 0.000013  \n",
      "Epoch: [1][15000/36908] Elapsed 195m 30s (remain 285m 30s) Loss: 0.0003(0.0131) Grad: 275.6888  LR: 0.000013  \n",
      "Epoch: [1][15100/36908] Elapsed 196m 46s (remain 284m 9s) Loss: 0.0033(0.0130) Grad: 12585.7676  LR: 0.000013  \n",
      "Epoch: [1][15200/36908] Elapsed 198m 2s (remain 282m 48s) Loss: 0.0034(0.0130) Grad: 4233.1035  LR: 0.000013  \n",
      "Epoch: [1][15300/36908] Elapsed 199m 18s (remain 281m 26s) Loss: 0.0019(0.0129) Grad: 9444.9443  LR: 0.000013  \n",
      "Epoch: [1][15400/36908] Elapsed 200m 35s (remain 280m 7s) Loss: 0.0004(0.0128) Grad: 1922.7423  LR: 0.000013  \n",
      "Epoch: [1][15500/36908] Elapsed 201m 55s (remain 278m 51s) Loss: 0.0152(0.0128) Grad: 36163.5742  LR: 0.000013  \n",
      "Epoch: [1][15600/36908] Elapsed 203m 11s (remain 277m 30s) Loss: 0.0017(0.0127) Grad: 54109.1680  LR: 0.000013  \n",
      "Epoch: [1][15700/36908] Elapsed 204m 28s (remain 276m 10s) Loss: 0.0004(0.0127) Grad: 781.0988  LR: 0.000013  \n",
      "Epoch: [1][15800/36908] Elapsed 205m 44s (remain 274m 49s) Loss: 0.0020(0.0126) Grad: 22845.8965  LR: 0.000013  \n",
      "Epoch: [1][15900/36908] Elapsed 207m 1s (remain 273m 30s) Loss: 0.0002(0.0125) Grad: 86.1266  LR: 0.000013  \n",
      "Epoch: [1][16000/36908] Elapsed 208m 19s (remain 272m 12s) Loss: 0.0105(0.0125) Grad: 29609.5137  LR: 0.000013  \n",
      "Epoch: [1][16100/36908] Elapsed 209m 36s (remain 270m 52s) Loss: 0.0043(0.0124) Grad: 7243.4233  LR: 0.000013  \n",
      "Epoch: [1][16200/36908] Elapsed 210m 52s (remain 269m 32s) Loss: 0.0003(0.0124) Grad: 81.9302  LR: 0.000012  \n",
      "Epoch: [1][16300/36908] Elapsed 212m 8s (remain 268m 11s) Loss: 0.0005(0.0123) Grad: 89.7246  LR: 0.000012  \n",
      "Epoch: [1][16400/36908] Elapsed 213m 25s (remain 266m 51s) Loss: 0.0005(0.0123) Grad: 3089.2190  LR: 0.000012  \n",
      "Epoch: [1][16500/36908] Elapsed 214m 43s (remain 265m 32s) Loss: 0.0018(0.0122) Grad: 24936.5078  LR: 0.000012  \n",
      "Epoch: [1][16600/36908] Elapsed 215m 59s (remain 264m 12s) Loss: 0.0001(0.0121) Grad: 12.4461  LR: 0.000012  \n",
      "Epoch: [1][16700/36908] Elapsed 217m 14s (remain 262m 51s) Loss: 0.0106(0.0121) Grad: 128145.7422  LR: 0.000012  \n",
      "Epoch: [1][16800/36908] Elapsed 218m 33s (remain 261m 33s) Loss: 0.0012(0.0120) Grad: 1996.9596  LR: 0.000012  \n",
      "Epoch: [1][16900/36908] Elapsed 219m 50s (remain 260m 14s) Loss: 0.0028(0.0120) Grad: 8839.5186  LR: 0.000012  \n",
      "Epoch: [1][17000/36908] Elapsed 221m 6s (remain 258m 53s) Loss: 0.0002(0.0119) Grad: 250.9756  LR: 0.000012  \n",
      "Epoch: [1][17100/36908] Elapsed 222m 22s (remain 257m 33s) Loss: 0.0009(0.0119) Grad: 869.7329  LR: 0.000012  \n",
      "Epoch: [1][17200/36908] Elapsed 223m 38s (remain 256m 13s) Loss: 0.0002(0.0118) Grad: 179.0312  LR: 0.000012  \n",
      "Epoch: [1][17300/36908] Elapsed 224m 56s (remain 254m 55s) Loss: 0.0001(0.0118) Grad: 111.1477  LR: 0.000012  \n",
      "Epoch: [1][17400/36908] Elapsed 226m 13s (remain 253m 36s) Loss: 0.0059(0.0117) Grad: 248674.4062  LR: 0.000012  \n",
      "Epoch: [1][17500/36908] Elapsed 227m 29s (remain 252m 16s) Loss: 0.0011(0.0117) Grad: 1280.3208  LR: 0.000012  \n",
      "Epoch: [1][17600/36908] Elapsed 228m 45s (remain 250m 56s) Loss: 0.0003(0.0116) Grad: 2803.1792  LR: 0.000012  \n",
      "Epoch: [1][17700/36908] Elapsed 230m 2s (remain 249m 37s) Loss: 0.0028(0.0116) Grad: 20880.3887  LR: 0.000012  \n",
      "Epoch: [1][17800/36908] Elapsed 231m 20s (remain 248m 18s) Loss: 0.0056(0.0115) Grad: 43495.6992  LR: 0.000012  \n",
      "Epoch: [1][17900/36908] Elapsed 232m 38s (remain 247m 0s) Loss: 0.0645(0.0115) Grad: 155480.9844  LR: 0.000011  \n",
      "Epoch: [1][18000/36908] Elapsed 233m 54s (remain 245m 40s) Loss: 0.0013(0.0115) Grad: 5169.5923  LR: 0.000011  \n",
      "Epoch: [1][18100/36908] Elapsed 235m 10s (remain 244m 21s) Loss: 0.0002(0.0114) Grad: 254.7823  LR: 0.000011  \n",
      "Epoch: [1][18200/36908] Elapsed 236m 28s (remain 243m 3s) Loss: 0.0004(0.0114) Grad: 440.3450  LR: 0.000011  \n",
      "Epoch: [1][18300/36908] Elapsed 237m 47s (remain 241m 45s) Loss: 0.0008(0.0113) Grad: 11980.9961  LR: 0.000011  \n",
      "Epoch: [1][18400/36908] Elapsed 239m 5s (remain 240m 28s) Loss: 0.0115(0.0113) Grad: 96313.2812  LR: 0.000011  \n",
      "Epoch: [1][18500/36908] Elapsed 240m 27s (remain 239m 14s) Loss: 0.0002(0.0112) Grad: 88.5686  LR: 0.000011  \n",
      "Epoch: [1][18600/36908] Elapsed 241m 45s (remain 237m 56s) Loss: 0.0029(0.0112) Grad: 7973.2563  LR: 0.000011  \n",
      "Epoch: [1][18700/36908] Elapsed 243m 2s (remain 236m 37s) Loss: 0.0007(0.0111) Grad: 18825.7285  LR: 0.000011  \n",
      "Epoch: [1][18800/36908] Elapsed 244m 19s (remain 235m 18s) Loss: 0.0008(0.0111) Grad: 3264.5493  LR: 0.000011  \n",
      "Epoch: [1][18900/36908] Elapsed 245m 39s (remain 234m 1s) Loss: 0.0101(0.0111) Grad: 20483.8848  LR: 0.000011  \n",
      "Epoch: [1][19000/36908] Elapsed 247m 0s (remain 232m 46s) Loss: 0.0004(0.0110) Grad: 1672.7780  LR: 0.000011  \n",
      "Epoch: [1][19100/36908] Elapsed 248m 23s (remain 231m 33s) Loss: 0.0002(0.0110) Grad: 607.1412  LR: 0.000011  \n",
      "Epoch: [1][19200/36908] Elapsed 249m 47s (remain 230m 21s) Loss: 0.0009(0.0109) Grad: 2515.7087  LR: 0.000011  \n",
      "Epoch: [1][19300/36908] Elapsed 251m 7s (remain 229m 5s) Loss: 0.0009(0.0109) Grad: 14043.2373  LR: 0.000011  \n",
      "Epoch: [1][19400/36908] Elapsed 252m 25s (remain 227m 46s) Loss: 0.0034(0.0109) Grad: 52970.6172  LR: 0.000011  \n",
      "Epoch: [1][19500/36908] Elapsed 253m 42s (remain 226m 27s) Loss: 0.0007(0.0108) Grad: 5608.5825  LR: 0.000010  \n",
      "Epoch: [1][19600/36908] Elapsed 254m 58s (remain 225m 7s) Loss: 0.0019(0.0108) Grad: 5930.9326  LR: 0.000010  \n",
      "Epoch: [1][19700/36908] Elapsed 256m 14s (remain 223m 48s) Loss: 0.0003(0.0107) Grad: 221.4637  LR: 0.000010  \n",
      "Epoch: [1][19800/36908] Elapsed 257m 31s (remain 222m 29s) Loss: 0.0016(0.0107) Grad: 5339.5913  LR: 0.000010  \n",
      "Epoch: [1][19900/36908] Elapsed 258m 49s (remain 221m 11s) Loss: 0.0027(0.0107) Grad: 10378.6182  LR: 0.000010  \n",
      "Epoch: [1][20000/36908] Elapsed 260m 6s (remain 219m 52s) Loss: 0.0054(0.0106) Grad: 7989.4429  LR: 0.000010  \n",
      "Epoch: [1][20100/36908] Elapsed 261m 23s (remain 218m 33s) Loss: 0.0004(0.0106) Grad: 110.1870  LR: 0.000010  \n",
      "Epoch: [1][20200/36908] Elapsed 262m 41s (remain 217m 14s) Loss: 0.0002(0.0106) Grad: 114.8187  LR: 0.000010  \n",
      "Epoch: [1][20300/36908] Elapsed 264m 0s (remain 215m 57s) Loss: 0.0005(0.0105) Grad: 1844.7146  LR: 0.000010  \n",
      "Epoch: [1][20400/36908] Elapsed 265m 17s (remain 214m 39s) Loss: 0.0032(0.0105) Grad: 9003.9531  LR: 0.000010  \n",
      "Epoch: [1][20500/36908] Elapsed 266m 33s (remain 213m 19s) Loss: 0.0006(0.0104) Grad: 1492.4174  LR: 0.000010  \n",
      "Epoch: [1][20600/36908] Elapsed 267m 51s (remain 212m 1s) Loss: 0.0013(0.0104) Grad: 9288.2754  LR: 0.000010  \n",
      "Epoch: [1][20700/36908] Elapsed 269m 9s (remain 210m 43s) Loss: 0.0021(0.0104) Grad: 28671.3887  LR: 0.000010  \n",
      "Epoch: [1][20800/36908] Elapsed 270m 25s (remain 209m 24s) Loss: 0.0021(0.0103) Grad: 34346.0117  LR: 0.000010  \n",
      "Epoch: [1][20900/36908] Elapsed 271m 42s (remain 208m 5s) Loss: 0.0081(0.0103) Grad: 62802.1484  LR: 0.000010  \n",
      "Epoch: [1][21000/36908] Elapsed 273m 0s (remain 206m 47s) Loss: 0.0003(0.0103) Grad: 3937.1775  LR: 0.000010  \n",
      "Epoch: [1][21100/36908] Elapsed 274m 16s (remain 205m 27s) Loss: 0.0002(0.0102) Grad: 254.3925  LR: 0.000010  \n",
      "Epoch: [1][21200/36908] Elapsed 275m 33s (remain 204m 9s) Loss: 0.0001(0.0102) Grad: 1796.4220  LR: 0.000009  \n",
      "Epoch: [1][21300/36908] Elapsed 276m 51s (remain 202m 51s) Loss: 0.0002(0.0102) Grad: 475.2784  LR: 0.000009  \n",
      "Epoch: [1][21400/36908] Elapsed 278m 9s (remain 201m 32s) Loss: 0.0021(0.0101) Grad: 105359.4297  LR: 0.000009  \n",
      "Epoch: [1][21500/36908] Elapsed 279m 26s (remain 200m 14s) Loss: 0.0005(0.0101) Grad: 1046.5396  LR: 0.000009  \n",
      "Epoch: [1][21600/36908] Elapsed 280m 42s (remain 198m 55s) Loss: 0.0003(0.0101) Grad: 579.3145  LR: 0.000009  \n",
      "Epoch: [1][21700/36908] Elapsed 282m 1s (remain 197m 37s) Loss: 0.0002(0.0100) Grad: 1231.8147  LR: 0.000009  \n",
      "Epoch: [1][21800/36908] Elapsed 283m 17s (remain 196m 18s) Loss: 0.0005(0.0100) Grad: 1158.3561  LR: 0.000009  \n",
      "Epoch: [1][21900/36908] Elapsed 284m 33s (remain 194m 59s) Loss: 0.0003(0.0100) Grad: 1263.4061  LR: 0.000009  \n",
      "Epoch: [1][22000/36908] Elapsed 285m 51s (remain 193m 40s) Loss: 0.0004(0.0099) Grad: 291.9592  LR: 0.000009  \n",
      "Epoch: [1][22100/36908] Elapsed 287m 7s (remain 192m 21s) Loss: 0.0002(0.0099) Grad: 396.1313  LR: 0.000009  \n",
      "Epoch: [1][22200/36908] Elapsed 288m 23s (remain 191m 2s) Loss: 0.0003(0.0099) Grad: 167.6399  LR: 0.000009  \n",
      "Epoch: [1][22300/36908] Elapsed 289m 39s (remain 189m 43s) Loss: 0.0010(0.0098) Grad: 14859.3193  LR: 0.000009  \n",
      "Epoch: [1][22400/36908] Elapsed 290m 58s (remain 188m 26s) Loss: 0.0006(0.0098) Grad: 5726.2241  LR: 0.000009  \n",
      "Epoch: [1][22500/36908] Elapsed 292m 15s (remain 187m 7s) Loss: 0.0007(0.0098) Grad: 581.5569  LR: 0.000009  \n",
      "Epoch: [1][22600/36908] Elapsed 293m 33s (remain 185m 49s) Loss: 0.0002(0.0097) Grad: 928.9739  LR: 0.000009  \n",
      "Epoch: [1][22700/36908] Elapsed 294m 50s (remain 184m 31s) Loss: 0.0034(0.0097) Grad: 65149.8711  LR: 0.000009  \n",
      "Epoch: [1][22800/36908] Elapsed 296m 8s (remain 183m 13s) Loss: 0.0047(0.0097) Grad: 56999.1289  LR: 0.000008  \n",
      "Epoch: [1][22900/36908] Elapsed 297m 26s (remain 181m 55s) Loss: 0.0083(0.0097) Grad: 271682.3750  LR: 0.000008  \n",
      "Epoch: [1][23000/36908] Elapsed 298m 43s (remain 180m 37s) Loss: 0.0004(0.0096) Grad: 1993.1108  LR: 0.000008  \n",
      "Epoch: [1][23100/36908] Elapsed 300m 1s (remain 179m 18s) Loss: 0.0001(0.0096) Grad: 166.7306  LR: 0.000008  \n",
      "Epoch: [1][23200/36908] Elapsed 301m 18s (remain 178m 0s) Loss: 0.0000(0.0096) Grad: 349.3360  LR: 0.000008  \n",
      "Epoch: [1][23300/36908] Elapsed 302m 36s (remain 176m 42s) Loss: 0.0070(0.0095) Grad: 103777.1406  LR: 0.000008  \n",
      "Epoch: [1][23400/36908] Elapsed 303m 56s (remain 175m 26s) Loss: 0.0011(0.0095) Grad: 5003.5171  LR: 0.000008  \n",
      "Epoch: [1][23500/36908] Elapsed 305m 14s (remain 174m 8s) Loss: 0.0055(0.0095) Grad: 31202.5840  LR: 0.000008  \n",
      "Epoch: [1][23600/36908] Elapsed 306m 32s (remain 172m 50s) Loss: 0.0026(0.0094) Grad: 39565.0469  LR: 0.000008  \n",
      "Epoch: [1][23700/36908] Elapsed 307m 51s (remain 171m 33s) Loss: 0.0045(0.0094) Grad: 28613.3223  LR: 0.000008  \n",
      "Epoch: [1][23800/36908] Elapsed 309m 10s (remain 170m 15s) Loss: 0.0167(0.0094) Grad: 97780.8594  LR: 0.000008  \n",
      "Epoch: [1][23900/36908] Elapsed 310m 26s (remain 168m 56s) Loss: 0.0013(0.0094) Grad: 2791.7222  LR: 0.000008  \n",
      "Epoch: [1][24000/36908] Elapsed 311m 42s (remain 167m 37s) Loss: 0.0030(0.0093) Grad: 44167.3555  LR: 0.000008  \n",
      "Epoch: [1][24100/36908] Elapsed 313m 0s (remain 166m 19s) Loss: 0.0040(0.0093) Grad: 16776.4316  LR: 0.000008  \n",
      "Epoch: [1][24200/36908] Elapsed 314m 17s (remain 165m 1s) Loss: 0.0025(0.0093) Grad: 25263.4551  LR: 0.000008  \n",
      "Epoch: [1][24300/36908] Elapsed 315m 33s (remain 163m 42s) Loss: 0.0030(0.0092) Grad: 16015.4434  LR: 0.000008  \n",
      "Epoch: [1][24400/36908] Elapsed 316m 51s (remain 162m 24s) Loss: 0.0053(0.0092) Grad: 213426.1562  LR: 0.000008  \n",
      "Epoch: [1][24500/36908] Elapsed 318m 15s (remain 161m 9s) Loss: 0.0024(0.0092) Grad: 17608.5098  LR: 0.000007  \n",
      "Epoch: [1][24600/36908] Elapsed 319m 31s (remain 159m 50s) Loss: 0.0005(0.0092) Grad: 1369.1018  LR: 0.000007  \n",
      "Epoch: [1][24700/36908] Elapsed 320m 48s (remain 158m 32s) Loss: 0.0049(0.0091) Grad: 273723.4062  LR: 0.000007  \n",
      "Epoch: [1][24800/36908] Elapsed 322m 5s (remain 157m 14s) Loss: 0.0001(0.0091) Grad: 130.5851  LR: 0.000007  \n",
      "Epoch: [1][24900/36908] Elapsed 323m 22s (remain 155m 55s) Loss: 0.0012(0.0091) Grad: 5440.3950  LR: 0.000007  \n",
      "Epoch: [1][25000/36908] Elapsed 324m 39s (remain 154m 37s) Loss: 0.0009(0.0091) Grad: 24968.9707  LR: 0.000007  \n",
      "Epoch: [1][25100/36908] Elapsed 325m 57s (remain 153m 19s) Loss: 0.0034(0.0090) Grad: 32440.2207  LR: 0.000007  \n",
      "Epoch: [1][25200/36908] Elapsed 327m 15s (remain 152m 1s) Loss: 0.0010(0.0090) Grad: 8325.8740  LR: 0.000007  \n",
      "Epoch: [1][25300/36908] Elapsed 328m 31s (remain 150m 42s) Loss: 0.0021(0.0090) Grad: 59558.8672  LR: 0.000007  \n",
      "Epoch: [1][25400/36908] Elapsed 329m 49s (remain 149m 24s) Loss: 0.0016(0.0090) Grad: 135863.1250  LR: 0.000007  \n",
      "Epoch: [1][25500/36908] Elapsed 331m 6s (remain 148m 6s) Loss: 0.0002(0.0090) Grad: 1007.1163  LR: 0.000007  \n",
      "Epoch: [1][25600/36908] Elapsed 332m 23s (remain 146m 48s) Loss: 0.0001(0.0089) Grad: 1631.1226  LR: 0.000007  \n",
      "Epoch: [1][25700/36908] Elapsed 333m 39s (remain 145m 29s) Loss: 0.0011(0.0089) Grad: 36611.1875  LR: 0.000007  \n",
      "Epoch: [1][25800/36908] Elapsed 335m 2s (remain 144m 13s) Loss: 0.0025(0.0089) Grad: 40584.9180  LR: 0.000007  \n",
      "Epoch: [1][25900/36908] Elapsed 336m 25s (remain 142m 58s) Loss: 0.0004(0.0089) Grad: 21866.0254  LR: 0.000007  \n",
      "Epoch: [1][26000/36908] Elapsed 337m 48s (remain 141m 42s) Loss: 0.0004(0.0088) Grad: 692.3323  LR: 0.000007  \n",
      "Epoch: [1][26100/36908] Elapsed 339m 12s (remain 140m 26s) Loss: 0.0025(0.0088) Grad: 8767.4775  LR: 0.000007  \n",
      "Epoch: [1][26200/36908] Elapsed 340m 33s (remain 139m 9s) Loss: 0.0064(0.0088) Grad: 128177.3438  LR: 0.000006  \n",
      "Epoch: [1][26300/36908] Elapsed 341m 49s (remain 137m 51s) Loss: 0.0001(0.0088) Grad: 148.9412  LR: 0.000006  \n",
      "Epoch: [1][26400/36908] Elapsed 343m 7s (remain 136m 33s) Loss: 0.0009(0.0087) Grad: 7970.0801  LR: 0.000006  \n",
      "Epoch: [1][26500/36908] Elapsed 344m 30s (remain 135m 17s) Loss: 0.0006(0.0087) Grad: 6449.4902  LR: 0.000006  \n",
      "Epoch: [1][26600/36908] Elapsed 345m 46s (remain 133m 58s) Loss: 0.0029(0.0087) Grad: 23745.6172  LR: 0.000006  \n",
      "Epoch: [1][26700/36908] Elapsed 347m 5s (remain 132m 40s) Loss: 0.0092(0.0087) Grad: 59595.5938  LR: 0.000006  \n",
      "Epoch: [1][26800/36908] Elapsed 348m 21s (remain 131m 22s) Loss: 0.0005(0.0087) Grad: 1342.7212  LR: 0.000006  \n",
      "Epoch: [1][26900/36908] Elapsed 349m 38s (remain 130m 3s) Loss: 0.0016(0.0086) Grad: 4100.8896  LR: 0.000006  \n",
      "Epoch: [1][27000/36908] Elapsed 350m 53s (remain 128m 44s) Loss: 0.0054(0.0086) Grad: 98184.3984  LR: 0.000006  \n",
      "Epoch: [1][27100/36908] Elapsed 352m 9s (remain 127m 26s) Loss: 0.0004(0.0086) Grad: 1783.5978  LR: 0.000006  \n",
      "Epoch: [1][27200/36908] Elapsed 353m 26s (remain 126m 7s) Loss: 0.0002(0.0086) Grad: 711.0630  LR: 0.000006  \n",
      "Epoch: [1][27300/36908] Elapsed 354m 44s (remain 124m 49s) Loss: 0.0012(0.0086) Grad: 5095.7705  LR: 0.000006  \n",
      "Epoch: [1][27400/36908] Elapsed 356m 1s (remain 123m 31s) Loss: 0.0032(0.0085) Grad: 17861.1445  LR: 0.000006  \n",
      "Epoch: [1][27500/36908] Elapsed 357m 16s (remain 122m 12s) Loss: 0.0114(0.0085) Grad: 82984.2109  LR: 0.000006  \n",
      "Epoch: [1][27600/36908] Elapsed 358m 33s (remain 120m 54s) Loss: 0.0004(0.0085) Grad: 261.8690  LR: 0.000006  \n",
      "Epoch: [1][27700/36908] Elapsed 359m 52s (remain 119m 36s) Loss: 0.0069(0.0085) Grad: 59508.1445  LR: 0.000006  \n",
      "Epoch: [1][27800/36908] Elapsed 361m 9s (remain 118m 18s) Loss: 0.0012(0.0085) Grad: 20558.9082  LR: 0.000005  \n",
      "Epoch: [1][27900/36908] Elapsed 362m 27s (remain 117m 0s) Loss: 0.0095(0.0084) Grad: 513677.5938  LR: 0.000005  \n",
      "Epoch: [1][28000/36908] Elapsed 363m 43s (remain 115m 42s) Loss: 0.0002(0.0084) Grad: 48.9581  LR: 0.000005  \n",
      "Epoch: [1][28100/36908] Elapsed 365m 1s (remain 114m 23s) Loss: 0.0001(0.0084) Grad: 374.1752  LR: 0.000005  \n",
      "Epoch: [1][28200/36908] Elapsed 366m 21s (remain 113m 6s) Loss: 0.0001(0.0084) Grad: 643.1739  LR: 0.000005  \n",
      "Epoch: [1][28300/36908] Elapsed 367m 40s (remain 111m 49s) Loss: 0.0041(0.0083) Grad: 99879.0781  LR: 0.000005  \n",
      "Epoch: [1][28400/36908] Elapsed 368m 57s (remain 110m 31s) Loss: 0.0081(0.0083) Grad: 203716.5469  LR: 0.000005  \n",
      "Epoch: [1][28500/36908] Elapsed 370m 14s (remain 109m 12s) Loss: 0.0014(0.0083) Grad: 26349.9648  LR: 0.000005  \n",
      "Epoch: [1][28600/36908] Elapsed 371m 31s (remain 107m 54s) Loss: 0.0015(0.0083) Grad: 29596.3516  LR: 0.000005  \n",
      "Epoch: [1][28700/36908] Elapsed 372m 49s (remain 106m 36s) Loss: 0.0016(0.0083) Grad: 5158.4287  LR: 0.000005  \n",
      "Epoch: [1][28800/36908] Elapsed 374m 7s (remain 105m 18s) Loss: 0.0006(0.0083) Grad: 4682.2627  LR: 0.000005  \n",
      "Epoch: [1][28900/36908] Elapsed 375m 25s (remain 104m 0s) Loss: 0.0021(0.0082) Grad: 116797.6953  LR: 0.000005  \n",
      "Epoch: [1][29000/36908] Elapsed 376m 43s (remain 102m 42s) Loss: 0.0004(0.0082) Grad: 760.1837  LR: 0.000005  \n",
      "Epoch: [1][29100/36908] Elapsed 378m 1s (remain 101m 24s) Loss: 0.0072(0.0082) Grad: 121113.9453  LR: 0.000005  \n",
      "Epoch: [1][29200/36908] Elapsed 379m 18s (remain 100m 6s) Loss: 0.0031(0.0082) Grad: 14193.7031  LR: 0.000005  \n",
      "Epoch: [1][29300/36908] Elapsed 380m 37s (remain 98m 48s) Loss: 0.0041(0.0082) Grad: 116586.8281  LR: 0.000005  \n",
      "Epoch: [1][29400/36908] Elapsed 381m 54s (remain 97m 30s) Loss: 0.0017(0.0081) Grad: 7182.7319  LR: 0.000005  \n",
      "Epoch: [1][29500/36908] Elapsed 383m 12s (remain 96m 12s) Loss: 0.0025(0.0081) Grad: 8550.0059  LR: 0.000004  \n",
      "Epoch: [1][29600/36908] Elapsed 384m 33s (remain 94m 55s) Loss: 0.0003(0.0081) Grad: 1149.5179  LR: 0.000004  \n",
      "Epoch: [1][29700/36908] Elapsed 385m 53s (remain 93m 38s) Loss: 0.0132(0.0081) Grad: 214723.5156  LR: 0.000004  \n",
      "Epoch: [1][29800/36908] Elapsed 387m 12s (remain 92m 20s) Loss: 0.0038(0.0081) Grad: 185048.3438  LR: 0.000004  \n",
      "Epoch: [1][29900/36908] Elapsed 388m 30s (remain 91m 2s) Loss: 0.0020(0.0080) Grad: 52568.9766  LR: 0.000004  \n",
      "Epoch: [1][30000/36908] Elapsed 389m 54s (remain 89m 45s) Loss: 0.0008(0.0080) Grad: 4002.7556  LR: 0.000004  \n",
      "Epoch: [1][30100/36908] Elapsed 391m 17s (remain 88m 29s) Loss: 0.0001(0.0080) Grad: 113.2610  LR: 0.000004  \n",
      "Epoch: [1][30200/36908] Elapsed 392m 40s (remain 87m 12s) Loss: 0.0078(0.0080) Grad: 324391.5312  LR: 0.000004  \n",
      "Epoch: [1][30300/36908] Elapsed 394m 5s (remain 85m 55s) Loss: 0.0001(0.0080) Grad: 66.5301  LR: 0.000004  \n",
      "Epoch: [1][30400/36908] Elapsed 395m 26s (remain 84m 38s) Loss: 0.0036(0.0080) Grad: 26495.3477  LR: 0.000004  \n",
      "Epoch: [1][30500/36908] Elapsed 396m 43s (remain 83m 20s) Loss: 0.0022(0.0079) Grad: 7992.6670  LR: 0.000004  \n",
      "Epoch: [1][30600/36908] Elapsed 398m 1s (remain 82m 2s) Loss: 0.0015(0.0079) Grad: 34904.5859  LR: 0.000004  \n",
      "Epoch: [1][30700/36908] Elapsed 399m 20s (remain 80m 44s) Loss: 0.0005(0.0079) Grad: 576.0126  LR: 0.000004  \n",
      "Epoch: [1][30800/36908] Elapsed 400m 37s (remain 79m 25s) Loss: 0.0039(0.0079) Grad: 97248.5000  LR: 0.000004  \n",
      "Epoch: [1][30900/36908] Elapsed 401m 54s (remain 78m 7s) Loss: 0.0004(0.0079) Grad: 1069.7686  LR: 0.000004  \n",
      "Epoch: [1][31000/36908] Elapsed 403m 12s (remain 76m 49s) Loss: 0.0003(0.0079) Grad: 532.8948  LR: 0.000004  \n",
      "Epoch: [1][31100/36908] Elapsed 404m 30s (remain 75m 31s) Loss: 0.0014(0.0078) Grad: 1438.1326  LR: 0.000003  \n",
      "Epoch: [1][31200/36908] Elapsed 405m 47s (remain 74m 13s) Loss: 0.0000(0.0078) Grad: 19.5625  LR: 0.000003  \n",
      "Epoch: [1][31300/36908] Elapsed 407m 4s (remain 72m 55s) Loss: 0.0003(0.0078) Grad: 178.3665  LR: 0.000003  \n",
      "Epoch: [1][31400/36908] Elapsed 408m 22s (remain 71m 37s) Loss: 0.0002(0.0078) Grad: 1175.4015  LR: 0.000003  \n",
      "Epoch: [1][31500/36908] Elapsed 409m 39s (remain 70m 19s) Loss: 0.0022(0.0078) Grad: 9148.0850  LR: 0.000003  \n",
      "Epoch: [1][31600/36908] Elapsed 410m 56s (remain 69m 0s) Loss: 0.0002(0.0078) Grad: 221.0098  LR: 0.000003  \n",
      "Epoch: [1][31700/36908] Elapsed 412m 12s (remain 67m 42s) Loss: 0.0050(0.0078) Grad: 19091.9316  LR: 0.000003  \n",
      "Epoch: [1][31800/36908] Elapsed 413m 29s (remain 66m 24s) Loss: 0.0002(0.0077) Grad: 46.1064  LR: 0.000003  \n",
      "Epoch: [1][31900/36908] Elapsed 414m 46s (remain 65m 5s) Loss: 0.0010(0.0077) Grad: 7860.2764  LR: 0.000003  \n",
      "Epoch: [1][32000/36908] Elapsed 416m 2s (remain 63m 47s) Loss: 0.0002(0.0077) Grad: 52.2645  LR: 0.000003  \n",
      "Epoch: [1][32100/36908] Elapsed 417m 19s (remain 62m 29s) Loss: 0.0002(0.0077) Grad: 58.2287  LR: 0.000003  \n",
      "Epoch: [1][32200/36908] Elapsed 418m 35s (remain 61m 11s) Loss: 0.0018(0.0077) Grad: 7797.1836  LR: 0.000003  \n",
      "Epoch: [1][32300/36908] Elapsed 419m 52s (remain 59m 53s) Loss: 0.0017(0.0077) Grad: 15209.8125  LR: 0.000003  \n",
      "Epoch: [1][32400/36908] Elapsed 421m 11s (remain 58m 35s) Loss: 0.0081(0.0076) Grad: 9814.2314  LR: 0.000003  \n",
      "Epoch: [1][32500/36908] Elapsed 422m 29s (remain 57m 17s) Loss: 0.0001(0.0076) Grad: 106.8624  LR: 0.000003  \n",
      "Epoch: [1][32600/36908] Elapsed 423m 49s (remain 55m 59s) Loss: 0.0019(0.0076) Grad: 3781.4028  LR: 0.000003  \n",
      "Epoch: [1][32700/36908] Elapsed 425m 7s (remain 54m 41s) Loss: 0.0051(0.0076) Grad: 16072.9561  LR: 0.000003  \n",
      "Epoch: [1][32800/36908] Elapsed 426m 25s (remain 53m 23s) Loss: 0.0010(0.0076) Grad: 1911.7974  LR: 0.000002  \n",
      "Epoch: [1][32900/36908] Elapsed 427m 43s (remain 52m 5s) Loss: 0.0038(0.0076) Grad: 19738.2715  LR: 0.000002  \n",
      "Epoch: [1][33000/36908] Elapsed 428m 59s (remain 50m 47s) Loss: 0.0000(0.0076) Grad: 36.8495  LR: 0.000002  \n",
      "Epoch: [1][33100/36908] Elapsed 430m 16s (remain 49m 29s) Loss: 0.0083(0.0076) Grad: 10698.5908  LR: 0.000002  \n",
      "Epoch: [1][33200/36908] Elapsed 431m 34s (remain 48m 11s) Loss: 0.0001(0.0075) Grad: 89.8167  LR: 0.000002  \n",
      "Epoch: [1][33300/36908] Elapsed 432m 53s (remain 46m 53s) Loss: 0.0001(0.0075) Grad: 28.1283  LR: 0.000002  \n",
      "Epoch: [1][33400/36908] Elapsed 434m 11s (remain 45m 35s) Loss: 0.0031(0.0075) Grad: 31362.6055  LR: 0.000002  \n",
      "Epoch: [1][33500/36908] Elapsed 435m 26s (remain 44m 17s) Loss: 0.0017(0.0075) Grad: 14633.3418  LR: 0.000002  \n",
      "Epoch: [1][33600/36908] Elapsed 436m 42s (remain 42m 58s) Loss: 0.0041(0.0075) Grad: 20416.7500  LR: 0.000002  \n",
      "Epoch: [1][33700/36908] Elapsed 438m 0s (remain 41m 40s) Loss: 0.0015(0.0075) Grad: 3420.5193  LR: 0.000002  \n",
      "Epoch: [1][33800/36908] Elapsed 439m 19s (remain 40m 22s) Loss: 0.0007(0.0075) Grad: 1566.9615  LR: 0.000002  \n",
      "Epoch: [1][33900/36908] Elapsed 440m 36s (remain 39m 4s) Loss: 0.0069(0.0074) Grad: 32138.2070  LR: 0.000002  \n",
      "Epoch: [1][34000/36908] Elapsed 441m 54s (remain 37m 46s) Loss: 0.0019(0.0074) Grad: 1171.5406  LR: 0.000002  \n",
      "Epoch: [1][34100/36908] Elapsed 443m 14s (remain 36m 29s) Loss: 0.0104(0.0074) Grad: 69137.3125  LR: 0.000002  \n",
      "Epoch: [1][34200/36908] Elapsed 444m 32s (remain 35m 11s) Loss: 0.0032(0.0074) Grad: 26652.2773  LR: 0.000002  \n",
      "Epoch: [1][34300/36908] Elapsed 445m 49s (remain 33m 53s) Loss: 0.0002(0.0074) Grad: 268.2532  LR: 0.000002  \n",
      "Epoch: [1][34400/36908] Elapsed 447m 7s (remain 32m 35s) Loss: 0.0004(0.0074) Grad: 1190.7791  LR: 0.000002  \n",
      "Epoch: [1][34500/36908] Elapsed 448m 23s (remain 31m 16s) Loss: 0.0041(0.0074) Grad: 45253.3359  LR: 0.000001  \n",
      "Epoch: [1][34600/36908] Elapsed 449m 40s (remain 29m 58s) Loss: 0.0072(0.0073) Grad: 194349.0469  LR: 0.000001  \n",
      "Epoch: [1][34700/36908] Elapsed 450m 59s (remain 28m 40s) Loss: 0.0006(0.0073) Grad: 1486.6738  LR: 0.000001  \n",
      "Epoch: [1][34800/36908] Elapsed 452m 16s (remain 27m 22s) Loss: 0.0005(0.0073) Grad: 10729.8838  LR: 0.000001  \n",
      "Epoch: [1][34900/36908] Elapsed 453m 36s (remain 26m 5s) Loss: 0.0002(0.0073) Grad: 104.7141  LR: 0.000001  \n",
      "Epoch: [1][35000/36908] Elapsed 454m 53s (remain 24m 47s) Loss: 0.0007(0.0073) Grad: 1255.4767  LR: 0.000001  \n",
      "Epoch: [1][35100/36908] Elapsed 456m 11s (remain 23m 29s) Loss: 0.0024(0.0073) Grad: 104254.5391  LR: 0.000001  \n",
      "Epoch: [1][35200/36908] Elapsed 457m 30s (remain 22m 11s) Loss: 0.0010(0.0073) Grad: 4310.7617  LR: 0.000001  \n",
      "Epoch: [1][35300/36908] Elapsed 458m 51s (remain 20m 53s) Loss: 0.0034(0.0073) Grad: 103517.3672  LR: 0.000001  \n",
      "Epoch: [1][35400/36908] Elapsed 460m 8s (remain 19m 35s) Loss: 0.0037(0.0073) Grad: 53424.9883  LR: 0.000001  \n",
      "Epoch: [1][35500/36908] Elapsed 461m 26s (remain 18m 17s) Loss: 0.0002(0.0072) Grad: 4015.8123  LR: 0.000001  \n",
      "Epoch: [1][35600/36908] Elapsed 462m 44s (remain 16m 59s) Loss: 0.0064(0.0072) Grad: 159435.4062  LR: 0.000001  \n",
      "Epoch: [1][35700/36908] Elapsed 464m 1s (remain 15m 41s) Loss: 0.0004(0.0072) Grad: 1175.4285  LR: 0.000001  \n",
      "Epoch: [1][35800/36908] Elapsed 465m 20s (remain 14m 23s) Loss: 0.0001(0.0072) Grad: 86.0884  LR: 0.000001  \n",
      "Epoch: [1][35900/36908] Elapsed 466m 43s (remain 13m 5s) Loss: 0.0002(0.0072) Grad: 241.2455  LR: 0.000001  \n",
      "Epoch: [1][36000/36908] Elapsed 468m 1s (remain 11m 47s) Loss: 0.0002(0.0072) Grad: 510.7772  LR: 0.000001  \n",
      "Epoch: [1][36100/36908] Elapsed 469m 18s (remain 10m 29s) Loss: 0.0043(0.0072) Grad: 40924.0000  LR: 0.000000  \n",
      "Epoch: [1][36200/36908] Elapsed 470m 35s (remain 9m 11s) Loss: 0.0007(0.0072) Grad: 3504.4414  LR: 0.000000  \n",
      "Epoch: [1][36300/36908] Elapsed 471m 51s (remain 7m 53s) Loss: 0.0008(0.0071) Grad: 664.4585  LR: 0.000000  \n",
      "Epoch: [1][36400/36908] Elapsed 473m 8s (remain 6m 35s) Loss: 0.0023(0.0071) Grad: 17285.9434  LR: 0.000000  \n",
      "Epoch: [1][36500/36908] Elapsed 474m 24s (remain 5m 17s) Loss: 0.0008(0.0071) Grad: 3153.2053  LR: 0.000000  \n",
      "Epoch: [1][36600/36908] Elapsed 475m 40s (remain 3m 59s) Loss: 0.0037(0.0071) Grad: 5297.9917  LR: 0.000000  \n",
      "Epoch: [1][36700/36908] Elapsed 476m 56s (remain 2m 41s) Loss: 0.0006(0.0071) Grad: 1558.0240  LR: 0.000000  \n",
      "Epoch: [1][36800/36908] Elapsed 478m 11s (remain 1m 23s) Loss: 0.0001(0.0071) Grad: 365.2860  LR: 0.000000  \n",
      "Epoch: [1][36900/36908] Elapsed 479m 27s (remain 0m 5s) Loss: 0.0110(0.0071) Grad: 188493.9062  LR: 0.000000  \n",
      "Epoch: [1][36907/36908] Elapsed 479m 33s (remain 0m 0s) Loss: 0.0002(0.0071) Grad: 213.7045  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 14m 19s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 32s (remain 5m 53s) Loss: 0.0307(0.0072) \n",
      "EVAL: [200/1192] Elapsed 1m 2s (remain 5m 8s) Loss: 0.0088(0.0070) \n",
      "EVAL: [300/1192] Elapsed 1m 32s (remain 4m 33s) Loss: 0.0100(0.0066) \n",
      "EVAL: [400/1192] Elapsed 2m 1s (remain 4m 0s) Loss: 0.0001(0.0070) \n",
      "EVAL: [500/1192] Elapsed 2m 31s (remain 3m 29s) Loss: 0.0000(0.0064) \n",
      "EVAL: [600/1192] Elapsed 3m 2s (remain 2m 59s) Loss: 0.0050(0.0066) \n",
      "EVAL: [700/1192] Elapsed 3m 33s (remain 2m 29s) Loss: 0.0099(0.0074) \n",
      "EVAL: [800/1192] Elapsed 4m 4s (remain 1m 59s) Loss: 0.0000(0.0074) \n",
      "EVAL: [900/1192] Elapsed 4m 34s (remain 1m 28s) Loss: 0.0082(0.0075) \n",
      "EVAL: [1000/1192] Elapsed 5m 4s (remain 0m 58s) Loss: 0.0001(0.0072) \n",
      "EVAL: [1100/1192] Elapsed 5m 36s (remain 0m 27s) Loss: 0.0294(0.0070) \n",
      "EVAL: [1191/1192] Elapsed 6m 4s (remain 0m 0s) Loss: 0.0000(0.0067) \n",
      "Epoch 1 - avg_train_loss: 0.0071  avg_val_loss: 0.0067  time: 29141s\n",
      "Epoch 1 - Score: 0.8911\n",
      "Epoch 1 - Save Best Score: 0.8911 Model\n",
      "========== fold: 3 training ==========\n",
      "get pseudo plain from ../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_3.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_3.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp083/pseudo_labels_3.npy\n",
      "(612602, 6) (612602, 950)\n",
      "(100000, 7)\n",
      "(110725, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\n",
      "Epoch: [1][0/36908] Elapsed 0m 1s (remain 800m 13s) Loss: 0.3540(0.3540) Grad: 133015.8594  LR: 0.000000  \n",
      "Epoch: [1][100/36908] Elapsed 1m 20s (remain 487m 14s) Loss: 0.3240(0.3436) Grad: 118244.6953  LR: 0.000001  \n",
      "Epoch: [1][200/36908] Elapsed 2m 37s (remain 480m 40s) Loss: 0.2452(0.3169) Grad: 100577.1328  LR: 0.000001  \n",
      "Epoch: [1][300/36908] Elapsed 3m 56s (remain 479m 51s) Loss: 0.1553(0.2774) Grad: 68106.3750  LR: 0.000002  \n",
      "Epoch: [1][400/36908] Elapsed 5m 18s (remain 483m 8s) Loss: 0.0664(0.2339) Grad: 26025.0078  LR: 0.000002  \n",
      "Epoch: [1][500/36908] Elapsed 6m 37s (remain 482m 1s) Loss: 0.0525(0.1970) Grad: 3521.9177  LR: 0.000003  \n",
      "Epoch: [1][600/36908] Elapsed 8m 2s (remain 485m 45s) Loss: 0.0339(0.1709) Grad: 4590.9839  LR: 0.000003  \n",
      "Epoch: [1][700/36908] Elapsed 9m 26s (remain 487m 53s) Loss: 0.0223(0.1520) Grad: 3869.6973  LR: 0.000004  \n",
      "Epoch: [1][800/36908] Elapsed 10m 50s (remain 488m 55s) Loss: 0.0339(0.1373) Grad: 4908.1963  LR: 0.000004  \n",
      "Epoch: [1][900/36908] Elapsed 12m 14s (remain 489m 27s) Loss: 0.0121(0.1262) Grad: 6574.8535  LR: 0.000005  \n",
      "Epoch: [1][1000/36908] Elapsed 13m 38s (remain 489m 34s) Loss: 0.0390(0.1173) Grad: 7346.7871  LR: 0.000005  \n",
      "Epoch: [1][1100/36908] Elapsed 15m 5s (remain 491m 4s) Loss: 0.0454(0.1099) Grad: 11914.4990  LR: 0.000006  \n",
      "Epoch: [1][1200/36908] Elapsed 16m 30s (remain 491m 0s) Loss: 0.0116(0.1036) Grad: 13328.6094  LR: 0.000007  \n",
      "Epoch: [1][1300/36908] Elapsed 17m 55s (remain 490m 32s) Loss: 0.0311(0.0977) Grad: 48626.2969  LR: 0.000007  \n",
      "Epoch: [1][1400/36908] Elapsed 19m 20s (remain 490m 18s) Loss: 0.0100(0.0923) Grad: 39800.8008  LR: 0.000008  \n",
      "Epoch: [1][1500/36908] Elapsed 20m 45s (remain 489m 32s) Loss: 0.0316(0.0874) Grad: 9279.0615  LR: 0.000008  \n",
      "Epoch: [1][1600/36908] Elapsed 22m 9s (remain 488m 35s) Loss: 0.0026(0.0830) Grad: 13084.4834  LR: 0.000009  \n",
      "Epoch: [1][1700/36908] Elapsed 23m 33s (remain 487m 27s) Loss: 0.0054(0.0790) Grad: 8754.7910  LR: 0.000009  \n",
      "Epoch: [1][1800/36908] Elapsed 24m 51s (remain 484m 41s) Loss: 0.0076(0.0756) Grad: 17551.4258  LR: 0.000010  \n",
      "Epoch: [1][1900/36908] Elapsed 26m 9s (remain 481m 41s) Loss: 0.0089(0.0723) Grad: 27704.3848  LR: 0.000010  \n",
      "Epoch: [1][2000/36908] Elapsed 27m 27s (remain 478m 55s) Loss: 0.0174(0.0694) Grad: 54357.5195  LR: 0.000011  \n",
      "Epoch: [1][2100/36908] Elapsed 28m 45s (remain 476m 33s) Loss: 0.0169(0.0668) Grad: 41285.7734  LR: 0.000011  \n",
      "Epoch: [1][2200/36908] Elapsed 30m 4s (remain 474m 7s) Loss: 0.0056(0.0644) Grad: 20498.9219  LR: 0.000012  \n",
      "Epoch: [1][2300/36908] Elapsed 31m 26s (remain 472m 53s) Loss: 0.0052(0.0621) Grad: 2164.4851  LR: 0.000012  \n",
      "Epoch: [1][2400/36908] Elapsed 32m 45s (remain 470m 51s) Loss: 0.0054(0.0602) Grad: 10225.1270  LR: 0.000013  \n",
      "Epoch: [1][2500/36908] Elapsed 34m 3s (remain 468m 36s) Loss: 0.0088(0.0583) Grad: 35679.5234  LR: 0.000014  \n",
      "Epoch: [1][2600/36908] Elapsed 35m 20s (remain 466m 9s) Loss: 0.0049(0.0565) Grad: 9006.8398  LR: 0.000014  \n",
      "Epoch: [1][2700/36908] Elapsed 36m 38s (remain 463m 56s) Loss: 0.0036(0.0547) Grad: 8464.5215  LR: 0.000015  \n",
      "Epoch: [1][2800/36908] Elapsed 37m 56s (remain 462m 4s) Loss: 0.0007(0.0532) Grad: 1884.7246  LR: 0.000015  \n",
      "Epoch: [1][2900/36908] Elapsed 39m 15s (remain 460m 15s) Loss: 0.0038(0.0518) Grad: 8571.8672  LR: 0.000016  \n",
      "Epoch: [1][3000/36908] Elapsed 40m 33s (remain 458m 10s) Loss: 0.0048(0.0504) Grad: 8773.5273  LR: 0.000016  \n",
      "Epoch: [1][3100/36908] Elapsed 41m 50s (remain 456m 7s) Loss: 0.0031(0.0491) Grad: 5622.8101  LR: 0.000017  \n",
      "Epoch: [1][3200/36908] Elapsed 43m 9s (remain 454m 28s) Loss: 0.0024(0.0479) Grad: 2568.5435  LR: 0.000017  \n",
      "Epoch: [1][3300/36908] Elapsed 44m 35s (remain 454m 1s) Loss: 0.0025(0.0467) Grad: 13682.4043  LR: 0.000018  \n",
      "Epoch: [1][3400/36908] Elapsed 45m 55s (remain 452m 25s) Loss: 0.0028(0.0456) Grad: 2408.4810  LR: 0.000018  \n",
      "Epoch: [1][3500/36908] Elapsed 47m 14s (remain 450m 46s) Loss: 0.0284(0.0447) Grad: 59978.0469  LR: 0.000019  \n",
      "Epoch: [1][3600/36908] Elapsed 48m 33s (remain 449m 8s) Loss: 0.0063(0.0436) Grad: 5090.3779  LR: 0.000020  \n",
      "Epoch: [1][3700/36908] Elapsed 49m 52s (remain 447m 29s) Loss: 0.0059(0.0427) Grad: 75756.1484  LR: 0.000020  \n",
      "Epoch: [1][3800/36908] Elapsed 51m 14s (remain 446m 19s) Loss: 0.0008(0.0419) Grad: 3416.3086  LR: 0.000020  \n",
      "Epoch: [1][3900/36908] Elapsed 52m 32s (remain 444m 35s) Loss: 0.0317(0.0410) Grad: 78089.2578  LR: 0.000020  \n",
      "Epoch: [1][4000/36908] Elapsed 53m 52s (remain 443m 3s) Loss: 0.0095(0.0402) Grad: 96704.6250  LR: 0.000020  \n",
      "Epoch: [1][4100/36908] Elapsed 55m 12s (remain 441m 37s) Loss: 0.0023(0.0394) Grad: 13828.1787  LR: 0.000020  \n",
      "Epoch: [1][4200/36908] Elapsed 56m 29s (remain 439m 49s) Loss: 0.0045(0.0386) Grad: 33992.8242  LR: 0.000020  \n",
      "Epoch: [1][4300/36908] Elapsed 57m 48s (remain 438m 16s) Loss: 0.0039(0.0379) Grad: 2415.7539  LR: 0.000020  \n",
      "Epoch: [1][4400/36908] Elapsed 59m 7s (remain 436m 46s) Loss: 0.0033(0.0372) Grad: 15749.7373  LR: 0.000020  \n",
      "Epoch: [1][4500/36908] Elapsed 60m 26s (remain 435m 13s) Loss: 0.0009(0.0365) Grad: 1038.7990  LR: 0.000020  \n",
      "Epoch: [1][4600/36908] Elapsed 61m 44s (remain 433m 35s) Loss: 0.0070(0.0359) Grad: 21902.4062  LR: 0.000019  \n",
      "Epoch: [1][4700/36908] Elapsed 63m 3s (remain 432m 3s) Loss: 0.0052(0.0353) Grad: 29232.6172  LR: 0.000019  \n",
      "Epoch: [1][4800/36908] Elapsed 64m 20s (remain 430m 20s) Loss: 0.0015(0.0347) Grad: 1707.2239  LR: 0.000019  \n",
      "Epoch: [1][4900/36908] Elapsed 65m 38s (remain 428m 40s) Loss: 0.0046(0.0342) Grad: 3907.9172  LR: 0.000019  \n",
      "Epoch: [1][5000/36908] Elapsed 66m 56s (remain 427m 7s) Loss: 0.0014(0.0336) Grad: 1177.2283  LR: 0.000019  \n",
      "Epoch: [1][5100/36908] Elapsed 68m 14s (remain 425m 32s) Loss: 0.0214(0.0331) Grad: 24394.9512  LR: 0.000019  \n",
      "Epoch: [1][5200/36908] Elapsed 69m 31s (remain 423m 50s) Loss: 0.0082(0.0326) Grad: 36091.6797  LR: 0.000019  \n",
      "Epoch: [1][5300/36908] Elapsed 70m 49s (remain 422m 17s) Loss: 0.0017(0.0321) Grad: 1060.6665  LR: 0.000019  \n",
      "Epoch: [1][5400/36908] Elapsed 72m 11s (remain 421m 7s) Loss: 0.0090(0.0316) Grad: 3119.0391  LR: 0.000019  \n",
      "Epoch: [1][5500/36908] Elapsed 73m 32s (remain 419m 53s) Loss: 0.0089(0.0312) Grad: 70495.2734  LR: 0.000019  \n",
      "Epoch: [1][5600/36908] Elapsed 74m 50s (remain 418m 19s) Loss: 0.0010(0.0308) Grad: 331.0921  LR: 0.000019  \n",
      "Epoch: [1][5700/36908] Elapsed 76m 7s (remain 416m 44s) Loss: 0.0067(0.0303) Grad: 97828.3750  LR: 0.000019  \n",
      "Epoch: [1][5800/36908] Elapsed 77m 29s (remain 415m 32s) Loss: 0.0008(0.0299) Grad: 2536.6121  LR: 0.000019  \n",
      "Epoch: [1][5900/36908] Elapsed 78m 48s (remain 414m 6s) Loss: 0.0039(0.0295) Grad: 13032.8984  LR: 0.000019  \n",
      "Epoch: [1][6000/36908] Elapsed 80m 7s (remain 412m 38s) Loss: 0.0031(0.0291) Grad: 8689.4561  LR: 0.000019  \n",
      "Epoch: [1][6100/36908] Elapsed 81m 31s (remain 411m 38s) Loss: 0.0011(0.0288) Grad: 4251.2383  LR: 0.000019  \n",
      "Epoch: [1][6200/36908] Elapsed 82m 49s (remain 410m 9s) Loss: 0.0013(0.0284) Grad: 16382.4541  LR: 0.000018  \n",
      "Epoch: [1][6300/36908] Elapsed 84m 6s (remain 408m 34s) Loss: 0.0053(0.0281) Grad: 98758.5312  LR: 0.000018  \n",
      "Epoch: [1][6400/36908] Elapsed 85m 26s (remain 407m 11s) Loss: 0.0005(0.0277) Grad: 138.5370  LR: 0.000018  \n",
      "Epoch: [1][6500/36908] Elapsed 86m 46s (remain 405m 53s) Loss: 0.0056(0.0274) Grad: 43385.5391  LR: 0.000018  \n",
      "Epoch: [1][6600/36908] Elapsed 88m 5s (remain 404m 25s) Loss: 0.0016(0.0271) Grad: 5699.4502  LR: 0.000018  \n",
      "Epoch: [1][6700/36908] Elapsed 89m 23s (remain 402m 56s) Loss: 0.0117(0.0267) Grad: 48559.5000  LR: 0.000018  \n",
      "Epoch: [1][6800/36908] Elapsed 90m 42s (remain 401m 34s) Loss: 0.0003(0.0264) Grad: 3123.4678  LR: 0.000018  \n",
      "Epoch: [1][6900/36908] Elapsed 92m 3s (remain 400m 15s) Loss: 0.0091(0.0261) Grad: 17402.2715  LR: 0.000018  \n",
      "Epoch: [1][7000/36908] Elapsed 93m 22s (remain 398m 54s) Loss: 0.0031(0.0258) Grad: 24278.3125  LR: 0.000018  \n",
      "Epoch: [1][7100/36908] Elapsed 94m 41s (remain 397m 27s) Loss: 0.0040(0.0255) Grad: 3124.9065  LR: 0.000018  \n",
      "Epoch: [1][7200/36908] Elapsed 95m 59s (remain 396m 1s) Loss: 0.0012(0.0253) Grad: 222.0672  LR: 0.000018  \n",
      "Epoch: [1][7300/36908] Elapsed 97m 20s (remain 394m 42s) Loss: 0.0038(0.0250) Grad: 7063.4233  LR: 0.000018  \n",
      "Epoch: [1][7400/36908] Elapsed 98m 39s (remain 393m 22s) Loss: 0.0089(0.0247) Grad: 20086.2441  LR: 0.000018  \n",
      "Epoch: [1][7500/36908] Elapsed 99m 59s (remain 392m 0s) Loss: 0.0099(0.0245) Grad: 60406.8281  LR: 0.000018  \n",
      "Epoch: [1][7600/36908] Elapsed 101m 22s (remain 390m 53s) Loss: 0.0010(0.0242) Grad: 12687.3262  LR: 0.000018  \n",
      "Epoch: [1][7700/36908] Elapsed 102m 41s (remain 389m 26s) Loss: 0.0008(0.0240) Grad: 1577.3666  LR: 0.000018  \n",
      "Epoch: [1][7800/36908] Elapsed 103m 59s (remain 388m 0s) Loss: 0.0060(0.0237) Grad: 10481.0361  LR: 0.000018  \n",
      "Epoch: [1][7900/36908] Elapsed 105m 20s (remain 386m 42s) Loss: 0.0063(0.0235) Grad: 11017.6016  LR: 0.000017  \n",
      "Epoch: [1][8000/36908] Elapsed 106m 39s (remain 385m 20s) Loss: 0.0011(0.0233) Grad: 344.3772  LR: 0.000017  \n",
      "Epoch: [1][8100/36908] Elapsed 107m 59s (remain 384m 0s) Loss: 0.0017(0.0231) Grad: 21784.5898  LR: 0.000017  \n",
      "Epoch: [1][8200/36908] Elapsed 109m 18s (remain 382m 38s) Loss: 0.0117(0.0229) Grad: 70107.2422  LR: 0.000017  \n",
      "Epoch: [1][8300/36908] Elapsed 110m 40s (remain 381m 25s) Loss: 0.0014(0.0227) Grad: 2155.3931  LR: 0.000017  \n",
      "Epoch: [1][8400/36908] Elapsed 112m 4s (remain 380m 19s) Loss: 0.0049(0.0224) Grad: 4737.1021  LR: 0.000017  \n",
      "Epoch: [1][8500/36908] Elapsed 113m 28s (remain 379m 10s) Loss: 0.0002(0.0222) Grad: 317.6898  LR: 0.000017  \n",
      "Epoch: [1][8600/36908] Elapsed 114m 48s (remain 377m 49s) Loss: 0.0006(0.0220) Grad: 4711.6021  LR: 0.000017  \n",
      "Epoch: [1][8700/36908] Elapsed 116m 7s (remain 376m 27s) Loss: 0.0062(0.0218) Grad: 85225.6406  LR: 0.000017  \n",
      "Epoch: [1][8800/36908] Elapsed 117m 27s (remain 375m 6s) Loss: 0.0186(0.0216) Grad: 151293.1094  LR: 0.000017  \n",
      "Epoch: [1][8900/36908] Elapsed 118m 45s (remain 373m 40s) Loss: 0.0000(0.0215) Grad: 167.4979  LR: 0.000017  \n",
      "Epoch: [1][9000/36908] Elapsed 120m 3s (remain 372m 15s) Loss: 0.0035(0.0213) Grad: 38579.8320  LR: 0.000017  \n",
      "Epoch: [1][9100/36908] Elapsed 121m 22s (remain 370m 50s) Loss: 0.0184(0.0211) Grad: 88671.6250  LR: 0.000017  \n",
      "Epoch: [1][9200/36908] Elapsed 122m 41s (remain 369m 27s) Loss: 0.0073(0.0209) Grad: 105568.9531  LR: 0.000017  \n",
      "Epoch: [1][9300/36908] Elapsed 124m 3s (remain 368m 12s) Loss: 0.0004(0.0208) Grad: 4331.0332  LR: 0.000017  \n",
      "Epoch: [1][9400/36908] Elapsed 125m 21s (remain 366m 47s) Loss: 0.0023(0.0206) Grad: 10322.4688  LR: 0.000017  \n",
      "Epoch: [1][9500/36908] Elapsed 126m 41s (remain 365m 27s) Loss: 0.0137(0.0204) Grad: 48744.3984  LR: 0.000017  \n",
      "Epoch: [1][9600/36908] Elapsed 128m 2s (remain 364m 10s) Loss: 0.0020(0.0203) Grad: 6590.6562  LR: 0.000016  \n",
      "Epoch: [1][9700/36908] Elapsed 129m 20s (remain 362m 46s) Loss: 0.0009(0.0201) Grad: 817.6879  LR: 0.000016  \n",
      "Epoch: [1][9800/36908] Elapsed 130m 39s (remain 361m 21s) Loss: 0.0008(0.0200) Grad: 535.1066  LR: 0.000016  \n",
      "Epoch: [1][9900/36908] Elapsed 131m 58s (remain 359m 59s) Loss: 0.0005(0.0198) Grad: 3315.4895  LR: 0.000016  \n",
      "Epoch: [1][10000/36908] Elapsed 133m 18s (remain 358m 38s) Loss: 0.0065(0.0197) Grad: 21251.8711  LR: 0.000016  \n",
      "Epoch: [1][10100/36908] Elapsed 134m 36s (remain 357m 15s) Loss: 0.0097(0.0195) Grad: 159096.2031  LR: 0.000016  \n",
      "Epoch: [1][10200/36908] Elapsed 135m 54s (remain 355m 49s) Loss: 0.0036(0.0194) Grad: 24853.4668  LR: 0.000016  \n",
      "Epoch: [1][10300/36908] Elapsed 137m 14s (remain 354m 28s) Loss: 0.0047(0.0193) Grad: 1050.0367  LR: 0.000016  \n",
      "Epoch: [1][10400/36908] Elapsed 138m 31s (remain 353m 2s) Loss: 0.0007(0.0191) Grad: 1159.4594  LR: 0.000016  \n",
      "Epoch: [1][10500/36908] Elapsed 139m 49s (remain 351m 36s) Loss: 0.0002(0.0190) Grad: 103.0726  LR: 0.000016  \n",
      "Epoch: [1][10600/36908] Elapsed 141m 8s (remain 350m 15s) Loss: 0.0072(0.0188) Grad: 108977.2812  LR: 0.000016  \n",
      "Epoch: [1][10700/36908] Elapsed 142m 26s (remain 348m 50s) Loss: 0.0029(0.0187) Grad: 6438.6484  LR: 0.000016  \n",
      "Epoch: [1][10800/36908] Elapsed 143m 45s (remain 347m 29s) Loss: 0.0012(0.0186) Grad: 16059.4482  LR: 0.000016  \n",
      "Epoch: [1][10900/36908] Elapsed 145m 2s (remain 346m 2s) Loss: 0.0010(0.0184) Grad: 6745.6411  LR: 0.000016  \n",
      "Epoch: [1][11000/36908] Elapsed 146m 19s (remain 344m 35s) Loss: 0.0070(0.0183) Grad: 105245.7500  LR: 0.000016  \n",
      "Epoch: [1][11100/36908] Elapsed 147m 37s (remain 343m 11s) Loss: 0.0016(0.0182) Grad: 4077.2495  LR: 0.000016  \n",
      "Epoch: [1][11200/36908] Elapsed 148m 55s (remain 341m 48s) Loss: 0.0149(0.0181) Grad: 40667.1445  LR: 0.000015  \n",
      "Epoch: [1][11300/36908] Elapsed 150m 18s (remain 340m 36s) Loss: 0.0004(0.0179) Grad: 360.5638  LR: 0.000015  \n",
      "Epoch: [1][11400/36908] Elapsed 151m 39s (remain 339m 17s) Loss: 0.0038(0.0178) Grad: 10733.2832  LR: 0.000015  \n",
      "Epoch: [1][11500/36908] Elapsed 152m 56s (remain 337m 52s) Loss: 0.0032(0.0177) Grad: 17965.2461  LR: 0.000015  \n",
      "Epoch: [1][11600/36908] Elapsed 154m 13s (remain 336m 26s) Loss: 0.0022(0.0176) Grad: 11232.0049  LR: 0.000015  \n",
      "Epoch: [1][11700/36908] Elapsed 155m 33s (remain 335m 7s) Loss: 0.0013(0.0175) Grad: 18674.1035  LR: 0.000015  \n",
      "Epoch: [1][11800/36908] Elapsed 156m 51s (remain 333m 43s) Loss: 0.0023(0.0174) Grad: 12662.3740  LR: 0.000015  \n",
      "Epoch: [1][11900/36908] Elapsed 158m 8s (remain 332m 18s) Loss: 0.0010(0.0173) Grad: 8829.9697  LR: 0.000015  \n",
      "Epoch: [1][12000/36908] Elapsed 159m 29s (remain 331m 1s) Loss: 0.0104(0.0172) Grad: 22378.7832  LR: 0.000015  \n",
      "Epoch: [1][12100/36908] Elapsed 160m 50s (remain 329m 43s) Loss: 0.0014(0.0171) Grad: 6550.4473  LR: 0.000015  \n",
      "Epoch: [1][12200/36908] Elapsed 162m 9s (remain 328m 22s) Loss: 0.0049(0.0170) Grad: 106628.6406  LR: 0.000015  \n",
      "Epoch: [1][12300/36908] Elapsed 163m 28s (remain 327m 1s) Loss: 0.0021(0.0169) Grad: 34007.4844  LR: 0.000015  \n",
      "Epoch: [1][12400/36908] Elapsed 164m 47s (remain 325m 39s) Loss: 0.0010(0.0168) Grad: 902.9497  LR: 0.000015  \n",
      "Epoch: [1][12500/36908] Elapsed 166m 11s (remain 324m 27s) Loss: 0.0002(0.0167) Grad: 4986.5649  LR: 0.000015  \n",
      "Epoch: [1][12600/36908] Elapsed 167m 29s (remain 323m 6s) Loss: 0.0036(0.0166) Grad: 135917.2656  LR: 0.000015  \n",
      "Epoch: [1][12700/36908] Elapsed 168m 48s (remain 321m 44s) Loss: 0.0011(0.0165) Grad: 21673.0781  LR: 0.000015  \n",
      "Epoch: [1][12800/36908] Elapsed 170m 7s (remain 320m 22s) Loss: 0.0102(0.0164) Grad: 100590.2656  LR: 0.000015  \n",
      "Epoch: [1][12900/36908] Elapsed 171m 26s (remain 319m 1s) Loss: 0.0017(0.0163) Grad: 889.2844  LR: 0.000014  \n",
      "Epoch: [1][13000/36908] Elapsed 172m 45s (remain 317m 39s) Loss: 0.0029(0.0162) Grad: 17314.8125  LR: 0.000014  \n",
      "Epoch: [1][13100/36908] Elapsed 174m 2s (remain 316m 15s) Loss: 0.0006(0.0162) Grad: 8548.0605  LR: 0.000014  \n",
      "Epoch: [1][13200/36908] Elapsed 175m 21s (remain 314m 54s) Loss: 0.0054(0.0161) Grad: 100545.1172  LR: 0.000014  \n",
      "Epoch: [1][13300/36908] Elapsed 176m 42s (remain 313m 37s) Loss: 0.0071(0.0160) Grad: 45959.5039  LR: 0.000014  \n",
      "Epoch: [1][13400/36908] Elapsed 178m 0s (remain 312m 14s) Loss: 0.0075(0.0159) Grad: 87109.3750  LR: 0.000014  \n",
      "Epoch: [1][13500/36908] Elapsed 179m 18s (remain 310m 51s) Loss: 0.0055(0.0158) Grad: 25937.0430  LR: 0.000014  \n",
      "Epoch: [1][13600/36908] Elapsed 180m 38s (remain 309m 32s) Loss: 0.0001(0.0157) Grad: 134.0706  LR: 0.000014  \n",
      "Epoch: [1][13700/36908] Elapsed 181m 56s (remain 308m 10s) Loss: 0.0056(0.0156) Grad: 24703.1602  LR: 0.000014  \n",
      "Epoch: [1][13800/36908] Elapsed 183m 15s (remain 306m 50s) Loss: 0.0076(0.0156) Grad: 88228.4844  LR: 0.000014  \n",
      "Epoch: [1][13900/36908] Elapsed 184m 33s (remain 305m 26s) Loss: 0.0118(0.0155) Grad: 43689.6289  LR: 0.000014  \n",
      "Epoch: [1][14000/36908] Elapsed 185m 51s (remain 304m 5s) Loss: 0.0028(0.0154) Grad: 830.3495  LR: 0.000014  \n",
      "Epoch: [1][14100/36908] Elapsed 187m 12s (remain 302m 47s) Loss: 0.0067(0.0153) Grad: 18255.3223  LR: 0.000014  \n",
      "Epoch: [1][14200/36908] Elapsed 188m 34s (remain 301m 31s) Loss: 0.0064(0.0153) Grad: 44135.7148  LR: 0.000014  \n",
      "Epoch: [1][14300/36908] Elapsed 189m 53s (remain 300m 10s) Loss: 0.0013(0.0152) Grad: 2032.6389  LR: 0.000014  \n",
      "Epoch: [1][14400/36908] Elapsed 191m 11s (remain 298m 48s) Loss: 0.0010(0.0151) Grad: 49454.5039  LR: 0.000014  \n",
      "Epoch: [1][14500/36908] Elapsed 192m 29s (remain 297m 26s) Loss: 0.0034(0.0150) Grad: 5594.6279  LR: 0.000013  \n",
      "Epoch: [1][14600/36908] Elapsed 193m 51s (remain 296m 10s) Loss: 0.0061(0.0150) Grad: 8036.1406  LR: 0.000013  \n",
      "Epoch: [1][14700/36908] Elapsed 195m 10s (remain 294m 48s) Loss: 0.0007(0.0149) Grad: 431.2457  LR: 0.000013  \n",
      "Epoch: [1][14800/36908] Elapsed 196m 27s (remain 293m 25s) Loss: 0.0013(0.0148) Grad: 31090.8789  LR: 0.000013  \n",
      "Epoch: [1][14900/36908] Elapsed 197m 46s (remain 292m 4s) Loss: 0.0046(0.0148) Grad: 40550.5742  LR: 0.000013  \n",
      "Epoch: [1][15000/36908] Elapsed 199m 5s (remain 290m 45s) Loss: 0.0012(0.0147) Grad: 4184.4185  LR: 0.000013  \n",
      "Epoch: [1][15100/36908] Elapsed 200m 23s (remain 289m 23s) Loss: 0.0056(0.0146) Grad: 21644.3242  LR: 0.000013  \n",
      "Epoch: [1][15200/36908] Elapsed 201m 41s (remain 288m 0s) Loss: 0.0023(0.0146) Grad: 4161.2905  LR: 0.000013  \n",
      "Epoch: [1][15300/36908] Elapsed 202m 58s (remain 286m 37s) Loss: 0.0070(0.0145) Grad: 72725.3359  LR: 0.000013  \n",
      "Epoch: [1][15400/36908] Elapsed 204m 16s (remain 285m 16s) Loss: 0.0008(0.0144) Grad: 144.9796  LR: 0.000013  \n",
      "Epoch: [1][15500/36908] Elapsed 205m 36s (remain 283m 56s) Loss: 0.0024(0.0144) Grad: 7771.6992  LR: 0.000013  \n",
      "Epoch: [1][15600/36908] Elapsed 206m 53s (remain 282m 33s) Loss: 0.0038(0.0143) Grad: 7180.0459  LR: 0.000013  \n",
      "Epoch: [1][15700/36908] Elapsed 208m 10s (remain 281m 10s) Loss: 0.0026(0.0143) Grad: 5715.9033  LR: 0.000013  \n",
      "Epoch: [1][15800/36908] Elapsed 209m 31s (remain 279m 52s) Loss: 0.0238(0.0142) Grad: 368766.1250  LR: 0.000013  \n",
      "Epoch: [1][15900/36908] Elapsed 210m 51s (remain 278m 34s) Loss: 0.0081(0.0141) Grad: 38239.4258  LR: 0.000013  \n",
      "Epoch: [1][16000/36908] Elapsed 212m 12s (remain 277m 16s) Loss: 0.0077(0.0141) Grad: 155730.0781  LR: 0.000013  \n",
      "Epoch: [1][16100/36908] Elapsed 213m 34s (remain 276m 0s) Loss: 0.0003(0.0140) Grad: 766.7601  LR: 0.000013  \n",
      "Epoch: [1][16200/36908] Elapsed 214m 54s (remain 274m 41s) Loss: 0.0045(0.0139) Grad: 25146.0957  LR: 0.000012  \n",
      "Epoch: [1][16300/36908] Elapsed 216m 12s (remain 273m 18s) Loss: 0.0037(0.0139) Grad: 26038.9512  LR: 0.000012  \n",
      "Epoch: [1][16400/36908] Elapsed 217m 30s (remain 271m 57s) Loss: 0.0023(0.0138) Grad: 21891.4043  LR: 0.000012  \n",
      "Epoch: [1][16500/36908] Elapsed 218m 48s (remain 270m 36s) Loss: 0.0049(0.0138) Grad: 60392.9492  LR: 0.000012  \n",
      "Epoch: [1][16600/36908] Elapsed 220m 6s (remain 269m 14s) Loss: 0.0006(0.0137) Grad: 810.5416  LR: 0.000012  \n",
      "Epoch: [1][16700/36908] Elapsed 221m 24s (remain 267m 53s) Loss: 0.0043(0.0136) Grad: 51126.0664  LR: 0.000012  \n",
      "Epoch: [1][16800/36908] Elapsed 222m 46s (remain 266m 36s) Loss: 0.0001(0.0136) Grad: 843.3828  LR: 0.000012  \n",
      "Epoch: [1][16900/36908] Elapsed 224m 5s (remain 265m 15s) Loss: 0.0024(0.0135) Grad: 23499.9746  LR: 0.000012  \n",
      "Epoch: [1][17000/36908] Elapsed 225m 23s (remain 263m 54s) Loss: 0.0021(0.0135) Grad: 103788.4844  LR: 0.000012  \n",
      "Epoch: [1][17100/36908] Elapsed 226m 42s (remain 262m 35s) Loss: 0.0023(0.0134) Grad: 6387.9380  LR: 0.000012  \n",
      "Epoch: [1][17200/36908] Elapsed 228m 0s (remain 261m 13s) Loss: 0.0049(0.0134) Grad: 188335.8906  LR: 0.000012  \n",
      "Epoch: [1][17300/36908] Elapsed 229m 18s (remain 259m 51s) Loss: 0.0014(0.0133) Grad: 10606.7217  LR: 0.000012  \n",
      "Epoch: [1][17400/36908] Elapsed 230m 36s (remain 258m 31s) Loss: 0.0010(0.0133) Grad: 9547.5381  LR: 0.000012  \n",
      "Epoch: [1][17500/36908] Elapsed 231m 53s (remain 257m 9s) Loss: 0.0047(0.0132) Grad: 59912.8789  LR: 0.000012  \n",
      "Epoch: [1][17600/36908] Elapsed 233m 11s (remain 255m 47s) Loss: 0.0035(0.0132) Grad: 21644.1562  LR: 0.000012  \n",
      "Epoch: [1][17700/36908] Elapsed 234m 31s (remain 254m 28s) Loss: 0.0025(0.0131) Grad: 36196.3008  LR: 0.000012  \n",
      "Epoch: [1][17800/36908] Elapsed 235m 47s (remain 253m 5s) Loss: 0.0002(0.0130) Grad: 250.2447  LR: 0.000012  \n",
      "Epoch: [1][17900/36908] Elapsed 237m 5s (remain 251m 44s) Loss: 0.0028(0.0130) Grad: 32875.9766  LR: 0.000011  \n",
      "Epoch: [1][18000/36908] Elapsed 238m 24s (remain 250m 24s) Loss: 0.0061(0.0130) Grad: 60235.5234  LR: 0.000011  \n",
      "Epoch: [1][18100/36908] Elapsed 239m 46s (remain 249m 7s) Loss: 0.0008(0.0129) Grad: 13752.2520  LR: 0.000011  \n",
      "Epoch: [1][18200/36908] Elapsed 241m 4s (remain 247m 46s) Loss: 0.0035(0.0129) Grad: 21392.0605  LR: 0.000011  \n",
      "Epoch: [1][18300/36908] Elapsed 242m 20s (remain 246m 23s) Loss: 0.0010(0.0128) Grad: 7527.3296  LR: 0.000011  \n",
      "Epoch: [1][18400/36908] Elapsed 243m 38s (remain 245m 2s) Loss: 0.0007(0.0128) Grad: 4232.4629  LR: 0.000011  \n",
      "Epoch: [1][18500/36908] Elapsed 244m 57s (remain 243m 42s) Loss: 0.0017(0.0127) Grad: 15461.2627  LR: 0.000011  \n",
      "Epoch: [1][18600/36908] Elapsed 246m 15s (remain 242m 22s) Loss: 0.1595(0.0127) Grad: inf  LR: 0.000011  \n",
      "Epoch: [1][18700/36908] Elapsed 247m 34s (remain 241m 2s) Loss: 0.0005(0.0126) Grad: 301.3362  LR: 0.000011  \n",
      "Epoch: [1][18800/36908] Elapsed 248m 52s (remain 239m 41s) Loss: 0.0049(0.0126) Grad: 32435.2051  LR: 0.000011  \n",
      "Epoch: [1][18900/36908] Elapsed 250m 16s (remain 238m 26s) Loss: 0.0053(0.0126) Grad: 17018.9062  LR: 0.000011  \n",
      "Epoch: [1][19000/36908] Elapsed 251m 38s (remain 237m 8s) Loss: 0.0055(0.0125) Grad: 14162.0000  LR: 0.000011  \n",
      "Epoch: [1][19100/36908] Elapsed 252m 55s (remain 235m 47s) Loss: 0.0077(0.0125) Grad: 27044.6191  LR: 0.000011  \n",
      "Epoch: [1][19200/36908] Elapsed 254m 14s (remain 234m 27s) Loss: 0.0105(0.0124) Grad: 102779.8281  LR: 0.000011  \n",
      "Epoch: [1][19300/36908] Elapsed 255m 34s (remain 233m 9s) Loss: 0.0015(0.0124) Grad: 3032.0266  LR: 0.000011  \n",
      "Epoch: [1][19400/36908] Elapsed 256m 56s (remain 231m 51s) Loss: 0.0015(0.0124) Grad: 15403.1152  LR: 0.000011  \n",
      "Epoch: [1][19500/36908] Elapsed 258m 15s (remain 230m 31s) Loss: 0.0012(0.0123) Grad: 6749.2905  LR: 0.000010  \n",
      "Epoch: [1][19600/36908] Elapsed 259m 36s (remain 229m 13s) Loss: 0.0048(0.0123) Grad: 26041.6328  LR: 0.000010  \n",
      "Epoch: [1][19700/36908] Elapsed 260m 54s (remain 227m 53s) Loss: 0.0018(0.0122) Grad: 6639.0732  LR: 0.000010  \n",
      "Epoch: [1][19800/36908] Elapsed 262m 13s (remain 226m 33s) Loss: 0.0131(0.0122) Grad: 12366.2715  LR: 0.000010  \n",
      "Epoch: [1][19900/36908] Elapsed 263m 33s (remain 225m 14s) Loss: 0.0003(0.0122) Grad: 164.3993  LR: 0.000010  \n",
      "Epoch: [1][20000/36908] Elapsed 264m 52s (remain 223m 54s) Loss: 0.0009(0.0121) Grad: 280.0748  LR: 0.000010  \n",
      "Epoch: [1][20100/36908] Elapsed 266m 12s (remain 222m 34s) Loss: 0.0049(0.0121) Grad: 9175.9795  LR: 0.000010  \n",
      "Epoch: [1][20200/36908] Elapsed 267m 32s (remain 221m 15s) Loss: 0.0014(0.0120) Grad: 1610.6016  LR: 0.000010  \n",
      "Epoch: [1][20300/36908] Elapsed 268m 51s (remain 219m 56s) Loss: 0.0040(0.0120) Grad: 3031.5916  LR: 0.000010  \n",
      "Epoch: [1][20400/36908] Elapsed 270m 16s (remain 218m 41s) Loss: 0.0106(0.0120) Grad: 86178.9766  LR: 0.000010  \n",
      "Epoch: [1][20500/36908] Elapsed 271m 35s (remain 217m 21s) Loss: 0.0035(0.0119) Grad: 1795.1735  LR: 0.000010  \n",
      "Epoch: [1][20600/36908] Elapsed 273m 0s (remain 216m 6s) Loss: 0.0002(0.0119) Grad: 3133.3821  LR: 0.000010  \n",
      "Epoch: [1][20700/36908] Elapsed 274m 24s (remain 214m 50s) Loss: 0.0039(0.0119) Grad: 4519.8374  LR: 0.000010  \n",
      "Epoch: [1][20800/36908] Elapsed 275m 48s (remain 213m 34s) Loss: 0.0104(0.0118) Grad: 29040.7734  LR: 0.000010  \n",
      "Epoch: [1][20900/36908] Elapsed 277m 14s (remain 212m 19s) Loss: 0.0075(0.0118) Grad: 8944.7793  LR: 0.000010  \n",
      "Epoch: [1][21000/36908] Elapsed 278m 40s (remain 211m 4s) Loss: 0.0026(0.0117) Grad: 6553.8857  LR: 0.000010  \n",
      "Epoch: [1][21100/36908] Elapsed 280m 4s (remain 209m 48s) Loss: 0.0000(0.0117) Grad: 135.6857  LR: 0.000010  \n",
      "Epoch: [1][21200/36908] Elapsed 281m 28s (remain 208m 31s) Loss: 0.0027(0.0117) Grad: 3615.6084  LR: 0.000009  \n",
      "Epoch: [1][21300/36908] Elapsed 282m 54s (remain 207m 17s) Loss: 0.0104(0.0116) Grad: 82544.6875  LR: 0.000009  \n",
      "Epoch: [1][21400/36908] Elapsed 284m 20s (remain 206m 1s) Loss: 0.0005(0.0116) Grad: 944.8209  LR: 0.000009  \n",
      "Epoch: [1][21500/36908] Elapsed 285m 44s (remain 204m 45s) Loss: 0.0011(0.0116) Grad: 1255.3679  LR: 0.000009  \n",
      "Epoch: [1][21600/36908] Elapsed 287m 8s (remain 203m 28s) Loss: 0.0013(0.0115) Grad: 564.2521  LR: 0.000009  \n",
      "Epoch: [1][21700/36908] Elapsed 288m 33s (remain 202m 12s) Loss: 0.0044(0.0115) Grad: 5893.6636  LR: 0.000009  \n",
      "Epoch: [1][21800/36908] Elapsed 289m 58s (remain 200m 56s) Loss: 0.0007(0.0115) Grad: 11332.8584  LR: 0.000009  \n",
      "Epoch: [1][21900/36908] Elapsed 291m 21s (remain 199m 38s) Loss: 0.0024(0.0114) Grad: 8088.3867  LR: 0.000009  \n",
      "Epoch: [1][22000/36908] Elapsed 292m 39s (remain 198m 17s) Loss: 0.0035(0.0114) Grad: 48151.5703  LR: 0.000009  \n",
      "Epoch: [1][22100/36908] Elapsed 293m 59s (remain 196m 58s) Loss: 0.0272(0.0114) Grad: 220017.9375  LR: 0.000009  \n",
      "Epoch: [1][22200/36908] Elapsed 295m 18s (remain 195m 37s) Loss: 0.0018(0.0113) Grad: 6921.1411  LR: 0.000009  \n",
      "Epoch: [1][22300/36908] Elapsed 296m 35s (remain 194m 15s) Loss: 0.0042(0.0113) Grad: 24088.8750  LR: 0.000009  \n",
      "Epoch: [1][22400/36908] Elapsed 297m 53s (remain 192m 54s) Loss: 0.0031(0.0113) Grad: 8259.5400  LR: 0.000009  \n",
      "Epoch: [1][22500/36908] Elapsed 299m 13s (remain 191m 35s) Loss: 0.0065(0.0112) Grad: 28797.4414  LR: 0.000009  \n",
      "Epoch: [1][22600/36908] Elapsed 300m 30s (remain 190m 13s) Loss: 0.0027(0.0112) Grad: 1629.1171  LR: 0.000009  \n",
      "Epoch: [1][22700/36908] Elapsed 301m 48s (remain 188m 52s) Loss: 0.0018(0.0112) Grad: 4141.7119  LR: 0.000009  \n",
      "Epoch: [1][22800/36908] Elapsed 303m 9s (remain 187m 33s) Loss: 0.0010(0.0111) Grad: 154.2074  LR: 0.000008  \n",
      "Epoch: [1][22900/36908] Elapsed 304m 31s (remain 186m 15s) Loss: 0.0088(0.0111) Grad: 81981.4062  LR: 0.000008  \n",
      "Epoch: [1][23000/36908] Elapsed 305m 49s (remain 184m 54s) Loss: 0.0012(0.0111) Grad: 4635.9180  LR: 0.000008  \n",
      "Epoch: [1][23100/36908] Elapsed 307m 6s (remain 183m 33s) Loss: 0.0043(0.0111) Grad: 84436.5078  LR: 0.000008  \n",
      "Epoch: [1][23200/36908] Elapsed 308m 24s (remain 182m 12s) Loss: 0.0042(0.0110) Grad: 6308.8242  LR: 0.000008  \n",
      "Epoch: [1][23300/36908] Elapsed 309m 43s (remain 180m 52s) Loss: 0.0106(0.0110) Grad: 2368.9282  LR: 0.000008  \n",
      "Epoch: [1][23400/36908] Elapsed 311m 0s (remain 179m 30s) Loss: 0.0071(0.0110) Grad: 80816.1328  LR: 0.000008  \n",
      "Epoch: [1][23500/36908] Elapsed 312m 18s (remain 178m 10s) Loss: 0.0005(0.0109) Grad: 16609.7051  LR: 0.000008  \n",
      "Epoch: [1][23600/36908] Elapsed 313m 38s (remain 176m 50s) Loss: 0.0008(0.0109) Grad: 286.2913  LR: 0.000008  \n",
      "Epoch: [1][23700/36908] Elapsed 314m 58s (remain 175m 30s) Loss: 0.0001(0.0109) Grad: 40.0549  LR: 0.000008  \n",
      "Epoch: [1][23800/36908] Elapsed 316m 16s (remain 174m 10s) Loss: 0.0022(0.0108) Grad: 14395.2949  LR: 0.000008  \n",
      "Epoch: [1][23900/36908] Elapsed 317m 34s (remain 172m 49s) Loss: 0.0011(0.0108) Grad: 5707.6162  LR: 0.000008  \n",
      "Epoch: [1][24000/36908] Elapsed 318m 54s (remain 171m 29s) Loss: 0.0048(0.0108) Grad: 29582.5059  LR: 0.000008  \n",
      "Epoch: [1][24100/36908] Elapsed 320m 13s (remain 170m 9s) Loss: 0.0080(0.0108) Grad: 91516.8594  LR: 0.000008  \n",
      "Epoch: [1][24200/36908] Elapsed 321m 31s (remain 168m 49s) Loss: 0.0013(0.0107) Grad: 26204.3398  LR: 0.000008  \n",
      "Epoch: [1][24300/36908] Elapsed 322m 48s (remain 167m 28s) Loss: 0.0016(0.0107) Grad: 1359.7001  LR: 0.000008  \n",
      "Epoch: [1][24400/36908] Elapsed 324m 7s (remain 166m 7s) Loss: 0.0023(0.0107) Grad: 5743.7529  LR: 0.000008  \n",
      "Epoch: [1][24500/36908] Elapsed 325m 28s (remain 164m 48s) Loss: 0.0004(0.0106) Grad: 3228.6772  LR: 0.000007  \n",
      "Epoch: [1][24600/36908] Elapsed 326m 47s (remain 163m 28s) Loss: 0.0020(0.0106) Grad: 2430.0728  LR: 0.000007  \n",
      "Epoch: [1][24700/36908] Elapsed 328m 6s (remain 162m 8s) Loss: 0.0004(0.0106) Grad: 1830.3975  LR: 0.000007  \n",
      "Epoch: [1][24800/36908] Elapsed 329m 26s (remain 160m 49s) Loss: 0.0018(0.0106) Grad: 14978.0273  LR: 0.000007  \n",
      "Epoch: [1][24900/36908] Elapsed 330m 46s (remain 159m 29s) Loss: 0.0020(0.0105) Grad: 1155.9657  LR: 0.000007  \n",
      "Epoch: [1][25000/36908] Elapsed 332m 5s (remain 158m 9s) Loss: 0.0032(0.0105) Grad: 17747.3594  LR: 0.000007  \n",
      "Epoch: [1][25100/36908] Elapsed 333m 24s (remain 156m 49s) Loss: 0.0002(0.0105) Grad: 186.0403  LR: 0.000007  \n",
      "Epoch: [1][25200/36908] Elapsed 334m 43s (remain 155m 29s) Loss: 0.0070(0.0105) Grad: 19307.7109  LR: 0.000007  \n",
      "Epoch: [1][25300/36908] Elapsed 336m 2s (remain 154m 9s) Loss: 0.0017(0.0104) Grad: 13632.2812  LR: 0.000007  \n",
      "Epoch: [1][25400/36908] Elapsed 337m 23s (remain 152m 50s) Loss: 0.0001(0.0104) Grad: 387.2143  LR: 0.000007  \n",
      "Epoch: [1][25500/36908] Elapsed 338m 42s (remain 151m 30s) Loss: 0.0003(0.0104) Grad: 1631.3395  LR: 0.000007  \n",
      "Epoch: [1][25600/36908] Elapsed 340m 2s (remain 150m 10s) Loss: 0.0019(0.0104) Grad: 18344.6445  LR: 0.000007  \n",
      "Epoch: [1][25700/36908] Elapsed 341m 23s (remain 148m 51s) Loss: 0.0073(0.0103) Grad: 2628.8235  LR: 0.000007  \n",
      "Epoch: [1][25800/36908] Elapsed 342m 41s (remain 147m 31s) Loss: 0.0125(0.0103) Grad: 52275.3047  LR: 0.000007  \n",
      "Epoch: [1][25900/36908] Elapsed 343m 59s (remain 146m 10s) Loss: 0.0006(0.0103) Grad: 10744.1250  LR: 0.000007  \n",
      "Epoch: [1][26000/36908] Elapsed 345m 16s (remain 144m 50s) Loss: 0.0039(0.0103) Grad: 12506.3223  LR: 0.000007  \n",
      "Epoch: [1][26100/36908] Elapsed 346m 33s (remain 143m 29s) Loss: 0.0022(0.0102) Grad: 8460.3428  LR: 0.000007  \n",
      "Epoch: [1][26200/36908] Elapsed 347m 50s (remain 142m 8s) Loss: 0.0065(0.0102) Grad: 126805.5000  LR: 0.000006  \n",
      "Epoch: [1][26300/36908] Elapsed 349m 9s (remain 140m 48s) Loss: 0.0023(0.0102) Grad: 10495.5488  LR: 0.000006  \n",
      "Epoch: [1][26400/36908] Elapsed 350m 27s (remain 139m 28s) Loss: 0.0062(0.0102) Grad: 81616.3203  LR: 0.000006  \n",
      "Epoch: [1][26500/36908] Elapsed 351m 45s (remain 138m 8s) Loss: 0.0036(0.0102) Grad: 55177.9727  LR: 0.000006  \n",
      "Epoch: [1][26600/36908] Elapsed 353m 4s (remain 136m 48s) Loss: 0.0025(0.0101) Grad: 76676.8359  LR: 0.000006  \n",
      "Epoch: [1][26700/36908] Elapsed 354m 23s (remain 135m 28s) Loss: 0.0004(0.0101) Grad: 1584.2217  LR: 0.000006  \n",
      "Epoch: [1][26800/36908] Elapsed 355m 40s (remain 134m 7s) Loss: 0.0065(0.0101) Grad: 48666.4180  LR: 0.000006  \n",
      "Epoch: [1][26900/36908] Elapsed 356m 57s (remain 132m 47s) Loss: 0.0008(0.0101) Grad: 1135.2306  LR: 0.000006  \n",
      "Epoch: [1][27000/36908] Elapsed 358m 17s (remain 131m 27s) Loss: 0.0075(0.0100) Grad: 174311.2188  LR: 0.000006  \n",
      "Epoch: [1][27100/36908] Elapsed 359m 35s (remain 130m 7s) Loss: 0.0109(0.0100) Grad: 104968.0000  LR: 0.000006  \n",
      "Epoch: [1][27200/36908] Elapsed 360m 52s (remain 128m 46s) Loss: 0.0018(0.0100) Grad: 2670.7773  LR: 0.000006  \n",
      "Epoch: [1][27300/36908] Elapsed 362m 11s (remain 127m 27s) Loss: 0.0016(0.0100) Grad: 6633.2715  LR: 0.000006  \n",
      "Epoch: [1][27400/36908] Elapsed 363m 29s (remain 126m 7s) Loss: 0.0018(0.0099) Grad: 12291.6299  LR: 0.000006  \n",
      "Epoch: [1][27500/36908] Elapsed 364m 47s (remain 124m 46s) Loss: 0.0079(0.0099) Grad: 37832.8320  LR: 0.000006  \n",
      "Epoch: [1][27600/36908] Elapsed 366m 4s (remain 123m 26s) Loss: 0.0004(0.0099) Grad: 2941.1858  LR: 0.000006  \n",
      "Epoch: [1][27700/36908] Elapsed 367m 22s (remain 122m 6s) Loss: 0.0007(0.0099) Grad: 1883.9335  LR: 0.000006  \n",
      "Epoch: [1][27800/36908] Elapsed 368m 44s (remain 120m 47s) Loss: 0.0038(0.0099) Grad: 10343.8330  LR: 0.000005  \n",
      "Epoch: [1][27900/36908] Elapsed 370m 10s (remain 119m 29s) Loss: 0.0031(0.0098) Grad: 28904.8340  LR: 0.000005  \n",
      "Epoch: [1][28000/36908] Elapsed 371m 31s (remain 118m 10s) Loss: 0.0049(0.0098) Grad: 8320.3711  LR: 0.000005  \n",
      "Epoch: [1][28100/36908] Elapsed 372m 49s (remain 116m 50s) Loss: 0.0204(0.0098) Grad: 329838.9062  LR: 0.000005  \n",
      "Epoch: [1][28200/36908] Elapsed 374m 7s (remain 115m 30s) Loss: 0.0088(0.0098) Grad: 34547.3789  LR: 0.000005  \n",
      "Epoch: [1][28300/36908] Elapsed 375m 25s (remain 114m 10s) Loss: 0.0024(0.0098) Grad: 2563.4104  LR: 0.000005  \n",
      "Epoch: [1][28400/36908] Elapsed 376m 43s (remain 112m 50s) Loss: 0.0197(0.0097) Grad: 330699.0938  LR: 0.000005  \n",
      "Epoch: [1][28500/36908] Elapsed 378m 7s (remain 111m 32s) Loss: 0.0015(0.0097) Grad: 2874.5950  LR: 0.000005  \n",
      "Epoch: [1][28600/36908] Elapsed 379m 28s (remain 110m 13s) Loss: 0.0006(0.0097) Grad: 5631.7974  LR: 0.000005  \n",
      "Epoch: [1][28700/36908] Elapsed 380m 46s (remain 108m 53s) Loss: 0.0002(0.0097) Grad: 187.1778  LR: 0.000005  \n",
      "Epoch: [1][28800/36908] Elapsed 382m 5s (remain 107m 33s) Loss: 0.0132(0.0097) Grad: 112323.9531  LR: 0.000005  \n",
      "Epoch: [1][28900/36908] Elapsed 383m 26s (remain 106m 13s) Loss: 0.0029(0.0097) Grad: 6671.6440  LR: 0.000005  \n",
      "Epoch: [1][29000/36908] Elapsed 384m 45s (remain 104m 54s) Loss: 0.0003(0.0096) Grad: 73.7158  LR: 0.000005  \n",
      "Epoch: [1][29100/36908] Elapsed 386m 4s (remain 103m 34s) Loss: 0.0111(0.0096) Grad: 35045.0781  LR: 0.000005  \n",
      "Epoch: [1][29200/36908] Elapsed 387m 24s (remain 102m 14s) Loss: 0.0066(0.0096) Grad: 30743.4004  LR: 0.000005  \n",
      "Epoch: [1][29300/36908] Elapsed 388m 45s (remain 100m 55s) Loss: 0.0098(0.0096) Grad: 103938.0859  LR: 0.000005  \n",
      "Epoch: [1][29400/36908] Elapsed 390m 4s (remain 99m 35s) Loss: 0.0033(0.0095) Grad: 9389.7871  LR: 0.000005  \n",
      "Epoch: [1][29500/36908] Elapsed 391m 23s (remain 98m 16s) Loss: 0.0108(0.0095) Grad: 14194.1641  LR: 0.000004  \n",
      "Epoch: [1][29600/36908] Elapsed 392m 42s (remain 96m 56s) Loss: 0.0053(0.0095) Grad: 6058.0977  LR: 0.000004  \n",
      "Epoch: [1][29700/36908] Elapsed 394m 6s (remain 95m 37s) Loss: 0.0203(0.0095) Grad: 290727.4062  LR: 0.000004  \n",
      "Epoch: [1][29800/36908] Elapsed 395m 27s (remain 94m 18s) Loss: 0.0019(0.0095) Grad: 1226.7664  LR: 0.000004  \n",
      "Epoch: [1][29900/36908] Elapsed 396m 46s (remain 92m 58s) Loss: 0.0002(0.0095) Grad: 117.4892  LR: 0.000004  \n",
      "Epoch: [1][30000/36908] Elapsed 398m 5s (remain 91m 38s) Loss: 0.0007(0.0094) Grad: 607.3788  LR: 0.000004  \n",
      "Epoch: [1][30100/36908] Elapsed 399m 29s (remain 90m 20s) Loss: 0.0087(0.0094) Grad: 61601.3984  LR: 0.000004  \n",
      "Epoch: [1][30200/36908] Elapsed 400m 50s (remain 89m 1s) Loss: 0.0018(0.0094) Grad: 1268.1465  LR: 0.000004  \n",
      "Epoch: [1][30300/36908] Elapsed 402m 8s (remain 87m 41s) Loss: 0.0162(0.0094) Grad: 291503.3438  LR: 0.000004  \n",
      "Epoch: [1][30400/36908] Elapsed 403m 29s (remain 86m 21s) Loss: 0.0023(0.0094) Grad: 27945.3281  LR: 0.000004  \n",
      "Epoch: [1][30500/36908] Elapsed 404m 52s (remain 85m 2s) Loss: 0.0007(0.0094) Grad: 303.4032  LR: 0.000004  \n",
      "Epoch: [1][30600/36908] Elapsed 406m 17s (remain 83m 44s) Loss: 0.0052(0.0093) Grad: 72695.3125  LR: 0.000004  \n",
      "Epoch: [1][30700/36908] Elapsed 407m 41s (remain 82m 25s) Loss: 0.0044(0.0093) Grad: 15476.2988  LR: 0.000004  \n",
      "Epoch: [1][30800/36908] Elapsed 409m 5s (remain 81m 6s) Loss: 0.0102(0.0093) Grad: 174193.6094  LR: 0.000004  \n",
      "Epoch: [1][30900/36908] Elapsed 410m 26s (remain 79m 47s) Loss: 0.0032(0.0093) Grad: 12144.6006  LR: 0.000004  \n",
      "Epoch: [1][31000/36908] Elapsed 411m 45s (remain 78m 27s) Loss: 0.0031(0.0093) Grad: 16807.1797  LR: 0.000004  \n",
      "Epoch: [1][31100/36908] Elapsed 413m 3s (remain 77m 7s) Loss: 0.0023(0.0093) Grad: 3642.4268  LR: 0.000003  \n",
      "Epoch: [1][31200/36908] Elapsed 414m 25s (remain 75m 48s) Loss: 0.0058(0.0092) Grad: 19298.1074  LR: 0.000003  \n",
      "Epoch: [1][31300/36908] Elapsed 415m 47s (remain 74m 28s) Loss: 0.0099(0.0092) Grad: 25661.6973  LR: 0.000003  \n",
      "Epoch: [1][31400/36908] Elapsed 417m 5s (remain 73m 8s) Loss: 0.0040(0.0092) Grad: 21765.8145  LR: 0.000003  \n",
      "Epoch: [1][31500/36908] Elapsed 418m 24s (remain 71m 48s) Loss: 0.0007(0.0092) Grad: 2680.3472  LR: 0.000003  \n",
      "Epoch: [1][31600/36908] Elapsed 419m 43s (remain 70m 29s) Loss: 0.0007(0.0092) Grad: 300.3574  LR: 0.000003  \n",
      "Epoch: [1][31700/36908] Elapsed 421m 6s (remain 69m 10s) Loss: 0.0034(0.0091) Grad: 20057.4766  LR: 0.000003  \n",
      "Epoch: [1][31800/36908] Elapsed 422m 24s (remain 67m 50s) Loss: 0.0001(0.0091) Grad: 282.3476  LR: 0.000003  \n",
      "Epoch: [1][31900/36908] Elapsed 423m 44s (remain 66m 30s) Loss: 0.0004(0.0091) Grad: 1256.6971  LR: 0.000003  \n",
      "Epoch: [1][32000/36908] Elapsed 425m 7s (remain 65m 11s) Loss: 0.0120(0.0091) Grad: 177335.2656  LR: 0.000003  \n",
      "Epoch: [1][32100/36908] Elapsed 426m 28s (remain 63m 51s) Loss: 0.0050(0.0091) Grad: 127181.7109  LR: 0.000003  \n",
      "Epoch: [1][32200/36908] Elapsed 427m 46s (remain 62m 31s) Loss: 0.0051(0.0091) Grad: 61839.9453  LR: 0.000003  \n",
      "Epoch: [1][32300/36908] Elapsed 429m 2s (remain 61m 11s) Loss: 0.0016(0.0091) Grad: 1013.0582  LR: 0.000003  \n",
      "Epoch: [1][32400/36908] Elapsed 430m 21s (remain 59m 51s) Loss: 0.0032(0.0090) Grad: 18130.4902  LR: 0.000003  \n",
      "Epoch: [1][32500/36908] Elapsed 431m 42s (remain 58m 32s) Loss: 0.0019(0.0090) Grad: 5096.4141  LR: 0.000003  \n",
      "Epoch: [1][32600/36908] Elapsed 433m 2s (remain 57m 12s) Loss: 0.0053(0.0090) Grad: 32665.2227  LR: 0.000003  \n",
      "Epoch: [1][32700/36908] Elapsed 434m 21s (remain 55m 52s) Loss: 0.0006(0.0090) Grad: 8334.1270  LR: 0.000003  \n",
      "Epoch: [1][32800/36908] Elapsed 435m 38s (remain 54m 32s) Loss: 0.0013(0.0090) Grad: 10449.9141  LR: 0.000002  \n",
      "Epoch: [1][32900/36908] Elapsed 436m 58s (remain 53m 13s) Loss: 0.0030(0.0090) Grad: 4571.8013  LR: 0.000002  \n",
      "Epoch: [1][33000/36908] Elapsed 438m 16s (remain 51m 53s) Loss: 0.0124(0.0089) Grad: 36505.3984  LR: 0.000002  \n",
      "Epoch: [1][33100/36908] Elapsed 439m 33s (remain 50m 33s) Loss: 0.0050(0.0089) Grad: 63481.4492  LR: 0.000002  \n",
      "Epoch: [1][33200/36908] Elapsed 440m 51s (remain 49m 13s) Loss: 0.0026(0.0089) Grad: 2586.0750  LR: 0.000002  \n",
      "Epoch: [1][33300/36908] Elapsed 442m 11s (remain 47m 53s) Loss: 0.0024(0.0089) Grad: 7643.9355  LR: 0.000002  \n",
      "Epoch: [1][33400/36908] Elapsed 443m 36s (remain 46m 34s) Loss: 0.0004(0.0089) Grad: 67.8354  LR: 0.000002  \n",
      "Epoch: [1][33500/36908] Elapsed 444m 56s (remain 45m 15s) Loss: 0.0102(0.0089) Grad: 38370.1445  LR: 0.000002  \n",
      "Epoch: [1][33600/36908] Elapsed 446m 16s (remain 43m 55s) Loss: 0.0028(0.0089) Grad: 5662.6353  LR: 0.000002  \n",
      "Epoch: [1][33700/36908] Elapsed 447m 36s (remain 42m 35s) Loss: 0.0014(0.0088) Grad: 144.8909  LR: 0.000002  \n",
      "Epoch: [1][33800/36908] Elapsed 448m 54s (remain 41m 15s) Loss: 0.0022(0.0088) Grad: 76857.7969  LR: 0.000002  \n",
      "Epoch: [1][33900/36908] Elapsed 450m 11s (remain 39m 55s) Loss: 0.0011(0.0088) Grad: 19765.5117  LR: 0.000002  \n",
      "Epoch: [1][34000/36908] Elapsed 451m 32s (remain 38m 36s) Loss: 0.0177(0.0088) Grad: 95382.5781  LR: 0.000002  \n",
      "Epoch: [1][34100/36908] Elapsed 452m 50s (remain 37m 16s) Loss: 0.0001(0.0088) Grad: 37.8307  LR: 0.000002  \n",
      "Epoch: [1][34200/36908] Elapsed 454m 7s (remain 35m 56s) Loss: 0.0078(0.0088) Grad: 17320.5293  LR: 0.000002  \n",
      "Epoch: [1][34300/36908] Elapsed 455m 29s (remain 34m 37s) Loss: 0.0017(0.0087) Grad: 2724.5381  LR: 0.000002  \n",
      "Epoch: [1][34400/36908] Elapsed 456m 49s (remain 33m 17s) Loss: 0.0014(0.0087) Grad: 3938.4094  LR: 0.000002  \n",
      "Epoch: [1][34500/36908] Elapsed 458m 8s (remain 31m 57s) Loss: 0.0004(0.0087) Grad: 949.0665  LR: 0.000001  \n",
      "Epoch: [1][34600/36908] Elapsed 459m 29s (remain 30m 38s) Loss: 0.0018(0.0087) Grad: 10014.2578  LR: 0.000001  \n",
      "Epoch: [1][34700/36908] Elapsed 460m 54s (remain 29m 18s) Loss: 0.0012(0.0087) Grad: 1309.0471  LR: 0.000001  \n",
      "Epoch: [1][34800/36908] Elapsed 462m 17s (remain 27m 59s) Loss: 0.0003(0.0087) Grad: 43.2503  LR: 0.000001  \n",
      "Epoch: [1][34900/36908] Elapsed 463m 43s (remain 26m 40s) Loss: 0.0024(0.0087) Grad: 13660.6523  LR: 0.000001  \n",
      "Epoch: [1][35000/36908] Elapsed 465m 4s (remain 25m 20s) Loss: 0.0082(0.0086) Grad: 22425.7246  LR: 0.000001  \n",
      "Epoch: [1][35100/36908] Elapsed 466m 26s (remain 24m 0s) Loss: 0.0047(0.0086) Grad: 31473.8359  LR: 0.000001  \n",
      "Epoch: [1][35200/36908] Elapsed 467m 51s (remain 22m 41s) Loss: 0.0017(0.0086) Grad: 1168.3462  LR: 0.000001  \n",
      "Epoch: [1][35300/36908] Elapsed 469m 9s (remain 21m 21s) Loss: 0.0067(0.0086) Grad: 95822.3125  LR: 0.000001  \n",
      "Epoch: [1][35400/36908] Elapsed 470m 28s (remain 20m 1s) Loss: 0.0044(0.0086) Grad: 2311.1880  LR: 0.000001  \n",
      "Epoch: [1][35500/36908] Elapsed 471m 50s (remain 18m 42s) Loss: 0.0114(0.0086) Grad: 124773.0312  LR: 0.000001  \n",
      "Epoch: [1][35600/36908] Elapsed 473m 8s (remain 17m 22s) Loss: 0.0015(0.0086) Grad: 1370.0338  LR: 0.000001  \n",
      "Epoch: [1][35700/36908] Elapsed 474m 27s (remain 16m 2s) Loss: 0.0008(0.0085) Grad: 2500.2251  LR: 0.000001  \n",
      "Epoch: [1][35800/36908] Elapsed 475m 47s (remain 14m 42s) Loss: 0.0054(0.0085) Grad: 64184.0078  LR: 0.000001  \n",
      "Epoch: [1][35900/36908] Elapsed 477m 6s (remain 13m 22s) Loss: 0.0034(0.0085) Grad: 3606.1384  LR: 0.000001  \n",
      "Epoch: [1][36000/36908] Elapsed 478m 24s (remain 12m 3s) Loss: 0.0020(0.0085) Grad: 45315.6562  LR: 0.000001  \n",
      "Epoch: [1][36100/36908] Elapsed 479m 43s (remain 10m 43s) Loss: 0.0007(0.0085) Grad: 4220.5845  LR: 0.000000  \n",
      "Epoch: [1][36200/36908] Elapsed 481m 3s (remain 9m 23s) Loss: 0.0025(0.0085) Grad: 3775.3237  LR: 0.000000  \n",
      "Epoch: [1][36300/36908] Elapsed 482m 22s (remain 8m 3s) Loss: 0.0005(0.0085) Grad: 22142.4531  LR: 0.000000  \n",
      "Epoch: [1][36400/36908] Elapsed 483m 40s (remain 6m 44s) Loss: 0.0042(0.0085) Grad: 26971.6641  LR: 0.000000  \n",
      "Epoch: [1][36500/36908] Elapsed 485m 2s (remain 5m 24s) Loss: 0.0041(0.0084) Grad: 147967.8594  LR: 0.000000  \n",
      "Epoch: [1][36600/36908] Elapsed 486m 19s (remain 4m 4s) Loss: 0.0001(0.0084) Grad: 194.9817  LR: 0.000000  \n",
      "Epoch: [1][36700/36908] Elapsed 487m 37s (remain 2m 45s) Loss: 0.0011(0.0084) Grad: 12475.0693  LR: 0.000000  \n",
      "Epoch: [1][36800/36908] Elapsed 488m 57s (remain 1m 25s) Loss: 0.0023(0.0084) Grad: 4246.0835  LR: 0.000000  \n",
      "Epoch: [1][36900/36908] Elapsed 490m 15s (remain 0m 5s) Loss: 0.0099(0.0084) Grad: 71638.6172  LR: 0.000000  \n",
      "Epoch: [1][36907/36908] Elapsed 490m 20s (remain 0m 0s) Loss: 0.0025(0.0084) Grad: 4193.7109  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 18m 14s) Loss: 0.0008(0.0008) \n",
      "EVAL: [100/1192] Elapsed 0m 32s (remain 5m 46s) Loss: 0.0276(0.0059) \n",
      "EVAL: [200/1192] Elapsed 1m 2s (remain 5m 5s) Loss: 0.0065(0.0058) \n",
      "EVAL: [300/1192] Elapsed 1m 32s (remain 4m 32s) Loss: 0.0095(0.0067) \n",
      "EVAL: [400/1192] Elapsed 2m 2s (remain 4m 0s) Loss: 0.0000(0.0066) \n",
      "EVAL: [500/1192] Elapsed 2m 33s (remain 3m 31s) Loss: 0.0253(0.0064) \n",
      "EVAL: [600/1192] Elapsed 3m 4s (remain 3m 1s) Loss: 0.0100(0.0066) \n",
      "EVAL: [700/1192] Elapsed 3m 36s (remain 2m 31s) Loss: 0.0064(0.0074) \n",
      "EVAL: [800/1192] Elapsed 4m 7s (remain 2m 0s) Loss: 0.0093(0.0075) \n",
      "EVAL: [900/1192] Elapsed 4m 38s (remain 1m 29s) Loss: 0.0044(0.0076) \n",
      "EVAL: [1000/1192] Elapsed 5m 11s (remain 0m 59s) Loss: 0.0000(0.0073) \n",
      "EVAL: [1100/1192] Elapsed 5m 41s (remain 0m 28s) Loss: 0.0231(0.0071) \n",
      "EVAL: [1191/1192] Elapsed 6m 8s (remain 0m 0s) Loss: 0.0000(0.0070) \n",
      "Epoch 1 - avg_train_loss: 0.0084  avg_val_loss: 0.0070  time: 29794s\n",
      "Epoch 1 - Score: 0.8944\n",
      "Epoch 1 - Save Best Score: 0.8944 Model\n",
      "best_thres: 0.53  score: 0.89284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n",
      "load weights from ../output/nbme-score-clinical-patient-notes/nbme-exp086/fold2_best.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854321d787404957b90fea69b91d7d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x7f2dc5304170>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 132, in __del__\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n",
      "load weights from ../output/nbme-score-clinical-patient-notes/nbme-exp086/fold3_best.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45def77706594c009ece49915ba9e713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-essence",
   "metadata": {
    "id": "N5kZWfSSfJMf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "name": "nbme-exp085.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 641.572862,
   "end_time": "2022-02-27T11:39:50.972497",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-27T11:29:09.399635",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "054630edadaa453fb86d66a20e49030f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_747b8a73ce544c569f4d063fc9d6d18a",
      "placeholder": "​",
      "style": "IPY_MODEL_c7aa62f5eaca401dbbfbfd5f843d6a59",
      "value": " 28720/42146 [00:27&lt;00:07, 1839.83it/s]"
     }
    },
    "18b47303dd5b48bfb118053a80c44a40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_971945a52a6d4f06ba35e5363d264037",
       "IPY_MODEL_2d7cfbb1d1a54c0590e59de0b280ab22",
       "IPY_MODEL_054630edadaa453fb86d66a20e49030f"
      ],
      "layout": "IPY_MODEL_8fce27d68c1c41ec9a3c58d439c2a5e7"
     }
    },
    "28c710503f154bdea731991801a85a47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d7cfbb1d1a54c0590e59de0b280ab22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28c710503f154bdea731991801a85a47",
      "max": 42146,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8c57bf78a80247db9521bd5b163502ef",
      "value": 28905
     }
    },
    "747b8a73ce544c569f4d063fc9d6d18a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c57bf78a80247db9521bd5b163502ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8fce27d68c1c41ec9a3c58d439c2a5e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "971945a52a6d4f06ba35e5363d264037": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9a3708f7e4c486da3a09e066b6936fb",
      "placeholder": "​",
      "style": "IPY_MODEL_cd1144e40332427fa3e8d5bc7f57b924",
      "value": " 69%"
     }
    },
    "c7aa62f5eaca401dbbfbfd5f843d6a59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd1144e40332427fa3e8d5bc7f57b924": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e9a3708f7e4c486da3a09e066b6936fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
