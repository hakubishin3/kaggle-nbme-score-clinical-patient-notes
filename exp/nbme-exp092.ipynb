{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "documentary-majority",
   "metadata": {
    "id": "national-fancy"
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-greene",
   "metadata": {
    "id": "copyrighted-centre"
   },
   "source": [
    "- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-garage",
   "metadata": {
    "id": "imported-offset"
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "postal-brighton",
   "metadata": {
    "id": "complimentary-wyoming"
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"nbme-exp092\"\n",
    "ENV = \"local\"\n",
    "DEBUG_MODE = False\n",
    "SUBMISSION_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "suffering-tumor",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y03AHjwJAlGL",
    "outputId": "c33caf3f-c530-4628-bcbf-b9af87b252a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pediatric-expert",
   "metadata": {
    "id": "allied-circuit"
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    env=ENV\n",
    "    exp_name=EXP_NAME\n",
    "    debug=DEBUG_MODE\n",
    "    submission=SUBMISSION_MODE\n",
    "    apex=True\n",
    "    input_dir=None\n",
    "    output_dir=None\n",
    "    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n",
    "    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n",
    "    competition_name=\"nbme-score-clinical-patient-notes\"\n",
    "    id_col=\"id\"\n",
    "    target_col=\"location\"\n",
    "    pretrained_model_name=\"microsoft/deberta-v3-large\"\n",
    "    tokenizer=None\n",
    "    max_len=None\n",
    "    pseudo_plain_path='../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl'\n",
    "    #pseudo_plain_path=\"./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\"\n",
    "    n_pseudo_labels=10725\n",
    "    output_dim=1\n",
    "    dropout=0.2\n",
    "    num_workers=4\n",
    "    batch_size=3\n",
    "    lr=2e-5\n",
    "    betas=(0.9, 0.98)\n",
    "    weight_decay=0.1\n",
    "    alpha=1\n",
    "    gamma=2\n",
    "    smoothing=0.0001\n",
    "    num_warmup_steps_rate=0.1\n",
    "    batch_scheduler=True\n",
    "    epochs=5\n",
    "    n_fold=4\n",
    "    train_fold=[0, 1, 2, 3]\n",
    "    seed=71\n",
    "    gradient_accumulation_steps=2\n",
    "    max_grad_norm=1000\n",
    "    print_freq=100\n",
    "    train=True\n",
    "    inference=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "robust-mention",
   "metadata": {
    "id": "geographic-hindu"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.train_fold = [0, 1]\n",
    "\n",
    "if CFG.submission:\n",
    "    CFG.train = False\n",
    "    CFG.inference = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-ridge",
   "metadata": {
    "id": "confident-fifth"
   },
   "source": [
    "## Directory Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "judicial-clone",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "miniature-greeting",
    "outputId": "6a439b4d-636c-4026-8cfd-f86093855807"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "print(CFG.env)\n",
    "if CFG.env == \"colab\":\n",
    "    # colab環境\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "    # install packages\n",
    "    !pip install transformers==4.16.2\n",
    "    !pip install -q sentencepiece==0.1.96\n",
    "\n",
    "elif CFG.env == \"local\":\n",
    "    # ローカルサーバ\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "\n",
    "elif CFG.env == \"kaggle\":\n",
    "    # kaggle環境\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "heated-offer",
   "metadata": {
    "id": "nMFg9zv8YGcx"
   },
   "outputs": [],
   "source": [
    "# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n",
    "# This must be done before importing transformers\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "if CFG.env == \"colab\":\n",
    "    input_dir = Path(\"./drive/MyDrive/00.kaggle/input/deberta-v2-3-fast-tokenizer\")\n",
    "    transformers_path = Path(\"/usr/local/lib/python3.7/dist-packages/transformers\")\n",
    "else:\n",
    "    input_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n",
    "    transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n",
    "\n",
    "convert_file = input_dir / \"convert_slow_tokenizer.py\"\n",
    "conversion_path = transformers_path/convert_file.name\n",
    "\n",
    "if conversion_path.exists():\n",
    "    conversion_path.unlink()\n",
    "\n",
    "shutil.copy(convert_file, transformers_path)\n",
    "deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n",
    "\n",
    "for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n",
    "    filepath = deberta_v2_path/filename\n",
    "    if filepath.exists():\n",
    "        filepath.unlink()\n",
    "\n",
    "    shutil.copy(input_dir/filename, filepath)\n",
    "    \n",
    "    \n",
    "from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dependent-fever",
   "metadata": {
    "id": "guilty-filename"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import BartModel,BertModel,BertTokenizer\n",
    "from transformers import DebertaModel,DebertaTokenizer\n",
    "from transformers import RobertaModel,RobertaTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-portuguese",
   "metadata": {
    "id": "cubic-designation"
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "approximate-argument",
   "metadata": {
    "id": "opposite-plasma"
   },
   "outputs": [],
   "source": [
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "corporate-australia",
   "metadata": {
    "id": "multiple-poland"
   },
   "outputs": [],
   "source": [
    "def create_labels_for_scoring(df):\n",
    "    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n",
    "    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, \"location\"]\n",
    "        if lst:\n",
    "            new_lst = \";\".join(lst)\n",
    "            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n",
    "\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df[\"location_for_create_labels\"].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, token_probs, tokenizer):\n",
    "    res = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n",
    "        encoded = tokenizer(\n",
    "            text=text,\n",
    "            max_length=CFG.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n",
    "            start, end = offset_mapping\n",
    "            res[i][start:end] = pred\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_predicted_location_str(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        # result = np.where(char_prob >= th)[0] + 1\n",
    "        result = np.where(char_prob >= th)[0]\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        # result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = [f\"{min(r)} {max(r) + 1}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def scoring(df, th=0.5, use_token_prob=True):\n",
    "    labels = create_labels_for_scoring(df)\n",
    "\n",
    "    if use_token_prob:\n",
    "        token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n",
    "        char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n",
    "    else:\n",
    "        char_probs = df[[str(i) for i in range(CFG.max_char_len)]].values\n",
    "        char_probs = [char_probs[i] for i in range(len(char_probs))]\n",
    "\n",
    "    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n",
    "    preds = get_predictions(predicted_location_str)\n",
    "\n",
    "    score = get_score(labels, preds)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_best_thres(oof_df):\n",
    "    def f1_opt(x):\n",
    "        return -1 * scoring(oof_df, th=x)\n",
    "\n",
    "    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n",
    "    return best_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "legal-detective",
   "metadata": {
    "id": "seventh-fighter"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bright-turtle",
   "metadata": {
    "id": "fifty-boundary"
   },
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "moved-lending",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(texts, preds):\n",
    "    fix_tokenize_dict = {\n",
    "        'heart': ['h', 'eart'],\n",
    "        'hair': ['h', 'air'],\n",
    "        'adderal': ['a', 'dderal'],\n",
    "        'mother': ['m', 'other'],\n",
    "        'intermittent': ['i', 'ntermittent'],\n",
    "        'temperature': ['t', 'emperature'],\n",
    "        'episodes': ['e', 'pisodes'],\n",
    "        'no': ['n', 'o'],\n",
    "        'has': ['h', 'as'],\n",
    "        'LMP': ['L', 'MP'],\n",
    "        '10': ['1', '0'],\n",
    "        'blood': ['b', 'lood'],\n",
    "        'recurrent': ['r', 'ecurrent'],\n",
    "        'denies': ['d', 'enies'],\n",
    "        'sudden': ['s', 'udden'],\n",
    "        'Sexually': ['S', 'exually'],\n",
    "        'up': ['u', 'p'],\n",
    "        'wakes': ['w', 'akes'],\n",
    "        'sweats': ['s', 'weats'],\n",
    "        'hot': ['h', 'ot'],\n",
    "        'drenched': ['d', 'renched'],\n",
    "        'gnawing': ['g', 'nawing'],\n",
    "        'Uses': ['U', 'ses'],\n",
    "        'Begin': ['B', 'egin'],\n",
    "        'Nausea': ['N', 'ausea'],\n",
    "        'Burning': ['B', 'urning'],\n",
    "        'Started': ['S', 'tarted'],\n",
    "        'neurvousness': ['n', 'eurvousness'],\n",
    "        'constipation': ['c', 'onstipation'],\n",
    "        'nervousness': ['n', 'ervousness'],\n",
    "        'cold': ['c', 'old'],\n",
    "        'loss': ['l', 'oss'],\n",
    "        'CBC': ['C', 'BC'],\n",
    "        'Hx': ['H', 'x'],\n",
    "        'tingling': ['t', 'ingling'],\n",
    "        'feels': ['f', 'eels'],\n",
    "        'Lost': ['L', 'ost'],\n",
    "        'she': ['s', 'he'],\n",
    "        'racing': ['r', 'acing'],\n",
    "        'throat': ['t', 'hroat'],\n",
    "        'PATIENT': ['P', 'ATIENT'],\n",
    "        'recreational': ['r', 'ecreational'],\n",
    "        'clammy': ['c', 'lammy'],\n",
    "        'numbness': ['n', 'umbness'],\n",
    "        'like': ['l', 'ike'],\n",
    "        'reports': ['r', 'eports'],\n",
    "        'exercise': ['e', 'xercise'],\n",
    "        'started': ['s', 'tarted'],\n",
    "        'brough': ['b', 'rough'],\n",
    "        'Associated': ['A', 'ssociated'],\n",
    "        'exacerbated': ['e', 'xacerbated'],\n",
    "        'sharp': ['s', 'harp'],\n",
    "        'cannot': ['c', 'annot'],\n",
    "        'heavy': ['h', 'eavy'],\n",
    "        'fatigue': ['f', 'atigue'],\n",
    "        'trouble': ['t', 'rouble'],\n",
    "        'hearing': ['h', 'earing'],\n",
    "        'reduced': ['r', 'educed'],\n",
    "        'lack': ['l', 'ack'],\n",
    "        'vomiting': ['v', 'omiting'],\n",
    "        'generalized': ['g', 'eneralized'],\n",
    "        'body': ['b', 'ody'],\n",
    "        'all': ['a', 'll'],\n",
    "        'scratchy': ['s', 'cratchy'],\n",
    "        'mom': ['m', 'om'],\n",
    "        'discomfort': ['d', 'iscomfort'],\n",
    "        'CAD': ['C', 'AD'],\n",
    "        'Thyroid': ['T', 'hyroid'],\n",
    "        'BLADDER': ['B', 'LADDER'],\n",
    "        'diarrhea': ['d', 'iarrhea'],\n",
    "        'Started': ['S', 'tarted'],\n",
    "        'Vaginal': ['V', 'aginal'],\n",
    "        'sleeping': ['s', 'leeping'],\n",
    "        'UNCLE': ['U', 'NCLE'],\n",
    "        'USING': ['U', 'SING'],\n",
    "        'BURNING': ['B', 'URNING'],\n",
    "        'GETTING': ['G', 'ETTING'],\n",
    "        'ETOH': ['E', 'TOH'],\n",
    "        'ON': ['O', 'N'],\n",
    "        'INITIALLY': ['I', 'NITIALLY'],\n",
    "        'epigastric': ['e', 'pigastric'],\n",
    "        'occurs': ['o', 'ccurs'],\n",
    "        'began': ['b', 'egan'],\n",
    "        'alleviated': ['a', 'lleviated'],\n",
    "        'overwhelmed': ['o', 'verwhelmed'],\n",
    "        'clamminess': ['c', 'lamminess'],\n",
    "        'strongly': ['s', 'trongly'],\n",
    "        'lump': ['l', 'ump'],\n",
    "        'drugs': ['d', 'rugs'],\n",
    "        'chest': ['c', 'hest'],\n",
    "        'stuffy': ['s', 'tuffy'],\n",
    "        'changes': ['c', 'hanges'],\n",
    "        'trouble': ['t', 'rouble'],\n",
    "        'takes': ['t', 'akes'],\n",
    "        'tossing': ['t', 'ossing'],\n",
    "        'Fam': ['F', 'am'],\n",
    "        'sweating': ['s', 'weating'],\n",
    "        'dyspareunia': ['d', 'yspareunia'],\n",
    "        'irregular': ['i', 'rregular'],\n",
    "        'time': ['t', 'ime'],\n",
    "        'unpredictable': ['u', 'npredictable'],\n",
    "        'darkened': ['d', 'arkened'],\n",
    "        'anxiety': ['a', 'nxiety'],\n",
    "        'nervous': ['n', 'ervous'],\n",
    "        'TAKING': ['T', 'AKING'],\n",
    "        'losing': ['l', 'osing'],\n",
    "        'Difficulyt': ['D', 'ifficulyt'],\n",
    "        'Appetite': ['A', 'ppetite'],\n",
    "        'increased': ['i', 'ncreased'],\n",
    "        'fingers': ['f', 'ingers'],\n",
    "        'illicit': ['i', 'llicit'],\n",
    "        'claminess': ['c', 'laminess'],\n",
    "        'clamy': ['c', 'lamy'],\n",
    "        'Recently': ['R', 'ecently'],\n",
    "        'feeling': ['f', 'eeling'],\n",
    "        'aggrav': ['a', 'ggrav'],\n",
    "        'changing': ['c', 'hanging'],\n",
    "        'unable': ['u', 'nable'],\n",
    "        'SEEING': ['S', 'EEING'],\n",
    "        'staying': ['s', 'taying'],\n",
    "        'lightheadedness': ['l', 'ightheadedness'],\n",
    "        'lighheadeness': ['l', 'ighheadeness'],\n",
    "        'nail': ['n', 'ail'],\n",
    "        'pounding': ['p', 'ounding'],\n",
    "        'My': ['M', 'y'],\n",
    "        'Father': ['F', 'ather'],\n",
    "        'urinary': ['u', 'rinary'],\n",
    "        'pain': ['p', 'ain'],\n",
    "        'not': ['n', 'ot'],\n",
    "        'lower': ['l', 'ower'],\n",
    "        'menses': ['m', 'enses'],\n",
    "        'at': ['a', 't'],\n",
    "        'takes': ['t', 'akes'],\n",
    "        'initally': ['i', 'nitally'],\n",
    "        'melena': ['m', 'elena'],\n",
    "        'BOWEL': ['B', 'OWEL'],\n",
    "        'WEIGHT': ['W', 'EIGHT'],\n",
    "        'difficulty': ['d', 'ifficulty'],\n",
    "        'condo': ['c', 'ondo'],\n",
    "        'experiences': ['e', 'xperiences'],\n",
    "        'stuffy': ['s', 'tuffy'],\n",
    "        'rhinorrhea': ['r', 'hinorrhea'],\n",
    "        'felt': ['f', 'elt'],\n",
    "        'feverish': ['f', 'everish'],\n",
    "        'CYCLE': ['C', 'YCLE'],\n",
    "        'tampon': ['t', 'ampon'],\n",
    "        'Last': ['L', 'ast'],\n",
    "        'Son': ['S', 'on'],\n",
    "        'saw': ['s', 'aw'],\n",
    "        'tightness': ['t', 'ightness'],\n",
    "        'rash': ['r', 'ash'],\n",
    "        'ibuprofen': ['i', 'buprofen'],\n",
    "        'SCRATHY': ['S', 'CRATHY'],\n",
    "        'PHOTOPHOBIA': ['P', 'HOTOPHOBIA'],\n",
    "    }\n",
    "    preds_pp = preds.copy()\n",
    "    tk0 = tqdm(range(len(preds_pp)), total=len(preds_pp))\n",
    "    for raw_idx in tk0:\n",
    "        pred = preds[raw_idx]\n",
    "        text = texts[raw_idx]\n",
    "        if len(pred) != 0:\n",
    "            # pp1: indexが1から始まる予測値は0から始まるように修正 ## 0.88579 -> 0.88702\n",
    "            if pred[0][0] == 1:\n",
    "                preds_pp[raw_idx][0][0] = 0\n",
    "            for p_index, pp in enumerate(pred):\n",
    "                start, end = pred[p_index]\n",
    "                # pp2: startとendが同じ予測値はstartを前に１ずらす ## 0.88702 -> 0.88714\n",
    "                if start == end:\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                # pp3: 始点が改行の場合始点を1つ後ろにずらす ## 0.88714 -> 0.88746\n",
    "                if text[start] == '\\n':\n",
    "                    preds_pp[raw_idx][p_index][0] = start + 1\n",
    "                    start = start + 1\n",
    "                # pp4: 1-2などは-2で予測されることがあるので修正 ## 0.88746 -> 0.88747\n",
    "                if text[start-1].isdigit() and text[start] == '-' and text[start+1].isdigit():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                if text[start-1].isdigit() and text[start] == '/' and text[start+1].isdigit():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                # pp5: 67などは7で予測されることがあるので修正 ## 0.88747 -> 0.88748\n",
    "                if text[start-1].isdigit() and text[start].isdigit():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                # pp6: 文頭が大文字で始まるものは大文字部分が除かれて予測されることがあるので修正 ## 0.88748 -> 0.88761\n",
    "                if text[start-2] == '.' and text[start-1].isupper():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                if text[start-2] == ',' and text[start-1].isupper():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                if text[start-2] == ':' and text[start-1].isupper():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                if text[start-2] == '-' and text[start-1].isupper():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                # pp7: heart -> h + eart となっているようなものを修正する ## 0.88761 -> 0.88806\n",
    "                for key, fix_tokenize in fix_tokenize_dict.items():\n",
    "                    _s, s = fix_tokenize[0], fix_tokenize[1]\n",
    "                    if text[start-1].lower() == _s.lower() and text[start:start+len(s)].lower() == s.lower():\n",
    "                        preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                        start = start - 1\n",
    "    return preds_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "stuck-converter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_from_preds_list(preds):\n",
    "    results = []\n",
    "    for pred in preds:\n",
    "        s = []\n",
    "        for p in pred:\n",
    "            s.append(' '.join(list(map(str, p))))\n",
    "        s = ';'.join(s)\n",
    "        results.append(s)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "respected-viking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trunc_pred(texts, preds):\n",
    "    preds_pp = preds.copy()\n",
    "    tk0 = tqdm(range(len(preds_pp)), total=len(preds_pp))\n",
    "    for raw_idx in tk0:\n",
    "        text = texts[raw_idx]\n",
    "        num_text = len(text)\n",
    "        preds_pp[raw_idx, num_text:] = 0\n",
    "    return preds_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "governmental-colony",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label(pn_history, location_list, max_char_len):\n",
    "    label = np.zeros(max_char_len)\n",
    "    label[len(pn_history):] = -1\n",
    "    if len(location_list) > 0:\n",
    "        for location in location_list:\n",
    "            start, end = int(location[0]), int(location[1])\n",
    "            label[start:end] = 1\n",
    "    return label\n",
    "\n",
    "def get_preds_from_results(results, texts, max_char_len):\n",
    "    labels = []\n",
    "    for idx, result in enumerate(results):\n",
    "        label = create_label(texts[idx], result, max_char_len)\n",
    "        labels.append(label)\n",
    "    labels = np.stack(labels)\n",
    "    print(labels.shape)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-springfield",
   "metadata": {
    "id": "unlimited-hotel"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "appointed-drove",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "classical-machine",
    "outputId": "918ddc66-5d2d-44dc-c637-0145350ded5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 6), (143, 3), (42146, 3), (5, 4))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(CFG.input_dir / \"train.csv\")\n",
    "features = pd.read_csv(CFG.input_dir / \"features.csv\")\n",
    "patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n",
    "test = pd.read_csv(CFG.input_dir / \"test.csv\")\n",
    "\n",
    "train.shape, features.shape, patient_notes.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cellular-boulder",
   "metadata": {
    "id": "vanilla-iceland"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n",
    "    print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-fight",
   "metadata": {
    "id": "convenient-plant"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sublime-cylinder",
   "metadata": {
    "id": "convertible-thunder"
   },
   "outputs": [],
   "source": [
    "def preprocess_features(features):\n",
    "    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "\n",
    "\n",
    "features = preprocess_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "voluntary-diary",
   "metadata": {
    "id": "a7YBS_idYKtL"
   },
   "outputs": [],
   "source": [
    "features['feature_text'] = features['feature_text'].str.lower()\n",
    "patient_notes['pn_history'] = patient_notes['pn_history'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "massive-firmware",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "charitable-memphis",
    "outputId": "fca9323a-7ebb-469c-e494-f3ea3aa5db24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 8), (5, 6))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "national-monroe",
   "metadata": {
    "id": "governing-election"
   },
   "outputs": [],
   "source": [
    "train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n",
    "train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "agricultural-tolerance",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "negative-provincial",
    "outputId": "a14cf34d-27c0-41c1-a97f-a02cb70b1482"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4399\n",
       "1    8181\n",
       "2    1296\n",
       "3     287\n",
       "4      99\n",
       "5      27\n",
       "6       9\n",
       "7       1\n",
       "8       1\n",
       "Name: annotation_length, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n",
    "display(train['annotation_length'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-january",
   "metadata": {
    "id": "arbitrary-beatles"
   },
   "source": [
    "## CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fallen-supplement",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "important-murray",
    "outputId": "0f77c675-4cc4-4be5-9d99-346e40c7135a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    3575\n",
       "1    3575\n",
       "2    3575\n",
       "3    3575\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Fold = GroupKFold(n_splits=CFG.n_fold)\n",
    "groups = train['pn_num'].values\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-xerox",
   "metadata": {
    "id": "configured-chemistry"
   },
   "source": [
    "## Setup tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "pointed-writing",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hindu-contest",
    "outputId": "93f3e77c-2a10-4a0e-8e67-65805e82e164"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "if CFG.submission:\n",
    "    tokenizer = DebertaV2TokenizerFast.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n",
    "else:\n",
    "    tokenizer = DebertaV2TokenizerFast.from_pretrained(CFG.pretrained_model_name)\n",
    "    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n",
    "\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-intermediate",
   "metadata": {
    "id": "alleged-protein"
   },
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "victorian-raise",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "18b47303dd5b48bfb118053a80c44a40",
      "971945a52a6d4f06ba35e5363d264037",
      "2d7cfbb1d1a54c0590e59de0b280ab22",
      "054630edadaa453fb86d66a20e49030f",
      "8fce27d68c1c41ec9a3c58d439c2a5e7",
      "e9a3708f7e4c486da3a09e066b6936fb",
      "cd1144e40332427fa3e8d5bc7f57b924",
      "28c710503f154bdea731991801a85a47",
      "8c57bf78a80247db9521bd5b163502ef",
      "747b8a73ce544c569f4d063fc9d6d18a",
      "c7aa62f5eaca401dbbfbfd5f843d6a59"
     ]
    },
    "id": "composed-stroke",
    "outputId": "71ab3663-99ff-4577-ac6f-28425ca4c488"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44da3e8c00134dc5a2e3b6be5e0bd6e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 284\n"
     ]
    }
   ],
   "source": [
    "pn_history_lengths = []\n",
    "tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    pn_history_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(pn_history_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "vital-complement",
   "metadata": {
    "id": "emotional-region"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdee67b3bfae498c94591f2dcef00c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 28\n"
     ]
    }
   ],
   "source": [
    "feature_text_lengths = []\n",
    "tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    feature_text_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(feature_text_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cooked-kingdom",
   "metadata": {
    "id": "wrong-leisure"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 315\n"
     ]
    }
   ],
   "source": [
    "CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n",
    "\n",
    "print(\"max length:\", CFG.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "referenced-moisture",
   "metadata": {
    "id": "convenient-gospel"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3e14ae708945db8a815cdd2d2ccc95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 950\n"
     ]
    }
   ],
   "source": [
    "pn_history_lengths = []\n",
    "tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n",
    "for text in tk0:\n",
    "    length = len(text)\n",
    "    pn_history_lengths.append(length)\n",
    "\n",
    "CFG.max_char_len = max(pn_history_lengths)\n",
    "\n",
    "print(\"max length:\", CFG.max_char_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "received-mason",
   "metadata": {
    "id": "representative-contributor"
   },
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, cfg, df, pseudo_label=None):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.max_char_len = self.cfg.max_char_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "        self.annotation_lengths = self.df[\"annotation_length\"].values\n",
    "        self.locations = self.df[\"location\"].values\n",
    "        if \"pseudo_idx\" in df.columns:\n",
    "            self.pseudo_idx = self.df[\"pseudo_idx\"].values\n",
    "            self.pseudo_label = pseudo_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def _create_mapping_from_token_to_char(self, pn_history):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        mapping_from_token_to_char = np.zeros(self.max_char_len)\n",
    "        offset_mapping = encoded[\"offset_mapping\"]\n",
    "        for i, offset in enumerate(offset_mapping):\n",
    "            start_idx, end_idx = offset\n",
    "            mapping_from_token_to_char[start_idx:end_idx] = i\n",
    "        return torch.tensor(mapping_from_token_to_char, dtype=torch.long)\n",
    "\n",
    "    def _create_label(self, pn_history, annotation_length, location_list):\n",
    "        label = np.zeros(self.max_char_len)\n",
    "        label[len(pn_history):] = -1\n",
    "        if annotation_length > 0:\n",
    "            for location in location_list:\n",
    "                for loc in [s.split() for s in location.split(\";\")]:\n",
    "                    start, end = int(loc[0]), int(loc[1])\n",
    "                    label[start:end] = 1\n",
    "        return torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        if not np.isnan(self.annotation_lengths[idx]):\n",
    "            label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n",
    "        else:\n",
    "            p_idx = int(self.pseudo_idx[idx])\n",
    "            label = torch.tensor(self.pseudo_label[p_idx], dtype=torch.float)\n",
    "        mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n",
    "        return input_, label, mapping_from_token_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abstract-caution",
   "metadata": {
    "id": "decent-johnson"
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.max_char_len = self.cfg.max_char_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def _create_mapping_from_token_to_char(self, pn_history):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        mapping_from_token_to_char = np.zeros(self.max_char_len)\n",
    "        offset_mapping = encoded[\"offset_mapping\"]\n",
    "        for i, offset in enumerate(offset_mapping):\n",
    "            start_idx, end_idx = offset\n",
    "            mapping_from_token_to_char[start_idx:end_idx] = i\n",
    "        return torch.tensor(mapping_from_token_to_char, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n",
    "        return input_, mapping_from_token_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-newton",
   "metadata": {
    "id": "arctic-joint"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "recognized-parade",
   "metadata": {
    "id": "qTRu8eKOTlcX"
   },
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import MaskedLMOutput\n",
    "\n",
    "class MaskedModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(\n",
    "                cfg.pretrained_model_name,\n",
    "                output_hidden_states=False\n",
    "                )\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        \n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.pretrained_model_name, config=self.config)\n",
    "            self.lm_head = AutoModelForMaskedLM.from_pretrained(cfg.pretrained_model_name, config=self.config).cls # [cls, lm_head]\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "            self.lm_head = AutoModelForMaskedLM(self.config).cls # [cls, lm_head]\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            #position_ids=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None):\n",
    "        \n",
    "        outputs = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            #position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,)\n",
    "        \n",
    "        sequence_output = outputs[0]\n",
    "        prediction_scores = self.lm_head(sequence_output)\n",
    "\n",
    "        masked_lm_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        return MaskedLMOutput(loss=masked_lm_loss,\n",
    "                              logits=prediction_scores,\n",
    "                              hidden_states=outputs.hidden_states,\n",
    "                              attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "viral-patrol",
   "metadata": {
    "id": "OJt_cHeyTmDS"
   },
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, model_config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if model_config_path is None:\n",
    "            self.model_config = AutoConfig.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "        else:\n",
    "            self.model_config = torch.load(model_config_path)\n",
    "\n",
    "        if pretrained:\n",
    "            self.backbone = AutoModel.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                config=self.model_config,\n",
    "            )\n",
    "            print(f\"Load weight from pretrained\")\n",
    "        else:\n",
    "            #self.backbone = AutoModel.from_config(self.model_config)\n",
    "            # itpt = AutoModelForMaskedLM.from_config(self.model_config)\n",
    "            #path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp010/checkpoint-130170/pytorch_model.bin\")\n",
    "            # path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n",
    "            # state_dict = torch.load(path)\n",
    "            # itpt.load_state_dict(state_dict)\n",
    "            #path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\")\n",
    "            path = str(Path(\"../output\") / CFG.competition_name /  \"nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\")\n",
    "            masked_model = MaskedModel(CFG, config_path=None, pretrained=True)\n",
    "            state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "            masked_model.load_state_dict(state)\n",
    "            self.backbone = masked_model.model\n",
    "            print(f\"Load weight from {path}\")\n",
    "            del state, masked_model; gc.collect()\n",
    "\n",
    "        self.lstm = nn.GRU(\n",
    "            input_size=self.model_config.hidden_size,\n",
    "            bidirectional=True,\n",
    "            hidden_size=self.model_config.hidden_size // 2,\n",
    "            num_layers=4,\n",
    "            dropout=self.cfg.dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(self.cfg.dropout),\n",
    "            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, mappings_from_token_to_char):\n",
    "        h = self.backbone(**inputs)[\"last_hidden_state\"]  # [batch, seq_len, d_model]\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.unsqueeze(2).expand(-1, -1, self.model_config.hidden_size)\n",
    "        h = torch.gather(h, 1, mappings_from_token_to_char)    # [batch, seq_len, d_model]\n",
    "        h, _ = self.lstm(h)\n",
    "        output = self.fc(h)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-classics",
   "metadata": {
    "id": "therapeutic-assembly"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "checked-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, reduction='none', alpha=1, gamma=2):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        loss = self.alpha * (1. - pt)**self.gamma * bce_loss\n",
    "        if self.reduction == 'none':\n",
    "            loss = loss\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class SmoothFocalLoss(nn.Module):\n",
    "    def __init__(self, reduction='none', alpha=1, gamma=2, smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "        self.focal_loss = FocalLoss(reduction='none', alpha=alpha, gamma=gamma)\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothFocalLoss._smooth(targets, self.smoothing)\n",
    "        loss = self.focal_loss(inputs, targets)\n",
    "        if self.reduction == 'none':\n",
    "            loss = loss\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "    \n",
    "class CEFocalLoss(nn.Module):\n",
    "    def __init__(self, reduction='none', alpha=1, gamma=2):\n",
    "        super(CEFocalLoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        loss = self.alpha * (1. - pt)**self.gamma * ce_loss\n",
    "        if self.reduction == 'none':\n",
    "            loss = loss\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "    \n",
    "class SmoothCEFocalLoss(nn.Module):\n",
    "    def __init__(self, reduction='none', alpha=1, gamma=2, smoothing=0.0):\n",
    "        super(SmoothCEFocalLoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', label_smoothing=self.smoothing) # torch >= 1.10.0\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        loss = self.alpha * (1. - pt)**self.gamma * ce_loss\n",
    "        if self.reduction == 'none':\n",
    "            loss = loss\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "sharp-perspective",
   "metadata": {
    "id": "going-conversion"
   },
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    train_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    epoch,\n",
    "    scheduler,\n",
    "    device,\n",
    "):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels, mappings_from_token_to_char) in enumerate(train_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device) \n",
    "        batch_size = labels.size(0)\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            output = model(inputs, mappings_from_token_to_char)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if CFG.batch_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n",
    "            print(\n",
    "                \"Epoch: [{0}][{1}/{2}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                \"Grad: {grad_norm:.4f}  \"\n",
    "                \"LR: {lr:.6f}  \"\n",
    "                .format(\n",
    "                    epoch+1,\n",
    "                    step,\n",
    "                    len(train_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n",
    "                    loss=losses,\n",
    "                     grad_norm=grad_norm,\n",
    "                     lr=scheduler.get_lr()[0],\n",
    "                )\n",
    "            )\n",
    "    del output, loss, inputs, labels, mappings_from_token_to_char, scaler, grad_norm; gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "boring-elizabeth",
   "metadata": {
    "id": "alleged-commonwealth"
   },
   "outputs": [],
   "source": [
    "def valid_fn(\n",
    "    val_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    device,\n",
    "):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels, mappings_from_token_to_char) in enumerate(val_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device) \n",
    "        batch_size = labels.size(0)\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            output = model(inputs, mappings_from_token_to_char)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "        loss = loss.mean()\n",
    "    \n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n",
    "            print(\n",
    "                \"EVAL: [{0}/{1}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                .format(\n",
    "                    step, len(val_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n",
    "                    loss=losses,\n",
    "                )\n",
    "            )\n",
    "    preds = np.concatenate(preds)\n",
    "    return losses.avg, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "directed-cuisine",
   "metadata": {
    "id": "middle-determination"
   },
   "outputs": [],
   "source": [
    "def inference_fn(test_dataloader, model, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preds = []\n",
    "    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n",
    "    for (inputs, mappings_from_token_to_char) in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs, mappings_from_token_to_char)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "regional-bahrain",
   "metadata": {
    "id": "familiar-participation"
   },
   "outputs": [],
   "source": [
    "def train_loop(df, i_fold, device):\n",
    "    print(f\"========== fold: {i_fold} training ==========\")\n",
    "    train_idx = df[df[\"fold\"] != i_fold].index\n",
    "    val_idx = df[df[\"fold\"] == i_fold].index\n",
    "\n",
    "    train_folds = df.loc[train_idx].reset_index(drop=True)\n",
    "    val_folds = df.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    if CFG.pseudo_plain_path is not None:\n",
    "        pseudo_plain = pd.read_pickle(CFG.pseudo_plain_path)\n",
    "        print(f\"get pseudo plain from {CFG.pseudo_plain_path}\")\n",
    "        pseudo_label_list = []\n",
    "        weights = [0.4433659049657008, 0.20859987143371844, 0.3480342236005807]\n",
    "        for exp_name in [\"nbme-exp060\", \"nbme-exp067\", \"nbme-exp083\"]:\n",
    "            #pseudo_label_path = f'./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/{exp_name}/pseudo_labels_{i_fold}.npy'\n",
    "            pseudo_label_path = f'../output/nbme-score-clinical-patient-notes/{exp_name}/pseudo_labels_{i_fold}.npy'\n",
    "            pseudo_label = np.load(pseudo_label_path)\n",
    "            print(f\"get pseudo labels from {pseudo_label_path}\")\n",
    "            pseudo_label_list.append(pseudo_label)\n",
    "\n",
    "        pseudo_label = weights[0] * pseudo_label_list[0] + weights[1] * pseudo_label_list[1] + weights[2] * pseudo_label_list[2]\n",
    "        pseudo_label = trunc_pred(pseudo_plain[\"pn_history\"].values, pseudo_label)\n",
    "        predicted_location_str = get_predicted_location_str(pseudo_label, th=0.5)\n",
    "        preds = get_predictions(predicted_location_str)\n",
    "        results_postprocess = postprocess(pseudo_plain[\"pn_history\"].values, preds)\n",
    "        #results_postprocess = get_results_from_preds_list(results_postprocess)\n",
    "        pseudo_label = get_preds_from_results(results_postprocess, pseudo_plain[\"pn_history\"].values, pseudo_label.shape[1])\n",
    "        print(pseudo_plain.shape, pseudo_label.shape)\n",
    "\n",
    "        pseudo_plain['feature_text'] = pseudo_plain['feature_text'].str.lower()\n",
    "        pseudo_plain['pn_history'] = pseudo_plain['pn_history'].str.lower()\n",
    "\n",
    "        pseudo_plain[\"pseudo_idx\"] = np.arange(len(pseudo_plain))\n",
    "        pseudo_plain = pseudo_plain.sample(n=CFG.n_pseudo_labels)\n",
    "        print(pseudo_plain.shape)\n",
    "        train_folds = pd.concat([train_folds, pseudo_plain], axis=0, ignore_index=True)\n",
    "        print(train_folds.shape)\n",
    "\n",
    "    train_dataset = TrainingDataset(CFG, train_folds, pseudo_label)\n",
    "    val_dataset = TrainingDataset(CFG, val_folds)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "    model = CustomModel(CFG, model_config_path=None, pretrained=False)   # itptを使うため\n",
    "    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n",
    "    model.to(device)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in param_optimizer if not any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n",
    "        {\"params\": [p for n, p in param_optimizer if any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=CFG.lr,\n",
    "        betas=CFG.betas,\n",
    "        weight_decay=CFG.weight_decay,\n",
    "    )\n",
    "    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n",
    "    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_train_optimization_steps,\n",
    "    )\n",
    "\n",
    "    criterion = SmoothFocalLoss(reduction='none', alpha=CFG.alpha, gamma=CFG.gamma, smoothing=CFG.smoothing)\n",
    "    #criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    best_score = -1 * np.inf\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "        avg_loss = train_fn(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            scheduler,\n",
    "            device,\n",
    "        )\n",
    "        avg_val_loss, val_preds = valid_fn(\n",
    "            val_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "\n",
    "        # scoring\n",
    "        val_folds[[str(i) for i in range(CFG.max_char_len)]] = val_preds\n",
    "        score = scoring(val_folds, th=0.5, use_token_prob=False)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n",
    "        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"predictions\": val_preds,\n",
    "                },\n",
    "                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "            )\n",
    "\n",
    "    predictions = torch.load(\n",
    "        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )[\"predictions\"]\n",
    "    val_folds[[str(i) for i in range(CFG.max_char_len)]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return val_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-hypothesis",
   "metadata": {
    "id": "coated-cameroon"
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "manufactured-reviewer",
   "metadata": {
    "id": "quality-expansion"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for i_fold in range(CFG.n_fold):\n",
    "            if i_fold in CFG.train_fold:\n",
    "                _oof_df = train_loop(train, i_fold, device)\n",
    "                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n",
    "        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    if CFG.submission:\n",
    "        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n",
    "    else:\n",
    "        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    best_thres = 0.5\n",
    "    best_score = 0.\n",
    "    for th in np.arange(0.45, 0.55, 0.01):\n",
    "        th = np.round(th, 2)\n",
    "        score = scoring(oof_df, th=th, use_token_prob=False)\n",
    "        if best_score < score:\n",
    "            best_thres = th\n",
    "            best_score = score\n",
    "    print(f\"best_thres: {best_thres}  score: {best_score:.5f}\")\n",
    "\n",
    "    if CFG.inference:\n",
    "        test_dataset = TestDataset(CFG, test)\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=CFG.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=CFG.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        predictions = []\n",
    "        for i_fold in CFG.train_fold:\n",
    "            if CFG.submission:\n",
    "                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n",
    "                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n",
    "            else:\n",
    "                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n",
    "\n",
    "            state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "            model.load_state_dict(state[\"model\"])\n",
    "            print(f\"load weights from {path}\")\n",
    "            test_char_probs = inference_fn(test_dataloader, model, device)\n",
    "            predictions.append(test_char_probs)\n",
    "\n",
    "            del state, test_char_probs, model; gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        predictions = np.mean(predictions, axis=0)\n",
    "        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n",
    "        test[CFG.target_col] = predicted_location_str\n",
    "        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n",
    "        test[[CFG.id_col, CFG.target_col]].to_csv(\n",
    "            CFG.output_dir / \"submission.csv\", index=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "removed-knock",
   "metadata": {
    "id": "proprietary-civilian"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "get pseudo plain from ../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_0.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_0.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp083/pseudo_labels_0.npy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360d9601ebef44098706db92f49fcb93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a58f73e4dc4f43f6b04de6299fc4c41c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(612602, 950)\n",
      "(612602, 6) (612602, 950)\n",
      "(10725, 7)\n",
      "(21450, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'deberta.embeddings.position_embeddings.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\n",
      "Epoch: [1][0/7150] Elapsed 0m 1s (remain 156m 12s) Loss: 0.0857(0.0857) Grad: 80466.7969  LR: 0.000000  \n",
      "Epoch: [1][100/7150] Elapsed 1m 16s (remain 89m 11s) Loss: 0.0706(0.0808) Grad: 68367.0703  LR: 0.000001  \n",
      "Epoch: [1][200/7150] Elapsed 2m 30s (remain 86m 50s) Loss: 0.0295(0.0656) Grad: 35621.5391  LR: 0.000001  \n",
      "Epoch: [1][300/7150] Elapsed 3m 44s (remain 85m 7s) Loss: 0.0146(0.0503) Grad: 3285.6604  LR: 0.000002  \n",
      "Epoch: [1][400/7150] Elapsed 5m 0s (remain 84m 12s) Loss: 0.0167(0.0409) Grad: 5114.9946  LR: 0.000002  \n",
      "Epoch: [1][500/7150] Elapsed 6m 13s (remain 82m 43s) Loss: 0.0137(0.0353) Grad: 5351.3047  LR: 0.000003  \n",
      "Epoch: [1][600/7150] Elapsed 7m 27s (remain 81m 21s) Loss: 0.0136(0.0314) Grad: 4677.0132  LR: 0.000003  \n",
      "Epoch: [1][700/7150] Elapsed 8m 43s (remain 80m 13s) Loss: 0.0121(0.0287) Grad: 4523.0103  LR: 0.000004  \n",
      "Epoch: [1][800/7150] Elapsed 9m 59s (remain 79m 14s) Loss: 0.0119(0.0265) Grad: 3739.1035  LR: 0.000004  \n",
      "Epoch: [1][900/7150] Elapsed 11m 14s (remain 78m 1s) Loss: 0.0087(0.0248) Grad: 4725.8081  LR: 0.000005  \n",
      "Epoch: [1][1000/7150] Elapsed 12m 29s (remain 76m 46s) Loss: 0.0025(0.0235) Grad: 4364.9937  LR: 0.000006  \n",
      "Epoch: [1][1100/7150] Elapsed 13m 45s (remain 75m 36s) Loss: 0.0062(0.0222) Grad: 4239.0005  LR: 0.000006  \n",
      "Epoch: [1][1200/7150] Elapsed 15m 5s (remain 74m 45s) Loss: 0.0035(0.0211) Grad: 2491.3252  LR: 0.000007  \n",
      "Epoch: [1][1300/7150] Elapsed 16m 25s (remain 73m 49s) Loss: 0.0039(0.0199) Grad: 7761.4575  LR: 0.000007  \n",
      "Epoch: [1][1400/7150] Elapsed 17m 41s (remain 72m 37s) Loss: 0.0036(0.0189) Grad: 11172.4873  LR: 0.000008  \n",
      "Epoch: [1][1500/7150] Elapsed 19m 3s (remain 71m 42s) Loss: 0.0024(0.0180) Grad: 3220.4795  LR: 0.000008  \n",
      "Epoch: [1][1600/7150] Elapsed 20m 20s (remain 70m 31s) Loss: 0.0024(0.0171) Grad: 23421.5137  LR: 0.000009  \n",
      "Epoch: [1][1700/7150] Elapsed 21m 34s (remain 69m 8s) Loss: 0.0005(0.0164) Grad: 1707.4346  LR: 0.000010  \n",
      "Epoch: [1][1800/7150] Elapsed 22m 51s (remain 67m 52s) Loss: 0.0068(0.0156) Grad: 25997.3926  LR: 0.000010  \n",
      "Epoch: [1][1900/7150] Elapsed 24m 6s (remain 66m 32s) Loss: 0.0151(0.0151) Grad: 22502.7988  LR: 0.000011  \n",
      "Epoch: [1][2000/7150] Elapsed 25m 22s (remain 65m 17s) Loss: 0.0019(0.0145) Grad: 5471.4531  LR: 0.000011  \n",
      "Epoch: [1][2100/7150] Elapsed 26m 41s (remain 64m 8s) Loss: 0.0017(0.0140) Grad: 6124.4707  LR: 0.000012  \n",
      "Epoch: [1][2200/7150] Elapsed 28m 0s (remain 62m 59s) Loss: 0.0030(0.0135) Grad: 10253.1338  LR: 0.000012  \n",
      "Epoch: [1][2300/7150] Elapsed 29m 15s (remain 61m 39s) Loss: 0.0012(0.0130) Grad: 2428.0979  LR: 0.000013  \n",
      "Epoch: [1][2400/7150] Elapsed 30m 35s (remain 60m 30s) Loss: 0.0047(0.0126) Grad: 43091.4414  LR: 0.000013  \n",
      "Epoch: [1][2500/7150] Elapsed 31m 57s (remain 59m 23s) Loss: 0.0045(0.0123) Grad: 13038.9414  LR: 0.000014  \n",
      "Epoch: [1][2600/7150] Elapsed 33m 16s (remain 58m 11s) Loss: 0.0019(0.0119) Grad: 5980.1963  LR: 0.000015  \n",
      "Epoch: [1][2700/7150] Elapsed 34m 31s (remain 56m 51s) Loss: 0.0005(0.0116) Grad: 2972.1057  LR: 0.000015  \n",
      "Epoch: [1][2800/7150] Elapsed 35m 46s (remain 55m 33s) Loss: 0.0016(0.0113) Grad: 12780.7734  LR: 0.000016  \n",
      "Epoch: [1][2900/7150] Elapsed 37m 1s (remain 54m 13s) Loss: 0.0005(0.0110) Grad: 2986.4058  LR: 0.000016  \n",
      "Epoch: [1][3000/7150] Elapsed 38m 15s (remain 52m 53s) Loss: 0.0006(0.0107) Grad: 1422.9305  LR: 0.000017  \n",
      "Epoch: [1][3100/7150] Elapsed 39m 30s (remain 51m 35s) Loss: 0.0001(0.0105) Grad: 337.4084  LR: 0.000017  \n",
      "Epoch: [1][3200/7150] Elapsed 40m 44s (remain 50m 15s) Loss: 0.0012(0.0102) Grad: 7606.2837  LR: 0.000018  \n",
      "Epoch: [1][3300/7150] Elapsed 41m 58s (remain 48m 56s) Loss: 0.0018(0.0100) Grad: 4089.4504  LR: 0.000018  \n",
      "Epoch: [1][3400/7150] Elapsed 43m 13s (remain 47m 39s) Loss: 0.0016(0.0098) Grad: 10452.4482  LR: 0.000019  \n",
      "Epoch: [1][3500/7150] Elapsed 44m 29s (remain 46m 21s) Loss: 0.0005(0.0096) Grad: 662.5225  LR: 0.000020  \n",
      "Epoch: [1][3600/7150] Elapsed 45m 43s (remain 45m 3s) Loss: 0.0112(0.0094) Grad: 31031.1523  LR: 0.000020  \n",
      "Epoch: [1][3700/7150] Elapsed 46m 57s (remain 43m 45s) Loss: 0.0038(0.0092) Grad: 12840.4678  LR: 0.000020  \n",
      "Epoch: [1][3800/7150] Elapsed 48m 15s (remain 42m 30s) Loss: 0.0005(0.0090) Grad: 1916.5956  LR: 0.000020  \n",
      "Epoch: [1][3900/7150] Elapsed 49m 30s (remain 41m 13s) Loss: 0.0013(0.0089) Grad: 4739.4961  LR: 0.000020  \n",
      "Epoch: [1][4000/7150] Elapsed 50m 45s (remain 39m 56s) Loss: 0.0006(0.0087) Grad: 5421.4814  LR: 0.000020  \n",
      "Epoch: [1][4100/7150] Elapsed 52m 0s (remain 38m 40s) Loss: 0.0002(0.0086) Grad: 1953.4006  LR: 0.000020  \n",
      "Epoch: [1][4200/7150] Elapsed 53m 19s (remain 37m 26s) Loss: 0.0003(0.0084) Grad: 1769.7810  LR: 0.000020  \n",
      "Epoch: [1][4300/7150] Elapsed 54m 35s (remain 36m 9s) Loss: 0.0024(0.0083) Grad: 26741.5430  LR: 0.000020  \n",
      "Epoch: [1][4400/7150] Elapsed 55m 50s (remain 34m 52s) Loss: 0.0001(0.0081) Grad: 899.5473  LR: 0.000019  \n",
      "Epoch: [1][4500/7150] Elapsed 57m 9s (remain 33m 38s) Loss: 0.0039(0.0080) Grad: 15840.7471  LR: 0.000019  \n",
      "Epoch: [1][4600/7150] Elapsed 58m 26s (remain 32m 22s) Loss: 0.0001(0.0079) Grad: 463.4131  LR: 0.000019  \n",
      "Epoch: [1][4700/7150] Elapsed 59m 41s (remain 31m 5s) Loss: 0.0022(0.0078) Grad: 36949.5547  LR: 0.000019  \n",
      "Epoch: [1][4800/7150] Elapsed 61m 1s (remain 29m 51s) Loss: 0.0108(0.0076) Grad: 111917.1953  LR: 0.000019  \n",
      "Epoch: [1][4900/7150] Elapsed 62m 22s (remain 28m 37s) Loss: 0.0014(0.0075) Grad: 2720.5298  LR: 0.000019  \n",
      "Epoch: [1][5000/7150] Elapsed 63m 45s (remain 27m 23s) Loss: 0.0009(0.0074) Grad: 6262.5732  LR: 0.000019  \n",
      "Epoch: [1][5100/7150] Elapsed 65m 6s (remain 26m 9s) Loss: 0.0005(0.0073) Grad: 16154.0000  LR: 0.000019  \n",
      "Epoch: [1][5200/7150] Elapsed 66m 27s (remain 24m 54s) Loss: 0.0031(0.0072) Grad: 12455.9668  LR: 0.000019  \n",
      "Epoch: [1][5300/7150] Elapsed 67m 49s (remain 23m 39s) Loss: 0.0002(0.0071) Grad: 1072.7871  LR: 0.000019  \n",
      "Epoch: [1][5400/7150] Elapsed 69m 9s (remain 22m 23s) Loss: 0.0002(0.0070) Grad: 3462.4451  LR: 0.000019  \n",
      "Epoch: [1][5500/7150] Elapsed 70m 32s (remain 21m 8s) Loss: 0.0068(0.0070) Grad: 41092.8477  LR: 0.000019  \n",
      "Epoch: [1][5600/7150] Elapsed 71m 48s (remain 19m 51s) Loss: 0.0005(0.0069) Grad: 9513.0029  LR: 0.000019  \n",
      "Epoch: [1][5700/7150] Elapsed 73m 2s (remain 18m 33s) Loss: 0.0002(0.0068) Grad: 692.3248  LR: 0.000019  \n",
      "Epoch: [1][5800/7150] Elapsed 74m 17s (remain 17m 16s) Loss: 0.0000(0.0067) Grad: 161.5327  LR: 0.000019  \n",
      "Epoch: [1][5900/7150] Elapsed 75m 31s (remain 15m 59s) Loss: 0.0015(0.0066) Grad: 4349.4619  LR: 0.000019  \n",
      "Epoch: [1][6000/7150] Elapsed 76m 46s (remain 14m 42s) Loss: 0.0000(0.0066) Grad: 158.4098  LR: 0.000018  \n",
      "Epoch: [1][6100/7150] Elapsed 78m 1s (remain 13m 24s) Loss: 0.0001(0.0065) Grad: 676.9922  LR: 0.000018  \n",
      "Epoch: [1][6200/7150] Elapsed 79m 18s (remain 12m 8s) Loss: 0.0023(0.0064) Grad: 12520.1914  LR: 0.000018  \n",
      "Epoch: [1][6300/7150] Elapsed 80m 34s (remain 10m 51s) Loss: 0.0030(0.0064) Grad: 85656.6094  LR: 0.000018  \n",
      "Epoch: [1][6400/7150] Elapsed 81m 51s (remain 9m 34s) Loss: 0.0002(0.0063) Grad: 1158.0592  LR: 0.000018  \n",
      "Epoch: [1][6500/7150] Elapsed 83m 7s (remain 8m 17s) Loss: 0.0000(0.0062) Grad: 121.9322  LR: 0.000018  \n",
      "Epoch: [1][6600/7150] Elapsed 84m 22s (remain 7m 1s) Loss: 0.0004(0.0062) Grad: 5290.5410  LR: 0.000018  \n",
      "Epoch: [1][6700/7150] Elapsed 85m 39s (remain 5m 44s) Loss: 0.0001(0.0061) Grad: 198.4490  LR: 0.000018  \n",
      "Epoch: [1][6800/7150] Elapsed 86m 54s (remain 4m 27s) Loss: 0.0001(0.0060) Grad: 1764.4573  LR: 0.000018  \n",
      "Epoch: [1][6900/7150] Elapsed 88m 9s (remain 3m 10s) Loss: 0.0001(0.0060) Grad: 132.6090  LR: 0.000018  \n",
      "Epoch: [1][7000/7150] Elapsed 89m 25s (remain 1m 54s) Loss: 0.0003(0.0059) Grad: 1438.7434  LR: 0.000018  \n",
      "Epoch: [1][7100/7150] Elapsed 90m 41s (remain 0m 37s) Loss: 0.0005(0.0059) Grad: 3544.4126  LR: 0.000018  \n",
      "Epoch: [1][7149/7150] Elapsed 91m 17s (remain 0m 0s) Loss: 0.0069(0.0058) Grad: 5918.7544  LR: 0.000018  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 17m 16s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 30s (remain 5m 29s) Loss: 0.0084(0.0025) \n",
      "EVAL: [200/1192] Elapsed 0m 59s (remain 4m 51s) Loss: 0.0087(0.0033) \n",
      "EVAL: [300/1192] Elapsed 1m 27s (remain 4m 19s) Loss: 0.0016(0.0037) \n",
      "EVAL: [400/1192] Elapsed 1m 57s (remain 3m 52s) Loss: 0.0025(0.0037) \n",
      "EVAL: [500/1192] Elapsed 2m 27s (remain 3m 22s) Loss: 0.0031(0.0034) \n",
      "EVAL: [600/1192] Elapsed 2m 56s (remain 2m 53s) Loss: 0.0001(0.0034) \n",
      "EVAL: [700/1192] Elapsed 3m 25s (remain 2m 23s) Loss: 0.0329(0.0040) \n",
      "EVAL: [800/1192] Elapsed 3m 56s (remain 1m 55s) Loss: 0.0011(0.0041) \n",
      "EVAL: [900/1192] Elapsed 4m 25s (remain 1m 25s) Loss: 0.0005(0.0042) \n",
      "EVAL: [1000/1192] Elapsed 4m 55s (remain 0m 56s) Loss: 0.0000(0.0041) \n",
      "EVAL: [1100/1192] Elapsed 5m 24s (remain 0m 26s) Loss: 0.0054(0.0039) \n",
      "EVAL: [1191/1192] Elapsed 5m 50s (remain 0m 0s) Loss: 0.0000(0.0038) \n",
      "Epoch 1 - avg_train_loss: 0.0058  avg_val_loss: 0.0038  time: 5832s\n",
      "Epoch 1 - Score: 0.8598\n",
      "Epoch 1 - Save Best Score: 0.8598 Model\n",
      "Epoch: [2][0/7150] Elapsed 0m 1s (remain 168m 16s) Loss: 0.0001(0.0001) Grad: 36.8673  LR: 0.000018  \n",
      "Epoch: [2][100/7150] Elapsed 1m 16s (remain 88m 26s) Loss: 0.0007(0.0017) Grad: 4551.3887  LR: 0.000018  \n",
      "Epoch: [2][200/7150] Elapsed 2m 30s (remain 86m 45s) Loss: 0.0040(0.0018) Grad: 37966.3867  LR: 0.000018  \n",
      "Epoch: [2][300/7150] Elapsed 3m 45s (remain 85m 28s) Loss: 0.0011(0.0019) Grad: 3420.1252  LR: 0.000018  \n",
      "Epoch: [2][400/7150] Elapsed 5m 1s (remain 84m 37s) Loss: 0.0029(0.0020) Grad: 3856.3198  LR: 0.000018  \n",
      "Epoch: [2][500/7150] Elapsed 6m 17s (remain 83m 25s) Loss: 0.0001(0.0020) Grad: 221.5244  LR: 0.000017  \n",
      "Epoch: [2][600/7150] Elapsed 7m 31s (remain 81m 57s) Loss: 0.0012(0.0019) Grad: 7115.9595  LR: 0.000017  \n",
      "Epoch: [2][700/7150] Elapsed 8m 45s (remain 80m 30s) Loss: 0.0021(0.0019) Grad: 7060.6797  LR: 0.000017  \n",
      "Epoch: [2][800/7150] Elapsed 10m 2s (remain 79m 37s) Loss: 0.0006(0.0019) Grad: 1247.8479  LR: 0.000017  \n",
      "Epoch: [2][900/7150] Elapsed 11m 17s (remain 78m 21s) Loss: 0.0019(0.0019) Grad: 7753.5415  LR: 0.000017  \n",
      "Epoch: [2][1000/7150] Elapsed 12m 31s (remain 76m 57s) Loss: 0.0000(0.0018) Grad: 32.3670  LR: 0.000017  \n",
      "Epoch: [2][1100/7150] Elapsed 13m 44s (remain 75m 31s) Loss: 0.0001(0.0018) Grad: 563.7640  LR: 0.000017  \n",
      "Epoch: [2][1200/7150] Elapsed 15m 1s (remain 74m 24s) Loss: 0.0017(0.0018) Grad: 11649.8066  LR: 0.000017  \n",
      "Epoch: [2][1300/7150] Elapsed 16m 17s (remain 73m 12s) Loss: 0.0013(0.0018) Grad: 28529.1543  LR: 0.000017  \n",
      "Epoch: [2][1400/7150] Elapsed 17m 32s (remain 71m 58s) Loss: 0.0004(0.0018) Grad: 1482.9225  LR: 0.000017  \n",
      "Epoch: [2][1500/7150] Elapsed 18m 46s (remain 70m 39s) Loss: 0.0000(0.0018) Grad: 50.5045  LR: 0.000017  \n",
      "Epoch: [2][1600/7150] Elapsed 20m 0s (remain 69m 19s) Loss: 0.0000(0.0017) Grad: 407.2407  LR: 0.000017  \n",
      "Epoch: [2][1700/7150] Elapsed 21m 13s (remain 68m 0s) Loss: 0.0009(0.0017) Grad: 3067.2588  LR: 0.000017  \n",
      "Epoch: [2][1800/7150] Elapsed 22m 30s (remain 66m 50s) Loss: 0.0027(0.0017) Grad: 15319.8848  LR: 0.000017  \n",
      "Epoch: [2][1900/7150] Elapsed 23m 45s (remain 65m 35s) Loss: 0.0001(0.0018) Grad: 365.3675  LR: 0.000017  \n",
      "Epoch: [2][2000/7150] Elapsed 24m 59s (remain 64m 18s) Loss: 0.0048(0.0017) Grad: 10160.9131  LR: 0.000017  \n",
      "Epoch: [2][2100/7150] Elapsed 26m 15s (remain 63m 6s) Loss: 0.0023(0.0018) Grad: 15207.8916  LR: 0.000016  \n",
      "Epoch: [2][2200/7150] Elapsed 27m 35s (remain 62m 3s) Loss: 0.0002(0.0018) Grad: 408.2196  LR: 0.000016  \n",
      "Epoch: [2][2300/7150] Elapsed 28m 54s (remain 60m 54s) Loss: 0.0003(0.0017) Grad: 3309.9934  LR: 0.000016  \n",
      "Epoch: [2][2400/7150] Elapsed 30m 13s (remain 59m 46s) Loss: 0.0038(0.0017) Grad: 26798.5449  LR: 0.000016  \n",
      "Epoch: [2][2500/7150] Elapsed 31m 28s (remain 58m 29s) Loss: 0.0004(0.0017) Grad: 8265.0742  LR: 0.000016  \n",
      "Epoch: [2][2600/7150] Elapsed 32m 43s (remain 57m 13s) Loss: 0.0001(0.0017) Grad: 163.5280  LR: 0.000016  \n",
      "Epoch: [2][2700/7150] Elapsed 33m 59s (remain 55m 58s) Loss: 0.0009(0.0017) Grad: 7022.4722  LR: 0.000016  \n",
      "Epoch: [2][2800/7150] Elapsed 35m 13s (remain 54m 41s) Loss: 0.0089(0.0017) Grad: 9033.9072  LR: 0.000016  \n",
      "Epoch: [2][2900/7150] Elapsed 36m 26s (remain 53m 23s) Loss: 0.0001(0.0017) Grad: 85.9652  LR: 0.000016  \n",
      "Epoch: [2][3000/7150] Elapsed 37m 43s (remain 52m 9s) Loss: 0.0000(0.0017) Grad: 13.4570  LR: 0.000016  \n",
      "Epoch: [2][3100/7150] Elapsed 38m 56s (remain 50m 51s) Loss: 0.0010(0.0017) Grad: 2277.5581  LR: 0.000016  \n",
      "Epoch: [2][3200/7150] Elapsed 40m 10s (remain 49m 33s) Loss: 0.0001(0.0017) Grad: 803.3259  LR: 0.000016  \n",
      "Epoch: [2][3300/7150] Elapsed 41m 30s (remain 48m 24s) Loss: 0.0055(0.0017) Grad: 5404.8438  LR: 0.000016  \n",
      "Epoch: [2][3400/7150] Elapsed 42m 46s (remain 47m 9s) Loss: 0.0002(0.0017) Grad: 1058.2638  LR: 0.000016  \n",
      "Epoch: [2][3500/7150] Elapsed 44m 0s (remain 45m 51s) Loss: 0.0048(0.0017) Grad: 39297.9023  LR: 0.000016  \n",
      "Epoch: [2][3600/7150] Elapsed 45m 17s (remain 44m 38s) Loss: 0.0000(0.0016) Grad: 41.6863  LR: 0.000016  \n",
      "Epoch: [2][3700/7150] Elapsed 46m 31s (remain 43m 21s) Loss: 0.0029(0.0017) Grad: 12173.9385  LR: 0.000015  \n",
      "Epoch: [2][3800/7150] Elapsed 47m 45s (remain 42m 4s) Loss: 0.0104(0.0016) Grad: 12376.4717  LR: 0.000015  \n",
      "Epoch: [2][3900/7150] Elapsed 49m 0s (remain 40m 48s) Loss: 0.0143(0.0016) Grad: 14501.4922  LR: 0.000015  \n",
      "Epoch: [2][4000/7150] Elapsed 50m 14s (remain 39m 32s) Loss: 0.0000(0.0016) Grad: 66.0110  LR: 0.000015  \n",
      "Epoch: [2][4100/7150] Elapsed 51m 28s (remain 38m 16s) Loss: 0.0005(0.0016) Grad: 18894.6250  LR: 0.000015  \n",
      "Epoch: [2][4200/7150] Elapsed 52m 45s (remain 37m 2s) Loss: 0.0008(0.0016) Grad: 11974.1934  LR: 0.000015  \n",
      "Epoch: [2][4300/7150] Elapsed 53m 59s (remain 35m 45s) Loss: 0.0002(0.0016) Grad: 1490.7722  LR: 0.000015  \n",
      "Epoch: [2][4400/7150] Elapsed 55m 14s (remain 34m 30s) Loss: 0.0006(0.0016) Grad: 9160.1475  LR: 0.000015  \n",
      "Epoch: [2][4500/7150] Elapsed 56m 34s (remain 33m 17s) Loss: 0.0000(0.0016) Grad: 109.6456  LR: 0.000015  \n",
      "Epoch: [2][4600/7150] Elapsed 57m 54s (remain 32m 4s) Loss: 0.0000(0.0016) Grad: 207.7301  LR: 0.000015  \n",
      "Epoch: [2][4700/7150] Elapsed 59m 15s (remain 30m 52s) Loss: 0.0001(0.0016) Grad: 349.6149  LR: 0.000015  \n",
      "Epoch: [2][4800/7150] Elapsed 60m 30s (remain 29m 36s) Loss: 0.0000(0.0016) Grad: 53.4541  LR: 0.000015  \n",
      "Epoch: [2][4900/7150] Elapsed 61m 50s (remain 28m 22s) Loss: 0.0000(0.0016) Grad: 158.6003  LR: 0.000015  \n",
      "Epoch: [2][5000/7150] Elapsed 63m 5s (remain 27m 6s) Loss: 0.0000(0.0016) Grad: 82.0640  LR: 0.000015  \n",
      "Epoch: [2][5100/7150] Elapsed 64m 18s (remain 25m 50s) Loss: 0.0000(0.0016) Grad: 39.8369  LR: 0.000015  \n",
      "Epoch: [2][5200/7150] Elapsed 65m 33s (remain 24m 33s) Loss: 0.0003(0.0016) Grad: 2795.7207  LR: 0.000015  \n",
      "Epoch: [2][5300/7150] Elapsed 66m 50s (remain 23m 18s) Loss: 0.0004(0.0016) Grad: 16026.8799  LR: 0.000014  \n",
      "Epoch: [2][5400/7150] Elapsed 68m 4s (remain 22m 2s) Loss: 0.0001(0.0016) Grad: 617.7360  LR: 0.000014  \n",
      "Epoch: [2][5500/7150] Elapsed 69m 24s (remain 20m 48s) Loss: 0.0000(0.0016) Grad: 7.2428  LR: 0.000014  \n",
      "Epoch: [2][5600/7150] Elapsed 70m 42s (remain 19m 33s) Loss: 0.0000(0.0016) Grad: 34.2556  LR: 0.000014  \n",
      "Epoch: [2][5700/7150] Elapsed 71m 57s (remain 18m 17s) Loss: 0.0000(0.0016) Grad: 582.0674  LR: 0.000014  \n",
      "Epoch: [2][5800/7150] Elapsed 73m 12s (remain 17m 1s) Loss: 0.0029(0.0016) Grad: 30130.4785  LR: 0.000014  \n",
      "Epoch: [2][5900/7150] Elapsed 74m 26s (remain 15m 45s) Loss: 0.0005(0.0016) Grad: 12137.8584  LR: 0.000014  \n",
      "Epoch: [2][6000/7150] Elapsed 75m 40s (remain 14m 29s) Loss: 0.0085(0.0016) Grad: 27033.4336  LR: 0.000014  \n",
      "Epoch: [2][6100/7150] Elapsed 76m 57s (remain 13m 13s) Loss: 0.0004(0.0016) Grad: 4331.7476  LR: 0.000014  \n",
      "Epoch: [2][6200/7150] Elapsed 78m 11s (remain 11m 58s) Loss: 0.0000(0.0016) Grad: 44.2749  LR: 0.000014  \n",
      "Epoch: [2][6300/7150] Elapsed 79m 28s (remain 10m 42s) Loss: 0.0038(0.0016) Grad: 23616.4609  LR: 0.000014  \n",
      "Epoch: [2][6400/7150] Elapsed 80m 43s (remain 9m 26s) Loss: 0.0000(0.0016) Grad: 46.5811  LR: 0.000014  \n",
      "Epoch: [2][6500/7150] Elapsed 81m 59s (remain 8m 11s) Loss: 0.0017(0.0016) Grad: 10802.8799  LR: 0.000014  \n",
      "Epoch: [2][6600/7150] Elapsed 83m 14s (remain 6m 55s) Loss: 0.0000(0.0016) Grad: 51.3065  LR: 0.000014  \n",
      "Epoch: [2][6700/7150] Elapsed 84m 28s (remain 5m 39s) Loss: 0.0016(0.0016) Grad: 20861.3379  LR: 0.000014  \n",
      "Epoch: [2][6800/7150] Elapsed 85m 42s (remain 4m 23s) Loss: 0.0001(0.0016) Grad: 515.6211  LR: 0.000014  \n",
      "Epoch: [2][6900/7150] Elapsed 86m 55s (remain 3m 8s) Loss: 0.0017(0.0016) Grad: 22182.3750  LR: 0.000013  \n",
      "Epoch: [2][7000/7150] Elapsed 88m 12s (remain 1m 52s) Loss: 0.0000(0.0016) Grad: 16.2622  LR: 0.000013  \n",
      "Epoch: [2][7100/7150] Elapsed 89m 31s (remain 0m 37s) Loss: 0.0003(0.0016) Grad: 1348.4050  LR: 0.000013  \n",
      "Epoch: [2][7149/7150] Elapsed 90m 7s (remain 0m 0s) Loss: 0.0000(0.0016) Grad: 129.7868  LR: 0.000013  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 19m 20s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 31s (remain 5m 39s) Loss: 0.0038(0.0024) \n",
      "EVAL: [200/1192] Elapsed 1m 0s (remain 4m 57s) Loss: 0.0048(0.0028) \n",
      "EVAL: [300/1192] Elapsed 1m 29s (remain 4m 23s) Loss: 0.0019(0.0029) \n",
      "EVAL: [400/1192] Elapsed 1m 59s (remain 3m 55s) Loss: 0.0027(0.0031) \n",
      "EVAL: [500/1192] Elapsed 2m 30s (remain 3m 27s) Loss: 0.0032(0.0029) \n",
      "EVAL: [600/1192] Elapsed 2m 59s (remain 2m 56s) Loss: 0.0012(0.0030) \n",
      "EVAL: [700/1192] Elapsed 3m 28s (remain 2m 26s) Loss: 0.0342(0.0034) \n",
      "EVAL: [800/1192] Elapsed 3m 58s (remain 1m 56s) Loss: 0.0020(0.0034) \n",
      "EVAL: [900/1192] Elapsed 4m 29s (remain 1m 27s) Loss: 0.0011(0.0034) \n",
      "EVAL: [1000/1192] Elapsed 4m 58s (remain 0m 56s) Loss: 0.0000(0.0033) \n",
      "EVAL: [1100/1192] Elapsed 5m 27s (remain 0m 27s) Loss: 0.0002(0.0031) \n",
      "EVAL: [1191/1192] Elapsed 5m 53s (remain 0m 0s) Loss: 0.0000(0.0030) \n",
      "Epoch 2 - avg_train_loss: 0.0016  avg_val_loss: 0.0030  time: 5765s\n",
      "Epoch 2 - Score: 0.8813\n",
      "Epoch 2 - Save Best Score: 0.8813 Model\n",
      "Epoch: [3][0/7150] Elapsed 0m 1s (remain 159m 38s) Loss: 0.0002(0.0002) Grad: 404.0340  LR: 0.000013  \n",
      "Epoch: [3][100/7150] Elapsed 1m 20s (remain 93m 30s) Loss: 0.0001(0.0010) Grad: 303.0905  LR: 0.000013  \n",
      "Epoch: [3][200/7150] Elapsed 2m 39s (remain 91m 56s) Loss: 0.0011(0.0013) Grad: 2924.0374  LR: 0.000013  \n",
      "Epoch: [3][300/7150] Elapsed 3m 55s (remain 89m 24s) Loss: 0.0001(0.0012) Grad: 423.4317  LR: 0.000013  \n",
      "Epoch: [3][400/7150] Elapsed 5m 13s (remain 88m 1s) Loss: 0.0006(0.0012) Grad: 1359.9232  LR: 0.000013  \n",
      "Epoch: [3][500/7150] Elapsed 6m 28s (remain 85m 58s) Loss: 0.0000(0.0012) Grad: 297.2729  LR: 0.000013  \n",
      "Epoch: [3][600/7150] Elapsed 7m 44s (remain 84m 26s) Loss: 0.0000(0.0012) Grad: 266.3035  LR: 0.000013  \n",
      "Epoch: [3][700/7150] Elapsed 8m 59s (remain 82m 41s) Loss: 0.0006(0.0012) Grad: 4638.5737  LR: 0.000013  \n",
      "Epoch: [3][800/7150] Elapsed 10m 12s (remain 80m 57s) Loss: 0.0000(0.0011) Grad: 92.2159  LR: 0.000013  \n",
      "Epoch: [3][900/7150] Elapsed 11m 25s (remain 79m 11s) Loss: 0.0001(0.0011) Grad: 2241.1238  LR: 0.000013  \n",
      "Epoch: [3][1000/7150] Elapsed 12m 37s (remain 77m 35s) Loss: 0.0000(0.0011) Grad: 92.3082  LR: 0.000013  \n",
      "Epoch: [3][1100/7150] Elapsed 13m 51s (remain 76m 7s) Loss: 0.0000(0.0011) Grad: 2.5714  LR: 0.000013  \n",
      "Epoch: [3][1200/7150] Elapsed 15m 5s (remain 74m 45s) Loss: 0.0000(0.0011) Grad: 15.5306  LR: 0.000013  \n",
      "Epoch: [3][1300/7150] Elapsed 16m 20s (remain 73m 28s) Loss: 0.0001(0.0011) Grad: 810.9292  LR: 0.000013  \n",
      "Epoch: [3][1400/7150] Elapsed 17m 35s (remain 72m 11s) Loss: 0.0001(0.0012) Grad: 261.2867  LR: 0.000012  \n",
      "Epoch: [3][1500/7150] Elapsed 18m 49s (remain 70m 50s) Loss: 0.0000(0.0012) Grad: 14.0381  LR: 0.000012  \n",
      "Epoch: [3][1600/7150] Elapsed 20m 2s (remain 69m 28s) Loss: 0.0000(0.0012) Grad: 69.2562  LR: 0.000012  \n",
      "Epoch: [3][1700/7150] Elapsed 21m 16s (remain 68m 10s) Loss: 0.0004(0.0012) Grad: 956.3144  LR: 0.000012  \n",
      "Epoch: [3][1800/7150] Elapsed 22m 32s (remain 66m 56s) Loss: 0.0000(0.0012) Grad: 66.7631  LR: 0.000012  \n",
      "Epoch: [3][1900/7150] Elapsed 23m 47s (remain 65m 42s) Loss: 0.0024(0.0012) Grad: 9576.5518  LR: 0.000012  \n",
      "Epoch: [3][2000/7150] Elapsed 25m 2s (remain 64m 26s) Loss: 0.0026(0.0012) Grad: 53408.2383  LR: 0.000012  \n",
      "Epoch: [3][2100/7150] Elapsed 26m 22s (remain 63m 23s) Loss: 0.0000(0.0012) Grad: 157.5057  LR: 0.000012  \n",
      "Epoch: [3][2200/7150] Elapsed 27m 37s (remain 62m 6s) Loss: 0.0039(0.0012) Grad: 42126.7734  LR: 0.000012  \n",
      "Epoch: [3][2300/7150] Elapsed 28m 50s (remain 60m 46s) Loss: 0.0000(0.0012) Grad: 128.2212  LR: 0.000012  \n",
      "Epoch: [3][2400/7150] Elapsed 30m 5s (remain 59m 30s) Loss: 0.0002(0.0012) Grad: 1736.2009  LR: 0.000012  \n",
      "Epoch: [3][2500/7150] Elapsed 31m 18s (remain 58m 12s) Loss: 0.0000(0.0012) Grad: 41.5372  LR: 0.000012  \n",
      "Epoch: [3][2600/7150] Elapsed 32m 33s (remain 56m 57s) Loss: 0.0000(0.0012) Grad: 53.4163  LR: 0.000012  \n",
      "Epoch: [3][2700/7150] Elapsed 33m 50s (remain 55m 45s) Loss: 0.0013(0.0012) Grad: 2153.0461  LR: 0.000012  \n",
      "Epoch: [3][2800/7150] Elapsed 35m 4s (remain 54m 27s) Loss: 0.0002(0.0012) Grad: 1271.4224  LR: 0.000012  \n",
      "Epoch: [3][2900/7150] Elapsed 36m 18s (remain 53m 10s) Loss: 0.0006(0.0012) Grad: 2146.7566  LR: 0.000012  \n",
      "Epoch: [3][3000/7150] Elapsed 37m 35s (remain 51m 58s) Loss: 0.0001(0.0011) Grad: 931.0137  LR: 0.000011  \n",
      "Epoch: [3][3100/7150] Elapsed 38m 49s (remain 50m 42s) Loss: 0.0000(0.0011) Grad: 0.9388  LR: 0.000011  \n",
      "Epoch: [3][3200/7150] Elapsed 40m 8s (remain 49m 31s) Loss: 0.0001(0.0011) Grad: 81.4295  LR: 0.000011  \n",
      "Epoch: [3][3300/7150] Elapsed 41m 24s (remain 48m 17s) Loss: 0.0001(0.0011) Grad: 584.4208  LR: 0.000011  \n",
      "Epoch: [3][3400/7150] Elapsed 42m 39s (remain 47m 1s) Loss: 0.0000(0.0011) Grad: 49.6748  LR: 0.000011  \n",
      "Epoch: [3][3500/7150] Elapsed 43m 57s (remain 45m 48s) Loss: 0.0011(0.0011) Grad: 8630.1016  LR: 0.000011  \n",
      "Epoch: [3][3600/7150] Elapsed 45m 18s (remain 44m 39s) Loss: 0.0004(0.0011) Grad: 1472.1945  LR: 0.000011  \n",
      "Epoch: [3][3700/7150] Elapsed 46m 33s (remain 43m 22s) Loss: 0.0004(0.0011) Grad: 722.0267  LR: 0.000011  \n",
      "Epoch: [3][3800/7150] Elapsed 47m 47s (remain 42m 6s) Loss: 0.0000(0.0011) Grad: 5.7231  LR: 0.000011  \n",
      "Epoch: [3][3900/7150] Elapsed 49m 2s (remain 40m 50s) Loss: 0.0001(0.0011) Grad: 732.4657  LR: 0.000011  \n",
      "Epoch: [3][4000/7150] Elapsed 50m 16s (remain 39m 34s) Loss: 0.0000(0.0011) Grad: 16.5236  LR: 0.000011  \n",
      "Epoch: [3][4100/7150] Elapsed 51m 29s (remain 38m 16s) Loss: 0.0003(0.0011) Grad: 1922.5948  LR: 0.000011  \n",
      "Epoch: [3][4200/7150] Elapsed 52m 42s (remain 37m 0s) Loss: 0.0000(0.0011) Grad: 49.8216  LR: 0.000011  \n",
      "Epoch: [3][4300/7150] Elapsed 53m 56s (remain 35m 43s) Loss: 0.0021(0.0011) Grad: 23638.3965  LR: 0.000011  \n",
      "Epoch: [3][4400/7150] Elapsed 55m 10s (remain 34m 27s) Loss: 0.0000(0.0011) Grad: 15.7069  LR: 0.000011  \n",
      "Epoch: [3][4500/7150] Elapsed 56m 24s (remain 33m 11s) Loss: 0.0029(0.0011) Grad: 11182.7881  LR: 0.000011  \n",
      "Epoch: [3][4600/7150] Elapsed 57m 40s (remain 31m 57s) Loss: 0.0000(0.0011) Grad: 27.5882  LR: 0.000010  \n",
      "Epoch: [3][4700/7150] Elapsed 58m 54s (remain 30m 41s) Loss: 0.0005(0.0011) Grad: 5714.7251  LR: 0.000010  \n",
      "Epoch: [3][4800/7150] Elapsed 60m 9s (remain 29m 25s) Loss: 0.0002(0.0011) Grad: 3308.3271  LR: 0.000010  \n",
      "Epoch: [3][4900/7150] Elapsed 61m 23s (remain 28m 10s) Loss: 0.0000(0.0011) Grad: 42.2966  LR: 0.000010  \n",
      "Epoch: [3][5000/7150] Elapsed 62m 36s (remain 26m 54s) Loss: 0.0000(0.0011) Grad: 66.5503  LR: 0.000010  \n",
      "Epoch: [3][5100/7150] Elapsed 63m 49s (remain 25m 38s) Loss: 0.0041(0.0011) Grad: 15718.4316  LR: 0.000010  \n",
      "Epoch: [3][5200/7150] Elapsed 65m 4s (remain 24m 23s) Loss: 0.0001(0.0011) Grad: 1469.1262  LR: 0.000010  \n",
      "Epoch: [3][5300/7150] Elapsed 66m 20s (remain 23m 8s) Loss: 0.0013(0.0011) Grad: 14303.4365  LR: 0.000010  \n",
      "Epoch: [3][5400/7150] Elapsed 67m 35s (remain 21m 53s) Loss: 0.0000(0.0012) Grad: 28.9225  LR: 0.000010  \n",
      "Epoch: [3][5500/7150] Elapsed 68m 49s (remain 20m 37s) Loss: 0.0013(0.0012) Grad: 14975.0547  LR: 0.000010  \n",
      "Epoch: [3][5600/7150] Elapsed 70m 5s (remain 19m 22s) Loss: 0.0006(0.0012) Grad: 3283.2341  LR: 0.000010  \n",
      "Epoch: [3][5700/7150] Elapsed 71m 21s (remain 18m 8s) Loss: 0.0000(0.0012) Grad: 31.2521  LR: 0.000010  \n",
      "Epoch: [3][5800/7150] Elapsed 72m 35s (remain 16m 52s) Loss: 0.0000(0.0012) Grad: 5.2615  LR: 0.000010  \n",
      "Epoch: [3][5900/7150] Elapsed 73m 50s (remain 15m 37s) Loss: 0.0041(0.0012) Grad: 15413.9434  LR: 0.000010  \n",
      "Epoch: [3][6000/7150] Elapsed 75m 5s (remain 14m 22s) Loss: 0.0000(0.0012) Grad: 139.4616  LR: 0.000010  \n",
      "Epoch: [3][6100/7150] Elapsed 76m 19s (remain 13m 7s) Loss: 0.0010(0.0012) Grad: 3364.3406  LR: 0.000010  \n",
      "Epoch: [3][6200/7150] Elapsed 77m 33s (remain 11m 52s) Loss: 0.0001(0.0011) Grad: 848.3614  LR: 0.000009  \n",
      "Epoch: [3][6300/7150] Elapsed 78m 46s (remain 10m 36s) Loss: 0.0000(0.0011) Grad: 38.0338  LR: 0.000009  \n",
      "Epoch: [3][6400/7150] Elapsed 79m 59s (remain 9m 21s) Loss: 0.0000(0.0012) Grad: 19.8031  LR: 0.000009  \n",
      "Epoch: [3][6500/7150] Elapsed 81m 14s (remain 8m 6s) Loss: 0.0010(0.0012) Grad: 2741.3540  LR: 0.000009  \n",
      "Epoch: [3][6600/7150] Elapsed 82m 32s (remain 6m 51s) Loss: 0.0000(0.0012) Grad: 3.3914  LR: 0.000009  \n",
      "Epoch: [3][6700/7150] Elapsed 83m 46s (remain 5m 36s) Loss: 0.0329(0.0012) Grad: 88584.0312  LR: 0.000009  \n",
      "Epoch: [3][6800/7150] Elapsed 85m 1s (remain 4m 21s) Loss: 0.0000(0.0012) Grad: 4.9599  LR: 0.000009  \n",
      "Epoch: [3][6900/7150] Elapsed 86m 20s (remain 3m 6s) Loss: 0.0031(0.0012) Grad: 7968.3096  LR: 0.000009  \n",
      "Epoch: [3][7000/7150] Elapsed 87m 39s (remain 1m 51s) Loss: 0.0000(0.0012) Grad: 3.1182  LR: 0.000009  \n",
      "Epoch: [3][7100/7150] Elapsed 89m 0s (remain 0m 36s) Loss: 0.0000(0.0012) Grad: 133.7649  LR: 0.000009  \n",
      "Epoch: [3][7149/7150] Elapsed 89m 40s (remain 0m 0s) Loss: 0.0000(0.0012) Grad: 186.9566  LR: 0.000009  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 19m 25s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 32s (remain 5m 47s) Loss: 0.0043(0.0020) \n",
      "EVAL: [200/1192] Elapsed 1m 0s (remain 5m 0s) Loss: 0.0089(0.0027) \n",
      "EVAL: [300/1192] Elapsed 1m 29s (remain 4m 25s) Loss: 0.0006(0.0028) \n",
      "EVAL: [400/1192] Elapsed 1m 59s (remain 3m 56s) Loss: 0.0017(0.0030) \n",
      "EVAL: [500/1192] Elapsed 2m 29s (remain 3m 25s) Loss: 0.0009(0.0028) \n",
      "EVAL: [600/1192] Elapsed 2m 57s (remain 2m 55s) Loss: 0.0002(0.0030) \n",
      "EVAL: [700/1192] Elapsed 3m 26s (remain 2m 24s) Loss: 0.0332(0.0035) \n",
      "EVAL: [800/1192] Elapsed 3m 55s (remain 1m 54s) Loss: 0.0003(0.0036) \n",
      "EVAL: [900/1192] Elapsed 4m 24s (remain 1m 25s) Loss: 0.0005(0.0036) \n",
      "EVAL: [1000/1192] Elapsed 4m 55s (remain 0m 56s) Loss: 0.0000(0.0035) \n",
      "EVAL: [1100/1192] Elapsed 5m 24s (remain 0m 26s) Loss: 0.0001(0.0034) \n",
      "EVAL: [1191/1192] Elapsed 5m 51s (remain 0m 0s) Loss: 0.0000(0.0033) \n",
      "Epoch 3 - avg_train_loss: 0.0012  avg_val_loss: 0.0033  time: 5735s\n",
      "Epoch 3 - Score: 0.8853\n",
      "Epoch 3 - Save Best Score: 0.8853 Model\n",
      "Epoch: [4][0/7150] Elapsed 0m 1s (remain 172m 25s) Loss: 0.0000(0.0000) Grad: 14.9273  LR: 0.000009  \n",
      "Epoch: [4][100/7150] Elapsed 1m 21s (remain 94m 36s) Loss: 0.0001(0.0009) Grad: 177.6815  LR: 0.000009  \n",
      "Epoch: [4][200/7150] Elapsed 2m 42s (remain 93m 28s) Loss: 0.0068(0.0007) Grad: 20599.2891  LR: 0.000009  \n",
      "Epoch: [4][300/7150] Elapsed 4m 3s (remain 92m 27s) Loss: 0.0000(0.0008) Grad: 72.9250  LR: 0.000009  \n",
      "Epoch: [4][400/7150] Elapsed 5m 25s (remain 91m 17s) Loss: 0.0016(0.0008) Grad: 1631.9320  LR: 0.000009  \n",
      "Epoch: [4][500/7150] Elapsed 6m 46s (remain 89m 50s) Loss: 0.0004(0.0008) Grad: 10176.1992  LR: 0.000009  \n",
      "Epoch: [4][600/7150] Elapsed 8m 6s (remain 88m 19s) Loss: 0.0000(0.0008) Grad: 22.3032  LR: 0.000009  \n",
      "Epoch: [4][700/7150] Elapsed 9m 26s (remain 86m 48s) Loss: 0.0010(0.0008) Grad: 3630.3020  LR: 0.000008  \n",
      "Epoch: [4][800/7150] Elapsed 10m 46s (remain 85m 21s) Loss: 0.0000(0.0008) Grad: 20.8721  LR: 0.000008  \n",
      "Epoch: [4][900/7150] Elapsed 12m 5s (remain 83m 48s) Loss: 0.0002(0.0008) Grad: 367.2181  LR: 0.000008  \n",
      "Epoch: [4][1000/7150] Elapsed 13m 19s (remain 81m 50s) Loss: 0.0001(0.0008) Grad: 435.8841  LR: 0.000008  \n",
      "Epoch: [4][1100/7150] Elapsed 14m 33s (remain 79m 59s) Loss: 0.0000(0.0008) Grad: 27.3331  LR: 0.000008  \n",
      "Epoch: [4][1200/7150] Elapsed 15m 47s (remain 78m 15s) Loss: 0.0003(0.0008) Grad: 747.2610  LR: 0.000008  \n",
      "Epoch: [4][1300/7150] Elapsed 17m 2s (remain 76m 36s) Loss: 0.0000(0.0008) Grad: 18.9874  LR: 0.000008  \n",
      "Epoch: [4][1400/7150] Elapsed 18m 17s (remain 75m 3s) Loss: 0.0001(0.0008) Grad: 400.0826  LR: 0.000008  \n",
      "Epoch: [4][1500/7150] Elapsed 19m 33s (remain 73m 35s) Loss: 0.0000(0.0008) Grad: 24.3417  LR: 0.000008  \n",
      "Epoch: [4][1600/7150] Elapsed 20m 49s (remain 72m 11s) Loss: 0.0011(0.0008) Grad: 9090.0996  LR: 0.000008  \n",
      "Epoch: [4][1700/7150] Elapsed 22m 6s (remain 70m 49s) Loss: 0.0000(0.0008) Grad: 109.6476  LR: 0.000008  \n",
      "Epoch: [4][1800/7150] Elapsed 23m 20s (remain 69m 19s) Loss: 0.0003(0.0008) Grad: 4894.7466  LR: 0.000008  \n",
      "Epoch: [4][1900/7150] Elapsed 24m 33s (remain 67m 49s) Loss: 0.0005(0.0008) Grad: 2358.0435  LR: 0.000008  \n",
      "Epoch: [4][2000/7150] Elapsed 25m 47s (remain 66m 22s) Loss: 0.0001(0.0008) Grad: 648.1155  LR: 0.000008  \n",
      "Epoch: [4][2100/7150] Elapsed 27m 1s (remain 64m 56s) Loss: 0.0000(0.0008) Grad: 79.6239  LR: 0.000008  \n",
      "Epoch: [4][2200/7150] Elapsed 28m 18s (remain 63m 39s) Loss: 0.0000(0.0008) Grad: 21.4069  LR: 0.000008  \n",
      "Epoch: [4][2300/7150] Elapsed 29m 32s (remain 62m 15s) Loss: 0.0000(0.0008) Grad: 21.3463  LR: 0.000007  \n",
      "Epoch: [4][2400/7150] Elapsed 30m 46s (remain 60m 51s) Loss: 0.0011(0.0008) Grad: 6550.1895  LR: 0.000007  \n",
      "Epoch: [4][2500/7150] Elapsed 32m 1s (remain 59m 32s) Loss: 0.0001(0.0008) Grad: 312.7167  LR: 0.000007  \n",
      "Epoch: [4][2600/7150] Elapsed 33m 15s (remain 58m 10s) Loss: 0.0000(0.0008) Grad: 13.6193  LR: 0.000007  \n",
      "Epoch: [4][2700/7150] Elapsed 34m 32s (remain 56m 53s) Loss: 0.0000(0.0008) Grad: 0.9855  LR: 0.000007  \n",
      "Epoch: [4][2800/7150] Elapsed 35m 46s (remain 55m 32s) Loss: 0.0000(0.0008) Grad: 11.7301  LR: 0.000007  \n",
      "Epoch: [4][2900/7150] Elapsed 37m 0s (remain 54m 12s) Loss: 0.0010(0.0008) Grad: 1908.1947  LR: 0.000007  \n",
      "Epoch: [4][3000/7150] Elapsed 38m 21s (remain 53m 2s) Loss: 0.0000(0.0008) Grad: 16.3678  LR: 0.000007  \n",
      "Epoch: [4][3100/7150] Elapsed 39m 36s (remain 51m 43s) Loss: 0.0013(0.0008) Grad: 16870.9219  LR: 0.000007  \n",
      "Epoch: [4][3200/7150] Elapsed 40m 50s (remain 50m 23s) Loss: 0.0000(0.0008) Grad: 70.1749  LR: 0.000007  \n",
      "Epoch: [4][3300/7150] Elapsed 42m 3s (remain 49m 2s) Loss: 0.0000(0.0008) Grad: 214.4957  LR: 0.000007  \n",
      "Epoch: [4][3400/7150] Elapsed 43m 16s (remain 47m 41s) Loss: 0.0000(0.0008) Grad: 23.2424  LR: 0.000007  \n",
      "Epoch: [4][3500/7150] Elapsed 44m 31s (remain 46m 24s) Loss: 0.0012(0.0008) Grad: 6425.1602  LR: 0.000007  \n",
      "Epoch: [4][3600/7150] Elapsed 45m 47s (remain 45m 7s) Loss: 0.0000(0.0008) Grad: 1433.9747  LR: 0.000007  \n",
      "Epoch: [4][3700/7150] Elapsed 47m 1s (remain 43m 49s) Loss: 0.0000(0.0008) Grad: 44.6051  LR: 0.000007  \n",
      "Epoch: [4][3800/7150] Elapsed 48m 15s (remain 42m 30s) Loss: 0.0002(0.0008) Grad: 5550.3262  LR: 0.000007  \n",
      "Epoch: [4][3900/7150] Elapsed 49m 28s (remain 41m 12s) Loss: 0.0000(0.0008) Grad: 4.9916  LR: 0.000006  \n",
      "Epoch: [4][4000/7150] Elapsed 50m 43s (remain 39m 55s) Loss: 0.0000(0.0008) Grad: 88.4560  LR: 0.000006  \n",
      "Epoch: [4][4100/7150] Elapsed 51m 59s (remain 38m 39s) Loss: 0.0004(0.0008) Grad: 1105.9286  LR: 0.000006  \n",
      "Epoch: [4][4200/7150] Elapsed 53m 14s (remain 37m 22s) Loss: 0.0000(0.0008) Grad: 60.4567  LR: 0.000006  \n",
      "Epoch: [4][4300/7150] Elapsed 54m 28s (remain 36m 5s) Loss: 0.0002(0.0008) Grad: 2872.5022  LR: 0.000006  \n",
      "Epoch: [4][4400/7150] Elapsed 55m 47s (remain 34m 50s) Loss: 0.0053(0.0009) Grad: 37551.3945  LR: 0.000006  \n",
      "Epoch: [4][4500/7150] Elapsed 57m 7s (remain 33m 37s) Loss: 0.0004(0.0009) Grad: 4947.3115  LR: 0.000006  \n",
      "Epoch: [4][4600/7150] Elapsed 58m 22s (remain 32m 20s) Loss: 0.0000(0.0009) Grad: 32.6237  LR: 0.000006  \n",
      "Epoch: [4][4700/7150] Elapsed 59m 37s (remain 31m 3s) Loss: 0.0000(0.0009) Grad: 297.5071  LR: 0.000006  \n",
      "Epoch: [4][4800/7150] Elapsed 60m 52s (remain 29m 47s) Loss: 0.0000(0.0009) Grad: 6.3228  LR: 0.000006  \n",
      "Epoch: [4][4900/7150] Elapsed 62m 7s (remain 28m 30s) Loss: 0.0010(0.0009) Grad: 10979.1133  LR: 0.000006  \n",
      "Epoch: [4][5000/7150] Elapsed 63m 23s (remain 27m 14s) Loss: 0.0000(0.0009) Grad: 105.4069  LR: 0.000006  \n",
      "Epoch: [4][5100/7150] Elapsed 64m 43s (remain 25m 59s) Loss: 0.0079(0.0009) Grad: 106659.4922  LR: 0.000006  \n",
      "Epoch: [4][5200/7150] Elapsed 66m 3s (remain 24m 45s) Loss: 0.0000(0.0009) Grad: 147.5844  LR: 0.000006  \n",
      "Epoch: [4][5300/7150] Elapsed 67m 17s (remain 23m 28s) Loss: 0.0000(0.0009) Grad: 41.7523  LR: 0.000006  \n",
      "Epoch: [4][5400/7150] Elapsed 68m 32s (remain 22m 11s) Loss: 0.0000(0.0009) Grad: 33.5374  LR: 0.000006  \n",
      "Epoch: [4][5500/7150] Elapsed 69m 48s (remain 20m 55s) Loss: 0.0000(0.0009) Grad: 34.2534  LR: 0.000005  \n",
      "Epoch: [4][5600/7150] Elapsed 71m 3s (remain 19m 39s) Loss: 0.0000(0.0009) Grad: 271.5105  LR: 0.000005  \n",
      "Epoch: [4][5700/7150] Elapsed 72m 18s (remain 18m 22s) Loss: 0.0029(0.0009) Grad: 9701.5527  LR: 0.000005  \n",
      "Epoch: [4][5800/7150] Elapsed 73m 33s (remain 17m 6s) Loss: 0.0000(0.0009) Grad: 49.6091  LR: 0.000005  \n",
      "Epoch: [4][5900/7150] Elapsed 74m 53s (remain 15m 51s) Loss: 0.0000(0.0009) Grad: 37.2722  LR: 0.000005  \n",
      "Epoch: [4][6000/7150] Elapsed 76m 8s (remain 14m 34s) Loss: 0.0002(0.0009) Grad: 4863.0347  LR: 0.000005  \n",
      "Epoch: [4][6100/7150] Elapsed 77m 22s (remain 13m 18s) Loss: 0.0000(0.0009) Grad: 61.8733  LR: 0.000005  \n",
      "Epoch: [4][6200/7150] Elapsed 78m 36s (remain 12m 1s) Loss: 0.0000(0.0009) Grad: 58.4409  LR: 0.000005  \n",
      "Epoch: [4][6300/7150] Elapsed 79m 51s (remain 10m 45s) Loss: 0.0001(0.0009) Grad: 404.4731  LR: 0.000005  \n",
      "Epoch: [4][6400/7150] Elapsed 81m 6s (remain 9m 29s) Loss: 0.0000(0.0009) Grad: 64.5536  LR: 0.000005  \n",
      "Epoch: [4][6500/7150] Elapsed 82m 22s (remain 8m 13s) Loss: 0.0003(0.0009) Grad: 5437.4580  LR: 0.000005  \n",
      "Epoch: [4][6600/7150] Elapsed 83m 38s (remain 6m 57s) Loss: 0.0028(0.0009) Grad: 16725.7344  LR: 0.000005  \n",
      "Epoch: [4][6700/7150] Elapsed 84m 52s (remain 5m 41s) Loss: 0.0000(0.0009) Grad: 3.7888  LR: 0.000005  \n",
      "Epoch: [4][6800/7150] Elapsed 86m 7s (remain 4m 25s) Loss: 0.0000(0.0009) Grad: 15.4593  LR: 0.000005  \n",
      "Epoch: [4][6900/7150] Elapsed 87m 21s (remain 3m 9s) Loss: 0.0000(0.0009) Grad: 15.9179  LR: 0.000005  \n",
      "Epoch: [4][7000/7150] Elapsed 88m 37s (remain 1m 53s) Loss: 0.0011(0.0009) Grad: 12230.4121  LR: 0.000005  \n",
      "Epoch: [4][7100/7150] Elapsed 89m 54s (remain 0m 37s) Loss: 0.0000(0.0009) Grad: 57.8463  LR: 0.000004  \n",
      "Epoch: [4][7149/7150] Elapsed 90m 30s (remain 0m 0s) Loss: 0.0006(0.0009) Grad: 4841.0859  LR: 0.000004  \n",
      "EVAL: [0/1192] Elapsed 0m 1s (remain 20m 37s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 34s (remain 6m 7s) Loss: 0.0047(0.0020) \n",
      "EVAL: [200/1192] Elapsed 1m 3s (remain 5m 12s) Loss: 0.0046(0.0023) \n",
      "EVAL: [300/1192] Elapsed 1m 32s (remain 4m 33s) Loss: 0.0010(0.0024) \n",
      "EVAL: [400/1192] Elapsed 2m 1s (remain 3m 59s) Loss: 0.0025(0.0027) \n",
      "EVAL: [500/1192] Elapsed 2m 29s (remain 3m 26s) Loss: 0.0027(0.0026) \n",
      "EVAL: [600/1192] Elapsed 2m 58s (remain 2m 55s) Loss: 0.0008(0.0027) \n",
      "EVAL: [700/1192] Elapsed 3m 26s (remain 2m 24s) Loss: 0.0343(0.0032) \n",
      "EVAL: [800/1192] Elapsed 3m 56s (remain 1m 55s) Loss: 0.0013(0.0033) \n",
      "EVAL: [900/1192] Elapsed 4m 27s (remain 1m 26s) Loss: 0.0020(0.0033) \n",
      "EVAL: [1000/1192] Elapsed 4m 56s (remain 0m 56s) Loss: 0.0000(0.0033) \n",
      "EVAL: [1100/1192] Elapsed 5m 25s (remain 0m 26s) Loss: 0.0000(0.0031) \n",
      "EVAL: [1191/1192] Elapsed 5m 51s (remain 0m 0s) Loss: 0.0000(0.0030) \n",
      "Epoch 4 - avg_train_loss: 0.0009  avg_val_loss: 0.0030  time: 5786s\n",
      "Epoch 4 - Score: 0.8876\n",
      "Epoch 4 - Save Best Score: 0.8876 Model\n",
      "Epoch: [5][0/7150] Elapsed 0m 1s (remain 165m 2s) Loss: 0.0043(0.0043) Grad: 7452.6636  LR: 0.000004  \n",
      "Epoch: [5][100/7150] Elapsed 1m 17s (remain 89m 44s) Loss: 0.0002(0.0011) Grad: 3029.8826  LR: 0.000004  \n",
      "Epoch: [5][200/7150] Elapsed 2m 30s (remain 86m 49s) Loss: 0.0003(0.0009) Grad: 829.7968  LR: 0.000004  \n",
      "Epoch: [5][300/7150] Elapsed 3m 44s (remain 85m 4s) Loss: 0.0000(0.0007) Grad: 115.2626  LR: 0.000004  \n",
      "Epoch: [5][400/7150] Elapsed 4m 57s (remain 83m 25s) Loss: 0.0000(0.0006) Grad: 194.1425  LR: 0.000004  \n",
      "Epoch: [5][500/7150] Elapsed 6m 11s (remain 82m 7s) Loss: 0.0000(0.0006) Grad: 26.0105  LR: 0.000004  \n",
      "Epoch: [5][600/7150] Elapsed 7m 24s (remain 80m 43s) Loss: 0.0000(0.0006) Grad: 16.0287  LR: 0.000004  \n",
      "Epoch: [5][700/7150] Elapsed 8m 39s (remain 79m 36s) Loss: 0.0001(0.0006) Grad: 708.6412  LR: 0.000004  \n",
      "Epoch: [5][800/7150] Elapsed 9m 53s (remain 78m 24s) Loss: 0.0000(0.0006) Grad: 50.5516  LR: 0.000004  \n",
      "Epoch: [5][900/7150] Elapsed 11m 7s (remain 77m 6s) Loss: 0.0003(0.0006) Grad: 2247.4688  LR: 0.000004  \n",
      "Epoch: [5][1000/7150] Elapsed 12m 23s (remain 76m 8s) Loss: 0.0001(0.0006) Grad: 1030.9539  LR: 0.000004  \n",
      "Epoch: [5][1100/7150] Elapsed 13m 37s (remain 74m 51s) Loss: 0.0009(0.0007) Grad: 4342.5015  LR: 0.000004  \n",
      "Epoch: [5][1200/7150] Elapsed 14m 50s (remain 73m 33s) Loss: 0.0002(0.0006) Grad: 508.3264  LR: 0.000004  \n",
      "Epoch: [5][1300/7150] Elapsed 16m 4s (remain 72m 15s) Loss: 0.0000(0.0006) Grad: 1.0391  LR: 0.000004  \n",
      "Epoch: [5][1400/7150] Elapsed 17m 17s (remain 70m 56s) Loss: 0.0000(0.0006) Grad: 74.3218  LR: 0.000004  \n",
      "Epoch: [5][1500/7150] Elapsed 18m 30s (remain 69m 37s) Loss: 0.0000(0.0006) Grad: 33.5117  LR: 0.000004  \n",
      "Epoch: [5][1600/7150] Elapsed 19m 42s (remain 68m 19s) Loss: 0.0007(0.0007) Grad: 5370.4795  LR: 0.000003  \n",
      "Epoch: [5][1700/7150] Elapsed 20m 59s (remain 67m 13s) Loss: 0.0010(0.0007) Grad: 2766.7095  LR: 0.000003  \n",
      "Epoch: [5][1800/7150] Elapsed 22m 13s (remain 66m 0s) Loss: 0.0000(0.0006) Grad: 24.4200  LR: 0.000003  \n",
      "Epoch: [5][1900/7150] Elapsed 23m 25s (remain 64m 42s) Loss: 0.0003(0.0007) Grad: 2630.0554  LR: 0.000003  \n",
      "Epoch: [5][2000/7150] Elapsed 24m 39s (remain 63m 28s) Loss: 0.0006(0.0007) Grad: 6321.9243  LR: 0.000003  \n",
      "Epoch: [5][2100/7150] Elapsed 25m 57s (remain 62m 23s) Loss: 0.0001(0.0007) Grad: 193.0337  LR: 0.000003  \n",
      "Epoch: [5][2200/7150] Elapsed 27m 12s (remain 61m 10s) Loss: 0.0000(0.0007) Grad: 44.3610  LR: 0.000003  \n",
      "Epoch: [5][2300/7150] Elapsed 28m 27s (remain 59m 58s) Loss: 0.0001(0.0007) Grad: 1007.7207  LR: 0.000003  \n",
      "Epoch: [5][2400/7150] Elapsed 29m 41s (remain 58m 42s) Loss: 0.0002(0.0007) Grad: 5174.7993  LR: 0.000003  \n",
      "Epoch: [5][2500/7150] Elapsed 30m 54s (remain 57m 27s) Loss: 0.0015(0.0007) Grad: 2556.1472  LR: 0.000003  \n",
      "Epoch: [5][2600/7150] Elapsed 32m 10s (remain 56m 16s) Loss: 0.0000(0.0007) Grad: 86.2290  LR: 0.000003  \n",
      "Epoch: [5][2700/7150] Elapsed 33m 24s (remain 55m 1s) Loss: 0.0001(0.0007) Grad: 185.1266  LR: 0.000003  \n",
      "Epoch: [5][2800/7150] Elapsed 34m 38s (remain 53m 47s) Loss: 0.0002(0.0007) Grad: 1911.6831  LR: 0.000003  \n",
      "Epoch: [5][2900/7150] Elapsed 35m 53s (remain 52m 34s) Loss: 0.0032(0.0007) Grad: 13936.3369  LR: 0.000003  \n",
      "Epoch: [5][3000/7150] Elapsed 37m 10s (remain 51m 24s) Loss: 0.0000(0.0007) Grad: 150.1611  LR: 0.000003  \n",
      "Epoch: [5][3100/7150] Elapsed 38m 25s (remain 50m 10s) Loss: 0.0001(0.0007) Grad: 475.1996  LR: 0.000003  \n",
      "Epoch: [5][3200/7150] Elapsed 39m 40s (remain 48m 56s) Loss: 0.0005(0.0006) Grad: 439.4901  LR: 0.000002  \n",
      "Epoch: [5][3300/7150] Elapsed 40m 57s (remain 47m 44s) Loss: 0.0002(0.0007) Grad: 659.9366  LR: 0.000002  \n",
      "Epoch: [5][3400/7150] Elapsed 42m 13s (remain 46m 32s) Loss: 0.0019(0.0007) Grad: 18285.0352  LR: 0.000002  \n",
      "Epoch: [5][3500/7150] Elapsed 43m 27s (remain 45m 17s) Loss: 0.0000(0.0007) Grad: 2.5678  LR: 0.000002  \n",
      "Epoch: [5][3600/7150] Elapsed 44m 41s (remain 44m 3s) Loss: 0.0000(0.0007) Grad: 27.4502  LR: 0.000002  \n",
      "Epoch: [5][3700/7150] Elapsed 45m 56s (remain 42m 48s) Loss: 0.0001(0.0006) Grad: 1342.9016  LR: 0.000002  \n",
      "Epoch: [5][3800/7150] Elapsed 47m 12s (remain 41m 35s) Loss: 0.0000(0.0006) Grad: 40.0252  LR: 0.000002  \n",
      "Epoch: [5][3900/7150] Elapsed 48m 28s (remain 40m 22s) Loss: 0.0001(0.0006) Grad: 361.4498  LR: 0.000002  \n",
      "Epoch: [5][4000/7150] Elapsed 49m 42s (remain 39m 7s) Loss: 0.0000(0.0006) Grad: 120.9868  LR: 0.000002  \n",
      "Epoch: [5][4100/7150] Elapsed 50m 55s (remain 37m 51s) Loss: 0.0005(0.0006) Grad: 6558.8291  LR: 0.000002  \n",
      "Epoch: [5][4200/7150] Elapsed 52m 9s (remain 36m 36s) Loss: 0.0000(0.0006) Grad: 9.9530  LR: 0.000002  \n",
      "Epoch: [5][4300/7150] Elapsed 53m 23s (remain 35m 21s) Loss: 0.0012(0.0006) Grad: 9177.9678  LR: 0.000002  \n",
      "Epoch: [5][4400/7150] Elapsed 54m 38s (remain 34m 7s) Loss: 0.0006(0.0006) Grad: 4987.3306  LR: 0.000002  \n",
      "Epoch: [5][4500/7150] Elapsed 55m 53s (remain 32m 53s) Loss: 0.0001(0.0006) Grad: 528.1583  LR: 0.000002  \n",
      "Epoch: [5][4600/7150] Elapsed 57m 6s (remain 31m 38s) Loss: 0.0000(0.0006) Grad: 79.3266  LR: 0.000002  \n",
      "Epoch: [5][4700/7150] Elapsed 58m 19s (remain 30m 22s) Loss: 0.0000(0.0006) Grad: 187.8495  LR: 0.000002  \n",
      "Epoch: [5][4800/7150] Elapsed 59m 31s (remain 29m 7s) Loss: 0.0000(0.0006) Grad: 174.6510  LR: 0.000001  \n",
      "Epoch: [5][4900/7150] Elapsed 60m 47s (remain 27m 53s) Loss: 0.0000(0.0007) Grad: 69.4370  LR: 0.000001  \n",
      "Epoch: [5][5000/7150] Elapsed 62m 1s (remain 26m 38s) Loss: 0.0000(0.0007) Grad: 38.7129  LR: 0.000001  \n",
      "Epoch: [5][5100/7150] Elapsed 63m 14s (remain 25m 24s) Loss: 0.0000(0.0007) Grad: 88.1608  LR: 0.000001  \n",
      "Epoch: [5][5200/7150] Elapsed 64m 27s (remain 24m 9s) Loss: 0.0000(0.0007) Grad: 233.0141  LR: 0.000001  \n",
      "Epoch: [5][5300/7150] Elapsed 65m 40s (remain 22m 54s) Loss: 0.0000(0.0007) Grad: 84.9017  LR: 0.000001  \n",
      "Epoch: [5][5400/7150] Elapsed 66m 55s (remain 21m 40s) Loss: 0.0000(0.0007) Grad: 12.0243  LR: 0.000001  \n",
      "Epoch: [5][5500/7150] Elapsed 68m 15s (remain 20m 27s) Loss: 0.0000(0.0007) Grad: 129.2922  LR: 0.000001  \n",
      "Epoch: [5][5600/7150] Elapsed 69m 29s (remain 19m 13s) Loss: 0.0019(0.0007) Grad: 36881.4062  LR: 0.000001  \n",
      "Epoch: [5][5700/7150] Elapsed 70m 43s (remain 17m 58s) Loss: 0.0000(0.0007) Grad: 70.3271  LR: 0.000001  \n",
      "Epoch: [5][5800/7150] Elapsed 71m 56s (remain 16m 43s) Loss: 0.0001(0.0007) Grad: 716.1639  LR: 0.000001  \n",
      "Epoch: [5][5900/7150] Elapsed 73m 12s (remain 15m 29s) Loss: 0.0014(0.0007) Grad: 8607.2100  LR: 0.000001  \n",
      "Epoch: [5][6000/7150] Elapsed 74m 27s (remain 14m 15s) Loss: 0.0000(0.0007) Grad: 5.8320  LR: 0.000001  \n",
      "Epoch: [5][6100/7150] Elapsed 75m 43s (remain 13m 1s) Loss: 0.0000(0.0007) Grad: 37.4383  LR: 0.000001  \n",
      "Epoch: [5][6200/7150] Elapsed 77m 1s (remain 11m 47s) Loss: 0.0000(0.0007) Grad: 5.5890  LR: 0.000001  \n",
      "Epoch: [5][6300/7150] Elapsed 78m 16s (remain 10m 32s) Loss: 0.0000(0.0007) Grad: 22.4734  LR: 0.000001  \n",
      "Epoch: [5][6400/7150] Elapsed 79m 31s (remain 9m 18s) Loss: 0.0007(0.0007) Grad: 12735.4258  LR: 0.000000  \n",
      "Epoch: [5][6500/7150] Elapsed 80m 47s (remain 8m 3s) Loss: 0.0000(0.0007) Grad: 571.8822  LR: 0.000000  \n",
      "Epoch: [5][6600/7150] Elapsed 82m 2s (remain 6m 49s) Loss: 0.0009(0.0007) Grad: 9948.4805  LR: 0.000000  \n",
      "Epoch: [5][6700/7150] Elapsed 83m 17s (remain 5m 34s) Loss: 0.0000(0.0007) Grad: 83.0537  LR: 0.000000  \n",
      "Epoch: [5][6800/7150] Elapsed 84m 34s (remain 4m 20s) Loss: 0.0000(0.0007) Grad: 51.5071  LR: 0.000000  \n",
      "Epoch: [5][6900/7150] Elapsed 85m 49s (remain 3m 5s) Loss: 0.0087(0.0007) Grad: 23899.9453  LR: 0.000000  \n",
      "Epoch: [5][7000/7150] Elapsed 87m 5s (remain 1m 51s) Loss: 0.0000(0.0007) Grad: 4.8699  LR: 0.000000  \n",
      "Epoch: [5][7100/7150] Elapsed 88m 21s (remain 0m 36s) Loss: 0.0000(0.0007) Grad: 3.9020  LR: 0.000000  \n",
      "Epoch: [5][7149/7150] Elapsed 88m 58s (remain 0m 0s) Loss: 0.0000(0.0007) Grad: 248.6220  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 17m 48s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 32s (remain 5m 47s) Loss: 0.0076(0.0023) \n",
      "EVAL: [200/1192] Elapsed 1m 2s (remain 5m 6s) Loss: 0.0063(0.0028) \n",
      "EVAL: [300/1192] Elapsed 1m 31s (remain 4m 32s) Loss: 0.0006(0.0029) \n",
      "EVAL: [400/1192] Elapsed 2m 2s (remain 4m 2s) Loss: 0.0023(0.0031) \n",
      "EVAL: [500/1192] Elapsed 2m 32s (remain 3m 30s) Loss: 0.0032(0.0030) \n",
      "EVAL: [600/1192] Elapsed 3m 2s (remain 2m 59s) Loss: 0.0007(0.0032) \n",
      "EVAL: [700/1192] Elapsed 3m 32s (remain 2m 29s) Loss: 0.0383(0.0038) \n",
      "EVAL: [800/1192] Elapsed 4m 3s (remain 1m 58s) Loss: 0.0007(0.0039) \n",
      "EVAL: [900/1192] Elapsed 4m 33s (remain 1m 28s) Loss: 0.0010(0.0039) \n",
      "EVAL: [1000/1192] Elapsed 5m 3s (remain 0m 57s) Loss: 0.0000(0.0038) \n",
      "EVAL: [1100/1192] Elapsed 5m 33s (remain 0m 27s) Loss: 0.0001(0.0037) \n",
      "EVAL: [1191/1192] Elapsed 6m 0s (remain 0m 0s) Loss: 0.0000(0.0036) \n",
      "Epoch 5 - avg_train_loss: 0.0007  avg_val_loss: 0.0036  time: 5703s\n",
      "Epoch 5 - Score: 0.8873\n",
      "========== fold: 1 training ==========\n",
      "get pseudo plain from ../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_1.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_1.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp083/pseudo_labels_1.npy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8205fbe28745edb2f93b74fb290bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17056ab414b476ab712d1966072f916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(612602, 950)\n",
      "(612602, 6) (612602, 950)\n",
      "(10725, 7)\n",
      "(21450, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'deberta.embeddings.position_embeddings.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\n",
      "Epoch: [1][0/7150] Elapsed 0m 1s (remain 185m 40s) Loss: 0.0626(0.0626) Grad: 62986.7578  LR: 0.000000  \n",
      "Epoch: [1][100/7150] Elapsed 1m 19s (remain 92m 6s) Loss: 0.0479(0.0591) Grad: 48721.3867  LR: 0.000001  \n",
      "Epoch: [1][200/7150] Elapsed 2m 37s (remain 90m 50s) Loss: 0.0289(0.0482) Grad: 19064.6680  LR: 0.000001  \n",
      "Epoch: [1][300/7150] Elapsed 3m 52s (remain 88m 9s) Loss: 0.0060(0.0377) Grad: 5431.4907  LR: 0.000002  \n",
      "Epoch: [1][400/7150] Elapsed 5m 9s (remain 86m 41s) Loss: 0.0072(0.0313) Grad: 2388.9690  LR: 0.000002  \n",
      "Epoch: [1][500/7150] Elapsed 6m 25s (remain 85m 9s) Loss: 0.0124(0.0273) Grad: 3660.6504  LR: 0.000003  \n",
      "Epoch: [1][600/7150] Elapsed 7m 39s (remain 83m 26s) Loss: 0.0199(0.0247) Grad: 7906.6631  LR: 0.000003  \n",
      "Epoch: [1][700/7150] Elapsed 8m 54s (remain 81m 54s) Loss: 0.0043(0.0230) Grad: 3205.5105  LR: 0.000004  \n",
      "Epoch: [1][800/7150] Elapsed 10m 9s (remain 80m 29s) Loss: 0.0187(0.0216) Grad: 7055.8174  LR: 0.000004  \n",
      "Epoch: [1][900/7150] Elapsed 11m 24s (remain 79m 6s) Loss: 0.0040(0.0204) Grad: 3086.3118  LR: 0.000005  \n",
      "Epoch: [1][1000/7150] Elapsed 12m 46s (remain 78m 28s) Loss: 0.0057(0.0195) Grad: 3764.6990  LR: 0.000006  \n",
      "Epoch: [1][1100/7150] Elapsed 14m 2s (remain 77m 10s) Loss: 0.0057(0.0186) Grad: 4358.6646  LR: 0.000006  \n",
      "Epoch: [1][1200/7150] Elapsed 15m 17s (remain 75m 47s) Loss: 0.0094(0.0176) Grad: 18302.0215  LR: 0.000007  \n",
      "Epoch: [1][1300/7150] Elapsed 16m 35s (remain 74m 33s) Loss: 0.0044(0.0166) Grad: 14505.4404  LR: 0.000007  \n",
      "Epoch: [1][1400/7150] Elapsed 17m 54s (remain 73m 29s) Loss: 0.0065(0.0158) Grad: 24922.5488  LR: 0.000008  \n",
      "Epoch: [1][1500/7150] Elapsed 19m 9s (remain 72m 7s) Loss: 0.0021(0.0150) Grad: 7627.8623  LR: 0.000008  \n",
      "Epoch: [1][1600/7150] Elapsed 20m 24s (remain 70m 45s) Loss: 0.0044(0.0143) Grad: 9531.6992  LR: 0.000009  \n",
      "Epoch: [1][1700/7150] Elapsed 21m 41s (remain 69m 29s) Loss: 0.0005(0.0137) Grad: 1229.7827  LR: 0.000010  \n",
      "Epoch: [1][1800/7150] Elapsed 22m 58s (remain 68m 14s) Loss: 0.0009(0.0132) Grad: 4279.9165  LR: 0.000010  \n",
      "Epoch: [1][1900/7150] Elapsed 24m 14s (remain 66m 57s) Loss: 0.0007(0.0126) Grad: 1736.3297  LR: 0.000011  \n",
      "Epoch: [1][2000/7150] Elapsed 25m 29s (remain 65m 36s) Loss: 0.0011(0.0122) Grad: 3506.6968  LR: 0.000011  \n",
      "Epoch: [1][2100/7150] Elapsed 26m 44s (remain 64m 16s) Loss: 0.0012(0.0118) Grad: 4555.0933  LR: 0.000012  \n",
      "Epoch: [1][2200/7150] Elapsed 28m 0s (remain 62m 57s) Loss: 0.0007(0.0114) Grad: 1581.1724  LR: 0.000012  \n",
      "Epoch: [1][2300/7150] Elapsed 29m 18s (remain 61m 45s) Loss: 0.0010(0.0111) Grad: 2913.5095  LR: 0.000013  \n",
      "Epoch: [1][2400/7150] Elapsed 30m 33s (remain 60m 26s) Loss: 0.0003(0.0107) Grad: 584.8970  LR: 0.000013  \n",
      "Epoch: [1][2500/7150] Elapsed 31m 47s (remain 59m 6s) Loss: 0.0066(0.0104) Grad: 53166.6367  LR: 0.000014  \n",
      "Epoch: [1][2600/7150] Elapsed 33m 4s (remain 57m 50s) Loss: 0.0011(0.0101) Grad: 1742.2839  LR: 0.000015  \n",
      "Epoch: [1][2700/7150] Elapsed 34m 22s (remain 56m 36s) Loss: 0.0006(0.0099) Grad: 2767.6677  LR: 0.000015  \n",
      "Epoch: [1][2800/7150] Elapsed 35m 41s (remain 55m 25s) Loss: 0.0009(0.0096) Grad: 12135.8916  LR: 0.000016  \n",
      "Epoch: [1][2900/7150] Elapsed 37m 2s (remain 54m 15s) Loss: 0.0002(0.0094) Grad: 164.0495  LR: 0.000016  \n",
      "Epoch: [1][3000/7150] Elapsed 38m 18s (remain 52m 57s) Loss: 0.0008(0.0092) Grad: 9358.8486  LR: 0.000017  \n",
      "Epoch: [1][3100/7150] Elapsed 39m 34s (remain 51m 39s) Loss: 0.0007(0.0089) Grad: 1247.2235  LR: 0.000017  \n",
      "Epoch: [1][3200/7150] Elapsed 40m 55s (remain 50m 28s) Loss: 0.0049(0.0088) Grad: 12532.0684  LR: 0.000018  \n",
      "Epoch: [1][3300/7150] Elapsed 42m 16s (remain 49m 18s) Loss: 0.0015(0.0086) Grad: 5572.2935  LR: 0.000018  \n",
      "Epoch: [1][3400/7150] Elapsed 43m 32s (remain 47m 59s) Loss: 0.0003(0.0084) Grad: 436.3344  LR: 0.000019  \n",
      "Epoch: [1][3500/7150] Elapsed 44m 47s (remain 46m 40s) Loss: 0.0002(0.0082) Grad: 207.8736  LR: 0.000020  \n",
      "Epoch: [1][3600/7150] Elapsed 46m 7s (remain 45m 27s) Loss: 0.0005(0.0080) Grad: 1077.4736  LR: 0.000020  \n",
      "Epoch: [1][3700/7150] Elapsed 47m 26s (remain 44m 13s) Loss: 0.0001(0.0079) Grad: 53.6495  LR: 0.000020  \n",
      "Epoch: [1][3800/7150] Elapsed 48m 41s (remain 42m 54s) Loss: 0.0004(0.0077) Grad: 3062.3118  LR: 0.000020  \n",
      "Epoch: [1][3900/7150] Elapsed 49m 56s (remain 41m 35s) Loss: 0.0003(0.0076) Grad: 348.4226  LR: 0.000020  \n",
      "Epoch: [1][4000/7150] Elapsed 51m 10s (remain 40m 16s) Loss: 0.0050(0.0075) Grad: 57313.4688  LR: 0.000020  \n",
      "Epoch: [1][4100/7150] Elapsed 52m 26s (remain 38m 59s) Loss: 0.0000(0.0074) Grad: 103.4381  LR: 0.000020  \n",
      "Epoch: [1][4200/7150] Elapsed 53m 43s (remain 37m 42s) Loss: 0.0003(0.0072) Grad: 884.9050  LR: 0.000020  \n",
      "Epoch: [1][4300/7150] Elapsed 55m 5s (remain 36m 29s) Loss: 0.0008(0.0071) Grad: 6928.5322  LR: 0.000020  \n",
      "Epoch: [1][4400/7150] Elapsed 56m 25s (remain 35m 14s) Loss: 0.0058(0.0070) Grad: 49927.2461  LR: 0.000019  \n",
      "Epoch: [1][4500/7150] Elapsed 57m 41s (remain 33m 57s) Loss: 0.0027(0.0069) Grad: 24695.3438  LR: 0.000019  \n",
      "Epoch: [1][4600/7150] Elapsed 59m 3s (remain 32m 43s) Loss: 0.0012(0.0068) Grad: 3091.1487  LR: 0.000019  \n",
      "Epoch: [1][4700/7150] Elapsed 60m 25s (remain 31m 28s) Loss: 0.0004(0.0067) Grad: 2222.1831  LR: 0.000019  \n",
      "Epoch: [1][4800/7150] Elapsed 61m 46s (remain 30m 13s) Loss: 0.0001(0.0066) Grad: 138.9744  LR: 0.000019  \n",
      "Epoch: [1][4900/7150] Elapsed 63m 7s (remain 28m 58s) Loss: 0.0029(0.0066) Grad: 15759.5576  LR: 0.000019  \n",
      "Epoch: [1][5000/7150] Elapsed 64m 29s (remain 27m 42s) Loss: 0.0042(0.0065) Grad: 20141.7988  LR: 0.000019  \n",
      "Epoch: [1][5100/7150] Elapsed 65m 46s (remain 26m 25s) Loss: 0.0013(0.0064) Grad: 12207.9727  LR: 0.000019  \n",
      "Epoch: [1][5200/7150] Elapsed 67m 3s (remain 25m 7s) Loss: 0.0002(0.0063) Grad: 328.7871  LR: 0.000019  \n",
      "Epoch: [1][5300/7150] Elapsed 68m 20s (remain 23m 50s) Loss: 0.0000(0.0062) Grad: 76.0530  LR: 0.000019  \n",
      "Epoch: [1][5400/7150] Elapsed 69m 36s (remain 22m 32s) Loss: 0.0001(0.0061) Grad: 338.2843  LR: 0.000019  \n",
      "Epoch: [1][5500/7150] Elapsed 70m 56s (remain 21m 16s) Loss: 0.0003(0.0061) Grad: 2590.3721  LR: 0.000019  \n",
      "Epoch: [1][5600/7150] Elapsed 72m 14s (remain 19m 58s) Loss: 0.0006(0.0060) Grad: 6560.7715  LR: 0.000019  \n",
      "Epoch: [1][5700/7150] Elapsed 73m 29s (remain 18m 40s) Loss: 0.0012(0.0059) Grad: 9565.9248  LR: 0.000019  \n",
      "Epoch: [1][5800/7150] Elapsed 74m 45s (remain 17m 23s) Loss: 0.0002(0.0058) Grad: 717.4467  LR: 0.000019  \n",
      "Epoch: [1][5900/7150] Elapsed 76m 1s (remain 16m 5s) Loss: 0.0010(0.0058) Grad: 11405.2686  LR: 0.000019  \n",
      "Epoch: [1][6000/7150] Elapsed 77m 19s (remain 14m 48s) Loss: 0.0015(0.0057) Grad: 4937.6558  LR: 0.000018  \n",
      "Epoch: [1][6100/7150] Elapsed 78m 41s (remain 13m 31s) Loss: 0.0070(0.0057) Grad: 52164.6445  LR: 0.000018  \n",
      "Epoch: [1][6200/7150] Elapsed 79m 57s (remain 12m 14s) Loss: 0.0000(0.0056) Grad: 77.3430  LR: 0.000018  \n",
      "Epoch: [1][6300/7150] Elapsed 81m 13s (remain 10m 56s) Loss: 0.0000(0.0055) Grad: 43.1806  LR: 0.000018  \n",
      "Epoch: [1][6400/7150] Elapsed 82m 28s (remain 9m 39s) Loss: 0.0001(0.0055) Grad: 277.0281  LR: 0.000018  \n",
      "Epoch: [1][6500/7150] Elapsed 83m 45s (remain 8m 21s) Loss: 0.0001(0.0054) Grad: 1121.7534  LR: 0.000018  \n",
      "Epoch: [1][6600/7150] Elapsed 85m 1s (remain 7m 4s) Loss: 0.0002(0.0054) Grad: 1527.9283  LR: 0.000018  \n",
      "Epoch: [1][6700/7150] Elapsed 86m 17s (remain 5m 46s) Loss: 0.0004(0.0053) Grad: 1124.3894  LR: 0.000018  \n",
      "Epoch: [1][6800/7150] Elapsed 87m 32s (remain 4m 29s) Loss: 0.0001(0.0053) Grad: 859.3362  LR: 0.000018  \n",
      "Epoch: [1][6900/7150] Elapsed 88m 50s (remain 3m 12s) Loss: 0.0001(0.0053) Grad: 322.2468  LR: 0.000018  \n",
      "Epoch: [1][7000/7150] Elapsed 90m 8s (remain 1m 55s) Loss: 0.0029(0.0052) Grad: 30668.0566  LR: 0.000018  \n",
      "Epoch: [1][7100/7150] Elapsed 91m 24s (remain 0m 37s) Loss: 0.0001(0.0052) Grad: 439.6258  LR: 0.000018  \n",
      "Epoch: [1][7149/7150] Elapsed 92m 3s (remain 0m 0s) Loss: 0.0043(0.0051) Grad: 53146.7461  LR: 0.000018  \n",
      "EVAL: [0/1192] Elapsed 0m 1s (remain 24m 0s) Loss: 0.0001(0.0001) \n",
      "EVAL: [100/1192] Elapsed 0m 30s (remain 5m 34s) Loss: 0.0002(0.0021) \n",
      "EVAL: [200/1192] Elapsed 1m 0s (remain 4m 56s) Loss: 0.0001(0.0028) \n",
      "EVAL: [300/1192] Elapsed 1m 29s (remain 4m 25s) Loss: 0.0006(0.0040) \n",
      "EVAL: [400/1192] Elapsed 2m 1s (remain 4m 0s) Loss: 0.0077(0.0040) \n",
      "EVAL: [500/1192] Elapsed 2m 31s (remain 3m 29s) Loss: 0.0109(0.0037) \n",
      "EVAL: [600/1192] Elapsed 3m 0s (remain 2m 57s) Loss: 0.0547(0.0037) \n",
      "EVAL: [700/1192] Elapsed 3m 29s (remain 2m 27s) Loss: 0.0023(0.0042) \n",
      "EVAL: [800/1192] Elapsed 3m 59s (remain 1m 57s) Loss: 0.0019(0.0040) \n",
      "EVAL: [900/1192] Elapsed 4m 30s (remain 1m 27s) Loss: 0.0023(0.0039) \n",
      "EVAL: [1000/1192] Elapsed 5m 0s (remain 0m 57s) Loss: 0.0000(0.0037) \n",
      "EVAL: [1100/1192] Elapsed 5m 30s (remain 0m 27s) Loss: 0.0020(0.0036) \n",
      "EVAL: [1191/1192] Elapsed 5m 57s (remain 0m 0s) Loss: 0.0030(0.0034) \n",
      "Epoch 1 - avg_train_loss: 0.0051  avg_val_loss: 0.0034  time: 5885s\n",
      "Epoch 1 - Score: 0.8683\n",
      "Epoch 1 - Save Best Score: 0.8683 Model\n",
      "Epoch: [2][0/7150] Elapsed 0m 1s (remain 198m 26s) Loss: 0.0002(0.0002) Grad: 1092.3883  LR: 0.000018  \n",
      "Epoch: [2][100/7150] Elapsed 1m 20s (remain 93m 29s) Loss: 0.0000(0.0021) Grad: 52.2939  LR: 0.000018  \n",
      "Epoch: [2][200/7150] Elapsed 2m 42s (remain 93m 49s) Loss: 0.0054(0.0018) Grad: 9260.3408  LR: 0.000018  \n",
      "Epoch: [2][300/7150] Elapsed 3m 58s (remain 90m 18s) Loss: 0.0039(0.0017) Grad: 8800.9824  LR: 0.000018  \n",
      "Epoch: [2][400/7150] Elapsed 5m 12s (remain 87m 42s) Loss: 0.0002(0.0018) Grad: 476.8743  LR: 0.000018  \n",
      "Epoch: [2][500/7150] Elapsed 6m 27s (remain 85m 36s) Loss: 0.0004(0.0017) Grad: 979.3292  LR: 0.000017  \n",
      "Epoch: [2][600/7150] Elapsed 7m 41s (remain 83m 49s) Loss: 0.0000(0.0018) Grad: 94.8375  LR: 0.000017  \n",
      "Epoch: [2][700/7150] Elapsed 8m 57s (remain 82m 23s) Loss: 0.0028(0.0019) Grad: 7268.2139  LR: 0.000017  \n",
      "Epoch: [2][800/7150] Elapsed 10m 16s (remain 81m 26s) Loss: 0.0013(0.0019) Grad: 3039.0513  LR: 0.000017  \n",
      "Epoch: [2][900/7150] Elapsed 11m 33s (remain 80m 9s) Loss: 0.0025(0.0019) Grad: 3253.6375  LR: 0.000017  \n",
      "Epoch: [2][1000/7150] Elapsed 12m 48s (remain 78m 39s) Loss: 0.0001(0.0019) Grad: 67.5578  LR: 0.000017  \n",
      "Epoch: [2][1100/7150] Elapsed 14m 3s (remain 77m 13s) Loss: 0.0096(0.0019) Grad: 6867.3589  LR: 0.000017  \n",
      "Epoch: [2][1200/7150] Elapsed 15m 20s (remain 75m 57s) Loss: 0.0007(0.0018) Grad: 1489.3580  LR: 0.000017  \n",
      "Epoch: [2][1300/7150] Elapsed 16m 35s (remain 74m 36s) Loss: 0.0001(0.0018) Grad: 550.0604  LR: 0.000017  \n",
      "Epoch: [2][1400/7150] Elapsed 17m 50s (remain 73m 12s) Loss: 0.0013(0.0018) Grad: 9048.0127  LR: 0.000017  \n",
      "Epoch: [2][1500/7150] Elapsed 19m 4s (remain 71m 47s) Loss: 0.0005(0.0017) Grad: 1712.8484  LR: 0.000017  \n",
      "Epoch: [2][1600/7150] Elapsed 20m 18s (remain 70m 23s) Loss: 0.0013(0.0017) Grad: 17944.5938  LR: 0.000017  \n",
      "Epoch: [2][1700/7150] Elapsed 21m 35s (remain 69m 8s) Loss: 0.0005(0.0017) Grad: 708.8826  LR: 0.000017  \n",
      "Epoch: [2][1800/7150] Elapsed 22m 49s (remain 67m 48s) Loss: 0.0009(0.0017) Grad: 4198.1187  LR: 0.000017  \n",
      "Epoch: [2][1900/7150] Elapsed 24m 5s (remain 66m 30s) Loss: 0.0002(0.0017) Grad: 375.0592  LR: 0.000017  \n",
      "Epoch: [2][2000/7150] Elapsed 25m 19s (remain 65m 9s) Loss: 0.0016(0.0017) Grad: 8670.3477  LR: 0.000017  \n",
      "Epoch: [2][2100/7150] Elapsed 26m 32s (remain 63m 47s) Loss: 0.0004(0.0017) Grad: 1607.5425  LR: 0.000016  \n",
      "Epoch: [2][2200/7150] Elapsed 27m 48s (remain 62m 31s) Loss: 0.0001(0.0017) Grad: 597.2658  LR: 0.000016  \n",
      "Epoch: [2][2300/7150] Elapsed 29m 4s (remain 61m 16s) Loss: 0.0032(0.0017) Grad: 5255.1304  LR: 0.000016  \n",
      "Epoch: [2][2400/7150] Elapsed 30m 19s (remain 59m 58s) Loss: 0.0002(0.0017) Grad: 1129.0492  LR: 0.000016  \n",
      "Epoch: [2][2500/7150] Elapsed 31m 33s (remain 58m 40s) Loss: 0.0003(0.0017) Grad: 1151.9355  LR: 0.000016  \n",
      "Epoch: [2][2600/7150] Elapsed 32m 50s (remain 57m 26s) Loss: 0.0000(0.0016) Grad: 696.6935  LR: 0.000016  \n",
      "Epoch: [2][2700/7150] Elapsed 34m 6s (remain 56m 11s) Loss: 0.0000(0.0016) Grad: 42.3170  LR: 0.000016  \n",
      "Epoch: [2][2800/7150] Elapsed 35m 26s (remain 55m 1s) Loss: 0.0004(0.0016) Grad: 785.2144  LR: 0.000016  \n",
      "Epoch: [2][2900/7150] Elapsed 36m 42s (remain 53m 45s) Loss: 0.0006(0.0016) Grad: 5097.5342  LR: 0.000016  \n",
      "Epoch: [2][3000/7150] Elapsed 37m 57s (remain 52m 29s) Loss: 0.0017(0.0016) Grad: 13003.3320  LR: 0.000016  \n",
      "Epoch: [2][3100/7150] Elapsed 39m 13s (remain 51m 13s) Loss: 0.0000(0.0016) Grad: 214.9709  LR: 0.000016  \n",
      "Epoch: [2][3200/7150] Elapsed 40m 29s (remain 49m 57s) Loss: 0.0010(0.0016) Grad: 5840.9526  LR: 0.000016  \n",
      "Epoch: [2][3300/7150] Elapsed 41m 46s (remain 48m 42s) Loss: 0.0006(0.0016) Grad: 2449.3308  LR: 0.000016  \n",
      "Epoch: [2][3400/7150] Elapsed 43m 1s (remain 47m 25s) Loss: 0.0000(0.0016) Grad: 19.0894  LR: 0.000016  \n",
      "Epoch: [2][3500/7150] Elapsed 44m 16s (remain 46m 9s) Loss: 0.0005(0.0016) Grad: 3012.0095  LR: 0.000016  \n",
      "Epoch: [2][3600/7150] Elapsed 45m 32s (remain 44m 53s) Loss: 0.0017(0.0016) Grad: 3736.4617  LR: 0.000016  \n",
      "Epoch: [2][3700/7150] Elapsed 46m 48s (remain 43m 37s) Loss: 0.0001(0.0016) Grad: 171.5887  LR: 0.000015  \n",
      "Epoch: [2][3800/7150] Elapsed 48m 7s (remain 42m 24s) Loss: 0.0065(0.0016) Grad: 3462.3486  LR: 0.000015  \n",
      "Epoch: [2][3900/7150] Elapsed 49m 26s (remain 41m 10s) Loss: 0.0002(0.0016) Grad: 550.5424  LR: 0.000015  \n",
      "Epoch: [2][4000/7150] Elapsed 50m 41s (remain 39m 53s) Loss: 0.0001(0.0016) Grad: 243.6520  LR: 0.000015  \n",
      "Epoch: [2][4100/7150] Elapsed 51m 55s (remain 38m 36s) Loss: 0.0001(0.0016) Grad: 192.0811  LR: 0.000015  \n",
      "Epoch: [2][4200/7150] Elapsed 53m 9s (remain 37m 18s) Loss: 0.0007(0.0016) Grad: 7565.1494  LR: 0.000015  \n",
      "Epoch: [2][4300/7150] Elapsed 54m 23s (remain 36m 1s) Loss: 0.0003(0.0016) Grad: 3696.4973  LR: 0.000015  \n",
      "Epoch: [2][4400/7150] Elapsed 55m 39s (remain 34m 45s) Loss: 0.0000(0.0016) Grad: 64.6100  LR: 0.000015  \n",
      "Epoch: [2][4500/7150] Elapsed 56m 55s (remain 33m 30s) Loss: 0.0001(0.0016) Grad: 296.3728  LR: 0.000015  \n",
      "Epoch: [2][4600/7150] Elapsed 58m 10s (remain 32m 13s) Loss: 0.0012(0.0016) Grad: 13554.2432  LR: 0.000015  \n",
      "Epoch: [2][4700/7150] Elapsed 59m 24s (remain 30m 57s) Loss: 0.0013(0.0016) Grad: 9215.7354  LR: 0.000015  \n",
      "Epoch: [2][4800/7150] Elapsed 60m 39s (remain 29m 40s) Loss: 0.0002(0.0016) Grad: 423.2574  LR: 0.000015  \n",
      "Epoch: [2][4900/7150] Elapsed 61m 55s (remain 28m 25s) Loss: 0.0004(0.0016) Grad: 3433.4624  LR: 0.000015  \n",
      "Epoch: [2][5000/7150] Elapsed 63m 11s (remain 27m 9s) Loss: 0.0005(0.0016) Grad: 3696.5464  LR: 0.000015  \n",
      "Epoch: [2][5100/7150] Elapsed 64m 25s (remain 25m 52s) Loss: 0.0000(0.0016) Grad: 142.3239  LR: 0.000015  \n",
      "Epoch: [2][5200/7150] Elapsed 65m 40s (remain 24m 36s) Loss: 0.0031(0.0016) Grad: 10026.0068  LR: 0.000015  \n",
      "Epoch: [2][5300/7150] Elapsed 66m 54s (remain 23m 20s) Loss: 0.0001(0.0016) Grad: 210.3754  LR: 0.000014  \n",
      "Epoch: [2][5400/7150] Elapsed 68m 10s (remain 22m 4s) Loss: 0.0019(0.0016) Grad: 26482.5410  LR: 0.000014  \n",
      "Epoch: [2][5500/7150] Elapsed 69m 25s (remain 20m 48s) Loss: 0.0001(0.0015) Grad: 896.9677  LR: 0.000014  \n",
      "Epoch: [2][5600/7150] Elapsed 70m 39s (remain 19m 32s) Loss: 0.0001(0.0015) Grad: 385.5624  LR: 0.000014  \n",
      "Epoch: [2][5700/7150] Elapsed 71m 55s (remain 18m 16s) Loss: 0.0008(0.0015) Grad: 5593.4194  LR: 0.000014  \n",
      "Epoch: [2][5800/7150] Elapsed 73m 12s (remain 17m 1s) Loss: 0.0001(0.0015) Grad: 195.1075  LR: 0.000014  \n",
      "Epoch: [2][5900/7150] Elapsed 74m 29s (remain 15m 46s) Loss: 0.0002(0.0015) Grad: 2157.2566  LR: 0.000014  \n",
      "Epoch: [2][6000/7150] Elapsed 75m 45s (remain 14m 30s) Loss: 0.0015(0.0015) Grad: 8923.4727  LR: 0.000014  \n",
      "Epoch: [2][6100/7150] Elapsed 77m 0s (remain 13m 14s) Loss: 0.0000(0.0015) Grad: 14.8702  LR: 0.000014  \n",
      "Epoch: [2][6200/7150] Elapsed 78m 15s (remain 11m 58s) Loss: 0.0008(0.0015) Grad: 21010.7812  LR: 0.000014  \n",
      "Epoch: [2][6300/7150] Elapsed 79m 30s (remain 10m 42s) Loss: 0.0000(0.0015) Grad: 36.5813  LR: 0.000014  \n",
      "Epoch: [2][6400/7150] Elapsed 80m 45s (remain 9m 26s) Loss: 0.0000(0.0015) Grad: 39.7441  LR: 0.000014  \n",
      "Epoch: [2][6500/7150] Elapsed 82m 1s (remain 8m 11s) Loss: 0.0001(0.0015) Grad: 527.0962  LR: 0.000014  \n",
      "Epoch: [2][6600/7150] Elapsed 83m 18s (remain 6m 55s) Loss: 0.0020(0.0015) Grad: 18330.2500  LR: 0.000014  \n",
      "Epoch: [2][6700/7150] Elapsed 84m 34s (remain 5m 40s) Loss: 0.0032(0.0015) Grad: 15231.4512  LR: 0.000014  \n",
      "Epoch: [2][6800/7150] Elapsed 85m 49s (remain 4m 24s) Loss: 0.0004(0.0015) Grad: 4400.2983  LR: 0.000014  \n",
      "Epoch: [2][6900/7150] Elapsed 87m 4s (remain 3m 8s) Loss: 0.0000(0.0015) Grad: 56.2646  LR: 0.000013  \n",
      "Epoch: [2][7000/7150] Elapsed 88m 22s (remain 1m 52s) Loss: 0.0000(0.0015) Grad: 85.7113  LR: 0.000013  \n",
      "Epoch: [2][7100/7150] Elapsed 89m 40s (remain 0m 37s) Loss: 0.0001(0.0015) Grad: 286.5851  LR: 0.000013  \n",
      "Epoch: [2][7149/7150] Elapsed 90m 18s (remain 0m 0s) Loss: 0.0000(0.0015) Grad: 124.7592  LR: 0.000013  \n",
      "EVAL: [0/1192] Elapsed 0m 1s (remain 24m 6s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 30s (remain 5m 25s) Loss: 0.0001(0.0019) \n",
      "EVAL: [200/1192] Elapsed 1m 0s (remain 4m 59s) Loss: 0.0000(0.0027) \n",
      "EVAL: [300/1192] Elapsed 1m 33s (remain 4m 37s) Loss: 0.0005(0.0040) \n",
      "EVAL: [400/1192] Elapsed 2m 2s (remain 4m 1s) Loss: 0.0102(0.0041) \n",
      "EVAL: [500/1192] Elapsed 2m 31s (remain 3m 29s) Loss: 0.0121(0.0037) \n",
      "EVAL: [600/1192] Elapsed 3m 0s (remain 2m 57s) Loss: 0.0578(0.0037) \n",
      "EVAL: [700/1192] Elapsed 3m 29s (remain 2m 26s) Loss: 0.0018(0.0043) \n",
      "EVAL: [800/1192] Elapsed 3m 58s (remain 1m 56s) Loss: 0.0040(0.0041) \n",
      "EVAL: [900/1192] Elapsed 4m 27s (remain 1m 26s) Loss: 0.0002(0.0039) \n",
      "EVAL: [1000/1192] Elapsed 4m 57s (remain 0m 56s) Loss: 0.0000(0.0038) \n",
      "EVAL: [1100/1192] Elapsed 5m 28s (remain 0m 27s) Loss: 0.0022(0.0036) \n",
      "EVAL: [1191/1192] Elapsed 5m 57s (remain 0m 0s) Loss: 0.0031(0.0034) \n",
      "Epoch 2 - avg_train_loss: 0.0015  avg_val_loss: 0.0034  time: 5780s\n",
      "Epoch 2 - Score: 0.8761\n",
      "Epoch 2 - Save Best Score: 0.8761 Model\n",
      "Epoch: [3][0/7150] Elapsed 0m 1s (remain 191m 48s) Loss: 0.0003(0.0003) Grad: 2284.5491  LR: 0.000013  \n",
      "Epoch: [3][100/7150] Elapsed 1m 18s (remain 91m 31s) Loss: 0.0001(0.0017) Grad: 282.0364  LR: 0.000013  \n",
      "Epoch: [3][200/7150] Elapsed 2m 34s (remain 89m 1s) Loss: 0.0001(0.0013) Grad: 289.2612  LR: 0.000013  \n",
      "Epoch: [3][300/7150] Elapsed 3m 50s (remain 87m 19s) Loss: 0.0000(0.0012) Grad: 28.6055  LR: 0.000013  \n",
      "Epoch: [3][400/7150] Elapsed 5m 6s (remain 86m 1s) Loss: 0.0039(0.0012) Grad: 12993.1133  LR: 0.000013  \n",
      "Epoch: [3][500/7150] Elapsed 6m 22s (remain 84m 41s) Loss: 0.0000(0.0012) Grad: 49.4626  LR: 0.000013  \n",
      "Epoch: [3][600/7150] Elapsed 7m 39s (remain 83m 22s) Loss: 0.0000(0.0011) Grad: 13.9870  LR: 0.000013  \n",
      "Epoch: [3][700/7150] Elapsed 8m 54s (remain 82m 0s) Loss: 0.0001(0.0011) Grad: 375.1568  LR: 0.000013  \n",
      "Epoch: [3][800/7150] Elapsed 10m 10s (remain 80m 37s) Loss: 0.0000(0.0012) Grad: 70.8955  LR: 0.000013  \n",
      "Epoch: [3][900/7150] Elapsed 11m 25s (remain 79m 17s) Loss: 0.0003(0.0012) Grad: 2265.7417  LR: 0.000013  \n",
      "Epoch: [3][1000/7150] Elapsed 12m 42s (remain 78m 1s) Loss: 0.0001(0.0011) Grad: 1152.7610  LR: 0.000013  \n",
      "Epoch: [3][1100/7150] Elapsed 13m 59s (remain 76m 50s) Loss: 0.0015(0.0012) Grad: 3793.8999  LR: 0.000013  \n",
      "Epoch: [3][1200/7150] Elapsed 15m 19s (remain 75m 53s) Loss: 0.0000(0.0012) Grad: 102.1385  LR: 0.000013  \n",
      "Epoch: [3][1300/7150] Elapsed 16m 35s (remain 74m 34s) Loss: 0.0019(0.0012) Grad: 9673.8301  LR: 0.000013  \n",
      "Epoch: [3][1400/7150] Elapsed 17m 50s (remain 73m 14s) Loss: 0.0013(0.0011) Grad: 5209.8208  LR: 0.000012  \n",
      "Epoch: [3][1500/7150] Elapsed 19m 7s (remain 71m 59s) Loss: 0.0004(0.0012) Grad: 488.4362  LR: 0.000012  \n",
      "Epoch: [3][1600/7150] Elapsed 20m 23s (remain 70m 40s) Loss: 0.0000(0.0012) Grad: 82.5360  LR: 0.000012  \n",
      "Epoch: [3][1700/7150] Elapsed 21m 38s (remain 69m 21s) Loss: 0.0004(0.0012) Grad: 1102.3293  LR: 0.000012  \n",
      "Epoch: [3][1800/7150] Elapsed 22m 57s (remain 68m 9s) Loss: 0.0001(0.0011) Grad: 146.2595  LR: 0.000012  \n",
      "Epoch: [3][1900/7150] Elapsed 24m 13s (remain 66m 54s) Loss: 0.0001(0.0011) Grad: 130.3061  LR: 0.000012  \n",
      "Epoch: [3][2000/7150] Elapsed 25m 29s (remain 65m 35s) Loss: 0.0075(0.0011) Grad: 4620.4780  LR: 0.000012  \n",
      "Epoch: [3][2100/7150] Elapsed 26m 45s (remain 64m 17s) Loss: 0.0026(0.0011) Grad: 3049.4797  LR: 0.000012  \n",
      "Epoch: [3][2200/7150] Elapsed 28m 0s (remain 62m 58s) Loss: 0.0000(0.0011) Grad: 21.1899  LR: 0.000012  \n",
      "Epoch: [3][2300/7150] Elapsed 29m 18s (remain 61m 45s) Loss: 0.0030(0.0011) Grad: 11969.2129  LR: 0.000012  \n",
      "Epoch: [3][2400/7150] Elapsed 30m 40s (remain 60m 40s) Loss: 0.0032(0.0011) Grad: 15479.0586  LR: 0.000012  \n",
      "Epoch: [3][2500/7150] Elapsed 31m 56s (remain 59m 23s) Loss: 0.0000(0.0011) Grad: 239.2152  LR: 0.000012  \n",
      "Epoch: [3][2600/7150] Elapsed 33m 12s (remain 58m 5s) Loss: 0.0000(0.0011) Grad: 2.4577  LR: 0.000012  \n",
      "Epoch: [3][2700/7150] Elapsed 34m 28s (remain 56m 46s) Loss: 0.0023(0.0011) Grad: 17082.8711  LR: 0.000012  \n",
      "Epoch: [3][2800/7150] Elapsed 35m 42s (remain 55m 26s) Loss: 0.0038(0.0011) Grad: 7258.5234  LR: 0.000012  \n",
      "Epoch: [3][2900/7150] Elapsed 36m 57s (remain 54m 7s) Loss: 0.0001(0.0011) Grad: 3844.1680  LR: 0.000012  \n",
      "Epoch: [3][3000/7150] Elapsed 38m 13s (remain 52m 50s) Loss: 0.0001(0.0011) Grad: 173.8232  LR: 0.000011  \n",
      "Epoch: [3][3100/7150] Elapsed 39m 29s (remain 51m 33s) Loss: 0.0001(0.0011) Grad: 258.5984  LR: 0.000011  \n",
      "Epoch: [3][3200/7150] Elapsed 40m 45s (remain 50m 17s) Loss: 0.0000(0.0012) Grad: 12.6210  LR: 0.000011  \n",
      "Epoch: [3][3300/7150] Elapsed 42m 1s (remain 49m 0s) Loss: 0.0000(0.0012) Grad: 30.7950  LR: 0.000011  \n",
      "Epoch: [3][3400/7150] Elapsed 43m 16s (remain 47m 42s) Loss: 0.0078(0.0012) Grad: 22785.2871  LR: 0.000011  \n",
      "Epoch: [3][3500/7150] Elapsed 44m 37s (remain 46m 30s) Loss: 0.0020(0.0012) Grad: 23775.1523  LR: 0.000011  \n",
      "Epoch: [3][3600/7150] Elapsed 45m 54s (remain 45m 15s) Loss: 0.0007(0.0012) Grad: 1923.9856  LR: 0.000011  \n",
      "Epoch: [3][3700/7150] Elapsed 47m 10s (remain 43m 57s) Loss: 0.0000(0.0011) Grad: 51.8080  LR: 0.000011  \n",
      "Epoch: [3][3800/7150] Elapsed 48m 30s (remain 42m 44s) Loss: 0.0001(0.0012) Grad: 318.4860  LR: 0.000011  \n",
      "Epoch: [3][3900/7150] Elapsed 49m 48s (remain 41m 28s) Loss: 0.0065(0.0012) Grad: 7203.3276  LR: 0.000011  \n",
      "Epoch: [3][4000/7150] Elapsed 51m 2s (remain 40m 10s) Loss: 0.0001(0.0011) Grad: 1004.6409  LR: 0.000011  \n",
      "Epoch: [3][4100/7150] Elapsed 52m 17s (remain 38m 52s) Loss: 0.0066(0.0011) Grad: 20862.5391  LR: 0.000011  \n",
      "Epoch: [3][4200/7150] Elapsed 53m 31s (remain 37m 34s) Loss: 0.0004(0.0012) Grad: 2563.9656  LR: 0.000011  \n",
      "Epoch: [3][4300/7150] Elapsed 54m 46s (remain 36m 17s) Loss: 0.0000(0.0012) Grad: 25.8264  LR: 0.000011  \n",
      "Epoch: [3][4400/7150] Elapsed 56m 2s (remain 35m 0s) Loss: 0.0000(0.0012) Grad: 30.4802  LR: 0.000011  \n",
      "Epoch: [3][4500/7150] Elapsed 57m 16s (remain 33m 42s) Loss: 0.0002(0.0012) Grad: 15532.5586  LR: 0.000011  \n",
      "Epoch: [3][4600/7150] Elapsed 58m 31s (remain 32m 25s) Loss: 0.0029(0.0012) Grad: 14265.5205  LR: 0.000010  \n",
      "Epoch: [3][4700/7150] Elapsed 59m 45s (remain 31m 7s) Loss: 0.0000(0.0012) Grad: 47.6811  LR: 0.000010  \n",
      "Epoch: [3][4800/7150] Elapsed 60m 59s (remain 29m 50s) Loss: 0.0000(0.0012) Grad: 11.4538  LR: 0.000010  \n",
      "Epoch: [3][4900/7150] Elapsed 62m 14s (remain 28m 33s) Loss: 0.0002(0.0012) Grad: 801.4584  LR: 0.000010  \n",
      "Epoch: [3][5000/7150] Elapsed 63m 31s (remain 27m 18s) Loss: 0.0068(0.0012) Grad: 14995.5762  LR: 0.000010  \n",
      "Epoch: [3][5100/7150] Elapsed 64m 46s (remain 26m 1s) Loss: 0.0005(0.0012) Grad: 4803.0728  LR: 0.000010  \n",
      "Epoch: [3][5200/7150] Elapsed 66m 1s (remain 24m 44s) Loss: 0.0001(0.0012) Grad: 447.0939  LR: 0.000010  \n",
      "Epoch: [3][5300/7150] Elapsed 67m 19s (remain 23m 29s) Loss: 0.0000(0.0012) Grad: 232.9035  LR: 0.000010  \n",
      "Epoch: [3][5400/7150] Elapsed 68m 33s (remain 22m 12s) Loss: 0.0000(0.0012) Grad: 30.7719  LR: 0.000010  \n",
      "Epoch: [3][5500/7150] Elapsed 69m 48s (remain 20m 55s) Loss: 0.0030(0.0012) Grad: 15010.8740  LR: 0.000010  \n",
      "Epoch: [3][5600/7150] Elapsed 71m 2s (remain 19m 38s) Loss: 0.0000(0.0012) Grad: 43.2242  LR: 0.000010  \n",
      "Epoch: [3][5700/7150] Elapsed 72m 19s (remain 18m 22s) Loss: 0.0008(0.0012) Grad: 9207.9863  LR: 0.000010  \n",
      "Epoch: [3][5800/7150] Elapsed 73m 33s (remain 17m 6s) Loss: 0.0000(0.0012) Grad: 223.2743  LR: 0.000010  \n",
      "Epoch: [3][5900/7150] Elapsed 74m 48s (remain 15m 50s) Loss: 0.0000(0.0012) Grad: 203.2465  LR: 0.000010  \n",
      "Epoch: [3][6000/7150] Elapsed 76m 4s (remain 14m 33s) Loss: 0.0008(0.0012) Grad: 2306.6887  LR: 0.000010  \n",
      "Epoch: [3][6100/7150] Elapsed 77m 18s (remain 13m 17s) Loss: 0.0000(0.0012) Grad: 256.9313  LR: 0.000010  \n",
      "Epoch: [3][6200/7150] Elapsed 78m 33s (remain 12m 1s) Loss: 0.0000(0.0012) Grad: 29.2896  LR: 0.000009  \n",
      "Epoch: [3][6300/7150] Elapsed 79m 53s (remain 10m 45s) Loss: 0.0027(0.0012) Grad: 25001.6914  LR: 0.000009  \n",
      "Epoch: [3][6400/7150] Elapsed 81m 15s (remain 9m 30s) Loss: 0.0010(0.0012) Grad: 19849.7227  LR: 0.000009  \n",
      "Epoch: [3][6500/7150] Elapsed 82m 37s (remain 8m 14s) Loss: 0.0000(0.0012) Grad: 51.0892  LR: 0.000009  \n",
      "Epoch: [3][6600/7150] Elapsed 83m 58s (remain 6m 59s) Loss: 0.0033(0.0011) Grad: 10502.3887  LR: 0.000009  \n",
      "Epoch: [3][6700/7150] Elapsed 85m 20s (remain 5m 43s) Loss: 0.0000(0.0011) Grad: 157.7623  LR: 0.000009  \n",
      "Epoch: [3][6800/7150] Elapsed 86m 41s (remain 4m 26s) Loss: 0.0000(0.0011) Grad: 90.1247  LR: 0.000009  \n",
      "Epoch: [3][6900/7150] Elapsed 88m 3s (remain 3m 10s) Loss: 0.0000(0.0011) Grad: 41.7284  LR: 0.000009  \n",
      "Epoch: [3][7000/7150] Elapsed 89m 25s (remain 1m 54s) Loss: 0.0038(0.0011) Grad: 30201.2812  LR: 0.000009  \n",
      "Epoch: [3][7100/7150] Elapsed 90m 46s (remain 0m 37s) Loss: 0.0001(0.0011) Grad: 339.0793  LR: 0.000009  \n",
      "Epoch: [3][7149/7150] Elapsed 91m 26s (remain 0m 0s) Loss: 0.0000(0.0011) Grad: 928.4102  LR: 0.000009  \n",
      "EVAL: [0/1192] Elapsed 0m 1s (remain 24m 43s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 30s (remain 5m 26s) Loss: 0.0001(0.0022) \n",
      "EVAL: [200/1192] Elapsed 1m 1s (remain 5m 1s) Loss: 0.0000(0.0028) \n",
      "EVAL: [300/1192] Elapsed 1m 32s (remain 4m 32s) Loss: 0.0005(0.0042) \n",
      "EVAL: [400/1192] Elapsed 2m 1s (remain 3m 59s) Loss: 0.0122(0.0044) \n",
      "EVAL: [500/1192] Elapsed 2m 31s (remain 3m 28s) Loss: 0.0135(0.0040) \n",
      "EVAL: [600/1192] Elapsed 3m 1s (remain 2m 58s) Loss: 0.0629(0.0041) \n",
      "EVAL: [700/1192] Elapsed 3m 34s (remain 2m 30s) Loss: 0.0023(0.0047) \n",
      "EVAL: [800/1192] Elapsed 4m 4s (remain 1m 59s) Loss: 0.0007(0.0046) \n",
      "EVAL: [900/1192] Elapsed 4m 34s (remain 1m 28s) Loss: 0.0002(0.0045) \n",
      "EVAL: [1000/1192] Elapsed 5m 9s (remain 0m 59s) Loss: 0.0000(0.0043) \n",
      "EVAL: [1100/1192] Elapsed 5m 40s (remain 0m 28s) Loss: 0.0025(0.0042) \n",
      "EVAL: [1191/1192] Elapsed 6m 9s (remain 0m 0s) Loss: 0.0038(0.0039) \n",
      "Epoch 3 - avg_train_loss: 0.0011  avg_val_loss: 0.0039  time: 5859s\n",
      "Epoch 3 - Score: 0.8830\n",
      "Epoch 3 - Save Best Score: 0.8830 Model\n",
      "Epoch: [4][0/7150] Elapsed 0m 1s (remain 206m 44s) Loss: 0.0002(0.0002) Grad: 1480.9564  LR: 0.000009  \n",
      "Epoch: [4][100/7150] Elapsed 1m 22s (remain 96m 2s) Loss: 0.0001(0.0008) Grad: 4727.8848  LR: 0.000009  \n",
      "Epoch: [4][200/7150] Elapsed 2m 44s (remain 94m 53s) Loss: 0.0000(0.0009) Grad: 14.2269  LR: 0.000009  \n",
      "Epoch: [4][300/7150] Elapsed 4m 0s (remain 91m 4s) Loss: 0.0034(0.0010) Grad: 15633.8389  LR: 0.000009  \n",
      "Epoch: [4][400/7150] Elapsed 5m 14s (remain 88m 9s) Loss: 0.0000(0.0011) Grad: 3.7216  LR: 0.000009  \n",
      "Epoch: [4][500/7150] Elapsed 6m 30s (remain 86m 17s) Loss: 0.0000(0.0010) Grad: 344.3742  LR: 0.000009  \n",
      "Epoch: [4][600/7150] Elapsed 7m 43s (remain 84m 12s) Loss: 0.0001(0.0009) Grad: 600.4501  LR: 0.000009  \n",
      "Epoch: [4][700/7150] Elapsed 8m 57s (remain 82m 25s) Loss: 0.0000(0.0009) Grad: 9.2479  LR: 0.000008  \n",
      "Epoch: [4][800/7150] Elapsed 10m 11s (remain 80m 48s) Loss: 0.0001(0.0009) Grad: 486.7492  LR: 0.000008  \n",
      "Epoch: [4][900/7150] Elapsed 11m 25s (remain 79m 14s) Loss: 0.0007(0.0009) Grad: 36212.5000  LR: 0.000008  \n",
      "Epoch: [4][1000/7150] Elapsed 12m 45s (remain 78m 25s) Loss: 0.0000(0.0009) Grad: 1678.4967  LR: 0.000008  \n",
      "Epoch: [4][1100/7150] Elapsed 14m 1s (remain 77m 2s) Loss: 0.0000(0.0009) Grad: 61.5022  LR: 0.000008  \n",
      "Epoch: [4][1200/7150] Elapsed 15m 15s (remain 75m 33s) Loss: 0.0005(0.0009) Grad: 4063.7554  LR: 0.000008  \n",
      "Epoch: [4][1300/7150] Elapsed 16m 33s (remain 74m 26s) Loss: 0.0000(0.0008) Grad: 12.8754  LR: 0.000008  \n",
      "Epoch: [4][1400/7150] Elapsed 17m 47s (remain 73m 1s) Loss: 0.0003(0.0008) Grad: 1078.8988  LR: 0.000008  \n",
      "Epoch: [4][1500/7150] Elapsed 19m 2s (remain 71m 38s) Loss: 0.0020(0.0008) Grad: 6650.8921  LR: 0.000008  \n",
      "Epoch: [4][1600/7150] Elapsed 20m 18s (remain 70m 22s) Loss: 0.0001(0.0009) Grad: 376.1975  LR: 0.000008  \n",
      "Epoch: [4][1700/7150] Elapsed 21m 32s (remain 68m 58s) Loss: 0.0000(0.0009) Grad: 95.9721  LR: 0.000008  \n",
      "Epoch: [4][1800/7150] Elapsed 22m 45s (remain 67m 35s) Loss: 0.0000(0.0009) Grad: 29.1525  LR: 0.000008  \n",
      "Epoch: [4][1900/7150] Elapsed 24m 0s (remain 66m 16s) Loss: 0.0000(0.0009) Grad: 39.4176  LR: 0.000008  \n",
      "Epoch: [4][2000/7150] Elapsed 25m 14s (remain 64m 57s) Loss: 0.0024(0.0009) Grad: 13733.4541  LR: 0.000008  \n",
      "Epoch: [4][2100/7150] Elapsed 26m 28s (remain 63m 36s) Loss: 0.0001(0.0009) Grad: 650.1035  LR: 0.000008  \n",
      "Epoch: [4][2200/7150] Elapsed 27m 43s (remain 62m 20s) Loss: 0.0001(0.0009) Grad: 231.1081  LR: 0.000008  \n",
      "Epoch: [4][2300/7150] Elapsed 28m 59s (remain 61m 5s) Loss: 0.0000(0.0009) Grad: 213.2412  LR: 0.000007  \n",
      "Epoch: [4][2400/7150] Elapsed 30m 14s (remain 59m 48s) Loss: 0.0001(0.0009) Grad: 276.4814  LR: 0.000007  \n",
      "Epoch: [4][2500/7150] Elapsed 31m 28s (remain 58m 29s) Loss: 0.0011(0.0009) Grad: 3366.5039  LR: 0.000007  \n",
      "Epoch: [4][2600/7150] Elapsed 32m 42s (remain 57m 12s) Loss: 0.0000(0.0009) Grad: 96.8288  LR: 0.000007  \n",
      "Epoch: [4][2700/7150] Elapsed 34m 1s (remain 56m 2s) Loss: 0.0000(0.0009) Grad: 111.6211  LR: 0.000007  \n",
      "Epoch: [4][2800/7150] Elapsed 35m 16s (remain 54m 45s) Loss: 0.0003(0.0009) Grad: 467.0732  LR: 0.000007  \n",
      "Epoch: [4][2900/7150] Elapsed 36m 30s (remain 53m 29s) Loss: 0.0000(0.0009) Grad: 205.3631  LR: 0.000007  \n",
      "Epoch: [4][3000/7150] Elapsed 37m 45s (remain 52m 11s) Loss: 0.0000(0.0009) Grad: 125.5368  LR: 0.000007  \n",
      "Epoch: [4][3100/7150] Elapsed 38m 59s (remain 50m 54s) Loss: 0.0003(0.0009) Grad: 578.5416  LR: 0.000007  \n",
      "Epoch: [4][3200/7150] Elapsed 40m 13s (remain 49m 36s) Loss: 0.0016(0.0009) Grad: 7129.3955  LR: 0.000007  \n",
      "Epoch: [4][3300/7150] Elapsed 41m 32s (remain 48m 25s) Loss: 0.0016(0.0009) Grad: 3965.1516  LR: 0.000007  \n",
      "Epoch: [4][3400/7150] Elapsed 42m 47s (remain 47m 9s) Loss: 0.0000(0.0009) Grad: 4.0030  LR: 0.000007  \n",
      "Epoch: [4][3500/7150] Elapsed 44m 2s (remain 45m 53s) Loss: 0.0000(0.0009) Grad: 57.9175  LR: 0.000007  \n",
      "Epoch: [4][3600/7150] Elapsed 45m 17s (remain 44m 38s) Loss: 0.0002(0.0009) Grad: 868.7490  LR: 0.000007  \n",
      "Epoch: [4][3700/7150] Elapsed 46m 32s (remain 43m 22s) Loss: 0.0003(0.0009) Grad: 3416.0303  LR: 0.000007  \n",
      "Epoch: [4][3800/7150] Elapsed 47m 47s (remain 42m 6s) Loss: 0.0003(0.0009) Grad: 2332.9675  LR: 0.000007  \n",
      "Epoch: [4][3900/7150] Elapsed 49m 3s (remain 40m 51s) Loss: 0.0004(0.0009) Grad: 549.8716  LR: 0.000006  \n",
      "Epoch: [4][4000/7150] Elapsed 50m 19s (remain 39m 36s) Loss: 0.0000(0.0009) Grad: 132.3261  LR: 0.000006  \n",
      "Epoch: [4][4100/7150] Elapsed 51m 35s (remain 38m 21s) Loss: 0.0000(0.0009) Grad: 112.7655  LR: 0.000006  \n",
      "Epoch: [4][4200/7150] Elapsed 52m 52s (remain 37m 6s) Loss: 0.0000(0.0009) Grad: 420.7239  LR: 0.000006  \n",
      "Epoch: [4][4300/7150] Elapsed 54m 9s (remain 35m 52s) Loss: 0.0003(0.0009) Grad: 2304.2056  LR: 0.000006  \n",
      "Epoch: [4][4400/7150] Elapsed 55m 25s (remain 34m 37s) Loss: 0.0001(0.0009) Grad: 403.6270  LR: 0.000006  \n",
      "Epoch: [4][4500/7150] Elapsed 56m 42s (remain 33m 22s) Loss: 0.0004(0.0009) Grad: 3162.6992  LR: 0.000006  \n",
      "Epoch: [4][4600/7150] Elapsed 58m 4s (remain 32m 10s) Loss: 0.0000(0.0009) Grad: 219.4426  LR: 0.000006  \n",
      "Epoch: [4][4700/7150] Elapsed 59m 20s (remain 30m 54s) Loss: 0.0006(0.0009) Grad: 4523.2378  LR: 0.000006  \n",
      "Epoch: [4][4800/7150] Elapsed 60m 35s (remain 29m 38s) Loss: 0.0000(0.0009) Grad: 459.4368  LR: 0.000006  \n",
      "Epoch: [4][4900/7150] Elapsed 61m 51s (remain 28m 23s) Loss: 0.0007(0.0009) Grad: 10635.3389  LR: 0.000006  \n",
      "Epoch: [4][5000/7150] Elapsed 63m 7s (remain 27m 7s) Loss: 0.0003(0.0009) Grad: 4799.5229  LR: 0.000006  \n",
      "Epoch: [4][5100/7150] Elapsed 64m 23s (remain 25m 52s) Loss: 0.0001(0.0009) Grad: 314.0595  LR: 0.000006  \n",
      "Epoch: [4][5200/7150] Elapsed 65m 38s (remain 24m 35s) Loss: 0.0001(0.0009) Grad: 627.3939  LR: 0.000006  \n",
      "Epoch: [4][5300/7150] Elapsed 66m 52s (remain 23m 19s) Loss: 0.0000(0.0009) Grad: 358.5573  LR: 0.000006  \n",
      "Epoch: [4][5400/7150] Elapsed 68m 9s (remain 22m 4s) Loss: 0.0000(0.0009) Grad: 423.7345  LR: 0.000006  \n",
      "Epoch: [4][5500/7150] Elapsed 69m 23s (remain 20m 48s) Loss: 0.0000(0.0009) Grad: 119.4960  LR: 0.000005  \n",
      "Epoch: [4][5600/7150] Elapsed 70m 37s (remain 19m 31s) Loss: 0.0001(0.0009) Grad: 2337.3120  LR: 0.000005  \n",
      "Epoch: [4][5700/7150] Elapsed 71m 51s (remain 18m 15s) Loss: 0.0000(0.0009) Grad: 16.8593  LR: 0.000005  \n",
      "Epoch: [4][5800/7150] Elapsed 73m 10s (remain 17m 0s) Loss: 0.0001(0.0009) Grad: 522.0297  LR: 0.000005  \n",
      "Epoch: [4][5900/7150] Elapsed 74m 24s (remain 15m 44s) Loss: 0.0032(0.0009) Grad: 6311.1943  LR: 0.000005  \n",
      "Epoch: [4][6000/7150] Elapsed 75m 38s (remain 14m 29s) Loss: 0.0000(0.0009) Grad: 167.7605  LR: 0.000005  \n",
      "Epoch: [4][6100/7150] Elapsed 76m 54s (remain 13m 13s) Loss: 0.0017(0.0009) Grad: 12509.2207  LR: 0.000005  \n",
      "Epoch: [4][6200/7150] Elapsed 78m 11s (remain 11m 58s) Loss: 0.0000(0.0009) Grad: 87.7039  LR: 0.000005  \n",
      "Epoch: [4][6300/7150] Elapsed 79m 25s (remain 10m 42s) Loss: 0.0019(0.0009) Grad: 10466.8066  LR: 0.000005  \n",
      "Epoch: [4][6400/7150] Elapsed 80m 39s (remain 9m 26s) Loss: 0.0006(0.0009) Grad: 1918.9829  LR: 0.000005  \n",
      "Epoch: [4][6500/7150] Elapsed 81m 57s (remain 8m 10s) Loss: 0.0000(0.0009) Grad: 830.5895  LR: 0.000005  \n",
      "Epoch: [4][6600/7150] Elapsed 83m 11s (remain 6m 55s) Loss: 0.0000(0.0009) Grad: 23.9483  LR: 0.000005  \n",
      "Epoch: [4][6700/7150] Elapsed 84m 26s (remain 5m 39s) Loss: 0.0001(0.0009) Grad: 2420.0625  LR: 0.000005  \n",
      "Epoch: [4][6800/7150] Elapsed 85m 41s (remain 4m 23s) Loss: 0.0001(0.0009) Grad: 1509.4636  LR: 0.000005  \n",
      "Epoch: [4][6900/7150] Elapsed 86m 55s (remain 3m 8s) Loss: 0.0000(0.0009) Grad: 234.4754  LR: 0.000005  \n",
      "Epoch: [4][7000/7150] Elapsed 88m 9s (remain 1m 52s) Loss: 0.0000(0.0009) Grad: 39.8574  LR: 0.000005  \n",
      "Epoch: [4][7100/7150] Elapsed 89m 26s (remain 0m 37s) Loss: 0.0001(0.0009) Grad: 939.1331  LR: 0.000004  \n",
      "Epoch: [4][7149/7150] Elapsed 90m 5s (remain 0m 0s) Loss: 0.0002(0.0009) Grad: 3190.9792  LR: 0.000004  \n",
      "EVAL: [0/1192] Elapsed 0m 1s (remain 24m 36s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 31s (remain 5m 35s) Loss: 0.0001(0.0020) \n",
      "EVAL: [200/1192] Elapsed 1m 1s (remain 5m 1s) Loss: 0.0000(0.0025) \n",
      "EVAL: [300/1192] Elapsed 1m 31s (remain 4m 31s) Loss: 0.0004(0.0040) \n",
      "EVAL: [400/1192] Elapsed 2m 0s (remain 3m 58s) Loss: 0.0120(0.0041) \n",
      "EVAL: [500/1192] Elapsed 2m 34s (remain 3m 32s) Loss: 0.0133(0.0039) \n",
      "EVAL: [600/1192] Elapsed 3m 7s (remain 3m 4s) Loss: 0.0597(0.0039) \n",
      "EVAL: [700/1192] Elapsed 3m 36s (remain 2m 31s) Loss: 0.0022(0.0045) \n",
      "EVAL: [800/1192] Elapsed 4m 8s (remain 2m 1s) Loss: 0.0015(0.0044) \n",
      "EVAL: [900/1192] Elapsed 4m 38s (remain 1m 29s) Loss: 0.0002(0.0042) \n",
      "EVAL: [1000/1192] Elapsed 5m 8s (remain 0m 58s) Loss: 0.0000(0.0041) \n",
      "EVAL: [1100/1192] Elapsed 5m 39s (remain 0m 28s) Loss: 0.0023(0.0039) \n",
      "EVAL: [1191/1192] Elapsed 6m 6s (remain 0m 0s) Loss: 0.0033(0.0037) \n",
      "Epoch 4 - avg_train_loss: 0.0009  avg_val_loss: 0.0037  time: 5775s\n",
      "Epoch 4 - Score: 0.8841\n",
      "Epoch 4 - Save Best Score: 0.8841 Model\n",
      "Epoch: [5][0/7150] Elapsed 0m 1s (remain 183m 0s) Loss: 0.0036(0.0036) Grad: 6094.3398  LR: 0.000004  \n",
      "Epoch: [5][100/7150] Elapsed 1m 16s (remain 89m 32s) Loss: 0.0000(0.0006) Grad: 6.4488  LR: 0.000004  \n",
      "Epoch: [5][200/7150] Elapsed 2m 33s (remain 88m 16s) Loss: 0.0000(0.0006) Grad: 40.7364  LR: 0.000004  \n",
      "Epoch: [5][300/7150] Elapsed 3m 48s (remain 86m 35s) Loss: 0.0007(0.0006) Grad: 12920.6895  LR: 0.000004  \n",
      "Epoch: [5][400/7150] Elapsed 5m 2s (remain 84m 47s) Loss: 0.0018(0.0006) Grad: 2954.6289  LR: 0.000004  \n",
      "Epoch: [5][500/7150] Elapsed 6m 16s (remain 83m 10s) Loss: 0.0005(0.0006) Grad: 1749.5647  LR: 0.000004  \n",
      "Epoch: [5][600/7150] Elapsed 7m 34s (remain 82m 30s) Loss: 0.0009(0.0006) Grad: 2147.7236  LR: 0.000004  \n",
      "Epoch: [5][700/7150] Elapsed 8m 49s (remain 81m 9s) Loss: 0.0078(0.0006) Grad: 10016.7520  LR: 0.000004  \n",
      "Epoch: [5][800/7150] Elapsed 10m 3s (remain 79m 44s) Loss: 0.0001(0.0006) Grad: 734.8544  LR: 0.000004  \n",
      "Epoch: [5][900/7150] Elapsed 11m 18s (remain 78m 23s) Loss: 0.0000(0.0006) Grad: 38.1078  LR: 0.000004  \n",
      "Epoch: [5][1000/7150] Elapsed 12m 35s (remain 77m 23s) Loss: 0.0000(0.0006) Grad: 163.7114  LR: 0.000004  \n",
      "Epoch: [5][1100/7150] Elapsed 13m 52s (remain 76m 15s) Loss: 0.0018(0.0007) Grad: 9699.5947  LR: 0.000004  \n",
      "Epoch: [5][1200/7150] Elapsed 15m 7s (remain 74m 54s) Loss: 0.0001(0.0007) Grad: 464.5415  LR: 0.000004  \n",
      "Epoch: [5][1300/7150] Elapsed 16m 21s (remain 73m 34s) Loss: 0.0003(0.0007) Grad: 1443.7185  LR: 0.000004  \n",
      "Epoch: [5][1400/7150] Elapsed 17m 35s (remain 72m 10s) Loss: 0.0000(0.0006) Grad: 21.6006  LR: 0.000004  \n",
      "Epoch: [5][1500/7150] Elapsed 18m 49s (remain 70m 50s) Loss: 0.0000(0.0006) Grad: 14.1267  LR: 0.000004  \n",
      "Epoch: [5][1600/7150] Elapsed 20m 5s (remain 69m 37s) Loss: 0.0004(0.0006) Grad: 6023.2246  LR: 0.000003  \n",
      "Epoch: [5][1700/7150] Elapsed 21m 21s (remain 68m 25s) Loss: 0.0000(0.0006) Grad: 269.7635  LR: 0.000003  \n",
      "Epoch: [5][1800/7150] Elapsed 22m 37s (remain 67m 12s) Loss: 0.0035(0.0006) Grad: 19177.8828  LR: 0.000003  \n",
      "Epoch: [5][1900/7150] Elapsed 23m 52s (remain 65m 55s) Loss: 0.0000(0.0006) Grad: 39.5041  LR: 0.000003  \n",
      "Epoch: [5][2000/7150] Elapsed 25m 6s (remain 64m 37s) Loss: 0.0005(0.0006) Grad: 13545.5918  LR: 0.000003  \n",
      "Epoch: [5][2100/7150] Elapsed 26m 23s (remain 63m 25s) Loss: 0.0008(0.0006) Grad: 4246.5737  LR: 0.000003  \n",
      "Epoch: [5][2200/7150] Elapsed 27m 37s (remain 62m 7s) Loss: 0.0003(0.0006) Grad: 2384.0984  LR: 0.000003  \n",
      "Epoch: [5][2300/7150] Elapsed 28m 51s (remain 60m 48s) Loss: 0.0001(0.0006) Grad: 675.2661  LR: 0.000003  \n",
      "Epoch: [5][2400/7150] Elapsed 30m 5s (remain 59m 30s) Loss: 0.0007(0.0006) Grad: 3264.2957  LR: 0.000003  \n",
      "Epoch: [5][2500/7150] Elapsed 31m 20s (remain 58m 15s) Loss: 0.0000(0.0006) Grad: 51.4511  LR: 0.000003  \n",
      "Epoch: [5][2600/7150] Elapsed 32m 36s (remain 57m 1s) Loss: 0.0004(0.0006) Grad: 1231.5930  LR: 0.000003  \n",
      "Epoch: [5][2700/7150] Elapsed 33m 51s (remain 55m 46s) Loss: 0.0001(0.0006) Grad: 336.9143  LR: 0.000003  \n",
      "Epoch: [5][2800/7150] Elapsed 35m 8s (remain 54m 33s) Loss: 0.0003(0.0006) Grad: 653.9005  LR: 0.000003  \n",
      "Epoch: [5][2900/7150] Elapsed 36m 27s (remain 53m 23s) Loss: 0.0000(0.0006) Grad: 136.0398  LR: 0.000003  \n",
      "Epoch: [5][3000/7150] Elapsed 37m 43s (remain 52m 9s) Loss: 0.0004(0.0006) Grad: 3152.5620  LR: 0.000003  \n",
      "Epoch: [5][3100/7150] Elapsed 38m 59s (remain 50m 54s) Loss: 0.0000(0.0006) Grad: 15.3541  LR: 0.000003  \n",
      "Epoch: [5][3200/7150] Elapsed 40m 14s (remain 49m 39s) Loss: 0.0001(0.0006) Grad: 430.4318  LR: 0.000002  \n",
      "Epoch: [5][3300/7150] Elapsed 41m 32s (remain 48m 26s) Loss: 0.0021(0.0006) Grad: 7648.8604  LR: 0.000002  \n",
      "Epoch: [5][3400/7150] Elapsed 42m 54s (remain 47m 17s) Loss: 0.0002(0.0006) Grad: 1433.8955  LR: 0.000002  \n",
      "Epoch: [5][3500/7150] Elapsed 44m 15s (remain 46m 7s) Loss: 0.0000(0.0006) Grad: 200.3421  LR: 0.000002  \n",
      "Epoch: [5][3600/7150] Elapsed 45m 36s (remain 44m 57s) Loss: 0.0008(0.0006) Grad: 6327.7979  LR: 0.000002  \n",
      "Epoch: [5][3700/7150] Elapsed 46m 57s (remain 43m 45s) Loss: 0.0002(0.0006) Grad: 1037.7435  LR: 0.000002  \n",
      "Epoch: [5][3800/7150] Elapsed 48m 17s (remain 42m 33s) Loss: 0.0000(0.0006) Grad: 9.3208  LR: 0.000002  \n",
      "Epoch: [5][3900/7150] Elapsed 49m 39s (remain 41m 21s) Loss: 0.0002(0.0006) Grad: 512.6813  LR: 0.000002  \n",
      "Epoch: [5][4000/7150] Elapsed 50m 55s (remain 40m 5s) Loss: 0.0009(0.0006) Grad: 76693.4062  LR: 0.000002  \n",
      "Epoch: [5][4100/7150] Elapsed 52m 10s (remain 38m 47s) Loss: 0.0001(0.0006) Grad: 1244.0686  LR: 0.000002  \n",
      "Epoch: [5][4200/7150] Elapsed 53m 25s (remain 37m 29s) Loss: 0.0000(0.0006) Grad: 9.1255  LR: 0.000002  \n",
      "Epoch: [5][4300/7150] Elapsed 54m 38s (remain 36m 11s) Loss: 0.0000(0.0006) Grad: 27.5525  LR: 0.000002  \n",
      "Epoch: [5][4400/7150] Elapsed 55m 54s (remain 34m 55s) Loss: 0.0002(0.0006) Grad: 3457.8625  LR: 0.000002  \n",
      "Epoch: [5][4500/7150] Elapsed 57m 13s (remain 33m 40s) Loss: 0.0000(0.0006) Grad: 18.5422  LR: 0.000002  \n",
      "Epoch: [5][4600/7150] Elapsed 58m 29s (remain 32m 24s) Loss: 0.0000(0.0006) Grad: 47.0637  LR: 0.000002  \n",
      "Epoch: [5][4700/7150] Elapsed 59m 45s (remain 31m 7s) Loss: 0.0020(0.0006) Grad: 44104.8984  LR: 0.000002  \n",
      "Epoch: [5][4800/7150] Elapsed 61m 6s (remain 29m 53s) Loss: 0.0032(0.0007) Grad: 35895.1289  LR: 0.000001  \n",
      "Epoch: [5][4900/7150] Elapsed 62m 24s (remain 28m 38s) Loss: 0.0000(0.0007) Grad: 86.4716  LR: 0.000001  \n",
      "Epoch: [5][5000/7150] Elapsed 63m 40s (remain 27m 21s) Loss: 0.0000(0.0006) Grad: 64.1205  LR: 0.000001  \n",
      "Epoch: [5][5100/7150] Elapsed 64m 52s (remain 26m 3s) Loss: 0.0004(0.0007) Grad: 5243.6514  LR: 0.000001  \n",
      "Epoch: [5][5200/7150] Elapsed 66m 6s (remain 24m 46s) Loss: 0.0000(0.0006) Grad: 4.3529  LR: 0.000001  \n",
      "Epoch: [5][5300/7150] Elapsed 67m 21s (remain 23m 29s) Loss: 0.0135(0.0007) Grad: 40134.8672  LR: 0.000001  \n",
      "Epoch: [5][5400/7150] Elapsed 68m 35s (remain 22m 12s) Loss: 0.0000(0.0007) Grad: 2.5321  LR: 0.000001  \n",
      "Epoch: [5][5500/7150] Elapsed 69m 48s (remain 20m 55s) Loss: 0.0000(0.0007) Grad: 10.0394  LR: 0.000001  \n",
      "Epoch: [5][5600/7150] Elapsed 71m 3s (remain 19m 39s) Loss: 0.0000(0.0007) Grad: 81.0261  LR: 0.000001  \n",
      "Epoch: [5][5700/7150] Elapsed 72m 21s (remain 18m 23s) Loss: 0.0000(0.0007) Grad: 74.0367  LR: 0.000001  \n",
      "Epoch: [5][5800/7150] Elapsed 73m 35s (remain 17m 6s) Loss: 0.0000(0.0007) Grad: 527.8019  LR: 0.000001  \n",
      "Epoch: [5][5900/7150] Elapsed 74m 49s (remain 15m 50s) Loss: 0.0007(0.0007) Grad: 8993.9697  LR: 0.000001  \n",
      "Epoch: [5][6000/7150] Elapsed 76m 3s (remain 14m 33s) Loss: 0.0000(0.0007) Grad: 15.3742  LR: 0.000001  \n",
      "Epoch: [5][6100/7150] Elapsed 77m 18s (remain 13m 17s) Loss: 0.0061(0.0007) Grad: 50051.8203  LR: 0.000001  \n",
      "Epoch: [5][6200/7150] Elapsed 78m 36s (remain 12m 1s) Loss: 0.0000(0.0007) Grad: 86.8151  LR: 0.000001  \n",
      "Epoch: [5][6300/7150] Elapsed 79m 51s (remain 10m 45s) Loss: 0.0000(0.0007) Grad: 407.5960  LR: 0.000001  \n",
      "Epoch: [5][6400/7150] Elapsed 81m 5s (remain 9m 29s) Loss: 0.0001(0.0007) Grad: 810.6917  LR: 0.000000  \n",
      "Epoch: [5][6500/7150] Elapsed 82m 19s (remain 8m 13s) Loss: 0.0000(0.0007) Grad: 305.3640  LR: 0.000000  \n",
      "Epoch: [5][6600/7150] Elapsed 83m 36s (remain 6m 57s) Loss: 0.0001(0.0007) Grad: 525.9987  LR: 0.000000  \n",
      "Epoch: [5][6700/7150] Elapsed 84m 52s (remain 5m 41s) Loss: 0.0000(0.0007) Grad: 16.5742  LR: 0.000000  \n",
      "Epoch: [5][6800/7150] Elapsed 86m 7s (remain 4m 25s) Loss: 0.0001(0.0007) Grad: 834.5640  LR: 0.000000  \n",
      "Epoch: [5][6900/7150] Elapsed 87m 22s (remain 3m 9s) Loss: 0.0000(0.0007) Grad: 118.7730  LR: 0.000000  \n",
      "Epoch: [5][7000/7150] Elapsed 88m 39s (remain 1m 53s) Loss: 0.0003(0.0007) Grad: 1599.5657  LR: 0.000000  \n",
      "Epoch: [5][7100/7150] Elapsed 89m 54s (remain 0m 37s) Loss: 0.0001(0.0007) Grad: 1332.9625  LR: 0.000000  \n",
      "Epoch: [5][7149/7150] Elapsed 90m 31s (remain 0m 0s) Loss: 0.0008(0.0007) Grad: 33462.9570  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 1s (remain 24m 25s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 30s (remain 5m 25s) Loss: 0.0001(0.0022) \n",
      "EVAL: [200/1192] Elapsed 1m 0s (remain 4m 55s) Loss: 0.0000(0.0028) \n",
      "EVAL: [300/1192] Elapsed 1m 29s (remain 4m 23s) Loss: 0.0004(0.0044) \n",
      "EVAL: [400/1192] Elapsed 1m 58s (remain 3m 52s) Loss: 0.0142(0.0045) \n",
      "EVAL: [500/1192] Elapsed 2m 28s (remain 3m 24s) Loss: 0.0136(0.0042) \n",
      "EVAL: [600/1192] Elapsed 2m 56s (remain 2m 54s) Loss: 0.0607(0.0042) \n",
      "EVAL: [700/1192] Elapsed 3m 25s (remain 2m 24s) Loss: 0.0024(0.0049) \n",
      "EVAL: [800/1192] Elapsed 3m 54s (remain 1m 54s) Loss: 0.0018(0.0047) \n",
      "EVAL: [900/1192] Elapsed 4m 23s (remain 1m 25s) Loss: 0.0008(0.0046) \n",
      "EVAL: [1000/1192] Elapsed 4m 53s (remain 0m 56s) Loss: 0.0000(0.0044) \n",
      "EVAL: [1100/1192] Elapsed 5m 23s (remain 0m 26s) Loss: 0.0024(0.0042) \n",
      "EVAL: [1191/1192] Elapsed 5m 50s (remain 0m 0s) Loss: 0.0036(0.0040) \n",
      "Epoch 5 - avg_train_loss: 0.0007  avg_val_loss: 0.0040  time: 5787s\n",
      "Epoch 5 - Score: 0.8845\n",
      "Epoch 5 - Save Best Score: 0.8845 Model\n",
      "========== fold: 2 training ==========\n",
      "get pseudo plain from ../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_2.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_2.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp083/pseudo_labels_2.npy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7feb1ec1cab049f1b1ab76f1ee260b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f002898d29094afc9437735f1f34ff38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(612602, 950)\n",
      "(612602, 6) (612602, 950)\n",
      "(10725, 7)\n",
      "(21450, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'deberta.embeddings.position_embeddings.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\n",
      "Epoch: [1][0/7150] Elapsed 0m 1s (remain 167m 52s) Loss: 0.1085(0.1085) Grad: 100227.8906  LR: 0.000000  \n",
      "Epoch: [1][100/7150] Elapsed 1m 17s (remain 89m 38s) Loss: 0.0870(0.1023) Grad: 83225.4141  LR: 0.000001  \n",
      "Epoch: [1][200/7150] Elapsed 2m 32s (remain 88m 7s) Loss: 0.0393(0.0833) Grad: 43257.6875  LR: 0.000001  \n",
      "Epoch: [1][300/7150] Elapsed 3m 51s (remain 87m 39s) Loss: 0.0138(0.0636) Grad: 5954.5278  LR: 0.000002  \n",
      "Epoch: [1][400/7150] Elapsed 5m 7s (remain 86m 7s) Loss: 0.0048(0.0510) Grad: 3568.6892  LR: 0.000002  \n",
      "Epoch: [1][500/7150] Elapsed 6m 23s (remain 84m 50s) Loss: 0.0100(0.0436) Grad: 3266.0327  LR: 0.000003  \n",
      "Epoch: [1][600/7150] Elapsed 7m 44s (remain 84m 21s) Loss: 0.0123(0.0384) Grad: 4260.0396  LR: 0.000003  \n",
      "Epoch: [1][700/7150] Elapsed 9m 1s (remain 83m 1s) Loss: 0.0090(0.0345) Grad: 3580.9478  LR: 0.000004  \n",
      "Epoch: [1][800/7150] Elapsed 10m 17s (remain 81m 37s) Loss: 0.0145(0.0316) Grad: 4633.8262  LR: 0.000004  \n",
      "Epoch: [1][900/7150] Elapsed 11m 35s (remain 80m 22s) Loss: 0.0047(0.0294) Grad: 5610.6250  LR: 0.000005  \n",
      "Epoch: [1][1000/7150] Elapsed 12m 52s (remain 79m 3s) Loss: 0.0048(0.0275) Grad: 3222.6428  LR: 0.000006  \n",
      "Epoch: [1][1100/7150] Elapsed 14m 10s (remain 77m 50s) Loss: 0.0085(0.0259) Grad: 7404.7603  LR: 0.000006  \n",
      "Epoch: [1][1200/7150] Elapsed 15m 26s (remain 76m 28s) Loss: 0.0017(0.0244) Grad: 2649.7856  LR: 0.000007  \n",
      "Epoch: [1][1300/7150] Elapsed 16m 42s (remain 75m 8s) Loss: 0.0218(0.0229) Grad: 26016.5312  LR: 0.000007  \n",
      "Epoch: [1][1400/7150] Elapsed 17m 59s (remain 73m 48s) Loss: 0.0005(0.0217) Grad: 756.6563  LR: 0.000008  \n",
      "Epoch: [1][1500/7150] Elapsed 19m 18s (remain 72m 41s) Loss: 0.0009(0.0206) Grad: 917.3500  LR: 0.000008  \n",
      "Epoch: [1][1600/7150] Elapsed 20m 37s (remain 71m 27s) Loss: 0.0032(0.0195) Grad: 21567.3770  LR: 0.000009  \n",
      "Epoch: [1][1700/7150] Elapsed 21m 52s (remain 70m 4s) Loss: 0.0013(0.0186) Grad: 1478.0909  LR: 0.000010  \n",
      "Epoch: [1][1800/7150] Elapsed 23m 9s (remain 68m 45s) Loss: 0.0044(0.0177) Grad: 8793.9268  LR: 0.000010  \n",
      "Epoch: [1][1900/7150] Elapsed 24m 28s (remain 67m 35s) Loss: 0.0019(0.0170) Grad: 7991.9780  LR: 0.000011  \n",
      "Epoch: [1][2000/7150] Elapsed 25m 44s (remain 66m 14s) Loss: 0.0007(0.0163) Grad: 1534.8032  LR: 0.000011  \n",
      "Epoch: [1][2100/7150] Elapsed 27m 0s (remain 64m 53s) Loss: 0.0014(0.0157) Grad: 4943.2773  LR: 0.000012  \n",
      "Epoch: [1][2200/7150] Elapsed 28m 18s (remain 63m 38s) Loss: 0.0008(0.0152) Grad: 7505.7544  LR: 0.000012  \n",
      "Epoch: [1][2300/7150] Elapsed 29m 33s (remain 62m 18s) Loss: 0.0044(0.0146) Grad: 15520.1826  LR: 0.000013  \n",
      "Epoch: [1][2400/7150] Elapsed 30m 48s (remain 60m 57s) Loss: 0.0069(0.0142) Grad: 22442.9199  LR: 0.000013  \n",
      "Epoch: [1][2500/7150] Elapsed 32m 5s (remain 59m 39s) Loss: 0.0009(0.0138) Grad: 6272.0024  LR: 0.000014  \n",
      "Epoch: [1][2600/7150] Elapsed 33m 22s (remain 58m 22s) Loss: 0.0003(0.0133) Grad: 377.4329  LR: 0.000015  \n",
      "Epoch: [1][2700/7150] Elapsed 34m 38s (remain 57m 2s) Loss: 0.0117(0.0130) Grad: 14430.8398  LR: 0.000015  \n",
      "Epoch: [1][2800/7150] Elapsed 36m 0s (remain 55m 54s) Loss: 0.0008(0.0126) Grad: 2177.3933  LR: 0.000016  \n",
      "Epoch: [1][2900/7150] Elapsed 37m 18s (remain 54m 38s) Loss: 0.0127(0.0123) Grad: 24429.7031  LR: 0.000016  \n",
      "Epoch: [1][3000/7150] Elapsed 38m 33s (remain 53m 18s) Loss: 0.0001(0.0120) Grad: 184.0644  LR: 0.000017  \n",
      "Epoch: [1][3100/7150] Elapsed 39m 49s (remain 52m 0s) Loss: 0.0028(0.0117) Grad: 9579.7031  LR: 0.000017  \n",
      "Epoch: [1][3200/7150] Elapsed 41m 6s (remain 50m 43s) Loss: 0.0008(0.0114) Grad: 2083.3062  LR: 0.000018  \n",
      "Epoch: [1][3300/7150] Elapsed 42m 22s (remain 49m 24s) Loss: 0.0003(0.0112) Grad: 1071.2162  LR: 0.000018  \n",
      "Epoch: [1][3400/7150] Elapsed 43m 37s (remain 48m 5s) Loss: 0.0001(0.0109) Grad: 107.9481  LR: 0.000019  \n",
      "Epoch: [1][3500/7150] Elapsed 44m 53s (remain 46m 46s) Loss: 0.0012(0.0107) Grad: 3090.9167  LR: 0.000020  \n",
      "Epoch: [1][3600/7150] Elapsed 46m 10s (remain 45m 30s) Loss: 0.0004(0.0104) Grad: 470.4074  LR: 0.000020  \n",
      "Epoch: [1][3700/7150] Elapsed 47m 25s (remain 44m 11s) Loss: 0.0084(0.0102) Grad: 6926.8643  LR: 0.000020  \n",
      "Epoch: [1][3800/7150] Elapsed 48m 39s (remain 42m 52s) Loss: 0.0001(0.0100) Grad: 113.7971  LR: 0.000020  \n",
      "Epoch: [1][3900/7150] Elapsed 49m 54s (remain 41m 33s) Loss: 0.0002(0.0098) Grad: 179.4540  LR: 0.000020  \n",
      "Epoch: [1][4000/7150] Elapsed 51m 10s (remain 40m 16s) Loss: 0.0020(0.0096) Grad: 8147.0869  LR: 0.000020  \n",
      "Epoch: [1][4100/7150] Elapsed 52m 26s (remain 38m 59s) Loss: 0.0003(0.0095) Grad: 3287.7671  LR: 0.000020  \n",
      "Epoch: [1][4200/7150] Elapsed 53m 41s (remain 37m 41s) Loss: 0.0058(0.0093) Grad: 39333.5312  LR: 0.000020  \n",
      "Epoch: [1][4300/7150] Elapsed 54m 56s (remain 36m 23s) Loss: 0.0006(0.0092) Grad: 23221.3164  LR: 0.000020  \n",
      "Epoch: [1][4400/7150] Elapsed 56m 12s (remain 35m 6s) Loss: 0.0003(0.0090) Grad: 1018.2849  LR: 0.000019  \n",
      "Epoch: [1][4500/7150] Elapsed 57m 28s (remain 33m 49s) Loss: 0.0028(0.0089) Grad: 10559.4307  LR: 0.000019  \n",
      "Epoch: [1][4600/7150] Elapsed 58m 42s (remain 32m 31s) Loss: 0.0002(0.0087) Grad: 1082.7012  LR: 0.000019  \n",
      "Epoch: [1][4700/7150] Elapsed 59m 57s (remain 31m 14s) Loss: 0.0029(0.0086) Grad: 8072.3022  LR: 0.000019  \n",
      "Epoch: [1][4800/7150] Elapsed 61m 12s (remain 29m 56s) Loss: 0.0009(0.0085) Grad: 9901.6289  LR: 0.000019  \n",
      "Epoch: [1][4900/7150] Elapsed 62m 29s (remain 28m 40s) Loss: 0.0053(0.0083) Grad: 48656.5000  LR: 0.000019  \n",
      "Epoch: [1][5000/7150] Elapsed 63m 44s (remain 27m 23s) Loss: 0.0035(0.0082) Grad: 21626.8008  LR: 0.000019  \n",
      "Epoch: [1][5100/7150] Elapsed 64m 59s (remain 26m 6s) Loss: 0.0000(0.0081) Grad: 61.1686  LR: 0.000019  \n",
      "Epoch: [1][5200/7150] Elapsed 66m 13s (remain 24m 49s) Loss: 0.0006(0.0080) Grad: 3468.1130  LR: 0.000019  \n",
      "Epoch: [1][5300/7150] Elapsed 67m 28s (remain 23m 32s) Loss: 0.0056(0.0079) Grad: 64750.8203  LR: 0.000019  \n",
      "Epoch: [1][5400/7150] Elapsed 68m 43s (remain 22m 15s) Loss: 0.0005(0.0078) Grad: 10695.0869  LR: 0.000019  \n",
      "Epoch: [1][5500/7150] Elapsed 70m 2s (remain 20m 59s) Loss: 0.0000(0.0077) Grad: 43.3959  LR: 0.000019  \n",
      "Epoch: [1][5600/7150] Elapsed 71m 16s (remain 19m 42s) Loss: 0.0000(0.0076) Grad: 79.5561  LR: 0.000019  \n",
      "Epoch: [1][5700/7150] Elapsed 72m 31s (remain 18m 26s) Loss: 0.0007(0.0075) Grad: 27503.9238  LR: 0.000019  \n",
      "Epoch: [1][5800/7150] Elapsed 73m 47s (remain 17m 9s) Loss: 0.0033(0.0074) Grad: 4459.3340  LR: 0.000019  \n",
      "Epoch: [1][5900/7150] Elapsed 75m 6s (remain 15m 53s) Loss: 0.0000(0.0073) Grad: 58.8172  LR: 0.000019  \n",
      "Epoch: [1][6000/7150] Elapsed 76m 26s (remain 14m 38s) Loss: 0.0001(0.0072) Grad: 396.7528  LR: 0.000018  \n",
      "Epoch: [1][6100/7150] Elapsed 77m 41s (remain 13m 21s) Loss: 0.0002(0.0071) Grad: 753.6262  LR: 0.000018  \n",
      "Epoch: [1][6200/7150] Elapsed 79m 0s (remain 12m 5s) Loss: 0.0001(0.0071) Grad: 360.3823  LR: 0.000018  \n",
      "Epoch: [1][6300/7150] Elapsed 80m 15s (remain 10m 48s) Loss: 0.0057(0.0070) Grad: 121894.1797  LR: 0.000018  \n",
      "Epoch: [1][6400/7150] Elapsed 81m 31s (remain 9m 32s) Loss: 0.0000(0.0069) Grad: 208.5383  LR: 0.000018  \n",
      "Epoch: [1][6500/7150] Elapsed 82m 49s (remain 8m 16s) Loss: 0.0001(0.0068) Grad: 220.9855  LR: 0.000018  \n",
      "Epoch: [1][6600/7150] Elapsed 84m 4s (remain 6m 59s) Loss: 0.0025(0.0067) Grad: 5965.8291  LR: 0.000018  \n",
      "Epoch: [1][6700/7150] Elapsed 85m 20s (remain 5m 43s) Loss: 0.0000(0.0067) Grad: 90.6297  LR: 0.000018  \n",
      "Epoch: [1][6800/7150] Elapsed 86m 37s (remain 4m 26s) Loss: 0.0006(0.0066) Grad: 1328.9182  LR: 0.000018  \n",
      "Epoch: [1][6900/7150] Elapsed 87m 58s (remain 3m 10s) Loss: 0.0001(0.0065) Grad: 853.9218  LR: 0.000018  \n",
      "Epoch: [1][7000/7150] Elapsed 89m 19s (remain 1m 54s) Loss: 0.0001(0.0065) Grad: 2610.6091  LR: 0.000018  \n",
      "Epoch: [1][7100/7150] Elapsed 90m 42s (remain 0m 37s) Loss: 0.0039(0.0064) Grad: 12473.5586  LR: 0.000018  \n",
      "Epoch: [1][7149/7150] Elapsed 91m 23s (remain 0m 0s) Loss: 0.0008(0.0064) Grad: 8251.8164  LR: 0.000018  \n",
      "EVAL: [0/1192] Elapsed 0m 1s (remain 22m 39s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 34s (remain 6m 16s) Loss: 0.0041(0.0019) \n",
      "EVAL: [200/1192] Elapsed 1m 4s (remain 5m 15s) Loss: 0.0013(0.0022) \n",
      "EVAL: [300/1192] Elapsed 1m 33s (remain 4m 35s) Loss: 0.0023(0.0023) \n",
      "EVAL: [400/1192] Elapsed 2m 3s (remain 4m 3s) Loss: 0.0003(0.0026) \n",
      "EVAL: [500/1192] Elapsed 2m 33s (remain 3m 32s) Loss: 0.0001(0.0024) \n",
      "EVAL: [600/1192] Elapsed 3m 4s (remain 3m 1s) Loss: 0.0005(0.0025) \n",
      "EVAL: [700/1192] Elapsed 3m 33s (remain 2m 29s) Loss: 0.0021(0.0029) \n",
      "EVAL: [800/1192] Elapsed 4m 2s (remain 1m 58s) Loss: 0.0000(0.0028) \n",
      "EVAL: [900/1192] Elapsed 4m 32s (remain 1m 28s) Loss: 0.0004(0.0028) \n",
      "EVAL: [1000/1192] Elapsed 5m 3s (remain 0m 57s) Loss: 0.0015(0.0028) \n",
      "EVAL: [1100/1192] Elapsed 5m 32s (remain 0m 27s) Loss: 0.0043(0.0026) \n",
      "EVAL: [1191/1192] Elapsed 5m 58s (remain 0m 0s) Loss: 0.0001(0.0025) \n",
      "Epoch 1 - avg_train_loss: 0.0064  avg_val_loss: 0.0025  time: 5846s\n",
      "Epoch 1 - Score: 0.8743\n",
      "Epoch 1 - Save Best Score: 0.8743 Model\n",
      "Epoch: [2][0/7150] Elapsed 0m 1s (remain 180m 29s) Loss: 0.0025(0.0025) Grad: 13477.3291  LR: 0.000018  \n",
      "Epoch: [2][100/7150] Elapsed 1m 23s (remain 96m 35s) Loss: 0.0002(0.0013) Grad: 315.0803  LR: 0.000018  \n",
      "Epoch: [2][200/7150] Elapsed 2m 44s (remain 94m 33s) Loss: 0.0001(0.0014) Grad: 48.8629  LR: 0.000018  \n",
      "Epoch: [2][300/7150] Elapsed 4m 6s (remain 93m 20s) Loss: 0.0010(0.0014) Grad: 5096.5186  LR: 0.000018  \n",
      "Epoch: [2][400/7150] Elapsed 5m 20s (remain 89m 49s) Loss: 0.0004(0.0015) Grad: 869.4168  LR: 0.000018  \n",
      "Epoch: [2][500/7150] Elapsed 6m 33s (remain 87m 6s) Loss: 0.0002(0.0015) Grad: 1477.8193  LR: 0.000017  \n",
      "Epoch: [2][600/7150] Elapsed 7m 47s (remain 84m 53s) Loss: 0.0001(0.0016) Grad: 306.8905  LR: 0.000017  \n",
      "Epoch: [2][700/7150] Elapsed 9m 1s (remain 82m 57s) Loss: 0.0001(0.0016) Grad: 125.3726  LR: 0.000017  \n",
      "Epoch: [2][800/7150] Elapsed 10m 15s (remain 81m 19s) Loss: 0.0002(0.0016) Grad: 1043.5734  LR: 0.000017  \n",
      "Epoch: [2][900/7150] Elapsed 11m 34s (remain 80m 16s) Loss: 0.0000(0.0016) Grad: 61.5892  LR: 0.000017  \n",
      "Epoch: [2][1000/7150] Elapsed 12m 49s (remain 78m 45s) Loss: 0.0000(0.0017) Grad: 456.4231  LR: 0.000017  \n",
      "Epoch: [2][1100/7150] Elapsed 14m 4s (remain 77m 17s) Loss: 0.0004(0.0016) Grad: 378.5090  LR: 0.000017  \n",
      "Epoch: [2][1200/7150] Elapsed 15m 21s (remain 76m 5s) Loss: 0.0005(0.0016) Grad: 2811.2458  LR: 0.000017  \n",
      "Epoch: [2][1300/7150] Elapsed 16m 38s (remain 74m 48s) Loss: 0.0000(0.0016) Grad: 91.0156  LR: 0.000017  \n",
      "Epoch: [2][1400/7150] Elapsed 17m 54s (remain 73m 30s) Loss: 0.0001(0.0017) Grad: 251.7350  LR: 0.000017  \n",
      "Epoch: [2][1500/7150] Elapsed 19m 10s (remain 72m 8s) Loss: 0.0002(0.0017) Grad: 902.7136  LR: 0.000017  \n",
      "Epoch: [2][1600/7150] Elapsed 20m 25s (remain 70m 46s) Loss: 0.0084(0.0017) Grad: 20764.3340  LR: 0.000017  \n",
      "Epoch: [2][1700/7150] Elapsed 21m 39s (remain 69m 22s) Loss: 0.0067(0.0017) Grad: 5327.7852  LR: 0.000017  \n",
      "Epoch: [2][1800/7150] Elapsed 22m 55s (remain 68m 5s) Loss: 0.0000(0.0017) Grad: 73.9979  LR: 0.000017  \n",
      "Epoch: [2][1900/7150] Elapsed 24m 11s (remain 66m 48s) Loss: 0.0018(0.0017) Grad: 7941.1855  LR: 0.000017  \n",
      "Epoch: [2][2000/7150] Elapsed 25m 26s (remain 65m 28s) Loss: 0.0004(0.0017) Grad: 1102.6422  LR: 0.000017  \n",
      "Epoch: [2][2100/7150] Elapsed 26m 40s (remain 64m 6s) Loss: 0.0013(0.0017) Grad: 4759.3740  LR: 0.000016  \n",
      "Epoch: [2][2200/7150] Elapsed 27m 56s (remain 62m 49s) Loss: 0.0028(0.0017) Grad: 2973.8286  LR: 0.000016  \n",
      "Epoch: [2][2300/7150] Elapsed 29m 12s (remain 61m 33s) Loss: 0.0075(0.0017) Grad: 6223.7588  LR: 0.000016  \n",
      "Epoch: [2][2400/7150] Elapsed 30m 28s (remain 60m 16s) Loss: 0.0001(0.0016) Grad: 157.9846  LR: 0.000016  \n",
      "Epoch: [2][2500/7150] Elapsed 31m 43s (remain 58m 59s) Loss: 0.0001(0.0017) Grad: 156.8415  LR: 0.000016  \n",
      "Epoch: [2][2600/7150] Elapsed 32m 57s (remain 57m 38s) Loss: 0.0005(0.0017) Grad: 1899.6841  LR: 0.000016  \n",
      "Epoch: [2][2700/7150] Elapsed 34m 11s (remain 56m 18s) Loss: 0.0002(0.0016) Grad: 584.6117  LR: 0.000016  \n",
      "Epoch: [2][2800/7150] Elapsed 35m 30s (remain 55m 8s) Loss: 0.0006(0.0016) Grad: 3198.5874  LR: 0.000016  \n",
      "Epoch: [2][2900/7150] Elapsed 36m 46s (remain 53m 51s) Loss: 0.0001(0.0016) Grad: 343.0647  LR: 0.000016  \n",
      "Epoch: [2][3000/7150] Elapsed 38m 0s (remain 52m 32s) Loss: 0.0067(0.0016) Grad: 18157.1582  LR: 0.000016  \n",
      "Epoch: [2][3100/7150] Elapsed 39m 14s (remain 51m 13s) Loss: 0.0001(0.0016) Grad: 143.8131  LR: 0.000016  \n",
      "Epoch: [2][3200/7150] Elapsed 40m 28s (remain 49m 55s) Loss: 0.0034(0.0016) Grad: 2751.9468  LR: 0.000016  \n",
      "Epoch: [2][3300/7150] Elapsed 41m 42s (remain 48m 37s) Loss: 0.0001(0.0016) Grad: 368.9806  LR: 0.000016  \n",
      "Epoch: [2][3400/7150] Elapsed 42m 57s (remain 47m 20s) Loss: 0.0015(0.0017) Grad: 6739.4404  LR: 0.000016  \n",
      "Epoch: [2][3500/7150] Elapsed 44m 12s (remain 46m 5s) Loss: 0.0054(0.0016) Grad: 14599.4971  LR: 0.000016  \n",
      "Epoch: [2][3600/7150] Elapsed 45m 27s (remain 44m 47s) Loss: 0.0001(0.0016) Grad: 94.4588  LR: 0.000016  \n",
      "Epoch: [2][3700/7150] Elapsed 46m 41s (remain 43m 30s) Loss: 0.0006(0.0016) Grad: 1166.0676  LR: 0.000015  \n",
      "Epoch: [2][3800/7150] Elapsed 47m 55s (remain 42m 13s) Loss: 0.0000(0.0017) Grad: 37.9738  LR: 0.000015  \n",
      "Epoch: [2][3900/7150] Elapsed 49m 9s (remain 40m 56s) Loss: 0.0028(0.0017) Grad: 6112.1099  LR: 0.000015  \n",
      "Epoch: [2][4000/7150] Elapsed 50m 23s (remain 39m 39s) Loss: 0.0001(0.0017) Grad: 937.0002  LR: 0.000015  \n",
      "Epoch: [2][4100/7150] Elapsed 51m 38s (remain 38m 23s) Loss: 0.0130(0.0017) Grad: 67189.1719  LR: 0.000015  \n",
      "Epoch: [2][4200/7150] Elapsed 52m 55s (remain 37m 9s) Loss: 0.0001(0.0016) Grad: 664.3541  LR: 0.000015  \n",
      "Epoch: [2][4300/7150] Elapsed 54m 15s (remain 35m 56s) Loss: 0.0015(0.0016) Grad: 57235.3516  LR: 0.000015  \n",
      "Epoch: [2][4400/7150] Elapsed 55m 31s (remain 34m 40s) Loss: 0.0000(0.0016) Grad: 24.4828  LR: 0.000015  \n",
      "Epoch: [2][4500/7150] Elapsed 56m 46s (remain 33m 24s) Loss: 0.0031(0.0016) Grad: 11006.5215  LR: 0.000015  \n",
      "Epoch: [2][4600/7150] Elapsed 58m 3s (remain 32m 10s) Loss: 0.0004(0.0016) Grad: 2245.7263  LR: 0.000015  \n",
      "Epoch: [2][4700/7150] Elapsed 59m 19s (remain 30m 54s) Loss: 0.0000(0.0016) Grad: 100.8584  LR: 0.000015  \n",
      "Epoch: [2][4800/7150] Elapsed 60m 34s (remain 29m 38s) Loss: 0.0005(0.0016) Grad: 6877.2080  LR: 0.000015  \n",
      "Epoch: [2][4900/7150] Elapsed 61m 50s (remain 28m 22s) Loss: 0.0005(0.0016) Grad: 6617.9980  LR: 0.000015  \n",
      "Epoch: [2][5000/7150] Elapsed 63m 5s (remain 27m 6s) Loss: 0.0022(0.0016) Grad: 7885.5342  LR: 0.000015  \n",
      "Epoch: [2][5100/7150] Elapsed 64m 24s (remain 25m 52s) Loss: 0.0001(0.0016) Grad: 1220.5553  LR: 0.000015  \n",
      "Epoch: [2][5200/7150] Elapsed 65m 40s (remain 24m 36s) Loss: 0.0002(0.0016) Grad: 915.5663  LR: 0.000015  \n",
      "Epoch: [2][5300/7150] Elapsed 66m 56s (remain 23m 20s) Loss: 0.0004(0.0016) Grad: 3452.3621  LR: 0.000014  \n",
      "Epoch: [2][5400/7150] Elapsed 68m 12s (remain 22m 5s) Loss: 0.0107(0.0016) Grad: 43367.7305  LR: 0.000014  \n",
      "Epoch: [2][5500/7150] Elapsed 69m 27s (remain 20m 49s) Loss: 0.0000(0.0016) Grad: 36.3887  LR: 0.000014  \n",
      "Epoch: [2][5600/7150] Elapsed 70m 42s (remain 19m 33s) Loss: 0.0006(0.0016) Grad: 12785.9316  LR: 0.000014  \n",
      "Epoch: [2][5700/7150] Elapsed 71m 57s (remain 18m 17s) Loss: 0.0000(0.0016) Grad: 97.1891  LR: 0.000014  \n",
      "Epoch: [2][5800/7150] Elapsed 73m 12s (remain 17m 1s) Loss: 0.0001(0.0016) Grad: 472.5274  LR: 0.000014  \n",
      "Epoch: [2][5900/7150] Elapsed 74m 28s (remain 15m 45s) Loss: 0.0085(0.0016) Grad: 44811.3945  LR: 0.000014  \n",
      "Epoch: [2][6000/7150] Elapsed 75m 47s (remain 14m 30s) Loss: 0.0000(0.0016) Grad: 48.8510  LR: 0.000014  \n",
      "Epoch: [2][6100/7150] Elapsed 77m 4s (remain 13m 15s) Loss: 0.0002(0.0016) Grad: 6352.8955  LR: 0.000014  \n",
      "Epoch: [2][6200/7150] Elapsed 78m 20s (remain 11m 59s) Loss: 0.0013(0.0016) Grad: 2093.3252  LR: 0.000014  \n",
      "Epoch: [2][6300/7150] Elapsed 79m 36s (remain 10m 43s) Loss: 0.0000(0.0016) Grad: 92.4361  LR: 0.000014  \n",
      "Epoch: [2][6400/7150] Elapsed 80m 51s (remain 9m 27s) Loss: 0.0000(0.0016) Grad: 29.4552  LR: 0.000014  \n",
      "Epoch: [2][6500/7150] Elapsed 82m 12s (remain 8m 12s) Loss: 0.0001(0.0016) Grad: 595.8521  LR: 0.000014  \n",
      "Epoch: [2][6600/7150] Elapsed 83m 27s (remain 6m 56s) Loss: 0.0000(0.0016) Grad: 223.6147  LR: 0.000014  \n",
      "Epoch: [2][6700/7150] Elapsed 84m 42s (remain 5m 40s) Loss: 0.0004(0.0016) Grad: 2686.9099  LR: 0.000014  \n",
      "Epoch: [2][6800/7150] Elapsed 85m 58s (remain 4m 24s) Loss: 0.0000(0.0016) Grad: 104.3598  LR: 0.000014  \n",
      "Epoch: [2][6900/7150] Elapsed 87m 13s (remain 3m 8s) Loss: 0.0001(0.0016) Grad: 622.3401  LR: 0.000013  \n",
      "Epoch: [2][7000/7150] Elapsed 88m 29s (remain 1m 52s) Loss: 0.0065(0.0016) Grad: 12839.2451  LR: 0.000013  \n",
      "Epoch: [2][7100/7150] Elapsed 89m 45s (remain 0m 37s) Loss: 0.0000(0.0016) Grad: 41.1864  LR: 0.000013  \n",
      "Epoch: [2][7149/7150] Elapsed 90m 21s (remain 0m 0s) Loss: 0.0000(0.0016) Grad: 1011.3558  LR: 0.000013  \n",
      "EVAL: [0/1192] Elapsed 0m 1s (remain 22m 6s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 31s (remain 5m 36s) Loss: 0.0107(0.0028) \n",
      "EVAL: [200/1192] Elapsed 1m 0s (remain 4m 58s) Loss: 0.0025(0.0028) \n",
      "EVAL: [300/1192] Elapsed 1m 29s (remain 4m 26s) Loss: 0.0023(0.0027) \n",
      "EVAL: [400/1192] Elapsed 2m 0s (remain 3m 56s) Loss: 0.0000(0.0030) \n",
      "EVAL: [500/1192] Elapsed 2m 30s (remain 3m 27s) Loss: 0.0001(0.0028) \n",
      "EVAL: [600/1192] Elapsed 3m 3s (remain 3m 0s) Loss: 0.0001(0.0029) \n",
      "EVAL: [700/1192] Elapsed 3m 33s (remain 2m 29s) Loss: 0.0026(0.0034) \n",
      "EVAL: [800/1192] Elapsed 4m 2s (remain 1m 58s) Loss: 0.0000(0.0033) \n",
      "EVAL: [900/1192] Elapsed 4m 35s (remain 1m 28s) Loss: 0.0018(0.0033) \n",
      "EVAL: [1000/1192] Elapsed 5m 7s (remain 0m 58s) Loss: 0.0010(0.0032) \n",
      "EVAL: [1100/1192] Elapsed 5m 37s (remain 0m 27s) Loss: 0.0056(0.0031) \n",
      "EVAL: [1191/1192] Elapsed 6m 4s (remain 0m 0s) Loss: 0.0000(0.0030) \n",
      "Epoch 2 - avg_train_loss: 0.0016  avg_val_loss: 0.0030  time: 5791s\n",
      "Epoch 2 - Score: 0.8889\n",
      "Epoch 2 - Save Best Score: 0.8889 Model\n",
      "Epoch: [3][0/7150] Elapsed 0m 1s (remain 198m 20s) Loss: 0.0000(0.0000) Grad: 15.6049  LR: 0.000013  \n",
      "Epoch: [3][100/7150] Elapsed 1m 21s (remain 95m 9s) Loss: 0.0000(0.0009) Grad: 54.1533  LR: 0.000013  \n",
      "Epoch: [3][200/7150] Elapsed 2m 37s (remain 90m 40s) Loss: 0.0002(0.0009) Grad: 224.5711  LR: 0.000013  \n",
      "Epoch: [3][300/7150] Elapsed 3m 52s (remain 88m 13s) Loss: 0.0000(0.0010) Grad: 11.9829  LR: 0.000013  \n",
      "Epoch: [3][400/7150] Elapsed 5m 7s (remain 86m 21s) Loss: 0.0000(0.0009) Grad: 23.1048  LR: 0.000013  \n",
      "Epoch: [3][500/7150] Elapsed 6m 23s (remain 84m 52s) Loss: 0.0002(0.0009) Grad: 820.2715  LR: 0.000013  \n",
      "Epoch: [3][600/7150] Elapsed 7m 41s (remain 83m 47s) Loss: 0.0000(0.0010) Grad: 28.4119  LR: 0.000013  \n",
      "Epoch: [3][700/7150] Elapsed 8m 56s (remain 82m 15s) Loss: 0.0038(0.0010) Grad: 12586.1416  LR: 0.000013  \n",
      "Epoch: [3][800/7150] Elapsed 10m 10s (remain 80m 42s) Loss: 0.0008(0.0010) Grad: 6385.3032  LR: 0.000013  \n",
      "Epoch: [3][900/7150] Elapsed 11m 25s (remain 79m 14s) Loss: 0.0000(0.0010) Grad: 40.9892  LR: 0.000013  \n",
      "Epoch: [3][1000/7150] Elapsed 12m 41s (remain 77m 57s) Loss: 0.0012(0.0010) Grad: 963.6649  LR: 0.000013  \n",
      "Epoch: [3][1100/7150] Elapsed 13m 56s (remain 76m 34s) Loss: 0.0001(0.0010) Grad: 198.0473  LR: 0.000013  \n",
      "Epoch: [3][1200/7150] Elapsed 15m 12s (remain 75m 18s) Loss: 0.0004(0.0011) Grad: 673.1454  LR: 0.000013  \n",
      "Epoch: [3][1300/7150] Elapsed 16m 28s (remain 74m 1s) Loss: 0.0005(0.0011) Grad: 3122.1448  LR: 0.000013  \n",
      "Epoch: [3][1400/7150] Elapsed 17m 43s (remain 72m 45s) Loss: 0.0009(0.0011) Grad: 7084.2261  LR: 0.000012  \n",
      "Epoch: [3][1500/7150] Elapsed 19m 1s (remain 71m 34s) Loss: 0.0061(0.0011) Grad: 5918.8564  LR: 0.000012  \n",
      "Epoch: [3][1600/7150] Elapsed 20m 15s (remain 70m 14s) Loss: 0.0000(0.0011) Grad: 61.6393  LR: 0.000012  \n",
      "Epoch: [3][1700/7150] Elapsed 21m 29s (remain 68m 50s) Loss: 0.0000(0.0011) Grad: 203.0207  LR: 0.000012  \n",
      "Epoch: [3][1800/7150] Elapsed 22m 44s (remain 67m 33s) Loss: 0.0056(0.0011) Grad: 23913.7363  LR: 0.000012  \n",
      "Epoch: [3][1900/7150] Elapsed 24m 5s (remain 66m 32s) Loss: 0.0003(0.0011) Grad: 2610.9685  LR: 0.000012  \n",
      "Epoch: [3][2000/7150] Elapsed 25m 22s (remain 65m 17s) Loss: 0.0016(0.0011) Grad: 5219.1304  LR: 0.000012  \n",
      "Epoch: [3][2100/7150] Elapsed 26m 36s (remain 63m 56s) Loss: 0.0009(0.0011) Grad: 5344.7754  LR: 0.000012  \n",
      "Epoch: [3][2200/7150] Elapsed 27m 50s (remain 62m 36s) Loss: 0.0057(0.0011) Grad: 10101.4395  LR: 0.000012  \n",
      "Epoch: [3][2300/7150] Elapsed 29m 4s (remain 61m 16s) Loss: 0.0000(0.0011) Grad: 49.7225  LR: 0.000012  \n",
      "Epoch: [3][2400/7150] Elapsed 30m 22s (remain 60m 4s) Loss: 0.0000(0.0011) Grad: 15.5123  LR: 0.000012  \n",
      "Epoch: [3][2500/7150] Elapsed 31m 42s (remain 58m 56s) Loss: 0.0002(0.0011) Grad: 1038.0620  LR: 0.000012  \n",
      "Epoch: [3][2600/7150] Elapsed 32m 57s (remain 57m 38s) Loss: 0.0000(0.0011) Grad: 197.4471  LR: 0.000012  \n",
      "Epoch: [3][2700/7150] Elapsed 34m 11s (remain 56m 19s) Loss: 0.0000(0.0011) Grad: 56.9597  LR: 0.000012  \n",
      "Epoch: [3][2800/7150] Elapsed 35m 25s (remain 55m 0s) Loss: 0.0001(0.0011) Grad: 940.0664  LR: 0.000012  \n",
      "Epoch: [3][2900/7150] Elapsed 36m 40s (remain 53m 42s) Loss: 0.0001(0.0011) Grad: 1884.9139  LR: 0.000012  \n",
      "Epoch: [3][3000/7150] Elapsed 37m 54s (remain 52m 24s) Loss: 0.0001(0.0011) Grad: 199.2443  LR: 0.000011  \n",
      "Epoch: [3][3100/7150] Elapsed 39m 8s (remain 51m 6s) Loss: 0.0001(0.0011) Grad: 571.9599  LR: 0.000011  \n",
      "Epoch: [3][3200/7150] Elapsed 40m 23s (remain 49m 49s) Loss: 0.0000(0.0011) Grad: 32.2036  LR: 0.000011  \n",
      "Epoch: [3][3300/7150] Elapsed 41m 39s (remain 48m 34s) Loss: 0.0007(0.0011) Grad: 1846.7666  LR: 0.000011  \n",
      "Epoch: [3][3400/7150] Elapsed 42m 55s (remain 47m 19s) Loss: 0.0017(0.0011) Grad: 3609.3652  LR: 0.000011  \n",
      "Epoch: [3][3500/7150] Elapsed 44m 15s (remain 46m 7s) Loss: 0.0012(0.0011) Grad: 2224.1833  LR: 0.000011  \n",
      "Epoch: [3][3600/7150] Elapsed 45m 37s (remain 44m 58s) Loss: 0.0000(0.0011) Grad: 30.7038  LR: 0.000011  \n",
      "Epoch: [3][3700/7150] Elapsed 46m 59s (remain 43m 47s) Loss: 0.0001(0.0011) Grad: 232.1116  LR: 0.000011  \n",
      "Epoch: [3][3800/7150] Elapsed 48m 22s (remain 42m 37s) Loss: 0.0008(0.0011) Grad: 5844.3657  LR: 0.000011  \n",
      "Epoch: [3][3900/7150] Elapsed 49m 45s (remain 41m 26s) Loss: 0.0068(0.0011) Grad: 75252.2031  LR: 0.000011  \n",
      "Epoch: [3][4000/7150] Elapsed 51m 0s (remain 40m 9s) Loss: 0.0003(0.0011) Grad: 845.1022  LR: 0.000011  \n",
      "Epoch: [3][4100/7150] Elapsed 52m 15s (remain 38m 51s) Loss: 0.0069(0.0011) Grad: 24320.0918  LR: 0.000011  \n",
      "Epoch: [3][4200/7150] Elapsed 53m 31s (remain 37m 34s) Loss: 0.0008(0.0011) Grad: 6170.4556  LR: 0.000011  \n",
      "Epoch: [3][4300/7150] Elapsed 54m 49s (remain 36m 18s) Loss: 0.0000(0.0011) Grad: 66.8583  LR: 0.000011  \n",
      "Epoch: [3][4400/7150] Elapsed 56m 3s (remain 35m 1s) Loss: 0.0003(0.0011) Grad: 1350.5397  LR: 0.000011  \n",
      "Epoch: [3][4500/7150] Elapsed 57m 18s (remain 33m 43s) Loss: 0.0003(0.0011) Grad: 6999.4658  LR: 0.000011  \n",
      "Epoch: [3][4600/7150] Elapsed 58m 33s (remain 32m 26s) Loss: 0.0004(0.0011) Grad: 3718.6719  LR: 0.000010  \n",
      "Epoch: [3][4700/7150] Elapsed 59m 49s (remain 31m 9s) Loss: 0.0003(0.0011) Grad: 711.2752  LR: 0.000010  \n",
      "Epoch: [3][4800/7150] Elapsed 61m 5s (remain 29m 53s) Loss: 0.0000(0.0011) Grad: 11.9969  LR: 0.000010  \n",
      "Epoch: [3][4900/7150] Elapsed 62m 19s (remain 28m 36s) Loss: 0.0000(0.0011) Grad: 152.7167  LR: 0.000010  \n",
      "Epoch: [3][5000/7150] Elapsed 63m 34s (remain 27m 18s) Loss: 0.0013(0.0011) Grad: 9446.2100  LR: 0.000010  \n",
      "Epoch: [3][5100/7150] Elapsed 64m 48s (remain 26m 1s) Loss: 0.0000(0.0011) Grad: 26.9620  LR: 0.000010  \n",
      "Epoch: [3][5200/7150] Elapsed 66m 4s (remain 24m 45s) Loss: 0.0024(0.0011) Grad: 8922.1289  LR: 0.000010  \n",
      "Epoch: [3][5300/7150] Elapsed 67m 20s (remain 23m 29s) Loss: 0.0001(0.0011) Grad: 1224.2057  LR: 0.000010  \n",
      "Epoch: [3][5400/7150] Elapsed 68m 35s (remain 22m 12s) Loss: 0.0003(0.0011) Grad: 24163.7520  LR: 0.000010  \n",
      "Epoch: [3][5500/7150] Elapsed 69m 49s (remain 20m 55s) Loss: 0.0024(0.0011) Grad: 22442.2207  LR: 0.000010  \n",
      "Epoch: [3][5600/7150] Elapsed 71m 3s (remain 19m 39s) Loss: 0.0000(0.0011) Grad: 20.5220  LR: 0.000010  \n",
      "Epoch: [3][5700/7150] Elapsed 72m 18s (remain 18m 22s) Loss: 0.0006(0.0012) Grad: 6629.0239  LR: 0.000010  \n",
      "Epoch: [3][5800/7150] Elapsed 73m 32s (remain 17m 6s) Loss: 0.0000(0.0011) Grad: 245.0373  LR: 0.000010  \n",
      "Epoch: [3][5900/7150] Elapsed 74m 49s (remain 15m 50s) Loss: 0.0000(0.0011) Grad: 464.6530  LR: 0.000010  \n",
      "Epoch: [3][6000/7150] Elapsed 76m 4s (remain 14m 33s) Loss: 0.0001(0.0012) Grad: 405.3794  LR: 0.000010  \n",
      "Epoch: [3][6100/7150] Elapsed 77m 21s (remain 13m 18s) Loss: 0.0000(0.0012) Grad: 54.0709  LR: 0.000010  \n",
      "Epoch: [3][6200/7150] Elapsed 78m 35s (remain 12m 1s) Loss: 0.0001(0.0012) Grad: 1115.9940  LR: 0.000009  \n",
      "Epoch: [3][6300/7150] Elapsed 79m 50s (remain 10m 45s) Loss: 0.0009(0.0012) Grad: 15450.7861  LR: 0.000009  \n",
      "Epoch: [3][6400/7150] Elapsed 81m 5s (remain 9m 29s) Loss: 0.0000(0.0012) Grad: 118.9683  LR: 0.000009  \n",
      "Epoch: [3][6500/7150] Elapsed 82m 20s (remain 8m 13s) Loss: 0.0000(0.0012) Grad: 8.2050  LR: 0.000009  \n",
      "Epoch: [3][6600/7150] Elapsed 83m 34s (remain 6m 57s) Loss: 0.0016(0.0012) Grad: 73903.0156  LR: 0.000009  \n",
      "Epoch: [3][6700/7150] Elapsed 84m 49s (remain 5m 41s) Loss: 0.0000(0.0011) Grad: 15.2469  LR: 0.000009  \n",
      "Epoch: [3][6800/7150] Elapsed 86m 4s (remain 4m 25s) Loss: 0.0000(0.0011) Grad: 16.1522  LR: 0.000009  \n",
      "Epoch: [3][6900/7150] Elapsed 87m 18s (remain 3m 9s) Loss: 0.0033(0.0011) Grad: 14577.7441  LR: 0.000009  \n",
      "Epoch: [3][7000/7150] Elapsed 88m 33s (remain 1m 53s) Loss: 0.0028(0.0011) Grad: 15372.8398  LR: 0.000009  \n",
      "Epoch: [3][7100/7150] Elapsed 89m 47s (remain 0m 37s) Loss: 0.0014(0.0011) Grad: 13131.4883  LR: 0.000009  \n",
      "Epoch: [3][7149/7150] Elapsed 90m 23s (remain 0m 0s) Loss: 0.0010(0.0011) Grad: 8607.4473  LR: 0.000009  \n",
      "EVAL: [0/1192] Elapsed 0m 1s (remain 22m 32s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 30s (remain 5m 28s) Loss: 0.0146(0.0029) \n",
      "EVAL: [200/1192] Elapsed 1m 2s (remain 5m 9s) Loss: 0.0052(0.0029) \n",
      "EVAL: [300/1192] Elapsed 1m 32s (remain 4m 34s) Loss: 0.0011(0.0027) \n",
      "EVAL: [400/1192] Elapsed 2m 2s (remain 4m 1s) Loss: 0.0000(0.0030) \n",
      "EVAL: [500/1192] Elapsed 2m 31s (remain 3m 28s) Loss: 0.0000(0.0028) \n",
      "EVAL: [600/1192] Elapsed 3m 0s (remain 2m 57s) Loss: 0.0003(0.0029) \n",
      "EVAL: [700/1192] Elapsed 3m 29s (remain 2m 26s) Loss: 0.0053(0.0033) \n",
      "EVAL: [800/1192] Elapsed 3m 58s (remain 1m 56s) Loss: 0.0000(0.0032) \n",
      "EVAL: [900/1192] Elapsed 4m 27s (remain 1m 26s) Loss: 0.0004(0.0033) \n",
      "EVAL: [1000/1192] Elapsed 4m 57s (remain 0m 56s) Loss: 0.0164(0.0032) \n",
      "EVAL: [1100/1192] Elapsed 5m 26s (remain 0m 27s) Loss: 0.0093(0.0031) \n",
      "EVAL: [1191/1192] Elapsed 5m 53s (remain 0m 0s) Loss: 0.0000(0.0030) \n",
      "Epoch 3 - avg_train_loss: 0.0011  avg_val_loss: 0.0030  time: 5781s\n",
      "Epoch 3 - Score: 0.8933\n",
      "Epoch 3 - Save Best Score: 0.8933 Model\n",
      "Epoch: [4][0/7150] Elapsed 0m 1s (remain 191m 58s) Loss: 0.0003(0.0003) Grad: 530.7466  LR: 0.000009  \n",
      "Epoch: [4][100/7150] Elapsed 1m 15s (remain 88m 6s) Loss: 0.0000(0.0008) Grad: 52.2391  LR: 0.000009  \n",
      "Epoch: [4][200/7150] Elapsed 2m 31s (remain 87m 5s) Loss: 0.0001(0.0010) Grad: 296.3744  LR: 0.000009  \n",
      "Epoch: [4][300/7150] Elapsed 3m 52s (remain 88m 13s) Loss: 0.0000(0.0009) Grad: 44.3578  LR: 0.000009  \n",
      "Epoch: [4][400/7150] Elapsed 5m 9s (remain 86m 56s) Loss: 0.0004(0.0008) Grad: 3994.6416  LR: 0.000009  \n",
      "Epoch: [4][500/7150] Elapsed 6m 24s (remain 84m 57s) Loss: 0.0000(0.0008) Grad: 88.0591  LR: 0.000009  \n",
      "Epoch: [4][600/7150] Elapsed 7m 38s (remain 83m 13s) Loss: 0.0001(0.0008) Grad: 218.0480  LR: 0.000009  \n",
      "Epoch: [4][700/7150] Elapsed 8m 52s (remain 81m 38s) Loss: 0.0000(0.0009) Grad: 19.6302  LR: 0.000008  \n",
      "Epoch: [4][800/7150] Elapsed 10m 5s (remain 80m 3s) Loss: 0.0009(0.0009) Grad: 3591.3557  LR: 0.000008  \n",
      "Epoch: [4][900/7150] Elapsed 11m 22s (remain 78m 50s) Loss: 0.0011(0.0009) Grad: 3227.5706  LR: 0.000008  \n",
      "Epoch: [4][1000/7150] Elapsed 12m 36s (remain 77m 26s) Loss: 0.0000(0.0008) Grad: 147.0520  LR: 0.000008  \n",
      "Epoch: [4][1100/7150] Elapsed 13m 50s (remain 76m 5s) Loss: 0.0036(0.0008) Grad: 2045.3800  LR: 0.000008  \n",
      "Epoch: [4][1200/7150] Elapsed 15m 6s (remain 74m 49s) Loss: 0.0000(0.0008) Grad: 67.2308  LR: 0.000008  \n",
      "Epoch: [4][1300/7150] Elapsed 16m 25s (remain 73m 50s) Loss: 0.0008(0.0009) Grad: 6908.7754  LR: 0.000008  \n",
      "Epoch: [4][1400/7150] Elapsed 17m 41s (remain 72m 34s) Loss: 0.0000(0.0009) Grad: 99.1275  LR: 0.000008  \n",
      "Epoch: [4][1500/7150] Elapsed 18m 56s (remain 71m 16s) Loss: 0.0000(0.0009) Grad: 6.8396  LR: 0.000008  \n",
      "Epoch: [4][1600/7150] Elapsed 20m 10s (remain 69m 56s) Loss: 0.0001(0.0008) Grad: 127.1586  LR: 0.000008  \n",
      "Epoch: [4][1700/7150] Elapsed 21m 25s (remain 68m 36s) Loss: 0.0012(0.0008) Grad: 1607.0488  LR: 0.000008  \n",
      "Epoch: [4][1800/7150] Elapsed 22m 40s (remain 67m 20s) Loss: 0.0000(0.0008) Grad: 39.4103  LR: 0.000008  \n",
      "Epoch: [4][1900/7150] Elapsed 23m 59s (remain 66m 14s) Loss: 0.0023(0.0008) Grad: 11431.3164  LR: 0.000008  \n",
      "Epoch: [4][2000/7150] Elapsed 25m 15s (remain 65m 0s) Loss: 0.0003(0.0008) Grad: 3029.5635  LR: 0.000008  \n",
      "Epoch: [4][2100/7150] Elapsed 26m 30s (remain 63m 41s) Loss: 0.0001(0.0008) Grad: 326.3721  LR: 0.000008  \n",
      "Epoch: [4][2200/7150] Elapsed 27m 43s (remain 62m 21s) Loss: 0.0012(0.0009) Grad: 3086.2742  LR: 0.000008  \n",
      "Epoch: [4][2300/7150] Elapsed 28m 58s (remain 61m 2s) Loss: 0.0000(0.0008) Grad: 19.4872  LR: 0.000007  \n",
      "Epoch: [4][2400/7150] Elapsed 30m 19s (remain 59m 59s) Loss: 0.0034(0.0009) Grad: 7930.4995  LR: 0.000007  \n",
      "Epoch: [4][2500/7150] Elapsed 31m 41s (remain 58m 54s) Loss: 0.0030(0.0009) Grad: 3140.2876  LR: 0.000007  \n",
      "Epoch: [4][2600/7150] Elapsed 33m 2s (remain 57m 46s) Loss: 0.0000(0.0009) Grad: 27.5224  LR: 0.000007  \n",
      "Epoch: [4][2700/7150] Elapsed 34m 24s (remain 56m 41s) Loss: 0.0139(0.0009) Grad: 769882.3750  LR: 0.000007  \n",
      "Epoch: [4][2800/7150] Elapsed 35m 41s (remain 55m 25s) Loss: 0.0000(0.0009) Grad: 122.1521  LR: 0.000007  \n",
      "Epoch: [4][2900/7150] Elapsed 36m 56s (remain 54m 6s) Loss: 0.0029(0.0009) Grad: 19536.9961  LR: 0.000007  \n",
      "Epoch: [4][3000/7150] Elapsed 38m 12s (remain 52m 49s) Loss: 0.0000(0.0009) Grad: 34.6148  LR: 0.000007  \n",
      "Epoch: [4][3100/7150] Elapsed 39m 27s (remain 51m 31s) Loss: 0.0050(0.0009) Grad: 6352.8828  LR: 0.000007  \n",
      "Epoch: [4][3200/7150] Elapsed 40m 41s (remain 50m 12s) Loss: 0.0016(0.0009) Grad: 14978.2363  LR: 0.000007  \n",
      "Epoch: [4][3300/7150] Elapsed 41m 58s (remain 48m 56s) Loss: 0.0002(0.0009) Grad: 596.5673  LR: 0.000007  \n",
      "Epoch: [4][3400/7150] Elapsed 43m 15s (remain 47m 40s) Loss: 0.0002(0.0009) Grad: 1494.9563  LR: 0.000007  \n",
      "Epoch: [4][3500/7150] Elapsed 44m 30s (remain 46m 23s) Loss: 0.0000(0.0009) Grad: 24.2504  LR: 0.000007  \n",
      "Epoch: [4][3600/7150] Elapsed 45m 44s (remain 45m 4s) Loss: 0.0001(0.0009) Grad: 192.9496  LR: 0.000007  \n",
      "Epoch: [4][3700/7150] Elapsed 46m 58s (remain 43m 47s) Loss: 0.0003(0.0009) Grad: 1043.3290  LR: 0.000007  \n",
      "Epoch: [4][3800/7150] Elapsed 48m 15s (remain 42m 30s) Loss: 0.0000(0.0009) Grad: 45.0443  LR: 0.000007  \n",
      "Epoch: [4][3900/7150] Elapsed 49m 29s (remain 41m 13s) Loss: 0.0000(0.0009) Grad: 15.8378  LR: 0.000006  \n",
      "Epoch: [4][4000/7150] Elapsed 50m 47s (remain 39m 58s) Loss: 0.0001(0.0009) Grad: 1264.1399  LR: 0.000006  \n",
      "Epoch: [4][4100/7150] Elapsed 52m 4s (remain 38m 42s) Loss: 0.0000(0.0009) Grad: 30.5057  LR: 0.000006  \n",
      "Epoch: [4][4200/7150] Elapsed 53m 19s (remain 37m 26s) Loss: 0.0000(0.0009) Grad: 61.1111  LR: 0.000006  \n",
      "Epoch: [4][4300/7150] Elapsed 54m 35s (remain 36m 10s) Loss: 0.0041(0.0009) Grad: 22307.2051  LR: 0.000006  \n",
      "Epoch: [4][4400/7150] Elapsed 55m 53s (remain 34m 54s) Loss: 0.0003(0.0009) Grad: 922.8070  LR: 0.000006  \n",
      "Epoch: [4][4500/7150] Elapsed 57m 10s (remain 33m 39s) Loss: 0.0001(0.0009) Grad: 2746.1047  LR: 0.000006  \n",
      "Epoch: [4][4600/7150] Elapsed 58m 25s (remain 32m 22s) Loss: 0.0028(0.0009) Grad: 8953.6777  LR: 0.000006  \n",
      "Epoch: [4][4700/7150] Elapsed 59m 40s (remain 31m 5s) Loss: 0.0024(0.0009) Grad: 4886.8125  LR: 0.000006  \n",
      "Epoch: [4][4800/7150] Elapsed 60m 56s (remain 29m 48s) Loss: 0.0047(0.0009) Grad: 8710.7051  LR: 0.000006  \n",
      "Epoch: [4][4900/7150] Elapsed 62m 13s (remain 28m 33s) Loss: 0.0000(0.0009) Grad: 458.1131  LR: 0.000006  \n",
      "Epoch: [4][5000/7150] Elapsed 63m 32s (remain 27m 18s) Loss: 0.0001(0.0009) Grad: 525.9216  LR: 0.000006  \n",
      "Epoch: [4][5100/7150] Elapsed 64m 48s (remain 26m 2s) Loss: 0.0000(0.0009) Grad: 1317.9646  LR: 0.000006  \n",
      "Epoch: [4][5200/7150] Elapsed 66m 4s (remain 24m 45s) Loss: 0.0000(0.0009) Grad: 592.0019  LR: 0.000006  \n",
      "Epoch: [4][5300/7150] Elapsed 67m 20s (remain 23m 29s) Loss: 0.0039(0.0009) Grad: 11948.0225  LR: 0.000006  \n",
      "Epoch: [4][5400/7150] Elapsed 68m 37s (remain 22m 13s) Loss: 0.0000(0.0009) Grad: 62.2250  LR: 0.000006  \n",
      "Epoch: [4][5500/7150] Elapsed 69m 52s (remain 20m 56s) Loss: 0.0000(0.0009) Grad: 16.3488  LR: 0.000005  \n",
      "Epoch: [4][5600/7150] Elapsed 71m 9s (remain 19m 40s) Loss: 0.0053(0.0009) Grad: 49099.8828  LR: 0.000005  \n",
      "Epoch: [4][5700/7150] Elapsed 72m 25s (remain 18m 24s) Loss: 0.0004(0.0009) Grad: 5494.6929  LR: 0.000005  \n",
      "Epoch: [4][5800/7150] Elapsed 73m 40s (remain 17m 8s) Loss: 0.0001(0.0009) Grad: 1590.7222  LR: 0.000005  \n",
      "Epoch: [4][5900/7150] Elapsed 74m 56s (remain 15m 51s) Loss: 0.0000(0.0009) Grad: 23.2672  LR: 0.000005  \n",
      "Epoch: [4][6000/7150] Elapsed 76m 13s (remain 14m 35s) Loss: 0.0113(0.0009) Grad: 14087.9629  LR: 0.000005  \n",
      "Epoch: [4][6100/7150] Elapsed 77m 28s (remain 13m 19s) Loss: 0.0008(0.0009) Grad: 4273.0098  LR: 0.000005  \n",
      "Epoch: [4][6200/7150] Elapsed 78m 43s (remain 12m 2s) Loss: 0.0000(0.0009) Grad: 67.8033  LR: 0.000005  \n",
      "Epoch: [4][6300/7150] Elapsed 79m 58s (remain 10m 46s) Loss: 0.0000(0.0009) Grad: 24.0272  LR: 0.000005  \n",
      "Epoch: [4][6400/7150] Elapsed 81m 14s (remain 9m 30s) Loss: 0.0001(0.0009) Grad: 1980.6251  LR: 0.000005  \n",
      "Epoch: [4][6500/7150] Elapsed 82m 29s (remain 8m 14s) Loss: 0.0000(0.0009) Grad: 8.5919  LR: 0.000005  \n",
      "Epoch: [4][6600/7150] Elapsed 83m 45s (remain 6m 57s) Loss: 0.0000(0.0009) Grad: 892.6385  LR: 0.000005  \n",
      "Epoch: [4][6700/7150] Elapsed 85m 1s (remain 5m 41s) Loss: 0.0000(0.0009) Grad: 23.8160  LR: 0.000005  \n",
      "Epoch: [4][6800/7150] Elapsed 86m 17s (remain 4m 25s) Loss: 0.0000(0.0009) Grad: 121.1524  LR: 0.000005  \n",
      "Epoch: [4][6900/7150] Elapsed 87m 36s (remain 3m 9s) Loss: 0.0001(0.0009) Grad: 9283.6748  LR: 0.000005  \n",
      "Epoch: [4][7000/7150] Elapsed 88m 51s (remain 1m 53s) Loss: 0.0011(0.0009) Grad: 7076.5312  LR: 0.000005  \n",
      "Epoch: [4][7100/7150] Elapsed 90m 7s (remain 0m 37s) Loss: 0.0006(0.0009) Grad: 23594.0879  LR: 0.000004  \n",
      "Epoch: [4][7149/7150] Elapsed 90m 44s (remain 0m 0s) Loss: 0.0000(0.0009) Grad: 1003.0388  LR: 0.000004  \n",
      "EVAL: [0/1192] Elapsed 0m 1s (remain 23m 17s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 32s (remain 5m 49s) Loss: 0.0123(0.0027) \n",
      "EVAL: [200/1192] Elapsed 1m 1s (remain 5m 2s) Loss: 0.0056(0.0028) \n",
      "EVAL: [300/1192] Elapsed 1m 30s (remain 4m 28s) Loss: 0.0013(0.0027) \n",
      "EVAL: [400/1192] Elapsed 2m 0s (remain 3m 57s) Loss: 0.0000(0.0029) \n",
      "EVAL: [500/1192] Elapsed 2m 30s (remain 3m 28s) Loss: 0.0000(0.0027) \n",
      "EVAL: [600/1192] Elapsed 2m 59s (remain 2m 56s) Loss: 0.0010(0.0029) \n",
      "EVAL: [700/1192] Elapsed 3m 29s (remain 2m 26s) Loss: 0.0030(0.0033) \n",
      "EVAL: [800/1192] Elapsed 3m 58s (remain 1m 56s) Loss: 0.0000(0.0032) \n",
      "EVAL: [900/1192] Elapsed 4m 27s (remain 1m 26s) Loss: 0.0026(0.0033) \n",
      "EVAL: [1000/1192] Elapsed 4m 57s (remain 0m 56s) Loss: 0.0000(0.0032) \n",
      "EVAL: [1100/1192] Elapsed 5m 28s (remain 0m 27s) Loss: 0.0127(0.0031) \n",
      "EVAL: [1191/1192] Elapsed 5m 55s (remain 0m 0s) Loss: 0.0000(0.0029) \n",
      "Epoch 4 - avg_train_loss: 0.0009  avg_val_loss: 0.0029  time: 5804s\n",
      "Epoch 4 - Score: 0.8938\n",
      "Epoch 4 - Save Best Score: 0.8938 Model\n",
      "Epoch: [5][0/7150] Elapsed 0m 1s (remain 189m 59s) Loss: 0.0001(0.0001) Grad: 446.2370  LR: 0.000004  \n",
      "Epoch: [5][100/7150] Elapsed 1m 17s (remain 89m 45s) Loss: 0.0000(0.0004) Grad: 10.1539  LR: 0.000004  \n",
      "Epoch: [5][200/7150] Elapsed 2m 33s (remain 88m 22s) Loss: 0.0003(0.0005) Grad: 1671.8519  LR: 0.000004  \n",
      "Epoch: [5][300/7150] Elapsed 3m 51s (remain 87m 43s) Loss: 0.0004(0.0005) Grad: 3634.7795  LR: 0.000004  \n",
      "Epoch: [5][400/7150] Elapsed 5m 8s (remain 86m 39s) Loss: 0.0019(0.0005) Grad: 3008.6479  LR: 0.000004  \n",
      "Epoch: [5][500/7150] Elapsed 6m 22s (remain 84m 36s) Loss: 0.0000(0.0006) Grad: 52.6812  LR: 0.000004  \n",
      "Epoch: [5][600/7150] Elapsed 7m 36s (remain 82m 56s) Loss: 0.0001(0.0006) Grad: 696.5706  LR: 0.000004  \n",
      "Epoch: [5][700/7150] Elapsed 8m 52s (remain 81m 39s) Loss: 0.0000(0.0006) Grad: 16.0011  LR: 0.000004  \n",
      "Epoch: [5][800/7150] Elapsed 10m 11s (remain 80m 47s) Loss: 0.0000(0.0006) Grad: 120.8076  LR: 0.000004  \n",
      "Epoch: [5][900/7150] Elapsed 11m 26s (remain 79m 18s) Loss: 0.0014(0.0006) Grad: 6540.9512  LR: 0.000004  \n",
      "Epoch: [5][1000/7150] Elapsed 12m 41s (remain 77m 59s) Loss: 0.0000(0.0006) Grad: 35.4875  LR: 0.000004  \n",
      "Epoch: [5][1100/7150] Elapsed 13m 57s (remain 76m 42s) Loss: 0.0000(0.0006) Grad: 42.3606  LR: 0.000004  \n",
      "Epoch: [5][1200/7150] Elapsed 15m 11s (remain 75m 16s) Loss: 0.0000(0.0006) Grad: 32.2116  LR: 0.000004  \n",
      "Epoch: [5][1300/7150] Elapsed 16m 25s (remain 73m 50s) Loss: 0.0000(0.0006) Grad: 9.4935  LR: 0.000004  \n",
      "Epoch: [5][1400/7150] Elapsed 17m 39s (remain 72m 27s) Loss: 0.0000(0.0007) Grad: 28.2072  LR: 0.000004  \n",
      "Epoch: [5][1500/7150] Elapsed 18m 54s (remain 71m 11s) Loss: 0.0000(0.0006) Grad: 11.9811  LR: 0.000004  \n",
      "Epoch: [5][1600/7150] Elapsed 20m 10s (remain 69m 54s) Loss: 0.0012(0.0006) Grad: 9335.5977  LR: 0.000003  \n",
      "Epoch: [5][1700/7150] Elapsed 21m 25s (remain 68m 37s) Loss: 0.0000(0.0006) Grad: 48.4873  LR: 0.000003  \n",
      "Epoch: [5][1800/7150] Elapsed 22m 39s (remain 67m 17s) Loss: 0.0019(0.0007) Grad: 4138.5435  LR: 0.000003  \n",
      "Epoch: [5][1900/7150] Elapsed 23m 54s (remain 66m 0s) Loss: 0.0000(0.0007) Grad: 32.7247  LR: 0.000003  \n",
      "Epoch: [5][2000/7150] Elapsed 25m 10s (remain 64m 46s) Loss: 0.0000(0.0007) Grad: 128.8585  LR: 0.000003  \n",
      "Epoch: [5][2100/7150] Elapsed 26m 26s (remain 63m 32s) Loss: 0.0000(0.0007) Grad: 35.2531  LR: 0.000003  \n",
      "Epoch: [5][2200/7150] Elapsed 27m 43s (remain 62m 20s) Loss: 0.0019(0.0007) Grad: 64860.9023  LR: 0.000003  \n",
      "Epoch: [5][2300/7150] Elapsed 28m 57s (remain 61m 2s) Loss: 0.0000(0.0007) Grad: 43.0275  LR: 0.000003  \n",
      "Epoch: [5][2400/7150] Elapsed 30m 14s (remain 59m 48s) Loss: 0.0000(0.0007) Grad: 9.2035  LR: 0.000003  \n",
      "Epoch: [5][2500/7150] Elapsed 31m 30s (remain 58m 33s) Loss: 0.0007(0.0007) Grad: 2422.9434  LR: 0.000003  \n",
      "Epoch: [5][2600/7150] Elapsed 32m 43s (remain 57m 14s) Loss: 0.0000(0.0007) Grad: 32.7768  LR: 0.000003  \n",
      "Epoch: [5][2700/7150] Elapsed 33m 57s (remain 55m 56s) Loss: 0.0000(0.0007) Grad: 56.5853  LR: 0.000003  \n",
      "Epoch: [5][2800/7150] Elapsed 35m 13s (remain 54m 42s) Loss: 0.0000(0.0007) Grad: 176.0718  LR: 0.000003  \n",
      "Epoch: [5][2900/7150] Elapsed 36m 29s (remain 53m 27s) Loss: 0.0009(0.0007) Grad: 9287.3701  LR: 0.000003  \n",
      "Epoch: [5][3000/7150] Elapsed 37m 43s (remain 52m 9s) Loss: 0.0000(0.0007) Grad: 28.8590  LR: 0.000003  \n",
      "Epoch: [5][3100/7150] Elapsed 38m 57s (remain 50m 52s) Loss: 0.0000(0.0007) Grad: 60.7511  LR: 0.000003  \n",
      "Epoch: [5][3200/7150] Elapsed 40m 12s (remain 49m 36s) Loss: 0.0052(0.0007) Grad: 9313.0352  LR: 0.000002  \n",
      "Epoch: [5][3300/7150] Elapsed 41m 26s (remain 48m 19s) Loss: 0.0001(0.0007) Grad: 449.1542  LR: 0.000002  \n",
      "Epoch: [5][3400/7150] Elapsed 42m 41s (remain 47m 3s) Loss: 0.0001(0.0007) Grad: 782.7895  LR: 0.000002  \n",
      "Epoch: [5][3500/7150] Elapsed 43m 56s (remain 45m 48s) Loss: 0.0005(0.0007) Grad: 1543.0563  LR: 0.000002  \n",
      "Epoch: [5][3600/7150] Elapsed 45m 12s (remain 44m 32s) Loss: 0.0001(0.0007) Grad: 789.5128  LR: 0.000002  \n",
      "Epoch: [5][3700/7150] Elapsed 46m 27s (remain 43m 17s) Loss: 0.0000(0.0007) Grad: 17.0529  LR: 0.000002  \n",
      "Epoch: [5][3800/7150] Elapsed 47m 42s (remain 42m 2s) Loss: 0.0000(0.0007) Grad: 332.9813  LR: 0.000002  \n",
      "Epoch: [5][3900/7150] Elapsed 48m 59s (remain 40m 48s) Loss: 0.0000(0.0007) Grad: 58.1733  LR: 0.000002  \n",
      "Epoch: [5][4000/7150] Elapsed 50m 21s (remain 39m 37s) Loss: 0.0000(0.0007) Grad: 24.2468  LR: 0.000002  \n",
      "Epoch: [5][4100/7150] Elapsed 51m 42s (remain 38m 26s) Loss: 0.0000(0.0007) Grad: 53.3298  LR: 0.000002  \n",
      "Epoch: [5][4200/7150] Elapsed 53m 4s (remain 37m 15s) Loss: 0.0050(0.0007) Grad: 9925.7715  LR: 0.000002  \n",
      "Epoch: [5][4300/7150] Elapsed 54m 21s (remain 36m 0s) Loss: 0.0000(0.0007) Grad: 23.3651  LR: 0.000002  \n",
      "Epoch: [5][4400/7150] Elapsed 55m 38s (remain 34m 45s) Loss: 0.0002(0.0007) Grad: 712.9182  LR: 0.000002  \n",
      "Epoch: [5][4500/7150] Elapsed 56m 53s (remain 33m 29s) Loss: 0.0019(0.0007) Grad: 9507.9619  LR: 0.000002  \n",
      "Epoch: [5][4600/7150] Elapsed 58m 9s (remain 32m 13s) Loss: 0.0001(0.0007) Grad: 582.2440  LR: 0.000002  \n",
      "Epoch: [5][4700/7150] Elapsed 59m 24s (remain 30m 56s) Loss: 0.0000(0.0007) Grad: 124.5758  LR: 0.000002  \n",
      "Epoch: [5][4800/7150] Elapsed 60m 43s (remain 29m 42s) Loss: 0.0003(0.0007) Grad: 12255.0508  LR: 0.000001  \n",
      "Epoch: [5][4900/7150] Elapsed 62m 3s (remain 28m 28s) Loss: 0.0000(0.0007) Grad: 61.7655  LR: 0.000001  \n",
      "Epoch: [5][5000/7150] Elapsed 63m 19s (remain 27m 12s) Loss: 0.0025(0.0007) Grad: 49251.0156  LR: 0.000001  \n",
      "Epoch: [5][5100/7150] Elapsed 64m 34s (remain 25m 56s) Loss: 0.0007(0.0007) Grad: 4049.8289  LR: 0.000001  \n",
      "Epoch: [5][5200/7150] Elapsed 65m 48s (remain 24m 39s) Loss: 0.0000(0.0007) Grad: 323.4379  LR: 0.000001  \n",
      "Epoch: [5][5300/7150] Elapsed 67m 2s (remain 23m 23s) Loss: 0.0000(0.0007) Grad: 432.7564  LR: 0.000001  \n",
      "Epoch: [5][5400/7150] Elapsed 68m 16s (remain 22m 6s) Loss: 0.0000(0.0007) Grad: 130.2109  LR: 0.000001  \n",
      "Epoch: [5][5500/7150] Elapsed 69m 31s (remain 20m 50s) Loss: 0.0001(0.0007) Grad: 1125.0134  LR: 0.000001  \n",
      "Epoch: [5][5600/7150] Elapsed 70m 46s (remain 19m 34s) Loss: 0.0000(0.0007) Grad: 108.5345  LR: 0.000001  \n",
      "Epoch: [5][5700/7150] Elapsed 71m 59s (remain 18m 17s) Loss: 0.0000(0.0007) Grad: 283.6902  LR: 0.000001  \n",
      "Epoch: [5][5800/7150] Elapsed 73m 16s (remain 17m 2s) Loss: 0.0001(0.0007) Grad: 3337.8196  LR: 0.000001  \n",
      "Epoch: [5][5900/7150] Elapsed 74m 31s (remain 15m 46s) Loss: 0.0002(0.0007) Grad: 6475.4565  LR: 0.000001  \n",
      "Epoch: [5][6000/7150] Elapsed 75m 45s (remain 14m 30s) Loss: 0.0000(0.0007) Grad: 31.2230  LR: 0.000001  \n",
      "Epoch: [5][6100/7150] Elapsed 76m 59s (remain 13m 14s) Loss: 0.0000(0.0007) Grad: 44.2417  LR: 0.000001  \n",
      "Epoch: [5][6200/7150] Elapsed 78m 14s (remain 11m 58s) Loss: 0.0000(0.0007) Grad: 90.7398  LR: 0.000001  \n",
      "Epoch: [5][6300/7150] Elapsed 79m 29s (remain 10m 42s) Loss: 0.0001(0.0007) Grad: 645.4764  LR: 0.000001  \n",
      "Epoch: [5][6400/7150] Elapsed 80m 48s (remain 9m 27s) Loss: 0.0017(0.0007) Grad: 44748.9141  LR: 0.000000  \n",
      "Epoch: [5][6500/7150] Elapsed 82m 10s (remain 8m 12s) Loss: 0.0000(0.0007) Grad: 24.7053  LR: 0.000000  \n",
      "Epoch: [5][6600/7150] Elapsed 83m 26s (remain 6m 56s) Loss: 0.0071(0.0007) Grad: 15766.3838  LR: 0.000000  \n",
      "Epoch: [5][6700/7150] Elapsed 84m 41s (remain 5m 40s) Loss: 0.0000(0.0007) Grad: 9.0204  LR: 0.000000  \n",
      "Epoch: [5][6800/7150] Elapsed 86m 0s (remain 4m 24s) Loss: 0.0057(0.0007) Grad: 44072.8242  LR: 0.000000  \n",
      "Epoch: [5][6900/7150] Elapsed 87m 15s (remain 3m 8s) Loss: 0.0000(0.0007) Grad: 16.3486  LR: 0.000000  \n",
      "Epoch: [5][7000/7150] Elapsed 88m 31s (remain 1m 53s) Loss: 0.0091(0.0007) Grad: 46502.5234  LR: 0.000000  \n",
      "Epoch: [5][7100/7150] Elapsed 89m 51s (remain 0m 37s) Loss: 0.0003(0.0007) Grad: 3531.3608  LR: 0.000000  \n",
      "Epoch: [5][7149/7150] Elapsed 90m 28s (remain 0m 0s) Loss: 0.0000(0.0007) Grad: 508.5863  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 1s (remain 20m 31s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 30s (remain 5m 26s) Loss: 0.0118(0.0032) \n",
      "EVAL: [200/1192] Elapsed 1m 0s (remain 4m 56s) Loss: 0.0051(0.0032) \n",
      "EVAL: [300/1192] Elapsed 1m 31s (remain 4m 29s) Loss: 0.0004(0.0030) \n",
      "EVAL: [400/1192] Elapsed 2m 0s (remain 3m 57s) Loss: 0.0000(0.0032) \n",
      "EVAL: [500/1192] Elapsed 2m 29s (remain 3m 26s) Loss: 0.0000(0.0031) \n",
      "EVAL: [600/1192] Elapsed 2m 58s (remain 2m 55s) Loss: 0.0006(0.0032) \n",
      "EVAL: [700/1192] Elapsed 3m 29s (remain 2m 26s) Loss: 0.0021(0.0036) \n",
      "EVAL: [800/1192] Elapsed 3m 58s (remain 1m 56s) Loss: 0.0000(0.0036) \n",
      "EVAL: [900/1192] Elapsed 4m 27s (remain 1m 26s) Loss: 0.0017(0.0036) \n",
      "EVAL: [1000/1192] Elapsed 4m 56s (remain 0m 56s) Loss: 0.0008(0.0035) \n",
      "EVAL: [1100/1192] Elapsed 5m 25s (remain 0m 26s) Loss: 0.0091(0.0034) \n",
      "EVAL: [1191/1192] Elapsed 5m 52s (remain 0m 0s) Loss: 0.0000(0.0033) \n",
      "Epoch 5 - avg_train_loss: 0.0007  avg_val_loss: 0.0033  time: 5785s\n",
      "Epoch 5 - Score: 0.8943\n",
      "Epoch 5 - Save Best Score: 0.8943 Model\n",
      "========== fold: 3 training ==========\n",
      "get pseudo plain from ../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_3.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_3.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp083/pseudo_labels_3.npy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d410e18082ef47129ca74fa719e2bfc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6e68cc7dce42cb867a52477f0e74f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(612602, 950)\n",
      "(612602, 6) (612602, 950)\n",
      "(10725, 7)\n",
      "(21450, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'deberta.embeddings.position_embeddings.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\n",
      "Epoch: [1][0/7150] Elapsed 0m 1s (remain 168m 30s) Loss: 0.0687(0.0687) Grad: 65634.9922  LR: 0.000000  \n",
      "Epoch: [1][100/7150] Elapsed 1m 18s (remain 91m 10s) Loss: 0.0523(0.0623) Grad: 51621.0352  LR: 0.000001  \n",
      "Epoch: [1][200/7150] Elapsed 2m 35s (remain 89m 38s) Loss: 0.0260(0.0512) Grad: 26032.6289  LR: 0.000001  \n",
      "Epoch: [1][300/7150] Elapsed 3m 51s (remain 87m 47s) Loss: 0.0096(0.0402) Grad: 5016.9082  LR: 0.000002  \n",
      "Epoch: [1][400/7150] Elapsed 5m 7s (remain 86m 12s) Loss: 0.0068(0.0336) Grad: 3070.7749  LR: 0.000002  \n",
      "Epoch: [1][500/7150] Elapsed 6m 24s (remain 85m 2s) Loss: 0.0143(0.0292) Grad: 6538.0542  LR: 0.000003  \n",
      "Epoch: [1][600/7150] Elapsed 7m 41s (remain 83m 46s) Loss: 0.0108(0.0264) Grad: 2725.0657  LR: 0.000003  \n",
      "Epoch: [1][700/7150] Elapsed 8m 57s (remain 82m 22s) Loss: 0.0087(0.0244) Grad: 2833.6597  LR: 0.000004  \n",
      "Epoch: [1][800/7150] Elapsed 10m 13s (remain 80m 59s) Loss: 0.0150(0.0228) Grad: 7416.9917  LR: 0.000004  \n",
      "Epoch: [1][900/7150] Elapsed 11m 30s (remain 79m 46s) Loss: 0.0082(0.0216) Grad: 5021.5996  LR: 0.000005  \n",
      "Epoch: [1][1000/7150] Elapsed 12m 47s (remain 78m 35s) Loss: 0.0095(0.0206) Grad: 3633.9453  LR: 0.000006  \n",
      "Epoch: [1][1100/7150] Elapsed 14m 3s (remain 77m 13s) Loss: 0.0052(0.0194) Grad: 5319.1865  LR: 0.000006  \n",
      "Epoch: [1][1200/7150] Elapsed 15m 21s (remain 76m 3s) Loss: 0.0016(0.0184) Grad: 1912.9877  LR: 0.000007  \n",
      "Epoch: [1][1300/7150] Elapsed 16m 37s (remain 74m 45s) Loss: 0.0104(0.0173) Grad: 12435.0039  LR: 0.000007  \n",
      "Epoch: [1][1400/7150] Elapsed 17m 53s (remain 73m 24s) Loss: 0.0040(0.0165) Grad: 16004.8730  LR: 0.000008  \n",
      "Epoch: [1][1500/7150] Elapsed 19m 9s (remain 72m 6s) Loss: 0.0003(0.0157) Grad: 1134.4053  LR: 0.000008  \n",
      "Epoch: [1][1600/7150] Elapsed 20m 25s (remain 70m 48s) Loss: 0.0059(0.0150) Grad: 11192.4844  LR: 0.000009  \n",
      "Epoch: [1][1700/7150] Elapsed 21m 41s (remain 69m 30s) Loss: 0.0008(0.0144) Grad: 3473.8953  LR: 0.000010  \n",
      "Epoch: [1][1800/7150] Elapsed 23m 1s (remain 68m 23s) Loss: 0.0039(0.0138) Grad: 13991.1650  LR: 0.000010  \n",
      "Epoch: [1][1900/7150] Elapsed 24m 17s (remain 67m 4s) Loss: 0.0021(0.0132) Grad: 7338.8560  LR: 0.000011  \n",
      "Epoch: [1][2000/7150] Elapsed 25m 33s (remain 65m 46s) Loss: 0.0024(0.0127) Grad: 9269.4102  LR: 0.000011  \n",
      "Epoch: [1][2100/7150] Elapsed 26m 49s (remain 64m 27s) Loss: 0.0001(0.0123) Grad: 3374.3604  LR: 0.000012  \n",
      "Epoch: [1][2200/7150] Elapsed 28m 5s (remain 63m 9s) Loss: 0.0013(0.0119) Grad: 17934.9121  LR: 0.000012  \n",
      "Epoch: [1][2300/7150] Elapsed 29m 22s (remain 61m 54s) Loss: 0.0003(0.0115) Grad: 1868.5182  LR: 0.000013  \n",
      "Epoch: [1][2400/7150] Elapsed 30m 38s (remain 60m 36s) Loss: 0.0003(0.0112) Grad: 1001.4506  LR: 0.000013  \n",
      "Epoch: [1][2500/7150] Elapsed 31m 57s (remain 59m 23s) Loss: 0.0017(0.0109) Grad: 4285.7104  LR: 0.000014  \n",
      "Epoch: [1][2600/7150] Elapsed 33m 13s (remain 58m 6s) Loss: 0.0005(0.0106) Grad: 1047.7059  LR: 0.000015  \n",
      "Epoch: [1][2700/7150] Elapsed 34m 28s (remain 56m 47s) Loss: 0.0004(0.0103) Grad: 893.9481  LR: 0.000015  \n",
      "Epoch: [1][2800/7150] Elapsed 35m 46s (remain 55m 33s) Loss: 0.0047(0.0100) Grad: 15138.1982  LR: 0.000016  \n",
      "Epoch: [1][2900/7150] Elapsed 37m 3s (remain 54m 15s) Loss: 0.0061(0.0098) Grad: 20395.8828  LR: 0.000016  \n",
      "Epoch: [1][3000/7150] Elapsed 38m 18s (remain 52m 57s) Loss: 0.0065(0.0096) Grad: 149502.7031  LR: 0.000017  \n",
      "Epoch: [1][3100/7150] Elapsed 39m 35s (remain 51m 41s) Loss: 0.0021(0.0094) Grad: 4129.6079  LR: 0.000017  \n",
      "Epoch: [1][3200/7150] Elapsed 40m 52s (remain 50m 25s) Loss: 0.0029(0.0092) Grad: 9295.1328  LR: 0.000018  \n",
      "Epoch: [1][3300/7150] Elapsed 42m 9s (remain 49m 9s) Loss: 0.0002(0.0090) Grad: 473.8520  LR: 0.000018  \n",
      "Epoch: [1][3400/7150] Elapsed 43m 25s (remain 47m 51s) Loss: 0.0020(0.0088) Grad: 4420.8340  LR: 0.000019  \n",
      "Epoch: [1][3500/7150] Elapsed 44m 40s (remain 46m 33s) Loss: 0.0022(0.0086) Grad: 6098.9785  LR: 0.000020  \n",
      "Epoch: [1][3600/7150] Elapsed 45m 56s (remain 45m 17s) Loss: 0.0228(0.0084) Grad: 76844.6406  LR: 0.000020  \n",
      "Epoch: [1][3700/7150] Elapsed 47m 14s (remain 44m 1s) Loss: 0.0001(0.0083) Grad: 493.2609  LR: 0.000020  \n",
      "Epoch: [1][3800/7150] Elapsed 48m 28s (remain 42m 42s) Loss: 0.0231(0.0081) Grad: 55669.0547  LR: 0.000020  \n",
      "Epoch: [1][3900/7150] Elapsed 49m 44s (remain 41m 25s) Loss: 0.0008(0.0079) Grad: 1901.3907  LR: 0.000020  \n",
      "Epoch: [1][4000/7150] Elapsed 50m 59s (remain 40m 7s) Loss: 0.0016(0.0078) Grad: 9446.0830  LR: 0.000020  \n",
      "Epoch: [1][4100/7150] Elapsed 52m 13s (remain 38m 49s) Loss: 0.0095(0.0077) Grad: 13683.1699  LR: 0.000020  \n",
      "Epoch: [1][4200/7150] Elapsed 53m 30s (remain 37m 33s) Loss: 0.0007(0.0075) Grad: 6557.8853  LR: 0.000020  \n",
      "Epoch: [1][4300/7150] Elapsed 54m 44s (remain 36m 15s) Loss: 0.0041(0.0074) Grad: 74562.2969  LR: 0.000020  \n",
      "Epoch: [1][4400/7150] Elapsed 56m 2s (remain 35m 0s) Loss: 0.0046(0.0073) Grad: 17492.0234  LR: 0.000019  \n",
      "Epoch: [1][4500/7150] Elapsed 57m 18s (remain 33m 43s) Loss: 0.0001(0.0072) Grad: 166.9169  LR: 0.000019  \n",
      "Epoch: [1][4600/7150] Elapsed 58m 32s (remain 32m 26s) Loss: 0.0001(0.0071) Grad: 236.5117  LR: 0.000019  \n",
      "Epoch: [1][4700/7150] Elapsed 59m 47s (remain 31m 8s) Loss: 0.0008(0.0070) Grad: 19992.4824  LR: 0.000019  \n",
      "Epoch: [1][4800/7150] Elapsed 61m 2s (remain 29m 52s) Loss: 0.0001(0.0069) Grad: 104.4010  LR: 0.000019  \n",
      "Epoch: [1][4900/7150] Elapsed 62m 19s (remain 28m 35s) Loss: 0.0032(0.0068) Grad: 43260.1602  LR: 0.000019  \n",
      "Epoch: [1][5000/7150] Elapsed 63m 35s (remain 27m 19s) Loss: 0.0092(0.0067) Grad: 28279.9062  LR: 0.000019  \n",
      "Epoch: [1][5100/7150] Elapsed 64m 50s (remain 26m 2s) Loss: 0.0007(0.0066) Grad: 3393.6719  LR: 0.000019  \n",
      "Epoch: [1][5200/7150] Elapsed 66m 8s (remain 24m 47s) Loss: 0.0002(0.0066) Grad: 1072.0389  LR: 0.000019  \n",
      "Epoch: [1][5300/7150] Elapsed 67m 22s (remain 23m 30s) Loss: 0.0001(0.0065) Grad: 214.0212  LR: 0.000019  \n",
      "Epoch: [1][5400/7150] Elapsed 68m 39s (remain 22m 13s) Loss: 0.0052(0.0064) Grad: 16871.1328  LR: 0.000019  \n",
      "Epoch: [1][5500/7150] Elapsed 69m 54s (remain 20m 57s) Loss: 0.0003(0.0063) Grad: 912.7873  LR: 0.000019  \n",
      "Epoch: [1][5600/7150] Elapsed 71m 9s (remain 19m 40s) Loss: 0.0025(0.0063) Grad: 21155.3555  LR: 0.000019  \n",
      "Epoch: [1][5700/7150] Elapsed 72m 26s (remain 18m 24s) Loss: 0.0041(0.0062) Grad: 25410.0312  LR: 0.000019  \n",
      "Epoch: [1][5800/7150] Elapsed 73m 40s (remain 17m 7s) Loss: 0.0010(0.0061) Grad: 4125.7495  LR: 0.000019  \n",
      "Epoch: [1][5900/7150] Elapsed 74m 54s (remain 15m 51s) Loss: 0.0002(0.0061) Grad: 1028.4067  LR: 0.000019  \n",
      "Epoch: [1][6000/7150] Elapsed 76m 10s (remain 14m 35s) Loss: 0.0002(0.0060) Grad: 892.6553  LR: 0.000018  \n",
      "Epoch: [1][6100/7150] Elapsed 77m 26s (remain 13m 18s) Loss: 0.0126(0.0059) Grad: 53256.4531  LR: 0.000018  \n",
      "Epoch: [1][6200/7150] Elapsed 78m 42s (remain 12m 2s) Loss: 0.0003(0.0059) Grad: 3120.4124  LR: 0.000018  \n",
      "Epoch: [1][6300/7150] Elapsed 79m 57s (remain 10m 46s) Loss: 0.0020(0.0058) Grad: 27349.4785  LR: 0.000018  \n",
      "Epoch: [1][6400/7150] Elapsed 81m 11s (remain 9m 30s) Loss: 0.0008(0.0058) Grad: 7606.6582  LR: 0.000018  \n",
      "Epoch: [1][6500/7150] Elapsed 82m 25s (remain 8m 13s) Loss: 0.0029(0.0057) Grad: 24348.7246  LR: 0.000018  \n",
      "Epoch: [1][6600/7150] Elapsed 83m 39s (remain 6m 57s) Loss: 0.0000(0.0056) Grad: 53.6772  LR: 0.000018  \n",
      "Epoch: [1][6700/7150] Elapsed 84m 55s (remain 5m 41s) Loss: 0.0001(0.0056) Grad: 3942.5967  LR: 0.000018  \n",
      "Epoch: [1][6800/7150] Elapsed 86m 11s (remain 4m 25s) Loss: 0.0001(0.0055) Grad: 365.4866  LR: 0.000018  \n",
      "Epoch: [1][6900/7150] Elapsed 87m 27s (remain 3m 9s) Loss: 0.0000(0.0055) Grad: 46.0758  LR: 0.000018  \n",
      "Epoch: [1][7000/7150] Elapsed 88m 43s (remain 1m 53s) Loss: 0.0004(0.0054) Grad: 2713.3035  LR: 0.000018  \n",
      "Epoch: [1][7100/7150] Elapsed 90m 4s (remain 0m 37s) Loss: 0.0001(0.0054) Grad: 130.2894  LR: 0.000018  \n",
      "Epoch: [1][7149/7150] Elapsed 90m 42s (remain 0m 0s) Loss: 0.0001(0.0054) Grad: 997.9949  LR: 0.000018  \n",
      "EVAL: [0/1192] Elapsed 0m 1s (remain 23m 42s) Loss: 0.0017(0.0017) \n",
      "EVAL: [100/1192] Elapsed 0m 33s (remain 5m 56s) Loss: 0.0146(0.0041) \n",
      "EVAL: [200/1192] Elapsed 1m 3s (remain 5m 13s) Loss: 0.0047(0.0034) \n",
      "EVAL: [300/1192] Elapsed 1m 32s (remain 4m 34s) Loss: 0.0018(0.0034) \n",
      "EVAL: [400/1192] Elapsed 2m 2s (remain 4m 0s) Loss: 0.0000(0.0032) \n",
      "EVAL: [500/1192] Elapsed 2m 35s (remain 3m 34s) Loss: 0.0069(0.0029) \n",
      "EVAL: [600/1192] Elapsed 3m 5s (remain 3m 2s) Loss: 0.0079(0.0031) \n",
      "EVAL: [700/1192] Elapsed 3m 34s (remain 2m 30s) Loss: 0.0013(0.0034) \n",
      "EVAL: [800/1192] Elapsed 4m 3s (remain 1m 58s) Loss: 0.0031(0.0033) \n",
      "EVAL: [900/1192] Elapsed 4m 32s (remain 1m 28s) Loss: 0.0007(0.0033) \n",
      "EVAL: [1000/1192] Elapsed 5m 2s (remain 0m 57s) Loss: 0.0000(0.0032) \n",
      "EVAL: [1100/1192] Elapsed 5m 31s (remain 0m 27s) Loss: 0.0042(0.0031) \n",
      "EVAL: [1191/1192] Elapsed 5m 58s (remain 0m 0s) Loss: 0.0001(0.0030) \n",
      "Epoch 1 - avg_train_loss: 0.0054  avg_val_loss: 0.0030  time: 5805s\n",
      "Epoch 1 - Score: 0.8654\n",
      "Epoch 1 - Save Best Score: 0.8654 Model\n",
      "Epoch: [2][0/7150] Elapsed 0m 1s (remain 183m 16s) Loss: 0.0022(0.0022) Grad: 19631.4180  LR: 0.000018  \n",
      "Epoch: [2][100/7150] Elapsed 1m 16s (remain 89m 14s) Loss: 0.0004(0.0018) Grad: 1373.8674  LR: 0.000018  \n",
      "Epoch: [2][200/7150] Elapsed 2m 31s (remain 87m 33s) Loss: 0.0003(0.0017) Grad: 2518.7593  LR: 0.000018  \n",
      "Epoch: [2][300/7150] Elapsed 3m 49s (remain 87m 7s) Loss: 0.0001(0.0016) Grad: 176.1930  LR: 0.000018  \n",
      "Epoch: [2][400/7150] Elapsed 5m 7s (remain 86m 7s) Loss: 0.0008(0.0018) Grad: 497.1908  LR: 0.000018  \n",
      "Epoch: [2][500/7150] Elapsed 6m 23s (remain 84m 45s) Loss: 0.0015(0.0018) Grad: 8284.0078  LR: 0.000017  \n",
      "Epoch: [2][600/7150] Elapsed 7m 39s (remain 83m 30s) Loss: 0.0024(0.0017) Grad: 12264.8535  LR: 0.000017  \n",
      "Epoch: [2][700/7150] Elapsed 8m 58s (remain 82m 30s) Loss: 0.0001(0.0018) Grad: 128.1219  LR: 0.000017  \n",
      "Epoch: [2][800/7150] Elapsed 10m 13s (remain 81m 2s) Loss: 0.0000(0.0018) Grad: 17.9941  LR: 0.000017  \n",
      "Epoch: [2][900/7150] Elapsed 11m 28s (remain 79m 37s) Loss: 0.0001(0.0018) Grad: 108.2516  LR: 0.000017  \n",
      "Epoch: [2][1000/7150] Elapsed 12m 44s (remain 78m 16s) Loss: 0.0001(0.0019) Grad: 188.9246  LR: 0.000017  \n",
      "Epoch: [2][1100/7150] Elapsed 14m 0s (remain 76m 59s) Loss: 0.0002(0.0019) Grad: 246.4677  LR: 0.000017  \n",
      "Epoch: [2][1200/7150] Elapsed 15m 16s (remain 75m 42s) Loss: 0.0083(0.0019) Grad: 11957.7070  LR: 0.000017  \n",
      "Epoch: [2][1300/7150] Elapsed 16m 38s (remain 74m 49s) Loss: 0.0004(0.0019) Grad: 2322.2209  LR: 0.000017  \n",
      "Epoch: [2][1400/7150] Elapsed 17m 56s (remain 73m 38s) Loss: 0.0147(0.0018) Grad: 80979.1875  LR: 0.000017  \n",
      "Epoch: [2][1500/7150] Elapsed 19m 12s (remain 72m 16s) Loss: 0.0018(0.0018) Grad: 16963.6348  LR: 0.000017  \n",
      "Epoch: [2][1600/7150] Elapsed 20m 29s (remain 71m 0s) Loss: 0.0004(0.0018) Grad: 1053.6406  LR: 0.000017  \n",
      "Epoch: [2][1700/7150] Elapsed 21m 45s (remain 69m 41s) Loss: 0.0003(0.0018) Grad: 1760.9467  LR: 0.000017  \n",
      "Epoch: [2][1800/7150] Elapsed 23m 1s (remain 68m 22s) Loss: 0.0001(0.0018) Grad: 355.8750  LR: 0.000017  \n",
      "Epoch: [2][1900/7150] Elapsed 24m 18s (remain 67m 6s) Loss: 0.0052(0.0018) Grad: 8236.3115  LR: 0.000017  \n",
      "Epoch: [2][2000/7150] Elapsed 25m 32s (remain 65m 44s) Loss: 0.0008(0.0018) Grad: 7340.0288  LR: 0.000017  \n",
      "Epoch: [2][2100/7150] Elapsed 26m 47s (remain 64m 22s) Loss: 0.0001(0.0018) Grad: 105.4157  LR: 0.000016  \n",
      "Epoch: [2][2200/7150] Elapsed 28m 2s (remain 63m 4s) Loss: 0.0003(0.0018) Grad: 2166.0339  LR: 0.000016  \n",
      "Epoch: [2][2300/7150] Elapsed 29m 19s (remain 61m 48s) Loss: 0.0010(0.0018) Grad: 4406.0132  LR: 0.000016  \n",
      "Epoch: [2][2400/7150] Elapsed 30m 41s (remain 60m 43s) Loss: 0.0000(0.0017) Grad: 33.6530  LR: 0.000016  \n",
      "Epoch: [2][2500/7150] Elapsed 31m 59s (remain 59m 27s) Loss: 0.0012(0.0017) Grad: 7134.3433  LR: 0.000016  \n",
      "Epoch: [2][2600/7150] Elapsed 33m 12s (remain 58m 4s) Loss: 0.0065(0.0018) Grad: 8344.3164  LR: 0.000016  \n",
      "Epoch: [2][2700/7150] Elapsed 34m 27s (remain 56m 44s) Loss: 0.0020(0.0017) Grad: 12508.6826  LR: 0.000016  \n",
      "Epoch: [2][2800/7150] Elapsed 35m 41s (remain 55m 24s) Loss: 0.0002(0.0017) Grad: 1087.6093  LR: 0.000016  \n",
      "Epoch: [2][2900/7150] Elapsed 36m 57s (remain 54m 7s) Loss: 0.0011(0.0017) Grad: 2899.7085  LR: 0.000016  \n",
      "Epoch: [2][3000/7150] Elapsed 38m 15s (remain 52m 53s) Loss: 0.0036(0.0017) Grad: 2205.8806  LR: 0.000016  \n",
      "Epoch: [2][3100/7150] Elapsed 39m 30s (remain 51m 35s) Loss: 0.0003(0.0017) Grad: 11297.7520  LR: 0.000016  \n",
      "Epoch: [2][3200/7150] Elapsed 40m 46s (remain 50m 18s) Loss: 0.0001(0.0017) Grad: 193.1691  LR: 0.000016  \n",
      "Epoch: [2][3300/7150] Elapsed 42m 6s (remain 49m 6s) Loss: 0.0002(0.0017) Grad: 328.9792  LR: 0.000016  \n",
      "Epoch: [2][3400/7150] Elapsed 43m 21s (remain 47m 48s) Loss: 0.0010(0.0017) Grad: 6293.5425  LR: 0.000016  \n",
      "Epoch: [2][3500/7150] Elapsed 44m 36s (remain 46m 29s) Loss: 0.0001(0.0017) Grad: 202.0693  LR: 0.000016  \n",
      "Epoch: [2][3600/7150] Elapsed 45m 51s (remain 45m 11s) Loss: 0.0008(0.0017) Grad: 2622.1611  LR: 0.000016  \n",
      "Epoch: [2][3700/7150] Elapsed 47m 6s (remain 43m 54s) Loss: 0.0017(0.0017) Grad: 5342.3320  LR: 0.000015  \n",
      "Epoch: [2][3800/7150] Elapsed 48m 23s (remain 42m 38s) Loss: 0.0006(0.0017) Grad: 2530.1257  LR: 0.000015  \n",
      "Epoch: [2][3900/7150] Elapsed 49m 40s (remain 41m 22s) Loss: 0.0004(0.0017) Grad: 913.5396  LR: 0.000015  \n",
      "Epoch: [2][4000/7150] Elapsed 50m 57s (remain 40m 6s) Loss: 0.0001(0.0017) Grad: 159.7704  LR: 0.000015  \n",
      "Epoch: [2][4100/7150] Elapsed 52m 12s (remain 38m 49s) Loss: 0.0001(0.0017) Grad: 761.5888  LR: 0.000015  \n",
      "Epoch: [2][4200/7150] Elapsed 53m 31s (remain 37m 34s) Loss: 0.0007(0.0017) Grad: 4903.4390  LR: 0.000015  \n",
      "Epoch: [2][4300/7150] Elapsed 54m 53s (remain 36m 21s) Loss: 0.0000(0.0017) Grad: 76.2934  LR: 0.000015  \n",
      "Epoch: [2][4400/7150] Elapsed 56m 9s (remain 35m 4s) Loss: 0.0000(0.0017) Grad: 53.9119  LR: 0.000015  \n",
      "Epoch: [2][4500/7150] Elapsed 57m 24s (remain 33m 47s) Loss: 0.0001(0.0017) Grad: 307.3876  LR: 0.000015  \n",
      "Epoch: [2][4600/7150] Elapsed 58m 43s (remain 32m 32s) Loss: 0.0005(0.0017) Grad: 2622.2791  LR: 0.000015  \n",
      "Epoch: [2][4700/7150] Elapsed 59m 58s (remain 31m 14s) Loss: 0.0113(0.0017) Grad: 31960.8359  LR: 0.000015  \n",
      "Epoch: [2][4800/7150] Elapsed 61m 14s (remain 29m 57s) Loss: 0.0019(0.0017) Grad: 7847.9902  LR: 0.000015  \n",
      "Epoch: [2][4900/7150] Elapsed 62m 30s (remain 28m 41s) Loss: 0.0001(0.0017) Grad: 2874.0129  LR: 0.000015  \n",
      "Epoch: [2][5000/7150] Elapsed 63m 47s (remain 27m 24s) Loss: 0.0005(0.0017) Grad: 5654.3486  LR: 0.000015  \n",
      "Epoch: [2][5100/7150] Elapsed 65m 2s (remain 26m 7s) Loss: 0.0018(0.0017) Grad: 18264.1348  LR: 0.000015  \n",
      "Epoch: [2][5200/7150] Elapsed 66m 16s (remain 24m 50s) Loss: 0.0019(0.0017) Grad: 4627.1143  LR: 0.000015  \n",
      "Epoch: [2][5300/7150] Elapsed 67m 31s (remain 23m 33s) Loss: 0.0015(0.0017) Grad: 21137.0332  LR: 0.000014  \n",
      "Epoch: [2][5400/7150] Elapsed 68m 47s (remain 22m 16s) Loss: 0.0000(0.0017) Grad: 200.3455  LR: 0.000014  \n",
      "Epoch: [2][5500/7150] Elapsed 70m 3s (remain 21m 0s) Loss: 0.0000(0.0017) Grad: 21.0177  LR: 0.000014  \n",
      "Epoch: [2][5600/7150] Elapsed 71m 18s (remain 19m 43s) Loss: 0.0028(0.0017) Grad: 14153.0557  LR: 0.000014  \n",
      "Epoch: [2][5700/7150] Elapsed 72m 35s (remain 18m 27s) Loss: 0.0030(0.0017) Grad: 12701.0127  LR: 0.000014  \n",
      "Epoch: [2][5800/7150] Elapsed 73m 50s (remain 17m 10s) Loss: 0.0000(0.0017) Grad: 129.3515  LR: 0.000014  \n",
      "Epoch: [2][5900/7150] Elapsed 75m 4s (remain 15m 53s) Loss: 0.0000(0.0016) Grad: 37.8541  LR: 0.000014  \n",
      "Epoch: [2][6000/7150] Elapsed 76m 19s (remain 14m 36s) Loss: 0.0001(0.0017) Grad: 465.1728  LR: 0.000014  \n",
      "Epoch: [2][6100/7150] Elapsed 77m 40s (remain 13m 21s) Loss: 0.0059(0.0016) Grad: 11197.2949  LR: 0.000014  \n",
      "Epoch: [2][6200/7150] Elapsed 79m 2s (remain 12m 5s) Loss: 0.0004(0.0016) Grad: 4632.9858  LR: 0.000014  \n",
      "Epoch: [2][6300/7150] Elapsed 80m 23s (remain 10m 49s) Loss: 0.0000(0.0016) Grad: 151.3988  LR: 0.000014  \n",
      "Epoch: [2][6400/7150] Elapsed 81m 45s (remain 9m 34s) Loss: 0.0010(0.0016) Grad: 4793.4380  LR: 0.000014  \n",
      "Epoch: [2][6500/7150] Elapsed 83m 9s (remain 8m 18s) Loss: 0.0000(0.0016) Grad: 60.5257  LR: 0.000014  \n",
      "Epoch: [2][6600/7150] Elapsed 84m 32s (remain 7m 1s) Loss: 0.0018(0.0016) Grad: 6391.9072  LR: 0.000014  \n",
      "Epoch: [2][6700/7150] Elapsed 85m 54s (remain 5m 45s) Loss: 0.0009(0.0016) Grad: 3234.4214  LR: 0.000014  \n",
      "Epoch: [2][6800/7150] Elapsed 87m 17s (remain 4m 28s) Loss: 0.0003(0.0016) Grad: 2233.7712  LR: 0.000014  \n",
      "Epoch: [2][6900/7150] Elapsed 88m 35s (remain 3m 11s) Loss: 0.0000(0.0016) Grad: 135.5624  LR: 0.000013  \n",
      "Epoch: [2][7000/7150] Elapsed 89m 53s (remain 1m 54s) Loss: 0.0018(0.0016) Grad: 9515.3340  LR: 0.000013  \n",
      "Epoch: [2][7100/7150] Elapsed 91m 9s (remain 0m 37s) Loss: 0.0000(0.0016) Grad: 142.6794  LR: 0.000013  \n",
      "Epoch: [2][7149/7150] Elapsed 91m 46s (remain 0m 0s) Loss: 0.0066(0.0016) Grad: 28632.0488  LR: 0.000013  \n",
      "EVAL: [0/1192] Elapsed 0m 1s (remain 22m 50s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 30s (remain 5m 25s) Loss: 0.0151(0.0028) \n",
      "EVAL: [200/1192] Elapsed 1m 0s (remain 4m 56s) Loss: 0.0010(0.0022) \n",
      "EVAL: [300/1192] Elapsed 1m 29s (remain 4m 24s) Loss: 0.0015(0.0024) \n",
      "EVAL: [400/1192] Elapsed 1m 58s (remain 3m 53s) Loss: 0.0000(0.0023) \n",
      "EVAL: [500/1192] Elapsed 2m 27s (remain 3m 23s) Loss: 0.0116(0.0022) \n",
      "EVAL: [600/1192] Elapsed 2m 56s (remain 2m 53s) Loss: 0.0049(0.0023) \n",
      "EVAL: [700/1192] Elapsed 3m 26s (remain 2m 24s) Loss: 0.0025(0.0027) \n",
      "EVAL: [800/1192] Elapsed 3m 56s (remain 1m 55s) Loss: 0.0037(0.0027) \n",
      "EVAL: [900/1192] Elapsed 4m 25s (remain 1m 25s) Loss: 0.0003(0.0027) \n",
      "EVAL: [1000/1192] Elapsed 4m 55s (remain 0m 56s) Loss: 0.0000(0.0027) \n",
      "EVAL: [1100/1192] Elapsed 5m 24s (remain 0m 26s) Loss: 0.0066(0.0026) \n",
      "EVAL: [1191/1192] Elapsed 5m 51s (remain 0m 0s) Loss: 0.0000(0.0025) \n",
      "Epoch 2 - avg_train_loss: 0.0016  avg_val_loss: 0.0025  time: 5862s\n",
      "Epoch 2 - Score: 0.8877\n",
      "Epoch 2 - Save Best Score: 0.8877 Model\n",
      "Epoch: [3][0/7150] Elapsed 0m 1s (remain 189m 7s) Loss: 0.0000(0.0000) Grad: 79.2711  LR: 0.000013  \n",
      "Epoch: [3][100/7150] Elapsed 1m 15s (remain 88m 14s) Loss: 0.0001(0.0009) Grad: 2890.2559  LR: 0.000013  \n",
      "Epoch: [3][200/7150] Elapsed 2m 30s (remain 86m 41s) Loss: 0.0020(0.0008) Grad: 13032.4170  LR: 0.000013  \n",
      "Epoch: [3][300/7150] Elapsed 3m 46s (remain 85m 48s) Loss: 0.0000(0.0007) Grad: 12.2663  LR: 0.000013  \n",
      "Epoch: [3][400/7150] Elapsed 5m 0s (remain 84m 18s) Loss: 0.0002(0.0010) Grad: 2142.3179  LR: 0.000013  \n",
      "Epoch: [3][500/7150] Elapsed 6m 15s (remain 83m 2s) Loss: 0.0000(0.0010) Grad: 95.2378  LR: 0.000013  \n",
      "Epoch: [3][600/7150] Elapsed 7m 31s (remain 81m 56s) Loss: 0.0006(0.0011) Grad: 4406.1064  LR: 0.000013  \n",
      "Epoch: [3][700/7150] Elapsed 8m 48s (remain 81m 0s) Loss: 0.0011(0.0011) Grad: 13180.3311  LR: 0.000013  \n",
      "Epoch: [3][800/7150] Elapsed 10m 10s (remain 80m 37s) Loss: 0.0000(0.0011) Grad: 61.8801  LR: 0.000013  \n",
      "Epoch: [3][900/7150] Elapsed 11m 27s (remain 79m 26s) Loss: 0.0004(0.0011) Grad: 1038.6592  LR: 0.000013  \n",
      "Epoch: [3][1000/7150] Elapsed 12m 41s (remain 77m 59s) Loss: 0.0000(0.0011) Grad: 34.5002  LR: 0.000013  \n",
      "Epoch: [3][1100/7150] Elapsed 13m 56s (remain 76m 35s) Loss: 0.0002(0.0011) Grad: 431.3734  LR: 0.000013  \n",
      "Epoch: [3][1200/7150] Elapsed 15m 10s (remain 75m 9s) Loss: 0.0000(0.0011) Grad: 52.9297  LR: 0.000013  \n",
      "Epoch: [3][1300/7150] Elapsed 16m 24s (remain 73m 47s) Loss: 0.0001(0.0011) Grad: 97.6314  LR: 0.000013  \n",
      "Epoch: [3][1400/7150] Elapsed 17m 39s (remain 72m 28s) Loss: 0.0003(0.0012) Grad: 1159.7734  LR: 0.000012  \n",
      "Epoch: [3][1500/7150] Elapsed 18m 54s (remain 71m 10s) Loss: 0.0000(0.0011) Grad: 18.7732  LR: 0.000012  \n",
      "Epoch: [3][1600/7150] Elapsed 20m 9s (remain 69m 50s) Loss: 0.0000(0.0011) Grad: 34.3322  LR: 0.000012  \n",
      "Epoch: [3][1700/7150] Elapsed 21m 23s (remain 68m 32s) Loss: 0.0016(0.0011) Grad: 7173.2744  LR: 0.000012  \n",
      "Epoch: [3][1800/7150] Elapsed 22m 44s (remain 67m 32s) Loss: 0.0000(0.0011) Grad: 29.6927  LR: 0.000012  \n",
      "Epoch: [3][1900/7150] Elapsed 23m 59s (remain 66m 15s) Loss: 0.0012(0.0011) Grad: 3289.3350  LR: 0.000012  \n",
      "Epoch: [3][2000/7150] Elapsed 25m 14s (remain 64m 56s) Loss: 0.0000(0.0012) Grad: 28.6456  LR: 0.000012  \n",
      "Epoch: [3][2100/7150] Elapsed 26m 28s (remain 63m 37s) Loss: 0.0000(0.0012) Grad: 7.6038  LR: 0.000012  \n",
      "Epoch: [3][2200/7150] Elapsed 27m 44s (remain 62m 22s) Loss: 0.0025(0.0011) Grad: 6448.1655  LR: 0.000012  \n",
      "Epoch: [3][2300/7150] Elapsed 29m 0s (remain 61m 8s) Loss: 0.0000(0.0011) Grad: 17.6085  LR: 0.000012  \n",
      "Epoch: [3][2400/7150] Elapsed 30m 15s (remain 59m 50s) Loss: 0.0000(0.0011) Grad: 45.2300  LR: 0.000012  \n",
      "Epoch: [3][2500/7150] Elapsed 31m 29s (remain 58m 32s) Loss: 0.0014(0.0011) Grad: 2592.3130  LR: 0.000012  \n",
      "Epoch: [3][2600/7150] Elapsed 32m 43s (remain 57m 13s) Loss: 0.0001(0.0011) Grad: 548.2956  LR: 0.000012  \n",
      "Epoch: [3][2700/7150] Elapsed 33m 58s (remain 55m 58s) Loss: 0.0022(0.0011) Grad: 25735.8555  LR: 0.000012  \n",
      "Epoch: [3][2800/7150] Elapsed 35m 16s (remain 54m 45s) Loss: 0.0021(0.0011) Grad: 1886.3466  LR: 0.000012  \n",
      "Epoch: [3][2900/7150] Elapsed 36m 33s (remain 53m 32s) Loss: 0.0102(0.0011) Grad: 9122.6738  LR: 0.000012  \n",
      "Epoch: [3][3000/7150] Elapsed 37m 49s (remain 52m 17s) Loss: 0.0000(0.0011) Grad: 85.1271  LR: 0.000011  \n",
      "Epoch: [3][3100/7150] Elapsed 39m 4s (remain 51m 1s) Loss: 0.0001(0.0011) Grad: 288.8414  LR: 0.000011  \n",
      "Epoch: [3][3200/7150] Elapsed 40m 20s (remain 49m 45s) Loss: 0.0003(0.0011) Grad: 983.8383  LR: 0.000011  \n",
      "Epoch: [3][3300/7150] Elapsed 41m 35s (remain 48m 29s) Loss: 0.0000(0.0011) Grad: 176.7663  LR: 0.000011  \n",
      "Epoch: [3][3400/7150] Elapsed 42m 51s (remain 47m 14s) Loss: 0.0002(0.0011) Grad: 903.0593  LR: 0.000011  \n",
      "Epoch: [3][3500/7150] Elapsed 44m 9s (remain 46m 1s) Loss: 0.0000(0.0011) Grad: 89.6899  LR: 0.000011  \n",
      "Epoch: [3][3600/7150] Elapsed 45m 23s (remain 44m 44s) Loss: 0.0001(0.0011) Grad: 216.3708  LR: 0.000011  \n",
      "Epoch: [3][3700/7150] Elapsed 46m 37s (remain 43m 27s) Loss: 0.0001(0.0012) Grad: 200.4928  LR: 0.000011  \n",
      "Epoch: [3][3800/7150] Elapsed 47m 54s (remain 42m 12s) Loss: 0.0000(0.0012) Grad: 78.6676  LR: 0.000011  \n",
      "Epoch: [3][3900/7150] Elapsed 49m 9s (remain 40m 56s) Loss: 0.0053(0.0011) Grad: 102081.5312  LR: 0.000011  \n",
      "Epoch: [3][4000/7150] Elapsed 50m 24s (remain 39m 40s) Loss: 0.0002(0.0011) Grad: 1189.8273  LR: 0.000011  \n",
      "Epoch: [3][4100/7150] Elapsed 51m 40s (remain 38m 25s) Loss: 0.0000(0.0011) Grad: 123.2046  LR: 0.000011  \n",
      "Epoch: [3][4200/7150] Elapsed 52m 57s (remain 37m 10s) Loss: 0.0022(0.0011) Grad: 20396.4492  LR: 0.000011  \n",
      "Epoch: [3][4300/7150] Elapsed 54m 13s (remain 35m 55s) Loss: 0.0000(0.0011) Grad: 330.8086  LR: 0.000011  \n",
      "Epoch: [3][4400/7150] Elapsed 55m 29s (remain 34m 39s) Loss: 0.0000(0.0012) Grad: 26.3434  LR: 0.000011  \n",
      "Epoch: [3][4500/7150] Elapsed 56m 44s (remain 33m 23s) Loss: 0.0005(0.0012) Grad: 13981.1816  LR: 0.000011  \n",
      "Epoch: [3][4600/7150] Elapsed 57m 59s (remain 32m 7s) Loss: 0.0000(0.0012) Grad: 30.7478  LR: 0.000010  \n",
      "Epoch: [3][4700/7150] Elapsed 59m 15s (remain 30m 52s) Loss: 0.0001(0.0012) Grad: 965.7343  LR: 0.000010  \n",
      "Epoch: [3][4800/7150] Elapsed 60m 29s (remain 29m 35s) Loss: 0.0002(0.0012) Grad: 783.7588  LR: 0.000010  \n",
      "Epoch: [3][4900/7150] Elapsed 61m 43s (remain 28m 19s) Loss: 0.0002(0.0012) Grad: 5891.5332  LR: 0.000010  \n",
      "Epoch: [3][5000/7150] Elapsed 62m 57s (remain 27m 3s) Loss: 0.0031(0.0012) Grad: 3690.8320  LR: 0.000010  \n",
      "Epoch: [3][5100/7150] Elapsed 64m 11s (remain 25m 47s) Loss: 0.0005(0.0012) Grad: 8139.6494  LR: 0.000010  \n",
      "Epoch: [3][5200/7150] Elapsed 65m 26s (remain 24m 31s) Loss: 0.0000(0.0012) Grad: 171.3536  LR: 0.000010  \n",
      "Epoch: [3][5300/7150] Elapsed 66m 43s (remain 23m 16s) Loss: 0.0000(0.0012) Grad: 78.3148  LR: 0.000010  \n",
      "Epoch: [3][5400/7150] Elapsed 67m 58s (remain 22m 0s) Loss: 0.0045(0.0012) Grad: 15591.0576  LR: 0.000010  \n",
      "Epoch: [3][5500/7150] Elapsed 69m 13s (remain 20m 44s) Loss: 0.0000(0.0012) Grad: 44.9927  LR: 0.000010  \n",
      "Epoch: [3][5600/7150] Elapsed 70m 29s (remain 19m 29s) Loss: 0.0004(0.0012) Grad: 4991.5566  LR: 0.000010  \n",
      "Epoch: [3][5700/7150] Elapsed 71m 44s (remain 18m 14s) Loss: 0.0000(0.0012) Grad: 197.3309  LR: 0.000010  \n",
      "Epoch: [3][5800/7150] Elapsed 72m 58s (remain 16m 58s) Loss: 0.0003(0.0012) Grad: 2732.4446  LR: 0.000010  \n",
      "Epoch: [3][5900/7150] Elapsed 74m 14s (remain 15m 42s) Loss: 0.0000(0.0012) Grad: 64.1105  LR: 0.000010  \n",
      "Epoch: [3][6000/7150] Elapsed 75m 36s (remain 14m 28s) Loss: 0.0002(0.0012) Grad: 5577.2188  LR: 0.000010  \n",
      "Epoch: [3][6100/7150] Elapsed 76m 52s (remain 13m 13s) Loss: 0.0000(0.0012) Grad: 21.3838  LR: 0.000010  \n",
      "Epoch: [3][6200/7150] Elapsed 78m 6s (remain 11m 57s) Loss: 0.0002(0.0012) Grad: 3649.0332  LR: 0.000009  \n",
      "Epoch: [3][6300/7150] Elapsed 79m 20s (remain 10m 41s) Loss: 0.0000(0.0012) Grad: 65.9391  LR: 0.000009  \n",
      "Epoch: [3][6400/7150] Elapsed 80m 36s (remain 9m 25s) Loss: 0.0000(0.0012) Grad: 71.7197  LR: 0.000009  \n",
      "Epoch: [3][6500/7150] Elapsed 81m 53s (remain 8m 10s) Loss: 0.0003(0.0012) Grad: 3670.5239  LR: 0.000009  \n",
      "Epoch: [3][6600/7150] Elapsed 83m 8s (remain 6m 54s) Loss: 0.0000(0.0012) Grad: 40.7480  LR: 0.000009  \n",
      "Epoch: [3][6700/7150] Elapsed 84m 22s (remain 5m 39s) Loss: 0.0002(0.0011) Grad: 456.4971  LR: 0.000009  \n",
      "Epoch: [3][6800/7150] Elapsed 85m 37s (remain 4m 23s) Loss: 0.0000(0.0011) Grad: 95.6474  LR: 0.000009  \n",
      "Epoch: [3][6900/7150] Elapsed 86m 52s (remain 3m 8s) Loss: 0.0000(0.0011) Grad: 182.0998  LR: 0.000009  \n",
      "Epoch: [3][7000/7150] Elapsed 88m 9s (remain 1m 52s) Loss: 0.0000(0.0011) Grad: 62.8793  LR: 0.000009  \n",
      "Epoch: [3][7100/7150] Elapsed 89m 26s (remain 0m 37s) Loss: 0.0003(0.0011) Grad: 1671.5682  LR: 0.000009  \n",
      "Epoch: [3][7149/7150] Elapsed 90m 4s (remain 0m 0s) Loss: 0.0000(0.0011) Grad: 1002.0587  LR: 0.000009  \n",
      "EVAL: [0/1192] Elapsed 0m 1s (remain 22m 43s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 30s (remain 5m 29s) Loss: 0.0180(0.0022) \n",
      "EVAL: [200/1192] Elapsed 0m 59s (remain 4m 53s) Loss: 0.0008(0.0020) \n",
      "EVAL: [300/1192] Elapsed 1m 29s (remain 4m 24s) Loss: 0.0031(0.0024) \n",
      "EVAL: [400/1192] Elapsed 1m 59s (remain 3m 56s) Loss: 0.0000(0.0023) \n",
      "EVAL: [500/1192] Elapsed 2m 29s (remain 3m 26s) Loss: 0.0178(0.0023) \n",
      "EVAL: [600/1192] Elapsed 2m 58s (remain 2m 55s) Loss: 0.0065(0.0025) \n",
      "EVAL: [700/1192] Elapsed 3m 27s (remain 2m 25s) Loss: 0.0015(0.0029) \n",
      "EVAL: [800/1192] Elapsed 3m 56s (remain 1m 55s) Loss: 0.0031(0.0030) \n",
      "EVAL: [900/1192] Elapsed 4m 27s (remain 1m 26s) Loss: 0.0016(0.0030) \n",
      "EVAL: [1000/1192] Elapsed 4m 56s (remain 0m 56s) Loss: 0.0000(0.0029) \n",
      "EVAL: [1100/1192] Elapsed 5m 25s (remain 0m 26s) Loss: 0.0072(0.0028) \n",
      "EVAL: [1191/1192] Elapsed 5m 52s (remain 0m 0s) Loss: 0.0000(0.0027) \n",
      "Epoch 3 - avg_train_loss: 0.0011  avg_val_loss: 0.0027  time: 5760s\n",
      "Epoch 3 - Score: 0.8947\n",
      "Epoch 3 - Save Best Score: 0.8947 Model\n",
      "Epoch: [4][0/7150] Elapsed 0m 1s (remain 191m 14s) Loss: 0.0000(0.0000) Grad: 11.6578  LR: 0.000009  \n",
      "Epoch: [4][100/7150] Elapsed 1m 16s (remain 89m 28s) Loss: 0.0011(0.0012) Grad: 2712.7273  LR: 0.000009  \n",
      "Epoch: [4][200/7150] Elapsed 2m 36s (remain 90m 9s) Loss: 0.0000(0.0009) Grad: 134.0482  LR: 0.000009  \n",
      "Epoch: [4][300/7150] Elapsed 3m 52s (remain 87m 59s) Loss: 0.0001(0.0009) Grad: 405.5294  LR: 0.000009  \n",
      "Epoch: [4][400/7150] Elapsed 5m 6s (remain 85m 50s) Loss: 0.0000(0.0009) Grad: 5.3603  LR: 0.000009  \n",
      "Epoch: [4][500/7150] Elapsed 6m 20s (remain 84m 8s) Loss: 0.0002(0.0008) Grad: 386.9864  LR: 0.000009  \n",
      "Epoch: [4][600/7150] Elapsed 7m 34s (remain 82m 35s) Loss: 0.0050(0.0008) Grad: 11954.8438  LR: 0.000009  \n",
      "Epoch: [4][700/7150] Elapsed 8m 50s (remain 81m 23s) Loss: 0.0000(0.0008) Grad: 15.2035  LR: 0.000008  \n",
      "Epoch: [4][800/7150] Elapsed 10m 5s (remain 79m 59s) Loss: 0.0000(0.0009) Grad: 3.6279  LR: 0.000008  \n",
      "Epoch: [4][900/7150] Elapsed 11m 19s (remain 78m 34s) Loss: 0.0102(0.0009) Grad: 8248.7559  LR: 0.000008  \n",
      "Epoch: [4][1000/7150] Elapsed 12m 34s (remain 77m 14s) Loss: 0.0000(0.0009) Grad: 10.3343  LR: 0.000008  \n",
      "Epoch: [4][1100/7150] Elapsed 13m 51s (remain 76m 9s) Loss: 0.0000(0.0009) Grad: 14.3991  LR: 0.000008  \n",
      "Epoch: [4][1200/7150] Elapsed 15m 6s (remain 74m 52s) Loss: 0.0013(0.0009) Grad: 2162.2515  LR: 0.000008  \n",
      "Epoch: [4][1300/7150] Elapsed 16m 21s (remain 73m 33s) Loss: 0.0001(0.0009) Grad: 498.4468  LR: 0.000008  \n",
      "Epoch: [4][1400/7150] Elapsed 17m 36s (remain 72m 15s) Loss: 0.0000(0.0009) Grad: 112.2569  LR: 0.000008  \n",
      "Epoch: [4][1500/7150] Elapsed 18m 52s (remain 71m 3s) Loss: 0.0012(0.0009) Grad: 10359.1953  LR: 0.000008  \n",
      "Epoch: [4][1600/7150] Elapsed 20m 10s (remain 69m 56s) Loss: 0.0000(0.0008) Grad: 55.8276  LR: 0.000008  \n",
      "Epoch: [4][1700/7150] Elapsed 21m 25s (remain 68m 38s) Loss: 0.0001(0.0008) Grad: 372.5373  LR: 0.000008  \n",
      "Epoch: [4][1800/7150] Elapsed 22m 41s (remain 67m 22s) Loss: 0.0000(0.0009) Grad: 14.9923  LR: 0.000008  \n",
      "Epoch: [4][1900/7150] Elapsed 23m 59s (remain 66m 13s) Loss: 0.0010(0.0009) Grad: 5189.4448  LR: 0.000008  \n",
      "Epoch: [4][2000/7150] Elapsed 25m 13s (remain 64m 54s) Loss: 0.0000(0.0009) Grad: 63.1459  LR: 0.000008  \n",
      "Epoch: [4][2100/7150] Elapsed 26m 27s (remain 63m 35s) Loss: 0.0000(0.0008) Grad: 37.0067  LR: 0.000008  \n",
      "Epoch: [4][2200/7150] Elapsed 27m 41s (remain 62m 16s) Loss: 0.0000(0.0008) Grad: 2.1343  LR: 0.000008  \n",
      "Epoch: [4][2300/7150] Elapsed 28m 56s (remain 61m 0s) Loss: 0.0002(0.0008) Grad: 406.2387  LR: 0.000007  \n",
      "Epoch: [4][2400/7150] Elapsed 30m 15s (remain 59m 50s) Loss: 0.0000(0.0008) Grad: 1.6675  LR: 0.000007  \n",
      "Epoch: [4][2500/7150] Elapsed 31m 30s (remain 58m 34s) Loss: 0.0001(0.0008) Grad: 241.3253  LR: 0.000007  \n",
      "Epoch: [4][2600/7150] Elapsed 32m 44s (remain 57m 15s) Loss: 0.0000(0.0008) Grad: 228.7743  LR: 0.000007  \n",
      "Epoch: [4][2700/7150] Elapsed 33m 58s (remain 55m 57s) Loss: 0.0000(0.0008) Grad: 10.5963  LR: 0.000007  \n",
      "Epoch: [4][2800/7150] Elapsed 35m 11s (remain 54m 38s) Loss: 0.0000(0.0008) Grad: 27.2033  LR: 0.000007  \n",
      "Epoch: [4][2900/7150] Elapsed 36m 26s (remain 53m 22s) Loss: 0.0126(0.0009) Grad: 18070.2793  LR: 0.000007  \n",
      "Epoch: [4][3000/7150] Elapsed 37m 42s (remain 52m 8s) Loss: 0.0008(0.0009) Grad: 3577.5066  LR: 0.000007  \n",
      "Epoch: [4][3100/7150] Elapsed 38m 57s (remain 50m 51s) Loss: 0.0016(0.0009) Grad: 10020.6680  LR: 0.000007  \n",
      "Epoch: [4][3200/7150] Elapsed 40m 12s (remain 49m 36s) Loss: 0.0000(0.0009) Grad: 89.0290  LR: 0.000007  \n",
      "Epoch: [4][3300/7150] Elapsed 41m 26s (remain 48m 19s) Loss: 0.0000(0.0009) Grad: 131.6245  LR: 0.000007  \n",
      "Epoch: [4][3400/7150] Elapsed 42m 40s (remain 47m 2s) Loss: 0.0000(0.0009) Grad: 12.6369  LR: 0.000007  \n",
      "Epoch: [4][3500/7150] Elapsed 43m 54s (remain 45m 46s) Loss: 0.0002(0.0009) Grad: 591.7355  LR: 0.000007  \n",
      "Epoch: [4][3600/7150] Elapsed 45m 11s (remain 44m 32s) Loss: 0.0003(0.0009) Grad: 1184.3368  LR: 0.000007  \n",
      "Epoch: [4][3700/7150] Elapsed 46m 25s (remain 43m 15s) Loss: 0.0001(0.0009) Grad: 669.3581  LR: 0.000007  \n",
      "Epoch: [4][3800/7150] Elapsed 47m 39s (remain 41m 59s) Loss: 0.0000(0.0009) Grad: 122.7432  LR: 0.000007  \n",
      "Epoch: [4][3900/7150] Elapsed 48m 53s (remain 40m 43s) Loss: 0.0001(0.0009) Grad: 652.8391  LR: 0.000006  \n",
      "Epoch: [4][4000/7150] Elapsed 50m 7s (remain 39m 26s) Loss: 0.0011(0.0009) Grad: 6374.2803  LR: 0.000006  \n",
      "Epoch: [4][4100/7150] Elapsed 51m 21s (remain 38m 11s) Loss: 0.0005(0.0009) Grad: 2187.9829  LR: 0.000006  \n",
      "Epoch: [4][4200/7150] Elapsed 52m 37s (remain 36m 56s) Loss: 0.0000(0.0009) Grad: 50.9368  LR: 0.000006  \n",
      "Epoch: [4][4300/7150] Elapsed 53m 53s (remain 35m 41s) Loss: 0.0009(0.0009) Grad: 15765.3359  LR: 0.000006  \n",
      "Epoch: [4][4400/7150] Elapsed 55m 7s (remain 34m 25s) Loss: 0.0018(0.0009) Grad: 75161.9062  LR: 0.000006  \n",
      "Epoch: [4][4500/7150] Elapsed 56m 21s (remain 33m 10s) Loss: 0.0273(0.0009) Grad: 18064.1992  LR: 0.000006  \n",
      "Epoch: [4][4600/7150] Elapsed 57m 36s (remain 31m 54s) Loss: 0.0040(0.0009) Grad: 12492.8770  LR: 0.000006  \n",
      "Epoch: [4][4700/7150] Elapsed 58m 52s (remain 30m 40s) Loss: 0.0000(0.0009) Grad: 30.4622  LR: 0.000006  \n",
      "Epoch: [4][4800/7150] Elapsed 60m 8s (remain 29m 25s) Loss: 0.0000(0.0009) Grad: 90.6370  LR: 0.000006  \n",
      "Epoch: [4][4900/7150] Elapsed 61m 24s (remain 28m 10s) Loss: 0.0000(0.0009) Grad: 32.4574  LR: 0.000006  \n",
      "Epoch: [4][5000/7150] Elapsed 62m 38s (remain 26m 55s) Loss: 0.0000(0.0009) Grad: 143.9798  LR: 0.000006  \n",
      "Epoch: [4][5100/7150] Elapsed 63m 53s (remain 25m 40s) Loss: 0.0001(0.0009) Grad: 954.1749  LR: 0.000006  \n",
      "Epoch: [4][5200/7150] Elapsed 65m 10s (remain 24m 25s) Loss: 0.0000(0.0009) Grad: 206.2953  LR: 0.000006  \n",
      "Epoch: [4][5300/7150] Elapsed 66m 26s (remain 23m 10s) Loss: 0.0000(0.0009) Grad: 152.8404  LR: 0.000006  \n",
      "Epoch: [4][5400/7150] Elapsed 67m 42s (remain 21m 55s) Loss: 0.0000(0.0009) Grad: 217.1771  LR: 0.000006  \n",
      "Epoch: [4][5500/7150] Elapsed 68m 58s (remain 20m 40s) Loss: 0.0000(0.0009) Grad: 368.2787  LR: 0.000005  \n",
      "Epoch: [4][5600/7150] Elapsed 70m 16s (remain 19m 26s) Loss: 0.0000(0.0009) Grad: 151.4886  LR: 0.000005  \n",
      "Epoch: [4][5700/7150] Elapsed 71m 37s (remain 18m 12s) Loss: 0.0000(0.0009) Grad: 263.7202  LR: 0.000005  \n",
      "Epoch: [4][5800/7150] Elapsed 72m 52s (remain 16m 56s) Loss: 0.0013(0.0009) Grad: 60645.6719  LR: 0.000005  \n",
      "Epoch: [4][5900/7150] Elapsed 74m 7s (remain 15m 41s) Loss: 0.0000(0.0009) Grad: 38.5292  LR: 0.000005  \n",
      "Epoch: [4][6000/7150] Elapsed 75m 22s (remain 14m 25s) Loss: 0.0001(0.0009) Grad: 1238.8855  LR: 0.000005  \n",
      "Epoch: [4][6100/7150] Elapsed 76m 38s (remain 13m 10s) Loss: 0.0003(0.0009) Grad: 2132.8269  LR: 0.000005  \n",
      "Epoch: [4][6200/7150] Elapsed 77m 54s (remain 11m 55s) Loss: 0.0082(0.0009) Grad: 22152.6426  LR: 0.000005  \n",
      "Epoch: [4][6300/7150] Elapsed 79m 9s (remain 10m 39s) Loss: 0.0170(0.0009) Grad: 51024.0742  LR: 0.000005  \n",
      "Epoch: [4][6400/7150] Elapsed 80m 24s (remain 9m 24s) Loss: 0.0000(0.0009) Grad: 27.1514  LR: 0.000005  \n",
      "Epoch: [4][6500/7150] Elapsed 81m 44s (remain 8m 9s) Loss: 0.0029(0.0009) Grad: 4729.4429  LR: 0.000005  \n",
      "Epoch: [4][6600/7150] Elapsed 83m 0s (remain 6m 54s) Loss: 0.0001(0.0009) Grad: 1008.7642  LR: 0.000005  \n",
      "Epoch: [4][6700/7150] Elapsed 84m 15s (remain 5m 38s) Loss: 0.0005(0.0009) Grad: 9403.9785  LR: 0.000005  \n",
      "Epoch: [4][6800/7150] Elapsed 85m 30s (remain 4m 23s) Loss: 0.0001(0.0009) Grad: 926.5665  LR: 0.000005  \n",
      "Epoch: [4][6900/7150] Elapsed 86m 44s (remain 3m 7s) Loss: 0.0021(0.0009) Grad: 10836.8066  LR: 0.000005  \n",
      "Epoch: [4][7000/7150] Elapsed 87m 59s (remain 1m 52s) Loss: 0.0078(0.0009) Grad: 51922.2227  LR: 0.000005  \n",
      "Epoch: [4][7100/7150] Elapsed 89m 15s (remain 0m 36s) Loss: 0.0053(0.0009) Grad: 16677.0605  LR: 0.000004  \n",
      "Epoch: [4][7149/7150] Elapsed 89m 52s (remain 0m 0s) Loss: 0.0000(0.0009) Grad: 810.2365  LR: 0.000004  \n",
      "EVAL: [0/1192] Elapsed 0m 1s (remain 20m 33s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 29s (remain 5m 23s) Loss: 0.0182(0.0028) \n",
      "EVAL: [200/1192] Elapsed 0m 59s (remain 4m 54s) Loss: 0.0037(0.0024) \n",
      "EVAL: [300/1192] Elapsed 1m 28s (remain 4m 22s) Loss: 0.0039(0.0028) \n",
      "EVAL: [400/1192] Elapsed 1m 58s (remain 3m 53s) Loss: 0.0000(0.0027) \n",
      "EVAL: [500/1192] Elapsed 2m 27s (remain 3m 23s) Loss: 0.0184(0.0027) \n",
      "EVAL: [600/1192] Elapsed 2m 56s (remain 2m 53s) Loss: 0.0073(0.0028) \n",
      "EVAL: [700/1192] Elapsed 3m 25s (remain 2m 24s) Loss: 0.0017(0.0033) \n",
      "EVAL: [800/1192] Elapsed 3m 54s (remain 1m 54s) Loss: 0.0048(0.0033) \n",
      "EVAL: [900/1192] Elapsed 4m 25s (remain 1m 25s) Loss: 0.0028(0.0034) \n",
      "EVAL: [1000/1192] Elapsed 4m 54s (remain 0m 56s) Loss: 0.0000(0.0034) \n",
      "EVAL: [1100/1192] Elapsed 5m 23s (remain 0m 26s) Loss: 0.0079(0.0033) \n",
      "EVAL: [1191/1192] Elapsed 5m 49s (remain 0m 0s) Loss: 0.0000(0.0032) \n",
      "Epoch 4 - avg_train_loss: 0.0009  avg_val_loss: 0.0032  time: 5746s\n",
      "Epoch 4 - Score: 0.8918\n",
      "Epoch: [5][0/7150] Elapsed 0m 1s (remain 186m 55s) Loss: 0.0000(0.0000) Grad: 124.9804  LR: 0.000004  \n",
      "Epoch: [5][100/7150] Elapsed 1m 19s (remain 92m 26s) Loss: 0.0000(0.0006) Grad: 31.4124  LR: 0.000004  \n",
      "Epoch: [5][200/7150] Elapsed 2m 33s (remain 88m 21s) Loss: 0.0271(0.0008) Grad: 24128.4785  LR: 0.000004  \n",
      "Epoch: [5][300/7150] Elapsed 3m 52s (remain 88m 8s) Loss: 0.0008(0.0008) Grad: 4996.2373  LR: 0.000004  \n",
      "Epoch: [5][400/7150] Elapsed 5m 13s (remain 87m 52s) Loss: 0.0000(0.0008) Grad: 145.2529  LR: 0.000004  \n",
      "Epoch: [5][500/7150] Elapsed 6m 27s (remain 85m 42s) Loss: 0.0001(0.0008) Grad: 273.4341  LR: 0.000004  \n",
      "Epoch: [5][600/7150] Elapsed 7m 41s (remain 83m 49s) Loss: 0.0008(0.0008) Grad: 2897.5842  LR: 0.000004  \n",
      "Epoch: [5][700/7150] Elapsed 8m 55s (remain 82m 10s) Loss: 0.0006(0.0008) Grad: 3263.6228  LR: 0.000004  \n",
      "Epoch: [5][800/7150] Elapsed 10m 12s (remain 80m 57s) Loss: 0.0030(0.0007) Grad: 21590.0840  LR: 0.000004  \n",
      "Epoch: [5][900/7150] Elapsed 11m 27s (remain 79m 26s) Loss: 0.0001(0.0008) Grad: 136.0092  LR: 0.000004  \n",
      "Epoch: [5][1000/7150] Elapsed 12m 41s (remain 77m 56s) Loss: 0.0000(0.0008) Grad: 103.9833  LR: 0.000004  \n",
      "Epoch: [5][1100/7150] Elapsed 13m 54s (remain 76m 26s) Loss: 0.0000(0.0008) Grad: 15.8605  LR: 0.000004  \n",
      "Epoch: [5][1200/7150] Elapsed 15m 8s (remain 75m 1s) Loss: 0.0000(0.0007) Grad: 29.5042  LR: 0.000004  \n",
      "Epoch: [5][1300/7150] Elapsed 16m 24s (remain 73m 44s) Loss: 0.0084(0.0007) Grad: 20160.4375  LR: 0.000004  \n",
      "Epoch: [5][1400/7150] Elapsed 17m 43s (remain 72m 43s) Loss: 0.0003(0.0008) Grad: 1327.2546  LR: 0.000004  \n",
      "Epoch: [5][1500/7150] Elapsed 18m 57s (remain 71m 20s) Loss: 0.0000(0.0007) Grad: 21.2193  LR: 0.000004  \n",
      "Epoch: [5][1600/7150] Elapsed 20m 11s (remain 69m 58s) Loss: 0.0060(0.0007) Grad: 10287.7002  LR: 0.000003  \n",
      "Epoch: [5][1700/7150] Elapsed 21m 25s (remain 68m 38s) Loss: 0.0000(0.0007) Grad: 34.1332  LR: 0.000003  \n",
      "Epoch: [5][1800/7150] Elapsed 22m 42s (remain 67m 26s) Loss: 0.0001(0.0007) Grad: 494.1070  LR: 0.000003  \n",
      "Epoch: [5][1900/7150] Elapsed 24m 2s (remain 66m 22s) Loss: 0.0000(0.0007) Grad: 536.9720  LR: 0.000003  \n",
      "Epoch: [5][2000/7150] Elapsed 25m 24s (remain 65m 22s) Loss: 0.0000(0.0007) Grad: 33.9874  LR: 0.000003  \n",
      "Epoch: [5][2100/7150] Elapsed 26m 39s (remain 64m 5s) Loss: 0.0000(0.0007) Grad: 99.3197  LR: 0.000003  \n",
      "Epoch: [5][2200/7150] Elapsed 27m 55s (remain 62m 46s) Loss: 0.0001(0.0007) Grad: 209.3382  LR: 0.000003  \n",
      "Epoch: [5][2300/7150] Elapsed 29m 10s (remain 61m 28s) Loss: 0.0001(0.0007) Grad: 550.9722  LR: 0.000003  \n",
      "Epoch: [5][2400/7150] Elapsed 30m 25s (remain 60m 11s) Loss: 0.0000(0.0007) Grad: 55.0562  LR: 0.000003  \n",
      "Epoch: [5][2500/7150] Elapsed 31m 42s (remain 58m 56s) Loss: 0.0000(0.0007) Grad: 3.3276  LR: 0.000003  \n",
      "Epoch: [5][2600/7150] Elapsed 33m 4s (remain 57m 50s) Loss: 0.0040(0.0007) Grad: 7261.9854  LR: 0.000003  \n",
      "Epoch: [5][2700/7150] Elapsed 34m 24s (remain 56m 40s) Loss: 0.0000(0.0007) Grad: 184.0270  LR: 0.000003  \n",
      "Epoch: [5][2800/7150] Elapsed 35m 45s (remain 55m 30s) Loss: 0.0000(0.0007) Grad: 87.6205  LR: 0.000003  \n",
      "Epoch: [5][2900/7150] Elapsed 37m 11s (remain 54m 28s) Loss: 0.0000(0.0007) Grad: 43.4002  LR: 0.000003  \n",
      "Epoch: [5][3000/7150] Elapsed 38m 36s (remain 53m 22s) Loss: 0.0000(0.0007) Grad: 9.2167  LR: 0.000003  \n",
      "Epoch: [5][3100/7150] Elapsed 39m 57s (remain 52m 9s) Loss: 0.0000(0.0007) Grad: 13.7209  LR: 0.000003  \n",
      "Epoch: [5][3200/7150] Elapsed 41m 17s (remain 50m 56s) Loss: 0.0002(0.0007) Grad: 1305.1934  LR: 0.000002  \n",
      "Epoch: [5][3300/7150] Elapsed 42m 38s (remain 49m 42s) Loss: 0.0012(0.0007) Grad: 2350.1538  LR: 0.000002  \n",
      "Epoch: [5][3400/7150] Elapsed 43m 59s (remain 48m 29s) Loss: 0.0002(0.0007) Grad: 1985.6525  LR: 0.000002  \n",
      "Epoch: [5][3500/7150] Elapsed 45m 21s (remain 47m 16s) Loss: 0.0000(0.0007) Grad: 8.6626  LR: 0.000002  \n",
      "Epoch: [5][3600/7150] Elapsed 46m 38s (remain 45m 58s) Loss: 0.0001(0.0007) Grad: 1153.2434  LR: 0.000002  \n",
      "Epoch: [5][3700/7150] Elapsed 47m 54s (remain 44m 39s) Loss: 0.0000(0.0007) Grad: 177.0036  LR: 0.000002  \n",
      "Epoch: [5][3800/7150] Elapsed 49m 14s (remain 43m 23s) Loss: 0.0000(0.0007) Grad: 354.9659  LR: 0.000002  \n",
      "Epoch: [5][3900/7150] Elapsed 50m 31s (remain 42m 4s) Loss: 0.0001(0.0007) Grad: 360.1072  LR: 0.000002  \n",
      "Epoch: [5][4000/7150] Elapsed 51m 48s (remain 40m 46s) Loss: 0.0000(0.0007) Grad: 75.7685  LR: 0.000002  \n",
      "Epoch: [5][4100/7150] Elapsed 53m 3s (remain 39m 26s) Loss: 0.0002(0.0007) Grad: 2133.8311  LR: 0.000002  \n",
      "Epoch: [5][4200/7150] Elapsed 54m 18s (remain 38m 7s) Loss: 0.0001(0.0007) Grad: 2641.1150  LR: 0.000002  \n",
      "Epoch: [5][4300/7150] Elapsed 55m 35s (remain 36m 49s) Loss: 0.0000(0.0007) Grad: 96.4341  LR: 0.000002  \n",
      "Epoch: [5][4400/7150] Elapsed 56m 51s (remain 35m 31s) Loss: 0.0000(0.0007) Grad: 152.9965  LR: 0.000002  \n",
      "Epoch: [5][4500/7150] Elapsed 58m 7s (remain 34m 12s) Loss: 0.0002(0.0007) Grad: 11871.7812  LR: 0.000002  \n",
      "Epoch: [5][4600/7150] Elapsed 59m 22s (remain 32m 53s) Loss: 0.0018(0.0007) Grad: 12016.0723  LR: 0.000002  \n",
      "Epoch: [5][4700/7150] Elapsed 60m 37s (remain 31m 34s) Loss: 0.0000(0.0007) Grad: 6.5893  LR: 0.000002  \n",
      "Epoch: [5][4800/7150] Elapsed 61m 52s (remain 30m 16s) Loss: 0.0018(0.0007) Grad: 18597.8828  LR: 0.000001  \n",
      "Epoch: [5][4900/7150] Elapsed 63m 7s (remain 28m 57s) Loss: 0.0000(0.0007) Grad: 44.3382  LR: 0.000001  \n",
      "Epoch: [5][5000/7150] Elapsed 64m 22s (remain 27m 39s) Loss: 0.0000(0.0007) Grad: 8.9185  LR: 0.000001  \n",
      "Epoch: [5][5100/7150] Elapsed 65m 42s (remain 26m 23s) Loss: 0.0000(0.0007) Grad: 227.2490  LR: 0.000001  \n",
      "Epoch: [5][5200/7150] Elapsed 67m 4s (remain 25m 7s) Loss: 0.0000(0.0007) Grad: 729.0667  LR: 0.000001  \n",
      "Epoch: [5][5300/7150] Elapsed 68m 19s (remain 23m 49s) Loss: 0.0000(0.0007) Grad: 25.2788  LR: 0.000001  \n",
      "Epoch: [5][5400/7150] Elapsed 69m 33s (remain 22m 31s) Loss: 0.0003(0.0007) Grad: 618.9048  LR: 0.000001  \n",
      "Epoch: [5][5500/7150] Elapsed 70m 52s (remain 21m 14s) Loss: 0.0000(0.0007) Grad: 127.7206  LR: 0.000001  \n",
      "Epoch: [5][5600/7150] Elapsed 72m 14s (remain 19m 58s) Loss: 0.0000(0.0007) Grad: 9.9609  LR: 0.000001  \n",
      "Epoch: [5][5700/7150] Elapsed 73m 29s (remain 18m 40s) Loss: 0.0007(0.0007) Grad: 12633.6592  LR: 0.000001  \n",
      "Epoch: [5][5800/7150] Elapsed 74m 43s (remain 17m 22s) Loss: 0.0003(0.0007) Grad: 1842.2856  LR: 0.000001  \n",
      "Epoch: [5][5900/7150] Elapsed 75m 57s (remain 16m 4s) Loss: 0.0000(0.0007) Grad: 116.7226  LR: 0.000001  \n",
      "Epoch: [5][6000/7150] Elapsed 77m 11s (remain 14m 46s) Loss: 0.0000(0.0007) Grad: 646.1622  LR: 0.000001  \n",
      "Epoch: [5][6100/7150] Elapsed 78m 27s (remain 13m 29s) Loss: 0.0007(0.0007) Grad: 7863.7427  LR: 0.000001  \n",
      "Epoch: [5][6200/7150] Elapsed 79m 42s (remain 12m 11s) Loss: 0.0000(0.0007) Grad: 5.4853  LR: 0.000001  \n",
      "Epoch: [5][6300/7150] Elapsed 80m 56s (remain 10m 54s) Loss: 0.0000(0.0007) Grad: 463.2818  LR: 0.000001  \n",
      "Epoch: [5][6400/7150] Elapsed 82m 13s (remain 9m 37s) Loss: 0.0001(0.0007) Grad: 1344.5515  LR: 0.000000  \n",
      "Epoch: [5][6500/7150] Elapsed 83m 28s (remain 8m 20s) Loss: 0.0000(0.0007) Grad: 21.5537  LR: 0.000000  \n",
      "Epoch: [5][6600/7150] Elapsed 84m 42s (remain 7m 2s) Loss: 0.0000(0.0007) Grad: 277.8039  LR: 0.000000  \n",
      "Epoch: [5][6700/7150] Elapsed 85m 56s (remain 5m 45s) Loss: 0.0006(0.0007) Grad: 6387.6021  LR: 0.000000  \n",
      "Epoch: [5][6800/7150] Elapsed 87m 10s (remain 4m 28s) Loss: 0.0000(0.0007) Grad: 85.9920  LR: 0.000000  \n",
      "Epoch: [5][6900/7150] Elapsed 88m 24s (remain 3m 11s) Loss: 0.0000(0.0007) Grad: 58.0512  LR: 0.000000  \n",
      "Epoch: [5][7000/7150] Elapsed 89m 39s (remain 1m 54s) Loss: 0.0000(0.0007) Grad: 18.5755  LR: 0.000000  \n",
      "Epoch: [5][7100/7150] Elapsed 90m 54s (remain 0m 37s) Loss: 0.0018(0.0007) Grad: 42386.6523  LR: 0.000000  \n",
      "Epoch: [5][7149/7150] Elapsed 91m 32s (remain 0m 0s) Loss: 0.0002(0.0007) Grad: 760.3915  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 1s (remain 20m 58s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 30s (remain 5m 28s) Loss: 0.0227(0.0030) \n",
      "EVAL: [200/1192] Elapsed 0m 59s (remain 4m 55s) Loss: 0.0045(0.0026) \n",
      "EVAL: [300/1192] Elapsed 1m 31s (remain 4m 31s) Loss: 0.0041(0.0031) \n",
      "EVAL: [400/1192] Elapsed 2m 1s (remain 3m 59s) Loss: 0.0000(0.0029) \n",
      "EVAL: [500/1192] Elapsed 2m 30s (remain 3m 27s) Loss: 0.0220(0.0028) \n",
      "EVAL: [600/1192] Elapsed 3m 0s (remain 2m 57s) Loss: 0.0064(0.0030) \n",
      "EVAL: [700/1192] Elapsed 3m 31s (remain 2m 28s) Loss: 0.0020(0.0035) \n",
      "EVAL: [800/1192] Elapsed 4m 5s (remain 1m 59s) Loss: 0.0052(0.0035) \n",
      "EVAL: [900/1192] Elapsed 4m 34s (remain 1m 28s) Loss: 0.0032(0.0036) \n",
      "EVAL: [1000/1192] Elapsed 5m 4s (remain 0m 58s) Loss: 0.0000(0.0035) \n",
      "EVAL: [1100/1192] Elapsed 5m 33s (remain 0m 27s) Loss: 0.0084(0.0034) \n",
      "EVAL: [1191/1192] Elapsed 6m 0s (remain 0m 0s) Loss: 0.0000(0.0033) \n",
      "Epoch 5 - avg_train_loss: 0.0007  avg_val_loss: 0.0033  time: 5857s\n",
      "Epoch 5 - Score: 0.8908\n",
      "best_thres: 0.5  score: 0.89024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n",
      "load weights from ../output/nbme-score-clinical-patient-notes/nbme-exp092/fold0_best.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b768ea6e704108932c23832f7961f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n",
      "load weights from ../output/nbme-score-clinical-patient-notes/nbme-exp092/fold1_best.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862708d4ec7843ab9b4fcc3c11ffef73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n",
      "load weights from ../output/nbme-score-clinical-patient-notes/nbme-exp092/fold2_best.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5321d5543534abe9667851537dc1431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n",
      "load weights from ../output/nbme-score-clinical-patient-notes/nbme-exp092/fold3_best.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eecccbabac941d6850705402daa356b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "name": "nbme-exp085.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 641.572862,
   "end_time": "2022-02-27T11:39:50.972497",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-27T11:29:09.399635",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "054630edadaa453fb86d66a20e49030f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_747b8a73ce544c569f4d063fc9d6d18a",
      "placeholder": "​",
      "style": "IPY_MODEL_c7aa62f5eaca401dbbfbfd5f843d6a59",
      "value": " 28720/42146 [00:27&lt;00:07, 1839.83it/s]"
     }
    },
    "18b47303dd5b48bfb118053a80c44a40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_971945a52a6d4f06ba35e5363d264037",
       "IPY_MODEL_2d7cfbb1d1a54c0590e59de0b280ab22",
       "IPY_MODEL_054630edadaa453fb86d66a20e49030f"
      ],
      "layout": "IPY_MODEL_8fce27d68c1c41ec9a3c58d439c2a5e7"
     }
    },
    "28c710503f154bdea731991801a85a47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d7cfbb1d1a54c0590e59de0b280ab22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28c710503f154bdea731991801a85a47",
      "max": 42146,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8c57bf78a80247db9521bd5b163502ef",
      "value": 28905
     }
    },
    "747b8a73ce544c569f4d063fc9d6d18a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c57bf78a80247db9521bd5b163502ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8fce27d68c1c41ec9a3c58d439c2a5e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "971945a52a6d4f06ba35e5363d264037": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9a3708f7e4c486da3a09e066b6936fb",
      "placeholder": "​",
      "style": "IPY_MODEL_cd1144e40332427fa3e8d5bc7f57b924",
      "value": " 69%"
     }
    },
    "c7aa62f5eaca401dbbfbfd5f843d6a59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd1144e40332427fa3e8d5bc7f57b924": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e9a3708f7e4c486da3a09e066b6936fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
