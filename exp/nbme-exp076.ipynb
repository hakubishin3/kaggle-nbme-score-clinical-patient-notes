{"cells":[{"cell_type":"markdown","id":"colored-security","metadata":{"id":"colored-security"},"source":["## References"]},{"cell_type":"markdown","id":"educational-operator","metadata":{"id":"educational-operator"},"source":["- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"]},{"cell_type":"markdown","id":"incorrect-greek","metadata":{"id":"incorrect-greek"},"source":["## Configurations"]},{"cell_type":"code","execution_count":1,"id":"alive-granny","metadata":{"id":"alive-granny","executionInfo":{"status":"ok","timestamp":1649248000762,"user_tz":-540,"elapsed":13,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["EXP_NAME = \"nbme-exp076\"\n","ENV = \"colab\"\n","DEBUG_MODE = False\n","SUBMISSION_MODE = False"]},{"cell_type":"code","execution_count":2,"id":"heavy-prophet","metadata":{"id":"heavy-prophet","executionInfo":{"status":"ok","timestamp":1649248000762,"user_tz":-540,"elapsed":11,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["class CFG:\n","    env=ENV\n","    exp_name=EXP_NAME\n","    debug=DEBUG_MODE\n","    submission=SUBMISSION_MODE\n","    apex=True\n","    input_dir=None\n","    output_dir=None\n","    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n","    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n","    competition_name=\"nbme-score-clinical-patient-notes\"\n","    id_col=\"id\"\n","    target_col=\"location\"\n","    pretrained_model_name=\"microsoft/deberta-v3-large\"\n","    tokenizer=None\n","    max_len=None\n","    output_dim=1\n","    dropout=0.2\n","    num_workers=4\n","    batch_size=3\n","    lr=2e-5\n","    betas=(0.9, 0.98)\n","    weight_decay=0.1\n","    num_warmup_steps_rate=0.1\n","    batch_scheduler=True\n","    epochs=5\n","    n_fold=4\n","    train_fold=[0, 1, 2, 3]\n","    seed=71\n","    gradient_accumulation_steps=2\n","    max_grad_norm=1000\n","    print_freq=100\n","    train=True\n","    inference=True"]},{"cell_type":"code","execution_count":3,"id":"vocational-coating","metadata":{"id":"vocational-coating","executionInfo":{"status":"ok","timestamp":1649248000763,"user_tz":-540,"elapsed":11,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.train_fold = [0, 1]\n","\n","if CFG.submission:\n","    CFG.train = False\n","    CFG.inference = True"]},{"cell_type":"markdown","id":"private-moderator","metadata":{"id":"private-moderator"},"source":["## Directory Settings"]},{"cell_type":"code","execution_count":4,"id":"married-tokyo","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"married-tokyo","outputId":"c2ca5d96-526e-436e-e6b3-2ef9dda351bd","executionInfo":{"status":"ok","timestamp":1649248038616,"user_tz":-540,"elapsed":37864,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["colab\n","Mounted at /content/drive\n","Collecting transformers==4.16.2\n","  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 15.2 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.11.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (1.21.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (3.6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (21.3)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 54.2 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 7.8 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.63.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 46.0 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,>=0.10.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 56.0 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.16.2) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.16.2) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (1.15.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.16.2\n","\u001b[K     |████████████████████████████████| 1.2 MB 14.6 MB/s \n","\u001b[?25h"]}],"source":["import sys\n","from pathlib import Path\n","\n","\n","print(CFG.env)\n","if CFG.env == \"colab\":\n","    # colab環境\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","    # install packages\n","    !pip install transformers==4.16.2\n","    !pip install -q sentencepiece==0.1.96\n","\n","elif CFG.env == \"local\":\n","    # ローカルサーバ\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","\n","elif CFG.env == \"kaggle\":\n","    # kaggle環境\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./\")"]},{"cell_type":"code","source":["# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n","# This must be done before importing transformers\n","import shutil\n","from pathlib import Path\n","\n","if CFG.env == \"colab\":\n","    input_dir = Path(\"./drive/MyDrive/00.kaggle/input/deberta-v2-3-fast-tokenizer\")\n","    transformers_path = Path(\"/usr/local/lib/python3.7/dist-packages/transformers\")\n","else:\n","    input_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n","    transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n","\n","convert_file = input_dir / \"convert_slow_tokenizer.py\"\n","conversion_path = transformers_path/convert_file.name\n","\n","if conversion_path.exists():\n","    conversion_path.unlink()\n","\n","shutil.copy(convert_file, transformers_path)\n","deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n","\n","for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n","    filepath = deberta_v2_path/filename\n","    if filepath.exists():\n","        filepath.unlink()\n","\n","    shutil.copy(input_dir/filename, filepath)\n","    \n","    \n","from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast"],"metadata":{"id":"cnGM_g9c3WJW","executionInfo":{"status":"ok","timestamp":1649248053457,"user_tz":-540,"elapsed":14846,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"id":"cnGM_g9c3WJW","execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"id":"blank-pierre","metadata":{"id":"blank-pierre","executionInfo":{"status":"ok","timestamp":1649248055037,"user_tz":-540,"elapsed":1585,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["import gc\n","import os\n","import ast\n","import time\n","import math\n","import random\n","import itertools\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","from scipy.optimize import minimize\n","from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as T\n","from torchvision.io import read_image\n","from torch.utils.data import DataLoader, Dataset\n","\n","from transformers import AutoModelForMaskedLM\n","from transformers import BartModel,BertModel,BertTokenizer\n","from transformers import DebertaModel,DebertaTokenizer\n","from transformers import RobertaModel,RobertaTokenizer\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","id":"sound-still","metadata":{"id":"sound-still"},"source":["## Utilities"]},{"cell_type":"code","execution_count":7,"id":"surprised-commercial","metadata":{"id":"surprised-commercial","executionInfo":{"status":"ok","timestamp":1649248055038,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on binary arrays.\n","\n","    Args:\n","        preds (list of lists of ints): Predictions.\n","        truths (list of lists of ints): Ground truths.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    # Micro : aggregating over all instances\n","    preds = np.concatenate(preds)\n","    truths = np.concatenate(truths)\n","    return f1_score(truths, preds)\n","\n","\n","def spans_to_binary(spans, length=None):\n","    \"\"\"\n","    Converts spans to a binary array indicating whether each character is in the span.\n","\n","    Args:\n","        spans (list of lists of two ints): Spans.\n","\n","    Returns:\n","        np array [length]: Binarized spans.\n","    \"\"\"\n","    length = np.max(spans) if length is None else length\n","    binary = np.zeros(length)\n","    for start, end in spans:\n","        binary[start:end] = 1\n","    return binary\n","\n","\n","def span_micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on spans.\n","\n","    Args:\n","        preds (list of lists of two ints): Prediction spans.\n","        truths (list of lists of two ints): Ground truth spans.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    bin_preds = []\n","    bin_truths = []\n","    for pred, truth in zip(preds, truths):\n","        if not len(pred) and not len(truth):\n","            continue\n","        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n","        bin_preds.append(spans_to_binary(pred, length))\n","        bin_truths.append(spans_to_binary(truth, length))\n","    return micro_f1(bin_preds, bin_truths)\n","\n","\n","def get_score(y_true, y_pred):\n","    score = span_micro_f1(y_true, y_pred)\n","    return score"]},{"cell_type":"code","execution_count":8,"id":"interstate-accident","metadata":{"id":"interstate-accident","executionInfo":{"status":"ok","timestamp":1649248055039,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def create_labels_for_scoring(df):\n","    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n","    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n","    for i in range(len(df)):\n","        lst = df.loc[i, \"location\"]\n","        if lst:\n","            new_lst = \";\".join(lst)\n","            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n","\n","    # create labels\n","    truths = []\n","    for location_list in df[\"location_for_create_labels\"].values:\n","        truth = []\n","        if len(location_list) > 0:\n","            location = location_list[0]\n","            for loc in [s.split() for s in location.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                truth.append([start, end])\n","        truths.append(truth)\n","\n","    return truths\n","\n","\n","def get_char_probs(texts, token_probs, tokenizer):\n","    res = [np.zeros(len(t)) for t in texts]\n","    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n","        encoded = tokenizer(\n","            text=text,\n","            max_length=CFG.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n","            start, end = offset_mapping\n","            res[i][start:end] = pred\n","    return res\n","\n","\n","def get_predicted_location_str(char_probs, th=0.5):\n","    results = []\n","    for char_prob in char_probs:\n","        result = np.where(char_prob >= th)[0] + 1\n","        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n","        result = [f\"{min(r)} {max(r)}\" for r in result]\n","        result = \";\".join(result)\n","        results.append(result)\n","    return results\n","\n","\n","def get_predictions(results):\n","    predictions = []\n","    for result in results:\n","        prediction = []\n","        if result != \"\":\n","            for loc in [s.split() for s in result.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                prediction.append([start, end])\n","        predictions.append(prediction)\n","    return predictions\n","\n","\n","def scoring(df, th=0.5):\n","    labels = create_labels_for_scoring(df)\n","\n","    token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n","    char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n","    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n","    preds = get_predictions(predicted_location_str)\n","\n","    score = get_score(labels, preds)\n","    return score\n","\n","\n","def get_best_thres(oof_df):\n","    def f1_opt(x):\n","        return -1 * scoring(oof_df, th=x)\n","\n","    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n","    return best_thres"]},{"cell_type":"code","execution_count":9,"id":"coated-pioneer","metadata":{"id":"coated-pioneer","executionInfo":{"status":"ok","timestamp":1649248055039,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return \"%dm %ds\" % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":10,"id":"nervous-delaware","metadata":{"id":"nervous-delaware","executionInfo":{"status":"ok","timestamp":1649248055040,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["seed_everything()"]},{"cell_type":"markdown","id":"functioning-destruction","metadata":{"id":"functioning-destruction"},"source":["## Data Loading"]},{"cell_type":"code","execution_count":11,"id":"global-monte","metadata":{"id":"global-monte","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649248057742,"user_tz":-540,"elapsed":2708,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"d521682d-644c-4d91-9abc-cdc98c3e04ec"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14300, 6), (143, 3), (42146, 3), (5, 4))"]},"metadata":{},"execution_count":11}],"source":["train = pd.read_csv(CFG.input_dir / \"train.csv\")\n","features = pd.read_csv(CFG.input_dir / \"features.csv\")\n","patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n","test = pd.read_csv(CFG.input_dir / \"test.csv\")\n","\n","train.shape, features.shape, patient_notes.shape, test.shape"]},{"cell_type":"code","execution_count":12,"id":"independent-airfare","metadata":{"id":"independent-airfare","executionInfo":{"status":"ok","timestamp":1649248057743,"user_tz":-540,"elapsed":4,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["if CFG.debug:\n","    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n","    print(train.shape)"]},{"cell_type":"markdown","id":"silent-locator","metadata":{"id":"silent-locator"},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":13,"id":"unusual-fifty","metadata":{"id":"unusual-fifty","executionInfo":{"status":"ok","timestamp":1649248057743,"user_tz":-540,"elapsed":3,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def preprocess_features(features):\n","    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n","    return features\n","\n","\n","features = preprocess_features(features)"]},{"cell_type":"code","execution_count":14,"id":"decreased-mustang","metadata":{"id":"decreased-mustang","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649248058219,"user_tz":-540,"elapsed":15,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"28ba9898-a577-4a04-bc1f-6b4ff5fd24aa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14300, 8), (5, 6))"]},"metadata":{},"execution_count":14}],"source":["train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","\n","train.shape, test.shape"]},{"cell_type":"code","execution_count":15,"id":"boolean-trade","metadata":{"id":"boolean-trade","executionInfo":{"status":"ok","timestamp":1649248058220,"user_tz":-540,"elapsed":13,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n","train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"]},{"cell_type":"code","execution_count":16,"id":"accomplished-dakota","metadata":{"id":"accomplished-dakota","colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"status":"ok","timestamp":1649248058220,"user_tz":-540,"elapsed":12,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"5e208290-5e5a-4509-da07-39e5ca77f4f1"},"outputs":[{"output_type":"display_data","data":{"text/plain":["0    4399\n","1    8181\n","2    1296\n","3     287\n","4      99\n","5      27\n","6       9\n","7       1\n","8       1\n","Name: annotation_length, dtype: int64"]},"metadata":{}}],"source":["train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n","display(train['annotation_length'].value_counts().sort_index())"]},{"cell_type":"markdown","id":"funded-elizabeth","metadata":{"id":"funded-elizabeth"},"source":["## CV split"]},{"cell_type":"code","execution_count":17,"id":"unexpected-columbia","metadata":{"id":"unexpected-columbia","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1649248058221,"user_tz":-540,"elapsed":11,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"bfd1a0e2-11db-4f90-f288-4ab5efce0c9a"},"outputs":[{"output_type":"display_data","data":{"text/plain":["fold\n","0    3575\n","1    3575\n","2    3575\n","3    3575\n","dtype: int64"]},"metadata":{}}],"source":["Fold = GroupKFold(n_splits=CFG.n_fold)\n","groups = train['pn_num'].values\n","for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n","    train.loc[val_index, 'fold'] = int(n)\n","train['fold'] = train['fold'].astype(int)\n","display(train.groupby('fold').size())"]},{"cell_type":"markdown","id":"critical-archive","metadata":{"id":"critical-archive"},"source":["## Setup tokenizer"]},{"cell_type":"code","execution_count":18,"id":"broken-generator","metadata":{"id":"broken-generator","colab":{"base_uri":"https://localhost:8080/","height":148,"referenced_widgets":["da6c964c345a43a9ab76b5813da01b1e","3683782634ee41e7a4a85d4e2ed61f95","73869eed7bbe4f208521f5b0bf3cb5b1","2cb906b3ae0e44159bfde592839ba67f","697f35237f474b90a43b352e7ca81fe7","f0db979c013942868e2d13910af1220f","28522f44c6c04ac9849df660f1122ca2","5c075db03d2b4453a1d00d65b6d9406d","a2a5834c4acc4217bf009acb82d88ff1","4dcc4fd12bb64fdd85c4bf46c7e8c266","2ef15c50beab42ae957ebc62369a19bc","9ce266701cd64e95945a118ff843ad2b","9f9c397c3b224e26b26b805a52f90a41","33b8c475a441496790ccb74967fef5a4","89516a0ef1454248a52e63cddd42bdf1","9faacaae71514d13ac83bdf4cc3e19f3","44f561c038794eba8014a20b1248ff1e","fd4e3e521b4d4f09bd77ae4da9de94ec","fe4c449ff77848a6b099b854c035f914","830e08bce77d4a0391c50ccdb72b9d2c","f8b8a8f19f81410fb45f8fb82666599b","c066b333d7c6492698ae623c24acd992","e58e4ca26dd44363b2cfd30ab7226b46","d59267f7193e4c13968f5dc31c23a2fa","0de1f93124684f2fa60dae8025fca5f5","230608f3d4264538a802fefa037c9a10","02769f76ae644caeacbc07e0c653a941","cfa6da5eaa9740aa82e24027d1daf982","bde9113ddb5b4d73a287721cc1b77b0b","afc32c01fb6b4791ad447ea61d8d3687","310d0cf6ca82493c840367ee6ee1ae08","c4cc22905bda42e6910973d2063ecc99","6c548c3d48f14bcaa14e286c2320a41e"]},"executionInfo":{"status":"ok","timestamp":1649248065212,"user_tz":-540,"elapsed":7000,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"cdb51b1b-7289-48a0-9296-61a88bd614e2"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da6c964c345a43a9ab76b5813da01b1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/2.35M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ce266701cd64e95945a118ff843ad2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/580 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e58e4ca26dd44363b2cfd30ab7226b46"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["if CFG.submission:\n","    tokenizer = DebertaV2TokenizerFast.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n","else:\n","    tokenizer = DebertaV2TokenizerFast.from_pretrained(CFG.pretrained_model_name)\n","    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n","\n","CFG.tokenizer = tokenizer"]},{"cell_type":"markdown","id":"compatible-lincoln","metadata":{"id":"compatible-lincoln"},"source":["## Create dataset"]},{"cell_type":"code","execution_count":19,"id":"fluid-nancy","metadata":{"id":"fluid-nancy","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["f5913793c8364d2e88e39c6d72390b6e","260fd91bab5242659bb278b4f4803fac","2ce1d89c470c49c395a6f08346fabbce","65d4f8d31b434b8096e70a1b0e3349da","91ee8cabfde54f53ba7e54b813049280","730c1742b14549fb86a8d75e20eaf7a7","ace17e186a264496b42db32254d5835f","46b85fcfad194020aa30273407f6696a","b3e88277a95149e6b3d0fa2cc6718a60","d1e8b3baba674ace892834e95fcc6798","468d683fd55040c1a8bf66280a8ba517"]},"executionInfo":{"status":"ok","timestamp":1649248099524,"user_tz":-540,"elapsed":34328,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"ec81dc55-cdd0-4081-837b-e3c8cfcaf576"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/42146 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5913793c8364d2e88e39c6d72390b6e"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 323\n"]}],"source":["pn_history_lengths = []\n","tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    pn_history_lengths.append(length)\n","\n","print(\"max length:\", np.max(pn_history_lengths))"]},{"cell_type":"code","execution_count":20,"id":"posted-humidity","metadata":{"id":"posted-humidity","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["72bacbcffef34cb3accb06920129139e","bade747ff6164db292889a7e4449dea2","2ba3f42caea248c7aea1d0e4beae4d9e","589a6d6ca15b4e0dbf5fa8293b5ea21a","439808b9845a4ff9be6791ccbbec37f3","eac296db92804f4a924e2e443729d8a4","f7caf05fba964ad5a42a2fd6a3d23895","c21524dcf08c4644be92d7d21f783e54","f8f30ef643bf47b59952bbc06f082782","dffe673347a54dddb4d430071c15bd57","2c496a81d245472e98d63b5086714d21"]},"executionInfo":{"status":"ok","timestamp":1649248100119,"user_tz":-540,"elapsed":611,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"5a6c02f9-6db9-4e5b-f9cc-1a7fb497e4ee"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/143 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72bacbcffef34cb3accb06920129139e"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 28\n"]}],"source":["feature_text_lengths = []\n","tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    feature_text_lengths.append(length)\n","\n","print(\"max length:\", np.max(feature_text_lengths))"]},{"cell_type":"code","execution_count":21,"id":"resistant-amount","metadata":{"id":"resistant-amount","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649248100120,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"8f1e1dfb-587c-47f8-c7f7-cfa51f717980"},"outputs":[{"output_type":"stream","name":"stdout","text":["max length: 354\n"]}],"source":["CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n","\n","print(\"max length:\", CFG.max_len)"]},{"cell_type":"code","execution_count":22,"id":"august-equity","metadata":{"id":"august-equity","executionInfo":{"status":"ok","timestamp":1649248100120,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["class TrainingDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","        self.annotation_lengths = self.df[\"annotation_length\"].values\n","        self.locations = self.df[\"location\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def _create_label(self, pn_history, annotation_length, location_list):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        offset_mapping = encoded[\"offset_mapping\"]\n","        ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n","        label = np.zeros(len(offset_mapping))\n","        label[ignore_idxes] = -1\n","\n","        if annotation_length > 0:\n","            for location in location_list:\n","                for loc in [s.split() for s in location.split(\";\")]:\n","                    start, end = int(loc[0]), int(loc[1])\n","                    start_idx = -1\n","                    end_idx = -1\n","                    for idx in range(len(offset_mapping)):\n","                        if (start_idx == -1) & (start < offset_mapping[idx][0]):\n","                            start_idx = idx - 1\n","                        if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n","                            end_idx = idx + 1\n","                    if start_idx == -1:\n","                        start_idx = end_idx\n","                    if (start_idx != -1) & (end_idx != -1):\n","                        label[start_idx:end_idx] = 1\n","\n","        return torch.tensor(label, dtype=torch.float)\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n","        return input_, label"]},{"cell_type":"code","execution_count":23,"id":"weird-interaction","metadata":{"id":"weird-interaction","executionInfo":{"status":"ok","timestamp":1649248100121,"user_tz":-540,"elapsed":8,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["class TestDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        return input_"]},{"cell_type":"markdown","id":"upper-mobility","metadata":{"id":"upper-mobility"},"source":["## Model"]},{"cell_type":"code","source":["from transformers.modeling_outputs import MaskedLMOutput\n","\n","class MaskedModel(nn.Module):\n","    def __init__(self, cfg, config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","        if config_path is None:\n","            self.config = AutoConfig.from_pretrained(\n","                cfg.pretrained_model_name,\n","                output_hidden_states=False\n","                )\n","        else:\n","            self.config = torch.load(config_path)\n","        \n","        if pretrained:\n","            self.model = AutoModel.from_pretrained(cfg.pretrained_model_name, config=self.config)\n","            self.lm_head = AutoModelForMaskedLM.from_pretrained(cfg.pretrained_model_name, config=self.config).cls # [cls, lm_head]\n","        else:\n","            self.model = AutoModel(self.config)\n","            self.lm_head = AutoModelForMaskedLM(self.config).cls # [cls, lm_head]\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def forward(\n","            self, \n","            input_ids=None,\n","            attention_mask=None,\n","            token_type_ids=None,\n","            #position_ids=None,\n","            inputs_embeds=None,\n","            labels=None,\n","            output_attentions=None,\n","            output_hidden_states=None,\n","            return_dict=None):\n","        \n","        outputs = self.model(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            #position_ids=position_ids,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,)\n","        \n","        sequence_output = outputs[0]\n","        prediction_scores = self.lm_head(sequence_output)\n","\n","        masked_lm_loss = None\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss()\n","            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n","\n","        return MaskedLMOutput(loss=masked_lm_loss,\n","                              logits=prediction_scores,\n","                              hidden_states=outputs.hidden_states,\n","                              attentions=outputs.attentions)"],"metadata":{"id":"a4HSgs6b8wQT","executionInfo":{"status":"ok","timestamp":1649248100121,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"id":"a4HSgs6b8wQT","execution_count":24,"outputs":[]},{"cell_type":"code","execution_count":25,"id":"spanish-destruction","metadata":{"id":"spanish-destruction","executionInfo":{"status":"ok","timestamp":1649248100121,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["class CustomModel(nn.Module):\n","    def __init__(self, cfg, model_config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        if model_config_path is None:\n","            self.model_config = AutoConfig.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                output_hidden_states=True,\n","            )\n","        else:\n","            self.model_config = torch.load(model_config_path)\n","\n","        if pretrained:\n","            self.backbone = AutoModel.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                config=self.model_config,\n","            )\n","            print(f\"Load weight from pretrained\")\n","        else:\n","            #self.backbone = AutoModel.from_config(self.model_config)\n","            # itpt = AutoModelForMaskedLM.from_config(self.model_config)\n","            #path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp010/checkpoint-130170/pytorch_model.bin\")\n","            # path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n","            # state_dict = torch.load(path)\n","            # itpt.load_state_dict(state_dict)\n","            path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\")\n","            masked_model = MaskedModel(CFG, config_path=None, pretrained=True)\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            masked_model.load_state_dict(state)\n","            self.backbone = masked_model.model\n","            print(f\"Load weight from {path}\")\n","\n","        self.fc = nn.Sequential(\n","            nn.Dropout(self.cfg.dropout),\n","            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n","        )\n","\n","    def forward(self, inputs):\n","        h = self.backbone(**inputs)[\"last_hidden_state\"]\n","        output = self.fc(h)\n","        return output"]},{"cell_type":"markdown","id":"chronic-bullet","metadata":{"id":"chronic-bullet"},"source":["## Training"]},{"cell_type":"code","execution_count":26,"id":"biological-hunger","metadata":{"id":"biological-hunger","executionInfo":{"status":"ok","timestamp":1649248100122,"user_tz":-540,"elapsed":8,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def train_fn(\n","    train_dataloader,\n","    model,\n","    criterion,\n","    optimizer,\n","    epoch,\n","    scheduler,\n","    device,\n","):\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(train_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","        loss = loss.mean()\n","\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","\n","        if CFG.batch_scheduler:\n","            scheduler.step()\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n","            print(\n","                \"Epoch: [{0}][{1}/{2}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                \"Grad: {grad_norm:.4f}  \"\n","                \"LR: {lr:.6f}  \"\n","                .format(\n","                    epoch+1,\n","                    step,\n","                    len(train_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n","                    loss=losses,\n","                     grad_norm=grad_norm,\n","                     lr=scheduler.get_lr()[0],\n","                )\n","            )\n","    return losses.avg"]},{"cell_type":"code","execution_count":27,"id":"satisfied-sterling","metadata":{"id":"satisfied-sterling","executionInfo":{"status":"ok","timestamp":1649248100122,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def valid_fn(\n","    val_dataloader,\n","    model,\n","    criterion,\n","    device,\n","):\n","    model.eval()\n","    preds = []\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(val_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.no_grad():\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","        loss = loss.mean()\n","\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n","            print(\n","                \"EVAL: [{0}/{1}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                .format(\n","                    step, len(val_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n","                    loss=losses,\n","                )\n","            )\n","    preds = np.concatenate(preds)\n","    return losses.avg, preds"]},{"cell_type":"code","execution_count":28,"id":"incorporate-viking","metadata":{"id":"incorporate-viking","executionInfo":{"status":"ok","timestamp":1649248100122,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def inference_fn(test_dataloader, model, device):\n","    model.eval()\n","    model.to(device)\n","    preds = []\n","    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n","    for inputs in tk0:\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        with torch.no_grad():\n","            output = model(inputs)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","    preds = np.concatenate(preds)\n","    return preds"]},{"cell_type":"code","execution_count":29,"id":"dental-sunset","metadata":{"id":"dental-sunset","executionInfo":{"status":"ok","timestamp":1649248100123,"user_tz":-540,"elapsed":8,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def train_loop(df, i_fold, device):\n","    print(f\"========== fold: {i_fold} training ==========\")\n","    train_idx = df[df[\"fold\"] != i_fold].index\n","    val_idx = df[df[\"fold\"] == i_fold].index\n","\n","    train_folds = df.loc[train_idx].reset_index(drop=True)\n","    val_folds = df.loc[val_idx].reset_index(drop=True)\n","\n","    train_dataset = TrainingDataset(CFG, train_folds)\n","    val_dataset = TrainingDataset(CFG, val_folds)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=True,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=True,\n","    )\n","    val_dataloader = DataLoader(\n","        val_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=False,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=False,\n","    )\n","\n","    model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n","    model.to(device)\n","\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\"params\": [p for n, p in param_optimizer if not any(\n","            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n","        {\"params\": [p for n, p in param_optimizer if any(\n","            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n","    ]\n","    optimizer = AdamW(\n","        optimizer_grouped_parameters,\n","        lr=CFG.lr,\n","        betas=CFG.betas,\n","        weight_decay=CFG.weight_decay,\n","    )\n","    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n","    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        num_training_steps=num_train_optimization_steps,\n","    )\n","\n","    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    best_score = -1 * np.inf\n","\n","    for epoch in range(CFG.epochs):\n","        start_time = time.time()\n","        avg_loss = train_fn(\n","            train_dataloader,\n","            model,\n","            criterion,\n","            optimizer,\n","            epoch,\n","            scheduler,\n","            device,\n","        )\n","        avg_val_loss, val_preds = valid_fn(\n","            val_dataloader,\n","            model,\n","            criterion,\n","            device,\n","        )\n","\n","        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n","            scheduler.step()\n","\n","        # scoring\n","        val_folds[[str(i) for i in range(CFG.max_len)]] = val_preds\n","        score = scoring(val_folds, th=0.5)\n","\n","        elapsed = time.time() - start_time\n","\n","        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n","        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n","        if score > best_score:\n","            best_score = score\n","            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n","            torch.save({\n","                \"model\": model.state_dict(),\n","                \"predictions\": val_preds,\n","                },\n","                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","            )\n","\n","    predictions = torch.load(\n","        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","        map_location=torch.device(\"cpu\"),\n","    )[\"predictions\"]\n","    val_folds[[str(i) for i in range(CFG.max_len)]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return val_folds"]},{"cell_type":"markdown","id":"brazilian-graphics","metadata":{"id":"brazilian-graphics"},"source":["## Main"]},{"cell_type":"code","execution_count":30,"id":"connected-protein","metadata":{"id":"connected-protein","executionInfo":{"status":"ok","timestamp":1649248100123,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def main():\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    if CFG.train:\n","        oof_df = pd.DataFrame()\n","        for i_fold in range(CFG.n_fold):\n","            if i_fold in CFG.train_fold:\n","                _oof_df = train_loop(train, i_fold, device)\n","                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n","        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    if CFG.submission:\n","        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n","    else:\n","        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    score = scoring(oof_df, th=0.5)\n","    print(f\"Best thres: 0.5, Score: {score:.4f}\")\n","    best_thres = get_best_thres(oof_df)\n","    score = scoring(oof_df, th=best_thres)\n","    print(f\"Best thres: {best_thres}, Score: {score:.4f}\")\n","\n","    if CFG.inference:\n","        test_dataset = TestDataset(CFG, test)\n","        test_dataloader = DataLoader(\n","            test_dataset,\n","            batch_size=CFG.batch_size,\n","            shuffle=False,\n","            num_workers=CFG.num_workers,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","        predictions = []\n","        for i_fold in CFG.train_fold:\n","            if CFG.submission:\n","                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n","                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n","            else:\n","                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n","\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            model.load_state_dict(state[\"model\"])\n","            test_token_probs = inference_fn(test_dataloader, model, device)\n","            test[[f\"fold{i_fold}_{i}\" for i in range(CFG.max_len)]] = test_token_probs\n","            test_char_probs = get_char_probs(test[\"pn_history\"].values, test_token_probs, CFG.tokenizer)\n","            predictions.append(test_char_probs)\n","\n","            del state, test_token_probs, model; gc.collect()\n","            torch.cuda.empty_cache()\n","\n","        predictions = np.mean(predictions, axis=0)\n","        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n","        test[CFG.target_col] = predicted_location_str\n","        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n","        test[[CFG.id_col, CFG.target_col]].to_csv(\n","            CFG.output_dir / \"submission.csv\", index=False\n","        )"]},{"cell_type":"code","execution_count":31,"id":"serious-bunny","metadata":{"id":"serious-bunny","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["e667b88bf635469cb1bb33a2f16b47d7","630ddeaba5074e83acd2a71fbf0764c6","0655b1f6b47a4033a6b81b9fe8df1f84","b19bc8864c214e1ba648d1da78907107","5282a4fd8bf8478d81a2ac1b6c91a6f1","f1ffc87e487a4337a9d6a8924321ccea","015a6285c82a4711a668cbec66f0dc57","8330119346a34a42a98252aeed3d01a4","c2a998ebc00641cda55ec9669e821f11","b8d99a2cdfc54341ac348ba6a945db13","7b843aaec3a0468298200afe363fe370","d94e37f798aa4a3fbbca8ad1ce9c7b17","9f65d5dc5d664029b90de005149f4da9","b710be4e13354f76bf44d4401a1171e0","764cac9abfd54252bb89c4c7c14d9c9a","d8f66aeeb36f4635a9e67b5382461855","7768c4c6e4004f018aa08eb770757742","05fa8478577a45ff8410c3157f09408e","48b16a9d04b84c2887b4be3c4383e3da","f108d69cf6694ee496e71f7a8c0be61e","02e2f16e6e8b4488a258d1c1a258838e","faf4ef578ff44c758e60514fad54ff6b","1964597bf33240f9922040375ddbaa0d","72e58439a4844bffb9356278dabb29b3","2f721aed13db457aba058392f705d61f","a37104e5ed7145ff96bbae8c50f57b68","7980c58dbe40490f9f21a324b2341103","710aa1157dc4434fb71cf495d0487aa4","33a0bbe43d7c48ac944051b2694f3059","342054a6d1f84877ab79b38816678015","7cb8cac105174f5d832800d2f58016cc","fa9f62a520254ff9a50a3edef12dad43","e4e37996a7ff4fca9f3d6df949460290","19e3faa2fd19480dbac0c464cbdedd49","0080d0e9ec74475cbff084bae4c9dc01","ae164b8f00f24ef68e49001b625b04b3","feb44c53263c4e5d81e4f6da44e3ebbc","b253b6f5ba1f46fb822e612357c15b0b","6b85dedce6e240fd8d9b93e47f001eab","6a48a45186e44af2bf77050e27cabc14","f86a578350774fbd9a93f1bdfc9cf94e","c2d6039b921f48f89066a8c427765a89","5c61ab375b0d4fa7933fc51d9a622ef5","55a20a14af4b44fbaf735a1212c66b37","63b6cb90f56a439ba2446054dc575f22","9e7d8d57ed9f45409a189d693aa1589c","396a1f40091a4931bb2068e56c2c625f","ce0b11ae3be6428c97fce3ed847ce9ca","e13b7aed842d4247bee36b9980ff6b45","574e390e037e4f65bfddf30a49457102","42e06f7f8c3b42cf97a1fce36cdf873e","0f4130b7420e45bdb143e569e988fba4","b21886ca05e44453907225be7161fef6","21b0a0410e2e4b66afa7e47784bffdcd","c5afb03f9f9b4e509b2d4a360d765b95"]},"executionInfo":{"status":"ok","timestamp":1649267864640,"user_tz":-540,"elapsed":19764524,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"58625339-dd96-4522-f5a0-34d468856e46"},"outputs":[{"output_type":"stream","name":"stdout","text":["========== fold: 0 training ==========\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/833M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e667b88bf635469cb1bb33a2f16b47d7"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n","Epoch: [1][0/3575] Elapsed 0m 0s (remain 54m 53s) Loss: 0.2461(0.2461) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/3575] Elapsed 0m 24s (remain 14m 14s) Loss: 0.1822(0.2345) Grad: 50926.4531  LR: 0.000001  \n","Epoch: [1][200/3575] Elapsed 0m 46s (remain 13m 6s) Loss: 0.0429(0.1645) Grad: 5247.2964  LR: 0.000002  \n","Epoch: [1][300/3575] Elapsed 1m 8s (remain 12m 27s) Loss: 0.0478(0.1230) Grad: 4438.7427  LR: 0.000003  \n","Epoch: [1][400/3575] Elapsed 1m 30s (remain 11m 54s) Loss: 0.0315(0.0983) Grad: 19193.1191  LR: 0.000004  \n","Epoch: [1][500/3575] Elapsed 1m 52s (remain 11m 31s) Loss: 0.0049(0.0822) Grad: 3747.8110  LR: 0.000006  \n","Epoch: [1][600/3575] Elapsed 2m 18s (remain 11m 23s) Loss: 0.0097(0.0712) Grad: 4889.3853  LR: 0.000007  \n","Epoch: [1][700/3575] Elapsed 2m 41s (remain 11m 0s) Loss: 0.0049(0.0636) Grad: 1836.4138  LR: 0.000008  \n","Epoch: [1][800/3575] Elapsed 3m 3s (remain 10m 33s) Loss: 0.0002(0.0572) Grad: 194.9809  LR: 0.000009  \n","Epoch: [1][900/3575] Elapsed 3m 25s (remain 10m 8s) Loss: 0.0142(0.0524) Grad: 21266.9219  LR: 0.000010  \n","Epoch: [1][1000/3575] Elapsed 3m 48s (remain 9m 46s) Loss: 0.0019(0.0485) Grad: 1764.1464  LR: 0.000011  \n","Epoch: [1][1100/3575] Elapsed 4m 10s (remain 9m 23s) Loss: 0.0324(0.0452) Grad: 19594.8574  LR: 0.000012  \n","Epoch: [1][1200/3575] Elapsed 4m 32s (remain 8m 59s) Loss: 0.0053(0.0425) Grad: 7867.6348  LR: 0.000013  \n","Epoch: [1][1300/3575] Elapsed 4m 55s (remain 8m 35s) Loss: 0.0019(0.0402) Grad: 2436.1526  LR: 0.000015  \n","Epoch: [1][1400/3575] Elapsed 5m 17s (remain 8m 12s) Loss: 0.0249(0.0383) Grad: 4324.3872  LR: 0.000016  \n","Epoch: [1][1500/3575] Elapsed 5m 39s (remain 7m 49s) Loss: 0.0305(0.0364) Grad: 17382.1914  LR: 0.000017  \n","Epoch: [1][1600/3575] Elapsed 6m 1s (remain 7m 26s) Loss: 0.0666(0.0348) Grad: 45297.1172  LR: 0.000018  \n","Epoch: [1][1700/3575] Elapsed 6m 23s (remain 7m 2s) Loss: 0.0020(0.0335) Grad: 10618.6572  LR: 0.000019  \n","Epoch: [1][1800/3575] Elapsed 6m 45s (remain 6m 39s) Loss: 0.0022(0.0323) Grad: 1961.7493  LR: 0.000020  \n","Epoch: [1][1900/3575] Elapsed 7m 7s (remain 6m 16s) Loss: 0.0020(0.0312) Grad: 1873.2479  LR: 0.000020  \n","Epoch: [1][2000/3575] Elapsed 7m 29s (remain 5m 53s) Loss: 0.0637(0.0303) Grad: 24667.7305  LR: 0.000020  \n","Epoch: [1][2100/3575] Elapsed 7m 52s (remain 5m 31s) Loss: 0.0045(0.0293) Grad: 2127.7246  LR: 0.000020  \n","Epoch: [1][2200/3575] Elapsed 8m 14s (remain 5m 8s) Loss: 0.0001(0.0283) Grad: 144.8270  LR: 0.000019  \n","Epoch: [1][2300/3575] Elapsed 8m 36s (remain 4m 45s) Loss: 0.0044(0.0275) Grad: 2394.4958  LR: 0.000019  \n","Epoch: [1][2400/3575] Elapsed 8m 58s (remain 4m 23s) Loss: 0.0005(0.0268) Grad: 1361.4448  LR: 0.000019  \n","Epoch: [1][2500/3575] Elapsed 9m 20s (remain 4m 0s) Loss: 0.0096(0.0261) Grad: 8101.5015  LR: 0.000019  \n","Epoch: [1][2600/3575] Elapsed 9m 42s (remain 3m 38s) Loss: 0.0167(0.0255) Grad: 9649.0723  LR: 0.000019  \n","Epoch: [1][2700/3575] Elapsed 10m 5s (remain 3m 15s) Loss: 0.0003(0.0250) Grad: 573.3824  LR: 0.000019  \n","Epoch: [1][2800/3575] Elapsed 10m 27s (remain 2m 53s) Loss: 0.0053(0.0244) Grad: 2701.3992  LR: 0.000019  \n","Epoch: [1][2900/3575] Elapsed 10m 49s (remain 2m 30s) Loss: 0.0015(0.0238) Grad: 2143.1687  LR: 0.000019  \n","Epoch: [1][3000/3575] Elapsed 11m 11s (remain 2m 8s) Loss: 0.0294(0.0234) Grad: 14489.3105  LR: 0.000018  \n","Epoch: [1][3100/3575] Elapsed 11m 34s (remain 1m 46s) Loss: 0.0001(0.0230) Grad: 87.1635  LR: 0.000018  \n","Epoch: [1][3200/3575] Elapsed 11m 56s (remain 1m 23s) Loss: 0.0010(0.0225) Grad: 511.7127  LR: 0.000018  \n","Epoch: [1][3300/3575] Elapsed 12m 18s (remain 1m 1s) Loss: 0.0003(0.0221) Grad: 451.8693  LR: 0.000018  \n","Epoch: [1][3400/3575] Elapsed 12m 40s (remain 0m 38s) Loss: 0.0030(0.0216) Grad: 5301.6895  LR: 0.000018  \n","Epoch: [1][3500/3575] Elapsed 13m 2s (remain 0m 16s) Loss: 0.0048(0.0214) Grad: 10243.5381  LR: 0.000018  \n","Epoch: [1][3574/3575] Elapsed 13m 19s (remain 0m 0s) Loss: 0.0009(0.0211) Grad: 1161.6772  LR: 0.000018  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 43s) Loss: 0.0006(0.0006) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 6s) Loss: 0.0099(0.0047) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 53s) Loss: 0.0126(0.0067) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 41s) Loss: 0.0169(0.0072) \n","EVAL: [400/1192] Elapsed 0m 45s (remain 1m 30s) Loss: 0.0053(0.0071) \n","EVAL: [500/1192] Elapsed 0m 57s (remain 1m 18s) Loss: 0.0078(0.0068) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 7s) Loss: 0.0027(0.0075) \n","EVAL: [700/1192] Elapsed 1m 19s (remain 0m 55s) Loss: 0.0764(0.0087) \n","EVAL: [800/1192] Elapsed 1m 30s (remain 0m 44s) Loss: 0.0054(0.0088) \n","EVAL: [900/1192] Elapsed 1m 42s (remain 0m 33s) Loss: 0.0092(0.0088) \n","EVAL: [1000/1192] Elapsed 1m 53s (remain 0m 21s) Loss: 0.0002(0.0087) \n","EVAL: [1100/1192] Elapsed 2m 4s (remain 0m 10s) Loss: 0.0091(0.0086) \n","EVAL: [1191/1192] Elapsed 2m 14s (remain 0m 0s) Loss: 0.0001(0.0083) \n","Epoch 1 - avg_train_loss: 0.0211  avg_val_loss: 0.0083  time: 949s\n","Epoch 1 - Score: 0.8231\n","Epoch 1 - Save Best Score: 0.8231 Model\n","Epoch: [2][0/3575] Elapsed 0m 0s (remain 31m 2s) Loss: 0.0002(0.0002) Grad: 1259.4016  LR: 0.000018  \n","Epoch: [2][100/3575] Elapsed 0m 26s (remain 15m 20s) Loss: 0.0001(0.0062) Grad: 248.6377  LR: 0.000018  \n","Epoch: [2][200/3575] Elapsed 0m 50s (remain 14m 11s) Loss: 0.0008(0.0059) Grad: 5835.5283  LR: 0.000018  \n","Epoch: [2][300/3575] Elapsed 1m 12s (remain 13m 11s) Loss: 0.0055(0.0055) Grad: 5203.3130  LR: 0.000017  \n","Epoch: [2][400/3575] Elapsed 1m 35s (remain 12m 32s) Loss: 0.0039(0.0062) Grad: 19866.3086  LR: 0.000017  \n","Epoch: [2][500/3575] Elapsed 1m 57s (remain 12m 0s) Loss: 0.0001(0.0062) Grad: 536.3441  LR: 0.000017  \n","Epoch: [2][600/3575] Elapsed 2m 19s (remain 11m 27s) Loss: 0.0089(0.0064) Grad: 27284.8652  LR: 0.000017  \n","Epoch: [2][700/3575] Elapsed 2m 41s (remain 11m 0s) Loss: 0.0104(0.0063) Grad: 37370.0117  LR: 0.000017  \n","Epoch: [2][800/3575] Elapsed 3m 3s (remain 10m 35s) Loss: 0.0000(0.0066) Grad: 125.3695  LR: 0.000017  \n","Epoch: [2][900/3575] Elapsed 3m 26s (remain 10m 11s) Loss: 0.0054(0.0067) Grad: 30506.9785  LR: 0.000017  \n","Epoch: [2][1000/3575] Elapsed 3m 48s (remain 9m 46s) Loss: 0.0165(0.0071) Grad: 12395.7158  LR: 0.000017  \n","Epoch: [2][1100/3575] Elapsed 4m 10s (remain 9m 23s) Loss: 0.0846(0.0071) Grad: 192936.8125  LR: 0.000016  \n","Epoch: [2][1200/3575] Elapsed 4m 33s (remain 9m 0s) Loss: 0.0034(0.0070) Grad: 3198.8379  LR: 0.000016  \n","Epoch: [2][1300/3575] Elapsed 4m 55s (remain 8m 36s) Loss: 0.0137(0.0071) Grad: 60098.6250  LR: 0.000016  \n","Epoch: [2][1400/3575] Elapsed 5m 17s (remain 8m 13s) Loss: 0.0045(0.0070) Grad: 19865.0527  LR: 0.000016  \n","Epoch: [2][1500/3575] Elapsed 5m 40s (remain 7m 49s) Loss: 0.0012(0.0069) Grad: 7587.2646  LR: 0.000016  \n","Epoch: [2][1600/3575] Elapsed 6m 2s (remain 7m 27s) Loss: 0.0019(0.0068) Grad: 28216.8477  LR: 0.000016  \n","Epoch: [2][1700/3575] Elapsed 6m 25s (remain 7m 4s) Loss: 0.0001(0.0068) Grad: 173.9024  LR: 0.000016  \n","Epoch: [2][1800/3575] Elapsed 6m 48s (remain 6m 42s) Loss: 0.0036(0.0068) Grad: 125628.9297  LR: 0.000016  \n","Epoch: [2][1900/3575] Elapsed 7m 10s (remain 6m 19s) Loss: 0.0086(0.0068) Grad: 60188.0625  LR: 0.000015  \n","Epoch: [2][2000/3575] Elapsed 7m 33s (remain 5m 56s) Loss: 0.0017(0.0068) Grad: 8922.6045  LR: 0.000015  \n","Epoch: [2][2100/3575] Elapsed 7m 55s (remain 5m 33s) Loss: 0.0000(0.0068) Grad: 61.9894  LR: 0.000015  \n","Epoch: [2][2200/3575] Elapsed 8m 17s (remain 5m 10s) Loss: 0.0038(0.0068) Grad: 9953.7471  LR: 0.000015  \n","Epoch: [2][2300/3575] Elapsed 8m 39s (remain 4m 47s) Loss: 0.0088(0.0068) Grad: 4208.5576  LR: 0.000015  \n","Epoch: [2][2400/3575] Elapsed 9m 1s (remain 4m 24s) Loss: 0.0026(0.0067) Grad: 18304.7188  LR: 0.000015  \n","Epoch: [2][2500/3575] Elapsed 9m 23s (remain 4m 1s) Loss: 0.0002(0.0067) Grad: 551.4228  LR: 0.000015  \n","Epoch: [2][2600/3575] Elapsed 9m 45s (remain 3m 39s) Loss: 0.0029(0.0067) Grad: 2390.8491  LR: 0.000015  \n","Epoch: [2][2700/3575] Elapsed 10m 7s (remain 3m 16s) Loss: 0.0001(0.0068) Grad: 155.0171  LR: 0.000014  \n","Epoch: [2][2800/3575] Elapsed 10m 29s (remain 2m 53s) Loss: 0.0388(0.0068) Grad: 85045.5703  LR: 0.000014  \n","Epoch: [2][2900/3575] Elapsed 10m 51s (remain 2m 31s) Loss: 0.0238(0.0068) Grad: 19381.7188  LR: 0.000014  \n","Epoch: [2][3000/3575] Elapsed 11m 13s (remain 2m 8s) Loss: 0.0362(0.0068) Grad: 134351.5156  LR: 0.000014  \n","Epoch: [2][3100/3575] Elapsed 11m 35s (remain 1m 46s) Loss: 0.0226(0.0068) Grad: 25472.6934  LR: 0.000014  \n","Epoch: [2][3200/3575] Elapsed 11m 57s (remain 1m 23s) Loss: 0.0000(0.0068) Grad: 45.5954  LR: 0.000014  \n","Epoch: [2][3300/3575] Elapsed 12m 19s (remain 1m 1s) Loss: 0.0008(0.0068) Grad: 3519.8298  LR: 0.000014  \n","Epoch: [2][3400/3575] Elapsed 12m 42s (remain 0m 38s) Loss: 0.0027(0.0068) Grad: 3437.8447  LR: 0.000014  \n","Epoch: [2][3500/3575] Elapsed 13m 4s (remain 0m 16s) Loss: 0.0012(0.0068) Grad: 2629.6113  LR: 0.000013  \n","Epoch: [2][3574/3575] Elapsed 13m 20s (remain 0m 0s) Loss: 0.0168(0.0069) Grad: 43598.3477  LR: 0.000013  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 7m 43s) Loss: 0.0001(0.0001) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 6s) Loss: 0.0253(0.0059) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 53s) Loss: 0.0040(0.0063) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 41s) Loss: 0.0049(0.0073) \n","EVAL: [400/1192] Elapsed 0m 45s (remain 1m 29s) Loss: 0.0078(0.0075) \n","EVAL: [500/1192] Elapsed 0m 56s (remain 1m 18s) Loss: 0.0073(0.0069) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 6s) Loss: 0.0066(0.0071) \n","EVAL: [700/1192] Elapsed 1m 19s (remain 0m 55s) Loss: 0.0709(0.0080) \n","EVAL: [800/1192] Elapsed 1m 30s (remain 0m 44s) Loss: 0.0034(0.0083) \n","EVAL: [900/1192] Elapsed 1m 41s (remain 0m 32s) Loss: 0.0017(0.0084) \n","EVAL: [1000/1192] Elapsed 1m 53s (remain 0m 21s) Loss: 0.0000(0.0082) \n","EVAL: [1100/1192] Elapsed 2m 4s (remain 0m 10s) Loss: 0.0012(0.0078) \n","EVAL: [1191/1192] Elapsed 2m 14s (remain 0m 0s) Loss: 0.0000(0.0076) \n","Epoch 2 - avg_train_loss: 0.0069  avg_val_loss: 0.0076  time: 940s\n","Epoch 2 - Score: 0.8642\n","Epoch 2 - Save Best Score: 0.8642 Model\n","Epoch: [3][0/3575] Elapsed 0m 0s (remain 29m 58s) Loss: 0.0035(0.0035) Grad: 8231.9941  LR: 0.000013  \n","Epoch: [3][100/3575] Elapsed 0m 25s (remain 14m 45s) Loss: 0.0094(0.0076) Grad: 24262.6582  LR: 0.000013  \n","Epoch: [3][200/3575] Elapsed 0m 50s (remain 14m 3s) Loss: 0.0040(0.0060) Grad: 10449.0625  LR: 0.000013  \n","Epoch: [3][300/3575] Elapsed 1m 12s (remain 13m 3s) Loss: 0.0119(0.0059) Grad: 27079.7344  LR: 0.000013  \n","Epoch: [3][400/3575] Elapsed 1m 34s (remain 12m 24s) Loss: 0.0000(0.0054) Grad: 698.2628  LR: 0.000013  \n","Epoch: [3][500/3575] Elapsed 1m 56s (remain 11m 51s) Loss: 0.0027(0.0050) Grad: 119218.9297  LR: 0.000013  \n","Epoch: [3][600/3575] Elapsed 2m 18s (remain 11m 27s) Loss: 0.0053(0.0051) Grad: 7356.1479  LR: 0.000013  \n","Epoch: [3][700/3575] Elapsed 2m 41s (remain 11m 2s) Loss: 0.0005(0.0053) Grad: 22709.7168  LR: 0.000012  \n","Epoch: [3][800/3575] Elapsed 3m 4s (remain 10m 39s) Loss: 0.0230(0.0052) Grad: 62359.3359  LR: 0.000012  \n","Epoch: [3][900/3575] Elapsed 3m 27s (remain 10m 16s) Loss: 0.0119(0.0054) Grad: 19967.9785  LR: 0.000012  \n","Epoch: [3][1000/3575] Elapsed 3m 50s (remain 9m 52s) Loss: 0.0297(0.0055) Grad: 53132.7930  LR: 0.000012  \n","Epoch: [3][1100/3575] Elapsed 4m 12s (remain 9m 28s) Loss: 0.0028(0.0055) Grad: 15558.5537  LR: 0.000012  \n","Epoch: [3][1200/3575] Elapsed 4m 34s (remain 9m 3s) Loss: 0.0055(0.0055) Grad: 16029.9209  LR: 0.000012  \n","Epoch: [3][1300/3575] Elapsed 4m 57s (remain 8m 40s) Loss: 0.0009(0.0054) Grad: 5723.0596  LR: 0.000012  \n","Epoch: [3][1400/3575] Elapsed 5m 20s (remain 8m 17s) Loss: 0.0000(0.0054) Grad: 109.7236  LR: 0.000012  \n","Epoch: [3][1500/3575] Elapsed 5m 42s (remain 7m 53s) Loss: 0.0074(0.0053) Grad: 18768.6699  LR: 0.000011  \n","Epoch: [3][1600/3575] Elapsed 6m 5s (remain 7m 30s) Loss: 0.0020(0.0053) Grad: 6091.8818  LR: 0.000011  \n","Epoch: [3][1700/3575] Elapsed 6m 27s (remain 7m 7s) Loss: 0.0001(0.0053) Grad: 627.7933  LR: 0.000011  \n","Epoch: [3][1800/3575] Elapsed 6m 49s (remain 6m 43s) Loss: 0.0288(0.0052) Grad: 29811.1230  LR: 0.000011  \n","Epoch: [3][1900/3575] Elapsed 7m 12s (remain 6m 20s) Loss: 0.0048(0.0052) Grad: 9960.5098  LR: 0.000011  \n","Epoch: [3][2000/3575] Elapsed 7m 34s (remain 5m 57s) Loss: 0.0000(0.0053) Grad: 90.2800  LR: 0.000011  \n","Epoch: [3][2100/3575] Elapsed 7m 56s (remain 5m 34s) Loss: 0.0059(0.0053) Grad: 92351.2266  LR: 0.000011  \n","Epoch: [3][2200/3575] Elapsed 8m 19s (remain 5m 11s) Loss: 0.0182(0.0053) Grad: 61591.4883  LR: 0.000011  \n","Epoch: [3][2300/3575] Elapsed 8m 41s (remain 4m 48s) Loss: 0.0038(0.0052) Grad: 21093.7012  LR: 0.000010  \n","Epoch: [3][2400/3575] Elapsed 9m 3s (remain 4m 25s) Loss: 0.0027(0.0053) Grad: 11314.4951  LR: 0.000010  \n","Epoch: [3][2500/3575] Elapsed 9m 26s (remain 4m 3s) Loss: 0.0000(0.0053) Grad: 635.6837  LR: 0.000010  \n","Epoch: [3][2600/3575] Elapsed 9m 48s (remain 3m 40s) Loss: 0.0087(0.0053) Grad: 5241.6445  LR: 0.000010  \n","Epoch: [3][2700/3575] Elapsed 10m 11s (remain 3m 17s) Loss: 0.0005(0.0054) Grad: 3572.8081  LR: 0.000010  \n","Epoch: [3][2800/3575] Elapsed 10m 33s (remain 2m 54s) Loss: 0.0400(0.0055) Grad: 225678.4688  LR: 0.000010  \n","Epoch: [3][2900/3575] Elapsed 10m 55s (remain 2m 32s) Loss: 0.0161(0.0055) Grad: 99286.4922  LR: 0.000010  \n","Epoch: [3][3000/3575] Elapsed 11m 17s (remain 2m 9s) Loss: 0.0006(0.0055) Grad: 3035.2834  LR: 0.000010  \n","Epoch: [3][3100/3575] Elapsed 11m 40s (remain 1m 47s) Loss: 0.0000(0.0055) Grad: 48.1751  LR: 0.000009  \n","Epoch: [3][3200/3575] Elapsed 12m 2s (remain 1m 24s) Loss: 0.0100(0.0054) Grad: 28815.5938  LR: 0.000009  \n","Epoch: [3][3300/3575] Elapsed 12m 24s (remain 1m 1s) Loss: 0.0142(0.0054) Grad: 53443.6797  LR: 0.000009  \n","Epoch: [3][3400/3575] Elapsed 12m 46s (remain 0m 39s) Loss: 0.0028(0.0053) Grad: 6828.5938  LR: 0.000009  \n","Epoch: [3][3500/3575] Elapsed 13m 9s (remain 0m 16s) Loss: 0.0000(0.0054) Grad: 9.1232  LR: 0.000009  \n","Epoch: [3][3574/3575] Elapsed 13m 25s (remain 0m 0s) Loss: 0.0060(0.0053) Grad: 54593.4492  LR: 0.000009  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 18s) Loss: 0.0001(0.0001) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 5s) Loss: 0.0117(0.0053) \n","EVAL: [200/1192] Elapsed 0m 22s (remain 1m 52s) Loss: 0.0076(0.0065) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 41s) Loss: 0.0079(0.0074) \n","EVAL: [400/1192] Elapsed 0m 45s (remain 1m 29s) Loss: 0.0102(0.0076) \n","EVAL: [500/1192] Elapsed 0m 56s (remain 1m 18s) Loss: 0.0085(0.0070) \n","EVAL: [600/1192] Elapsed 1m 7s (remain 1m 6s) Loss: 0.0039(0.0073) \n","EVAL: [700/1192] Elapsed 1m 19s (remain 0m 55s) Loss: 0.0829(0.0082) \n","EVAL: [800/1192] Elapsed 1m 30s (remain 0m 44s) Loss: 0.0017(0.0084) \n","EVAL: [900/1192] Elapsed 1m 41s (remain 0m 32s) Loss: 0.0017(0.0085) \n","EVAL: [1000/1192] Elapsed 1m 52s (remain 0m 21s) Loss: 0.0000(0.0083) \n","EVAL: [1100/1192] Elapsed 2m 3s (remain 0m 10s) Loss: 0.0004(0.0080) \n","EVAL: [1191/1192] Elapsed 2m 14s (remain 0m 0s) Loss: 0.0000(0.0078) \n","Epoch 3 - avg_train_loss: 0.0053  avg_val_loss: 0.0078  time: 945s\n","Epoch 3 - Score: 0.8739\n","Epoch 3 - Save Best Score: 0.8739 Model\n","Epoch: [4][0/3575] Elapsed 0m 0s (remain 33m 15s) Loss: 0.0273(0.0273) Grad: 34932.7227  LR: 0.000009  \n","Epoch: [4][100/3575] Elapsed 0m 25s (remain 14m 46s) Loss: 0.0013(0.0024) Grad: 6876.0820  LR: 0.000009  \n","Epoch: [4][200/3575] Elapsed 0m 50s (remain 14m 0s) Loss: 0.0063(0.0032) Grad: 17189.5996  LR: 0.000009  \n","Epoch: [4][300/3575] Elapsed 1m 12s (remain 13m 4s) Loss: 0.0000(0.0037) Grad: 115.6842  LR: 0.000009  \n","Epoch: [4][400/3575] Elapsed 1m 34s (remain 12m 26s) Loss: 0.0000(0.0036) Grad: 273.3249  LR: 0.000008  \n","Epoch: [4][500/3575] Elapsed 1m 56s (remain 11m 53s) Loss: 0.0015(0.0038) Grad: 13299.0830  LR: 0.000008  \n","Epoch: [4][600/3575] Elapsed 2m 18s (remain 11m 24s) Loss: 0.0154(0.0042) Grad: 233178.5312  LR: 0.000008  \n","Epoch: [4][700/3575] Elapsed 2m 40s (remain 10m 58s) Loss: 0.0008(0.0041) Grad: 4782.5439  LR: 0.000008  \n","Epoch: [4][800/3575] Elapsed 3m 2s (remain 10m 31s) Loss: 0.0337(0.0043) Grad: 76233.0234  LR: 0.000008  \n","Epoch: [4][900/3575] Elapsed 3m 24s (remain 10m 5s) Loss: 0.0000(0.0042) Grad: 161.0942  LR: 0.000008  \n","Epoch: [4][1000/3575] Elapsed 3m 46s (remain 9m 42s) Loss: 0.0041(0.0042) Grad: 12743.8232  LR: 0.000008  \n","Epoch: [4][1100/3575] Elapsed 4m 8s (remain 9m 18s) Loss: 0.0075(0.0043) Grad: 7824.0776  LR: 0.000008  \n","Epoch: [4][1200/3575] Elapsed 4m 30s (remain 8m 55s) Loss: 0.0004(0.0043) Grad: 2262.7356  LR: 0.000007  \n","Epoch: [4][1300/3575] Elapsed 4m 52s (remain 8m 31s) Loss: 0.0000(0.0042) Grad: 13.0134  LR: 0.000007  \n","Epoch: [4][1400/3575] Elapsed 5m 14s (remain 8m 8s) Loss: 0.0235(0.0042) Grad: 43114.8477  LR: 0.000007  \n","Epoch: [4][1500/3575] Elapsed 5m 36s (remain 7m 45s) Loss: 0.0000(0.0042) Grad: 131.4273  LR: 0.000007  \n","Epoch: [4][1600/3575] Elapsed 5m 58s (remain 7m 22s) Loss: 0.0076(0.0042) Grad: 29200.2832  LR: 0.000007  \n","Epoch: [4][1700/3575] Elapsed 6m 20s (remain 6m 59s) Loss: 0.0000(0.0042) Grad: 13.6968  LR: 0.000007  \n","Epoch: [4][1800/3575] Elapsed 6m 42s (remain 6m 36s) Loss: 0.0007(0.0043) Grad: 4819.9805  LR: 0.000007  \n","Epoch: [4][1900/3575] Elapsed 7m 4s (remain 6m 14s) Loss: 0.0010(0.0042) Grad: 21463.4922  LR: 0.000007  \n","Epoch: [4][2000/3575] Elapsed 7m 26s (remain 5m 51s) Loss: 0.0000(0.0042) Grad: 231.8296  LR: 0.000006  \n","Epoch: [4][2100/3575] Elapsed 7m 48s (remain 5m 28s) Loss: 0.0000(0.0042) Grad: 55.6344  LR: 0.000006  \n","Epoch: [4][2200/3575] Elapsed 8m 11s (remain 5m 6s) Loss: 0.0358(0.0042) Grad: 87106.8672  LR: 0.000006  \n","Epoch: [4][2300/3575] Elapsed 8m 33s (remain 4m 44s) Loss: 0.0024(0.0042) Grad: 12631.8486  LR: 0.000006  \n","Epoch: [4][2400/3575] Elapsed 8m 55s (remain 4m 21s) Loss: 0.0000(0.0042) Grad: 62.1388  LR: 0.000006  \n","Epoch: [4][2500/3575] Elapsed 9m 17s (remain 3m 59s) Loss: 0.0059(0.0042) Grad: 61816.4766  LR: 0.000006  \n","Epoch: [4][2600/3575] Elapsed 9m 39s (remain 3m 37s) Loss: 0.0015(0.0042) Grad: 7174.4648  LR: 0.000006  \n","Epoch: [4][2700/3575] Elapsed 10m 1s (remain 3m 14s) Loss: 0.0000(0.0042) Grad: 40.5487  LR: 0.000006  \n","Epoch: [4][2800/3575] Elapsed 10m 23s (remain 2m 52s) Loss: 0.0008(0.0042) Grad: 5130.7104  LR: 0.000005  \n","Epoch: [4][2900/3575] Elapsed 10m 45s (remain 2m 30s) Loss: 0.0027(0.0042) Grad: 14645.8018  LR: 0.000005  \n","Epoch: [4][3000/3575] Elapsed 11m 8s (remain 2m 7s) Loss: 0.0000(0.0041) Grad: 39.9890  LR: 0.000005  \n","Epoch: [4][3100/3575] Elapsed 11m 30s (remain 1m 45s) Loss: 0.0001(0.0042) Grad: 540.4595  LR: 0.000005  \n","Epoch: [4][3200/3575] Elapsed 11m 52s (remain 1m 23s) Loss: 0.0000(0.0042) Grad: 113.4693  LR: 0.000005  \n","Epoch: [4][3300/3575] Elapsed 12m 15s (remain 1m 1s) Loss: 0.0005(0.0042) Grad: 3173.2815  LR: 0.000005  \n","Epoch: [4][3400/3575] Elapsed 12m 37s (remain 0m 38s) Loss: 0.0093(0.0041) Grad: 104906.1406  LR: 0.000005  \n","Epoch: [4][3500/3575] Elapsed 12m 59s (remain 0m 16s) Loss: 0.0000(0.0041) Grad: 38.2777  LR: 0.000005  \n","Epoch: [4][3574/3575] Elapsed 13m 15s (remain 0m 0s) Loss: 0.0001(0.0041) Grad: 1756.8466  LR: 0.000004  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 15s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 6s) Loss: 0.0152(0.0063) \n","EVAL: [200/1192] Elapsed 0m 22s (remain 1m 52s) Loss: 0.0081(0.0075) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 41s) Loss: 0.0108(0.0084) \n","EVAL: [400/1192] Elapsed 0m 45s (remain 1m 29s) Loss: 0.0111(0.0084) \n","EVAL: [500/1192] Elapsed 0m 56s (remain 1m 18s) Loss: 0.0052(0.0078) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 6s) Loss: 0.0003(0.0082) \n","EVAL: [700/1192] Elapsed 1m 19s (remain 0m 55s) Loss: 0.0941(0.0093) \n","EVAL: [800/1192] Elapsed 1m 30s (remain 0m 44s) Loss: 0.0003(0.0095) \n","EVAL: [900/1192] Elapsed 1m 41s (remain 0m 32s) Loss: 0.0013(0.0096) \n","EVAL: [1000/1192] Elapsed 1m 53s (remain 0m 21s) Loss: 0.0000(0.0095) \n","EVAL: [1100/1192] Elapsed 2m 4s (remain 0m 10s) Loss: 0.0005(0.0091) \n","EVAL: [1191/1192] Elapsed 2m 14s (remain 0m 0s) Loss: 0.0000(0.0089) \n","Epoch 4 - avg_train_loss: 0.0041  avg_val_loss: 0.0089  time: 935s\n","Epoch 4 - Score: 0.8766\n","Epoch 4 - Save Best Score: 0.8766 Model\n","Epoch: [5][0/3575] Elapsed 0m 0s (remain 30m 37s) Loss: 0.0001(0.0001) Grad: 1176.3574  LR: 0.000004  \n","Epoch: [5][100/3575] Elapsed 0m 24s (remain 13m 57s) Loss: 0.0000(0.0040) Grad: 5.8379  LR: 0.000004  \n","Epoch: [5][200/3575] Elapsed 0m 48s (remain 13m 32s) Loss: 0.0074(0.0035) Grad: 18195.7285  LR: 0.000004  \n","Epoch: [5][300/3575] Elapsed 1m 10s (remain 12m 44s) Loss: 0.0000(0.0034) Grad: 59.8469  LR: 0.000004  \n","Epoch: [5][400/3575] Elapsed 1m 32s (remain 12m 11s) Loss: 0.0239(0.0035) Grad: 21751.8086  LR: 0.000004  \n","Epoch: [5][500/3575] Elapsed 1m 54s (remain 11m 43s) Loss: 0.0001(0.0033) Grad: 308.0439  LR: 0.000004  \n","Epoch: [5][600/3575] Elapsed 2m 16s (remain 11m 17s) Loss: 0.0002(0.0034) Grad: 1857.5470  LR: 0.000004  \n","Epoch: [5][700/3575] Elapsed 2m 39s (remain 10m 52s) Loss: 0.0000(0.0033) Grad: 21.4174  LR: 0.000004  \n","Epoch: [5][800/3575] Elapsed 3m 1s (remain 10m 28s) Loss: 0.0001(0.0033) Grad: 303.3348  LR: 0.000003  \n","Epoch: [5][900/3575] Elapsed 3m 23s (remain 10m 3s) Loss: 0.0000(0.0033) Grad: 16.1781  LR: 0.000003  \n","Epoch: [5][1000/3575] Elapsed 3m 45s (remain 9m 39s) Loss: 0.0000(0.0032) Grad: 42.0471  LR: 0.000003  \n","Epoch: [5][1100/3575] Elapsed 4m 7s (remain 9m 16s) Loss: 0.0011(0.0032) Grad: 3258.0195  LR: 0.000003  \n","Epoch: [5][1200/3575] Elapsed 4m 30s (remain 8m 53s) Loss: 0.0000(0.0033) Grad: 25.3346  LR: 0.000003  \n","Epoch: [5][1300/3575] Elapsed 4m 52s (remain 8m 30s) Loss: 0.0014(0.0033) Grad: 6828.5679  LR: 0.000003  \n","Epoch: [5][1400/3575] Elapsed 5m 14s (remain 8m 7s) Loss: 0.0000(0.0033) Grad: 174.0686  LR: 0.000003  \n","Epoch: [5][1500/3575] Elapsed 5m 36s (remain 7m 44s) Loss: 0.0000(0.0033) Grad: 23.0281  LR: 0.000003  \n","Epoch: [5][1600/3575] Elapsed 5m 58s (remain 7m 21s) Loss: 0.0000(0.0032) Grad: 264.7716  LR: 0.000002  \n","Epoch: [5][1700/3575] Elapsed 6m 20s (remain 6m 59s) Loss: 0.0004(0.0032) Grad: 3198.0173  LR: 0.000002  \n","Epoch: [5][1800/3575] Elapsed 6m 42s (remain 6m 36s) Loss: 0.0000(0.0033) Grad: 16.2188  LR: 0.000002  \n","Epoch: [5][1900/3575] Elapsed 7m 5s (remain 6m 14s) Loss: 0.0001(0.0033) Grad: 277.3985  LR: 0.000002  \n","Epoch: [5][2000/3575] Elapsed 7m 27s (remain 5m 51s) Loss: 0.0001(0.0032) Grad: 217.3334  LR: 0.000002  \n","Epoch: [5][2100/3575] Elapsed 7m 49s (remain 5m 29s) Loss: 0.0003(0.0032) Grad: 2017.2526  LR: 0.000002  \n","Epoch: [5][2200/3575] Elapsed 8m 11s (remain 5m 6s) Loss: 0.0026(0.0033) Grad: 20062.3457  LR: 0.000002  \n","Epoch: [5][2300/3575] Elapsed 8m 34s (remain 4m 44s) Loss: 0.0022(0.0033) Grad: 4967.1533  LR: 0.000002  \n","Epoch: [5][2400/3575] Elapsed 8m 56s (remain 4m 22s) Loss: 0.0000(0.0033) Grad: 15.1633  LR: 0.000001  \n","Epoch: [5][2500/3575] Elapsed 9m 18s (remain 3m 59s) Loss: 0.0000(0.0032) Grad: 117.0186  LR: 0.000001  \n","Epoch: [5][2600/3575] Elapsed 9m 40s (remain 3m 37s) Loss: 0.0146(0.0032) Grad: 137294.7969  LR: 0.000001  \n","Epoch: [5][2700/3575] Elapsed 10m 2s (remain 3m 15s) Loss: 0.0001(0.0032) Grad: 338.9278  LR: 0.000001  \n","Epoch: [5][2800/3575] Elapsed 10m 24s (remain 2m 52s) Loss: 0.0000(0.0033) Grad: 12.0909  LR: 0.000001  \n","Epoch: [5][2900/3575] Elapsed 10m 46s (remain 2m 30s) Loss: 0.0044(0.0032) Grad: 4849.9243  LR: 0.000001  \n","Epoch: [5][3000/3575] Elapsed 11m 8s (remain 2m 7s) Loss: 0.0002(0.0032) Grad: 2632.6448  LR: 0.000001  \n","Epoch: [5][3100/3575] Elapsed 11m 30s (remain 1m 45s) Loss: 0.0015(0.0032) Grad: 33376.0352  LR: 0.000001  \n","Epoch: [5][3200/3575] Elapsed 11m 52s (remain 1m 23s) Loss: 0.0023(0.0032) Grad: 6539.0601  LR: 0.000000  \n","Epoch: [5][3300/3575] Elapsed 12m 14s (remain 1m 0s) Loss: 0.0000(0.0032) Grad: 3.4360  LR: 0.000000  \n","Epoch: [5][3400/3575] Elapsed 12m 36s (remain 0m 38s) Loss: 0.0000(0.0032) Grad: 7.3022  LR: 0.000000  \n","Epoch: [5][3500/3575] Elapsed 12m 59s (remain 0m 16s) Loss: 0.0020(0.0033) Grad: 20676.3809  LR: 0.000000  \n","Epoch: [5][3574/3575] Elapsed 13m 15s (remain 0m 0s) Loss: 0.0000(0.0033) Grad: 12.1150  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 27s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 7s) Loss: 0.0102(0.0060) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 54s) Loss: 0.0114(0.0069) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 42s) Loss: 0.0074(0.0078) \n","EVAL: [400/1192] Elapsed 0m 46s (remain 1m 30s) Loss: 0.0120(0.0078) \n","EVAL: [500/1192] Elapsed 0m 57s (remain 1m 19s) Loss: 0.0029(0.0074) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 7s) Loss: 0.0065(0.0077) \n","EVAL: [700/1192] Elapsed 1m 20s (remain 0m 56s) Loss: 0.0933(0.0088) \n","EVAL: [800/1192] Elapsed 1m 31s (remain 0m 44s) Loss: 0.0010(0.0091) \n","EVAL: [900/1192] Elapsed 1m 42s (remain 0m 33s) Loss: 0.0008(0.0092) \n","EVAL: [1000/1192] Elapsed 1m 54s (remain 0m 21s) Loss: 0.0000(0.0090) \n","EVAL: [1100/1192] Elapsed 2m 5s (remain 0m 10s) Loss: 0.0006(0.0087) \n","EVAL: [1191/1192] Elapsed 2m 15s (remain 0m 0s) Loss: 0.0000(0.0085) \n","Epoch 5 - avg_train_loss: 0.0033  avg_val_loss: 0.0085  time: 936s\n","Epoch 5 - Score: 0.8785\n","Epoch 5 - Save Best Score: 0.8785 Model\n","========== fold: 1 training ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n","Epoch: [1][0/3575] Elapsed 0m 0s (remain 40m 39s) Loss: 0.3723(0.3723) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/3575] Elapsed 0m 28s (remain 16m 10s) Loss: 0.2360(0.3206) Grad: 36891.8047  LR: 0.000001  \n","Epoch: [1][200/3575] Elapsed 0m 50s (remain 14m 13s) Loss: 0.0235(0.2163) Grad: 2085.8525  LR: 0.000002  \n","Epoch: [1][300/3575] Elapsed 1m 13s (remain 13m 17s) Loss: 0.0165(0.1566) Grad: 1928.4055  LR: 0.000003  \n","Epoch: [1][400/3575] Elapsed 1m 35s (remain 12m 36s) Loss: 0.0059(0.1225) Grad: 2640.1941  LR: 0.000004  \n","Epoch: [1][500/3575] Elapsed 1m 58s (remain 12m 4s) Loss: 0.0084(0.1016) Grad: 2487.0698  LR: 0.000006  \n","Epoch: [1][600/3575] Elapsed 2m 20s (remain 11m 35s) Loss: 0.0061(0.0869) Grad: 2232.3645  LR: 0.000007  \n","Epoch: [1][700/3575] Elapsed 2m 43s (remain 11m 8s) Loss: 0.0570(0.0765) Grad: 11991.8604  LR: 0.000008  \n","Epoch: [1][800/3575] Elapsed 3m 5s (remain 10m 42s) Loss: 0.0009(0.0688) Grad: 821.1017  LR: 0.000009  \n","Epoch: [1][900/3575] Elapsed 3m 28s (remain 10m 17s) Loss: 0.0004(0.0623) Grad: 246.0161  LR: 0.000010  \n","Epoch: [1][1000/3575] Elapsed 3m 50s (remain 9m 53s) Loss: 0.0024(0.0572) Grad: 1138.7546  LR: 0.000011  \n","Epoch: [1][1100/3575] Elapsed 4m 13s (remain 9m 28s) Loss: 0.0002(0.0532) Grad: 116.2700  LR: 0.000012  \n","Epoch: [1][1200/3575] Elapsed 4m 35s (remain 9m 5s) Loss: 0.0016(0.0497) Grad: 2810.5320  LR: 0.000013  \n","Epoch: [1][1300/3575] Elapsed 4m 59s (remain 8m 42s) Loss: 0.0050(0.0467) Grad: 2717.6592  LR: 0.000015  \n","Epoch: [1][1400/3575] Elapsed 5m 21s (remain 8m 19s) Loss: 0.0253(0.0440) Grad: 6359.7056  LR: 0.000016  \n","Epoch: [1][1500/3575] Elapsed 5m 44s (remain 7m 55s) Loss: 0.0003(0.0418) Grad: 94.2558  LR: 0.000017  \n","Epoch: [1][1600/3575] Elapsed 6m 6s (remain 7m 31s) Loss: 0.0245(0.0400) Grad: 2016.6713  LR: 0.000018  \n","Epoch: [1][1700/3575] Elapsed 6m 28s (remain 7m 8s) Loss: 0.0086(0.0383) Grad: 3079.5093  LR: 0.000019  \n","Epoch: [1][1800/3575] Elapsed 6m 51s (remain 6m 45s) Loss: 0.0043(0.0368) Grad: 2216.2126  LR: 0.000020  \n","Epoch: [1][1900/3575] Elapsed 7m 13s (remain 6m 21s) Loss: 0.0051(0.0354) Grad: 1907.3270  LR: 0.000020  \n","Epoch: [1][2000/3575] Elapsed 7m 35s (remain 5m 58s) Loss: 0.0099(0.0341) Grad: 7783.5728  LR: 0.000020  \n","Epoch: [1][2100/3575] Elapsed 7m 58s (remain 5m 35s) Loss: 0.0068(0.0329) Grad: 1984.6927  LR: 0.000020  \n","Epoch: [1][2200/3575] Elapsed 8m 20s (remain 5m 12s) Loss: 0.0002(0.0319) Grad: 119.1821  LR: 0.000019  \n","Epoch: [1][2300/3575] Elapsed 8m 42s (remain 4m 49s) Loss: 0.0045(0.0308) Grad: 1841.6334  LR: 0.000019  \n","Epoch: [1][2400/3575] Elapsed 9m 5s (remain 4m 26s) Loss: 0.0036(0.0299) Grad: 2103.5574  LR: 0.000019  \n","Epoch: [1][2500/3575] Elapsed 9m 27s (remain 4m 3s) Loss: 0.0516(0.0291) Grad: 8283.9102  LR: 0.000019  \n","Epoch: [1][2600/3575] Elapsed 9m 49s (remain 3m 40s) Loss: 0.0073(0.0283) Grad: 3255.4189  LR: 0.000019  \n","Epoch: [1][2700/3575] Elapsed 10m 11s (remain 3m 18s) Loss: 0.0259(0.0277) Grad: 4983.0645  LR: 0.000019  \n","Epoch: [1][2800/3575] Elapsed 10m 34s (remain 2m 55s) Loss: 0.0096(0.0270) Grad: 2420.9551  LR: 0.000019  \n","Epoch: [1][2900/3575] Elapsed 10m 56s (remain 2m 32s) Loss: 0.0001(0.0263) Grad: 57.7191  LR: 0.000019  \n","Epoch: [1][3000/3575] Elapsed 11m 18s (remain 2m 9s) Loss: 0.0018(0.0258) Grad: 449.8677  LR: 0.000018  \n","Epoch: [1][3100/3575] Elapsed 11m 40s (remain 1m 47s) Loss: 0.0022(0.0252) Grad: 1375.0494  LR: 0.000018  \n","Epoch: [1][3200/3575] Elapsed 12m 2s (remain 1m 24s) Loss: 0.0095(0.0246) Grad: 3056.0334  LR: 0.000018  \n","Epoch: [1][3300/3575] Elapsed 12m 25s (remain 1m 1s) Loss: 0.0009(0.0241) Grad: 527.4526  LR: 0.000018  \n","Epoch: [1][3400/3575] Elapsed 12m 47s (remain 0m 39s) Loss: 0.0034(0.0236) Grad: 1648.6473  LR: 0.000018  \n","Epoch: [1][3500/3575] Elapsed 13m 9s (remain 0m 16s) Loss: 0.0111(0.0232) Grad: 1756.5680  LR: 0.000018  \n","Epoch: [1][3574/3575] Elapsed 13m 25s (remain 0m 0s) Loss: 0.0002(0.0229) Grad: 56.3688  LR: 0.000018  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 39s) Loss: 0.0003(0.0003) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 6s) Loss: 0.0008(0.0045) \n","EVAL: [200/1192] Elapsed 0m 22s (remain 1m 53s) Loss: 0.0004(0.0083) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 41s) Loss: 0.0183(0.0104) \n","EVAL: [400/1192] Elapsed 0m 45s (remain 1m 30s) Loss: 0.0121(0.0108) \n","EVAL: [500/1192] Elapsed 0m 56s (remain 1m 18s) Loss: 0.0184(0.0103) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 7s) Loss: 0.1002(0.0103) \n","EVAL: [700/1192] Elapsed 1m 19s (remain 0m 55s) Loss: 0.0133(0.0111) \n","EVAL: [800/1192] Elapsed 1m 31s (remain 0m 44s) Loss: 0.0474(0.0107) \n","EVAL: [900/1192] Elapsed 1m 42s (remain 0m 33s) Loss: 0.0024(0.0106) \n","EVAL: [1000/1192] Elapsed 1m 53s (remain 0m 21s) Loss: 0.0002(0.0102) \n","EVAL: [1100/1192] Elapsed 2m 4s (remain 0m 10s) Loss: 0.0044(0.0099) \n","EVAL: [1191/1192] Elapsed 2m 15s (remain 0m 0s) Loss: 0.0051(0.0098) \n","Epoch 1 - avg_train_loss: 0.0229  avg_val_loss: 0.0098  time: 946s\n","Epoch 1 - Score: 0.8278\n","Epoch 1 - Save Best Score: 0.8278 Model\n","Epoch: [2][0/3575] Elapsed 0m 0s (remain 39m 6s) Loss: 0.0031(0.0031) Grad: 5439.6704  LR: 0.000018  \n","Epoch: [2][100/3575] Elapsed 0m 29s (remain 16m 37s) Loss: 0.0067(0.0066) Grad: 36136.5820  LR: 0.000018  \n","Epoch: [2][200/3575] Elapsed 0m 53s (remain 14m 57s) Loss: 0.0001(0.0070) Grad: 462.7928  LR: 0.000018  \n","Epoch: [2][300/3575] Elapsed 1m 16s (remain 13m 47s) Loss: 0.0001(0.0070) Grad: 180.0147  LR: 0.000017  \n","Epoch: [2][400/3575] Elapsed 1m 38s (remain 13m 0s) Loss: 0.0001(0.0065) Grad: 589.0858  LR: 0.000017  \n","Epoch: [2][500/3575] Elapsed 2m 0s (remain 12m 18s) Loss: 0.0019(0.0071) Grad: 13021.4189  LR: 0.000017  \n","Epoch: [2][600/3575] Elapsed 2m 22s (remain 11m 42s) Loss: 0.0026(0.0070) Grad: 12464.6982  LR: 0.000017  \n","Epoch: [2][700/3575] Elapsed 2m 44s (remain 11m 13s) Loss: 0.0252(0.0070) Grad: 14070.9150  LR: 0.000017  \n","Epoch: [2][800/3575] Elapsed 3m 5s (remain 10m 43s) Loss: 0.0129(0.0071) Grad: 22657.8398  LR: 0.000017  \n","Epoch: [2][900/3575] Elapsed 3m 27s (remain 10m 16s) Loss: 0.0038(0.0071) Grad: 2685.9070  LR: 0.000017  \n","Epoch: [2][1000/3575] Elapsed 3m 50s (remain 9m 51s) Loss: 0.0004(0.0071) Grad: 1069.9762  LR: 0.000017  \n","Epoch: [2][1100/3575] Elapsed 4m 12s (remain 9m 27s) Loss: 0.0071(0.0070) Grad: 13822.2900  LR: 0.000016  \n","Epoch: [2][1200/3575] Elapsed 4m 34s (remain 9m 3s) Loss: 0.0045(0.0069) Grad: 5768.7344  LR: 0.000016  \n","Epoch: [2][1300/3575] Elapsed 4m 57s (remain 8m 39s) Loss: 0.0025(0.0068) Grad: 51137.0078  LR: 0.000016  \n","Epoch: [2][1400/3575] Elapsed 5m 19s (remain 8m 15s) Loss: 0.0133(0.0067) Grad: 15477.2012  LR: 0.000016  \n","Epoch: [2][1500/3575] Elapsed 5m 42s (remain 7m 53s) Loss: 0.0001(0.0067) Grad: 740.6133  LR: 0.000016  \n","Epoch: [2][1600/3575] Elapsed 6m 5s (remain 7m 30s) Loss: 0.0010(0.0068) Grad: 5273.6162  LR: 0.000016  \n","Epoch: [2][1700/3575] Elapsed 6m 27s (remain 7m 6s) Loss: 0.0061(0.0068) Grad: 8806.5342  LR: 0.000016  \n","Epoch: [2][1800/3575] Elapsed 6m 50s (remain 6m 44s) Loss: 0.0002(0.0068) Grad: 557.8065  LR: 0.000016  \n","Epoch: [2][1900/3575] Elapsed 7m 12s (remain 6m 21s) Loss: 0.0019(0.0068) Grad: 19063.2363  LR: 0.000015  \n","Epoch: [2][2000/3575] Elapsed 7m 35s (remain 5m 57s) Loss: 0.0017(0.0067) Grad: 13766.6689  LR: 0.000015  \n","Epoch: [2][2100/3575] Elapsed 7m 57s (remain 5m 35s) Loss: 0.0025(0.0066) Grad: 9334.7520  LR: 0.000015  \n","Epoch: [2][2200/3575] Elapsed 8m 19s (remain 5m 12s) Loss: 0.0249(0.0066) Grad: 30576.0977  LR: 0.000015  \n","Epoch: [2][2300/3575] Elapsed 8m 42s (remain 4m 49s) Loss: 0.0005(0.0066) Grad: 4490.0400  LR: 0.000015  \n","Epoch: [2][2400/3575] Elapsed 9m 5s (remain 4m 26s) Loss: 0.0003(0.0066) Grad: 1074.2040  LR: 0.000015  \n","Epoch: [2][2500/3575] Elapsed 9m 27s (remain 4m 3s) Loss: 0.0010(0.0066) Grad: 2674.1057  LR: 0.000015  \n","Epoch: [2][2600/3575] Elapsed 9m 50s (remain 3m 40s) Loss: 0.0013(0.0066) Grad: 12566.4580  LR: 0.000015  \n","Epoch: [2][2700/3575] Elapsed 10m 12s (remain 3m 18s) Loss: 0.0000(0.0065) Grad: 61.5179  LR: 0.000014  \n","Epoch: [2][2800/3575] Elapsed 10m 35s (remain 2m 55s) Loss: 0.0109(0.0065) Grad: 16961.7207  LR: 0.000014  \n","Epoch: [2][2900/3575] Elapsed 10m 57s (remain 2m 32s) Loss: 0.0001(0.0065) Grad: 603.3433  LR: 0.000014  \n","Epoch: [2][3000/3575] Elapsed 11m 20s (remain 2m 10s) Loss: 0.0004(0.0065) Grad: 13507.9434  LR: 0.000014  \n","Epoch: [2][3100/3575] Elapsed 11m 42s (remain 1m 47s) Loss: 0.0001(0.0065) Grad: 884.7938  LR: 0.000014  \n","Epoch: [2][3200/3575] Elapsed 12m 5s (remain 1m 24s) Loss: 0.0005(0.0065) Grad: 3174.8904  LR: 0.000014  \n","Epoch: [2][3300/3575] Elapsed 12m 27s (remain 1m 2s) Loss: 0.0061(0.0064) Grad: 22127.9668  LR: 0.000014  \n","Epoch: [2][3400/3575] Elapsed 12m 50s (remain 0m 39s) Loss: 0.0221(0.0064) Grad: 26838.6426  LR: 0.000014  \n","Epoch: [2][3500/3575] Elapsed 13m 13s (remain 0m 16s) Loss: 0.0002(0.0064) Grad: 765.1230  LR: 0.000013  \n","Epoch: [2][3574/3575] Elapsed 13m 29s (remain 0m 0s) Loss: 0.0196(0.0063) Grad: 37267.3867  LR: 0.000013  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 20s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 8s) Loss: 0.0009(0.0054) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 54s) Loss: 0.0004(0.0076) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 42s) Loss: 0.0001(0.0108) \n","EVAL: [400/1192] Elapsed 0m 46s (remain 1m 31s) Loss: 0.0173(0.0111) \n","EVAL: [500/1192] Elapsed 0m 57s (remain 1m 19s) Loss: 0.0236(0.0105) \n","EVAL: [600/1192] Elapsed 1m 9s (remain 1m 7s) Loss: 0.1153(0.0104) \n","EVAL: [700/1192] Elapsed 1m 20s (remain 0m 56s) Loss: 0.0179(0.0115) \n","EVAL: [800/1192] Elapsed 1m 32s (remain 0m 44s) Loss: 0.0065(0.0108) \n","EVAL: [900/1192] Elapsed 1m 43s (remain 0m 33s) Loss: 0.0006(0.0105) \n","EVAL: [1000/1192] Elapsed 1m 54s (remain 0m 21s) Loss: 0.0000(0.0101) \n","EVAL: [1100/1192] Elapsed 2m 6s (remain 0m 10s) Loss: 0.0051(0.0095) \n","EVAL: [1191/1192] Elapsed 2m 16s (remain 0m 0s) Loss: 0.0081(0.0090) \n","Epoch 2 - avg_train_loss: 0.0063  avg_val_loss: 0.0090  time: 952s\n","Epoch 2 - Score: 0.8701\n","Epoch 2 - Save Best Score: 0.8701 Model\n","Epoch: [3][0/3575] Elapsed 0m 0s (remain 35m 4s) Loss: 0.0034(0.0034) Grad: 12202.6523  LR: 0.000013  \n","Epoch: [3][100/3575] Elapsed 0m 26s (remain 15m 24s) Loss: 0.0009(0.0051) Grad: 7123.7231  LR: 0.000013  \n","Epoch: [3][200/3575] Elapsed 0m 51s (remain 14m 29s) Loss: 0.0014(0.0049) Grad: 16101.6543  LR: 0.000013  \n","Epoch: [3][300/3575] Elapsed 1m 14s (remain 13m 31s) Loss: 0.0003(0.0048) Grad: 2337.7371  LR: 0.000013  \n","Epoch: [3][400/3575] Elapsed 1m 37s (remain 12m 49s) Loss: 0.0020(0.0046) Grad: 11253.4678  LR: 0.000013  \n","Epoch: [3][500/3575] Elapsed 1m 59s (remain 12m 14s) Loss: 0.0116(0.0047) Grad: 9878.3828  LR: 0.000013  \n","Epoch: [3][600/3575] Elapsed 2m 22s (remain 11m 46s) Loss: 0.0001(0.0048) Grad: 790.4009  LR: 0.000013  \n","Epoch: [3][700/3575] Elapsed 2m 45s (remain 11m 20s) Loss: 0.0003(0.0050) Grad: 7328.8643  LR: 0.000012  \n","Epoch: [3][800/3575] Elapsed 3m 8s (remain 10m 52s) Loss: 0.0075(0.0048) Grad: 57806.1094  LR: 0.000012  \n","Epoch: [3][900/3575] Elapsed 3m 31s (remain 10m 26s) Loss: 0.0045(0.0049) Grad: 10113.9434  LR: 0.000012  \n","Epoch: [3][1000/3575] Elapsed 3m 53s (remain 10m 1s) Loss: 0.0001(0.0048) Grad: 192.5536  LR: 0.000012  \n","Epoch: [3][1100/3575] Elapsed 4m 16s (remain 9m 35s) Loss: 0.0000(0.0050) Grad: 23.9518  LR: 0.000012  \n","Epoch: [3][1200/3575] Elapsed 4m 38s (remain 9m 11s) Loss: 0.0014(0.0048) Grad: 10861.9453  LR: 0.000012  \n","Epoch: [3][1300/3575] Elapsed 5m 1s (remain 8m 47s) Loss: 0.0005(0.0048) Grad: 4392.2139  LR: 0.000012  \n","Epoch: [3][1400/3575] Elapsed 5m 24s (remain 8m 22s) Loss: 0.0033(0.0048) Grad: 6193.6782  LR: 0.000012  \n","Epoch: [3][1500/3575] Elapsed 5m 46s (remain 7m 59s) Loss: 0.0000(0.0047) Grad: 46.8212  LR: 0.000011  \n","Epoch: [3][1600/3575] Elapsed 6m 9s (remain 7m 35s) Loss: 0.0010(0.0047) Grad: 10406.3701  LR: 0.000011  \n","Epoch: [3][1700/3575] Elapsed 6m 31s (remain 7m 11s) Loss: 0.0042(0.0047) Grad: 6952.9067  LR: 0.000011  \n","Epoch: [3][1800/3575] Elapsed 6m 54s (remain 6m 47s) Loss: 0.0002(0.0047) Grad: 993.8789  LR: 0.000011  \n","Epoch: [3][1900/3575] Elapsed 7m 16s (remain 6m 24s) Loss: 0.0002(0.0047) Grad: 1681.3799  LR: 0.000011  \n","Epoch: [3][2000/3575] Elapsed 7m 38s (remain 6m 1s) Loss: 0.0043(0.0048) Grad: 19601.2285  LR: 0.000011  \n","Epoch: [3][2100/3575] Elapsed 8m 1s (remain 5m 37s) Loss: 0.0002(0.0049) Grad: 861.6231  LR: 0.000011  \n","Epoch: [3][2200/3575] Elapsed 8m 23s (remain 5m 14s) Loss: 0.0000(0.0050) Grad: 89.8134  LR: 0.000011  \n","Epoch: [3][2300/3575] Elapsed 8m 45s (remain 4m 50s) Loss: 0.0081(0.0049) Grad: 29586.4043  LR: 0.000010  \n","Epoch: [3][2400/3575] Elapsed 9m 8s (remain 4m 27s) Loss: 0.0103(0.0049) Grad: 39811.3125  LR: 0.000010  \n","Epoch: [3][2500/3575] Elapsed 9m 30s (remain 4m 5s) Loss: 0.0006(0.0049) Grad: 3706.2896  LR: 0.000010  \n","Epoch: [3][2600/3575] Elapsed 9m 52s (remain 3m 41s) Loss: 0.0035(0.0049) Grad: 8677.2080  LR: 0.000010  \n","Epoch: [3][2700/3575] Elapsed 10m 15s (remain 3m 19s) Loss: 0.0013(0.0049) Grad: 10036.9561  LR: 0.000010  \n","Epoch: [3][2800/3575] Elapsed 10m 37s (remain 2m 56s) Loss: 0.0174(0.0049) Grad: 90560.1250  LR: 0.000010  \n","Epoch: [3][2900/3575] Elapsed 10m 59s (remain 2m 33s) Loss: 0.0001(0.0049) Grad: 368.1663  LR: 0.000010  \n","Epoch: [3][3000/3575] Elapsed 11m 22s (remain 2m 10s) Loss: 0.0191(0.0049) Grad: 31860.5176  LR: 0.000010  \n","Epoch: [3][3100/3575] Elapsed 11m 44s (remain 1m 47s) Loss: 0.0001(0.0049) Grad: 100.4272  LR: 0.000009  \n","Epoch: [3][3200/3575] Elapsed 12m 6s (remain 1m 24s) Loss: 0.0046(0.0049) Grad: 19897.7656  LR: 0.000009  \n","Epoch: [3][3300/3575] Elapsed 12m 28s (remain 1m 2s) Loss: 0.0044(0.0048) Grad: 7528.2407  LR: 0.000009  \n","Epoch: [3][3400/3575] Elapsed 12m 50s (remain 0m 39s) Loss: 0.0010(0.0048) Grad: 4921.0371  LR: 0.000009  \n","Epoch: [3][3500/3575] Elapsed 13m 13s (remain 0m 16s) Loss: 0.0012(0.0048) Grad: 6809.1079  LR: 0.000009  \n","Epoch: [3][3574/3575] Elapsed 13m 29s (remain 0m 0s) Loss: 0.0003(0.0048) Grad: 757.7612  LR: 0.000009  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 8s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 5s) Loss: 0.0002(0.0047) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 53s) Loss: 0.0003(0.0068) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 42s) Loss: 0.0002(0.0101) \n","EVAL: [400/1192] Elapsed 0m 45s (remain 1m 30s) Loss: 0.0235(0.0105) \n","EVAL: [500/1192] Elapsed 0m 57s (remain 1m 19s) Loss: 0.0277(0.0099) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 7s) Loss: 0.1297(0.0101) \n","EVAL: [700/1192] Elapsed 1m 20s (remain 0m 56s) Loss: 0.0133(0.0110) \n","EVAL: [800/1192] Elapsed 1m 31s (remain 0m 44s) Loss: 0.0058(0.0106) \n","EVAL: [900/1192] Elapsed 1m 43s (remain 0m 33s) Loss: 0.0012(0.0102) \n","EVAL: [1000/1192] Elapsed 1m 54s (remain 0m 21s) Loss: 0.0000(0.0100) \n","EVAL: [1100/1192] Elapsed 2m 5s (remain 0m 10s) Loss: 0.0053(0.0094) \n","EVAL: [1191/1192] Elapsed 2m 16s (remain 0m 0s) Loss: 0.0063(0.0088) \n","Epoch 3 - avg_train_loss: 0.0048  avg_val_loss: 0.0088  time: 952s\n","Epoch 3 - Score: 0.8726\n","Epoch 3 - Save Best Score: 0.8726 Model\n","Epoch: [4][0/3575] Elapsed 0m 0s (remain 34m 39s) Loss: 0.0016(0.0016) Grad: 14420.6143  LR: 0.000009  \n","Epoch: [4][100/3575] Elapsed 0m 26s (remain 15m 21s) Loss: 0.0026(0.0027) Grad: 14783.0518  LR: 0.000009  \n","Epoch: [4][200/3575] Elapsed 0m 52s (remain 14m 38s) Loss: 0.0053(0.0031) Grad: 25258.2266  LR: 0.000009  \n","Epoch: [4][300/3575] Elapsed 1m 14s (remain 13m 35s) Loss: 0.0002(0.0031) Grad: 6477.4224  LR: 0.000009  \n","Epoch: [4][400/3575] Elapsed 1m 37s (remain 12m 52s) Loss: 0.0002(0.0033) Grad: 663.7826  LR: 0.000008  \n","Epoch: [4][500/3575] Elapsed 1m 59s (remain 12m 14s) Loss: 0.0000(0.0036) Grad: 129.7861  LR: 0.000008  \n","Epoch: [4][600/3575] Elapsed 2m 22s (remain 11m 44s) Loss: 0.0000(0.0036) Grad: 43.6841  LR: 0.000008  \n","Epoch: [4][700/3575] Elapsed 2m 44s (remain 11m 15s) Loss: 0.0001(0.0034) Grad: 868.4937  LR: 0.000008  \n","Epoch: [4][800/3575] Elapsed 3m 7s (remain 10m 48s) Loss: 0.0001(0.0034) Grad: 233.1901  LR: 0.000008  \n","Epoch: [4][900/3575] Elapsed 3m 29s (remain 10m 21s) Loss: 0.0001(0.0034) Grad: 8079.1548  LR: 0.000008  \n","Epoch: [4][1000/3575] Elapsed 3m 51s (remain 9m 56s) Loss: 0.0000(0.0034) Grad: 17.9607  LR: 0.000008  \n","Epoch: [4][1100/3575] Elapsed 4m 13s (remain 9m 30s) Loss: 0.0032(0.0034) Grad: 10554.9541  LR: 0.000008  \n","Epoch: [4][1200/3575] Elapsed 4m 36s (remain 9m 6s) Loss: 0.0171(0.0035) Grad: 12883.5967  LR: 0.000007  \n","Epoch: [4][1300/3575] Elapsed 4m 58s (remain 8m 42s) Loss: 0.0004(0.0036) Grad: 2254.8623  LR: 0.000007  \n","Epoch: [4][1400/3575] Elapsed 5m 21s (remain 8m 18s) Loss: 0.0020(0.0036) Grad: 7181.7495  LR: 0.000007  \n","Epoch: [4][1500/3575] Elapsed 5m 43s (remain 7m 54s) Loss: 0.0137(0.0036) Grad: 20394.1152  LR: 0.000007  \n","Epoch: [4][1600/3575] Elapsed 6m 5s (remain 7m 30s) Loss: 0.0000(0.0037) Grad: 21.6458  LR: 0.000007  \n","Epoch: [4][1700/3575] Elapsed 6m 28s (remain 7m 7s) Loss: 0.0000(0.0037) Grad: 88.2173  LR: 0.000007  \n","Epoch: [4][1800/3575] Elapsed 6m 51s (remain 6m 45s) Loss: 0.0032(0.0037) Grad: 5180.5708  LR: 0.000007  \n","Epoch: [4][1900/3575] Elapsed 7m 14s (remain 6m 22s) Loss: 0.0000(0.0036) Grad: 79.5006  LR: 0.000007  \n","Epoch: [4][2000/3575] Elapsed 7m 36s (remain 5m 59s) Loss: 0.0025(0.0036) Grad: 16823.3652  LR: 0.000006  \n","Epoch: [4][2100/3575] Elapsed 7m 58s (remain 5m 36s) Loss: 0.0064(0.0037) Grad: 14599.7002  LR: 0.000006  \n","Epoch: [4][2200/3575] Elapsed 8m 20s (remain 5m 12s) Loss: 0.0000(0.0037) Grad: 218.4692  LR: 0.000006  \n","Epoch: [4][2300/3575] Elapsed 8m 42s (remain 4m 49s) Loss: 0.0000(0.0037) Grad: 36.1991  LR: 0.000006  \n","Epoch: [4][2400/3575] Elapsed 9m 5s (remain 4m 26s) Loss: 0.0019(0.0037) Grad: 8556.5996  LR: 0.000006  \n","Epoch: [4][2500/3575] Elapsed 9m 27s (remain 4m 3s) Loss: 0.0033(0.0037) Grad: 16756.2871  LR: 0.000006  \n","Epoch: [4][2600/3575] Elapsed 9m 50s (remain 3m 41s) Loss: 0.0063(0.0037) Grad: 29880.0801  LR: 0.000006  \n","Epoch: [4][2700/3575] Elapsed 10m 12s (remain 3m 18s) Loss: 0.0022(0.0037) Grad: 59678.5273  LR: 0.000006  \n","Epoch: [4][2800/3575] Elapsed 10m 35s (remain 2m 55s) Loss: 0.0000(0.0037) Grad: 31.2002  LR: 0.000005  \n","Epoch: [4][2900/3575] Elapsed 10m 57s (remain 2m 32s) Loss: 0.0000(0.0037) Grad: 69.1193  LR: 0.000005  \n","Epoch: [4][3000/3575] Elapsed 11m 20s (remain 2m 10s) Loss: 0.0127(0.0037) Grad: 26091.4102  LR: 0.000005  \n","Epoch: [4][3100/3575] Elapsed 11m 43s (remain 1m 47s) Loss: 0.0076(0.0036) Grad: 41875.2305  LR: 0.000005  \n","Epoch: [4][3200/3575] Elapsed 12m 5s (remain 1m 24s) Loss: 0.0000(0.0036) Grad: 64.3645  LR: 0.000005  \n","Epoch: [4][3300/3575] Elapsed 12m 28s (remain 1m 2s) Loss: 0.0130(0.0036) Grad: 25394.8086  LR: 0.000005  \n","Epoch: [4][3400/3575] Elapsed 12m 51s (remain 0m 39s) Loss: 0.0042(0.0037) Grad: 5977.8398  LR: 0.000005  \n","Epoch: [4][3500/3575] Elapsed 13m 13s (remain 0m 16s) Loss: 0.0036(0.0037) Grad: 16545.2969  LR: 0.000005  \n","Epoch: [4][3574/3575] Elapsed 13m 30s (remain 0m 0s) Loss: 0.0001(0.0036) Grad: 320.4998  LR: 0.000004  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 41s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 7s) Loss: 0.0000(0.0061) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 54s) Loss: 0.0001(0.0075) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 42s) Loss: 0.0001(0.0112) \n","EVAL: [400/1192] Elapsed 0m 46s (remain 1m 30s) Loss: 0.0293(0.0115) \n","EVAL: [500/1192] Elapsed 0m 57s (remain 1m 19s) Loss: 0.0300(0.0110) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 7s) Loss: 0.1394(0.0114) \n","EVAL: [700/1192] Elapsed 1m 20s (remain 0m 56s) Loss: 0.0213(0.0124) \n","EVAL: [800/1192] Elapsed 1m 31s (remain 0m 44s) Loss: 0.0141(0.0119) \n","EVAL: [900/1192] Elapsed 1m 43s (remain 0m 33s) Loss: 0.0020(0.0118) \n","EVAL: [1000/1192] Elapsed 1m 54s (remain 0m 21s) Loss: 0.0000(0.0114) \n","EVAL: [1100/1192] Elapsed 2m 5s (remain 0m 10s) Loss: 0.0059(0.0108) \n","EVAL: [1191/1192] Elapsed 2m 16s (remain 0m 0s) Loss: 0.0079(0.0101) \n","Epoch 4 - avg_train_loss: 0.0036  avg_val_loss: 0.0101  time: 953s\n","Epoch 4 - Score: 0.8744\n","Epoch 4 - Save Best Score: 0.8744 Model\n","Epoch: [5][0/3575] Elapsed 0m 0s (remain 35m 35s) Loss: 0.0064(0.0064) Grad: 6674.3491  LR: 0.000004  \n","Epoch: [5][100/3575] Elapsed 0m 27s (remain 15m 58s) Loss: 0.0098(0.0037) Grad: 82506.1172  LR: 0.000004  \n","Epoch: [5][200/3575] Elapsed 0m 52s (remain 14m 39s) Loss: 0.0000(0.0031) Grad: 111.7700  LR: 0.000004  \n","Epoch: [5][300/3575] Elapsed 1m 15s (remain 13m 39s) Loss: 0.0015(0.0031) Grad: 14014.9883  LR: 0.000004  \n","Epoch: [5][400/3575] Elapsed 1m 38s (remain 12m 56s) Loss: 0.0000(0.0029) Grad: 48.2238  LR: 0.000004  \n","Epoch: [5][500/3575] Elapsed 2m 1s (remain 12m 22s) Loss: 0.0026(0.0028) Grad: 17636.0156  LR: 0.000004  \n","Epoch: [5][600/3575] Elapsed 2m 23s (remain 11m 51s) Loss: 0.0008(0.0028) Grad: 16974.2832  LR: 0.000004  \n","Epoch: [5][700/3575] Elapsed 2m 46s (remain 11m 22s) Loss: 0.0003(0.0028) Grad: 1584.3062  LR: 0.000004  \n","Epoch: [5][800/3575] Elapsed 3m 9s (remain 10m 54s) Loss: 0.0000(0.0029) Grad: 36.5884  LR: 0.000003  \n","Epoch: [5][900/3575] Elapsed 3m 31s (remain 10m 27s) Loss: 0.0044(0.0029) Grad: 6479.8022  LR: 0.000003  \n","Epoch: [5][1000/3575] Elapsed 3m 53s (remain 10m 1s) Loss: 0.0033(0.0030) Grad: 8776.1787  LR: 0.000003  \n","Epoch: [5][1100/3575] Elapsed 4m 16s (remain 9m 35s) Loss: 0.0000(0.0031) Grad: 30.9914  LR: 0.000003  \n","Epoch: [5][1200/3575] Elapsed 4m 38s (remain 9m 11s) Loss: 0.0038(0.0030) Grad: 5652.3193  LR: 0.000003  \n","Epoch: [5][1300/3575] Elapsed 5m 1s (remain 8m 47s) Loss: 0.0067(0.0030) Grad: 25330.8125  LR: 0.000003  \n","Epoch: [5][1400/3575] Elapsed 5m 24s (remain 8m 23s) Loss: 0.0021(0.0029) Grad: 30311.6895  LR: 0.000003  \n","Epoch: [5][1500/3575] Elapsed 5m 47s (remain 7m 59s) Loss: 0.0003(0.0029) Grad: 4238.6650  LR: 0.000003  \n","Epoch: [5][1600/3575] Elapsed 6m 9s (remain 7m 35s) Loss: 0.0000(0.0029) Grad: 13.8521  LR: 0.000002  \n","Epoch: [5][1700/3575] Elapsed 6m 32s (remain 7m 12s) Loss: 0.0044(0.0029) Grad: 4408.9326  LR: 0.000002  \n","Epoch: [5][1800/3575] Elapsed 6m 54s (remain 6m 48s) Loss: 0.0000(0.0029) Grad: 9.4243  LR: 0.000002  \n","Epoch: [5][1900/3575] Elapsed 7m 17s (remain 6m 25s) Loss: 0.0000(0.0029) Grad: 83.4705  LR: 0.000002  \n","Epoch: [5][2000/3575] Elapsed 7m 40s (remain 6m 2s) Loss: 0.0000(0.0029) Grad: 172.1939  LR: 0.000002  \n","Epoch: [5][2100/3575] Elapsed 8m 3s (remain 5m 39s) Loss: 0.0011(0.0029) Grad: 9571.4375  LR: 0.000002  \n","Epoch: [5][2200/3575] Elapsed 8m 26s (remain 5m 16s) Loss: 0.0000(0.0030) Grad: 101.9540  LR: 0.000002  \n","Epoch: [5][2300/3575] Elapsed 8m 50s (remain 4m 53s) Loss: 0.0019(0.0030) Grad: 20403.0293  LR: 0.000002  \n","Epoch: [5][2400/3575] Elapsed 9m 13s (remain 4m 30s) Loss: 0.0046(0.0030) Grad: 61134.8984  LR: 0.000001  \n","Epoch: [5][2500/3575] Elapsed 9m 37s (remain 4m 7s) Loss: 0.0047(0.0030) Grad: 5603.8101  LR: 0.000001  \n","Epoch: [5][2600/3575] Elapsed 10m 0s (remain 3m 44s) Loss: 0.0089(0.0030) Grad: 8330.3838  LR: 0.000001  \n","Epoch: [5][2700/3575] Elapsed 10m 23s (remain 3m 21s) Loss: 0.0001(0.0030) Grad: 704.2706  LR: 0.000001  \n","Epoch: [5][2800/3575] Elapsed 10m 47s (remain 2m 58s) Loss: 0.0000(0.0030) Grad: 14.8321  LR: 0.000001  \n","Epoch: [5][2900/3575] Elapsed 11m 10s (remain 2m 35s) Loss: 0.0000(0.0030) Grad: 84.6427  LR: 0.000001  \n","Epoch: [5][3000/3575] Elapsed 11m 33s (remain 2m 12s) Loss: 0.0037(0.0030) Grad: 4193.1313  LR: 0.000001  \n","Epoch: [5][3100/3575] Elapsed 11m 56s (remain 1m 49s) Loss: 0.0001(0.0030) Grad: 1933.3600  LR: 0.000001  \n","Epoch: [5][3200/3575] Elapsed 12m 19s (remain 1m 26s) Loss: 0.0000(0.0030) Grad: 14.0644  LR: 0.000000  \n","Epoch: [5][3300/3575] Elapsed 12m 42s (remain 1m 3s) Loss: 0.0001(0.0030) Grad: 321.3722  LR: 0.000000  \n","Epoch: [5][3400/3575] Elapsed 13m 5s (remain 0m 40s) Loss: 0.0001(0.0030) Grad: 559.2563  LR: 0.000000  \n","Epoch: [5][3500/3575] Elapsed 13m 28s (remain 0m 17s) Loss: 0.0036(0.0030) Grad: 38349.8516  LR: 0.000000  \n","Epoch: [5][3574/3575] Elapsed 13m 45s (remain 0m 0s) Loss: 0.0000(0.0030) Grad: 12.8454  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 34s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 8s) Loss: 0.0000(0.0062) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 55s) Loss: 0.0000(0.0077) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 43s) Loss: 0.0000(0.0114) \n","EVAL: [400/1192] Elapsed 0m 46s (remain 1m 31s) Loss: 0.0336(0.0118) \n","EVAL: [500/1192] Elapsed 0m 57s (remain 1m 19s) Loss: 0.0309(0.0113) \n","EVAL: [600/1192] Elapsed 1m 9s (remain 1m 7s) Loss: 0.1413(0.0114) \n","EVAL: [700/1192] Elapsed 1m 20s (remain 0m 56s) Loss: 0.0205(0.0126) \n","EVAL: [800/1192] Elapsed 1m 31s (remain 0m 44s) Loss: 0.0207(0.0122) \n","EVAL: [900/1192] Elapsed 1m 42s (remain 0m 33s) Loss: 0.0002(0.0120) \n","EVAL: [1000/1192] Elapsed 1m 54s (remain 0m 21s) Loss: 0.0000(0.0117) \n","EVAL: [1100/1192] Elapsed 2m 5s (remain 0m 10s) Loss: 0.0068(0.0110) \n","EVAL: [1191/1192] Elapsed 2m 16s (remain 0m 0s) Loss: 0.0086(0.0104) \n","Epoch 5 - avg_train_loss: 0.0030  avg_val_loss: 0.0104  time: 968s\n","Epoch 5 - Score: 0.8744\n","Epoch 5 - Save Best Score: 0.8744 Model\n","========== fold: 2 training ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n","Epoch: [1][0/3575] Elapsed 0m 0s (remain 50m 19s) Loss: 0.2946(0.2946) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/3575] Elapsed 0m 29s (remain 16m 41s) Loss: 0.1703(0.2640) Grad: 32729.6426  LR: 0.000001  \n","Epoch: [1][200/3575] Elapsed 0m 51s (remain 14m 32s) Loss: 0.0685(0.1781) Grad: 2921.8396  LR: 0.000002  \n","Epoch: [1][300/3575] Elapsed 1m 14s (remain 13m 33s) Loss: 0.0179(0.1318) Grad: 2129.9045  LR: 0.000003  \n","Epoch: [1][400/3575] Elapsed 1m 37s (remain 12m 49s) Loss: 0.0115(0.1053) Grad: 8021.5010  LR: 0.000004  \n","Epoch: [1][500/3575] Elapsed 2m 0s (remain 12m 16s) Loss: 0.0024(0.0885) Grad: 1390.8582  LR: 0.000006  \n","Epoch: [1][600/3575] Elapsed 2m 22s (remain 11m 46s) Loss: 0.0032(0.0765) Grad: 1328.7902  LR: 0.000007  \n","Epoch: [1][700/3575] Elapsed 2m 45s (remain 11m 18s) Loss: 0.0011(0.0677) Grad: 322.2690  LR: 0.000008  \n","Epoch: [1][800/3575] Elapsed 3m 8s (remain 10m 52s) Loss: 0.1329(0.0609) Grad: 23596.9551  LR: 0.000009  \n","Epoch: [1][900/3575] Elapsed 3m 31s (remain 10m 26s) Loss: 0.0007(0.0555) Grad: 615.9798  LR: 0.000010  \n","Epoch: [1][1000/3575] Elapsed 3m 53s (remain 10m 0s) Loss: 0.0433(0.0515) Grad: 39480.6562  LR: 0.000011  \n","Epoch: [1][1100/3575] Elapsed 4m 16s (remain 9m 35s) Loss: 0.0119(0.0478) Grad: 5383.8921  LR: 0.000012  \n","Epoch: [1][1200/3575] Elapsed 4m 38s (remain 9m 10s) Loss: 0.0018(0.0449) Grad: 1681.7446  LR: 0.000013  \n","Epoch: [1][1300/3575] Elapsed 5m 2s (remain 8m 48s) Loss: 0.0028(0.0423) Grad: 2430.1472  LR: 0.000015  \n","Epoch: [1][1400/3575] Elapsed 5m 25s (remain 8m 25s) Loss: 0.1104(0.0400) Grad: 13735.4395  LR: 0.000016  \n","Epoch: [1][1500/3575] Elapsed 5m 48s (remain 8m 1s) Loss: 0.0014(0.0382) Grad: 916.4040  LR: 0.000017  \n","Epoch: [1][1600/3575] Elapsed 6m 11s (remain 7m 37s) Loss: 0.0042(0.0365) Grad: 1845.0891  LR: 0.000018  \n","Epoch: [1][1700/3575] Elapsed 6m 33s (remain 7m 13s) Loss: 0.0010(0.0351) Grad: 332.3835  LR: 0.000019  \n","Epoch: [1][1800/3575] Elapsed 6m 56s (remain 6m 50s) Loss: 0.0094(0.0337) Grad: 7779.0225  LR: 0.000020  \n","Epoch: [1][1900/3575] Elapsed 7m 19s (remain 6m 26s) Loss: 0.0138(0.0326) Grad: 12447.0342  LR: 0.000020  \n","Epoch: [1][2000/3575] Elapsed 7m 41s (remain 6m 3s) Loss: 0.0046(0.0314) Grad: 1615.0806  LR: 0.000020  \n","Epoch: [1][2100/3575] Elapsed 8m 4s (remain 5m 39s) Loss: 0.0111(0.0304) Grad: 4082.5923  LR: 0.000020  \n","Epoch: [1][2200/3575] Elapsed 8m 26s (remain 5m 16s) Loss: 0.0481(0.0295) Grad: 9843.4189  LR: 0.000019  \n","Epoch: [1][2300/3575] Elapsed 8m 48s (remain 4m 52s) Loss: 0.0283(0.0287) Grad: 8234.6787  LR: 0.000019  \n","Epoch: [1][2400/3575] Elapsed 9m 10s (remain 4m 29s) Loss: 0.0487(0.0279) Grad: 2788.1401  LR: 0.000019  \n","Epoch: [1][2500/3575] Elapsed 9m 32s (remain 4m 5s) Loss: 0.0000(0.0271) Grad: 21.1280  LR: 0.000019  \n","Epoch: [1][2600/3575] Elapsed 9m 55s (remain 3m 42s) Loss: 0.0059(0.0265) Grad: 11148.1709  LR: 0.000019  \n","Epoch: [1][2700/3575] Elapsed 10m 17s (remain 3m 19s) Loss: 0.0027(0.0258) Grad: 1106.7291  LR: 0.000019  \n","Epoch: [1][2800/3575] Elapsed 10m 40s (remain 2m 57s) Loss: 0.0007(0.0251) Grad: 243.6918  LR: 0.000019  \n","Epoch: [1][2900/3575] Elapsed 11m 3s (remain 2m 34s) Loss: 0.0049(0.0245) Grad: 2273.0845  LR: 0.000019  \n","Epoch: [1][3000/3575] Elapsed 11m 26s (remain 2m 11s) Loss: 0.0169(0.0240) Grad: 2871.3311  LR: 0.000018  \n","Epoch: [1][3100/3575] Elapsed 11m 48s (remain 1m 48s) Loss: 0.0001(0.0236) Grad: 33.3521  LR: 0.000018  \n","Epoch: [1][3200/3575] Elapsed 12m 10s (remain 1m 25s) Loss: 0.0002(0.0232) Grad: 65.2799  LR: 0.000018  \n","Epoch: [1][3300/3575] Elapsed 12m 33s (remain 1m 2s) Loss: 0.0073(0.0227) Grad: 2614.6719  LR: 0.000018  \n","Epoch: [1][3400/3575] Elapsed 12m 55s (remain 0m 39s) Loss: 0.0006(0.0223) Grad: 1547.2415  LR: 0.000018  \n","Epoch: [1][3500/3575] Elapsed 13m 17s (remain 0m 16s) Loss: 0.0005(0.0218) Grad: 176.1426  LR: 0.000018  \n","Epoch: [1][3574/3575] Elapsed 13m 33s (remain 0m 0s) Loss: 0.0086(0.0215) Grad: 3926.1287  LR: 0.000018  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 9m 3s) Loss: 0.0001(0.0001) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 7s) Loss: 0.0108(0.0066) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 53s) Loss: 0.0407(0.0077) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 41s) Loss: 0.0192(0.0074) \n","EVAL: [400/1192] Elapsed 0m 45s (remain 1m 30s) Loss: 0.0013(0.0079) \n","EVAL: [500/1192] Elapsed 0m 56s (remain 1m 18s) Loss: 0.0004(0.0077) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 7s) Loss: 0.0032(0.0086) \n","EVAL: [700/1192] Elapsed 1m 19s (remain 0m 55s) Loss: 0.0080(0.0099) \n","EVAL: [800/1192] Elapsed 1m 30s (remain 0m 44s) Loss: 0.0012(0.0099) \n","EVAL: [900/1192] Elapsed 1m 42s (remain 0m 32s) Loss: 0.0020(0.0099) \n","EVAL: [1000/1192] Elapsed 1m 53s (remain 0m 21s) Loss: 0.0009(0.0094) \n","EVAL: [1100/1192] Elapsed 2m 4s (remain 0m 10s) Loss: 0.0504(0.0089) \n","EVAL: [1191/1192] Elapsed 2m 14s (remain 0m 0s) Loss: 0.0006(0.0085) \n","Epoch 1 - avg_train_loss: 0.0215  avg_val_loss: 0.0085  time: 954s\n","Epoch 1 - Score: 0.8338\n","Epoch 1 - Save Best Score: 0.8338 Model\n","Epoch: [2][0/3575] Elapsed 0m 0s (remain 39m 59s) Loss: 0.0068(0.0068) Grad: 22729.0996  LR: 0.000018  \n","Epoch: [2][100/3575] Elapsed 0m 27s (remain 15m 58s) Loss: 0.0001(0.0092) Grad: 297.2755  LR: 0.000018  \n","Epoch: [2][200/3575] Elapsed 0m 51s (remain 14m 31s) Loss: 0.0000(0.0077) Grad: 73.6911  LR: 0.000018  \n","Epoch: [2][300/3575] Elapsed 1m 14s (remain 13m 29s) Loss: 0.0103(0.0078) Grad: 110461.7656  LR: 0.000017  \n","Epoch: [2][400/3575] Elapsed 1m 36s (remain 12m 46s) Loss: 0.0065(0.0077) Grad: 16032.8926  LR: 0.000017  \n","Epoch: [2][500/3575] Elapsed 1m 59s (remain 12m 12s) Loss: 0.0114(0.0078) Grad: 43328.3438  LR: 0.000017  \n","Epoch: [2][600/3575] Elapsed 2m 21s (remain 11m 41s) Loss: 0.0026(0.0078) Grad: 4537.9990  LR: 0.000017  \n","Epoch: [2][700/3575] Elapsed 2m 44s (remain 11m 12s) Loss: 0.0001(0.0074) Grad: 582.3989  LR: 0.000017  \n","Epoch: [2][800/3575] Elapsed 3m 6s (remain 10m 46s) Loss: 0.0002(0.0073) Grad: 1067.6523  LR: 0.000017  \n","Epoch: [2][900/3575] Elapsed 3m 29s (remain 10m 20s) Loss: 0.0268(0.0074) Grad: 23851.7363  LR: 0.000017  \n","Epoch: [2][1000/3575] Elapsed 3m 51s (remain 9m 54s) Loss: 0.0000(0.0073) Grad: 301.6668  LR: 0.000017  \n","Epoch: [2][1100/3575] Elapsed 4m 13s (remain 9m 30s) Loss: 0.0381(0.0072) Grad: 202939.9375  LR: 0.000016  \n","Epoch: [2][1200/3575] Elapsed 4m 36s (remain 9m 6s) Loss: 0.0016(0.0072) Grad: 5508.1055  LR: 0.000016  \n","Epoch: [2][1300/3575] Elapsed 4m 58s (remain 8m 42s) Loss: 0.0037(0.0072) Grad: 5562.4326  LR: 0.000016  \n","Epoch: [2][1400/3575] Elapsed 5m 20s (remain 8m 18s) Loss: 0.0040(0.0071) Grad: 16039.5293  LR: 0.000016  \n","Epoch: [2][1500/3575] Elapsed 5m 43s (remain 7m 54s) Loss: 0.0051(0.0070) Grad: 13884.0391  LR: 0.000016  \n","Epoch: [2][1600/3575] Elapsed 6m 6s (remain 7m 31s) Loss: 0.0057(0.0070) Grad: 40703.7891  LR: 0.000016  \n","Epoch: [2][1700/3575] Elapsed 6m 28s (remain 7m 7s) Loss: 0.0128(0.0071) Grad: 17616.1582  LR: 0.000016  \n","Epoch: [2][1800/3575] Elapsed 6m 50s (remain 6m 44s) Loss: 0.0039(0.0071) Grad: 9495.3066  LR: 0.000016  \n","Epoch: [2][1900/3575] Elapsed 7m 13s (remain 6m 21s) Loss: 0.0034(0.0071) Grad: 27166.4141  LR: 0.000015  \n","Epoch: [2][2000/3575] Elapsed 7m 35s (remain 5m 58s) Loss: 0.0001(0.0071) Grad: 238.7917  LR: 0.000015  \n","Epoch: [2][2100/3575] Elapsed 7m 58s (remain 5m 35s) Loss: 0.0044(0.0071) Grad: 17633.0430  LR: 0.000015  \n","Epoch: [2][2200/3575] Elapsed 8m 20s (remain 5m 12s) Loss: 0.0061(0.0070) Grad: 10626.4971  LR: 0.000015  \n","Epoch: [2][2300/3575] Elapsed 8m 43s (remain 4m 49s) Loss: 0.0001(0.0070) Grad: 889.5660  LR: 0.000015  \n","Epoch: [2][2400/3575] Elapsed 9m 5s (remain 4m 26s) Loss: 0.0013(0.0070) Grad: 3473.7932  LR: 0.000015  \n","Epoch: [2][2500/3575] Elapsed 9m 27s (remain 4m 3s) Loss: 0.0195(0.0070) Grad: 73946.9453  LR: 0.000015  \n","Epoch: [2][2600/3575] Elapsed 9m 50s (remain 3m 41s) Loss: 0.0629(0.0069) Grad: 29303.7559  LR: 0.000015  \n","Epoch: [2][2700/3575] Elapsed 10m 12s (remain 3m 18s) Loss: 0.0004(0.0069) Grad: 6807.6323  LR: 0.000014  \n","Epoch: [2][2800/3575] Elapsed 10m 34s (remain 2m 55s) Loss: 0.0001(0.0069) Grad: 525.6901  LR: 0.000014  \n","Epoch: [2][2900/3575] Elapsed 10m 56s (remain 2m 32s) Loss: 0.0037(0.0069) Grad: 6128.8076  LR: 0.000014  \n","Epoch: [2][3000/3575] Elapsed 11m 18s (remain 2m 9s) Loss: 0.0060(0.0069) Grad: 5970.7280  LR: 0.000014  \n","Epoch: [2][3100/3575] Elapsed 11m 41s (remain 1m 47s) Loss: 0.0002(0.0068) Grad: 926.3712  LR: 0.000014  \n","Epoch: [2][3200/3575] Elapsed 12m 4s (remain 1m 24s) Loss: 0.0000(0.0068) Grad: 94.8295  LR: 0.000014  \n","Epoch: [2][3300/3575] Elapsed 12m 26s (remain 1m 1s) Loss: 0.0639(0.0067) Grad: 276942.5625  LR: 0.000014  \n","Epoch: [2][3400/3575] Elapsed 12m 48s (remain 0m 39s) Loss: 0.0007(0.0067) Grad: 4165.8730  LR: 0.000014  \n","Epoch: [2][3500/3575] Elapsed 13m 10s (remain 0m 16s) Loss: 0.0136(0.0067) Grad: 23792.1582  LR: 0.000013  \n","Epoch: [2][3574/3575] Elapsed 13m 27s (remain 0m 0s) Loss: 0.0005(0.0067) Grad: 2275.9473  LR: 0.000013  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 9m 17s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 7s) Loss: 0.0158(0.0072) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 53s) Loss: 0.0033(0.0063) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 42s) Loss: 0.0078(0.0060) \n","EVAL: [400/1192] Elapsed 0m 45s (remain 1m 30s) Loss: 0.0013(0.0070) \n","EVAL: [500/1192] Elapsed 0m 57s (remain 1m 18s) Loss: 0.0000(0.0069) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 7s) Loss: 0.0047(0.0070) \n","EVAL: [700/1192] Elapsed 1m 19s (remain 0m 55s) Loss: 0.0088(0.0076) \n","EVAL: [800/1192] Elapsed 1m 31s (remain 0m 44s) Loss: 0.0000(0.0076) \n","EVAL: [900/1192] Elapsed 1m 42s (remain 0m 33s) Loss: 0.0104(0.0079) \n","EVAL: [1000/1192] Elapsed 1m 53s (remain 0m 21s) Loss: 0.0002(0.0076) \n","EVAL: [1100/1192] Elapsed 2m 5s (remain 0m 10s) Loss: 0.0050(0.0072) \n","EVAL: [1191/1192] Elapsed 2m 15s (remain 0m 0s) Loss: 0.0001(0.0068) \n","Epoch 2 - avg_train_loss: 0.0067  avg_val_loss: 0.0068  time: 948s\n","Epoch 2 - Score: 0.8701\n","Epoch 2 - Save Best Score: 0.8701 Model\n","Epoch: [3][0/3575] Elapsed 0m 0s (remain 34m 38s) Loss: 0.0000(0.0000) Grad: 1029.4012  LR: 0.000013  \n","Epoch: [3][100/3575] Elapsed 0m 27s (remain 15m 33s) Loss: 0.0033(0.0042) Grad: 12252.2217  LR: 0.000013  \n","Epoch: [3][200/3575] Elapsed 0m 51s (remain 14m 26s) Loss: 0.0001(0.0049) Grad: 777.1572  LR: 0.000013  \n","Epoch: [3][300/3575] Elapsed 1m 13s (remain 13m 22s) Loss: 0.0020(0.0053) Grad: 12293.5537  LR: 0.000013  \n","Epoch: [3][400/3575] Elapsed 1m 36s (remain 12m 42s) Loss: 0.0179(0.0053) Grad: 44027.8711  LR: 0.000013  \n","Epoch: [3][500/3575] Elapsed 1m 58s (remain 12m 7s) Loss: 0.0277(0.0053) Grad: 148247.7031  LR: 0.000013  \n","Epoch: [3][600/3575] Elapsed 2m 20s (remain 11m 34s) Loss: 0.0000(0.0051) Grad: 241.6871  LR: 0.000013  \n","Epoch: [3][700/3575] Elapsed 2m 42s (remain 11m 6s) Loss: 0.0003(0.0050) Grad: 1026.3273  LR: 0.000012  \n","Epoch: [3][800/3575] Elapsed 3m 4s (remain 10m 39s) Loss: 0.0001(0.0050) Grad: 2712.1472  LR: 0.000012  \n","Epoch: [3][900/3575] Elapsed 3m 27s (remain 10m 14s) Loss: 0.0027(0.0049) Grad: 18651.0840  LR: 0.000012  \n","Epoch: [3][1000/3575] Elapsed 3m 49s (remain 9m 50s) Loss: 0.0000(0.0048) Grad: 26.7902  LR: 0.000012  \n","Epoch: [3][1100/3575] Elapsed 4m 11s (remain 9m 25s) Loss: 0.0213(0.0049) Grad: 102681.6641  LR: 0.000012  \n","Epoch: [3][1200/3575] Elapsed 4m 34s (remain 9m 1s) Loss: 0.0157(0.0049) Grad: 20210.2617  LR: 0.000012  \n","Epoch: [3][1300/3575] Elapsed 4m 56s (remain 8m 37s) Loss: 0.0000(0.0049) Grad: 27.2719  LR: 0.000012  \n","Epoch: [3][1400/3575] Elapsed 5m 18s (remain 8m 14s) Loss: 0.0001(0.0050) Grad: 461.1476  LR: 0.000012  \n","Epoch: [3][1500/3575] Elapsed 5m 40s (remain 7m 51s) Loss: 0.0040(0.0050) Grad: 41187.2734  LR: 0.000011  \n","Epoch: [3][1600/3575] Elapsed 6m 3s (remain 7m 27s) Loss: 0.0016(0.0050) Grad: 10385.8984  LR: 0.000011  \n","Epoch: [3][1700/3575] Elapsed 6m 25s (remain 7m 4s) Loss: 0.0001(0.0050) Grad: 685.4561  LR: 0.000011  \n","Epoch: [3][1800/3575] Elapsed 6m 47s (remain 6m 41s) Loss: 0.0122(0.0049) Grad: 152786.7188  LR: 0.000011  \n","Epoch: [3][1900/3575] Elapsed 7m 10s (remain 6m 18s) Loss: 0.0027(0.0049) Grad: 12360.4375  LR: 0.000011  \n","Epoch: [3][2000/3575] Elapsed 7m 32s (remain 5m 55s) Loss: 0.0000(0.0050) Grad: 85.6958  LR: 0.000011  \n","Epoch: [3][2100/3575] Elapsed 7m 54s (remain 5m 33s) Loss: 0.0229(0.0050) Grad: 22574.6074  LR: 0.000011  \n","Epoch: [3][2200/3575] Elapsed 8m 17s (remain 5m 10s) Loss: 0.0034(0.0050) Grad: 13493.7168  LR: 0.000011  \n","Epoch: [3][2300/3575] Elapsed 8m 39s (remain 4m 47s) Loss: 0.0060(0.0050) Grad: 11356.2588  LR: 0.000010  \n","Epoch: [3][2400/3575] Elapsed 9m 1s (remain 4m 24s) Loss: 0.0113(0.0050) Grad: 190791.9375  LR: 0.000010  \n","Epoch: [3][2500/3575] Elapsed 9m 24s (remain 4m 2s) Loss: 0.0001(0.0050) Grad: 487.6885  LR: 0.000010  \n","Epoch: [3][2600/3575] Elapsed 9m 46s (remain 3m 39s) Loss: 0.0194(0.0051) Grad: 14826.4326  LR: 0.000010  \n","Epoch: [3][2700/3575] Elapsed 10m 8s (remain 3m 16s) Loss: 0.0000(0.0051) Grad: 32.8667  LR: 0.000010  \n","Epoch: [3][2800/3575] Elapsed 10m 30s (remain 2m 54s) Loss: 0.0007(0.0051) Grad: 7085.2793  LR: 0.000010  \n","Epoch: [3][2900/3575] Elapsed 10m 53s (remain 2m 31s) Loss: 0.0085(0.0051) Grad: 10928.2061  LR: 0.000010  \n","Epoch: [3][3000/3575] Elapsed 11m 15s (remain 2m 9s) Loss: 0.0053(0.0051) Grad: 48926.7969  LR: 0.000010  \n","Epoch: [3][3100/3575] Elapsed 11m 37s (remain 1m 46s) Loss: 0.0000(0.0050) Grad: 38.4939  LR: 0.000009  \n","Epoch: [3][3200/3575] Elapsed 11m 59s (remain 1m 24s) Loss: 0.0039(0.0051) Grad: 6893.8633  LR: 0.000009  \n","Epoch: [3][3300/3575] Elapsed 12m 22s (remain 1m 1s) Loss: 0.0000(0.0051) Grad: 53.2231  LR: 0.000009  \n","Epoch: [3][3400/3575] Elapsed 12m 45s (remain 0m 39s) Loss: 0.0024(0.0051) Grad: 5804.2627  LR: 0.000009  \n","Epoch: [3][3500/3575] Elapsed 13m 8s (remain 0m 16s) Loss: 0.0002(0.0051) Grad: 1731.7460  LR: 0.000009  \n","Epoch: [3][3574/3575] Elapsed 13m 24s (remain 0m 0s) Loss: 0.0001(0.0051) Grad: 967.5663  LR: 0.000009  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 44s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 7s) Loss: 0.0307(0.0080) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 53s) Loss: 0.0041(0.0073) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 42s) Loss: 0.0087(0.0068) \n","EVAL: [400/1192] Elapsed 0m 45s (remain 1m 30s) Loss: 0.0003(0.0071) \n","EVAL: [500/1192] Elapsed 0m 57s (remain 1m 18s) Loss: 0.0000(0.0068) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 7s) Loss: 0.0057(0.0069) \n","EVAL: [700/1192] Elapsed 1m 19s (remain 0m 55s) Loss: 0.0094(0.0076) \n","EVAL: [800/1192] Elapsed 1m 31s (remain 0m 44s) Loss: 0.0000(0.0075) \n","EVAL: [900/1192] Elapsed 1m 42s (remain 0m 33s) Loss: 0.0038(0.0076) \n","EVAL: [1000/1192] Elapsed 1m 53s (remain 0m 21s) Loss: 0.0001(0.0075) \n","EVAL: [1100/1192] Elapsed 2m 4s (remain 0m 10s) Loss: 0.0090(0.0072) \n","EVAL: [1191/1192] Elapsed 2m 15s (remain 0m 0s) Loss: 0.0001(0.0068) \n","Epoch 3 - avg_train_loss: 0.0051  avg_val_loss: 0.0068  time: 945s\n","Epoch 3 - Score: 0.8806\n","Epoch 3 - Save Best Score: 0.8806 Model\n","Epoch: [4][0/3575] Elapsed 0m 0s (remain 34m 9s) Loss: 0.0002(0.0002) Grad: 1445.3881  LR: 0.000009  \n","Epoch: [4][100/3575] Elapsed 0m 26s (remain 15m 21s) Loss: 0.0000(0.0029) Grad: 107.7795  LR: 0.000009  \n","Epoch: [4][200/3575] Elapsed 0m 51s (remain 14m 18s) Loss: 0.0010(0.0032) Grad: 10006.1973  LR: 0.000009  \n","Epoch: [4][300/3575] Elapsed 1m 13s (remain 13m 15s) Loss: 0.0036(0.0032) Grad: 28408.3633  LR: 0.000009  \n","Epoch: [4][400/3575] Elapsed 1m 35s (remain 12m 35s) Loss: 0.0012(0.0034) Grad: 13595.6260  LR: 0.000008  \n","Epoch: [4][500/3575] Elapsed 1m 57s (remain 12m 3s) Loss: 0.0376(0.0036) Grad: 40246.4297  LR: 0.000008  \n","Epoch: [4][600/3575] Elapsed 2m 20s (remain 11m 33s) Loss: 0.0000(0.0036) Grad: 234.3292  LR: 0.000008  \n","Epoch: [4][700/3575] Elapsed 2m 42s (remain 11m 6s) Loss: 0.0000(0.0038) Grad: 10.2267  LR: 0.000008  \n","Epoch: [4][800/3575] Elapsed 3m 5s (remain 10m 40s) Loss: 0.0000(0.0038) Grad: 27.4883  LR: 0.000008  \n","Epoch: [4][900/3575] Elapsed 3m 27s (remain 10m 15s) Loss: 0.0001(0.0038) Grad: 297.3849  LR: 0.000008  \n","Epoch: [4][1000/3575] Elapsed 3m 49s (remain 9m 50s) Loss: 0.0041(0.0039) Grad: 8854.4844  LR: 0.000008  \n","Epoch: [4][1100/3575] Elapsed 4m 11s (remain 9m 25s) Loss: 0.0010(0.0039) Grad: 4803.7832  LR: 0.000008  \n","Epoch: [4][1200/3575] Elapsed 4m 33s (remain 9m 1s) Loss: 0.0083(0.0038) Grad: 24495.4199  LR: 0.000007  \n","Epoch: [4][1300/3575] Elapsed 4m 56s (remain 8m 37s) Loss: 0.0000(0.0039) Grad: 260.3421  LR: 0.000007  \n","Epoch: [4][1400/3575] Elapsed 5m 18s (remain 8m 14s) Loss: 0.0032(0.0039) Grad: 6189.9189  LR: 0.000007  \n","Epoch: [4][1500/3575] Elapsed 5m 40s (remain 7m 50s) Loss: 0.0117(0.0038) Grad: 97098.6953  LR: 0.000007  \n","Epoch: [4][1600/3575] Elapsed 6m 3s (remain 7m 27s) Loss: 0.0000(0.0038) Grad: 53.4752  LR: 0.000007  \n","Epoch: [4][1700/3575] Elapsed 6m 25s (remain 7m 4s) Loss: 0.0001(0.0037) Grad: 635.7783  LR: 0.000007  \n","Epoch: [4][1800/3575] Elapsed 6m 47s (remain 6m 41s) Loss: 0.0086(0.0037) Grad: 31055.2090  LR: 0.000007  \n","Epoch: [4][1900/3575] Elapsed 7m 9s (remain 6m 18s) Loss: 0.0008(0.0038) Grad: 16222.6924  LR: 0.000007  \n","Epoch: [4][2000/3575] Elapsed 7m 32s (remain 5m 55s) Loss: 0.0000(0.0038) Grad: 188.5591  LR: 0.000006  \n","Epoch: [4][2100/3575] Elapsed 7m 54s (remain 5m 33s) Loss: 0.0017(0.0039) Grad: 16385.8438  LR: 0.000006  \n","Epoch: [4][2200/3575] Elapsed 8m 17s (remain 5m 10s) Loss: 0.0282(0.0040) Grad: 13610.1182  LR: 0.000006  \n","Epoch: [4][2300/3575] Elapsed 8m 39s (remain 4m 47s) Loss: 0.0000(0.0040) Grad: 66.3566  LR: 0.000006  \n","Epoch: [4][2400/3575] Elapsed 9m 2s (remain 4m 25s) Loss: 0.0002(0.0040) Grad: 1455.9666  LR: 0.000006  \n","Epoch: [4][2500/3575] Elapsed 9m 24s (remain 4m 2s) Loss: 0.0000(0.0040) Grad: 41.0950  LR: 0.000006  \n","Epoch: [4][2600/3575] Elapsed 9m 46s (remain 3m 39s) Loss: 0.0001(0.0040) Grad: 345.3140  LR: 0.000006  \n","Epoch: [4][2700/3575] Elapsed 10m 8s (remain 3m 17s) Loss: 0.0031(0.0041) Grad: 23897.7402  LR: 0.000006  \n","Epoch: [4][2800/3575] Elapsed 10m 31s (remain 2m 54s) Loss: 0.0000(0.0040) Grad: 11.1804  LR: 0.000005  \n","Epoch: [4][2900/3575] Elapsed 10m 52s (remain 2m 31s) Loss: 0.0001(0.0040) Grad: 1519.8107  LR: 0.000005  \n","Epoch: [4][3000/3575] Elapsed 11m 14s (remain 2m 9s) Loss: 0.0000(0.0040) Grad: 8.6855  LR: 0.000005  \n","Epoch: [4][3100/3575] Elapsed 11m 36s (remain 1m 46s) Loss: 0.0079(0.0041) Grad: 59041.1445  LR: 0.000005  \n","Epoch: [4][3200/3575] Elapsed 11m 58s (remain 1m 23s) Loss: 0.0000(0.0041) Grad: 20.3532  LR: 0.000005  \n","Epoch: [4][3300/3575] Elapsed 12m 21s (remain 1m 1s) Loss: 0.0000(0.0041) Grad: 24.5080  LR: 0.000005  \n","Epoch: [4][3400/3575] Elapsed 12m 43s (remain 0m 39s) Loss: 0.0001(0.0041) Grad: 254.8259  LR: 0.000005  \n","Epoch: [4][3500/3575] Elapsed 13m 5s (remain 0m 16s) Loss: 0.0000(0.0041) Grad: 43.3830  LR: 0.000005  \n","Epoch: [4][3574/3575] Elapsed 13m 21s (remain 0m 0s) Loss: 0.0003(0.0041) Grad: 2080.4395  LR: 0.000004  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 47s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 5s) Loss: 0.0228(0.0090) \n","EVAL: [200/1192] Elapsed 0m 22s (remain 1m 53s) Loss: 0.0015(0.0077) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 41s) Loss: 0.0042(0.0075) \n","EVAL: [400/1192] Elapsed 0m 45s (remain 1m 29s) Loss: 0.0001(0.0078) \n","EVAL: [500/1192] Elapsed 0m 56s (remain 1m 18s) Loss: 0.0000(0.0076) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 7s) Loss: 0.0054(0.0075) \n","EVAL: [700/1192] Elapsed 1m 19s (remain 0m 55s) Loss: 0.0055(0.0082) \n","EVAL: [800/1192] Elapsed 1m 30s (remain 0m 44s) Loss: 0.0000(0.0083) \n","EVAL: [900/1192] Elapsed 1m 42s (remain 0m 33s) Loss: 0.0043(0.0085) \n","EVAL: [1000/1192] Elapsed 1m 53s (remain 0m 21s) Loss: 0.0000(0.0083) \n","EVAL: [1100/1192] Elapsed 2m 4s (remain 0m 10s) Loss: 0.0026(0.0080) \n","EVAL: [1191/1192] Elapsed 2m 15s (remain 0m 0s) Loss: 0.0000(0.0076) \n","Epoch 4 - avg_train_loss: 0.0041  avg_val_loss: 0.0076  time: 942s\n","Epoch 4 - Score: 0.8824\n","Epoch 4 - Save Best Score: 0.8824 Model\n","Epoch: [5][0/3575] Elapsed 0m 0s (remain 35m 46s) Loss: 0.0053(0.0053) Grad: 10631.8096  LR: 0.000004  \n","Epoch: [5][100/3575] Elapsed 0m 27s (remain 15m 43s) Loss: 0.0034(0.0045) Grad: 4451.8735  LR: 0.000004  \n","Epoch: [5][200/3575] Elapsed 0m 51s (remain 14m 29s) Loss: 0.0043(0.0034) Grad: 17016.2891  LR: 0.000004  \n","Epoch: [5][300/3575] Elapsed 1m 13s (remain 13m 23s) Loss: 0.0028(0.0032) Grad: 43130.4414  LR: 0.000004  \n","Epoch: [5][400/3575] Elapsed 1m 36s (remain 12m 41s) Loss: 0.0031(0.0030) Grad: 22424.3926  LR: 0.000004  \n","Epoch: [5][500/3575] Elapsed 1m 58s (remain 12m 6s) Loss: 0.0009(0.0031) Grad: 7626.7158  LR: 0.000004  \n","Epoch: [5][600/3575] Elapsed 2m 20s (remain 11m 34s) Loss: 0.0000(0.0030) Grad: 37.4919  LR: 0.000004  \n","Epoch: [5][700/3575] Elapsed 2m 41s (remain 11m 3s) Loss: 0.0000(0.0031) Grad: 738.7103  LR: 0.000004  \n","Epoch: [5][800/3575] Elapsed 3m 3s (remain 10m 37s) Loss: 0.0000(0.0030) Grad: 8.1911  LR: 0.000003  \n","Epoch: [5][900/3575] Elapsed 3m 26s (remain 10m 12s) Loss: 0.0017(0.0030) Grad: 2764.6079  LR: 0.000003  \n","Epoch: [5][1000/3575] Elapsed 3m 48s (remain 9m 48s) Loss: 0.0000(0.0030) Grad: 29.4402  LR: 0.000003  \n","Epoch: [5][1100/3575] Elapsed 4m 10s (remain 9m 23s) Loss: 0.0000(0.0031) Grad: 4.2005  LR: 0.000003  \n","Epoch: [5][1200/3575] Elapsed 4m 33s (remain 8m 59s) Loss: 0.0000(0.0030) Grad: 88.9498  LR: 0.000003  \n","Epoch: [5][1300/3575] Elapsed 4m 55s (remain 8m 36s) Loss: 0.0000(0.0030) Grad: 5.3135  LR: 0.000003  \n","Epoch: [5][1400/3575] Elapsed 5m 17s (remain 8m 12s) Loss: 0.0001(0.0030) Grad: 620.3118  LR: 0.000003  \n","Epoch: [5][1500/3575] Elapsed 5m 39s (remain 7m 49s) Loss: 0.0000(0.0031) Grad: 16.7071  LR: 0.000003  \n","Epoch: [5][1600/3575] Elapsed 6m 2s (remain 7m 26s) Loss: 0.0007(0.0031) Grad: 6008.2446  LR: 0.000002  \n","Epoch: [5][1700/3575] Elapsed 6m 23s (remain 7m 2s) Loss: 0.0001(0.0030) Grad: 1256.8014  LR: 0.000002  \n","Epoch: [5][1800/3575] Elapsed 6m 45s (remain 6m 39s) Loss: 0.0001(0.0030) Grad: 335.5535  LR: 0.000002  \n","Epoch: [5][1900/3575] Elapsed 7m 7s (remain 6m 16s) Loss: 0.0000(0.0030) Grad: 27.6613  LR: 0.000002  \n","Epoch: [5][2000/3575] Elapsed 7m 29s (remain 5m 53s) Loss: 0.0042(0.0031) Grad: 23121.5977  LR: 0.000002  \n","Epoch: [5][2100/3575] Elapsed 7m 52s (remain 5m 31s) Loss: 0.0000(0.0032) Grad: 13.7725  LR: 0.000002  \n","Epoch: [5][2200/3575] Elapsed 8m 14s (remain 5m 8s) Loss: 0.0038(0.0032) Grad: 6852.4502  LR: 0.000002  \n","Epoch: [5][2300/3575] Elapsed 8m 36s (remain 4m 46s) Loss: 0.0027(0.0032) Grad: 11930.9443  LR: 0.000002  \n","Epoch: [5][2400/3575] Elapsed 8m 58s (remain 4m 23s) Loss: 0.0006(0.0032) Grad: 3333.4094  LR: 0.000001  \n","Epoch: [5][2500/3575] Elapsed 9m 20s (remain 4m 0s) Loss: 0.0000(0.0032) Grad: 673.6276  LR: 0.000001  \n","Epoch: [5][2600/3575] Elapsed 9m 43s (remain 3m 38s) Loss: 0.0000(0.0032) Grad: 24.3795  LR: 0.000001  \n","Epoch: [5][2700/3575] Elapsed 10m 5s (remain 3m 15s) Loss: 0.0000(0.0032) Grad: 14.0390  LR: 0.000001  \n","Epoch: [5][2800/3575] Elapsed 10m 28s (remain 2m 53s) Loss: 0.0029(0.0031) Grad: 14040.8311  LR: 0.000001  \n","Epoch: [5][2900/3575] Elapsed 10m 50s (remain 2m 31s) Loss: 0.0029(0.0031) Grad: 8280.9619  LR: 0.000001  \n","Epoch: [5][3000/3575] Elapsed 11m 12s (remain 2m 8s) Loss: 0.0000(0.0032) Grad: 74.5765  LR: 0.000001  \n","Epoch: [5][3100/3575] Elapsed 11m 34s (remain 1m 46s) Loss: 0.0135(0.0032) Grad: 54350.4297  LR: 0.000001  \n","Epoch: [5][3200/3575] Elapsed 11m 57s (remain 1m 23s) Loss: 0.0023(0.0032) Grad: 31618.9785  LR: 0.000000  \n","Epoch: [5][3300/3575] Elapsed 12m 19s (remain 1m 1s) Loss: 0.0000(0.0032) Grad: 25.7571  LR: 0.000000  \n","Epoch: [5][3400/3575] Elapsed 12m 41s (remain 0m 38s) Loss: 0.0003(0.0032) Grad: 4184.6826  LR: 0.000000  \n","Epoch: [5][3500/3575] Elapsed 13m 4s (remain 0m 16s) Loss: 0.0000(0.0032) Grad: 3.8269  LR: 0.000000  \n","Epoch: [5][3574/3575] Elapsed 13m 20s (remain 0m 0s) Loss: 0.0026(0.0032) Grad: 23477.6816  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 48s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 7s) Loss: 0.0338(0.0103) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 53s) Loss: 0.0112(0.0091) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 41s) Loss: 0.0077(0.0085) \n","EVAL: [400/1192] Elapsed 0m 45s (remain 1m 30s) Loss: 0.0000(0.0089) \n","EVAL: [500/1192] Elapsed 0m 57s (remain 1m 18s) Loss: 0.0000(0.0086) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 7s) Loss: 0.0094(0.0086) \n","EVAL: [700/1192] Elapsed 1m 19s (remain 0m 55s) Loss: 0.0051(0.0094) \n","EVAL: [800/1192] Elapsed 1m 31s (remain 0m 44s) Loss: 0.0000(0.0094) \n","EVAL: [900/1192] Elapsed 1m 42s (remain 0m 33s) Loss: 0.0053(0.0096) \n","EVAL: [1000/1192] Elapsed 1m 53s (remain 0m 21s) Loss: 0.0000(0.0094) \n","EVAL: [1100/1192] Elapsed 2m 5s (remain 0m 10s) Loss: 0.0034(0.0091) \n","EVAL: [1191/1192] Elapsed 2m 15s (remain 0m 0s) Loss: 0.0000(0.0086) \n","Epoch 5 - avg_train_loss: 0.0032  avg_val_loss: 0.0086  time: 941s\n","Epoch 5 - Score: 0.8825\n","Epoch 5 - Save Best Score: 0.8825 Model\n","========== fold: 3 training ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n","Epoch: [1][0/3575] Elapsed 0m 0s (remain 45m 34s) Loss: 0.3912(0.3912) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/3575] Elapsed 0m 28s (remain 16m 18s) Loss: 0.2600(0.3757) Grad: 44150.8828  LR: 0.000001  \n","Epoch: [1][200/3575] Elapsed 0m 51s (remain 14m 27s) Loss: 0.0204(0.2527) Grad: 1926.9010  LR: 0.000002  \n","Epoch: [1][300/3575] Elapsed 1m 14s (remain 13m 32s) Loss: 0.0484(0.1802) Grad: 4162.3198  LR: 0.000003  \n","Epoch: [1][400/3575] Elapsed 1m 37s (remain 12m 50s) Loss: 0.0189(0.1414) Grad: 5993.6211  LR: 0.000004  \n","Epoch: [1][500/3575] Elapsed 2m 0s (remain 12m 16s) Loss: 0.0413(0.1175) Grad: 3681.3594  LR: 0.000006  \n","Epoch: [1][600/3575] Elapsed 2m 22s (remain 11m 46s) Loss: 0.0099(0.1011) Grad: 5316.8740  LR: 0.000007  \n","Epoch: [1][700/3575] Elapsed 2m 45s (remain 11m 16s) Loss: 0.0013(0.0887) Grad: 812.9894  LR: 0.000008  \n","Epoch: [1][800/3575] Elapsed 3m 7s (remain 10m 48s) Loss: 0.0035(0.0794) Grad: 594.9609  LR: 0.000009  \n","Epoch: [1][900/3575] Elapsed 3m 29s (remain 10m 22s) Loss: 0.0061(0.0719) Grad: 2765.4502  LR: 0.000010  \n","Epoch: [1][1000/3575] Elapsed 3m 51s (remain 9m 56s) Loss: 0.0049(0.0660) Grad: 1511.3214  LR: 0.000011  \n","Epoch: [1][1100/3575] Elapsed 4m 14s (remain 9m 31s) Loss: 0.0059(0.0608) Grad: 6205.6392  LR: 0.000012  \n","Epoch: [1][1200/3575] Elapsed 4m 36s (remain 9m 7s) Loss: 0.0015(0.0568) Grad: 815.4476  LR: 0.000013  \n","Epoch: [1][1300/3575] Elapsed 5m 0s (remain 8m 45s) Loss: 0.0023(0.0536) Grad: 1278.4701  LR: 0.000015  \n","Epoch: [1][1400/3575] Elapsed 5m 23s (remain 8m 21s) Loss: 0.0072(0.0503) Grad: 5057.8340  LR: 0.000016  \n","Epoch: [1][1500/3575] Elapsed 5m 45s (remain 7m 57s) Loss: 0.0379(0.0477) Grad: 2017.7644  LR: 0.000017  \n","Epoch: [1][1600/3575] Elapsed 6m 8s (remain 7m 34s) Loss: 0.0096(0.0454) Grad: 3200.7620  LR: 0.000018  \n","Epoch: [1][1700/3575] Elapsed 6m 30s (remain 7m 10s) Loss: 0.0044(0.0435) Grad: 2465.2625  LR: 0.000019  \n","Epoch: [1][1800/3575] Elapsed 6m 53s (remain 6m 47s) Loss: 0.0092(0.0417) Grad: 2105.8420  LR: 0.000020  \n","Epoch: [1][1900/3575] Elapsed 7m 15s (remain 6m 23s) Loss: 0.0090(0.0400) Grad: 7945.3779  LR: 0.000020  \n","Epoch: [1][2000/3575] Elapsed 7m 38s (remain 6m 0s) Loss: 0.0046(0.0385) Grad: 1884.6495  LR: 0.000020  \n","Epoch: [1][2100/3575] Elapsed 8m 0s (remain 5m 37s) Loss: 0.0014(0.0373) Grad: 549.9117  LR: 0.000020  \n","Epoch: [1][2200/3575] Elapsed 8m 22s (remain 5m 13s) Loss: 0.0058(0.0361) Grad: 1931.6765  LR: 0.000019  \n","Epoch: [1][2300/3575] Elapsed 8m 44s (remain 4m 50s) Loss: 0.0182(0.0349) Grad: 1709.2939  LR: 0.000019  \n","Epoch: [1][2400/3575] Elapsed 9m 7s (remain 4m 27s) Loss: 0.0049(0.0339) Grad: 1270.0208  LR: 0.000019  \n","Epoch: [1][2500/3575] Elapsed 9m 30s (remain 4m 4s) Loss: 0.0170(0.0329) Grad: 8417.2881  LR: 0.000019  \n","Epoch: [1][2600/3575] Elapsed 9m 53s (remain 3m 42s) Loss: 0.0078(0.0319) Grad: 2269.8137  LR: 0.000019  \n","Epoch: [1][2700/3575] Elapsed 10m 15s (remain 3m 19s) Loss: 0.0123(0.0311) Grad: 4622.5557  LR: 0.000019  \n","Epoch: [1][2800/3575] Elapsed 10m 38s (remain 2m 56s) Loss: 0.0001(0.0303) Grad: 69.6977  LR: 0.000019  \n","Epoch: [1][2900/3575] Elapsed 11m 0s (remain 2m 33s) Loss: 0.0026(0.0295) Grad: 765.7563  LR: 0.000019  \n","Epoch: [1][3000/3575] Elapsed 11m 23s (remain 2m 10s) Loss: 0.0019(0.0288) Grad: 854.2687  LR: 0.000018  \n","Epoch: [1][3100/3575] Elapsed 11m 45s (remain 1m 47s) Loss: 0.0014(0.0281) Grad: 3343.1653  LR: 0.000018  \n","Epoch: [1][3200/3575] Elapsed 12m 7s (remain 1m 25s) Loss: 0.0021(0.0275) Grad: 829.5974  LR: 0.000018  \n","Epoch: [1][3300/3575] Elapsed 12m 29s (remain 1m 2s) Loss: 0.0004(0.0271) Grad: 101.4714  LR: 0.000018  \n","Epoch: [1][3400/3575] Elapsed 12m 51s (remain 0m 39s) Loss: 0.0002(0.0265) Grad: 139.1956  LR: 0.000018  \n","Epoch: [1][3500/3575] Elapsed 13m 13s (remain 0m 16s) Loss: 0.0021(0.0259) Grad: 670.8148  LR: 0.000018  \n","Epoch: [1][3574/3575] Elapsed 13m 30s (remain 0m 0s) Loss: 0.0054(0.0256) Grad: 707.3905  LR: 0.000018  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 9m 24s) Loss: 0.0012(0.0012) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 7s) Loss: 0.0253(0.0061) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 54s) Loss: 0.0059(0.0060) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 42s) Loss: 0.0060(0.0065) \n","EVAL: [400/1192] Elapsed 0m 45s (remain 1m 30s) Loss: 0.0002(0.0064) \n","EVAL: [500/1192] Elapsed 0m 57s (remain 1m 18s) Loss: 0.0175(0.0065) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 7s) Loss: 0.0024(0.0067) \n","EVAL: [700/1192] Elapsed 1m 19s (remain 0m 55s) Loss: 0.0055(0.0072) \n","EVAL: [800/1192] Elapsed 1m 30s (remain 0m 44s) Loss: 0.0119(0.0071) \n","EVAL: [900/1192] Elapsed 1m 41s (remain 0m 32s) Loss: 0.0044(0.0072) \n","EVAL: [1000/1192] Elapsed 1m 52s (remain 0m 21s) Loss: 0.0003(0.0070) \n","EVAL: [1100/1192] Elapsed 2m 4s (remain 0m 10s) Loss: 0.0132(0.0068) \n","EVAL: [1191/1192] Elapsed 2m 14s (remain 0m 0s) Loss: 0.0002(0.0068) \n","Epoch 1 - avg_train_loss: 0.0256  avg_val_loss: 0.0068  time: 950s\n","Epoch 1 - Score: 0.8495\n","Epoch 1 - Save Best Score: 0.8495 Model\n","Epoch: [2][0/3575] Elapsed 0m 0s (remain 33m 29s) Loss: 0.0004(0.0004) Grad: 1263.5391  LR: 0.000018  \n","Epoch: [2][100/3575] Elapsed 0m 26s (remain 15m 0s) Loss: 0.0117(0.0067) Grad: 20860.4785  LR: 0.000018  \n","Epoch: [2][200/3575] Elapsed 0m 50s (remain 14m 15s) Loss: 0.0177(0.0064) Grad: 38459.2344  LR: 0.000018  \n","Epoch: [2][300/3575] Elapsed 1m 13s (remain 13m 17s) Loss: 0.0053(0.0062) Grad: 20781.3691  LR: 0.000017  \n","Epoch: [2][400/3575] Elapsed 1m 35s (remain 12m 37s) Loss: 0.0019(0.0058) Grad: 6141.7422  LR: 0.000017  \n","Epoch: [2][500/3575] Elapsed 1m 58s (remain 12m 4s) Loss: 0.0002(0.0059) Grad: 8452.2607  LR: 0.000017  \n","Epoch: [2][600/3575] Elapsed 2m 20s (remain 11m 33s) Loss: 0.0013(0.0062) Grad: 9423.5518  LR: 0.000017  \n","Epoch: [2][700/3575] Elapsed 2m 42s (remain 11m 6s) Loss: 0.0017(0.0064) Grad: 23434.2930  LR: 0.000017  \n","Epoch: [2][800/3575] Elapsed 3m 5s (remain 10m 42s) Loss: 0.0004(0.0065) Grad: 1839.5586  LR: 0.000017  \n","Epoch: [2][900/3575] Elapsed 3m 28s (remain 10m 18s) Loss: 0.0004(0.0067) Grad: 2364.2349  LR: 0.000017  \n","Epoch: [2][1000/3575] Elapsed 3m 50s (remain 9m 53s) Loss: 0.0109(0.0067) Grad: 36979.1914  LR: 0.000017  \n","Epoch: [2][1100/3575] Elapsed 4m 13s (remain 9m 29s) Loss: 0.0100(0.0066) Grad: 21353.4062  LR: 0.000016  \n","Epoch: [2][1200/3575] Elapsed 4m 35s (remain 9m 5s) Loss: 0.0033(0.0067) Grad: 9291.1416  LR: 0.000016  \n","Epoch: [2][1300/3575] Elapsed 4m 58s (remain 8m 41s) Loss: 0.0048(0.0065) Grad: 85109.1406  LR: 0.000016  \n","Epoch: [2][1400/3575] Elapsed 5m 20s (remain 8m 17s) Loss: 0.0302(0.0065) Grad: 32926.0977  LR: 0.000016  \n","Epoch: [2][1500/3575] Elapsed 5m 43s (remain 7m 54s) Loss: 0.0002(0.0065) Grad: 1108.9376  LR: 0.000016  \n","Epoch: [2][1600/3575] Elapsed 6m 5s (remain 7m 30s) Loss: 0.0032(0.0064) Grad: 10631.0605  LR: 0.000016  \n","Epoch: [2][1700/3575] Elapsed 6m 28s (remain 7m 7s) Loss: 0.0026(0.0064) Grad: 13363.1543  LR: 0.000016  \n","Epoch: [2][1800/3575] Elapsed 6m 50s (remain 6m 44s) Loss: 0.0001(0.0064) Grad: 440.9843  LR: 0.000016  \n","Epoch: [2][1900/3575] Elapsed 7m 12s (remain 6m 21s) Loss: 0.0136(0.0065) Grad: 9780.8271  LR: 0.000015  \n","Epoch: [2][2000/3575] Elapsed 7m 35s (remain 5m 57s) Loss: 0.0359(0.0065) Grad: 134522.3906  LR: 0.000015  \n","Epoch: [2][2100/3575] Elapsed 7m 57s (remain 5m 34s) Loss: 0.0019(0.0066) Grad: 2711.6411  LR: 0.000015  \n","Epoch: [2][2200/3575] Elapsed 8m 19s (remain 5m 11s) Loss: 0.0005(0.0065) Grad: 4546.8564  LR: 0.000015  \n","Epoch: [2][2300/3575] Elapsed 8m 41s (remain 4m 48s) Loss: 0.0002(0.0065) Grad: 631.4020  LR: 0.000015  \n","Epoch: [2][2400/3575] Elapsed 9m 4s (remain 4m 26s) Loss: 0.0015(0.0065) Grad: 9294.5723  LR: 0.000015  \n","Epoch: [2][2500/3575] Elapsed 9m 26s (remain 4m 3s) Loss: 0.0006(0.0066) Grad: 2147.9624  LR: 0.000015  \n","Epoch: [2][2600/3575] Elapsed 9m 49s (remain 3m 40s) Loss: 0.0140(0.0066) Grad: 42839.6523  LR: 0.000015  \n","Epoch: [2][2700/3575] Elapsed 10m 11s (remain 3m 17s) Loss: 0.0000(0.0066) Grad: 54.8250  LR: 0.000014  \n","Epoch: [2][2800/3575] Elapsed 10m 33s (remain 2m 55s) Loss: 0.0513(0.0066) Grad: 146530.7031  LR: 0.000014  \n","Epoch: [2][2900/3575] Elapsed 10m 56s (remain 2m 32s) Loss: 0.0012(0.0067) Grad: 6229.9395  LR: 0.000014  \n","Epoch: [2][3000/3575] Elapsed 11m 18s (remain 2m 9s) Loss: 0.0000(0.0066) Grad: 57.2422  LR: 0.000014  \n","Epoch: [2][3100/3575] Elapsed 11m 41s (remain 1m 47s) Loss: 0.0001(0.0066) Grad: 554.6150  LR: 0.000014  \n","Epoch: [2][3200/3575] Elapsed 12m 3s (remain 1m 24s) Loss: 0.0002(0.0066) Grad: 916.5054  LR: 0.000014  \n","Epoch: [2][3300/3575] Elapsed 12m 25s (remain 1m 1s) Loss: 0.0040(0.0066) Grad: 6517.2153  LR: 0.000014  \n","Epoch: [2][3400/3575] Elapsed 12m 48s (remain 0m 39s) Loss: 0.0024(0.0066) Grad: 12782.8213  LR: 0.000014  \n","Epoch: [2][3500/3575] Elapsed 13m 10s (remain 0m 16s) Loss: 0.0171(0.0066) Grad: 15948.4668  LR: 0.000013  \n","Epoch: [2][3574/3575] Elapsed 13m 27s (remain 0m 0s) Loss: 0.0061(0.0066) Grad: 14495.2129  LR: 0.000013  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 34s) Loss: 0.0001(0.0001) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 6s) Loss: 0.0330(0.0080) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 53s) Loss: 0.0075(0.0068) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 41s) Loss: 0.0048(0.0076) \n","EVAL: [400/1192] Elapsed 0m 45s (remain 1m 30s) Loss: 0.0000(0.0075) \n","EVAL: [500/1192] Elapsed 0m 57s (remain 1m 18s) Loss: 0.0271(0.0074) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 7s) Loss: 0.0099(0.0079) \n","EVAL: [700/1192] Elapsed 1m 19s (remain 0m 55s) Loss: 0.0049(0.0088) \n","EVAL: [800/1192] Elapsed 1m 30s (remain 0m 44s) Loss: 0.0041(0.0086) \n","EVAL: [900/1192] Elapsed 1m 42s (remain 0m 32s) Loss: 0.0010(0.0089) \n","EVAL: [1000/1192] Elapsed 1m 53s (remain 0m 21s) Loss: 0.0000(0.0085) \n","EVAL: [1100/1192] Elapsed 2m 4s (remain 0m 10s) Loss: 0.0136(0.0082) \n","EVAL: [1191/1192] Elapsed 2m 14s (remain 0m 0s) Loss: 0.0000(0.0080) \n","Epoch 2 - avg_train_loss: 0.0066  avg_val_loss: 0.0080  time: 947s\n","Epoch 2 - Score: 0.8684\n","Epoch 2 - Save Best Score: 0.8684 Model\n","Epoch: [3][0/3575] Elapsed 0m 0s (remain 34m 37s) Loss: 0.0000(0.0000) Grad: 1005.3741  LR: 0.000013  \n","Epoch: [3][100/3575] Elapsed 0m 25s (remain 14m 41s) Loss: 0.0000(0.0060) Grad: 205.9212  LR: 0.000013  \n","Epoch: [3][200/3575] Elapsed 0m 50s (remain 14m 15s) Loss: 0.0000(0.0068) Grad: 209.9073  LR: 0.000013  \n","Epoch: [3][300/3575] Elapsed 1m 12s (remain 13m 4s) Loss: 0.0001(0.0064) Grad: 1287.1143  LR: 0.000013  \n","Epoch: [3][400/3575] Elapsed 1m 34s (remain 12m 27s) Loss: 0.0000(0.0062) Grad: 31.8247  LR: 0.000013  \n","Epoch: [3][500/3575] Elapsed 1m 56s (remain 11m 55s) Loss: 0.0002(0.0061) Grad: 909.2473  LR: 0.000013  \n","Epoch: [3][600/3575] Elapsed 2m 18s (remain 11m 26s) Loss: 0.0000(0.0060) Grad: 88.3415  LR: 0.000013  \n","Epoch: [3][700/3575] Elapsed 2m 40s (remain 10m 59s) Loss: 0.0003(0.0059) Grad: 1066.7662  LR: 0.000012  \n","Epoch: [3][800/3575] Elapsed 3m 3s (remain 10m 33s) Loss: 0.0000(0.0057) Grad: 101.9457  LR: 0.000012  \n","Epoch: [3][900/3575] Elapsed 3m 25s (remain 10m 8s) Loss: 0.0000(0.0059) Grad: 48.0113  LR: 0.000012  \n","Epoch: [3][1000/3575] Elapsed 3m 46s (remain 9m 41s) Loss: 0.0000(0.0057) Grad: 27.5298  LR: 0.000012  \n","Epoch: [3][1100/3575] Elapsed 4m 8s (remain 9m 18s) Loss: 0.0002(0.0059) Grad: 805.9367  LR: 0.000012  \n","Epoch: [3][1200/3575] Elapsed 4m 30s (remain 8m 54s) Loss: 0.0060(0.0059) Grad: 10560.0801  LR: 0.000012  \n","Epoch: [3][1300/3575] Elapsed 4m 52s (remain 8m 31s) Loss: 0.0002(0.0058) Grad: 2844.2700  LR: 0.000012  \n","Epoch: [3][1400/3575] Elapsed 5m 15s (remain 8m 10s) Loss: 0.0509(0.0058) Grad: 32565.2480  LR: 0.000012  \n","Epoch: [3][1500/3575] Elapsed 5m 38s (remain 7m 47s) Loss: 0.0007(0.0058) Grad: 3588.2302  LR: 0.000011  \n","Epoch: [3][1600/3575] Elapsed 6m 1s (remain 7m 25s) Loss: 0.0103(0.0058) Grad: 48838.5781  LR: 0.000011  \n","Epoch: [3][1700/3575] Elapsed 6m 23s (remain 7m 2s) Loss: 0.0018(0.0058) Grad: 13380.7842  LR: 0.000011  \n","Epoch: [3][1800/3575] Elapsed 6m 44s (remain 6m 38s) Loss: 0.0090(0.0057) Grad: 6672.0024  LR: 0.000011  \n","Epoch: [3][1900/3575] Elapsed 7m 6s (remain 6m 15s) Loss: 0.0408(0.0058) Grad: 40190.4844  LR: 0.000011  \n","Epoch: [3][2000/3575] Elapsed 7m 28s (remain 5m 53s) Loss: 0.0040(0.0057) Grad: 5102.7456  LR: 0.000011  \n","Epoch: [3][2100/3575] Elapsed 7m 50s (remain 5m 30s) Loss: 0.0016(0.0057) Grad: 5692.3911  LR: 0.000011  \n","Epoch: [3][2200/3575] Elapsed 8m 12s (remain 5m 7s) Loss: 0.0029(0.0057) Grad: 36243.1445  LR: 0.000011  \n","Epoch: [3][2300/3575] Elapsed 8m 35s (remain 4m 45s) Loss: 0.0375(0.0056) Grad: 20390.1230  LR: 0.000010  \n","Epoch: [3][2400/3575] Elapsed 8m 57s (remain 4m 22s) Loss: 0.0000(0.0056) Grad: 212.8166  LR: 0.000010  \n","Epoch: [3][2500/3575] Elapsed 9m 19s (remain 4m 0s) Loss: 0.0002(0.0056) Grad: 9097.9795  LR: 0.000010  \n","Epoch: [3][2600/3575] Elapsed 9m 40s (remain 3m 37s) Loss: 0.0012(0.0056) Grad: 4061.8535  LR: 0.000010  \n","Epoch: [3][2700/3575] Elapsed 10m 2s (remain 3m 15s) Loss: 0.0001(0.0057) Grad: 833.4112  LR: 0.000010  \n","Epoch: [3][2800/3575] Elapsed 10m 25s (remain 2m 52s) Loss: 0.0001(0.0056) Grad: 1226.9314  LR: 0.000010  \n","Epoch: [3][2900/3575] Elapsed 10m 47s (remain 2m 30s) Loss: 0.0000(0.0056) Grad: 91.0123  LR: 0.000010  \n","Epoch: [3][3000/3575] Elapsed 11m 9s (remain 2m 8s) Loss: 0.0001(0.0057) Grad: 238.8217  LR: 0.000010  \n","Epoch: [3][3100/3575] Elapsed 11m 31s (remain 1m 45s) Loss: 0.0104(0.0056) Grad: 13061.8281  LR: 0.000009  \n","Epoch: [3][3200/3575] Elapsed 11m 54s (remain 1m 23s) Loss: 0.0054(0.0056) Grad: 40573.1562  LR: 0.000009  \n","Epoch: [3][3300/3575] Elapsed 12m 16s (remain 1m 1s) Loss: 0.0000(0.0056) Grad: 16.3594  LR: 0.000009  \n","Epoch: [3][3400/3575] Elapsed 12m 38s (remain 0m 38s) Loss: 0.0002(0.0056) Grad: 2172.2021  LR: 0.000009  \n","Epoch: [3][3500/3575] Elapsed 13m 0s (remain 0m 16s) Loss: 0.0000(0.0056) Grad: 31.0659  LR: 0.000009  \n","Epoch: [3][3574/3575] Elapsed 13m 16s (remain 0m 0s) Loss: 0.0042(0.0055) Grad: 22697.5488  LR: 0.000009  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 58s) Loss: 0.0002(0.0002) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 7s) Loss: 0.0362(0.0074) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 53s) Loss: 0.0166(0.0074) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 41s) Loss: 0.0107(0.0075) \n","EVAL: [400/1192] Elapsed 0m 45s (remain 1m 30s) Loss: 0.0000(0.0074) \n","EVAL: [500/1192] Elapsed 0m 56s (remain 1m 18s) Loss: 0.0300(0.0070) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 7s) Loss: 0.0118(0.0075) \n","EVAL: [700/1192] Elapsed 1m 19s (remain 0m 55s) Loss: 0.0039(0.0083) \n","EVAL: [800/1192] Elapsed 1m 30s (remain 0m 44s) Loss: 0.0063(0.0082) \n","EVAL: [900/1192] Elapsed 1m 42s (remain 0m 32s) Loss: 0.0064(0.0085) \n","EVAL: [1000/1192] Elapsed 1m 53s (remain 0m 21s) Loss: 0.0000(0.0081) \n","EVAL: [1100/1192] Elapsed 2m 4s (remain 0m 10s) Loss: 0.0130(0.0079) \n","EVAL: [1191/1192] Elapsed 2m 14s (remain 0m 0s) Loss: 0.0000(0.0078) \n","Epoch 3 - avg_train_loss: 0.0055  avg_val_loss: 0.0078  time: 937s\n","Epoch 3 - Score: 0.8847\n","Epoch 3 - Save Best Score: 0.8847 Model\n","Epoch: [4][0/3575] Elapsed 0m 0s (remain 36m 24s) Loss: 0.0020(0.0020) Grad: 10344.6709  LR: 0.000009  \n","Epoch: [4][100/3575] Elapsed 0m 27s (remain 15m 42s) Loss: 0.0000(0.0042) Grad: 21.0404  LR: 0.000009  \n","Epoch: [4][200/3575] Elapsed 0m 51s (remain 14m 30s) Loss: 0.0068(0.0053) Grad: 33475.4336  LR: 0.000009  \n","Epoch: [4][300/3575] Elapsed 1m 14s (remain 13m 28s) Loss: 0.0247(0.0053) Grad: 89293.5703  LR: 0.000009  \n","Epoch: [4][400/3575] Elapsed 1m 36s (remain 12m 44s) Loss: 0.0000(0.0048) Grad: 29.8153  LR: 0.000008  \n","Epoch: [4][500/3575] Elapsed 1m 58s (remain 12m 8s) Loss: 0.0246(0.0045) Grad: 11061.3730  LR: 0.000008  \n","Epoch: [4][600/3575] Elapsed 2m 20s (remain 11m 36s) Loss: 0.0037(0.0042) Grad: 3524.1514  LR: 0.000008  \n","Epoch: [4][700/3575] Elapsed 2m 42s (remain 11m 7s) Loss: 0.0001(0.0041) Grad: 167.0514  LR: 0.000008  \n","Epoch: [4][800/3575] Elapsed 3m 5s (remain 10m 42s) Loss: 0.0156(0.0042) Grad: 8507.7520  LR: 0.000008  \n","Epoch: [4][900/3575] Elapsed 3m 27s (remain 10m 16s) Loss: 0.0000(0.0043) Grad: 20.4845  LR: 0.000008  \n","Epoch: [4][1000/3575] Elapsed 3m 50s (remain 9m 51s) Loss: 0.0994(0.0044) Grad: 40807.1914  LR: 0.000008  \n","Epoch: [4][1100/3575] Elapsed 4m 12s (remain 9m 27s) Loss: 0.0001(0.0042) Grad: 289.6081  LR: 0.000008  \n","Epoch: [4][1200/3575] Elapsed 4m 34s (remain 9m 2s) Loss: 0.0004(0.0043) Grad: 1385.0112  LR: 0.000007  \n","Epoch: [4][1300/3575] Elapsed 4m 57s (remain 8m 39s) Loss: 0.0002(0.0041) Grad: 656.7980  LR: 0.000007  \n","Epoch: [4][1400/3575] Elapsed 5m 19s (remain 8m 15s) Loss: 0.0071(0.0041) Grad: 12479.9766  LR: 0.000007  \n","Epoch: [4][1500/3575] Elapsed 5m 41s (remain 7m 52s) Loss: 0.0000(0.0041) Grad: 18.0274  LR: 0.000007  \n","Epoch: [4][1600/3575] Elapsed 6m 4s (remain 7m 28s) Loss: 0.0001(0.0042) Grad: 294.1326  LR: 0.000007  \n","Epoch: [4][1700/3575] Elapsed 6m 26s (remain 7m 5s) Loss: 0.0001(0.0042) Grad: 188.7581  LR: 0.000007  \n","Epoch: [4][1800/3575] Elapsed 6m 48s (remain 6m 42s) Loss: 0.0004(0.0041) Grad: 1624.6785  LR: 0.000007  \n","Epoch: [4][1900/3575] Elapsed 7m 10s (remain 6m 19s) Loss: 0.0147(0.0041) Grad: 16457.1719  LR: 0.000007  \n","Epoch: [4][2000/3575] Elapsed 7m 33s (remain 5m 56s) Loss: 0.0000(0.0041) Grad: 17.9583  LR: 0.000006  \n","Epoch: [4][2100/3575] Elapsed 7m 56s (remain 5m 34s) Loss: 0.0002(0.0042) Grad: 428.4840  LR: 0.000006  \n","Epoch: [4][2200/3575] Elapsed 8m 18s (remain 5m 11s) Loss: 0.0000(0.0041) Grad: 72.3566  LR: 0.000006  \n","Epoch: [4][2300/3575] Elapsed 8m 41s (remain 4m 48s) Loss: 0.0039(0.0041) Grad: 7507.9287  LR: 0.000006  \n","Epoch: [4][2400/3575] Elapsed 9m 3s (remain 4m 25s) Loss: 0.0007(0.0042) Grad: 2151.6023  LR: 0.000006  \n","Epoch: [4][2500/3575] Elapsed 9m 25s (remain 4m 2s) Loss: 0.0233(0.0042) Grad: 40881.3203  LR: 0.000006  \n","Epoch: [4][2600/3575] Elapsed 9m 48s (remain 3m 40s) Loss: 0.0076(0.0042) Grad: 12403.0547  LR: 0.000006  \n","Epoch: [4][2700/3575] Elapsed 10m 10s (remain 3m 17s) Loss: 0.0003(0.0042) Grad: 531.8594  LR: 0.000006  \n","Epoch: [4][2800/3575] Elapsed 10m 32s (remain 2m 54s) Loss: 0.0176(0.0042) Grad: 12958.3828  LR: 0.000005  \n","Epoch: [4][2900/3575] Elapsed 10m 54s (remain 2m 32s) Loss: 0.0000(0.0042) Grad: 106.8521  LR: 0.000005  \n","Epoch: [4][3000/3575] Elapsed 11m 17s (remain 2m 9s) Loss: 0.0000(0.0042) Grad: 197.7240  LR: 0.000005  \n","Epoch: [4][3100/3575] Elapsed 11m 39s (remain 1m 46s) Loss: 0.0000(0.0042) Grad: 7.5593  LR: 0.000005  \n","Epoch: [4][3200/3575] Elapsed 12m 2s (remain 1m 24s) Loss: 0.0002(0.0042) Grad: 716.7931  LR: 0.000005  \n","Epoch: [4][3300/3575] Elapsed 12m 24s (remain 1m 1s) Loss: 0.0000(0.0042) Grad: 25.9208  LR: 0.000005  \n","Epoch: [4][3400/3575] Elapsed 12m 46s (remain 0m 39s) Loss: 0.0025(0.0041) Grad: 12341.1826  LR: 0.000005  \n","Epoch: [4][3500/3575] Elapsed 13m 9s (remain 0m 16s) Loss: 0.0054(0.0042) Grad: 7552.3228  LR: 0.000005  \n","Epoch: [4][3574/3575] Elapsed 13m 25s (remain 0m 0s) Loss: 0.0007(0.0042) Grad: 2581.2212  LR: 0.000004  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 7m 56s) Loss: 0.0002(0.0002) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 6s) Loss: 0.0318(0.0063) \n","EVAL: [200/1192] Elapsed 0m 22s (remain 1m 52s) Loss: 0.0024(0.0059) \n","EVAL: [300/1192] Elapsed 0m 33s (remain 1m 40s) Loss: 0.0102(0.0065) \n","EVAL: [400/1192] Elapsed 0m 45s (remain 1m 28s) Loss: 0.0000(0.0063) \n","EVAL: [500/1192] Elapsed 0m 56s (remain 1m 17s) Loss: 0.0186(0.0062) \n","EVAL: [600/1192] Elapsed 1m 7s (remain 1m 6s) Loss: 0.0057(0.0067) \n","EVAL: [700/1192] Elapsed 1m 18s (remain 0m 55s) Loss: 0.0053(0.0075) \n","EVAL: [800/1192] Elapsed 1m 30s (remain 0m 44s) Loss: 0.0074(0.0074) \n","EVAL: [900/1192] Elapsed 1m 41s (remain 0m 32s) Loss: 0.0067(0.0074) \n","EVAL: [1000/1192] Elapsed 1m 52s (remain 0m 21s) Loss: 0.0001(0.0071) \n","EVAL: [1100/1192] Elapsed 2m 4s (remain 0m 10s) Loss: 0.0156(0.0068) \n","EVAL: [1191/1192] Elapsed 2m 14s (remain 0m 0s) Loss: 0.0000(0.0067) \n","Epoch 4 - avg_train_loss: 0.0042  avg_val_loss: 0.0067  time: 945s\n","Epoch 4 - Score: 0.8820\n","Epoch: [5][0/3575] Elapsed 0m 0s (remain 31m 5s) Loss: 0.0001(0.0001) Grad: 1535.8341  LR: 0.000004  \n","Epoch: [5][100/3575] Elapsed 0m 22s (remain 13m 4s) Loss: 0.0154(0.0045) Grad: 50999.7695  LR: 0.000004  \n","Epoch: [5][200/3575] Elapsed 0m 45s (remain 12m 38s) Loss: 0.0000(0.0034) Grad: 60.9114  LR: 0.000004  \n","Epoch: [5][300/3575] Elapsed 1m 7s (remain 12m 12s) Loss: 0.0000(0.0033) Grad: 53.6846  LR: 0.000004  \n","Epoch: [5][400/3575] Elapsed 1m 29s (remain 11m 48s) Loss: 0.0001(0.0033) Grad: 285.6170  LR: 0.000004  \n","Epoch: [5][500/3575] Elapsed 1m 51s (remain 11m 26s) Loss: 0.0005(0.0035) Grad: 2905.2104  LR: 0.000004  \n","Epoch: [5][600/3575] Elapsed 2m 14s (remain 11m 4s) Loss: 0.0083(0.0037) Grad: 22952.2617  LR: 0.000004  \n","Epoch: [5][700/3575] Elapsed 2m 36s (remain 10m 42s) Loss: 0.0084(0.0037) Grad: 5868.0703  LR: 0.000004  \n","Epoch: [5][800/3575] Elapsed 2m 59s (remain 10m 20s) Loss: 0.0049(0.0036) Grad: 13464.9004  LR: 0.000003  \n","Epoch: [5][900/3575] Elapsed 3m 21s (remain 9m 58s) Loss: 0.0000(0.0036) Grad: 58.0413  LR: 0.000003  \n","Epoch: [5][1000/3575] Elapsed 3m 44s (remain 9m 36s) Loss: 0.0000(0.0036) Grad: 52.2855  LR: 0.000003  \n","Epoch: [5][1100/3575] Elapsed 4m 6s (remain 9m 14s) Loss: 0.0002(0.0037) Grad: 918.8483  LR: 0.000003  \n","Epoch: [5][1200/3575] Elapsed 4m 29s (remain 8m 52s) Loss: 0.0001(0.0037) Grad: 240.0323  LR: 0.000003  \n","Epoch: [5][1300/3575] Elapsed 4m 51s (remain 8m 29s) Loss: 0.0000(0.0037) Grad: 10.8409  LR: 0.000003  \n","Epoch: [5][1400/3575] Elapsed 5m 13s (remain 8m 6s) Loss: 0.0000(0.0036) Grad: 30.2222  LR: 0.000003  \n","Epoch: [5][1500/3575] Elapsed 5m 36s (remain 7m 44s) Loss: 0.0296(0.0036) Grad: 35424.7773  LR: 0.000003  \n","Epoch: [5][1600/3575] Elapsed 5m 58s (remain 7m 21s) Loss: 0.0002(0.0035) Grad: 1043.9078  LR: 0.000002  \n","Epoch: [5][1700/3575] Elapsed 6m 20s (remain 6m 59s) Loss: 0.0000(0.0035) Grad: 119.0529  LR: 0.000002  \n","Epoch: [5][1800/3575] Elapsed 6m 43s (remain 6m 37s) Loss: 0.0027(0.0034) Grad: 19207.4160  LR: 0.000002  \n","Epoch: [5][1900/3575] Elapsed 7m 5s (remain 6m 14s) Loss: 0.0057(0.0035) Grad: 76260.8438  LR: 0.000002  \n","Epoch: [5][2000/3575] Elapsed 7m 28s (remain 5m 52s) Loss: 0.0010(0.0035) Grad: 8014.0518  LR: 0.000002  \n","Epoch: [5][2100/3575] Elapsed 7m 50s (remain 5m 30s) Loss: 0.0000(0.0035) Grad: 198.0287  LR: 0.000002  \n","Epoch: [5][2200/3575] Elapsed 8m 12s (remain 5m 7s) Loss: 0.0013(0.0035) Grad: 18336.1309  LR: 0.000002  \n","Epoch: [5][2300/3575] Elapsed 8m 34s (remain 4m 45s) Loss: 0.0050(0.0035) Grad: 30947.7480  LR: 0.000002  \n","Epoch: [5][2400/3575] Elapsed 8m 57s (remain 4m 22s) Loss: 0.0000(0.0035) Grad: 22.9705  LR: 0.000001  \n","Epoch: [5][2500/3575] Elapsed 9m 20s (remain 4m 0s) Loss: 0.0171(0.0035) Grad: 64233.7109  LR: 0.000001  \n","Epoch: [5][2600/3575] Elapsed 9m 43s (remain 3m 38s) Loss: 0.0000(0.0035) Grad: 28.5462  LR: 0.000001  \n","Epoch: [5][2700/3575] Elapsed 10m 5s (remain 3m 15s) Loss: 0.0000(0.0034) Grad: 34.6172  LR: 0.000001  \n","Epoch: [5][2800/3575] Elapsed 10m 27s (remain 2m 53s) Loss: 0.0028(0.0034) Grad: 7409.7305  LR: 0.000001  \n","Epoch: [5][2900/3575] Elapsed 10m 50s (remain 2m 31s) Loss: 0.0000(0.0034) Grad: 38.9269  LR: 0.000001  \n","Epoch: [5][3000/3575] Elapsed 11m 12s (remain 2m 8s) Loss: 0.0016(0.0034) Grad: 10788.9678  LR: 0.000001  \n","Epoch: [5][3100/3575] Elapsed 11m 34s (remain 1m 46s) Loss: 0.0000(0.0033) Grad: 6.6463  LR: 0.000001  \n","Epoch: [5][3200/3575] Elapsed 11m 56s (remain 1m 23s) Loss: 0.0000(0.0034) Grad: 68.3246  LR: 0.000000  \n","Epoch: [5][3300/3575] Elapsed 12m 19s (remain 1m 1s) Loss: 0.0000(0.0033) Grad: 35.3074  LR: 0.000000  \n","Epoch: [5][3400/3575] Elapsed 12m 41s (remain 0m 38s) Loss: 0.0000(0.0033) Grad: 141.1582  LR: 0.000000  \n","Epoch: [5][3500/3575] Elapsed 13m 3s (remain 0m 16s) Loss: 0.0003(0.0033) Grad: 2585.6687  LR: 0.000000  \n","Epoch: [5][3574/3575] Elapsed 13m 20s (remain 0m 0s) Loss: 0.0000(0.0033) Grad: 179.5471  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 13s) Loss: 0.0001(0.0001) \n","EVAL: [100/1192] Elapsed 0m 11s (remain 2m 7s) Loss: 0.0383(0.0083) \n","EVAL: [200/1192] Elapsed 0m 23s (remain 1m 54s) Loss: 0.0083(0.0078) \n","EVAL: [300/1192] Elapsed 0m 34s (remain 1m 42s) Loss: 0.0115(0.0083) \n","EVAL: [400/1192] Elapsed 0m 45s (remain 1m 30s) Loss: 0.0000(0.0082) \n","EVAL: [500/1192] Elapsed 0m 57s (remain 1m 18s) Loss: 0.0310(0.0080) \n","EVAL: [600/1192] Elapsed 1m 8s (remain 1m 7s) Loss: 0.0084(0.0085) \n","EVAL: [700/1192] Elapsed 1m 19s (remain 0m 55s) Loss: 0.0042(0.0093) \n","EVAL: [800/1192] Elapsed 1m 31s (remain 0m 44s) Loss: 0.0077(0.0092) \n","EVAL: [900/1192] Elapsed 1m 42s (remain 0m 33s) Loss: 0.0090(0.0094) \n","EVAL: [1000/1192] Elapsed 1m 53s (remain 0m 21s) Loss: 0.0000(0.0091) \n","EVAL: [1100/1192] Elapsed 2m 5s (remain 0m 10s) Loss: 0.0173(0.0088) \n","EVAL: [1191/1192] Elapsed 2m 15s (remain 0m 0s) Loss: 0.0000(0.0086) \n","Epoch 5 - avg_train_loss: 0.0033  avg_val_loss: 0.0086  time: 941s\n","Epoch 5 - Score: 0.8829\n","Best thres: 0.5, Score: 0.8800\n","Best thres: 0.4837890625, Score: 0.8803\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d94e37f798aa4a3fbbca8ad1ce9c7b17"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1964597bf33240f9922040375ddbaa0d"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19e3faa2fd19480dbac0c464cbdedd49"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63b6cb90f56a439ba2446054dc575f22"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n"]}],"source":["if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"name":"nbme-exp076.ipynb","provenance":[{"file_id":"1RX_ZvZAkBkJKpYjOf4JkbUw7fDIvFDvK","timestamp":1649247786944},{"file_id":"10yG4L3_nzpdL2CDwqxa9r-KWq6jYkWfl","timestamp":1649164439720}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"papermill":{"default_parameters":{},"duration":641.572862,"end_time":"2022-02-27T11:39:50.972497","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-02-27T11:29:09.399635","version":"2.3.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"da6c964c345a43a9ab76b5813da01b1e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3683782634ee41e7a4a85d4e2ed61f95","IPY_MODEL_73869eed7bbe4f208521f5b0bf3cb5b1","IPY_MODEL_2cb906b3ae0e44159bfde592839ba67f"],"layout":"IPY_MODEL_697f35237f474b90a43b352e7ca81fe7"}},"3683782634ee41e7a4a85d4e2ed61f95":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f0db979c013942868e2d13910af1220f","placeholder":"​","style":"IPY_MODEL_28522f44c6c04ac9849df660f1122ca2","value":"Downloading: 100%"}},"73869eed7bbe4f208521f5b0bf3cb5b1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c075db03d2b4453a1d00d65b6d9406d","max":52,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a2a5834c4acc4217bf009acb82d88ff1","value":52}},"2cb906b3ae0e44159bfde592839ba67f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4dcc4fd12bb64fdd85c4bf46c7e8c266","placeholder":"​","style":"IPY_MODEL_2ef15c50beab42ae957ebc62369a19bc","value":" 52.0/52.0 [00:00&lt;00:00, 1.21kB/s]"}},"697f35237f474b90a43b352e7ca81fe7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0db979c013942868e2d13910af1220f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28522f44c6c04ac9849df660f1122ca2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c075db03d2b4453a1d00d65b6d9406d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2a5834c4acc4217bf009acb82d88ff1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4dcc4fd12bb64fdd85c4bf46c7e8c266":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ef15c50beab42ae957ebc62369a19bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9ce266701cd64e95945a118ff843ad2b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9f9c397c3b224e26b26b805a52f90a41","IPY_MODEL_33b8c475a441496790ccb74967fef5a4","IPY_MODEL_89516a0ef1454248a52e63cddd42bdf1"],"layout":"IPY_MODEL_9faacaae71514d13ac83bdf4cc3e19f3"}},"9f9c397c3b224e26b26b805a52f90a41":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44f561c038794eba8014a20b1248ff1e","placeholder":"​","style":"IPY_MODEL_fd4e3e521b4d4f09bd77ae4da9de94ec","value":"Downloading: 100%"}},"33b8c475a441496790ccb74967fef5a4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe4c449ff77848a6b099b854c035f914","max":2464616,"min":0,"orientation":"horizontal","style":"IPY_MODEL_830e08bce77d4a0391c50ccdb72b9d2c","value":2464616}},"89516a0ef1454248a52e63cddd42bdf1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8b8a8f19f81410fb45f8fb82666599b","placeholder":"​","style":"IPY_MODEL_c066b333d7c6492698ae623c24acd992","value":" 2.35M/2.35M [00:00&lt;00:00, 3.87MB/s]"}},"9faacaae71514d13ac83bdf4cc3e19f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44f561c038794eba8014a20b1248ff1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd4e3e521b4d4f09bd77ae4da9de94ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe4c449ff77848a6b099b854c035f914":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"830e08bce77d4a0391c50ccdb72b9d2c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f8b8a8f19f81410fb45f8fb82666599b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c066b333d7c6492698ae623c24acd992":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e58e4ca26dd44363b2cfd30ab7226b46":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d59267f7193e4c13968f5dc31c23a2fa","IPY_MODEL_0de1f93124684f2fa60dae8025fca5f5","IPY_MODEL_230608f3d4264538a802fefa037c9a10"],"layout":"IPY_MODEL_02769f76ae644caeacbc07e0c653a941"}},"d59267f7193e4c13968f5dc31c23a2fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfa6da5eaa9740aa82e24027d1daf982","placeholder":"​","style":"IPY_MODEL_bde9113ddb5b4d73a287721cc1b77b0b","value":"Downloading: 100%"}},"0de1f93124684f2fa60dae8025fca5f5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_afc32c01fb6b4791ad447ea61d8d3687","max":580,"min":0,"orientation":"horizontal","style":"IPY_MODEL_310d0cf6ca82493c840367ee6ee1ae08","value":580}},"230608f3d4264538a802fefa037c9a10":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4cc22905bda42e6910973d2063ecc99","placeholder":"​","style":"IPY_MODEL_6c548c3d48f14bcaa14e286c2320a41e","value":" 580/580 [00:00&lt;00:00, 10.3kB/s]"}},"02769f76ae644caeacbc07e0c653a941":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfa6da5eaa9740aa82e24027d1daf982":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bde9113ddb5b4d73a287721cc1b77b0b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"afc32c01fb6b4791ad447ea61d8d3687":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"310d0cf6ca82493c840367ee6ee1ae08":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c4cc22905bda42e6910973d2063ecc99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c548c3d48f14bcaa14e286c2320a41e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f5913793c8364d2e88e39c6d72390b6e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_260fd91bab5242659bb278b4f4803fac","IPY_MODEL_2ce1d89c470c49c395a6f08346fabbce","IPY_MODEL_65d4f8d31b434b8096e70a1b0e3349da"],"layout":"IPY_MODEL_91ee8cabfde54f53ba7e54b813049280"}},"260fd91bab5242659bb278b4f4803fac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_730c1742b14549fb86a8d75e20eaf7a7","placeholder":"​","style":"IPY_MODEL_ace17e186a264496b42db32254d5835f","value":"100%"}},"2ce1d89c470c49c395a6f08346fabbce":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_46b85fcfad194020aa30273407f6696a","max":42146,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b3e88277a95149e6b3d0fa2cc6718a60","value":42146}},"65d4f8d31b434b8096e70a1b0e3349da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1e8b3baba674ace892834e95fcc6798","placeholder":"​","style":"IPY_MODEL_468d683fd55040c1a8bf66280a8ba517","value":" 42146/42146 [00:34&lt;00:00, 1948.37it/s]"}},"91ee8cabfde54f53ba7e54b813049280":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"730c1742b14549fb86a8d75e20eaf7a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ace17e186a264496b42db32254d5835f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46b85fcfad194020aa30273407f6696a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3e88277a95149e6b3d0fa2cc6718a60":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d1e8b3baba674ace892834e95fcc6798":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"468d683fd55040c1a8bf66280a8ba517":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72bacbcffef34cb3accb06920129139e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bade747ff6164db292889a7e4449dea2","IPY_MODEL_2ba3f42caea248c7aea1d0e4beae4d9e","IPY_MODEL_589a6d6ca15b4e0dbf5fa8293b5ea21a"],"layout":"IPY_MODEL_439808b9845a4ff9be6791ccbbec37f3"}},"bade747ff6164db292889a7e4449dea2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eac296db92804f4a924e2e443729d8a4","placeholder":"​","style":"IPY_MODEL_f7caf05fba964ad5a42a2fd6a3d23895","value":"100%"}},"2ba3f42caea248c7aea1d0e4beae4d9e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c21524dcf08c4644be92d7d21f783e54","max":143,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f8f30ef643bf47b59952bbc06f082782","value":143}},"589a6d6ca15b4e0dbf5fa8293b5ea21a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dffe673347a54dddb4d430071c15bd57","placeholder":"​","style":"IPY_MODEL_2c496a81d245472e98d63b5086714d21","value":" 143/143 [00:00&lt;00:00, 2503.52it/s]"}},"439808b9845a4ff9be6791ccbbec37f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eac296db92804f4a924e2e443729d8a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7caf05fba964ad5a42a2fd6a3d23895":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c21524dcf08c4644be92d7d21f783e54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8f30ef643bf47b59952bbc06f082782":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dffe673347a54dddb4d430071c15bd57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c496a81d245472e98d63b5086714d21":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e667b88bf635469cb1bb33a2f16b47d7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_630ddeaba5074e83acd2a71fbf0764c6","IPY_MODEL_0655b1f6b47a4033a6b81b9fe8df1f84","IPY_MODEL_b19bc8864c214e1ba648d1da78907107"],"layout":"IPY_MODEL_5282a4fd8bf8478d81a2ac1b6c91a6f1"}},"630ddeaba5074e83acd2a71fbf0764c6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1ffc87e487a4337a9d6a8924321ccea","placeholder":"​","style":"IPY_MODEL_015a6285c82a4711a668cbec66f0dc57","value":"Downloading: 100%"}},"0655b1f6b47a4033a6b81b9fe8df1f84":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8330119346a34a42a98252aeed3d01a4","max":873673253,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c2a998ebc00641cda55ec9669e821f11","value":873673253}},"b19bc8864c214e1ba648d1da78907107":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8d99a2cdfc54341ac348ba6a945db13","placeholder":"​","style":"IPY_MODEL_7b843aaec3a0468298200afe363fe370","value":" 833M/833M [00:16&lt;00:00, 53.8MB/s]"}},"5282a4fd8bf8478d81a2ac1b6c91a6f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1ffc87e487a4337a9d6a8924321ccea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"015a6285c82a4711a668cbec66f0dc57":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8330119346a34a42a98252aeed3d01a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2a998ebc00641cda55ec9669e821f11":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b8d99a2cdfc54341ac348ba6a945db13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b843aaec3a0468298200afe363fe370":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d94e37f798aa4a3fbbca8ad1ce9c7b17":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9f65d5dc5d664029b90de005149f4da9","IPY_MODEL_b710be4e13354f76bf44d4401a1171e0","IPY_MODEL_764cac9abfd54252bb89c4c7c14d9c9a"],"layout":"IPY_MODEL_d8f66aeeb36f4635a9e67b5382461855"}},"9f65d5dc5d664029b90de005149f4da9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7768c4c6e4004f018aa08eb770757742","placeholder":"​","style":"IPY_MODEL_05fa8478577a45ff8410c3157f09408e","value":"100%"}},"b710be4e13354f76bf44d4401a1171e0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_48b16a9d04b84c2887b4be3c4383e3da","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f108d69cf6694ee496e71f7a8c0be61e","value":2}},"764cac9abfd54252bb89c4c7c14d9c9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02e2f16e6e8b4488a258d1c1a258838e","placeholder":"​","style":"IPY_MODEL_faf4ef578ff44c758e60514fad54ff6b","value":" 2/2 [00:01&lt;00:00,  1.81it/s]"}},"d8f66aeeb36f4635a9e67b5382461855":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7768c4c6e4004f018aa08eb770757742":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05fa8478577a45ff8410c3157f09408e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"48b16a9d04b84c2887b4be3c4383e3da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f108d69cf6694ee496e71f7a8c0be61e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"02e2f16e6e8b4488a258d1c1a258838e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"faf4ef578ff44c758e60514fad54ff6b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1964597bf33240f9922040375ddbaa0d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_72e58439a4844bffb9356278dabb29b3","IPY_MODEL_2f721aed13db457aba058392f705d61f","IPY_MODEL_a37104e5ed7145ff96bbae8c50f57b68"],"layout":"IPY_MODEL_7980c58dbe40490f9f21a324b2341103"}},"72e58439a4844bffb9356278dabb29b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_710aa1157dc4434fb71cf495d0487aa4","placeholder":"​","style":"IPY_MODEL_33a0bbe43d7c48ac944051b2694f3059","value":"100%"}},"2f721aed13db457aba058392f705d61f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_342054a6d1f84877ab79b38816678015","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7cb8cac105174f5d832800d2f58016cc","value":2}},"a37104e5ed7145ff96bbae8c50f57b68":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa9f62a520254ff9a50a3edef12dad43","placeholder":"​","style":"IPY_MODEL_e4e37996a7ff4fca9f3d6df949460290","value":" 2/2 [00:01&lt;00:00,  1.34s/it]"}},"7980c58dbe40490f9f21a324b2341103":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"710aa1157dc4434fb71cf495d0487aa4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33a0bbe43d7c48ac944051b2694f3059":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"342054a6d1f84877ab79b38816678015":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cb8cac105174f5d832800d2f58016cc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fa9f62a520254ff9a50a3edef12dad43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4e37996a7ff4fca9f3d6df949460290":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"19e3faa2fd19480dbac0c464cbdedd49":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0080d0e9ec74475cbff084bae4c9dc01","IPY_MODEL_ae164b8f00f24ef68e49001b625b04b3","IPY_MODEL_feb44c53263c4e5d81e4f6da44e3ebbc"],"layout":"IPY_MODEL_b253b6f5ba1f46fb822e612357c15b0b"}},"0080d0e9ec74475cbff084bae4c9dc01":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b85dedce6e240fd8d9b93e47f001eab","placeholder":"​","style":"IPY_MODEL_6a48a45186e44af2bf77050e27cabc14","value":"100%"}},"ae164b8f00f24ef68e49001b625b04b3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f86a578350774fbd9a93f1bdfc9cf94e","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c2d6039b921f48f89066a8c427765a89","value":2}},"feb44c53263c4e5d81e4f6da44e3ebbc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c61ab375b0d4fa7933fc51d9a622ef5","placeholder":"​","style":"IPY_MODEL_55a20a14af4b44fbaf735a1212c66b37","value":" 2/2 [00:02&lt;00:00,  1.50it/s]"}},"b253b6f5ba1f46fb822e612357c15b0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b85dedce6e240fd8d9b93e47f001eab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a48a45186e44af2bf77050e27cabc14":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f86a578350774fbd9a93f1bdfc9cf94e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2d6039b921f48f89066a8c427765a89":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5c61ab375b0d4fa7933fc51d9a622ef5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55a20a14af4b44fbaf735a1212c66b37":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"63b6cb90f56a439ba2446054dc575f22":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9e7d8d57ed9f45409a189d693aa1589c","IPY_MODEL_396a1f40091a4931bb2068e56c2c625f","IPY_MODEL_ce0b11ae3be6428c97fce3ed847ce9ca"],"layout":"IPY_MODEL_e13b7aed842d4247bee36b9980ff6b45"}},"9e7d8d57ed9f45409a189d693aa1589c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_574e390e037e4f65bfddf30a49457102","placeholder":"​","style":"IPY_MODEL_42e06f7f8c3b42cf97a1fce36cdf873e","value":"100%"}},"396a1f40091a4931bb2068e56c2c625f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f4130b7420e45bdb143e569e988fba4","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b21886ca05e44453907225be7161fef6","value":2}},"ce0b11ae3be6428c97fce3ed847ce9ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_21b0a0410e2e4b66afa7e47784bffdcd","placeholder":"​","style":"IPY_MODEL_c5afb03f9f9b4e509b2d4a360d765b95","value":" 2/2 [00:02&lt;00:00,  1.68s/it]"}},"e13b7aed842d4247bee36b9980ff6b45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"574e390e037e4f65bfddf30a49457102":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42e06f7f8c3b42cf97a1fce36cdf873e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f4130b7420e45bdb143e569e988fba4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b21886ca05e44453907225be7161fef6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"21b0a0410e2e4b66afa7e47784bffdcd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5afb03f9f9b4e509b2d4a360d765b95":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}