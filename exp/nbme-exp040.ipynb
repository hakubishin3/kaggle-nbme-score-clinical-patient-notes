{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bound-omega",
   "metadata": {
    "id": "aa1f8e80"
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-prediction",
   "metadata": {
    "id": "c0138fac"
   },
   "source": [
    "- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-receiver",
   "metadata": {
    "id": "cf1dfda9"
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "robust-tuner",
   "metadata": {
    "id": "a7a78d25"
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"nbme-exp040\"\n",
    "ENV = \"local\"\n",
    "DEBUG_MODE = False\n",
    "SUBMISSION_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accessory-upgrade",
   "metadata": {
    "id": "4ecc4e4d"
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    env=ENV\n",
    "    exp_name=EXP_NAME\n",
    "    debug=DEBUG_MODE\n",
    "    submission=SUBMISSION_MODE\n",
    "    apex=True\n",
    "    input_dir=None\n",
    "    output_dir=None\n",
    "    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n",
    "    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n",
    "    competition_name=\"nbme-score-clinical-patient-notes\"\n",
    "    id_col=\"id\"\n",
    "    target_col=\"location\"\n",
    "    pretrained_model_name=\"microsoft/deberta-large\"\n",
    "    tokenizer=None\n",
    "    max_len=None\n",
    "    output_dim=1\n",
    "    dropout=0.2\n",
    "    num_workers=4\n",
    "    batch_size=4\n",
    "    lr=2e-5\n",
    "    betas=(0.9, 0.98)\n",
    "    weight_decay=0.1\n",
    "    num_warmup_steps_rate=0.1\n",
    "    batch_scheduler=True\n",
    "    epochs=5\n",
    "    n_fold=5\n",
    "    train_fold=[0, 1, 2, 3, 4]\n",
    "    seed=71\n",
    "    gradient_accumulation_steps=2\n",
    "    max_grad_norm=1000\n",
    "    print_freq=100\n",
    "    train=True\n",
    "    inference=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "early-prior",
   "metadata": {
    "id": "3894c88b"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.train_fold = [0, 1]\n",
    "\n",
    "if CFG.submission:\n",
    "    CFG.train = False\n",
    "    CFG.inference = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-opposition",
   "metadata": {
    "id": "31768c85"
   },
   "source": [
    "## Directory Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "demonstrated-teaching",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4693,
     "status": "ok",
     "timestamp": 1646023773081,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "00e7d967",
    "outputId": "d56a483d-9171-44e6-856a-a90dfe8e0ac3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "print(CFG.env)\n",
    "if CFG.env == \"colab\":\n",
    "    # colab環境\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "    # install packages\n",
    "    !pip install transformers\n",
    "\n",
    "elif CFG.env == \"local\":\n",
    "    # ローカルサーバ\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "\n",
    "elif CFG.env == \"kaggle\":\n",
    "    # kaggle環境\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bizarre-brake",
   "metadata": {
    "id": "d726b7d9"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import BartModel,BertModel,BertTokenizer\n",
    "from transformers import DebertaModel,DebertaTokenizer\n",
    "from transformers import RobertaModel,RobertaTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-poster",
   "metadata": {
    "id": "b6d82f71"
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "varied-comedy",
   "metadata": {
    "id": "95abbe2c"
   },
   "outputs": [],
   "source": [
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "written-syntax",
   "metadata": {
    "id": "832ee36d"
   },
   "outputs": [],
   "source": [
    "def create_labels_for_scoring(df):\n",
    "    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n",
    "    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, \"location\"]\n",
    "        if lst:\n",
    "            new_lst = \";\".join(lst)\n",
    "            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n",
    "\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df[\"location_for_create_labels\"].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, token_probs, tokenizer):\n",
    "    res = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n",
    "        encoded = tokenizer(\n",
    "            text=text,\n",
    "            max_length=CFG.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n",
    "            start, end = offset_mapping\n",
    "            res[i][start:end] = pred\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_predicted_location_str(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def scoring(df, th=0.5):\n",
    "    labels = create_labels_for_scoring(df)\n",
    "\n",
    "    token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n",
    "    char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n",
    "    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n",
    "    preds = get_predictions(predicted_location_str)\n",
    "\n",
    "    score = get_score(labels, preds)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_best_thres(oof_df):\n",
    "    def f1_opt(x):\n",
    "        return -1 * scoring(oof_df, th=x)\n",
    "\n",
    "    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n",
    "    return best_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "legal-samba",
   "metadata": {
    "id": "918828a7"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "computational-wages",
   "metadata": {
    "id": "d02a78e1"
   },
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-former",
   "metadata": {
    "id": "47266f39"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "prompt-homework",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 815,
     "status": "ok",
     "timestamp": 1646023777557,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "20fed6da",
    "outputId": "64d3e7ad-0986-4799-f9df-f0242c1977a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 6), (143, 3), (42146, 3), (5, 4))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(CFG.input_dir / \"train.csv\")\n",
    "features = pd.read_csv(CFG.input_dir / \"features.csv\")\n",
    "patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n",
    "test = pd.read_csv(CFG.input_dir / \"test.csv\")\n",
    "\n",
    "train.shape, features.shape, patient_notes.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "married-argument",
   "metadata": {
    "id": "e67d0132"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n",
    "    print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-contamination",
   "metadata": {
    "id": "47bca11a"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "armed-insulin",
   "metadata": {
    "id": "d9c8e9ba"
   },
   "outputs": [],
   "source": [
    "def preprocess_features(features):\n",
    "    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "\n",
    "\n",
    "features = preprocess_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "nervous-reasoning",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1646023777558,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "7ef41e18",
    "outputId": "31edaa7d-c088-495a-95ee-f0d56f97074c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 8), (5, 6))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "interested-vaccine",
   "metadata": {
    "id": "8233df16"
   },
   "outputs": [],
   "source": [
    "train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n",
    "train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "guided-denmark",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1646023778018,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "e9143e61",
    "outputId": "cf45e2d7-5f66-4d96-c6e2-da79c888bcc8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4399\n",
       "1    8181\n",
       "2    1296\n",
       "3     287\n",
       "4      99\n",
       "5      27\n",
       "6       9\n",
       "7       1\n",
       "8       1\n",
       "Name: annotation_length, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n",
    "display(train['annotation_length'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-patio",
   "metadata": {
    "id": "6bdc7949"
   },
   "source": [
    "## CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "stuck-consultancy",
   "metadata": {
    "id": "c4acf61d"
   },
   "outputs": [],
   "source": [
    "def get_groupkfold(df, group_name):\n",
    "    groups = df[group_name].unique()\n",
    "\n",
    "    kf = KFold(\n",
    "        n_splits=CFG.n_fold,\n",
    "        shuffle=True,\n",
    "        random_state=CFG.seed,\n",
    "    )\n",
    "    folds_ids = []\n",
    "    for i_fold, (_, val_group_idx) in enumerate(kf.split(groups)):\n",
    "        val_group = groups[val_group_idx]\n",
    "        is_val = df[group_name].isin(val_group)\n",
    "        val_idx = df[is_val].index\n",
    "        df.loc[val_idx, \"fold\"] = int(i_fold)\n",
    "\n",
    "    df[\"fold\"] = df[\"fold\"].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "numerous-posting",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1646023778018,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "2ca0c08e",
    "outputId": "cfc9c06e-e30c-4cb5-a072-d0cfcfa5fdc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    2902\n",
       "1    2894\n",
       "2    2813\n",
       "3    2791\n",
       "4    2900\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = get_groupkfold(train, \"pn_num\")\n",
    "display(train.groupby(\"fold\").size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-advocacy",
   "metadata": {
    "id": "a8560070"
   },
   "source": [
    "## Setup tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "reserved-shanghai",
   "metadata": {
    "id": "c316b13f"
   },
   "outputs": [],
   "source": [
    "if CFG.submission:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\", trim_offsets=False)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG.pretrained_model_name, trim_offsets=False)\n",
    "    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n",
    "\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "postal-storm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'', 0, 0\n",
      "'dad', 0, 3\n",
      "' with', 3, 8\n",
      "' recent', 8, 15\n",
      "' heart', 15, 21\n",
      "' attack', 21, 28\n",
      "'', 0, 0\n",
      "ans\n",
      "\n",
      "'', 0, 0\n",
      "'dad', 0, 3\n",
      "' with', 3, 8\n",
      "' recent', 8, 15\n",
      "' heart', 15, 21\n",
      "' attack', 21, 28\n",
      "'', 0, 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp = 'dad with recent heart attack'\n",
    "encode = tokenizer(tmp, return_offsets_mapping=True)\n",
    "for (start,end) in encode['offset_mapping']:\n",
    "    print(f\"'{tmp[start:end]}', {start}, {end}\")\n",
    "\n",
    "print(\"ans\")\n",
    "print(\"\"\"\n",
    "'', 0, 0\n",
    "'dad', 0, 3\n",
    "' with', 3, 8\n",
    "' recent', 8, 15\n",
    "' heart', 15, 21\n",
    "' attack', 21, 28\n",
    "'', 0, 0\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-bibliography",
   "metadata": {
    "id": "e689a7fc"
   },
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bigger-moscow",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "a31c60ff4dab48e08d2ef9293d85df6d",
      "c40d970496ff447a8c0b80d787b07a4d",
      "40e6583408c447199ff5b94d23601936",
      "1141ae38bc6f473aab89db14fa4eeacf",
      "47b8a7f3d0544d79b30ad02e4222082e",
      "0260998578564385a0b5b9425a0a5ca1",
      "428ca357bd284d199e2558b1f577d79a",
      "2e32fee744ef42e0aaa89a7b03e82427",
      "d51d3aa414db4aa8b0ccae896e671152",
      "ad49cbf6b6e84ccaab873458182f22a1",
      "5375de82ce3a41a8b5550e0a6b4316c1"
     ]
    },
    "executionInfo": {
     "elapsed": 37449,
     "status": "ok",
     "timestamp": 1646023819498,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "df31758e",
    "outputId": "e3ee6910-2896-413b-9bb7-1e1dd630166c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b76bcfcb8014dcfa904b54f006ab092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 433\n"
     ]
    }
   ],
   "source": [
    "pn_history_lengths = []\n",
    "tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    pn_history_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(pn_history_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dominican-intervention",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "8a503d1abd884514a1e23101e03c6781",
      "e06e5e9eb0414b6fad63bdc99b44a313",
      "6b04b019813e458080f02bc9111433a6",
      "2e3818222bab4603a896be5976cb8409",
      "eeb468dbb94943fcb30219d4dd98fcab",
      "5e66444e9c714134bd2765cb3b6d1f15",
      "220f78b6119042af8729543465e1234e",
      "7f1d7796e2174485a0d1b1e9a71d7ade",
      "e72cad76f875451a8e2479e2df237575",
      "9754a5f1e61d49c8972df40ee9290375",
      "25bf78e432e641e0a435dc3626c3ee8a"
     ]
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1646023819500,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "3caff24a",
    "outputId": "09841871-9f3a-4e70-a528-07122a0ebba2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d86830b0b74199932b3539b2296925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 30\n"
     ]
    }
   ],
   "source": [
    "feature_text_lengths = []\n",
    "tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    feature_text_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(feature_text_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "random-address",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1646023819500,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "756d83ff",
    "outputId": "02d1e748-4ce8-4d68-f2a2-175f08316e9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 466\n"
     ]
    }
   ],
   "source": [
    "CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n",
    "\n",
    "print(\"max length:\", CFG.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "under-arrow",
   "metadata": {
    "id": "054b899a"
   },
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "        self.annotation_lengths = self.df[\"annotation_length\"].values\n",
    "        self.locations = self.df[\"location\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def _create_label(self, pn_history, annotation_length, location_list):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        offset_mapping = encoded[\"offset_mapping\"]\n",
    "        ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n",
    "        label = np.zeros(len(offset_mapping))\n",
    "        label[ignore_idxes] = -1\n",
    "\n",
    "        if annotation_length > 0:\n",
    "            for location in location_list:\n",
    "                for loc in [s.split() for s in location.split(\";\")]:\n",
    "                    start, end = int(loc[0]), int(loc[1])\n",
    "                    start_idx = -1\n",
    "                    end_idx = -1\n",
    "                    for idx in range(len(offset_mapping)):\n",
    "                        if (start_idx == -1) & (start < offset_mapping[idx][0]):\n",
    "                            start_idx = idx - 1\n",
    "                        if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n",
    "                            end_idx = idx + 1\n",
    "                    if start_idx == -1:\n",
    "                        start_idx = end_idx\n",
    "                    if (start_idx != -1) & (end_idx != -1):\n",
    "                        label[start_idx:end_idx] = 1\n",
    "\n",
    "        return torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n",
    "        return input_, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acquired-winner",
   "metadata": {
    "id": "1d58367c"
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        return input_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-defense",
   "metadata": {
    "id": "8c57abef"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "seeing-heater",
   "metadata": {
    "id": "54f92d89"
   },
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, model_config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if model_config_path is None:\n",
    "            self.model_config = AutoConfig.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "        else:\n",
    "            self.model_config = torch.load(model_config_path)\n",
    "\n",
    "        if pretrained:\n",
    "            self.backbone = AutoModel.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                config=self.model_config,\n",
    "            )\n",
    "            print(f\"Load weight from pretrained\")\n",
    "        else:\n",
    "            #self.backbone = AutoModel.from_config(self.model_config)\n",
    "            itpt = AutoModelForMaskedLM.from_config(self.model_config)\n",
    "            #path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp009/checkpoint-129000/pytorch_model.bin\")\n",
    "            path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n",
    "            state_dict = torch.load(path)\n",
    "            itpt.load_state_dict(state_dict)\n",
    "            self.backbone = itpt.deberta\n",
    "            print(f\"Load weight from {path}\")\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(self.cfg.dropout),\n",
    "            nn.Linear(self.model_config.hidden_size * 4, self.cfg.output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h = self.backbone(**inputs)[\"hidden_states\"]\n",
    "        h = torch.cat([h[-1*i][:, :] for i in range(1, 4 + 1)], dim=2)  # concatenate\n",
    "        output = self.fc(h)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-tuning",
   "metadata": {
    "id": "91401041"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "existing-details",
   "metadata": {
    "id": "eda8175d"
   },
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    train_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    epoch,\n",
    "    scheduler,\n",
    "    device,\n",
    "):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels) in enumerate(train_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            output = model(inputs)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "\n",
    "        pos_nums = (labels == 1).sum(axis=1)\n",
    "        pos_nums = torch.tensor([[pos_num] * CFG.max_len for pos_num in pos_nums]).view(-1, 1).to(device)\n",
    "        pos_nums = torch.masked_select(pos_nums, labels.view(-1, 1) != -1)\n",
    "        weight = []\n",
    "        for pos_num in pos_nums:\n",
    "            if pos_num == 0:\n",
    "                weight.append(3.0)\n",
    "            else:\n",
    "                weight.append(1.0)\n",
    "        weight = torch.tensor(weight).to(device)\n",
    "        loss = loss * weight\n",
    "\n",
    "        loss = loss.mean()\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if CFG.batch_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n",
    "            print(\n",
    "                \"Epoch: [{0}][{1}/{2}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                \"Grad: {grad_norm:.4f}  \"\n",
    "                \"LR: {lr:.6f}  \"\n",
    "                .format(\n",
    "                    epoch+1,\n",
    "                    step,\n",
    "                    len(train_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n",
    "                    loss=losses,\n",
    "                     grad_norm=grad_norm,\n",
    "                     lr=scheduler.get_lr()[0],\n",
    "                )\n",
    "            )\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "touched-person",
   "metadata": {
    "id": "c44b63a7"
   },
   "outputs": [],
   "source": [
    "def valid_fn(\n",
    "    val_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    device,\n",
    "):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels) in enumerate(val_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "\n",
    "        pos_nums = (labels == 1).sum(axis=1)\n",
    "        pos_nums = torch.tensor([[pos_num] * CFG.max_len for pos_num in pos_nums]).view(-1, 1).to(device)\n",
    "        pos_nums = torch.masked_select(pos_nums, labels.view(-1, 1) != -1)\n",
    "        weight = []\n",
    "        for pos_num in pos_nums:\n",
    "            if pos_num == 0:\n",
    "                weight.append(3.0)\n",
    "            else:\n",
    "                weight.append(1.0)\n",
    "        weight = torch.tensor(weight).to(device)\n",
    "        loss = loss * weight\n",
    "\n",
    "        loss = loss.mean()\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n",
    "            print(\n",
    "                \"EVAL: [{0}/{1}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                .format(\n",
    "                    step, len(val_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n",
    "                    loss=losses,\n",
    "                )\n",
    "            )\n",
    "    preds = np.concatenate(preds)\n",
    "    return losses.avg, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "minimal-findings",
   "metadata": {
    "id": "4219ac38"
   },
   "outputs": [],
   "source": [
    "def inference_fn(test_dataloader, model, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preds = []\n",
    "    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "grave-contractor",
   "metadata": {
    "id": "014a76b7"
   },
   "outputs": [],
   "source": [
    "def train_loop(df, i_fold, device):\n",
    "    print(f\"========== fold: {i_fold} training ==========\")\n",
    "    train_idx = df[df[\"fold\"] != i_fold].index\n",
    "    val_idx = df[df[\"fold\"] == i_fold].index\n",
    "\n",
    "    train_folds = df.loc[train_idx].reset_index(drop=True)\n",
    "    val_folds = df.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = TrainingDataset(CFG, train_folds)\n",
    "    val_dataset = TrainingDataset(CFG, val_folds)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "    model = CustomModel(CFG, model_config_path=None, pretrained=False)   # itptを使うため\n",
    "    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n",
    "    model.to(device)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in param_optimizer if not any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n",
    "        {\"params\": [p for n, p in param_optimizer if any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=CFG.lr,\n",
    "        betas=CFG.betas,\n",
    "        weight_decay=CFG.weight_decay,\n",
    "    )\n",
    "    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n",
    "    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_train_optimization_steps,\n",
    "    )\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    best_score = -1 * np.inf\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "        avg_loss = train_fn(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            scheduler,\n",
    "            device,\n",
    "        )\n",
    "        avg_val_loss, val_preds = valid_fn(\n",
    "            val_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "\n",
    "        # scoring\n",
    "        val_folds[[str(i) for i in range(CFG.max_len)]] = val_preds\n",
    "        score = scoring(val_folds, th=0.5)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n",
    "        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"predictions\": val_preds,\n",
    "                },\n",
    "                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "            )\n",
    "\n",
    "    predictions = torch.load(\n",
    "        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )[\"predictions\"]\n",
    "    val_folds[[str(i) for i in range(CFG.max_len)]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return val_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-remedy",
   "metadata": {
    "id": "c38fb834"
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "august-registrar",
   "metadata": {
    "id": "62d677cd"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for i_fold in range(CFG.n_fold):\n",
    "            if i_fold in CFG.train_fold:\n",
    "                _oof_df = train_loop(train, i_fold, device)\n",
    "                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n",
    "        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    if CFG.submission:\n",
    "        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n",
    "    else:\n",
    "        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    score = scoring(oof_df, th=0.5)\n",
    "    print(f\"Best thres: 0.5, Score: {score:.4f}\")\n",
    "    best_thres = get_best_thres(oof_df)\n",
    "    score = scoring(oof_df, th=best_thres)\n",
    "    print(f\"Best thres: {best_thres}, Score: {score:.4f}\")\n",
    "\n",
    "    if CFG.inference:\n",
    "        test_dataset = TestDataset(CFG, test)\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=CFG.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=CFG.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        predictions = []\n",
    "        for i_fold in CFG.train_fold:\n",
    "            if CFG.submission:\n",
    "                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n",
    "                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n",
    "            else:\n",
    "                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n",
    "\n",
    "            state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "            model.load_state_dict(state[\"model\"])\n",
    "            test_token_probs = inference_fn(test_dataloader, model, device)\n",
    "            test[[f\"fold{i_fold}_{i}\" for i in range(CFG.max_len)]] = test_token_probs\n",
    "            test_char_probs = get_char_probs(test[\"pn_history\"].values, test_token_probs, CFG.tokenizer)\n",
    "            predictions.append(test_char_probs)\n",
    "\n",
    "            del state, test_token_probs, model; gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        predictions = np.mean(predictions, axis=0)\n",
    "        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n",
    "        test[CFG.target_col] = predicted_location_str\n",
    "        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n",
    "        test[[CFG.id_col, CFG.target_col]].to_csv(\n",
    "            CFG.output_dir / \"submission.csv\", index=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "multiple-window",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "5a00641beddc46eb8da430f2d9999490",
      "04966868d2974cb8b3215a50572c2c94",
      "5a5c41748cba4234a6a6f9aabddfa861",
      "72044a7dbe7d4b5f839f38f6e827ec63",
      "bed6a691643a46d5bd25e03cdc5b73f7",
      "b4d0bd0dea5341a9b03f0092fe3cba39"
     ]
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1646034258180,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "1d4fcf7c",
    "outputId": "1362d223-3d70-4ba7-daa5-14b7300eef5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2849] Elapsed 0m 1s (remain 58m 46s) Loss: 0.7601(0.7601) Grad: inf  LR: 0.000000  \n",
      "Epoch: [1][100/2849] Elapsed 0m 50s (remain 23m 2s) Loss: 0.5324(0.6444) Grad: 51530.3320  LR: 0.000001  \n",
      "Epoch: [1][200/2849] Elapsed 1m 41s (remain 22m 12s) Loss: 0.0492(0.4525) Grad: 1208.9031  LR: 0.000003  \n",
      "Epoch: [1][300/2849] Elapsed 2m 30s (remain 21m 13s) Loss: 0.0263(0.3170) Grad: 368.1682  LR: 0.000004  \n",
      "Epoch: [1][400/2849] Elapsed 3m 19s (remain 20m 20s) Loss: 0.0474(0.2475) Grad: 597.1172  LR: 0.000006  \n",
      "Epoch: [1][500/2849] Elapsed 4m 9s (remain 19m 28s) Loss: 0.0599(0.2039) Grad: 3605.1023  LR: 0.000007  \n",
      "Epoch: [1][600/2849] Elapsed 4m 58s (remain 18m 38s) Loss: 0.0160(0.1732) Grad: 1054.9109  LR: 0.000008  \n",
      "Epoch: [1][700/2849] Elapsed 5m 48s (remain 17m 47s) Loss: 0.0064(0.1507) Grad: 1098.5038  LR: 0.000010  \n",
      "Epoch: [1][800/2849] Elapsed 6m 37s (remain 16m 56s) Loss: 0.0199(0.1339) Grad: 1044.0385  LR: 0.000011  \n",
      "Epoch: [1][900/2849] Elapsed 7m 29s (remain 16m 11s) Loss: 0.0009(0.1202) Grad: 100.0288  LR: 0.000013  \n",
      "Epoch: [1][1000/2849] Elapsed 8m 18s (remain 15m 20s) Loss: 0.0037(0.1092) Grad: 327.1168  LR: 0.000014  \n",
      "Epoch: [1][1100/2849] Elapsed 9m 8s (remain 14m 30s) Loss: 0.0018(0.1002) Grad: 118.7289  LR: 0.000015  \n",
      "Epoch: [1][1200/2849] Elapsed 9m 58s (remain 13m 41s) Loss: 0.0123(0.0928) Grad: 676.7711  LR: 0.000017  \n",
      "Epoch: [1][1300/2849] Elapsed 10m 48s (remain 12m 51s) Loss: 0.0042(0.0863) Grad: 302.8724  LR: 0.000018  \n",
      "Epoch: [1][1400/2849] Elapsed 11m 37s (remain 12m 1s) Loss: 0.0076(0.0807) Grad: 398.1974  LR: 0.000020  \n",
      "Epoch: [1][1500/2849] Elapsed 12m 27s (remain 11m 11s) Loss: 0.0040(0.0759) Grad: 527.6235  LR: 0.000020  \n",
      "Epoch: [1][1600/2849] Elapsed 13m 17s (remain 10m 21s) Loss: 0.0028(0.0717) Grad: 208.6156  LR: 0.000020  \n",
      "Epoch: [1][1700/2849] Elapsed 14m 7s (remain 9m 31s) Loss: 0.0138(0.0681) Grad: 1306.8885  LR: 0.000020  \n",
      "Epoch: [1][1800/2849] Elapsed 14m 56s (remain 8m 41s) Loss: 0.0225(0.0648) Grad: 808.7238  LR: 0.000019  \n",
      "Epoch: [1][1900/2849] Elapsed 15m 45s (remain 7m 51s) Loss: 0.0114(0.0618) Grad: 455.7094  LR: 0.000019  \n",
      "Epoch: [1][2000/2849] Elapsed 16m 35s (remain 7m 2s) Loss: 0.0003(0.0592) Grad: 88.2283  LR: 0.000019  \n",
      "Epoch: [1][2100/2849] Elapsed 17m 26s (remain 6m 12s) Loss: 0.0140(0.0567) Grad: 1015.2184  LR: 0.000019  \n",
      "Epoch: [1][2200/2849] Elapsed 18m 16s (remain 5m 22s) Loss: 0.0074(0.0545) Grad: 389.5060  LR: 0.000019  \n",
      "Epoch: [1][2300/2849] Elapsed 19m 5s (remain 4m 32s) Loss: 0.0137(0.0524) Grad: 916.4205  LR: 0.000019  \n",
      "Epoch: [1][2400/2849] Elapsed 19m 54s (remain 3m 42s) Loss: 0.0023(0.0506) Grad: 195.6447  LR: 0.000018  \n",
      "Epoch: [1][2500/2849] Elapsed 20m 43s (remain 2m 52s) Loss: 0.0008(0.0489) Grad: 80.2316  LR: 0.000018  \n",
      "Epoch: [1][2600/2849] Elapsed 21m 33s (remain 2m 3s) Loss: 0.0185(0.0474) Grad: 864.1846  LR: 0.000018  \n",
      "Epoch: [1][2700/2849] Elapsed 22m 22s (remain 1m 13s) Loss: 0.0024(0.0459) Grad: 161.2929  LR: 0.000018  \n",
      "Epoch: [1][2800/2849] Elapsed 23m 12s (remain 0m 23s) Loss: 0.0095(0.0445) Grad: 599.1755  LR: 0.000018  \n",
      "Epoch: [1][2848/2849] Elapsed 23m 36s (remain 0m 0s) Loss: 0.0019(0.0439) Grad: 146.2350  LR: 0.000018  \n",
      "EVAL: [0/726] Elapsed 0m 0s (remain 5m 36s) Loss: 0.0022(0.0022) \n",
      "EVAL: [100/726] Elapsed 0m 27s (remain 2m 49s) Loss: 0.0054(0.0063) \n",
      "EVAL: [200/726] Elapsed 0m 54s (remain 2m 23s) Loss: 0.0006(0.0063) \n",
      "EVAL: [300/726] Elapsed 1m 22s (remain 1m 56s) Loss: 0.0003(0.0059) \n",
      "EVAL: [400/726] Elapsed 1m 49s (remain 1m 29s) Loss: 0.0062(0.0073) \n",
      "EVAL: [500/726] Elapsed 2m 17s (remain 1m 1s) Loss: 0.0193(0.0074) \n",
      "EVAL: [600/726] Elapsed 2m 45s (remain 0m 34s) Loss: 0.0019(0.0071) \n",
      "EVAL: [700/726] Elapsed 3m 12s (remain 0m 6s) Loss: 0.0025(0.0068) \n",
      "EVAL: [725/726] Elapsed 3m 18s (remain 0m 0s) Loss: 0.0007(0.0067) \n",
      "Epoch 1 - avg_train_loss: 0.0439  avg_val_loss: 0.0067  time: 1621s\n",
      "Epoch 1 - Score: 0.8473\n",
      "Epoch 1 - Save Best Score: 0.8473 Model\n",
      "Epoch: [2][0/2849] Elapsed 0m 0s (remain 32m 4s) Loss: 0.0384(0.0384) Grad: 64398.1289  LR: 0.000018  \n",
      "Epoch: [2][100/2849] Elapsed 0m 53s (remain 24m 13s) Loss: 0.0001(0.0044) Grad: 525.5435  LR: 0.000018  \n",
      "Epoch: [2][200/2849] Elapsed 1m 44s (remain 22m 50s) Loss: 0.0004(0.0053) Grad: 2483.3901  LR: 0.000017  \n",
      "Epoch: [2][300/2849] Elapsed 2m 34s (remain 21m 46s) Loss: 0.0072(0.0050) Grad: 13230.9424  LR: 0.000017  \n",
      "Epoch: [2][400/2849] Elapsed 3m 24s (remain 20m 46s) Loss: 0.0003(0.0054) Grad: 1808.9755  LR: 0.000017  \n",
      "Epoch: [2][500/2849] Elapsed 4m 14s (remain 19m 53s) Loss: 0.0000(0.0051) Grad: 66.6441  LR: 0.000017  \n",
      "Epoch: [2][600/2849] Elapsed 5m 5s (remain 19m 1s) Loss: 0.0000(0.0052) Grad: 239.8530  LR: 0.000017  \n",
      "Epoch: [2][700/2849] Elapsed 5m 55s (remain 18m 9s) Loss: 0.0019(0.0053) Grad: 5808.8545  LR: 0.000017  \n",
      "Epoch: [2][800/2849] Elapsed 6m 48s (remain 17m 23s) Loss: 0.0070(0.0052) Grad: 14416.9033  LR: 0.000017  \n",
      "Epoch: [2][900/2849] Elapsed 7m 39s (remain 16m 32s) Loss: 0.0018(0.0051) Grad: 9775.4922  LR: 0.000016  \n",
      "Epoch: [2][1000/2849] Elapsed 8m 29s (remain 15m 40s) Loss: 0.0055(0.0051) Grad: 27468.7598  LR: 0.000016  \n",
      "Epoch: [2][1100/2849] Elapsed 9m 20s (remain 14m 49s) Loss: 0.0006(0.0052) Grad: 3002.4907  LR: 0.000016  \n",
      "Epoch: [2][1200/2849] Elapsed 10m 11s (remain 13m 58s) Loss: 0.0038(0.0052) Grad: 13249.8115  LR: 0.000016  \n",
      "Epoch: [2][1300/2849] Elapsed 11m 1s (remain 13m 6s) Loss: 0.0042(0.0053) Grad: 12673.7100  LR: 0.000016  \n",
      "Epoch: [2][1400/2849] Elapsed 11m 51s (remain 12m 15s) Loss: 0.0039(0.0054) Grad: 10043.0801  LR: 0.000016  \n",
      "Epoch: [2][1500/2849] Elapsed 12m 43s (remain 11m 25s) Loss: 0.0067(0.0054) Grad: 30368.6816  LR: 0.000015  \n",
      "Epoch: [2][1600/2849] Elapsed 13m 34s (remain 10m 34s) Loss: 0.0025(0.0055) Grad: 4742.5654  LR: 0.000015  \n",
      "Epoch: [2][1700/2849] Elapsed 14m 24s (remain 9m 43s) Loss: 0.0001(0.0055) Grad: 221.2063  LR: 0.000015  \n",
      "Epoch: [2][1800/2849] Elapsed 15m 14s (remain 8m 52s) Loss: 0.0338(0.0055) Grad: 20236.0371  LR: 0.000015  \n",
      "Epoch: [2][1900/2849] Elapsed 16m 4s (remain 8m 0s) Loss: 0.0002(0.0054) Grad: 690.7553  LR: 0.000015  \n",
      "Epoch: [2][2000/2849] Elapsed 16m 55s (remain 7m 10s) Loss: 0.0031(0.0055) Grad: 14924.0225  LR: 0.000015  \n",
      "Epoch: [2][2100/2849] Elapsed 17m 46s (remain 6m 19s) Loss: 0.0004(0.0056) Grad: 1024.3099  LR: 0.000014  \n",
      "Epoch: [2][2200/2849] Elapsed 18m 36s (remain 5m 28s) Loss: 0.0522(0.0057) Grad: 35463.5039  LR: 0.000014  \n",
      "Epoch: [2][2300/2849] Elapsed 19m 26s (remain 4m 37s) Loss: 0.0016(0.0056) Grad: 1010.1541  LR: 0.000014  \n",
      "Epoch: [2][2400/2849] Elapsed 20m 16s (remain 3m 46s) Loss: 0.0002(0.0056) Grad: 281.3663  LR: 0.000014  \n",
      "Epoch: [2][2500/2849] Elapsed 21m 6s (remain 2m 56s) Loss: 0.0000(0.0057) Grad: 55.6216  LR: 0.000014  \n",
      "Epoch: [2][2600/2849] Elapsed 21m 58s (remain 2m 5s) Loss: 0.0001(0.0057) Grad: 61.4717  LR: 0.000014  \n",
      "Epoch: [2][2700/2849] Elapsed 22m 48s (remain 1m 15s) Loss: 0.0002(0.0056) Grad: 240.5946  LR: 0.000014  \n",
      "Epoch: [2][2800/2849] Elapsed 23m 38s (remain 0m 24s) Loss: 0.0000(0.0056) Grad: 67.4258  LR: 0.000013  \n",
      "Epoch: [2][2848/2849] Elapsed 24m 2s (remain 0m 0s) Loss: 0.0008(0.0056) Grad: 1119.9536  LR: 0.000013  \n",
      "EVAL: [0/726] Elapsed 0m 0s (remain 5m 43s) Loss: 0.0014(0.0014) \n",
      "EVAL: [100/726] Elapsed 0m 27s (remain 2m 52s) Loss: 0.0074(0.0065) \n",
      "EVAL: [200/726] Elapsed 0m 55s (remain 2m 24s) Loss: 0.0002(0.0068) \n",
      "EVAL: [300/726] Elapsed 1m 22s (remain 1m 56s) Loss: 0.0000(0.0064) \n",
      "EVAL: [400/726] Elapsed 1m 50s (remain 1m 29s) Loss: 0.0030(0.0080) \n",
      "EVAL: [500/726] Elapsed 2m 17s (remain 1m 1s) Loss: 0.0228(0.0081) \n",
      "EVAL: [600/726] Elapsed 2m 45s (remain 0m 34s) Loss: 0.0004(0.0075) \n",
      "EVAL: [700/726] Elapsed 3m 12s (remain 0m 6s) Loss: 0.0018(0.0074) \n",
      "EVAL: [725/726] Elapsed 3m 18s (remain 0m 0s) Loss: 0.0001(0.0072) \n",
      "Epoch 2 - avg_train_loss: 0.0056  avg_val_loss: 0.0072  time: 1647s\n",
      "Epoch 2 - Score: 0.8669\n",
      "Epoch 2 - Save Best Score: 0.8669 Model\n",
      "Epoch: [3][0/2849] Elapsed 0m 0s (remain 34m 32s) Loss: 0.0065(0.0065) Grad: 19582.9922  LR: 0.000013  \n",
      "Epoch: [3][100/2849] Elapsed 0m 51s (remain 23m 20s) Loss: 0.1097(0.0055) Grad: 192207.6562  LR: 0.000013  \n",
      "Epoch: [3][200/2849] Elapsed 1m 41s (remain 22m 16s) Loss: 0.0039(0.0046) Grad: 8779.5859  LR: 0.000013  \n",
      "Epoch: [3][300/2849] Elapsed 2m 31s (remain 21m 20s) Loss: 0.0027(0.0044) Grad: 9400.3916  LR: 0.000013  \n",
      "Epoch: [3][400/2849] Elapsed 3m 21s (remain 20m 27s) Loss: 0.0076(0.0043) Grad: 30905.5996  LR: 0.000013  \n",
      "Epoch: [3][500/2849] Elapsed 4m 11s (remain 19m 40s) Loss: 0.0000(0.0047) Grad: 64.2971  LR: 0.000013  \n",
      "Epoch: [3][600/2849] Elapsed 5m 3s (remain 18m 53s) Loss: 0.0033(0.0044) Grad: 7932.3936  LR: 0.000012  \n",
      "Epoch: [3][700/2849] Elapsed 5m 53s (remain 18m 1s) Loss: 0.0012(0.0046) Grad: 4918.3481  LR: 0.000012  \n",
      "Epoch: [3][800/2849] Elapsed 6m 42s (remain 17m 10s) Loss: 0.0002(0.0045) Grad: 1165.9998  LR: 0.000012  \n",
      "Epoch: [3][900/2849] Elapsed 7m 33s (remain 16m 21s) Loss: 0.0018(0.0047) Grad: 10798.5010  LR: 0.000012  \n",
      "Epoch: [3][1000/2849] Elapsed 8m 26s (remain 15m 34s) Loss: 0.0040(0.0049) Grad: 13352.5244  LR: 0.000012  \n",
      "Epoch: [3][1100/2849] Elapsed 9m 16s (remain 14m 43s) Loss: 0.0001(0.0048) Grad: 1304.9741  LR: 0.000012  \n",
      "Epoch: [3][1200/2849] Elapsed 10m 7s (remain 13m 53s) Loss: 0.0008(0.0049) Grad: 9678.3359  LR: 0.000011  \n",
      "Epoch: [3][1300/2849] Elapsed 11m 0s (remain 13m 5s) Loss: 0.0069(0.0048) Grad: 8882.2305  LR: 0.000011  \n",
      "Epoch: [3][1400/2849] Elapsed 11m 52s (remain 12m 16s) Loss: 0.0106(0.0047) Grad: 16484.7773  LR: 0.000011  \n",
      "Epoch: [3][1500/2849] Elapsed 12m 42s (remain 11m 24s) Loss: 0.0010(0.0046) Grad: 6963.7271  LR: 0.000011  \n",
      "Epoch: [3][1600/2849] Elapsed 13m 32s (remain 10m 33s) Loss: 0.0001(0.0046) Grad: 2490.2617  LR: 0.000011  \n",
      "Epoch: [3][1700/2849] Elapsed 14m 22s (remain 9m 42s) Loss: 0.0001(0.0046) Grad: 590.5786  LR: 0.000011  \n",
      "Epoch: [3][1800/2849] Elapsed 15m 13s (remain 8m 51s) Loss: 0.0014(0.0047) Grad: 8063.1953  LR: 0.000011  \n",
      "Epoch: [3][1900/2849] Elapsed 16m 4s (remain 8m 1s) Loss: 0.0017(0.0047) Grad: 10804.5244  LR: 0.000010  \n",
      "Epoch: [3][2000/2849] Elapsed 16m 54s (remain 7m 9s) Loss: 0.0005(0.0047) Grad: 2261.3035  LR: 0.000010  \n",
      "Epoch: [3][2100/2849] Elapsed 17m 45s (remain 6m 19s) Loss: 0.0006(0.0047) Grad: 3108.6655  LR: 0.000010  \n",
      "Epoch: [3][2200/2849] Elapsed 18m 35s (remain 5m 28s) Loss: 0.0017(0.0047) Grad: 8749.6279  LR: 0.000010  \n",
      "Epoch: [3][2300/2849] Elapsed 19m 27s (remain 4m 38s) Loss: 0.0048(0.0047) Grad: 21096.8008  LR: 0.000010  \n",
      "Epoch: [3][2400/2849] Elapsed 20m 19s (remain 3m 47s) Loss: 0.0050(0.0047) Grad: 15293.4150  LR: 0.000010  \n",
      "Epoch: [3][2500/2849] Elapsed 21m 9s (remain 2m 56s) Loss: 0.0079(0.0048) Grad: 69284.4844  LR: 0.000009  \n",
      "Epoch: [3][2600/2849] Elapsed 22m 0s (remain 2m 5s) Loss: 0.0000(0.0048) Grad: 146.4422  LR: 0.000009  \n",
      "Epoch: [3][2700/2849] Elapsed 22m 51s (remain 1m 15s) Loss: 0.0004(0.0048) Grad: 3242.9299  LR: 0.000009  \n",
      "Epoch: [3][2800/2849] Elapsed 23m 41s (remain 0m 24s) Loss: 0.0014(0.0048) Grad: 5893.9268  LR: 0.000009  \n",
      "Epoch: [3][2848/2849] Elapsed 24m 5s (remain 0m 0s) Loss: 0.0001(0.0048) Grad: 228.2947  LR: 0.000009  \n",
      "EVAL: [0/726] Elapsed 0m 0s (remain 5m 51s) Loss: 0.0010(0.0010) \n",
      "EVAL: [100/726] Elapsed 0m 27s (remain 2m 49s) Loss: 0.0093(0.0076) \n",
      "EVAL: [200/726] Elapsed 0m 55s (remain 2m 23s) Loss: 0.0000(0.0073) \n",
      "EVAL: [300/726] Elapsed 1m 22s (remain 1m 56s) Loss: 0.0000(0.0070) \n",
      "EVAL: [400/726] Elapsed 1m 49s (remain 1m 29s) Loss: 0.0014(0.0085) \n",
      "EVAL: [500/726] Elapsed 2m 17s (remain 1m 1s) Loss: 0.0244(0.0084) \n",
      "EVAL: [600/726] Elapsed 2m 44s (remain 0m 34s) Loss: 0.0033(0.0077) \n",
      "EVAL: [700/726] Elapsed 3m 11s (remain 0m 6s) Loss: 0.0008(0.0077) \n",
      "EVAL: [725/726] Elapsed 3m 18s (remain 0m 0s) Loss: 0.0000(0.0075) \n",
      "Epoch 3 - avg_train_loss: 0.0048  avg_val_loss: 0.0075  time: 1648s\n",
      "Epoch 3 - Score: 0.8832\n",
      "Epoch 3 - Save Best Score: 0.8832 Model\n",
      "Epoch: [4][0/2849] Elapsed 0m 0s (remain 35m 54s) Loss: 0.0016(0.0016) Grad: 7317.6489  LR: 0.000009  \n",
      "Epoch: [4][100/2849] Elapsed 0m 53s (remain 24m 6s) Loss: 0.0097(0.0033) Grad: 61405.6914  LR: 0.000009  \n",
      "Epoch: [4][200/2849] Elapsed 1m 43s (remain 22m 37s) Loss: 0.0018(0.0038) Grad: 3649.1111  LR: 0.000009  \n",
      "Epoch: [4][300/2849] Elapsed 2m 34s (remain 21m 46s) Loss: 0.0007(0.0034) Grad: 3241.0154  LR: 0.000008  \n",
      "Epoch: [4][400/2849] Elapsed 3m 24s (remain 20m 48s) Loss: 0.0001(0.0032) Grad: 1179.6908  LR: 0.000008  \n",
      "Epoch: [4][500/2849] Elapsed 4m 14s (remain 19m 52s) Loss: 0.0007(0.0033) Grad: 4655.2744  LR: 0.000008  \n",
      "Epoch: [4][600/2849] Elapsed 5m 4s (remain 18m 57s) Loss: 0.0255(0.0034) Grad: 30132.7891  LR: 0.000008  \n",
      "Epoch: [4][700/2849] Elapsed 5m 54s (remain 18m 4s) Loss: 0.0002(0.0035) Grad: 2129.7478  LR: 0.000008  \n",
      "Epoch: [4][800/2849] Elapsed 6m 43s (remain 17m 12s) Loss: 0.0049(0.0036) Grad: 20202.3516  LR: 0.000008  \n",
      "Epoch: [4][900/2849] Elapsed 7m 34s (remain 16m 22s) Loss: 0.0015(0.0036) Grad: 7513.8677  LR: 0.000007  \n",
      "Epoch: [4][1000/2849] Elapsed 8m 24s (remain 15m 31s) Loss: 0.0017(0.0036) Grad: 13107.7588  LR: 0.000007  \n",
      "Epoch: [4][1100/2849] Elapsed 9m 14s (remain 14m 40s) Loss: 0.0005(0.0036) Grad: 3469.1497  LR: 0.000007  \n",
      "Epoch: [4][1200/2849] Elapsed 10m 4s (remain 13m 49s) Loss: 0.0017(0.0036) Grad: 5770.9712  LR: 0.000007  \n",
      "Epoch: [4][1300/2849] Elapsed 10m 54s (remain 12m 58s) Loss: 0.0000(0.0036) Grad: 18.8213  LR: 0.000007  \n",
      "Epoch: [4][1400/2849] Elapsed 11m 43s (remain 12m 7s) Loss: 0.0000(0.0035) Grad: 47.2505  LR: 0.000007  \n",
      "Epoch: [4][1500/2849] Elapsed 12m 33s (remain 11m 17s) Loss: 0.0015(0.0037) Grad: 7362.7798  LR: 0.000007  \n",
      "Epoch: [4][1600/2849] Elapsed 13m 23s (remain 10m 26s) Loss: 0.0000(0.0038) Grad: 86.1024  LR: 0.000006  \n",
      "Epoch: [4][1700/2849] Elapsed 14m 14s (remain 9m 36s) Loss: 0.0000(0.0038) Grad: 70.5415  LR: 0.000006  \n",
      "Epoch: [4][1800/2849] Elapsed 15m 4s (remain 8m 46s) Loss: 0.0001(0.0038) Grad: 681.1282  LR: 0.000006  \n",
      "Epoch: [4][1900/2849] Elapsed 15m 55s (remain 7m 56s) Loss: 0.0002(0.0038) Grad: 1136.1227  LR: 0.000006  \n",
      "Epoch: [4][2000/2849] Elapsed 16m 45s (remain 7m 6s) Loss: 0.0000(0.0038) Grad: 61.0329  LR: 0.000006  \n",
      "Epoch: [4][2100/2849] Elapsed 17m 36s (remain 6m 16s) Loss: 0.0000(0.0038) Grad: 7.3578  LR: 0.000006  \n",
      "Epoch: [4][2200/2849] Elapsed 18m 27s (remain 5m 25s) Loss: 0.0000(0.0038) Grad: 18.0757  LR: 0.000005  \n",
      "Epoch: [4][2300/2849] Elapsed 19m 17s (remain 4m 35s) Loss: 0.0012(0.0039) Grad: 11654.7861  LR: 0.000005  \n",
      "Epoch: [4][2400/2849] Elapsed 20m 7s (remain 3m 45s) Loss: 0.0014(0.0039) Grad: 5035.2661  LR: 0.000005  \n",
      "Epoch: [4][2500/2849] Elapsed 20m 57s (remain 2m 54s) Loss: 0.0029(0.0039) Grad: 19173.1914  LR: 0.000005  \n",
      "Epoch: [4][2600/2849] Elapsed 21m 47s (remain 2m 4s) Loss: 0.0001(0.0039) Grad: 516.4949  LR: 0.000005  \n",
      "Epoch: [4][2700/2849] Elapsed 22m 37s (remain 1m 14s) Loss: 0.0000(0.0039) Grad: 89.7253  LR: 0.000005  \n",
      "Epoch: [4][2800/2849] Elapsed 23m 27s (remain 0m 24s) Loss: 0.0001(0.0039) Grad: 282.6696  LR: 0.000005  \n",
      "Epoch: [4][2848/2849] Elapsed 23m 52s (remain 0m 0s) Loss: 0.0000(0.0039) Grad: 10.8198  LR: 0.000004  \n",
      "EVAL: [0/726] Elapsed 0m 0s (remain 5m 40s) Loss: 0.0016(0.0016) \n",
      "EVAL: [100/726] Elapsed 0m 27s (remain 2m 49s) Loss: 0.0119(0.0100) \n",
      "EVAL: [200/726] Elapsed 0m 54s (remain 2m 23s) Loss: 0.0000(0.0089) \n",
      "EVAL: [300/726] Elapsed 1m 22s (remain 1m 56s) Loss: 0.0000(0.0082) \n",
      "EVAL: [400/726] Elapsed 1m 50s (remain 1m 29s) Loss: 0.0013(0.0100) \n",
      "EVAL: [500/726] Elapsed 2m 19s (remain 1m 2s) Loss: 0.0263(0.0100) \n",
      "EVAL: [600/726] Elapsed 2m 46s (remain 0m 34s) Loss: 0.0005(0.0091) \n",
      "EVAL: [700/726] Elapsed 3m 13s (remain 0m 6s) Loss: 0.0001(0.0091) \n",
      "EVAL: [725/726] Elapsed 3m 20s (remain 0m 0s) Loss: 0.0000(0.0089) \n",
      "Epoch 4 - avg_train_loss: 0.0039  avg_val_loss: 0.0089  time: 1637s\n",
      "Epoch 4 - Score: 0.8830\n",
      "Epoch: [5][0/2849] Elapsed 0m 0s (remain 30m 55s) Loss: 0.0000(0.0000) Grad: 140.5788  LR: 0.000004  \n",
      "Epoch: [5][100/2849] Elapsed 0m 50s (remain 22m 54s) Loss: 0.0000(0.0031) Grad: 6.9375  LR: 0.000004  \n",
      "Epoch: [5][200/2849] Elapsed 1m 40s (remain 21m 59s) Loss: 0.0049(0.0036) Grad: 16055.8701  LR: 0.000004  \n",
      "Epoch: [5][300/2849] Elapsed 2m 30s (remain 21m 14s) Loss: 0.0009(0.0036) Grad: 3587.4614  LR: 0.000004  \n",
      "Epoch: [5][400/2849] Elapsed 3m 21s (remain 20m 27s) Loss: 0.0003(0.0034) Grad: 1160.6444  LR: 0.000004  \n",
      "Epoch: [5][500/2849] Elapsed 4m 11s (remain 19m 40s) Loss: 0.0011(0.0032) Grad: 7119.3794  LR: 0.000004  \n",
      "Epoch: [5][600/2849] Elapsed 5m 4s (remain 18m 57s) Loss: 0.0000(0.0031) Grad: 104.8683  LR: 0.000004  \n",
      "Epoch: [5][700/2849] Elapsed 5m 54s (remain 18m 7s) Loss: 0.0048(0.0031) Grad: 14116.2734  LR: 0.000003  \n",
      "Epoch: [5][800/2849] Elapsed 6m 44s (remain 17m 14s) Loss: 0.0002(0.0032) Grad: 2112.0249  LR: 0.000003  \n",
      "Epoch: [5][900/2849] Elapsed 7m 34s (remain 16m 22s) Loss: 0.0003(0.0032) Grad: 1890.3718  LR: 0.000003  \n",
      "Epoch: [5][1000/2849] Elapsed 8m 25s (remain 15m 32s) Loss: 0.0000(0.0032) Grad: 262.8234  LR: 0.000003  \n",
      "Epoch: [5][1100/2849] Elapsed 9m 15s (remain 14m 42s) Loss: 0.0001(0.0033) Grad: 4323.3145  LR: 0.000003  \n",
      "Epoch: [5][1200/2849] Elapsed 10m 5s (remain 13m 51s) Loss: 0.0000(0.0033) Grad: 153.1098  LR: 0.000003  \n",
      "Epoch: [5][1300/2849] Elapsed 10m 55s (remain 13m 0s) Loss: 0.0000(0.0033) Grad: 77.6621  LR: 0.000002  \n",
      "Epoch: [5][1400/2849] Elapsed 11m 45s (remain 12m 9s) Loss: 0.0000(0.0032) Grad: 23.6958  LR: 0.000002  \n",
      "Epoch: [5][1500/2849] Elapsed 12m 36s (remain 11m 19s) Loss: 0.0049(0.0033) Grad: 136490.6875  LR: 0.000002  \n",
      "Epoch: [5][1600/2849] Elapsed 13m 26s (remain 10m 28s) Loss: 0.0037(0.0033) Grad: 37645.6719  LR: 0.000002  \n",
      "Epoch: [5][1700/2849] Elapsed 14m 17s (remain 9m 38s) Loss: 0.0035(0.0033) Grad: 13274.8594  LR: 0.000002  \n",
      "Epoch: [5][1800/2849] Elapsed 15m 8s (remain 8m 48s) Loss: 0.0563(0.0033) Grad: 90649.0000  LR: 0.000002  \n",
      "Epoch: [5][1900/2849] Elapsed 15m 58s (remain 7m 57s) Loss: 0.0245(0.0033) Grad: 48819.1172  LR: 0.000001  \n",
      "Epoch: [5][2000/2849] Elapsed 16m 48s (remain 7m 7s) Loss: 0.0000(0.0033) Grad: 11.0084  LR: 0.000001  \n",
      "Epoch: [5][2100/2849] Elapsed 17m 40s (remain 6m 17s) Loss: 0.0019(0.0032) Grad: 3828.3047  LR: 0.000001  \n",
      "Epoch: [5][2200/2849] Elapsed 18m 31s (remain 5m 27s) Loss: 0.0000(0.0032) Grad: 9.7055  LR: 0.000001  \n",
      "Epoch: [5][2300/2849] Elapsed 19m 20s (remain 4m 36s) Loss: 0.0029(0.0033) Grad: 13205.8281  LR: 0.000001  \n",
      "Epoch: [5][2400/2849] Elapsed 20m 13s (remain 3m 46s) Loss: 0.0003(0.0033) Grad: 3813.8008  LR: 0.000001  \n",
      "Epoch: [5][2500/2849] Elapsed 21m 4s (remain 2m 55s) Loss: 0.0019(0.0033) Grad: 11910.7949  LR: 0.000001  \n",
      "Epoch: [5][2600/2849] Elapsed 21m 54s (remain 2m 5s) Loss: 0.0001(0.0034) Grad: 860.4519  LR: 0.000000  \n",
      "Epoch: [5][2700/2849] Elapsed 22m 44s (remain 1m 14s) Loss: 0.0105(0.0033) Grad: 30666.5078  LR: 0.000000  \n",
      "Epoch: [5][2800/2849] Elapsed 23m 35s (remain 0m 24s) Loss: 0.0001(0.0033) Grad: 416.9294  LR: 0.000000  \n",
      "Epoch: [5][2848/2849] Elapsed 23m 59s (remain 0m 0s) Loss: 0.0000(0.0033) Grad: 52.2209  LR: 0.000000  \n",
      "EVAL: [0/726] Elapsed 0m 0s (remain 5m 41s) Loss: 0.0006(0.0006) \n",
      "EVAL: [100/726] Elapsed 0m 27s (remain 2m 50s) Loss: 0.0127(0.0106) \n",
      "EVAL: [200/726] Elapsed 0m 55s (remain 2m 23s) Loss: 0.0000(0.0098) \n",
      "EVAL: [300/726] Elapsed 1m 22s (remain 1m 55s) Loss: 0.0000(0.0090) \n",
      "EVAL: [400/726] Elapsed 1m 49s (remain 1m 28s) Loss: 0.0011(0.0109) \n",
      "EVAL: [500/726] Elapsed 2m 17s (remain 1m 1s) Loss: 0.0311(0.0108) \n",
      "EVAL: [600/726] Elapsed 2m 44s (remain 0m 34s) Loss: 0.0004(0.0099) \n",
      "EVAL: [700/726] Elapsed 3m 12s (remain 0m 6s) Loss: 0.0001(0.0098) \n",
      "EVAL: [725/726] Elapsed 3m 19s (remain 0m 0s) Loss: 0.0000(0.0097) \n",
      "Epoch 5 - avg_train_loss: 0.0033  avg_val_loss: 0.0097  time: 1644s\n",
      "Epoch 5 - Score: 0.8828\n",
      "========== fold: 1 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2851] Elapsed 0m 0s (remain 34m 36s) Loss: 0.5713(0.5713) Grad: inf  LR: 0.000000  \n",
      "Epoch: [1][100/2851] Elapsed 0m 50s (remain 22m 56s) Loss: 0.2602(0.5352) Grad: 25016.5059  LR: 0.000001  \n",
      "Epoch: [1][200/2851] Elapsed 1m 41s (remain 22m 16s) Loss: 0.0758(0.3896) Grad: 6969.4292  LR: 0.000003  \n",
      "Epoch: [1][300/2851] Elapsed 2m 31s (remain 21m 27s) Loss: 0.0413(0.2751) Grad: 1179.7117  LR: 0.000004  \n",
      "Epoch: [1][400/2851] Elapsed 3m 21s (remain 20m 33s) Loss: 0.0311(0.2165) Grad: 732.5551  LR: 0.000006  \n",
      "Epoch: [1][500/2851] Elapsed 4m 11s (remain 19m 40s) Loss: 0.0209(0.1796) Grad: 1398.5551  LR: 0.000007  \n",
      "Epoch: [1][600/2851] Elapsed 5m 2s (remain 18m 52s) Loss: 0.0199(0.1528) Grad: 3611.5288  LR: 0.000008  \n",
      "Epoch: [1][700/2851] Elapsed 5m 53s (remain 18m 3s) Loss: 0.0067(0.1338) Grad: 789.1927  LR: 0.000010  \n",
      "Epoch: [1][800/2851] Elapsed 6m 42s (remain 17m 11s) Loss: 0.0358(0.1189) Grad: 4952.0239  LR: 0.000011  \n",
      "Epoch: [1][900/2851] Elapsed 7m 32s (remain 16m 19s) Loss: 0.0062(0.1072) Grad: 870.3964  LR: 0.000013  \n",
      "Epoch: [1][1000/2851] Elapsed 8m 22s (remain 15m 28s) Loss: 0.0229(0.0977) Grad: 2415.9866  LR: 0.000014  \n",
      "Epoch: [1][1100/2851] Elapsed 9m 12s (remain 14m 38s) Loss: 0.0304(0.0899) Grad: 4397.3345  LR: 0.000015  \n",
      "Epoch: [1][1200/2851] Elapsed 10m 3s (remain 13m 48s) Loss: 0.0211(0.0832) Grad: 2441.3457  LR: 0.000017  \n",
      "Epoch: [1][1300/2851] Elapsed 10m 55s (remain 13m 0s) Loss: 0.0159(0.0776) Grad: 3392.4182  LR: 0.000018  \n",
      "Epoch: [1][1400/2851] Elapsed 11m 45s (remain 12m 9s) Loss: 0.0116(0.0728) Grad: 1518.2695  LR: 0.000020  \n",
      "Epoch: [1][1500/2851] Elapsed 12m 35s (remain 11m 19s) Loss: 0.0260(0.0686) Grad: 4146.8755  LR: 0.000020  \n",
      "Epoch: [1][1600/2851] Elapsed 13m 25s (remain 10m 28s) Loss: 0.0198(0.0649) Grad: 2063.8489  LR: 0.000020  \n",
      "Epoch: [1][1700/2851] Elapsed 14m 15s (remain 9m 38s) Loss: 0.0051(0.0616) Grad: 624.8767  LR: 0.000020  \n",
      "Epoch: [1][1800/2851] Elapsed 15m 6s (remain 8m 48s) Loss: 0.0008(0.0586) Grad: 164.3242  LR: 0.000019  \n",
      "Epoch: [1][1900/2851] Elapsed 15m 57s (remain 7m 58s) Loss: 0.0134(0.0559) Grad: 4166.9590  LR: 0.000019  \n",
      "Epoch: [1][2000/2851] Elapsed 16m 46s (remain 7m 7s) Loss: 0.0011(0.0536) Grad: 197.9046  LR: 0.000019  \n",
      "Epoch: [1][2100/2851] Elapsed 17m 36s (remain 6m 17s) Loss: 0.0001(0.0514) Grad: 59.3902  LR: 0.000019  \n",
      "Epoch: [1][2200/2851] Elapsed 18m 25s (remain 5m 26s) Loss: 0.0006(0.0494) Grad: 217.8593  LR: 0.000019  \n",
      "Epoch: [1][2300/2851] Elapsed 19m 16s (remain 4m 36s) Loss: 0.0001(0.0476) Grad: 30.9909  LR: 0.000019  \n",
      "Epoch: [1][2400/2851] Elapsed 20m 5s (remain 3m 46s) Loss: 0.0007(0.0459) Grad: 136.3503  LR: 0.000018  \n",
      "Epoch: [1][2500/2851] Elapsed 20m 55s (remain 2m 55s) Loss: 0.0240(0.0444) Grad: 2427.9500  LR: 0.000018  \n",
      "Epoch: [1][2600/2851] Elapsed 21m 45s (remain 2m 5s) Loss: 0.0072(0.0430) Grad: 881.5999  LR: 0.000018  \n",
      "Epoch: [1][2700/2851] Elapsed 22m 35s (remain 1m 15s) Loss: 0.0013(0.0416) Grad: 376.5804  LR: 0.000018  \n",
      "Epoch: [1][2800/2851] Elapsed 23m 25s (remain 0m 25s) Loss: 0.0085(0.0404) Grad: 2152.1392  LR: 0.000018  \n",
      "Epoch: [1][2850/2851] Elapsed 23m 50s (remain 0m 0s) Loss: 0.0558(0.0398) Grad: 11799.4473  LR: 0.000018  \n",
      "EVAL: [0/724] Elapsed 0m 0s (remain 6m 1s) Loss: 0.0019(0.0019) \n",
      "EVAL: [100/724] Elapsed 0m 28s (remain 2m 53s) Loss: 0.0058(0.0070) \n",
      "EVAL: [200/724] Elapsed 0m 56s (remain 2m 26s) Loss: 0.0004(0.0095) \n",
      "EVAL: [300/724] Elapsed 1m 24s (remain 1m 58s) Loss: 0.0004(0.0087) \n",
      "EVAL: [400/724] Elapsed 1m 51s (remain 1m 29s) Loss: 0.0002(0.0082) \n",
      "EVAL: [500/724] Elapsed 2m 18s (remain 1m 1s) Loss: 0.0090(0.0092) \n",
      "EVAL: [600/724] Elapsed 2m 46s (remain 0m 34s) Loss: 0.0046(0.0088) \n",
      "EVAL: [700/724] Elapsed 3m 14s (remain 0m 6s) Loss: 0.0000(0.0080) \n",
      "EVAL: [723/724] Elapsed 3m 19s (remain 0m 0s) Loss: 0.0002(0.0079) \n",
      "Epoch 1 - avg_train_loss: 0.0398  avg_val_loss: 0.0079  time: 1636s\n",
      "Epoch 1 - Score: 0.8527\n",
      "Epoch 1 - Save Best Score: 0.8527 Model\n",
      "Epoch: [2][0/2851] Elapsed 0m 0s (remain 33m 33s) Loss: 0.0034(0.0034) Grad: 8933.6748  LR: 0.000018  \n",
      "Epoch: [2][100/2851] Elapsed 0m 50s (remain 22m 59s) Loss: 0.0141(0.0094) Grad: 39859.3320  LR: 0.000018  \n",
      "Epoch: [2][200/2851] Elapsed 1m 40s (remain 22m 9s) Loss: 0.0013(0.0084) Grad: 3829.8821  LR: 0.000017  \n",
      "Epoch: [2][300/2851] Elapsed 2m 32s (remain 21m 28s) Loss: 0.0060(0.0076) Grad: 26011.3340  LR: 0.000017  \n",
      "Epoch: [2][400/2851] Elapsed 3m 22s (remain 20m 36s) Loss: 0.0009(0.0069) Grad: 4967.5181  LR: 0.000017  \n",
      "Epoch: [2][500/2851] Elapsed 4m 12s (remain 19m 43s) Loss: 0.0009(0.0067) Grad: 5045.9448  LR: 0.000017  \n",
      "Epoch: [2][600/2851] Elapsed 5m 2s (remain 18m 51s) Loss: 0.0009(0.0064) Grad: 5882.9287  LR: 0.000017  \n",
      "Epoch: [2][700/2851] Elapsed 5m 53s (remain 18m 3s) Loss: 0.0014(0.0064) Grad: 4073.5664  LR: 0.000017  \n",
      "Epoch: [2][800/2851] Elapsed 6m 43s (remain 17m 13s) Loss: 0.0001(0.0063) Grad: 266.8332  LR: 0.000017  \n",
      "Epoch: [2][900/2851] Elapsed 7m 33s (remain 16m 21s) Loss: 0.0025(0.0062) Grad: 13456.2393  LR: 0.000016  \n",
      "Epoch: [2][1000/2851] Elapsed 8m 24s (remain 15m 32s) Loss: 0.0013(0.0060) Grad: 20363.6250  LR: 0.000016  \n",
      "Epoch: [2][1100/2851] Elapsed 9m 15s (remain 14m 43s) Loss: 0.0426(0.0059) Grad: 40440.6641  LR: 0.000016  \n",
      "Epoch: [2][1200/2851] Elapsed 10m 5s (remain 13m 52s) Loss: 0.0017(0.0060) Grad: 9849.8066  LR: 0.000016  \n",
      "Epoch: [2][1300/2851] Elapsed 10m 55s (remain 13m 1s) Loss: 0.0000(0.0060) Grad: 17.0371  LR: 0.000016  \n",
      "Epoch: [2][1400/2851] Elapsed 11m 45s (remain 12m 10s) Loss: 0.0087(0.0060) Grad: 23996.8027  LR: 0.000016  \n",
      "Epoch: [2][1500/2851] Elapsed 12m 35s (remain 11m 19s) Loss: 0.0011(0.0061) Grad: 6567.5190  LR: 0.000015  \n",
      "Epoch: [2][1600/2851] Elapsed 13m 26s (remain 10m 29s) Loss: 0.0406(0.0060) Grad: 32093.1133  LR: 0.000015  \n",
      "Epoch: [2][1700/2851] Elapsed 14m 17s (remain 9m 39s) Loss: 0.0128(0.0060) Grad: 20812.5684  LR: 0.000015  \n",
      "Epoch: [2][1800/2851] Elapsed 15m 7s (remain 8m 49s) Loss: 0.0001(0.0061) Grad: 770.0610  LR: 0.000015  \n",
      "Epoch: [2][1900/2851] Elapsed 15m 57s (remain 7m 58s) Loss: 0.0047(0.0061) Grad: 8787.3564  LR: 0.000015  \n",
      "Epoch: [2][2000/2851] Elapsed 16m 48s (remain 7m 8s) Loss: 0.0061(0.0061) Grad: 20178.4805  LR: 0.000015  \n",
      "Epoch: [2][2100/2851] Elapsed 17m 38s (remain 6m 17s) Loss: 0.0008(0.0062) Grad: 5257.0962  LR: 0.000015  \n",
      "Epoch: [2][2200/2851] Elapsed 18m 28s (remain 5m 27s) Loss: 0.0015(0.0063) Grad: 7610.5103  LR: 0.000014  \n",
      "Epoch: [2][2300/2851] Elapsed 19m 18s (remain 4m 36s) Loss: 0.0084(0.0062) Grad: 8739.7549  LR: 0.000014  \n",
      "Epoch: [2][2400/2851] Elapsed 20m 9s (remain 3m 46s) Loss: 0.0266(0.0062) Grad: 33964.5781  LR: 0.000014  \n",
      "Epoch: [2][2500/2851] Elapsed 20m 58s (remain 2m 56s) Loss: 0.0000(0.0061) Grad: 34.7415  LR: 0.000014  \n",
      "Epoch: [2][2600/2851] Elapsed 21m 49s (remain 2m 5s) Loss: 0.0099(0.0061) Grad: 52577.7617  LR: 0.000014  \n",
      "Epoch: [2][2700/2851] Elapsed 22m 39s (remain 1m 15s) Loss: 0.0001(0.0061) Grad: 511.5473  LR: 0.000014  \n",
      "Epoch: [2][2800/2851] Elapsed 23m 30s (remain 0m 25s) Loss: 0.0024(0.0061) Grad: 9260.5088  LR: 0.000013  \n",
      "Epoch: [2][2850/2851] Elapsed 23m 55s (remain 0m 0s) Loss: 0.0010(0.0061) Grad: 3683.2942  LR: 0.000013  \n",
      "EVAL: [0/724] Elapsed 0m 0s (remain 6m 2s) Loss: 0.0005(0.0005) \n",
      "EVAL: [100/724] Elapsed 0m 28s (remain 2m 53s) Loss: 0.0052(0.0061) \n",
      "EVAL: [200/724] Elapsed 0m 55s (remain 2m 25s) Loss: 0.0000(0.0079) \n",
      "EVAL: [300/724] Elapsed 1m 23s (remain 1m 56s) Loss: 0.0001(0.0076) \n",
      "EVAL: [400/724] Elapsed 1m 50s (remain 1m 29s) Loss: 0.0000(0.0075) \n",
      "EVAL: [500/724] Elapsed 2m 17s (remain 1m 1s) Loss: 0.0095(0.0089) \n",
      "EVAL: [600/724] Elapsed 2m 45s (remain 0m 33s) Loss: 0.0005(0.0085) \n",
      "EVAL: [700/724] Elapsed 3m 13s (remain 0m 6s) Loss: 0.0000(0.0077) \n",
      "EVAL: [723/724] Elapsed 3m 18s (remain 0m 0s) Loss: 0.0000(0.0076) \n",
      "Epoch 2 - avg_train_loss: 0.0061  avg_val_loss: 0.0076  time: 1639s\n",
      "Epoch 2 - Score: 0.8824\n",
      "Epoch 2 - Save Best Score: 0.8824 Model\n",
      "Epoch: [3][0/2851] Elapsed 0m 0s (remain 34m 54s) Loss: 0.0027(0.0027) Grad: 7555.4385  LR: 0.000013  \n",
      "Epoch: [3][100/2851] Elapsed 0m 50s (remain 23m 5s) Loss: 0.0002(0.0045) Grad: 730.7227  LR: 0.000013  \n",
      "Epoch: [3][200/2851] Elapsed 1m 41s (remain 22m 19s) Loss: 0.0008(0.0044) Grad: 6996.3975  LR: 0.000013  \n",
      "Epoch: [3][300/2851] Elapsed 2m 32s (remain 21m 33s) Loss: 0.0016(0.0043) Grad: 4805.2012  LR: 0.000013  \n",
      "Epoch: [3][400/2851] Elapsed 3m 22s (remain 20m 38s) Loss: 0.0001(0.0046) Grad: 545.1218  LR: 0.000013  \n",
      "Epoch: [3][500/2851] Elapsed 4m 12s (remain 19m 44s) Loss: 0.0000(0.0046) Grad: 49.4501  LR: 0.000013  \n",
      "Epoch: [3][600/2851] Elapsed 5m 3s (remain 18m 54s) Loss: 0.0001(0.0047) Grad: 659.5797  LR: 0.000012  \n",
      "Epoch: [3][700/2851] Elapsed 5m 55s (remain 18m 8s) Loss: 0.0058(0.0047) Grad: 6610.1499  LR: 0.000012  \n",
      "Epoch: [3][800/2851] Elapsed 6m 45s (remain 17m 17s) Loss: 0.0031(0.0045) Grad: 12170.7549  LR: 0.000012  \n",
      "Epoch: [3][900/2851] Elapsed 7m 35s (remain 16m 25s) Loss: 0.0107(0.0045) Grad: 20621.9961  LR: 0.000012  \n",
      "Epoch: [3][1000/2851] Elapsed 8m 25s (remain 15m 34s) Loss: 0.0039(0.0045) Grad: 16608.3906  LR: 0.000012  \n",
      "Epoch: [3][1100/2851] Elapsed 9m 15s (remain 14m 42s) Loss: 0.0006(0.0045) Grad: 2339.3682  LR: 0.000012  \n",
      "Epoch: [3][1200/2851] Elapsed 10m 5s (remain 13m 52s) Loss: 0.0009(0.0045) Grad: 6552.6099  LR: 0.000011  \n",
      "Epoch: [3][1300/2851] Elapsed 10m 57s (remain 13m 3s) Loss: 0.0026(0.0045) Grad: 16960.3320  LR: 0.000011  \n",
      "Epoch: [3][1400/2851] Elapsed 11m 47s (remain 12m 12s) Loss: 0.0015(0.0045) Grad: 13128.3506  LR: 0.000011  \n",
      "Epoch: [3][1500/2851] Elapsed 12m 38s (remain 11m 22s) Loss: 0.0000(0.0045) Grad: 179.8828  LR: 0.000011  \n",
      "Epoch: [3][1600/2851] Elapsed 13m 28s (remain 10m 31s) Loss: 0.0003(0.0046) Grad: 2369.3210  LR: 0.000011  \n",
      "Epoch: [3][1700/2851] Elapsed 14m 18s (remain 9m 40s) Loss: 0.0419(0.0046) Grad: 38971.5156  LR: 0.000011  \n",
      "Epoch: [3][1800/2851] Elapsed 15m 8s (remain 8m 49s) Loss: 0.0092(0.0047) Grad: 24891.9492  LR: 0.000011  \n",
      "Epoch: [3][1900/2851] Elapsed 16m 0s (remain 8m 0s) Loss: 0.0027(0.0047) Grad: 8008.7163  LR: 0.000010  \n",
      "Epoch: [3][2000/2851] Elapsed 16m 50s (remain 7m 9s) Loss: 0.0000(0.0047) Grad: 170.7525  LR: 0.000010  \n",
      "Epoch: [3][2100/2851] Elapsed 17m 40s (remain 6m 18s) Loss: 0.0032(0.0047) Grad: 16724.3887  LR: 0.000010  \n",
      "Epoch: [3][2200/2851] Elapsed 18m 30s (remain 5m 27s) Loss: 0.0020(0.0047) Grad: 8378.2627  LR: 0.000010  \n",
      "Epoch: [3][2300/2851] Elapsed 19m 20s (remain 4m 37s) Loss: 0.0024(0.0047) Grad: 9616.6367  LR: 0.000010  \n",
      "Epoch: [3][2400/2851] Elapsed 20m 12s (remain 3m 47s) Loss: 0.0010(0.0048) Grad: 2897.9995  LR: 0.000010  \n",
      "Epoch: [3][2500/2851] Elapsed 21m 2s (remain 2m 56s) Loss: 0.0136(0.0048) Grad: 26241.6680  LR: 0.000009  \n",
      "Epoch: [3][2600/2851] Elapsed 21m 52s (remain 2m 6s) Loss: 0.0004(0.0049) Grad: 2497.1360  LR: 0.000009  \n",
      "Epoch: [3][2700/2851] Elapsed 22m 42s (remain 1m 15s) Loss: 0.0104(0.0049) Grad: 14559.8125  LR: 0.000009  \n",
      "Epoch: [3][2800/2851] Elapsed 23m 32s (remain 0m 25s) Loss: 0.0003(0.0049) Grad: 1660.8987  LR: 0.000009  \n",
      "Epoch: [3][2850/2851] Elapsed 23m 57s (remain 0m 0s) Loss: 0.0027(0.0049) Grad: 25525.8281  LR: 0.000009  \n",
      "EVAL: [0/724] Elapsed 0m 0s (remain 6m 35s) Loss: 0.0009(0.0009) \n",
      "EVAL: [100/724] Elapsed 0m 28s (remain 2m 57s) Loss: 0.0057(0.0061) \n",
      "EVAL: [200/724] Elapsed 0m 56s (remain 2m 27s) Loss: 0.0000(0.0076) \n",
      "EVAL: [300/724] Elapsed 1m 24s (remain 1m 58s) Loss: 0.0002(0.0075) \n",
      "EVAL: [400/724] Elapsed 1m 51s (remain 1m 29s) Loss: 0.0000(0.0074) \n",
      "EVAL: [500/724] Elapsed 2m 18s (remain 1m 1s) Loss: 0.0124(0.0089) \n",
      "EVAL: [600/724] Elapsed 2m 46s (remain 0m 34s) Loss: 0.0002(0.0085) \n",
      "EVAL: [700/724] Elapsed 3m 14s (remain 0m 6s) Loss: 0.0000(0.0078) \n",
      "EVAL: [723/724] Elapsed 3m 20s (remain 0m 0s) Loss: 0.0000(0.0077) \n",
      "Epoch 3 - avg_train_loss: 0.0049  avg_val_loss: 0.0077  time: 1643s\n",
      "Epoch 3 - Score: 0.8876\n",
      "Epoch 3 - Save Best Score: 0.8876 Model\n",
      "Epoch: [4][0/2851] Elapsed 0m 0s (remain 35m 52s) Loss: 0.0037(0.0037) Grad: 19540.8027  LR: 0.000009  \n",
      "Epoch: [4][100/2851] Elapsed 0m 50s (remain 23m 5s) Loss: 0.0031(0.0065) Grad: 16010.1328  LR: 0.000009  \n",
      "Epoch: [4][200/2851] Elapsed 1m 41s (remain 22m 17s) Loss: 0.0009(0.0053) Grad: 2517.0237  LR: 0.000009  \n",
      "Epoch: [4][300/2851] Elapsed 2m 32s (remain 21m 33s) Loss: 0.0000(0.0049) Grad: 113.8831  LR: 0.000008  \n",
      "Epoch: [4][400/2851] Elapsed 3m 22s (remain 20m 40s) Loss: 0.0022(0.0044) Grad: 17406.1172  LR: 0.000008  \n",
      "Epoch: [4][500/2851] Elapsed 4m 12s (remain 19m 45s) Loss: 0.0017(0.0046) Grad: 5381.1040  LR: 0.000008  \n",
      "Epoch: [4][600/2851] Elapsed 5m 2s (remain 18m 52s) Loss: 0.0004(0.0045) Grad: 2289.3596  LR: 0.000008  \n",
      "Epoch: [4][700/2851] Elapsed 5m 52s (remain 17m 59s) Loss: 0.0020(0.0043) Grad: 11422.8721  LR: 0.000008  \n",
      "Epoch: [4][800/2851] Elapsed 6m 42s (remain 17m 9s) Loss: 0.0014(0.0044) Grad: 5128.8516  LR: 0.000008  \n",
      "Epoch: [4][900/2851] Elapsed 7m 32s (remain 16m 19s) Loss: 0.0000(0.0045) Grad: 32.9780  LR: 0.000007  \n",
      "Epoch: [4][1000/2851] Elapsed 8m 22s (remain 15m 28s) Loss: 0.0000(0.0043) Grad: 406.2218  LR: 0.000007  \n",
      "Epoch: [4][1100/2851] Elapsed 9m 12s (remain 14m 38s) Loss: 0.0000(0.0042) Grad: 17.5970  LR: 0.000007  \n",
      "Epoch: [4][1200/2851] Elapsed 10m 2s (remain 13m 47s) Loss: 0.0001(0.0042) Grad: 696.8816  LR: 0.000007  \n",
      "Epoch: [4][1300/2851] Elapsed 10m 52s (remain 12m 57s) Loss: 0.0019(0.0042) Grad: 9026.2705  LR: 0.000007  \n",
      "Epoch: [4][1400/2851] Elapsed 11m 42s (remain 12m 7s) Loss: 0.0024(0.0042) Grad: 37633.9414  LR: 0.000007  \n",
      "Epoch: [4][1500/2851] Elapsed 12m 33s (remain 11m 17s) Loss: 0.0000(0.0042) Grad: 30.3035  LR: 0.000007  \n",
      "Epoch: [4][1600/2851] Elapsed 13m 23s (remain 10m 27s) Loss: 0.0000(0.0041) Grad: 32.6890  LR: 0.000006  \n",
      "Epoch: [4][1700/2851] Elapsed 14m 13s (remain 9m 37s) Loss: 0.0015(0.0041) Grad: 15765.3838  LR: 0.000006  \n",
      "Epoch: [4][1800/2851] Elapsed 15m 3s (remain 8m 46s) Loss: 0.0030(0.0041) Grad: 22992.2129  LR: 0.000006  \n",
      "Epoch: [4][1900/2851] Elapsed 15m 53s (remain 7m 56s) Loss: 0.0001(0.0041) Grad: 478.0113  LR: 0.000006  \n",
      "Epoch: [4][2000/2851] Elapsed 16m 43s (remain 7m 6s) Loss: 0.0000(0.0040) Grad: 11.7987  LR: 0.000006  \n",
      "Epoch: [4][2100/2851] Elapsed 17m 35s (remain 6m 16s) Loss: 0.0009(0.0040) Grad: 5348.7939  LR: 0.000006  \n",
      "Epoch: [4][2200/2851] Elapsed 18m 28s (remain 5m 27s) Loss: 0.0000(0.0041) Grad: 150.8800  LR: 0.000005  \n",
      "Epoch: [4][2300/2851] Elapsed 19m 17s (remain 4m 36s) Loss: 0.0019(0.0040) Grad: 5818.7832  LR: 0.000005  \n",
      "Epoch: [4][2400/2851] Elapsed 20m 7s (remain 3m 46s) Loss: 0.0037(0.0040) Grad: 14459.0693  LR: 0.000005  \n",
      "Epoch: [4][2500/2851] Elapsed 20m 57s (remain 2m 55s) Loss: 0.0019(0.0040) Grad: 8232.9805  LR: 0.000005  \n",
      "Epoch: [4][2600/2851] Elapsed 21m 47s (remain 2m 5s) Loss: 0.0025(0.0041) Grad: 4862.8784  LR: 0.000005  \n",
      "Epoch: [4][2700/2851] Elapsed 22m 38s (remain 1m 15s) Loss: 0.0080(0.0040) Grad: 26980.5449  LR: 0.000005  \n",
      "Epoch: [4][2800/2851] Elapsed 23m 29s (remain 0m 25s) Loss: 0.0006(0.0040) Grad: 5013.1401  LR: 0.000005  \n",
      "Epoch: [4][2850/2851] Elapsed 23m 54s (remain 0m 0s) Loss: 0.0020(0.0040) Grad: 7481.3970  LR: 0.000004  \n",
      "EVAL: [0/724] Elapsed 0m 0s (remain 6m 19s) Loss: 0.0014(0.0014) \n",
      "EVAL: [100/724] Elapsed 0m 28s (remain 2m 53s) Loss: 0.0039(0.0062) \n",
      "EVAL: [200/724] Elapsed 0m 56s (remain 2m 27s) Loss: 0.0000(0.0080) \n",
      "EVAL: [300/724] Elapsed 1m 24s (remain 1m 59s) Loss: 0.0005(0.0079) \n",
      "EVAL: [400/724] Elapsed 1m 52s (remain 1m 30s) Loss: 0.0000(0.0080) \n",
      "EVAL: [500/724] Elapsed 2m 20s (remain 1m 2s) Loss: 0.0135(0.0097) \n",
      "EVAL: [600/724] Elapsed 2m 47s (remain 0m 34s) Loss: 0.0003(0.0093) \n",
      "EVAL: [700/724] Elapsed 3m 14s (remain 0m 6s) Loss: 0.0000(0.0084) \n",
      "EVAL: [723/724] Elapsed 3m 20s (remain 0m 0s) Loss: 0.0000(0.0083) \n",
      "Epoch 4 - avg_train_loss: 0.0040  avg_val_loss: 0.0083  time: 1640s\n",
      "Epoch 4 - Score: 0.8867\n",
      "Epoch: [5][0/2851] Elapsed 0m 0s (remain 33m 57s) Loss: 0.0001(0.0001) Grad: 1142.6442  LR: 0.000004  \n",
      "Epoch: [5][100/2851] Elapsed 0m 50s (remain 22m 55s) Loss: 0.0000(0.0019) Grad: 151.3413  LR: 0.000004  \n",
      "Epoch: [5][200/2851] Elapsed 1m 40s (remain 22m 9s) Loss: 0.0091(0.0027) Grad: 15929.0752  LR: 0.000004  \n",
      "Epoch: [5][300/2851] Elapsed 2m 31s (remain 21m 23s) Loss: 0.0326(0.0033) Grad: 35453.1172  LR: 0.000004  \n",
      "Epoch: [5][400/2851] Elapsed 3m 21s (remain 20m 32s) Loss: 0.0003(0.0031) Grad: 2894.8716  LR: 0.000004  \n",
      "Epoch: [5][500/2851] Elapsed 4m 11s (remain 19m 40s) Loss: 0.0000(0.0035) Grad: 18.3926  LR: 0.000004  \n",
      "Epoch: [5][600/2851] Elapsed 5m 1s (remain 18m 50s) Loss: 0.0001(0.0033) Grad: 574.2301  LR: 0.000004  \n",
      "Epoch: [5][700/2851] Elapsed 5m 52s (remain 18m 1s) Loss: 0.0061(0.0033) Grad: 22290.6367  LR: 0.000003  \n",
      "Epoch: [5][800/2851] Elapsed 6m 45s (remain 17m 16s) Loss: 0.0001(0.0034) Grad: 1768.1775  LR: 0.000003  \n",
      "Epoch: [5][900/2851] Elapsed 7m 34s (remain 16m 24s) Loss: 0.0006(0.0034) Grad: 9503.1006  LR: 0.000003  \n",
      "Epoch: [5][1000/2851] Elapsed 8m 24s (remain 15m 32s) Loss: 0.0003(0.0034) Grad: 4454.1914  LR: 0.000003  \n",
      "Epoch: [5][1100/2851] Elapsed 9m 14s (remain 14m 42s) Loss: 0.0000(0.0035) Grad: 33.1114  LR: 0.000003  \n",
      "Epoch: [5][1200/2851] Elapsed 10m 5s (remain 13m 51s) Loss: 0.0000(0.0035) Grad: 25.5073  LR: 0.000003  \n",
      "Epoch: [5][1300/2851] Elapsed 10m 56s (remain 13m 2s) Loss: 0.0156(0.0035) Grad: 54959.7148  LR: 0.000002  \n",
      "Epoch: [5][1400/2851] Elapsed 11m 46s (remain 12m 11s) Loss: 0.0001(0.0036) Grad: 2053.3909  LR: 0.000002  \n",
      "Epoch: [5][1500/2851] Elapsed 12m 36s (remain 11m 20s) Loss: 0.0015(0.0036) Grad: 9788.1807  LR: 0.000002  \n",
      "Epoch: [5][1600/2851] Elapsed 13m 28s (remain 10m 31s) Loss: 0.0016(0.0035) Grad: 14462.2373  LR: 0.000002  \n",
      "Epoch: [5][1700/2851] Elapsed 14m 19s (remain 9m 40s) Loss: 0.0200(0.0035) Grad: 56369.5039  LR: 0.000002  \n",
      "Epoch: [5][1800/2851] Elapsed 15m 9s (remain 8m 50s) Loss: 0.0660(0.0035) Grad: 110098.7500  LR: 0.000002  \n",
      "Epoch: [5][1900/2851] Elapsed 16m 1s (remain 8m 0s) Loss: 0.0000(0.0034) Grad: 8.1758  LR: 0.000001  \n",
      "Epoch: [5][2000/2851] Elapsed 16m 50s (remain 7m 9s) Loss: 0.0000(0.0033) Grad: 62.9018  LR: 0.000001  \n",
      "Epoch: [5][2100/2851] Elapsed 17m 41s (remain 6m 19s) Loss: 0.0000(0.0034) Grad: 112.1674  LR: 0.000001  \n",
      "Epoch: [5][2200/2851] Elapsed 18m 31s (remain 5m 28s) Loss: 0.0027(0.0034) Grad: 11460.8105  LR: 0.000001  \n",
      "Epoch: [5][2300/2851] Elapsed 19m 21s (remain 4m 37s) Loss: 0.0000(0.0034) Grad: 19.9673  LR: 0.000001  \n",
      "Epoch: [5][2400/2851] Elapsed 20m 13s (remain 3m 47s) Loss: 0.0008(0.0034) Grad: 13368.2988  LR: 0.000001  \n",
      "Epoch: [5][2500/2851] Elapsed 21m 3s (remain 2m 56s) Loss: 0.0000(0.0034) Grad: 16.3587  LR: 0.000001  \n",
      "Epoch: [5][2600/2851] Elapsed 21m 53s (remain 2m 6s) Loss: 0.0000(0.0034) Grad: 5.0905  LR: 0.000000  \n",
      "Epoch: [5][2700/2851] Elapsed 22m 43s (remain 1m 15s) Loss: 0.0098(0.0034) Grad: 31179.9082  LR: 0.000000  \n",
      "Epoch: [5][2800/2851] Elapsed 23m 33s (remain 0m 25s) Loss: 0.0000(0.0034) Grad: 25.6692  LR: 0.000000  \n",
      "Epoch: [5][2850/2851] Elapsed 23m 58s (remain 0m 0s) Loss: 0.0000(0.0034) Grad: 65.4154  LR: 0.000000  \n",
      "EVAL: [0/724] Elapsed 0m 0s (remain 6m 12s) Loss: 0.0008(0.0008) \n",
      "EVAL: [100/724] Elapsed 0m 28s (remain 2m 58s) Loss: 0.0058(0.0070) \n",
      "EVAL: [200/724] Elapsed 0m 56s (remain 2m 27s) Loss: 0.0000(0.0092) \n",
      "EVAL: [300/724] Elapsed 1m 24s (remain 1m 58s) Loss: 0.0211(0.0091) \n",
      "EVAL: [400/724] Elapsed 1m 51s (remain 1m 29s) Loss: 0.0000(0.0091) \n",
      "EVAL: [500/724] Elapsed 2m 18s (remain 1m 1s) Loss: 0.0124(0.0110) \n",
      "EVAL: [600/724] Elapsed 2m 46s (remain 0m 34s) Loss: 0.0002(0.0105) \n",
      "EVAL: [700/724] Elapsed 3m 15s (remain 0m 6s) Loss: 0.0000(0.0096) \n",
      "EVAL: [723/724] Elapsed 3m 21s (remain 0m 0s) Loss: 0.0000(0.0095) \n",
      "Epoch 5 - avg_train_loss: 0.0034  avg_val_loss: 0.0095  time: 1644s\n",
      "Epoch 5 - Score: 0.8890\n",
      "Epoch 5 - Save Best Score: 0.8890 Model\n",
      "========== fold: 2 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2871] Elapsed 0m 0s (remain 34m 9s) Loss: 0.5893(0.5893) Grad: inf  LR: 0.000000  \n",
      "Epoch: [1][100/2871] Elapsed 0m 50s (remain 23m 8s) Loss: 0.2947(0.6233) Grad: 28601.5312  LR: 0.000001  \n",
      "Epoch: [1][200/2871] Elapsed 1m 41s (remain 22m 29s) Loss: 0.0723(0.4437) Grad: 4033.6343  LR: 0.000003  \n",
      "Epoch: [1][300/2871] Elapsed 2m 32s (remain 21m 40s) Loss: 0.0330(0.3123) Grad: 631.9304  LR: 0.000004  \n",
      "Epoch: [1][400/2871] Elapsed 3m 22s (remain 20m 45s) Loss: 0.0317(0.2444) Grad: 858.3969  LR: 0.000006  \n",
      "Epoch: [1][500/2871] Elapsed 4m 11s (remain 19m 51s) Loss: 0.0106(0.2023) Grad: 753.1826  LR: 0.000007  \n",
      "Epoch: [1][600/2871] Elapsed 5m 3s (remain 19m 5s) Loss: 0.0069(0.1719) Grad: 657.5070  LR: 0.000008  \n",
      "Epoch: [1][700/2871] Elapsed 5m 53s (remain 18m 13s) Loss: 0.0078(0.1498) Grad: 1486.3894  LR: 0.000010  \n",
      "Epoch: [1][800/2871] Elapsed 6m 44s (remain 17m 24s) Loss: 0.0101(0.1326) Grad: 1301.4343  LR: 0.000011  \n",
      "Epoch: [1][900/2871] Elapsed 7m 35s (remain 16m 36s) Loss: 0.0029(0.1190) Grad: 507.1531  LR: 0.000013  \n",
      "Epoch: [1][1000/2871] Elapsed 8m 25s (remain 15m 45s) Loss: 0.0065(0.1082) Grad: 1213.3584  LR: 0.000014  \n",
      "Epoch: [1][1100/2871] Elapsed 9m 15s (remain 14m 53s) Loss: 0.0187(0.0995) Grad: 3370.5742  LR: 0.000015  \n",
      "Epoch: [1][1200/2871] Elapsed 10m 6s (remain 14m 2s) Loss: 0.0053(0.0921) Grad: 512.3076  LR: 0.000017  \n",
      "Epoch: [1][1300/2871] Elapsed 10m 57s (remain 13m 13s) Loss: 0.0024(0.0856) Grad: 701.2459  LR: 0.000018  \n",
      "Epoch: [1][1400/2871] Elapsed 11m 47s (remain 12m 21s) Loss: 0.0065(0.0803) Grad: 801.3925  LR: 0.000020  \n",
      "Epoch: [1][1500/2871] Elapsed 12m 37s (remain 11m 31s) Loss: 0.0019(0.0756) Grad: 739.2960  LR: 0.000020  \n",
      "Epoch: [1][1600/2871] Elapsed 13m 26s (remain 10m 40s) Loss: 0.0006(0.0714) Grad: 130.6361  LR: 0.000020  \n",
      "Epoch: [1][1700/2871] Elapsed 14m 17s (remain 9m 49s) Loss: 0.0065(0.0677) Grad: 1062.0719  LR: 0.000020  \n",
      "Epoch: [1][1800/2871] Elapsed 15m 7s (remain 8m 58s) Loss: 0.0022(0.0643) Grad: 336.5680  LR: 0.000019  \n",
      "Epoch: [1][1900/2871] Elapsed 15m 56s (remain 8m 8s) Loss: 0.0015(0.0614) Grad: 316.9307  LR: 0.000019  \n",
      "Epoch: [1][2000/2871] Elapsed 16m 46s (remain 7m 17s) Loss: 0.0023(0.0587) Grad: 358.2885  LR: 0.000019  \n",
      "Epoch: [1][2100/2871] Elapsed 17m 36s (remain 6m 27s) Loss: 0.0029(0.0563) Grad: 313.1021  LR: 0.000019  \n",
      "Epoch: [1][2200/2871] Elapsed 18m 27s (remain 5m 37s) Loss: 0.0039(0.0543) Grad: 558.5816  LR: 0.000019  \n",
      "Epoch: [1][2300/2871] Elapsed 19m 16s (remain 4m 46s) Loss: 0.0054(0.0523) Grad: 642.6106  LR: 0.000019  \n",
      "Epoch: [1][2400/2871] Elapsed 20m 6s (remain 3m 56s) Loss: 0.0007(0.0504) Grad: 156.3119  LR: 0.000019  \n",
      "Epoch: [1][2500/2871] Elapsed 20m 55s (remain 3m 5s) Loss: 0.0002(0.0488) Grad: 54.3085  LR: 0.000018  \n",
      "Epoch: [1][2600/2871] Elapsed 21m 45s (remain 2m 15s) Loss: 0.0003(0.0471) Grad: 51.9723  LR: 0.000018  \n",
      "Epoch: [1][2700/2871] Elapsed 22m 35s (remain 1m 25s) Loss: 0.0058(0.0457) Grad: 1027.6576  LR: 0.000018  \n",
      "Epoch: [1][2800/2871] Elapsed 23m 25s (remain 0m 35s) Loss: 0.0060(0.0443) Grad: 589.7698  LR: 0.000018  \n",
      "Epoch: [1][2870/2871] Elapsed 24m 0s (remain 0m 0s) Loss: 0.0227(0.0434) Grad: 2645.5122  LR: 0.000018  \n",
      "EVAL: [0/704] Elapsed 0m 0s (remain 6m 33s) Loss: 0.0029(0.0029) \n",
      "EVAL: [100/704] Elapsed 0m 27s (remain 2m 46s) Loss: 0.0117(0.0088) \n",
      "EVAL: [200/704] Elapsed 0m 55s (remain 2m 18s) Loss: 0.0003(0.0078) \n",
      "EVAL: [300/704] Elapsed 1m 23s (remain 1m 51s) Loss: 0.0008(0.0073) \n",
      "EVAL: [400/704] Elapsed 1m 50s (remain 1m 23s) Loss: 0.0223(0.0081) \n",
      "EVAL: [500/704] Elapsed 2m 18s (remain 0m 56s) Loss: 0.0280(0.0086) \n",
      "EVAL: [600/704] Elapsed 2m 46s (remain 0m 28s) Loss: 0.0014(0.0086) \n",
      "EVAL: [700/704] Elapsed 3m 13s (remain 0m 0s) Loss: 0.0015(0.0081) \n",
      "EVAL: [703/704] Elapsed 3m 14s (remain 0m 0s) Loss: 0.0002(0.0081) \n",
      "Epoch 1 - avg_train_loss: 0.0434  avg_val_loss: 0.0081  time: 1640s\n",
      "Epoch 1 - Score: 0.8408\n",
      "Epoch 1 - Save Best Score: 0.8408 Model\n",
      "Epoch: [2][0/2871] Elapsed 0m 0s (remain 33m 16s) Loss: 0.0018(0.0018) Grad: 3290.4060  LR: 0.000018  \n",
      "Epoch: [2][100/2871] Elapsed 0m 50s (remain 23m 10s) Loss: 0.0046(0.0054) Grad: 6902.5767  LR: 0.000018  \n",
      "Epoch: [2][200/2871] Elapsed 1m 40s (remain 22m 17s) Loss: 0.0067(0.0053) Grad: 8700.2334  LR: 0.000017  \n",
      "Epoch: [2][300/2871] Elapsed 2m 31s (remain 21m 31s) Loss: 0.0033(0.0051) Grad: 13130.7275  LR: 0.000017  \n",
      "Epoch: [2][400/2871] Elapsed 3m 21s (remain 20m 39s) Loss: 0.0007(0.0055) Grad: 3041.0425  LR: 0.000017  \n",
      "Epoch: [2][500/2871] Elapsed 4m 10s (remain 19m 47s) Loss: 0.0004(0.0060) Grad: 1706.2382  LR: 0.000017  \n",
      "Epoch: [2][600/2871] Elapsed 5m 1s (remain 18m 57s) Loss: 0.0083(0.0061) Grad: 9473.9180  LR: 0.000017  \n",
      "Epoch: [2][700/2871] Elapsed 5m 52s (remain 18m 11s) Loss: 0.0003(0.0061) Grad: 3382.1531  LR: 0.000017  \n",
      "Epoch: [2][800/2871] Elapsed 6m 43s (remain 17m 21s) Loss: 0.0012(0.0060) Grad: 6474.6108  LR: 0.000017  \n",
      "Epoch: [2][900/2871] Elapsed 7m 32s (remain 16m 30s) Loss: 0.0333(0.0059) Grad: 52731.1641  LR: 0.000016  \n",
      "Epoch: [2][1000/2871] Elapsed 8m 22s (remain 15m 39s) Loss: 0.0105(0.0058) Grad: 23917.5996  LR: 0.000016  \n",
      "Epoch: [2][1100/2871] Elapsed 9m 13s (remain 14m 49s) Loss: 0.0004(0.0060) Grad: 9374.2266  LR: 0.000016  \n",
      "Epoch: [2][1200/2871] Elapsed 10m 4s (remain 14m 0s) Loss: 0.0135(0.0058) Grad: 21251.5488  LR: 0.000016  \n",
      "Epoch: [2][1300/2871] Elapsed 10m 54s (remain 13m 10s) Loss: 0.0022(0.0060) Grad: 4879.8540  LR: 0.000016  \n",
      "Epoch: [2][1400/2871] Elapsed 11m 44s (remain 12m 19s) Loss: 0.0041(0.0060) Grad: 15337.1924  LR: 0.000016  \n",
      "Epoch: [2][1500/2871] Elapsed 12m 34s (remain 11m 28s) Loss: 0.0037(0.0060) Grad: 10444.2676  LR: 0.000015  \n",
      "Epoch: [2][1600/2871] Elapsed 13m 25s (remain 10m 39s) Loss: 0.0011(0.0059) Grad: 11351.4668  LR: 0.000015  \n",
      "Epoch: [2][1700/2871] Elapsed 14m 18s (remain 9m 50s) Loss: 0.0002(0.0059) Grad: 1295.5303  LR: 0.000015  \n",
      "Epoch: [2][1800/2871] Elapsed 15m 8s (remain 8m 59s) Loss: 0.0000(0.0059) Grad: 41.3819  LR: 0.000015  \n",
      "Epoch: [2][1900/2871] Elapsed 15m 59s (remain 8m 9s) Loss: 0.0000(0.0059) Grad: 21.2092  LR: 0.000015  \n",
      "Epoch: [2][2000/2871] Elapsed 16m 50s (remain 7m 19s) Loss: 0.0082(0.0059) Grad: 14163.9033  LR: 0.000015  \n",
      "Epoch: [2][2100/2871] Elapsed 17m 40s (remain 6m 28s) Loss: 0.0002(0.0059) Grad: 853.1636  LR: 0.000015  \n",
      "Epoch: [2][2200/2871] Elapsed 18m 30s (remain 5m 37s) Loss: 0.0004(0.0059) Grad: 1488.8107  LR: 0.000014  \n",
      "Epoch: [2][2300/2871] Elapsed 19m 19s (remain 4m 47s) Loss: 0.0019(0.0059) Grad: 9897.3555  LR: 0.000014  \n",
      "Epoch: [2][2400/2871] Elapsed 20m 10s (remain 3m 56s) Loss: 0.0137(0.0058) Grad: 48366.8086  LR: 0.000014  \n",
      "Epoch: [2][2500/2871] Elapsed 21m 0s (remain 3m 6s) Loss: 0.0005(0.0058) Grad: 2072.4116  LR: 0.000014  \n",
      "Epoch: [2][2600/2871] Elapsed 21m 49s (remain 2m 15s) Loss: 0.0001(0.0058) Grad: 554.2521  LR: 0.000014  \n",
      "Epoch: [2][2700/2871] Elapsed 22m 40s (remain 1m 25s) Loss: 0.0010(0.0058) Grad: 3830.4556  LR: 0.000014  \n",
      "Epoch: [2][2800/2871] Elapsed 23m 30s (remain 0m 35s) Loss: 0.0004(0.0058) Grad: 1761.2513  LR: 0.000013  \n",
      "Epoch: [2][2870/2871] Elapsed 24m 6s (remain 0m 0s) Loss: 0.0058(0.0058) Grad: 18962.8301  LR: 0.000013  \n",
      "EVAL: [0/704] Elapsed 0m 0s (remain 6m 19s) Loss: 0.0020(0.0020) \n",
      "EVAL: [100/704] Elapsed 0m 27s (remain 2m 45s) Loss: 0.0045(0.0096) \n",
      "EVAL: [200/704] Elapsed 0m 55s (remain 2m 18s) Loss: 0.0000(0.0079) \n",
      "EVAL: [300/704] Elapsed 1m 22s (remain 1m 50s) Loss: 0.0001(0.0072) \n",
      "EVAL: [400/704] Elapsed 1m 50s (remain 1m 23s) Loss: 0.0159(0.0081) \n",
      "EVAL: [500/704] Elapsed 2m 18s (remain 0m 55s) Loss: 0.0092(0.0088) \n",
      "EVAL: [600/704] Elapsed 2m 46s (remain 0m 28s) Loss: 0.0000(0.0091) \n",
      "EVAL: [700/704] Elapsed 3m 14s (remain 0m 0s) Loss: 0.0000(0.0084) \n",
      "EVAL: [703/704] Elapsed 3m 15s (remain 0m 0s) Loss: 0.0000(0.0084) \n",
      "Epoch 2 - avg_train_loss: 0.0058  avg_val_loss: 0.0084  time: 1647s\n",
      "Epoch 2 - Score: 0.8725\n",
      "Epoch 2 - Save Best Score: 0.8725 Model\n",
      "Epoch: [3][0/2871] Elapsed 0m 0s (remain 35m 51s) Loss: 0.0001(0.0001) Grad: 1095.3976  LR: 0.000013  \n",
      "Epoch: [3][100/2871] Elapsed 0m 50s (remain 23m 4s) Loss: 0.0000(0.0056) Grad: 228.6165  LR: 0.000013  \n",
      "Epoch: [3][200/2871] Elapsed 1m 40s (remain 22m 19s) Loss: 0.0000(0.0059) Grad: 104.8743  LR: 0.000013  \n",
      "Epoch: [3][300/2871] Elapsed 2m 31s (remain 21m 34s) Loss: 0.0025(0.0051) Grad: 8461.8369  LR: 0.000013  \n",
      "Epoch: [3][400/2871] Elapsed 3m 21s (remain 20m 42s) Loss: 0.0259(0.0051) Grad: 40262.9219  LR: 0.000013  \n",
      "Epoch: [3][500/2871] Elapsed 4m 11s (remain 19m 51s) Loss: 0.0001(0.0049) Grad: 812.5526  LR: 0.000013  \n",
      "Epoch: [3][600/2871] Elapsed 5m 1s (remain 18m 58s) Loss: 0.0039(0.0049) Grad: 21761.7637  LR: 0.000012  \n",
      "Epoch: [3][700/2871] Elapsed 5m 51s (remain 18m 7s) Loss: 0.0001(0.0050) Grad: 403.6228  LR: 0.000012  \n",
      "Epoch: [3][800/2871] Elapsed 6m 41s (remain 17m 17s) Loss: 0.0097(0.0050) Grad: 136054.4688  LR: 0.000012  \n",
      "Epoch: [3][900/2871] Elapsed 7m 31s (remain 16m 28s) Loss: 0.0000(0.0048) Grad: 38.2259  LR: 0.000012  \n",
      "Epoch: [3][1000/2871] Elapsed 8m 23s (remain 15m 41s) Loss: 0.0087(0.0047) Grad: 25892.8105  LR: 0.000012  \n",
      "Epoch: [3][1100/2871] Elapsed 9m 13s (remain 14m 50s) Loss: 0.0004(0.0047) Grad: 3333.7097  LR: 0.000012  \n",
      "Epoch: [3][1200/2871] Elapsed 10m 4s (remain 14m 0s) Loss: 0.0003(0.0047) Grad: 1522.4119  LR: 0.000011  \n",
      "Epoch: [3][1300/2871] Elapsed 10m 55s (remain 13m 11s) Loss: 0.0002(0.0046) Grad: 665.5629  LR: 0.000011  \n",
      "Epoch: [3][1400/2871] Elapsed 11m 45s (remain 12m 20s) Loss: 0.0001(0.0046) Grad: 848.2075  LR: 0.000011  \n",
      "Epoch: [3][1500/2871] Elapsed 12m 35s (remain 11m 29s) Loss: 0.0027(0.0047) Grad: 6022.5928  LR: 0.000011  \n",
      "Epoch: [3][1600/2871] Elapsed 13m 25s (remain 10m 39s) Loss: 0.0007(0.0048) Grad: 2124.9119  LR: 0.000011  \n",
      "Epoch: [3][1700/2871] Elapsed 14m 15s (remain 9m 48s) Loss: 0.0113(0.0047) Grad: 17478.5098  LR: 0.000011  \n",
      "Epoch: [3][1800/2871] Elapsed 15m 5s (remain 8m 57s) Loss: 0.0000(0.0047) Grad: 47.0337  LR: 0.000011  \n",
      "Epoch: [3][1900/2871] Elapsed 15m 56s (remain 8m 7s) Loss: 0.0248(0.0047) Grad: 53566.1680  LR: 0.000010  \n",
      "Epoch: [3][2000/2871] Elapsed 16m 47s (remain 7m 17s) Loss: 0.0001(0.0047) Grad: 331.6964  LR: 0.000010  \n",
      "Epoch: [3][2100/2871] Elapsed 17m 39s (remain 6m 28s) Loss: 0.0063(0.0047) Grad: 14574.6006  LR: 0.000010  \n",
      "Epoch: [3][2200/2871] Elapsed 18m 28s (remain 5m 37s) Loss: 0.0000(0.0047) Grad: 102.0383  LR: 0.000010  \n",
      "Epoch: [3][2300/2871] Elapsed 19m 18s (remain 4m 47s) Loss: 0.0001(0.0047) Grad: 627.2477  LR: 0.000010  \n",
      "Epoch: [3][2400/2871] Elapsed 20m 10s (remain 3m 56s) Loss: 0.0003(0.0048) Grad: 915.9575  LR: 0.000010  \n",
      "Epoch: [3][2500/2871] Elapsed 21m 0s (remain 3m 6s) Loss: 0.0041(0.0047) Grad: 13311.0557  LR: 0.000009  \n",
      "Epoch: [3][2600/2871] Elapsed 21m 49s (remain 2m 15s) Loss: 0.0000(0.0047) Grad: 276.5805  LR: 0.000009  \n",
      "Epoch: [3][2700/2871] Elapsed 22m 40s (remain 1m 25s) Loss: 0.0000(0.0048) Grad: 260.4330  LR: 0.000009  \n",
      "Epoch: [3][2800/2871] Elapsed 23m 30s (remain 0m 35s) Loss: 0.0001(0.0049) Grad: 663.6335  LR: 0.000009  \n",
      "Epoch: [3][2870/2871] Elapsed 24m 6s (remain 0m 0s) Loss: 0.0098(0.0049) Grad: 20488.6328  LR: 0.000009  \n",
      "EVAL: [0/704] Elapsed 0m 0s (remain 6m 29s) Loss: 0.0008(0.0008) \n",
      "EVAL: [100/704] Elapsed 0m 27s (remain 2m 47s) Loss: 0.0030(0.0084) \n",
      "EVAL: [200/704] Elapsed 0m 55s (remain 2m 19s) Loss: 0.0000(0.0074) \n",
      "EVAL: [300/704] Elapsed 1m 23s (remain 1m 52s) Loss: 0.0003(0.0070) \n",
      "EVAL: [400/704] Elapsed 1m 52s (remain 1m 24s) Loss: 0.0221(0.0079) \n",
      "EVAL: [500/704] Elapsed 2m 20s (remain 0m 56s) Loss: 0.0104(0.0087) \n",
      "EVAL: [600/704] Elapsed 2m 47s (remain 0m 28s) Loss: 0.0000(0.0092) \n",
      "EVAL: [700/704] Elapsed 3m 14s (remain 0m 0s) Loss: 0.0000(0.0086) \n",
      "EVAL: [703/704] Elapsed 3m 15s (remain 0m 0s) Loss: 0.0000(0.0086) \n",
      "Epoch 3 - avg_train_loss: 0.0049  avg_val_loss: 0.0086  time: 1647s\n",
      "Epoch 3 - Score: 0.8737\n",
      "Epoch 3 - Save Best Score: 0.8737 Model\n",
      "Epoch: [4][0/2871] Elapsed 0m 0s (remain 37m 6s) Loss: 0.0042(0.0042) Grad: 13620.1279  LR: 0.000009  \n",
      "Epoch: [4][100/2871] Elapsed 0m 50s (remain 23m 12s) Loss: 0.0306(0.0033) Grad: 25410.1191  LR: 0.000009  \n",
      "Epoch: [4][200/2871] Elapsed 1m 41s (remain 22m 28s) Loss: 0.0001(0.0035) Grad: 222.3531  LR: 0.000009  \n",
      "Epoch: [4][300/2871] Elapsed 2m 33s (remain 21m 51s) Loss: 0.0125(0.0035) Grad: 76169.4219  LR: 0.000008  \n",
      "Epoch: [4][400/2871] Elapsed 3m 26s (remain 21m 10s) Loss: 0.0012(0.0038) Grad: 4207.1060  LR: 0.000008  \n",
      "Epoch: [4][500/2871] Elapsed 4m 16s (remain 20m 12s) Loss: 0.0001(0.0038) Grad: 686.4202  LR: 0.000008  \n",
      "Epoch: [4][600/2871] Elapsed 5m 6s (remain 19m 16s) Loss: 0.0004(0.0038) Grad: 2319.1819  LR: 0.000008  \n",
      "Epoch: [4][700/2871] Elapsed 5m 56s (remain 18m 23s) Loss: 0.0000(0.0040) Grad: 181.1355  LR: 0.000008  \n",
      "Epoch: [4][800/2871] Elapsed 6m 47s (remain 17m 33s) Loss: 0.0000(0.0039) Grad: 82.5471  LR: 0.000008  \n",
      "Epoch: [4][900/2871] Elapsed 7m 37s (remain 16m 40s) Loss: 0.0027(0.0039) Grad: 10427.8486  LR: 0.000007  \n",
      "Epoch: [4][1000/2871] Elapsed 8m 28s (remain 15m 49s) Loss: 0.0000(0.0039) Grad: 64.8150  LR: 0.000007  \n",
      "Epoch: [4][1100/2871] Elapsed 9m 18s (remain 14m 57s) Loss: 0.0013(0.0038) Grad: 8409.4219  LR: 0.000007  \n",
      "Epoch: [4][1200/2871] Elapsed 10m 7s (remain 14m 5s) Loss: 0.0000(0.0037) Grad: 218.4840  LR: 0.000007  \n",
      "Epoch: [4][1300/2871] Elapsed 10m 58s (remain 13m 14s) Loss: 0.0033(0.0038) Grad: 12268.7051  LR: 0.000007  \n",
      "Epoch: [4][1400/2871] Elapsed 11m 48s (remain 12m 23s) Loss: 0.0000(0.0037) Grad: 415.5922  LR: 0.000007  \n",
      "Epoch: [4][1500/2871] Elapsed 12m 38s (remain 11m 32s) Loss: 0.0000(0.0037) Grad: 75.1156  LR: 0.000007  \n",
      "Epoch: [4][1600/2871] Elapsed 13m 28s (remain 10m 41s) Loss: 0.0078(0.0037) Grad: 8804.7881  LR: 0.000006  \n",
      "Epoch: [4][1700/2871] Elapsed 14m 17s (remain 9m 50s) Loss: 0.0001(0.0037) Grad: 201.2022  LR: 0.000006  \n",
      "Epoch: [4][1800/2871] Elapsed 15m 7s (remain 8m 59s) Loss: 0.0027(0.0037) Grad: 47765.8594  LR: 0.000006  \n",
      "Epoch: [4][1900/2871] Elapsed 15m 58s (remain 8m 9s) Loss: 0.0022(0.0037) Grad: 9383.8926  LR: 0.000006  \n",
      "Epoch: [4][2000/2871] Elapsed 16m 49s (remain 7m 18s) Loss: 0.0113(0.0037) Grad: 77389.5547  LR: 0.000006  \n",
      "Epoch: [4][2100/2871] Elapsed 17m 38s (remain 6m 28s) Loss: 0.0000(0.0037) Grad: 15.1629  LR: 0.000006  \n",
      "Epoch: [4][2200/2871] Elapsed 18m 30s (remain 5m 37s) Loss: 0.0093(0.0037) Grad: 20156.3496  LR: 0.000005  \n",
      "Epoch: [4][2300/2871] Elapsed 19m 20s (remain 4m 47s) Loss: 0.0000(0.0038) Grad: 240.3808  LR: 0.000005  \n",
      "Epoch: [4][2400/2871] Elapsed 20m 10s (remain 3m 56s) Loss: 0.0005(0.0039) Grad: 2895.8379  LR: 0.000005  \n",
      "Epoch: [4][2500/2871] Elapsed 21m 0s (remain 3m 6s) Loss: 0.0000(0.0039) Grad: 12.1296  LR: 0.000005  \n",
      "Epoch: [4][2600/2871] Elapsed 21m 50s (remain 2m 16s) Loss: 0.0008(0.0039) Grad: 3487.5071  LR: 0.000005  \n",
      "Epoch: [4][2700/2871] Elapsed 22m 41s (remain 1m 25s) Loss: 0.0000(0.0038) Grad: 16.0504  LR: 0.000005  \n",
      "Epoch: [4][2800/2871] Elapsed 23m 31s (remain 0m 35s) Loss: 0.0006(0.0039) Grad: 4894.7095  LR: 0.000005  \n",
      "Epoch: [4][2870/2871] Elapsed 24m 6s (remain 0m 0s) Loss: 0.0001(0.0039) Grad: 396.8728  LR: 0.000004  \n",
      "EVAL: [0/704] Elapsed 0m 0s (remain 5m 51s) Loss: 0.0007(0.0007) \n",
      "EVAL: [100/704] Elapsed 0m 27s (remain 2m 45s) Loss: 0.0081(0.0096) \n",
      "EVAL: [200/704] Elapsed 0m 55s (remain 2m 19s) Loss: 0.0000(0.0085) \n",
      "EVAL: [300/704] Elapsed 1m 23s (remain 1m 51s) Loss: 0.0004(0.0078) \n",
      "EVAL: [400/704] Elapsed 1m 50s (remain 1m 23s) Loss: 0.0144(0.0091) \n",
      "EVAL: [500/704] Elapsed 2m 18s (remain 0m 56s) Loss: 0.0110(0.0099) \n",
      "EVAL: [600/704] Elapsed 2m 45s (remain 0m 28s) Loss: 0.0000(0.0102) \n",
      "EVAL: [700/704] Elapsed 3m 13s (remain 0m 0s) Loss: 0.0000(0.0096) \n",
      "EVAL: [703/704] Elapsed 3m 14s (remain 0m 0s) Loss: 0.0000(0.0095) \n",
      "Epoch 4 - avg_train_loss: 0.0039  avg_val_loss: 0.0095  time: 1645s\n",
      "Epoch 4 - Score: 0.8756\n",
      "Epoch 4 - Save Best Score: 0.8756 Model\n",
      "Epoch: [5][0/2871] Elapsed 0m 0s (remain 37m 11s) Loss: 0.0073(0.0073) Grad: 28799.2480  LR: 0.000004  \n",
      "Epoch: [5][100/2871] Elapsed 0m 50s (remain 23m 6s) Loss: 0.0000(0.0025) Grad: 114.3192  LR: 0.000004  \n",
      "Epoch: [5][200/2871] Elapsed 1m 40s (remain 22m 14s) Loss: 0.0014(0.0031) Grad: 5093.5010  LR: 0.000004  \n",
      "Epoch: [5][300/2871] Elapsed 2m 32s (remain 21m 39s) Loss: 0.0000(0.0033) Grad: 38.2418  LR: 0.000004  \n",
      "Epoch: [5][400/2871] Elapsed 3m 21s (remain 20m 43s) Loss: 0.0004(0.0033) Grad: 2308.8477  LR: 0.000004  \n",
      "Epoch: [5][500/2871] Elapsed 4m 11s (remain 19m 51s) Loss: 0.0000(0.0035) Grad: 91.2074  LR: 0.000004  \n",
      "Epoch: [5][600/2871] Elapsed 5m 1s (remain 18m 59s) Loss: 0.0016(0.0033) Grad: 11860.1367  LR: 0.000004  \n",
      "Epoch: [5][700/2871] Elapsed 5m 51s (remain 18m 9s) Loss: 0.0030(0.0032) Grad: 54357.4648  LR: 0.000003  \n",
      "Epoch: [5][800/2871] Elapsed 6m 42s (remain 17m 20s) Loss: 0.0000(0.0032) Grad: 262.7747  LR: 0.000003  \n",
      "Epoch: [5][900/2871] Elapsed 7m 34s (remain 16m 34s) Loss: 0.0001(0.0033) Grad: 894.4243  LR: 0.000003  \n",
      "Epoch: [5][1000/2871] Elapsed 8m 26s (remain 15m 46s) Loss: 0.0000(0.0033) Grad: 202.6109  LR: 0.000003  \n",
      "Epoch: [5][1100/2871] Elapsed 9m 16s (remain 14m 55s) Loss: 0.0001(0.0032) Grad: 443.0345  LR: 0.000003  \n",
      "Epoch: [5][1200/2871] Elapsed 10m 6s (remain 14m 3s) Loss: 0.0012(0.0031) Grad: 8171.8179  LR: 0.000003  \n",
      "Epoch: [5][1300/2871] Elapsed 10m 57s (remain 13m 12s) Loss: 0.0047(0.0032) Grad: 14049.8320  LR: 0.000002  \n",
      "Epoch: [5][1400/2871] Elapsed 11m 47s (remain 12m 22s) Loss: 0.0047(0.0033) Grad: 11439.0186  LR: 0.000002  \n",
      "Epoch: [5][1500/2871] Elapsed 12m 37s (remain 11m 31s) Loss: 0.0024(0.0033) Grad: 7934.8677  LR: 0.000002  \n",
      "Epoch: [5][1600/2871] Elapsed 13m 26s (remain 10m 40s) Loss: 0.0000(0.0032) Grad: 114.2214  LR: 0.000002  \n",
      "Epoch: [5][1700/2871] Elapsed 14m 17s (remain 9m 49s) Loss: 0.0000(0.0032) Grad: 19.6060  LR: 0.000002  \n",
      "Epoch: [5][1800/2871] Elapsed 15m 7s (remain 8m 58s) Loss: 0.0025(0.0032) Grad: 8662.9463  LR: 0.000002  \n",
      "Epoch: [5][1900/2871] Elapsed 15m 56s (remain 8m 8s) Loss: 0.0016(0.0032) Grad: 6107.3247  LR: 0.000002  \n",
      "Epoch: [5][2000/2871] Elapsed 16m 46s (remain 7m 17s) Loss: 0.0076(0.0033) Grad: 56610.4492  LR: 0.000001  \n",
      "Epoch: [5][2100/2871] Elapsed 17m 36s (remain 6m 27s) Loss: 0.0001(0.0033) Grad: 5371.7163  LR: 0.000001  \n",
      "Epoch: [5][2200/2871] Elapsed 18m 26s (remain 5m 36s) Loss: 0.0005(0.0033) Grad: 3504.7432  LR: 0.000001  \n",
      "Epoch: [5][2300/2871] Elapsed 19m 17s (remain 4m 46s) Loss: 0.0004(0.0033) Grad: 4490.8896  LR: 0.000001  \n",
      "Epoch: [5][2400/2871] Elapsed 20m 7s (remain 3m 56s) Loss: 0.0006(0.0033) Grad: 5994.5410  LR: 0.000001  \n",
      "Epoch: [5][2500/2871] Elapsed 20m 57s (remain 3m 6s) Loss: 0.0000(0.0033) Grad: 40.8436  LR: 0.000001  \n",
      "Epoch: [5][2600/2871] Elapsed 21m 47s (remain 2m 15s) Loss: 0.0001(0.0034) Grad: 656.8152  LR: 0.000000  \n",
      "Epoch: [5][2700/2871] Elapsed 22m 38s (remain 1m 25s) Loss: 0.0050(0.0034) Grad: 6998.0498  LR: 0.000000  \n",
      "Epoch: [5][2800/2871] Elapsed 23m 28s (remain 0m 35s) Loss: 0.0005(0.0033) Grad: 2701.4006  LR: 0.000000  \n",
      "Epoch: [5][2870/2871] Elapsed 24m 5s (remain 0m 0s) Loss: 0.0090(0.0034) Grad: 43612.6133  LR: 0.000000  \n",
      "EVAL: [0/704] Elapsed 0m 0s (remain 6m 4s) Loss: 0.0006(0.0006) \n",
      "EVAL: [100/704] Elapsed 0m 28s (remain 2m 49s) Loss: 0.0079(0.0097) \n",
      "EVAL: [200/704] Elapsed 0m 55s (remain 2m 19s) Loss: 0.0000(0.0086) \n",
      "EVAL: [300/704] Elapsed 1m 23s (remain 1m 52s) Loss: 0.0007(0.0079) \n",
      "EVAL: [400/704] Elapsed 1m 51s (remain 1m 24s) Loss: 0.0108(0.0093) \n",
      "EVAL: [500/704] Elapsed 2m 18s (remain 0m 56s) Loss: 0.0121(0.0102) \n",
      "EVAL: [600/704] Elapsed 2m 46s (remain 0m 28s) Loss: 0.0000(0.0105) \n",
      "EVAL: [700/704] Elapsed 3m 13s (remain 0m 0s) Loss: 0.0000(0.0098) \n",
      "EVAL: [703/704] Elapsed 3m 14s (remain 0m 0s) Loss: 0.0000(0.0098) \n",
      "Epoch 5 - avg_train_loss: 0.0034  avg_val_loss: 0.0098  time: 1644s\n",
      "Epoch 5 - Score: 0.8748\n",
      "========== fold: 3 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2877] Elapsed 0m 0s (remain 34m 41s) Loss: 1.0128(1.0128) Grad: inf  LR: 0.000000  \n",
      "Epoch: [1][100/2877] Elapsed 0m 50s (remain 23m 6s) Loss: 0.4621(0.6959) Grad: 11193.7207  LR: 0.000001  \n",
      "Epoch: [1][200/2877] Elapsed 1m 40s (remain 22m 18s) Loss: 0.0640(0.4611) Grad: 347.1920  LR: 0.000003  \n",
      "Epoch: [1][300/2877] Elapsed 2m 30s (remain 21m 26s) Loss: 0.0484(0.3220) Grad: 285.7364  LR: 0.000004  \n",
      "Epoch: [1][400/2877] Elapsed 3m 20s (remain 20m 40s) Loss: 0.0273(0.2524) Grad: 342.3198  LR: 0.000006  \n",
      "Epoch: [1][500/2877] Elapsed 4m 10s (remain 19m 47s) Loss: 0.0279(0.2087) Grad: 285.6270  LR: 0.000007  \n",
      "Epoch: [1][600/2877] Elapsed 5m 0s (remain 18m 56s) Loss: 0.0425(0.1779) Grad: 1442.1718  LR: 0.000008  \n",
      "Epoch: [1][700/2877] Elapsed 5m 49s (remain 18m 5s) Loss: 0.0107(0.1547) Grad: 231.8601  LR: 0.000010  \n",
      "Epoch: [1][800/2877] Elapsed 6m 39s (remain 17m 14s) Loss: 0.0088(0.1368) Grad: 189.1682  LR: 0.000011  \n",
      "Epoch: [1][900/2877] Elapsed 7m 28s (remain 16m 23s) Loss: 0.0274(0.1232) Grad: 1077.7690  LR: 0.000013  \n",
      "Epoch: [1][1000/2877] Elapsed 8m 17s (remain 15m 32s) Loss: 0.0009(0.1119) Grad: 48.9905  LR: 0.000014  \n",
      "Epoch: [1][1100/2877] Elapsed 9m 6s (remain 14m 41s) Loss: 0.0048(0.1028) Grad: 236.7867  LR: 0.000015  \n",
      "Epoch: [1][1200/2877] Elapsed 9m 56s (remain 13m 51s) Loss: 0.0213(0.0950) Grad: 519.8904  LR: 0.000017  \n",
      "Epoch: [1][1300/2877] Elapsed 10m 45s (remain 13m 2s) Loss: 0.0096(0.0885) Grad: 296.3440  LR: 0.000018  \n",
      "Epoch: [1][1400/2877] Elapsed 11m 35s (remain 12m 12s) Loss: 0.0074(0.0828) Grad: 176.1297  LR: 0.000019  \n",
      "Epoch: [1][1500/2877] Elapsed 12m 25s (remain 11m 23s) Loss: 0.0006(0.0779) Grad: 56.5923  LR: 0.000020  \n",
      "Epoch: [1][1600/2877] Elapsed 13m 14s (remain 10m 33s) Loss: 0.0035(0.0736) Grad: 253.3052  LR: 0.000020  \n",
      "Epoch: [1][1700/2877] Elapsed 14m 4s (remain 9m 43s) Loss: 0.0203(0.0699) Grad: 575.8851  LR: 0.000020  \n",
      "Epoch: [1][1800/2877] Elapsed 14m 53s (remain 8m 53s) Loss: 0.0027(0.0664) Grad: 93.7076  LR: 0.000019  \n",
      "Epoch: [1][1900/2877] Elapsed 15m 43s (remain 8m 4s) Loss: 0.0036(0.0633) Grad: 146.8614  LR: 0.000019  \n",
      "Epoch: [1][2000/2877] Elapsed 16m 32s (remain 7m 14s) Loss: 0.0052(0.0606) Grad: 288.9260  LR: 0.000019  \n",
      "Epoch: [1][2100/2877] Elapsed 17m 21s (remain 6m 24s) Loss: 0.0090(0.0581) Grad: 439.5543  LR: 0.000019  \n",
      "Epoch: [1][2200/2877] Elapsed 18m 10s (remain 5m 35s) Loss: 0.0411(0.0558) Grad: 740.6201  LR: 0.000019  \n",
      "Epoch: [1][2300/2877] Elapsed 19m 0s (remain 4m 45s) Loss: 0.0059(0.0537) Grad: 238.8458  LR: 0.000019  \n",
      "Epoch: [1][2400/2877] Elapsed 19m 49s (remain 3m 55s) Loss: 0.0019(0.0518) Grad: 53.4538  LR: 0.000019  \n",
      "Epoch: [1][2500/2877] Elapsed 20m 38s (remain 3m 6s) Loss: 0.0001(0.0500) Grad: 6.3802  LR: 0.000018  \n",
      "Epoch: [1][2600/2877] Elapsed 21m 27s (remain 2m 16s) Loss: 0.0019(0.0484) Grad: 70.6856  LR: 0.000018  \n",
      "Epoch: [1][2700/2877] Elapsed 22m 18s (remain 1m 27s) Loss: 0.0086(0.0469) Grad: 202.8857  LR: 0.000018  \n",
      "Epoch: [1][2800/2877] Elapsed 23m 7s (remain 0m 37s) Loss: 0.0445(0.0455) Grad: 591.3885  LR: 0.000018  \n",
      "Epoch: [1][2876/2877] Elapsed 23m 44s (remain 0m 0s) Loss: 0.0180(0.0445) Grad: 335.1504  LR: 0.000018  \n",
      "EVAL: [0/698] Elapsed 0m 0s (remain 6m 31s) Loss: 0.0038(0.0038) \n",
      "EVAL: [100/698] Elapsed 0m 28s (remain 2m 49s) Loss: 0.0045(0.0053) \n",
      "EVAL: [200/698] Elapsed 0m 56s (remain 2m 19s) Loss: 0.0019(0.0073) \n",
      "EVAL: [300/698] Elapsed 1m 23s (remain 1m 50s) Loss: 0.0030(0.0068) \n",
      "EVAL: [400/698] Elapsed 1m 51s (remain 1m 22s) Loss: 0.0347(0.0074) \n",
      "EVAL: [500/698] Elapsed 2m 18s (remain 0m 54s) Loss: 0.0049(0.0075) \n",
      "EVAL: [600/698] Elapsed 2m 46s (remain 0m 26s) Loss: 0.0044(0.0073) \n",
      "EVAL: [697/698] Elapsed 3m 12s (remain 0m 0s) Loss: 0.0006(0.0070) \n",
      "Epoch 1 - avg_train_loss: 0.0445  avg_val_loss: 0.0070  time: 1622s\n",
      "Epoch 1 - Score: 0.8533\n",
      "Epoch 1 - Save Best Score: 0.8533 Model\n",
      "Epoch: [2][0/2877] Elapsed 0m 0s (remain 32m 46s) Loss: 0.0051(0.0051) Grad: 24918.9688  LR: 0.000018  \n",
      "Epoch: [2][100/2877] Elapsed 0m 50s (remain 23m 16s) Loss: 0.0050(0.0055) Grad: 8920.1729  LR: 0.000018  \n",
      "Epoch: [2][200/2877] Elapsed 1m 40s (remain 22m 24s) Loss: 0.0025(0.0063) Grad: 6324.6899  LR: 0.000017  \n",
      "Epoch: [2][300/2877] Elapsed 2m 31s (remain 21m 38s) Loss: 0.0029(0.0060) Grad: 6788.7979  LR: 0.000017  \n",
      "Epoch: [2][400/2877] Elapsed 3m 22s (remain 20m 50s) Loss: 0.0097(0.0060) Grad: 10701.7295  LR: 0.000017  \n",
      "Epoch: [2][500/2877] Elapsed 4m 13s (remain 20m 0s) Loss: 0.0016(0.0059) Grad: 5874.7388  LR: 0.000017  \n",
      "Epoch: [2][600/2877] Elapsed 5m 3s (remain 19m 9s) Loss: 0.0038(0.0059) Grad: 10538.8955  LR: 0.000017  \n",
      "Epoch: [2][700/2877] Elapsed 5m 53s (remain 18m 16s) Loss: 0.0001(0.0058) Grad: 424.5300  LR: 0.000017  \n",
      "Epoch: [2][800/2877] Elapsed 6m 43s (remain 17m 26s) Loss: 0.0006(0.0055) Grad: 4617.2251  LR: 0.000017  \n",
      "Epoch: [2][900/2877] Elapsed 7m 34s (remain 16m 36s) Loss: 0.0035(0.0054) Grad: 7007.8564  LR: 0.000016  \n",
      "Epoch: [2][1000/2877] Elapsed 8m 24s (remain 15m 45s) Loss: 0.0003(0.0055) Grad: 880.4724  LR: 0.000016  \n",
      "Epoch: [2][1100/2877] Elapsed 9m 14s (remain 14m 54s) Loss: 0.0002(0.0054) Grad: 1184.0983  LR: 0.000016  \n",
      "Epoch: [2][1200/2877] Elapsed 10m 5s (remain 14m 4s) Loss: 0.0206(0.0054) Grad: 53660.9492  LR: 0.000016  \n",
      "Epoch: [2][1300/2877] Elapsed 10m 56s (remain 13m 15s) Loss: 0.0120(0.0054) Grad: 10056.6104  LR: 0.000016  \n",
      "Epoch: [2][1400/2877] Elapsed 11m 46s (remain 12m 24s) Loss: 0.0101(0.0055) Grad: 12299.3213  LR: 0.000016  \n",
      "Epoch: [2][1500/2877] Elapsed 12m 37s (remain 11m 34s) Loss: 0.0001(0.0055) Grad: 539.3642  LR: 0.000015  \n",
      "Epoch: [2][1600/2877] Elapsed 13m 26s (remain 10m 43s) Loss: 0.0005(0.0056) Grad: 3667.9407  LR: 0.000015  \n",
      "Epoch: [2][1700/2877] Elapsed 14m 17s (remain 9m 52s) Loss: 0.0000(0.0056) Grad: 60.5991  LR: 0.000015  \n",
      "Epoch: [2][1800/2877] Elapsed 15m 8s (remain 9m 2s) Loss: 0.0001(0.0056) Grad: 903.7423  LR: 0.000015  \n",
      "Epoch: [2][1900/2877] Elapsed 15m 58s (remain 8m 12s) Loss: 0.0020(0.0056) Grad: 6781.1719  LR: 0.000015  \n",
      "Epoch: [2][2000/2877] Elapsed 16m 48s (remain 7m 21s) Loss: 0.0032(0.0056) Grad: 7208.4795  LR: 0.000015  \n",
      "Epoch: [2][2100/2877] Elapsed 17m 38s (remain 6m 30s) Loss: 0.0025(0.0057) Grad: 5713.1821  LR: 0.000015  \n",
      "Epoch: [2][2200/2877] Elapsed 18m 28s (remain 5m 40s) Loss: 0.0031(0.0056) Grad: 11550.2988  LR: 0.000014  \n",
      "Epoch: [2][2300/2877] Elapsed 19m 18s (remain 4m 50s) Loss: 0.0026(0.0057) Grad: 14556.1348  LR: 0.000014  \n",
      "Epoch: [2][2400/2877] Elapsed 20m 8s (remain 3m 59s) Loss: 0.0192(0.0057) Grad: 48201.4883  LR: 0.000014  \n",
      "Epoch: [2][2500/2877] Elapsed 20m 58s (remain 3m 9s) Loss: 0.0001(0.0057) Grad: 1647.1158  LR: 0.000014  \n",
      "Epoch: [2][2600/2877] Elapsed 21m 49s (remain 2m 18s) Loss: 0.0182(0.0058) Grad: 25960.8242  LR: 0.000014  \n",
      "Epoch: [2][2700/2877] Elapsed 22m 41s (remain 1m 28s) Loss: 0.0001(0.0057) Grad: 364.8543  LR: 0.000014  \n",
      "Epoch: [2][2800/2877] Elapsed 23m 32s (remain 0m 38s) Loss: 0.0006(0.0056) Grad: 6081.1260  LR: 0.000013  \n",
      "Epoch: [2][2876/2877] Elapsed 24m 10s (remain 0m 0s) Loss: 0.0179(0.0057) Grad: 35161.3203  LR: 0.000013  \n",
      "EVAL: [0/698] Elapsed 0m 0s (remain 6m 3s) Loss: 0.0017(0.0017) \n",
      "EVAL: [100/698] Elapsed 0m 28s (remain 2m 45s) Loss: 0.0023(0.0052) \n",
      "EVAL: [200/698] Elapsed 0m 56s (remain 2m 18s) Loss: 0.0056(0.0072) \n",
      "EVAL: [300/698] Elapsed 1m 23s (remain 1m 49s) Loss: 0.0015(0.0066) \n",
      "EVAL: [400/698] Elapsed 1m 50s (remain 1m 21s) Loss: 0.0352(0.0072) \n",
      "EVAL: [500/698] Elapsed 2m 18s (remain 0m 54s) Loss: 0.0053(0.0073) \n",
      "EVAL: [600/698] Elapsed 2m 45s (remain 0m 26s) Loss: 0.0052(0.0070) \n",
      "EVAL: [697/698] Elapsed 3m 11s (remain 0m 0s) Loss: 0.0000(0.0068) \n",
      "Epoch 2 - avg_train_loss: 0.0057  avg_val_loss: 0.0068  time: 1647s\n",
      "Epoch 2 - Score: 0.8789\n",
      "Epoch 2 - Save Best Score: 0.8789 Model\n",
      "Epoch: [3][0/2877] Elapsed 0m 0s (remain 37m 34s) Loss: 0.0016(0.0016) Grad: 5644.1865  LR: 0.000013  \n",
      "Epoch: [3][100/2877] Elapsed 0m 50s (remain 23m 14s) Loss: 0.0003(0.0034) Grad: 2403.4067  LR: 0.000013  \n",
      "Epoch: [3][200/2877] Elapsed 1m 41s (remain 22m 29s) Loss: 0.0128(0.0040) Grad: 32702.5371  LR: 0.000013  \n",
      "Epoch: [3][300/2877] Elapsed 2m 32s (remain 21m 41s) Loss: 0.0037(0.0044) Grad: 13778.6328  LR: 0.000013  \n",
      "Epoch: [3][400/2877] Elapsed 3m 22s (remain 20m 48s) Loss: 0.0000(0.0043) Grad: 91.5282  LR: 0.000013  \n",
      "Epoch: [3][500/2877] Elapsed 4m 12s (remain 19m 56s) Loss: 0.0007(0.0045) Grad: 3358.2451  LR: 0.000013  \n",
      "Epoch: [3][600/2877] Elapsed 5m 2s (remain 19m 4s) Loss: 0.0008(0.0046) Grad: 10630.4951  LR: 0.000012  \n",
      "Epoch: [3][700/2877] Elapsed 5m 52s (remain 18m 14s) Loss: 0.0002(0.0046) Grad: 950.0206  LR: 0.000012  \n",
      "Epoch: [3][800/2877] Elapsed 6m 43s (remain 17m 25s) Loss: 0.0018(0.0048) Grad: 23391.2305  LR: 0.000012  \n",
      "Epoch: [3][900/2877] Elapsed 7m 33s (remain 16m 34s) Loss: 0.0001(0.0047) Grad: 625.0737  LR: 0.000012  \n",
      "Epoch: [3][1000/2877] Elapsed 8m 25s (remain 15m 46s) Loss: 0.0013(0.0047) Grad: 7501.9102  LR: 0.000012  \n",
      "Epoch: [3][1100/2877] Elapsed 9m 16s (remain 14m 56s) Loss: 0.0000(0.0047) Grad: 25.4207  LR: 0.000012  \n",
      "Epoch: [3][1200/2877] Elapsed 10m 5s (remain 14m 5s) Loss: 0.0154(0.0047) Grad: 23936.4980  LR: 0.000011  \n",
      "Epoch: [3][1300/2877] Elapsed 10m 55s (remain 13m 14s) Loss: 0.0355(0.0048) Grad: 226207.5312  LR: 0.000011  \n",
      "Epoch: [3][1400/2877] Elapsed 11m 46s (remain 12m 24s) Loss: 0.0038(0.0048) Grad: 9463.1455  LR: 0.000011  \n",
      "Epoch: [3][1500/2877] Elapsed 12m 37s (remain 11m 34s) Loss: 0.0017(0.0049) Grad: 4927.5713  LR: 0.000011  \n",
      "Epoch: [3][1600/2877] Elapsed 13m 30s (remain 10m 46s) Loss: 0.0037(0.0049) Grad: 10946.5498  LR: 0.000011  \n",
      "Epoch: [3][1700/2877] Elapsed 14m 21s (remain 9m 55s) Loss: 0.0009(0.0049) Grad: 3491.9988  LR: 0.000011  \n",
      "Epoch: [3][1800/2877] Elapsed 15m 11s (remain 9m 4s) Loss: 0.0036(0.0048) Grad: 11632.7461  LR: 0.000011  \n",
      "Epoch: [3][1900/2877] Elapsed 16m 1s (remain 8m 13s) Loss: 0.0039(0.0048) Grad: 84301.5469  LR: 0.000010  \n",
      "Epoch: [3][2000/2877] Elapsed 16m 51s (remain 7m 22s) Loss: 0.0021(0.0049) Grad: 5539.7119  LR: 0.000010  \n",
      "Epoch: [3][2100/2877] Elapsed 17m 41s (remain 6m 32s) Loss: 0.0055(0.0048) Grad: 20505.8047  LR: 0.000010  \n",
      "Epoch: [3][2200/2877] Elapsed 18m 34s (remain 5m 42s) Loss: 0.0245(0.0049) Grad: 35278.0352  LR: 0.000010  \n",
      "Epoch: [3][2300/2877] Elapsed 19m 26s (remain 4m 51s) Loss: 0.0001(0.0048) Grad: 543.5547  LR: 0.000010  \n",
      "Epoch: [3][2400/2877] Elapsed 20m 16s (remain 4m 1s) Loss: 0.0000(0.0048) Grad: 294.7527  LR: 0.000010  \n",
      "Epoch: [3][2500/2877] Elapsed 21m 8s (remain 3m 10s) Loss: 0.0002(0.0049) Grad: 3754.3767  LR: 0.000009  \n",
      "Epoch: [3][2600/2877] Elapsed 21m 58s (remain 2m 19s) Loss: 0.0002(0.0048) Grad: 601.0693  LR: 0.000009  \n",
      "Epoch: [3][2700/2877] Elapsed 22m 48s (remain 1m 29s) Loss: 0.0095(0.0049) Grad: 18034.5039  LR: 0.000009  \n",
      "Epoch: [3][2800/2877] Elapsed 23m 37s (remain 0m 38s) Loss: 0.0032(0.0048) Grad: 8846.6602  LR: 0.000009  \n",
      "Epoch: [3][2876/2877] Elapsed 24m 15s (remain 0m 0s) Loss: 0.0047(0.0048) Grad: 8777.6855  LR: 0.000009  \n",
      "EVAL: [0/698] Elapsed 0m 0s (remain 6m 12s) Loss: 0.0018(0.0018) \n",
      "EVAL: [100/698] Elapsed 0m 28s (remain 2m 45s) Loss: 0.0018(0.0054) \n",
      "EVAL: [200/698] Elapsed 0m 55s (remain 2m 17s) Loss: 0.0024(0.0077) \n",
      "EVAL: [300/698] Elapsed 1m 23s (remain 1m 49s) Loss: 0.0014(0.0072) \n",
      "EVAL: [400/698] Elapsed 1m 50s (remain 1m 21s) Loss: 0.0346(0.0077) \n",
      "EVAL: [500/698] Elapsed 2m 18s (remain 0m 54s) Loss: 0.0051(0.0078) \n",
      "EVAL: [600/698] Elapsed 2m 45s (remain 0m 26s) Loss: 0.0028(0.0074) \n",
      "EVAL: [697/698] Elapsed 3m 11s (remain 0m 0s) Loss: 0.0000(0.0072) \n",
      "Epoch 3 - avg_train_loss: 0.0048  avg_val_loss: 0.0072  time: 1652s\n",
      "Epoch 3 - Score: 0.8820\n",
      "Epoch 3 - Save Best Score: 0.8820 Model\n",
      "Epoch: [4][0/2877] Elapsed 0m 0s (remain 38m 36s) Loss: 0.0110(0.0110) Grad: 24559.1719  LR: 0.000009  \n",
      "Epoch: [4][100/2877] Elapsed 0m 51s (remain 23m 43s) Loss: 0.0000(0.0034) Grad: 229.5330  LR: 0.000009  \n",
      "Epoch: [4][200/2877] Elapsed 1m 41s (remain 22m 34s) Loss: 0.0220(0.0036) Grad: 39511.2891  LR: 0.000009  \n",
      "Epoch: [4][300/2877] Elapsed 2m 32s (remain 21m 41s) Loss: 0.0025(0.0039) Grad: 8083.6724  LR: 0.000008  \n",
      "Epoch: [4][400/2877] Elapsed 3m 22s (remain 20m 47s) Loss: 0.0000(0.0038) Grad: 22.9152  LR: 0.000008  \n",
      "Epoch: [4][500/2877] Elapsed 4m 12s (remain 19m 58s) Loss: 0.0008(0.0038) Grad: 3467.7905  LR: 0.000008  \n",
      "Epoch: [4][600/2877] Elapsed 5m 2s (remain 19m 6s) Loss: 0.0119(0.0039) Grad: 23269.5977  LR: 0.000008  \n",
      "Epoch: [4][700/2877] Elapsed 5m 52s (remain 18m 15s) Loss: 0.0027(0.0039) Grad: 9090.4707  LR: 0.000008  \n",
      "Epoch: [4][800/2877] Elapsed 6m 44s (remain 17m 27s) Loss: 0.0066(0.0039) Grad: 19547.0020  LR: 0.000008  \n",
      "Epoch: [4][900/2877] Elapsed 7m 35s (remain 16m 39s) Loss: 0.0065(0.0038) Grad: 25679.0059  LR: 0.000007  \n",
      "Epoch: [4][1000/2877] Elapsed 8m 25s (remain 15m 47s) Loss: 0.0003(0.0039) Grad: 4435.3271  LR: 0.000007  \n",
      "Epoch: [4][1100/2877] Elapsed 9m 15s (remain 14m 55s) Loss: 0.0068(0.0040) Grad: 42151.9961  LR: 0.000007  \n",
      "Epoch: [4][1200/2877] Elapsed 10m 5s (remain 14m 5s) Loss: 0.0023(0.0039) Grad: 5214.2979  LR: 0.000007  \n",
      "Epoch: [4][1300/2877] Elapsed 10m 56s (remain 13m 15s) Loss: 0.0009(0.0039) Grad: 6084.0107  LR: 0.000007  \n",
      "Epoch: [4][1400/2877] Elapsed 11m 46s (remain 12m 24s) Loss: 0.0008(0.0039) Grad: 4441.6055  LR: 0.000007  \n",
      "Epoch: [4][1500/2877] Elapsed 12m 36s (remain 11m 33s) Loss: 0.0007(0.0039) Grad: 4504.2168  LR: 0.000007  \n",
      "Epoch: [4][1600/2877] Elapsed 13m 26s (remain 10m 42s) Loss: 0.0000(0.0039) Grad: 243.9234  LR: 0.000006  \n",
      "Epoch: [4][1700/2877] Elapsed 14m 15s (remain 9m 51s) Loss: 0.0007(0.0040) Grad: 4500.8584  LR: 0.000006  \n",
      "Epoch: [4][1800/2877] Elapsed 15m 6s (remain 9m 1s) Loss: 0.0012(0.0040) Grad: 5366.7554  LR: 0.000006  \n",
      "Epoch: [4][1900/2877] Elapsed 15m 57s (remain 8m 11s) Loss: 0.0000(0.0040) Grad: 753.0231  LR: 0.000006  \n",
      "Epoch: [4][2000/2877] Elapsed 16m 49s (remain 7m 21s) Loss: 0.0026(0.0040) Grad: 10490.9795  LR: 0.000006  \n",
      "Epoch: [4][2100/2877] Elapsed 17m 39s (remain 6m 31s) Loss: 0.0001(0.0040) Grad: 1873.8293  LR: 0.000006  \n",
      "Epoch: [4][2200/2877] Elapsed 18m 30s (remain 5m 41s) Loss: 0.0011(0.0040) Grad: 4599.8262  LR: 0.000005  \n",
      "Epoch: [4][2300/2877] Elapsed 19m 21s (remain 4m 50s) Loss: 0.0065(0.0040) Grad: 12734.8027  LR: 0.000005  \n",
      "Epoch: [4][2400/2877] Elapsed 20m 11s (remain 4m 0s) Loss: 0.0070(0.0040) Grad: 20677.5293  LR: 0.000005  \n",
      "Epoch: [4][2500/2877] Elapsed 21m 1s (remain 3m 9s) Loss: 0.0100(0.0041) Grad: 55824.7578  LR: 0.000005  \n",
      "Epoch: [4][2600/2877] Elapsed 21m 51s (remain 2m 19s) Loss: 0.0014(0.0041) Grad: 6066.5732  LR: 0.000005  \n",
      "Epoch: [4][2700/2877] Elapsed 22m 42s (remain 1m 28s) Loss: 0.0014(0.0041) Grad: 11859.1035  LR: 0.000005  \n",
      "Epoch: [4][2800/2877] Elapsed 23m 31s (remain 0m 38s) Loss: 0.0000(0.0040) Grad: 133.9502  LR: 0.000005  \n",
      "Epoch: [4][2876/2877] Elapsed 24m 9s (remain 0m 0s) Loss: 0.0000(0.0040) Grad: 10.0288  LR: 0.000004  \n",
      "EVAL: [0/698] Elapsed 0m 0s (remain 6m 23s) Loss: 0.0006(0.0006) \n",
      "EVAL: [100/698] Elapsed 0m 28s (remain 2m 50s) Loss: 0.0012(0.0065) \n",
      "EVAL: [200/698] Elapsed 0m 56s (remain 2m 18s) Loss: 0.0110(0.0090) \n",
      "EVAL: [300/698] Elapsed 1m 23s (remain 1m 50s) Loss: 0.0009(0.0085) \n",
      "EVAL: [400/698] Elapsed 1m 50s (remain 1m 22s) Loss: 0.0395(0.0090) \n",
      "EVAL: [500/698] Elapsed 2m 18s (remain 0m 54s) Loss: 0.0044(0.0090) \n",
      "EVAL: [600/698] Elapsed 2m 45s (remain 0m 26s) Loss: 0.0009(0.0086) \n",
      "EVAL: [697/698] Elapsed 3m 12s (remain 0m 0s) Loss: 0.0000(0.0084) \n",
      "Epoch 4 - avg_train_loss: 0.0040  avg_val_loss: 0.0084  time: 1647s\n",
      "Epoch 4 - Score: 0.8853\n",
      "Epoch 4 - Save Best Score: 0.8853 Model\n",
      "Epoch: [5][0/2877] Elapsed 0m 0s (remain 35m 49s) Loss: 0.0002(0.0002) Grad: 1312.7787  LR: 0.000004  \n",
      "Epoch: [5][100/2877] Elapsed 0m 51s (remain 23m 29s) Loss: 0.0017(0.0024) Grad: 7462.6528  LR: 0.000004  \n",
      "Epoch: [5][200/2877] Elapsed 1m 41s (remain 22m 36s) Loss: 0.0000(0.0023) Grad: 167.6253  LR: 0.000004  \n",
      "Epoch: [5][300/2877] Elapsed 2m 31s (remain 21m 40s) Loss: 0.0005(0.0029) Grad: 3477.8330  LR: 0.000004  \n",
      "Epoch: [5][400/2877] Elapsed 3m 21s (remain 20m 44s) Loss: 0.0289(0.0032) Grad: 49183.5039  LR: 0.000004  \n",
      "Epoch: [5][500/2877] Elapsed 4m 11s (remain 19m 51s) Loss: 0.0018(0.0034) Grad: 36896.2070  LR: 0.000004  \n",
      "Epoch: [5][600/2877] Elapsed 5m 1s (remain 19m 2s) Loss: 0.0036(0.0035) Grad: 10470.5693  LR: 0.000004  \n",
      "Epoch: [5][700/2877] Elapsed 5m 52s (remain 18m 13s) Loss: 0.0002(0.0037) Grad: 797.5346  LR: 0.000003  \n",
      "Epoch: [5][800/2877] Elapsed 6m 43s (remain 17m 24s) Loss: 0.0019(0.0036) Grad: 6000.6230  LR: 0.000003  \n",
      "Epoch: [5][900/2877] Elapsed 7m 33s (remain 16m 35s) Loss: 0.0007(0.0035) Grad: 4621.2920  LR: 0.000003  \n",
      "Epoch: [5][1000/2877] Elapsed 8m 23s (remain 15m 43s) Loss: 0.0001(0.0034) Grad: 343.2401  LR: 0.000003  \n",
      "Epoch: [5][1100/2877] Elapsed 9m 13s (remain 14m 52s) Loss: 0.0028(0.0035) Grad: 21860.6035  LR: 0.000003  \n",
      "Epoch: [5][1200/2877] Elapsed 10m 3s (remain 14m 1s) Loss: 0.0059(0.0034) Grad: 15378.7627  LR: 0.000003  \n",
      "Epoch: [5][1300/2877] Elapsed 10m 53s (remain 13m 11s) Loss: 0.0000(0.0035) Grad: 16.9046  LR: 0.000002  \n",
      "Epoch: [5][1400/2877] Elapsed 11m 43s (remain 12m 21s) Loss: 0.0007(0.0035) Grad: 7640.8394  LR: 0.000002  \n",
      "Epoch: [5][1500/2877] Elapsed 12m 34s (remain 11m 31s) Loss: 0.0000(0.0034) Grad: 10.7612  LR: 0.000002  \n",
      "Epoch: [5][1600/2877] Elapsed 13m 23s (remain 10m 40s) Loss: 0.0008(0.0035) Grad: 3127.5271  LR: 0.000002  \n",
      "Epoch: [5][1700/2877] Elapsed 14m 15s (remain 9m 51s) Loss: 0.0059(0.0035) Grad: 21054.4355  LR: 0.000002  \n",
      "Epoch: [5][1800/2877] Elapsed 15m 5s (remain 9m 1s) Loss: 0.0068(0.0035) Grad: 45147.9570  LR: 0.000002  \n",
      "Epoch: [5][1900/2877] Elapsed 15m 55s (remain 8m 10s) Loss: 0.0001(0.0035) Grad: 260.3747  LR: 0.000002  \n",
      "Epoch: [5][2000/2877] Elapsed 16m 47s (remain 7m 20s) Loss: 0.0000(0.0035) Grad: 295.2580  LR: 0.000001  \n",
      "Epoch: [5][2100/2877] Elapsed 17m 38s (remain 6m 30s) Loss: 0.0016(0.0034) Grad: 20844.9004  LR: 0.000001  \n",
      "Epoch: [5][2200/2877] Elapsed 18m 28s (remain 5m 40s) Loss: 0.0016(0.0034) Grad: 6772.2261  LR: 0.000001  \n",
      "Epoch: [5][2300/2877] Elapsed 19m 17s (remain 4m 49s) Loss: 0.0000(0.0034) Grad: 159.3462  LR: 0.000001  \n",
      "Epoch: [5][2400/2877] Elapsed 20m 7s (remain 3m 59s) Loss: 0.0000(0.0033) Grad: 138.4742  LR: 0.000001  \n",
      "Epoch: [5][2500/2877] Elapsed 20m 57s (remain 3m 9s) Loss: 0.0001(0.0034) Grad: 829.2886  LR: 0.000001  \n",
      "Epoch: [5][2600/2877] Elapsed 21m 47s (remain 2m 18s) Loss: 0.0001(0.0034) Grad: 1073.2931  LR: 0.000000  \n",
      "Epoch: [5][2700/2877] Elapsed 22m 37s (remain 1m 28s) Loss: 0.0000(0.0034) Grad: 15.1577  LR: 0.000000  \n",
      "Epoch: [5][2800/2877] Elapsed 23m 27s (remain 0m 38s) Loss: 0.0011(0.0033) Grad: 5693.7544  LR: 0.000000  \n",
      "Epoch: [5][2876/2877] Elapsed 24m 5s (remain 0m 0s) Loss: 0.0003(0.0033) Grad: 4607.5005  LR: 0.000000  \n",
      "EVAL: [0/698] Elapsed 0m 0s (remain 6m 18s) Loss: 0.0005(0.0005) \n",
      "EVAL: [100/698] Elapsed 0m 28s (remain 2m 48s) Loss: 0.0014(0.0070) \n",
      "EVAL: [200/698] Elapsed 0m 56s (remain 2m 20s) Loss: 0.0091(0.0095) \n",
      "EVAL: [300/698] Elapsed 1m 24s (remain 1m 51s) Loss: 0.0015(0.0090) \n",
      "EVAL: [400/698] Elapsed 1m 51s (remain 1m 22s) Loss: 0.0310(0.0094) \n",
      "EVAL: [500/698] Elapsed 2m 18s (remain 0m 54s) Loss: 0.0037(0.0095) \n",
      "EVAL: [600/698] Elapsed 2m 46s (remain 0m 26s) Loss: 0.0013(0.0090) \n",
      "EVAL: [697/698] Elapsed 3m 12s (remain 0m 0s) Loss: 0.0000(0.0088) \n",
      "Epoch 5 - avg_train_loss: 0.0033  avg_val_loss: 0.0088  time: 1643s\n",
      "Epoch 5 - Score: 0.8858\n",
      "Epoch 5 - Save Best Score: 0.8858 Model\n",
      "========== fold: 4 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2850] Elapsed 0m 0s (remain 33m 13s) Loss: 0.4454(0.4454) Grad: inf  LR: 0.000000  \n",
      "Epoch: [1][100/2850] Elapsed 0m 50s (remain 22m 53s) Loss: 0.5889(0.6814) Grad: 59602.0117  LR: 0.000001  \n",
      "Epoch: [1][200/2850] Elapsed 1m 41s (remain 22m 11s) Loss: 0.0652(0.4769) Grad: 6703.7100  LR: 0.000003  \n",
      "Epoch: [1][300/2850] Elapsed 2m 32s (remain 21m 34s) Loss: 0.0262(0.3341) Grad: 783.5600  LR: 0.000004  \n",
      "Epoch: [1][400/2850] Elapsed 3m 22s (remain 20m 38s) Loss: 0.0220(0.2597) Grad: 811.3931  LR: 0.000006  \n",
      "Epoch: [1][500/2850] Elapsed 4m 13s (remain 19m 48s) Loss: 0.0190(0.2140) Grad: 1469.5911  LR: 0.000007  \n",
      "Epoch: [1][600/2850] Elapsed 5m 3s (remain 18m 55s) Loss: 0.0316(0.1817) Grad: 2954.4946  LR: 0.000008  \n",
      "Epoch: [1][700/2850] Elapsed 5m 53s (remain 18m 3s) Loss: 0.0033(0.1577) Grad: 655.9958  LR: 0.000010  \n",
      "Epoch: [1][800/2850] Elapsed 6m 43s (remain 17m 12s) Loss: 0.0230(0.1400) Grad: 1941.8827  LR: 0.000011  \n",
      "Epoch: [1][900/2850] Elapsed 7m 33s (remain 16m 21s) Loss: 0.0117(0.1259) Grad: 864.5408  LR: 0.000013  \n",
      "Epoch: [1][1000/2850] Elapsed 8m 24s (remain 15m 31s) Loss: 0.0091(0.1145) Grad: 1272.0623  LR: 0.000014  \n",
      "Epoch: [1][1100/2850] Elapsed 9m 14s (remain 14m 40s) Loss: 0.0100(0.1050) Grad: 953.4741  LR: 0.000015  \n",
      "Epoch: [1][1200/2850] Elapsed 10m 3s (remain 13m 49s) Loss: 0.0210(0.0971) Grad: 2137.5759  LR: 0.000017  \n",
      "Epoch: [1][1300/2850] Elapsed 10m 53s (remain 12m 58s) Loss: 0.0030(0.0904) Grad: 626.3222  LR: 0.000018  \n",
      "Epoch: [1][1400/2850] Elapsed 11m 43s (remain 12m 7s) Loss: 0.0029(0.0846) Grad: 724.7247  LR: 0.000020  \n",
      "Epoch: [1][1500/2850] Elapsed 12m 33s (remain 11m 17s) Loss: 0.0008(0.0796) Grad: 134.6571  LR: 0.000020  \n",
      "Epoch: [1][1600/2850] Elapsed 13m 25s (remain 10m 28s) Loss: 0.0023(0.0752) Grad: 314.8518  LR: 0.000020  \n",
      "Epoch: [1][1700/2850] Elapsed 14m 16s (remain 9m 38s) Loss: 0.0062(0.0712) Grad: 1494.7350  LR: 0.000020  \n",
      "Epoch: [1][1800/2850] Elapsed 15m 6s (remain 8m 48s) Loss: 0.0192(0.0677) Grad: 3265.1545  LR: 0.000019  \n",
      "Epoch: [1][1900/2850] Elapsed 15m 56s (remain 7m 57s) Loss: 0.0105(0.0646) Grad: 2143.8394  LR: 0.000019  \n",
      "Epoch: [1][2000/2850] Elapsed 16m 45s (remain 7m 6s) Loss: 0.0007(0.0618) Grad: 104.0590  LR: 0.000019  \n",
      "Epoch: [1][2100/2850] Elapsed 17m 35s (remain 6m 16s) Loss: 0.0028(0.0593) Grad: 358.2555  LR: 0.000019  \n",
      "Epoch: [1][2200/2850] Elapsed 18m 25s (remain 5m 25s) Loss: 0.0096(0.0570) Grad: 1229.8309  LR: 0.000019  \n",
      "Epoch: [1][2300/2850] Elapsed 19m 14s (remain 4m 35s) Loss: 0.0048(0.0549) Grad: 843.2484  LR: 0.000019  \n",
      "Epoch: [1][2400/2850] Elapsed 20m 5s (remain 3m 45s) Loss: 0.0001(0.0529) Grad: 20.9996  LR: 0.000018  \n",
      "Epoch: [1][2500/2850] Elapsed 20m 55s (remain 2m 55s) Loss: 0.0055(0.0511) Grad: 532.2547  LR: 0.000018  \n",
      "Epoch: [1][2600/2850] Elapsed 21m 46s (remain 2m 5s) Loss: 0.0014(0.0494) Grad: 463.9772  LR: 0.000018  \n",
      "Epoch: [1][2700/2850] Elapsed 22m 35s (remain 1m 14s) Loss: 0.0007(0.0478) Grad: 122.1327  LR: 0.000018  \n",
      "Epoch: [1][2800/2850] Elapsed 23m 25s (remain 0m 24s) Loss: 0.0012(0.0463) Grad: 164.4580  LR: 0.000018  \n",
      "Epoch: [1][2849/2850] Elapsed 23m 49s (remain 0m 0s) Loss: 0.0009(0.0456) Grad: 459.1742  LR: 0.000018  \n",
      "EVAL: [0/725] Elapsed 0m 0s (remain 6m 9s) Loss: 0.0085(0.0085) \n",
      "EVAL: [100/725] Elapsed 0m 27s (remain 2m 52s) Loss: 0.0011(0.0054) \n",
      "EVAL: [200/725] Elapsed 0m 55s (remain 2m 24s) Loss: 0.0115(0.0077) \n",
      "EVAL: [300/725] Elapsed 1m 22s (remain 1m 56s) Loss: 0.0069(0.0069) \n",
      "EVAL: [400/725] Elapsed 1m 50s (remain 1m 29s) Loss: 0.0422(0.0074) \n",
      "EVAL: [500/725] Elapsed 2m 18s (remain 1m 1s) Loss: 0.0081(0.0075) \n",
      "EVAL: [600/725] Elapsed 2m 46s (remain 0m 34s) Loss: 0.0017(0.0074) \n",
      "EVAL: [700/725] Elapsed 3m 14s (remain 0m 6s) Loss: 0.0006(0.0068) \n",
      "EVAL: [724/725] Elapsed 3m 20s (remain 0m 0s) Loss: 0.0106(0.0068) \n",
      "Epoch 1 - avg_train_loss: 0.0456  avg_val_loss: 0.0068  time: 1635s\n",
      "Epoch 1 - Score: 0.8634\n",
      "Epoch 1 - Save Best Score: 0.8634 Model\n",
      "Epoch: [2][0/2850] Elapsed 0m 0s (remain 33m 15s) Loss: 0.0036(0.0036) Grad: 46883.4219  LR: 0.000018  \n",
      "Epoch: [2][100/2850] Elapsed 0m 51s (remain 23m 10s) Loss: 0.0009(0.0044) Grad: 4043.0063  LR: 0.000018  \n",
      "Epoch: [2][200/2850] Elapsed 1m 42s (remain 22m 24s) Loss: 0.0010(0.0055) Grad: 2453.6221  LR: 0.000017  \n",
      "Epoch: [2][300/2850] Elapsed 2m 32s (remain 21m 34s) Loss: 0.0044(0.0055) Grad: 8605.4316  LR: 0.000017  \n",
      "Epoch: [2][400/2850] Elapsed 3m 22s (remain 20m 39s) Loss: 0.0152(0.0060) Grad: 33339.8320  LR: 0.000017  \n",
      "Epoch: [2][500/2850] Elapsed 4m 13s (remain 19m 46s) Loss: 0.0197(0.0064) Grad: 14496.3447  LR: 0.000017  \n",
      "Epoch: [2][600/2850] Elapsed 5m 3s (remain 18m 54s) Loss: 0.0006(0.0063) Grad: 936.7562  LR: 0.000017  \n",
      "Epoch: [2][700/2850] Elapsed 5m 53s (remain 18m 3s) Loss: 0.0026(0.0065) Grad: 2630.9705  LR: 0.000017  \n",
      "Epoch: [2][800/2850] Elapsed 6m 44s (remain 17m 14s) Loss: 0.0017(0.0063) Grad: 3741.5110  LR: 0.000017  \n",
      "Epoch: [2][900/2850] Elapsed 7m 34s (remain 16m 23s) Loss: 0.0033(0.0063) Grad: 5296.8770  LR: 0.000016  \n",
      "Epoch: [2][1000/2850] Elapsed 8m 24s (remain 15m 32s) Loss: 0.0001(0.0062) Grad: 117.1382  LR: 0.000016  \n",
      "Epoch: [2][1100/2850] Elapsed 9m 17s (remain 14m 44s) Loss: 0.0127(0.0061) Grad: 11564.2549  LR: 0.000016  \n",
      "Epoch: [2][1200/2850] Elapsed 10m 6s (remain 13m 53s) Loss: 0.0001(0.0060) Grad: 287.1270  LR: 0.000016  \n",
      "Epoch: [2][1300/2850] Elapsed 10m 57s (remain 13m 2s) Loss: 0.0203(0.0061) Grad: 13170.5010  LR: 0.000016  \n",
      "Epoch: [2][1400/2850] Elapsed 11m 48s (remain 12m 12s) Loss: 0.0002(0.0060) Grad: 708.7418  LR: 0.000016  \n",
      "Epoch: [2][1500/2850] Elapsed 12m 38s (remain 11m 21s) Loss: 0.0005(0.0061) Grad: 1299.2859  LR: 0.000015  \n",
      "Epoch: [2][1600/2850] Elapsed 13m 28s (remain 10m 31s) Loss: 0.0010(0.0061) Grad: 8109.5898  LR: 0.000015  \n",
      "Epoch: [2][1700/2850] Elapsed 14m 18s (remain 9m 40s) Loss: 0.0017(0.0060) Grad: 2999.8433  LR: 0.000015  \n",
      "Epoch: [2][1800/2850] Elapsed 15m 9s (remain 8m 49s) Loss: 0.0014(0.0060) Grad: 1684.2766  LR: 0.000015  \n",
      "Epoch: [2][1900/2850] Elapsed 15m 59s (remain 7m 59s) Loss: 0.0292(0.0059) Grad: 23191.9316  LR: 0.000015  \n",
      "Epoch: [2][2000/2850] Elapsed 16m 49s (remain 7m 8s) Loss: 0.0025(0.0059) Grad: 8543.1514  LR: 0.000015  \n",
      "Epoch: [2][2100/2850] Elapsed 17m 39s (remain 6m 17s) Loss: 0.0036(0.0058) Grad: 5038.7461  LR: 0.000015  \n",
      "Epoch: [2][2200/2850] Elapsed 18m 29s (remain 5m 27s) Loss: 0.0009(0.0058) Grad: 1728.7192  LR: 0.000014  \n",
      "Epoch: [2][2300/2850] Elapsed 19m 20s (remain 4m 36s) Loss: 0.0051(0.0058) Grad: 14677.6787  LR: 0.000014  \n",
      "Epoch: [2][2400/2850] Elapsed 20m 11s (remain 3m 46s) Loss: 0.0072(0.0058) Grad: 12284.5557  LR: 0.000014  \n",
      "Epoch: [2][2500/2850] Elapsed 21m 1s (remain 2m 56s) Loss: 0.0001(0.0057) Grad: 174.9021  LR: 0.000014  \n",
      "Epoch: [2][2600/2850] Elapsed 21m 51s (remain 2m 5s) Loss: 0.0140(0.0058) Grad: 14137.6475  LR: 0.000014  \n",
      "Epoch: [2][2700/2850] Elapsed 22m 41s (remain 1m 15s) Loss: 0.0000(0.0057) Grad: 68.2590  LR: 0.000014  \n",
      "Epoch: [2][2800/2850] Elapsed 23m 32s (remain 0m 24s) Loss: 0.0062(0.0058) Grad: 5022.2310  LR: 0.000013  \n",
      "Epoch: [2][2849/2850] Elapsed 23m 57s (remain 0m 0s) Loss: 0.0035(0.0057) Grad: 4403.8438  LR: 0.000013  \n",
      "EVAL: [0/725] Elapsed 0m 0s (remain 5m 52s) Loss: 0.0090(0.0090) \n",
      "EVAL: [100/725] Elapsed 0m 28s (remain 2m 54s) Loss: 0.0033(0.0071) \n",
      "EVAL: [200/725] Elapsed 0m 56s (remain 2m 27s) Loss: 0.0043(0.0095) \n",
      "EVAL: [300/725] Elapsed 1m 23s (remain 1m 58s) Loss: 0.0212(0.0082) \n",
      "EVAL: [400/725] Elapsed 1m 51s (remain 1m 29s) Loss: 0.0385(0.0089) \n",
      "EVAL: [500/725] Elapsed 2m 18s (remain 1m 1s) Loss: 0.0078(0.0089) \n",
      "EVAL: [600/725] Elapsed 2m 46s (remain 0m 34s) Loss: 0.0021(0.0087) \n",
      "EVAL: [700/725] Elapsed 3m 14s (remain 0m 6s) Loss: 0.0001(0.0080) \n",
      "EVAL: [724/725] Elapsed 3m 21s (remain 0m 0s) Loss: 0.0140(0.0079) \n",
      "Epoch 2 - avg_train_loss: 0.0057  avg_val_loss: 0.0079  time: 1644s\n",
      "Epoch 2 - Score: 0.8767\n",
      "Epoch 2 - Save Best Score: 0.8767 Model\n",
      "Epoch: [3][0/2850] Elapsed 0m 0s (remain 34m 27s) Loss: 0.0001(0.0001) Grad: 323.3972  LR: 0.000013  \n",
      "Epoch: [3][100/2850] Elapsed 0m 52s (remain 23m 38s) Loss: 0.0001(0.0043) Grad: 430.6196  LR: 0.000013  \n",
      "Epoch: [3][200/2850] Elapsed 1m 41s (remain 22m 20s) Loss: 0.0012(0.0046) Grad: 4873.0093  LR: 0.000013  \n",
      "Epoch: [3][300/2850] Elapsed 2m 31s (remain 21m 25s) Loss: 0.0014(0.0050) Grad: 3515.4121  LR: 0.000013  \n",
      "Epoch: [3][400/2850] Elapsed 3m 21s (remain 20m 32s) Loss: 0.0267(0.0046) Grad: 49865.7422  LR: 0.000013  \n",
      "Epoch: [3][500/2850] Elapsed 4m 11s (remain 19m 40s) Loss: 0.0365(0.0049) Grad: 33453.8672  LR: 0.000013  \n",
      "Epoch: [3][600/2850] Elapsed 5m 2s (remain 18m 51s) Loss: 0.0003(0.0052) Grad: 1499.8293  LR: 0.000012  \n",
      "Epoch: [3][700/2850] Elapsed 5m 53s (remain 18m 3s) Loss: 0.0009(0.0052) Grad: 3932.4229  LR: 0.000012  \n",
      "Epoch: [3][800/2850] Elapsed 6m 44s (remain 17m 15s) Loss: 0.0001(0.0050) Grad: 594.3596  LR: 0.000012  \n",
      "Epoch: [3][900/2850] Elapsed 7m 35s (remain 16m 24s) Loss: 0.0010(0.0051) Grad: 1602.5852  LR: 0.000012  \n",
      "Epoch: [3][1000/2850] Elapsed 8m 25s (remain 15m 33s) Loss: 0.0000(0.0051) Grad: 41.7334  LR: 0.000012  \n",
      "Epoch: [3][1100/2850] Elapsed 9m 15s (remain 14m 41s) Loss: 0.0044(0.0050) Grad: 8335.9189  LR: 0.000012  \n",
      "Epoch: [3][1200/2850] Elapsed 10m 5s (remain 13m 50s) Loss: 0.0008(0.0049) Grad: 1810.2539  LR: 0.000011  \n",
      "Epoch: [3][1300/2850] Elapsed 10m 54s (remain 12m 59s) Loss: 0.0021(0.0047) Grad: 4914.3643  LR: 0.000011  \n",
      "Epoch: [3][1400/2850] Elapsed 11m 45s (remain 12m 9s) Loss: 0.0000(0.0046) Grad: 13.9323  LR: 0.000011  \n",
      "Epoch: [3][1500/2850] Elapsed 12m 35s (remain 11m 18s) Loss: 0.0006(0.0046) Grad: 1408.8816  LR: 0.000011  \n",
      "Epoch: [3][1600/2850] Elapsed 13m 25s (remain 10m 28s) Loss: 0.0051(0.0046) Grad: 4716.9712  LR: 0.000011  \n",
      "Epoch: [3][1700/2850] Elapsed 14m 15s (remain 9m 37s) Loss: 0.0000(0.0046) Grad: 140.1657  LR: 0.000011  \n",
      "Epoch: [3][1800/2850] Elapsed 15m 5s (remain 8m 47s) Loss: 0.0013(0.0046) Grad: 2437.5295  LR: 0.000011  \n",
      "Epoch: [3][1900/2850] Elapsed 15m 55s (remain 7m 57s) Loss: 0.0001(0.0046) Grad: 204.5027  LR: 0.000010  \n",
      "Epoch: [3][2000/2850] Elapsed 16m 46s (remain 7m 6s) Loss: 0.0097(0.0046) Grad: 8460.1953  LR: 0.000010  \n",
      "Epoch: [3][2100/2850] Elapsed 17m 36s (remain 6m 16s) Loss: 0.0196(0.0046) Grad: 14509.1670  LR: 0.000010  \n",
      "Epoch: [3][2200/2850] Elapsed 18m 27s (remain 5m 26s) Loss: 0.0001(0.0046) Grad: 178.3709  LR: 0.000010  \n",
      "Epoch: [3][2300/2850] Elapsed 19m 19s (remain 4m 36s) Loss: 0.0003(0.0046) Grad: 640.2672  LR: 0.000010  \n",
      "Epoch: [3][2400/2850] Elapsed 20m 8s (remain 3m 46s) Loss: 0.0042(0.0046) Grad: 5050.2754  LR: 0.000010  \n",
      "Epoch: [3][2500/2850] Elapsed 20m 58s (remain 2m 55s) Loss: 0.0017(0.0047) Grad: 3731.1550  LR: 0.000009  \n",
      "Epoch: [3][2600/2850] Elapsed 21m 48s (remain 2m 5s) Loss: 0.0001(0.0046) Grad: 225.5386  LR: 0.000009  \n",
      "Epoch: [3][2700/2850] Elapsed 22m 38s (remain 1m 14s) Loss: 0.0000(0.0046) Grad: 19.6178  LR: 0.000009  \n",
      "Epoch: [3][2800/2850] Elapsed 23m 28s (remain 0m 24s) Loss: 0.0014(0.0045) Grad: 3626.6113  LR: 0.000009  \n",
      "Epoch: [3][2849/2850] Elapsed 23m 53s (remain 0m 0s) Loss: 0.0005(0.0045) Grad: 1958.7991  LR: 0.000009  \n",
      "EVAL: [0/725] Elapsed 0m 0s (remain 5m 45s) Loss: 0.0156(0.0156) \n",
      "EVAL: [100/725] Elapsed 0m 27s (remain 2m 51s) Loss: 0.0013(0.0073) \n",
      "EVAL: [200/725] Elapsed 0m 55s (remain 2m 25s) Loss: 0.0046(0.0098) \n",
      "EVAL: [300/725] Elapsed 1m 23s (remain 1m 57s) Loss: 0.0178(0.0086) \n",
      "EVAL: [400/725] Elapsed 1m 51s (remain 1m 29s) Loss: 0.0413(0.0092) \n",
      "EVAL: [500/725] Elapsed 2m 18s (remain 1m 2s) Loss: 0.0096(0.0091) \n",
      "EVAL: [600/725] Elapsed 2m 46s (remain 0m 34s) Loss: 0.0131(0.0089) \n",
      "EVAL: [700/725] Elapsed 3m 13s (remain 0m 6s) Loss: 0.0000(0.0082) \n",
      "EVAL: [724/725] Elapsed 3m 20s (remain 0m 0s) Loss: 0.0142(0.0081) \n",
      "Epoch 3 - avg_train_loss: 0.0045  avg_val_loss: 0.0081  time: 1639s\n",
      "Epoch 3 - Score: 0.8808\n",
      "Epoch 3 - Save Best Score: 0.8808 Model\n",
      "Epoch: [4][0/2850] Elapsed 0m 0s (remain 35m 15s) Loss: 0.0005(0.0005) Grad: 1511.4777  LR: 0.000009  \n",
      "Epoch: [4][100/2850] Elapsed 0m 50s (remain 22m 55s) Loss: 0.0186(0.0033) Grad: 21285.0820  LR: 0.000009  \n",
      "Epoch: [4][200/2850] Elapsed 1m 40s (remain 22m 7s) Loss: 0.0051(0.0036) Grad: 15966.0713  LR: 0.000009  \n",
      "Epoch: [4][300/2850] Elapsed 2m 31s (remain 21m 20s) Loss: 0.0000(0.0036) Grad: 246.5856  LR: 0.000008  \n",
      "Epoch: [4][400/2850] Elapsed 3m 21s (remain 20m 27s) Loss: 0.0004(0.0037) Grad: 1972.7294  LR: 0.000008  \n",
      "Epoch: [4][500/2850] Elapsed 4m 10s (remain 19m 36s) Loss: 0.0007(0.0036) Grad: 13890.2812  LR: 0.000008  \n",
      "Epoch: [4][600/2850] Elapsed 5m 1s (remain 18m 46s) Loss: 0.0028(0.0037) Grad: 7442.4658  LR: 0.000008  \n",
      "Epoch: [4][700/2850] Elapsed 5m 50s (remain 17m 55s) Loss: 0.0030(0.0036) Grad: 5387.0518  LR: 0.000008  \n",
      "Epoch: [4][800/2850] Elapsed 6m 40s (remain 17m 4s) Loss: 0.0003(0.0034) Grad: 5042.7896  LR: 0.000008  \n",
      "Epoch: [4][900/2850] Elapsed 7m 30s (remain 16m 14s) Loss: 0.0070(0.0034) Grad: 16893.7773  LR: 0.000007  \n",
      "Epoch: [4][1000/2850] Elapsed 8m 21s (remain 15m 26s) Loss: 0.0038(0.0035) Grad: 5232.0488  LR: 0.000007  \n",
      "Epoch: [4][1100/2850] Elapsed 9m 12s (remain 14m 38s) Loss: 0.0035(0.0035) Grad: 11949.9678  LR: 0.000007  \n",
      "Epoch: [4][1200/2850] Elapsed 10m 3s (remain 13m 48s) Loss: 0.0001(0.0036) Grad: 955.3539  LR: 0.000007  \n",
      "Epoch: [4][1300/2850] Elapsed 10m 52s (remain 12m 57s) Loss: 0.0171(0.0036) Grad: 34401.2773  LR: 0.000007  \n",
      "Epoch: [4][1400/2850] Elapsed 11m 42s (remain 12m 6s) Loss: 0.0000(0.0035) Grad: 13.5037  LR: 0.000007  \n",
      "Epoch: [4][1500/2850] Elapsed 12m 33s (remain 11m 17s) Loss: 0.0153(0.0036) Grad: 41356.4883  LR: 0.000007  \n",
      "Epoch: [4][1600/2850] Elapsed 13m 25s (remain 10m 28s) Loss: 0.0006(0.0036) Grad: 3695.4302  LR: 0.000006  \n",
      "Epoch: [4][1700/2850] Elapsed 14m 15s (remain 9m 38s) Loss: 0.0046(0.0037) Grad: 4143.3315  LR: 0.000006  \n",
      "Epoch: [4][1800/2850] Elapsed 15m 5s (remain 8m 47s) Loss: 0.0000(0.0037) Grad: 242.1469  LR: 0.000006  \n",
      "Epoch: [4][1900/2850] Elapsed 15m 56s (remain 7m 57s) Loss: 0.0002(0.0037) Grad: 1193.7230  LR: 0.000006  \n",
      "Epoch: [4][2000/2850] Elapsed 16m 47s (remain 7m 7s) Loss: 0.0089(0.0037) Grad: 15100.2539  LR: 0.000006  \n",
      "Epoch: [4][2100/2850] Elapsed 17m 39s (remain 6m 17s) Loss: 0.0029(0.0037) Grad: 16884.7500  LR: 0.000006  \n",
      "Epoch: [4][2200/2850] Elapsed 18m 29s (remain 5m 27s) Loss: 0.0003(0.0037) Grad: 2826.9072  LR: 0.000005  \n",
      "Epoch: [4][2300/2850] Elapsed 19m 19s (remain 4m 36s) Loss: 0.0005(0.0036) Grad: 3486.0227  LR: 0.000005  \n",
      "Epoch: [4][2400/2850] Elapsed 20m 10s (remain 3m 46s) Loss: 0.0073(0.0037) Grad: 13089.7197  LR: 0.000005  \n",
      "Epoch: [4][2500/2850] Elapsed 21m 0s (remain 2m 55s) Loss: 0.0016(0.0037) Grad: 5397.5684  LR: 0.000005  \n",
      "Epoch: [4][2600/2850] Elapsed 21m 50s (remain 2m 5s) Loss: 0.0002(0.0037) Grad: 1703.1639  LR: 0.000005  \n",
      "Epoch: [4][2700/2850] Elapsed 22m 40s (remain 1m 15s) Loss: 0.0003(0.0037) Grad: 2886.2998  LR: 0.000005  \n",
      "Epoch: [4][2800/2850] Elapsed 23m 30s (remain 0m 24s) Loss: 0.0035(0.0037) Grad: 10706.6562  LR: 0.000005  \n",
      "Epoch: [4][2849/2850] Elapsed 23m 54s (remain 0m 0s) Loss: 0.0060(0.0037) Grad: 16263.1016  LR: 0.000004  \n",
      "EVAL: [0/725] Elapsed 0m 0s (remain 5m 56s) Loss: 0.0318(0.0318) \n",
      "EVAL: [100/725] Elapsed 0m 27s (remain 2m 51s) Loss: 0.0018(0.0085) \n",
      "EVAL: [200/725] Elapsed 0m 55s (remain 2m 24s) Loss: 0.0025(0.0108) \n",
      "EVAL: [300/725] Elapsed 1m 23s (remain 1m 57s) Loss: 0.0337(0.0096) \n",
      "EVAL: [400/725] Elapsed 1m 51s (remain 1m 30s) Loss: 0.0574(0.0105) \n",
      "EVAL: [500/725] Elapsed 2m 18s (remain 1m 2s) Loss: 0.0088(0.0105) \n",
      "EVAL: [600/725] Elapsed 2m 46s (remain 0m 34s) Loss: 0.0143(0.0102) \n",
      "EVAL: [700/725] Elapsed 3m 14s (remain 0m 6s) Loss: 0.0000(0.0094) \n",
      "EVAL: [724/725] Elapsed 3m 20s (remain 0m 0s) Loss: 0.0169(0.0093) \n",
      "Epoch 4 - avg_train_loss: 0.0037  avg_val_loss: 0.0093  time: 1640s\n",
      "Epoch 4 - Score: 0.8845\n",
      "Epoch 4 - Save Best Score: 0.8845 Model\n",
      "Epoch: [5][0/2850] Elapsed 0m 0s (remain 34m 38s) Loss: 0.0000(0.0000) Grad: 25.9587  LR: 0.000004  \n",
      "Epoch: [5][100/2850] Elapsed 0m 51s (remain 23m 8s) Loss: 0.0002(0.0036) Grad: 973.8099  LR: 0.000004  \n",
      "Epoch: [5][200/2850] Elapsed 1m 41s (remain 22m 22s) Loss: 0.0000(0.0036) Grad: 18.6986  LR: 0.000004  \n",
      "Epoch: [5][300/2850] Elapsed 2m 31s (remain 21m 23s) Loss: 0.0011(0.0032) Grad: 18147.2695  LR: 0.000004  \n",
      "Epoch: [5][400/2850] Elapsed 3m 21s (remain 20m 30s) Loss: 0.0001(0.0031) Grad: 828.7735  LR: 0.000004  \n",
      "Epoch: [5][500/2850] Elapsed 4m 12s (remain 19m 44s) Loss: 0.0020(0.0029) Grad: 8506.5000  LR: 0.000004  \n",
      "Epoch: [5][600/2850] Elapsed 5m 2s (remain 18m 53s) Loss: 0.0009(0.0028) Grad: 3691.0288  LR: 0.000004  \n",
      "Epoch: [5][700/2850] Elapsed 5m 52s (remain 18m 0s) Loss: 0.0001(0.0028) Grad: 824.3530  LR: 0.000003  \n",
      "Epoch: [5][800/2850] Elapsed 6m 42s (remain 17m 9s) Loss: 0.0000(0.0028) Grad: 19.7222  LR: 0.000003  \n",
      "Epoch: [5][900/2850] Elapsed 7m 32s (remain 16m 18s) Loss: 0.0000(0.0030) Grad: 7.9321  LR: 0.000003  \n",
      "Epoch: [5][1000/2850] Elapsed 8m 22s (remain 15m 27s) Loss: 0.0188(0.0031) Grad: 157423.5625  LR: 0.000003  \n",
      "Epoch: [5][1100/2850] Elapsed 9m 13s (remain 14m 39s) Loss: 0.0000(0.0030) Grad: 17.2043  LR: 0.000003  \n",
      "Epoch: [5][1200/2850] Elapsed 10m 3s (remain 13m 48s) Loss: 0.0017(0.0030) Grad: 8468.2910  LR: 0.000003  \n",
      "Epoch: [5][1300/2850] Elapsed 10m 53s (remain 12m 58s) Loss: 0.0000(0.0029) Grad: 12.3286  LR: 0.000002  \n",
      "Epoch: [5][1400/2850] Elapsed 11m 44s (remain 12m 8s) Loss: 0.0034(0.0030) Grad: 29825.8438  LR: 0.000002  \n",
      "Epoch: [5][1500/2850] Elapsed 12m 33s (remain 11m 17s) Loss: 0.0013(0.0030) Grad: 7396.6499  LR: 0.000002  \n",
      "Epoch: [5][1600/2850] Elapsed 13m 23s (remain 10m 27s) Loss: 0.0026(0.0030) Grad: 19462.1621  LR: 0.000002  \n",
      "Epoch: [5][1700/2850] Elapsed 14m 13s (remain 9m 36s) Loss: 0.0219(0.0030) Grad: 47034.7930  LR: 0.000002  \n",
      "Epoch: [5][1800/2850] Elapsed 15m 4s (remain 8m 46s) Loss: 0.0019(0.0030) Grad: 11955.5098  LR: 0.000002  \n",
      "Epoch: [5][1900/2850] Elapsed 15m 55s (remain 7m 56s) Loss: 0.0003(0.0030) Grad: 2220.3611  LR: 0.000001  \n",
      "Epoch: [5][2000/2850] Elapsed 16m 45s (remain 7m 6s) Loss: 0.0000(0.0031) Grad: 88.9066  LR: 0.000001  \n",
      "Epoch: [5][2100/2850] Elapsed 17m 35s (remain 6m 16s) Loss: 0.0000(0.0031) Grad: 21.1981  LR: 0.000001  \n",
      "Epoch: [5][2200/2850] Elapsed 18m 25s (remain 5m 25s) Loss: 0.0084(0.0031) Grad: 89557.6797  LR: 0.000001  \n",
      "Epoch: [5][2300/2850] Elapsed 19m 15s (remain 4m 35s) Loss: 0.0004(0.0030) Grad: 3705.0527  LR: 0.000001  \n",
      "Epoch: [5][2400/2850] Elapsed 20m 6s (remain 3m 45s) Loss: 0.0011(0.0030) Grad: 8766.0693  LR: 0.000001  \n",
      "Epoch: [5][2500/2850] Elapsed 20m 56s (remain 2m 55s) Loss: 0.0022(0.0030) Grad: 8929.2246  LR: 0.000001  \n",
      "Epoch: [5][2600/2850] Elapsed 21m 46s (remain 2m 5s) Loss: 0.0138(0.0030) Grad: 14192.2646  LR: 0.000000  \n",
      "Epoch: [5][2700/2850] Elapsed 22m 36s (remain 1m 14s) Loss: 0.0024(0.0030) Grad: 21416.5488  LR: 0.000000  \n",
      "Epoch: [5][2800/2850] Elapsed 23m 26s (remain 0m 24s) Loss: 0.0030(0.0030) Grad: 9723.0430  LR: 0.000000  \n",
      "Epoch: [5][2849/2850] Elapsed 23m 51s (remain 0m 0s) Loss: 0.0003(0.0030) Grad: 2592.9673  LR: 0.000000  \n",
      "EVAL: [0/725] Elapsed 0m 0s (remain 5m 54s) Loss: 0.0293(0.0293) \n",
      "EVAL: [100/725] Elapsed 0m 28s (remain 2m 54s) Loss: 0.0017(0.0091) \n",
      "EVAL: [200/725] Elapsed 0m 56s (remain 2m 27s) Loss: 0.0029(0.0117) \n",
      "EVAL: [300/725] Elapsed 1m 24s (remain 1m 58s) Loss: 0.0410(0.0105) \n",
      "EVAL: [400/725] Elapsed 1m 52s (remain 1m 30s) Loss: 0.0492(0.0115) \n",
      "EVAL: [500/725] Elapsed 2m 19s (remain 1m 2s) Loss: 0.0102(0.0114) \n",
      "EVAL: [600/725] Elapsed 2m 47s (remain 0m 34s) Loss: 0.0165(0.0110) \n",
      "EVAL: [700/725] Elapsed 3m 14s (remain 0m 6s) Loss: 0.0001(0.0102) \n",
      "EVAL: [724/725] Elapsed 3m 21s (remain 0m 0s) Loss: 0.0177(0.0101) \n",
      "Epoch 5 - avg_train_loss: 0.0030  avg_val_loss: 0.0101  time: 1638s\n",
      "Epoch 5 - Score: 0.8836\n",
      "Best thres: 0.5, Score: 0.8836\n",
      "Best thres: 0.48164062500000004, Score: 0.8837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb7fcb8800e46c3997d9ffaabf5281e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c86a2de9a14494298738e6e53efd8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da63e58ba9d471083c9a825585602df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9913724d12c540fe9777170025a14658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292dbf22a32e42acb73402124e7527d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "name": "nbme-exp011.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 641.572862,
   "end_time": "2022-02-27T11:39:50.972497",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-27T11:29:09.399635",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0260998578564385a0b5b9425a0a5ca1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1141ae38bc6f473aab89db14fa4eeacf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad49cbf6b6e84ccaab873458182f22a1",
      "placeholder": "​",
      "style": "IPY_MODEL_5375de82ce3a41a8b5550e0a6b4316c1",
      "value": " 42146/42146 [00:36&lt;00:00, 2019.05it/s]"
     }
    },
    "220f78b6119042af8729543465e1234e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "25bf78e432e641e0a435dc3626c3ee8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2e32fee744ef42e0aaa89a7b03e82427": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e3818222bab4603a896be5976cb8409": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9754a5f1e61d49c8972df40ee9290375",
      "placeholder": "​",
      "style": "IPY_MODEL_25bf78e432e641e0a435dc3626c3ee8a",
      "value": " 143/143 [00:00&lt;00:00, 2166.88it/s]"
     }
    },
    "40e6583408c447199ff5b94d23601936": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e32fee744ef42e0aaa89a7b03e82427",
      "max": 42146,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d51d3aa414db4aa8b0ccae896e671152",
      "value": 42146
     }
    },
    "428ca357bd284d199e2558b1f577d79a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "47b8a7f3d0544d79b30ad02e4222082e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5375de82ce3a41a8b5550e0a6b4316c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e66444e9c714134bd2765cb3b6d1f15": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b04b019813e458080f02bc9111433a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f1d7796e2174485a0d1b1e9a71d7ade",
      "max": 143,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e72cad76f875451a8e2479e2df237575",
      "value": 143
     }
    },
    "7f1d7796e2174485a0d1b1e9a71d7ade": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a503d1abd884514a1e23101e03c6781": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e06e5e9eb0414b6fad63bdc99b44a313",
       "IPY_MODEL_6b04b019813e458080f02bc9111433a6",
       "IPY_MODEL_2e3818222bab4603a896be5976cb8409"
      ],
      "layout": "IPY_MODEL_eeb468dbb94943fcb30219d4dd98fcab"
     }
    },
    "9754a5f1e61d49c8972df40ee9290375": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a31c60ff4dab48e08d2ef9293d85df6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c40d970496ff447a8c0b80d787b07a4d",
       "IPY_MODEL_40e6583408c447199ff5b94d23601936",
       "IPY_MODEL_1141ae38bc6f473aab89db14fa4eeacf"
      ],
      "layout": "IPY_MODEL_47b8a7f3d0544d79b30ad02e4222082e"
     }
    },
    "ad49cbf6b6e84ccaab873458182f22a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c40d970496ff447a8c0b80d787b07a4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0260998578564385a0b5b9425a0a5ca1",
      "placeholder": "​",
      "style": "IPY_MODEL_428ca357bd284d199e2558b1f577d79a",
      "value": "100%"
     }
    },
    "d51d3aa414db4aa8b0ccae896e671152": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e06e5e9eb0414b6fad63bdc99b44a313": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e66444e9c714134bd2765cb3b6d1f15",
      "placeholder": "​",
      "style": "IPY_MODEL_220f78b6119042af8729543465e1234e",
      "value": "100%"
     }
    },
    "e72cad76f875451a8e2479e2df237575": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eeb468dbb94943fcb30219d4dd98fcab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
