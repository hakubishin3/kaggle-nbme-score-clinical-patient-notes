{"cells":[{"cell_type":"markdown","metadata":{"id":"strange-hospital"},"source":["## References"]},{"cell_type":"markdown","metadata":{"id":"considerable-spouse"},"source":["- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"]},{"cell_type":"markdown","metadata":{"id":"opponent-intake"},"source":["## Configurations"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1650798046978,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"v9PR18V4jSyN","outputId":"68d5f984-17b4-42fa-82ac-3d36954a36de"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sun Apr 24 11:00:46 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P0    25W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":379,"status":"ok","timestamp":1650798047354,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"packed-lebanon"},"outputs":[],"source":["EXP_NAME = \"nbme-exp094\"\n","ENV = \"colab\"\n","DEBUG_MODE = False\n","SUBMISSION_MODE = False"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1650798047354,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"reduced-flesh","outputId":"984e5629-3c66-43e7-f3ab-a36bc05609f6"},"outputs":[{"name":"stdout","output_type":"stream","text":["env: TOKENIZERS_PARALLELISM=true\n"]}],"source":["%env TOKENIZERS_PARALLELISM=true"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1650798047355,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"passive-class"},"outputs":[],"source":["class CFG:\n","    env=ENV\n","    exp_name=EXP_NAME\n","    debug=DEBUG_MODE\n","    submission=SUBMISSION_MODE\n","    apex=True\n","    input_dir=None\n","    output_dir=None\n","    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n","    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n","    competition_name=\"nbme-score-clinical-patient-notes\"\n","    id_col=\"id\"\n","    target_col=\"location\"\n","    pretrained_model_name=\"microsoft/deberta-v3-large\"\n","    tokenizer=None\n","    max_len=None\n","    #pseudo_plain_path='../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl'\n","    pseudo_plain_path=\"./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\"\n","    n_pseudo_labels=100000\n","    output_dim=1\n","    dropout=0.2\n","    num_workers=4\n","    batch_size=3\n","    lr=2e-5\n","    betas=(0.9, 0.98)\n","    weight_decay=0.1\n","    alpha=1\n","    gamma=2\n","    smoothing=0.0001\n","    num_warmup_steps_rate=0.1\n","    batch_scheduler=True\n","    epochs=1\n","    n_fold=4\n","    train_fold=[0, 1, 2, 3]\n","    mask_aug_p=0.5\n","    mask_ratio=0.15\n","    seed=71\n","    gradient_accumulation_steps=2\n","    max_grad_norm=1000\n","    print_freq=100\n","    train=True\n","    inference=True"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1650798047355,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"greenhouse-dinner"},"outputs":[],"source":["if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.train_fold = [0, 1]\n","\n","if CFG.submission:\n","    CFG.train = False\n","    CFG.inference = True"]},{"cell_type":"markdown","metadata":{"id":"analyzed-japan"},"source":["## Directory Settings"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7903,"status":"ok","timestamp":1650798055252,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"architectural-advocate","outputId":"c3e9ff22-2cc4-4537-c971-598417ca209f"},"outputs":[{"name":"stdout","output_type":"stream","text":["colab\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: transformers==4.16.2 in /usr/local/lib/python3.7/dist-packages (4.16.2)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2.23.0)\n","Requirement already satisfied: tokenizers!=0.11.3,\u003e=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (0.12.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (0.0.49)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.64.0)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (0.5.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (3.6.0)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.11.3)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (1.21.6)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.1.0-\u003etransformers==4.16.2) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=20.0-\u003etransformers==4.16.2) (3.0.8)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003etransformers==4.16.2) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==4.16.2) (1.24.3)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==4.16.2) (2021.10.8)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==4.16.2) (2.10)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==4.16.2) (3.0.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers==4.16.2) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers==4.16.2) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers==4.16.2) (7.1.2)\n"]}],"source":["import sys\n","from pathlib import Path\n","\n","\n","print(CFG.env)\n","if CFG.env == \"colab\":\n","    # colab環境\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","    # install packages\n","    !pip install transformers==4.16.2\n","    !pip install -q sentencepiece==0.1.96\n","\n","elif CFG.env == \"local\":\n","    # ローカルサーバ\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","\n","elif CFG.env == \"kaggle\":\n","    # kaggle環境\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./\")"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":3890,"status":"ok","timestamp":1650798059139,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"above-worst"},"outputs":[],"source":["# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n","# This must be done before importing transformers\n","import shutil\n","from pathlib import Path\n","\n","if CFG.env == \"colab\":\n","    input_dir = Path(\"./drive/MyDrive/00.kaggle/input/deberta-v2-3-fast-tokenizer\")\n","    transformers_path = Path(\"/usr/local/lib/python3.7/dist-packages/transformers\")\n","else:\n","    input_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n","    transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n","\n","convert_file = input_dir / \"convert_slow_tokenizer.py\"\n","conversion_path = transformers_path/convert_file.name\n","\n","if conversion_path.exists():\n","    conversion_path.unlink()\n","\n","shutil.copy(convert_file, transformers_path)\n","deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n","\n","for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n","    filepath = deberta_v2_path/filename\n","    if filepath.exists():\n","        filepath.unlink()\n","\n","    shutil.copy(input_dir/filename, filepath)\n","    \n","    \n","from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":584,"status":"ok","timestamp":1650798059721,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"separate-albania"},"outputs":[],"source":["import gc\n","import os\n","import ast\n","import time\n","import math\n","import random\n","import itertools\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","from scipy.optimize import minimize\n","from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision.transforms as T\n","from torchvision.io import read_image\n","from torch.utils.data import DataLoader, Dataset\n","\n","from transformers import AutoModelForMaskedLM\n","from transformers import BartModel,BertModel,BertTokenizer\n","from transformers import DebertaModel,DebertaTokenizer\n","from transformers import RobertaModel,RobertaTokenizer\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"sharp-smith"},"source":["## Utilities"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1650798059721,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"thirty-genius"},"outputs":[],"source":["def micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on binary arrays.\n","\n","    Args:\n","        preds (list of lists of ints): Predictions.\n","        truths (list of lists of ints): Ground truths.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    # Micro : aggregating over all instances\n","    preds = np.concatenate(preds)\n","    truths = np.concatenate(truths)\n","    return f1_score(truths, preds)\n","\n","\n","def spans_to_binary(spans, length=None):\n","    \"\"\"\n","    Converts spans to a binary array indicating whether each character is in the span.\n","\n","    Args:\n","        spans (list of lists of two ints): Spans.\n","\n","    Returns:\n","        np array [length]: Binarized spans.\n","    \"\"\"\n","    length = np.max(spans) if length is None else length\n","    binary = np.zeros(length)\n","    for start, end in spans:\n","        binary[start:end] = 1\n","    return binary\n","\n","\n","def span_micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on spans.\n","\n","    Args:\n","        preds (list of lists of two ints): Prediction spans.\n","        truths (list of lists of two ints): Ground truth spans.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    bin_preds = []\n","    bin_truths = []\n","    for pred, truth in zip(preds, truths):\n","        if not len(pred) and not len(truth):\n","            continue\n","        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n","        bin_preds.append(spans_to_binary(pred, length))\n","        bin_truths.append(spans_to_binary(truth, length))\n","    return micro_f1(bin_preds, bin_truths)\n","\n","\n","def get_score(y_true, y_pred):\n","    score = span_micro_f1(y_true, y_pred)\n","    return score"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1650798059722,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"processed-longitude"},"outputs":[],"source":["def create_labels_for_scoring(df):\n","    # example: ['48 61', '111 128'] -\u003e [[48, 61], [111, 128]]\n","    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n","    for i in range(len(df)):\n","        lst = df.loc[i, \"location\"]\n","        if lst:\n","            new_lst = \";\".join(lst)\n","            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n","\n","    # create labels\n","    truths = []\n","    for location_list in df[\"location_for_create_labels\"].values:\n","        truth = []\n","        if len(location_list) \u003e 0:\n","            location = location_list[0]\n","            for loc in [s.split() for s in location.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                truth.append([start, end])\n","        truths.append(truth)\n","\n","    return truths\n","\n","\n","def get_char_probs(texts, token_probs, tokenizer):\n","    res = [np.zeros(len(t)) for t in texts]\n","    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n","        encoded = tokenizer(\n","            text=text,\n","            max_length=CFG.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n","            start, end = offset_mapping\n","            res[i][start:end] = pred\n","    return res\n","\n","\n","def get_predicted_location_str(char_probs, th=0.5):\n","    results = []\n","    for char_prob in char_probs:\n","        # result = np.where(char_prob \u003e= th)[0] + 1\n","        result = np.where(char_prob \u003e= th)[0]\n","        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n","        # result = [f\"{min(r)} {max(r)}\" for r in result]\n","        result = [f\"{min(r)} {max(r) + 1}\" for r in result]\n","        result = \";\".join(result)\n","        results.append(result)\n","    return results\n","\n","\n","def get_predictions(results):\n","    predictions = []\n","    for result in results:\n","        prediction = []\n","        if result != \"\":\n","            for loc in [s.split() for s in result.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                prediction.append([start, end])\n","        predictions.append(prediction)\n","    return predictions\n","\n","\n","def scoring(df, th=0.5, use_token_prob=True):\n","    labels = create_labels_for_scoring(df)\n","\n","    if use_token_prob:\n","        token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n","        char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n","    else:\n","        char_probs = df[[str(i) for i in range(CFG.max_char_len)]].values\n","        char_probs = [char_probs[i] for i in range(len(char_probs))]\n","\n","    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n","    preds = get_predictions(predicted_location_str)\n","\n","    score = get_score(labels, preds)\n","    return score\n","\n","\n","def get_best_thres(oof_df):\n","    def f1_opt(x):\n","        return -1 * scoring(oof_df, th=x)\n","\n","    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n","    return best_thres"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1650798059722,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"intermediate-tonight"},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return \"%dm %ds\" % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1650798059723,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"turkish-attraction"},"outputs":[],"source":["seed_everything()"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":346,"status":"ok","timestamp":1650798060065,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"governing-marina"},"outputs":[],"source":["def postprocess(texts, preds):\n","    fix_tokenize_dict = {\n","        'heart': ['h', 'eart'],\n","        'hair': ['h', 'air'],\n","        'adderal': ['a', 'dderal'],\n","        'mother': ['m', 'other'],\n","        'intermittent': ['i', 'ntermittent'],\n","        'temperature': ['t', 'emperature'],\n","        'episodes': ['e', 'pisodes'],\n","        'no': ['n', 'o'],\n","        'has': ['h', 'as'],\n","        'LMP': ['L', 'MP'],\n","        '10': ['1', '0'],\n","        'blood': ['b', 'lood'],\n","        'recurrent': ['r', 'ecurrent'],\n","        'denies': ['d', 'enies'],\n","        'sudden': ['s', 'udden'],\n","        'Sexually': ['S', 'exually'],\n","        'up': ['u', 'p'],\n","        'wakes': ['w', 'akes'],\n","        'sweats': ['s', 'weats'],\n","        'hot': ['h', 'ot'],\n","        'drenched': ['d', 'renched'],\n","        'gnawing': ['g', 'nawing'],\n","        'Uses': ['U', 'ses'],\n","        'Begin': ['B', 'egin'],\n","        'Nausea': ['N', 'ausea'],\n","        'Burning': ['B', 'urning'],\n","        'Started': ['S', 'tarted'],\n","        'neurvousness': ['n', 'eurvousness'],\n","        'constipation': ['c', 'onstipation'],\n","        'nervousness': ['n', 'ervousness'],\n","        'cold': ['c', 'old'],\n","        'loss': ['l', 'oss'],\n","        'CBC': ['C', 'BC'],\n","        'Hx': ['H', 'x'],\n","        'tingling': ['t', 'ingling'],\n","        'feels': ['f', 'eels'],\n","        'Lost': ['L', 'ost'],\n","        'she': ['s', 'he'],\n","        'racing': ['r', 'acing'],\n","        'throat': ['t', 'hroat'],\n","        'PATIENT': ['P', 'ATIENT'],\n","        'recreational': ['r', 'ecreational'],\n","        'clammy': ['c', 'lammy'],\n","        'numbness': ['n', 'umbness'],\n","        'like': ['l', 'ike'],\n","        'reports': ['r', 'eports'],\n","        'exercise': ['e', 'xercise'],\n","        'started': ['s', 'tarted'],\n","        'brough': ['b', 'rough'],\n","        'Associated': ['A', 'ssociated'],\n","        'exacerbated': ['e', 'xacerbated'],\n","        'sharp': ['s', 'harp'],\n","        'cannot': ['c', 'annot'],\n","        'heavy': ['h', 'eavy'],\n","        'fatigue': ['f', 'atigue'],\n","        'trouble': ['t', 'rouble'],\n","        'hearing': ['h', 'earing'],\n","        'reduced': ['r', 'educed'],\n","        'lack': ['l', 'ack'],\n","        'vomiting': ['v', 'omiting'],\n","        'generalized': ['g', 'eneralized'],\n","        'body': ['b', 'ody'],\n","        'all': ['a', 'll'],\n","        'scratchy': ['s', 'cratchy'],\n","        'mom': ['m', 'om'],\n","        'discomfort': ['d', 'iscomfort'],\n","        'CAD': ['C', 'AD'],\n","        'Thyroid': ['T', 'hyroid'],\n","        'BLADDER': ['B', 'LADDER'],\n","        'diarrhea': ['d', 'iarrhea'],\n","        'Started': ['S', 'tarted'],\n","        'Vaginal': ['V', 'aginal'],\n","        'sleeping': ['s', 'leeping'],\n","        'UNCLE': ['U', 'NCLE'],\n","        'USING': ['U', 'SING'],\n","        'BURNING': ['B', 'URNING'],\n","        'GETTING': ['G', 'ETTING'],\n","        'ETOH': ['E', 'TOH'],\n","        'ON': ['O', 'N'],\n","        'INITIALLY': ['I', 'NITIALLY'],\n","        'epigastric': ['e', 'pigastric'],\n","        'occurs': ['o', 'ccurs'],\n","        'began': ['b', 'egan'],\n","        'alleviated': ['a', 'lleviated'],\n","        'overwhelmed': ['o', 'verwhelmed'],\n","        'clamminess': ['c', 'lamminess'],\n","        'strongly': ['s', 'trongly'],\n","        'lump': ['l', 'ump'],\n","        'drugs': ['d', 'rugs'],\n","        'chest': ['c', 'hest'],\n","        'stuffy': ['s', 'tuffy'],\n","        'changes': ['c', 'hanges'],\n","        'trouble': ['t', 'rouble'],\n","        'takes': ['t', 'akes'],\n","        'tossing': ['t', 'ossing'],\n","        'Fam': ['F', 'am'],\n","        'sweating': ['s', 'weating'],\n","        'dyspareunia': ['d', 'yspareunia'],\n","        'irregular': ['i', 'rregular'],\n","        'time': ['t', 'ime'],\n","        'unpredictable': ['u', 'npredictable'],\n","        'darkened': ['d', 'arkened'],\n","        'anxiety': ['a', 'nxiety'],\n","        'nervous': ['n', 'ervous'],\n","        'TAKING': ['T', 'AKING'],\n","        'losing': ['l', 'osing'],\n","        'Difficulyt': ['D', 'ifficulyt'],\n","        'Appetite': ['A', 'ppetite'],\n","        'increased': ['i', 'ncreased'],\n","        'fingers': ['f', 'ingers'],\n","        'illicit': ['i', 'llicit'],\n","        'claminess': ['c', 'laminess'],\n","        'clamy': ['c', 'lamy'],\n","        'Recently': ['R', 'ecently'],\n","        'feeling': ['f', 'eeling'],\n","        'aggrav': ['a', 'ggrav'],\n","        'changing': ['c', 'hanging'],\n","        'unable': ['u', 'nable'],\n","        'SEEING': ['S', 'EEING'],\n","        'staying': ['s', 'taying'],\n","        'lightheadedness': ['l', 'ightheadedness'],\n","        'lighheadeness': ['l', 'ighheadeness'],\n","        'nail': ['n', 'ail'],\n","        'pounding': ['p', 'ounding'],\n","        'My': ['M', 'y'],\n","        'Father': ['F', 'ather'],\n","        'urinary': ['u', 'rinary'],\n","        'pain': ['p', 'ain'],\n","        'not': ['n', 'ot'],\n","        'lower': ['l', 'ower'],\n","        'menses': ['m', 'enses'],\n","        'at': ['a', 't'],\n","        'takes': ['t', 'akes'],\n","        'initally': ['i', 'nitally'],\n","        'melena': ['m', 'elena'],\n","        'BOWEL': ['B', 'OWEL'],\n","        'WEIGHT': ['W', 'EIGHT'],\n","        'difficulty': ['d', 'ifficulty'],\n","        'condo': ['c', 'ondo'],\n","        'experiences': ['e', 'xperiences'],\n","        'stuffy': ['s', 'tuffy'],\n","        'rhinorrhea': ['r', 'hinorrhea'],\n","        'felt': ['f', 'elt'],\n","        'feverish': ['f', 'everish'],\n","        'CYCLE': ['C', 'YCLE'],\n","        'tampon': ['t', 'ampon'],\n","        'Last': ['L', 'ast'],\n","        'Son': ['S', 'on'],\n","        'saw': ['s', 'aw'],\n","        'tightness': ['t', 'ightness'],\n","        'rash': ['r', 'ash'],\n","        'ibuprofen': ['i', 'buprofen'],\n","        'SCRATHY': ['S', 'CRATHY'],\n","        'PHOTOPHOBIA': ['P', 'HOTOPHOBIA'],\n","    }\n","    preds_pp = preds.copy()\n","    tk0 = tqdm(range(len(preds_pp)), total=len(preds_pp))\n","    for raw_idx in tk0:\n","        pred = preds[raw_idx]\n","        text = texts[raw_idx]\n","        if len(pred) != 0:\n","            # pp1: indexが1から始まる予測値は0から始まるように修正 ## 0.88579 -\u003e 0.88702\n","            if pred[0][0] == 1:\n","                preds_pp[raw_idx][0][0] = 0\n","            for p_index, pp in enumerate(pred):\n","                start, end = pred[p_index]\n","                # pp2: startとendが同じ予測値はstartを前に１ずらす ## 0.88702 -\u003e 0.88714\n","                if start == end:\n","                    preds_pp[raw_idx][p_index][0] = start - 1\n","                    start = start - 1\n","                # pp3: 始点が改行の場合始点を1つ後ろにずらす ## 0.88714 -\u003e 0.88746\n","                if text[start] == '\\n':\n","                    preds_pp[raw_idx][p_index][0] = start + 1\n","                    start = start + 1\n","                # pp4: 1-2などは-2で予測されることがあるので修正 ## 0.88746 -\u003e 0.88747\n","                if text[start-1].isdigit() and text[start] == '-' and text[start+1].isdigit():\n","                    preds_pp[raw_idx][p_index][0] = start - 1\n","                    start = start - 1\n","                if text[start-1].isdigit() and text[start] == '/' and text[start+1].isdigit():\n","                    preds_pp[raw_idx][p_index][0] = start - 1\n","                    start = start - 1\n","                # pp5: 67などは7で予測されることがあるので修正 ## 0.88747 -\u003e 0.88748\n","                if text[start-1].isdigit() and text[start].isdigit():\n","                    preds_pp[raw_idx][p_index][0] = start - 1\n","                    start = start - 1\n","                # pp6: 文頭が大文字で始まるものは大文字部分が除かれて予測されることがあるので修正 ## 0.88748 -\u003e 0.88761\n","                if text[start-2] == '.' and text[start-1].isupper():\n","                    preds_pp[raw_idx][p_index][0] = start - 1\n","                    start = start - 1\n","                if text[start-2] == ',' and text[start-1].isupper():\n","                    preds_pp[raw_idx][p_index][0] = start - 1\n","                    start = start - 1\n","                if text[start-2] == ':' and text[start-1].isupper():\n","                    preds_pp[raw_idx][p_index][0] = start - 1\n","                    start = start - 1\n","                if text[start-2] == '-' and text[start-1].isupper():\n","                    preds_pp[raw_idx][p_index][0] = start - 1\n","                    start = start - 1\n","                # pp7: heart -\u003e h + eart となっているようなものを修正する ## 0.88761 -\u003e 0.88806\n","                for key, fix_tokenize in fix_tokenize_dict.items():\n","                    _s, s = fix_tokenize[0], fix_tokenize[1]\n","                    if text[start-1].lower() == _s.lower() and text[start:start+len(s)].lower() == s.lower():\n","                        preds_pp[raw_idx][p_index][0] = start - 1\n","                        start = start - 1\n","    return preds_pp"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1650798060065,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"aware-national"},"outputs":[],"source":["def get_results_from_preds_list(preds):\n","    results = []\n","    for pred in preds:\n","        s = []\n","        for p in pred:\n","            s.append(' '.join(list(map(str, p))))\n","        s = ';'.join(s)\n","        results.append(s)\n","    return results"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1650798060065,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"western-panic"},"outputs":[],"source":["def trunc_pred(texts, preds):\n","    preds_pp = preds.copy()\n","    tk0 = tqdm(range(len(preds_pp)), total=len(preds_pp))\n","    for raw_idx in tk0:\n","        text = texts[raw_idx]\n","        num_text = len(text)\n","        preds_pp[raw_idx, num_text:] = 0\n","    return preds_pp"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1650798060066,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"guilty-matter"},"outputs":[],"source":["def create_label(pn_history, location_list, max_char_len):\n","    label = np.zeros(max_char_len)\n","    label[len(pn_history):] = -1\n","    if len(location_list) \u003e 0:\n","        for location in location_list:\n","            start, end = int(location[0]), int(location[1])\n","            label[start:end] = 1\n","    return label\n","\n","def get_preds_from_results(results, texts, max_char_len):\n","    labels = []\n","    for idx, result in enumerate(results):\n","        label = create_label(texts[idx], result, max_char_len)\n","        labels.append(label)\n","    labels = np.stack(labels)\n","    print(labels.shape)\n","    return labels"]},{"cell_type":"markdown","metadata":{"id":"rubber-drill"},"source":["## Data Loading"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":754,"status":"ok","timestamp":1650798060817,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"adjusted-citizen","outputId":"86ec2cb9-9eb1-4c19-cf0a-bb4918a2a7e8"},"outputs":[{"data":{"text/plain":["((14300, 6), (143, 3), (42146, 3), (5, 4))"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["train = pd.read_csv(CFG.input_dir / \"train.csv\")\n","features = pd.read_csv(CFG.input_dir / \"features.csv\")\n","patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n","test = pd.read_csv(CFG.input_dir / \"test.csv\")\n","\n","train.shape, features.shape, patient_notes.shape, test.shape"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1650798060818,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"monetary-geneva"},"outputs":[],"source":["if CFG.debug:\n","    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n","    print(train.shape)"]},{"cell_type":"markdown","metadata":{"id":"pacific-fishing"},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1650798060818,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"universal-alberta"},"outputs":[],"source":["def preprocess_features(features):\n","    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n","    return features\n","\n","\n","features = preprocess_features(features)"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1650798060819,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"hazardous-soundtrack"},"outputs":[],"source":["features['feature_text'] = features['feature_text'].str.lower()\n","patient_notes['pn_history'] = patient_notes['pn_history'].str.lower()"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1650798060819,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"other-sound","outputId":"07561c29-86be-4b77-8895-98c294830c42"},"outputs":[{"data":{"text/plain":["((14300, 8), (5, 6))"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","\n","train.shape, test.shape"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":323,"status":"ok","timestamp":1650798061137,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"searching-knife"},"outputs":[],"source":["train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n","train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1650798061138,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"theoretical-insider","outputId":"34c4ca09-ff83-49d7-f521-fe4d898d84ba"},"outputs":[{"data":{"text/plain":["0    4399\n","1    8181\n","2    1296\n","3     287\n","4      99\n","5      27\n","6       9\n","7       1\n","8       1\n","Name: annotation_length, dtype: int64"]},"metadata":{},"output_type":"display_data"}],"source":["train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n","display(train['annotation_length'].value_counts().sort_index())"]},{"cell_type":"markdown","metadata":{"id":"fewer-crowd"},"source":["## CV split"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1650798061139,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"promising-alberta","outputId":"cbf2579c-50e1-40d3-daed-298040e2467e"},"outputs":[{"data":{"text/plain":["fold\n","0    3575\n","1    3575\n","2    3575\n","3    3575\n","dtype: int64"]},"metadata":{},"output_type":"display_data"}],"source":["Fold = GroupKFold(n_splits=CFG.n_fold)\n","groups = train['pn_num'].values\n","for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n","    train.loc[val_index, 'fold'] = int(n)\n","train['fold'] = train['fold'].astype(int)\n","display(train.groupby('fold').size())"]},{"cell_type":"markdown","metadata":{"id":"honey-rebecca"},"source":["## Setup tokenizer"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4048,"status":"ok","timestamp":1650798065181,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"short-dodge","outputId":"4cbd68c8-a6cd-442d-d4d9-15cc7269f966"},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["if CFG.submission:\n","    tokenizer = DebertaV2TokenizerFast.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n","else:\n","    tokenizer = DebertaV2TokenizerFast.from_pretrained(CFG.pretrained_model_name)\n","    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n","\n","CFG.tokenizer = tokenizer"]},{"cell_type":"markdown","metadata":{"id":"portuguese-knock"},"source":["## Create dataset"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"elapsed":23621,"status":"ok","timestamp":1650798088799,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"featured-affect","outputId":"250bc50f-0902-4686-c9a3-de569dc0d75d"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b4b5881a571e4f04be84cbe8fe8ff2e3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/42146 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["max length: 284\n"]}],"source":["pn_history_lengths = []\n","tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    pn_history_lengths.append(length)\n","\n","print(\"max length:\", np.max(pn_history_lengths))"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1650798089115,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"detailed-philadelphia","outputId":"4df359e3-4e6c-4f05-de3d-e6f64eeb0924"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1458c8ab5a854f7fac8ff5d3fea15065","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/143 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["max length: 28\n"]}],"source":["feature_text_lengths = []\n","tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    feature_text_lengths.append(length)\n","\n","print(\"max length:\", np.max(feature_text_lengths))"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1650798089115,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"scientific-cisco","outputId":"ba9b8161-8d07-4ade-85cb-f7f049dd4855"},"outputs":[{"name":"stdout","output_type":"stream","text":["max length: 315\n"]}],"source":["CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls \u0026 sep \u0026 sep\n","\n","print(\"max length:\", CFG.max_len)"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1650798089115,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"sound-storage","outputId":"b585ce9d-01a2-459c-8123-b03ebbc20093"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"486dc2396f7f454782ac62bef2727921","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/42146 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["max length: 950\n"]}],"source":["pn_history_lengths = []\n","tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n","for text in tk0:\n","    length = len(text)\n","    pn_history_lengths.append(length)\n","\n","CFG.max_char_len = max(pn_history_lengths)\n","\n","print(\"max length:\", CFG.max_char_len)"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1650798089116,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"fixed-gather"},"outputs":[],"source":["class TrainingDataset(Dataset):\n","    def __init__(self, cfg, df, pseudo_label=None):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.mask_aug_p = self.cfg.mask_aug_p\n","        self.mask_ratio = self.cfg.mask_ratio\n","        self.max_char_len = self.cfg.max_char_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","        self.annotation_lengths = self.df[\"annotation_length\"].values\n","        self.locations = self.df[\"location\"].values\n","        if \"pseudo_idx\" in df.columns:\n","            self.pseudo_idx = self.df[\"pseudo_idx\"].values\n","            self.pseudo_label = pseudo_label\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def mask_augment(self, inputs):\n","        all_inds = np.arange(1, len(inputs[\"input_ids\"]) - 1)\n","        n_mask = max(int(len(all_inds) * self.mask_ratio), 1)\n","        np.random.shuffle(all_inds)\n","        mask_inds = all_inds[:n_mask]\n","        sep_ind = np.where(np.array(inputs[\"input_ids\"]) == 2)[0]\n","        mask_inds = np.array([i for i in mask_inds if i \u003c sep_ind[0]])\n","        inputs_ids = np.array(inputs[\"input_ids\"])\n","        try:\n","            inputs_ids[mask_inds] = tokenizer.mask_token_id\n","            inputs[\"input_ids\"] = list(inputs_ids)\n","            return inputs\n","        except:\n","            return inputs\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        if float(torch.rand(1)) \u003c self.mask_aug_p:\n","            encoded = self.mask_augment(encoded)\n","\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def _create_mapping_from_token_to_char(self, pn_history):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        mapping_from_token_to_char = np.zeros(self.max_char_len)\n","        offset_mapping = encoded[\"offset_mapping\"]\n","        for i, offset in enumerate(offset_mapping):\n","            start_idx, end_idx = offset\n","            mapping_from_token_to_char[start_idx:end_idx] = i\n","        return torch.tensor(mapping_from_token_to_char, dtype=torch.long)\n","\n","    def _create_label(self, pn_history, annotation_length, location_list):\n","        label = np.zeros(self.max_char_len)\n","        label[len(pn_history):] = -1\n","        if annotation_length \u003e 0:\n","            for location in location_list:\n","                for loc in [s.split() for s in location.split(\";\")]:\n","                    start, end = int(loc[0]), int(loc[1])\n","                    label[start:end] = 1\n","        return torch.tensor(label, dtype=torch.float)\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        if not np.isnan(self.annotation_lengths[idx]):\n","            label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n","        else:\n","            p_idx = int(self.pseudo_idx[idx])\n","            label = torch.tensor(self.pseudo_label[p_idx], dtype=torch.float)\n","        mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n","        return input_, label, mapping_from_token_to_char"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1650798089116,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"interesting-crown"},"outputs":[],"source":["class TestDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.max_char_len = self.cfg.max_char_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def _create_mapping_from_token_to_char(self, pn_history):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        mapping_from_token_to_char = np.zeros(self.max_char_len)\n","        offset_mapping = encoded[\"offset_mapping\"]\n","        for i, offset in enumerate(offset_mapping):\n","            start_idx, end_idx = offset\n","            mapping_from_token_to_char[start_idx:end_idx] = i\n","        return torch.tensor(mapping_from_token_to_char, dtype=torch.long)\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n","        return input_, mapping_from_token_to_char"]},{"cell_type":"markdown","metadata":{"id":"nonprofit-syntax"},"source":["## Model"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1650798089116,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"interior-history"},"outputs":[],"source":["from transformers.modeling_outputs import MaskedLMOutput\n","\n","class MaskedModel(nn.Module):\n","    def __init__(self, cfg, config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","        if config_path is None:\n","            self.config = AutoConfig.from_pretrained(\n","                cfg.pretrained_model_name,\n","                output_hidden_states=False\n","                )\n","        else:\n","            self.config = torch.load(config_path)\n","        \n","        if pretrained:\n","            self.model = AutoModel.from_pretrained(cfg.pretrained_model_name, config=self.config)\n","            self.lm_head = AutoModelForMaskedLM.from_pretrained(cfg.pretrained_model_name, config=self.config).cls # [cls, lm_head]\n","        else:\n","            self.model = AutoModel(self.config)\n","            self.lm_head = AutoModelForMaskedLM(self.config).cls # [cls, lm_head]\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def forward(\n","            self, \n","            input_ids=None,\n","            attention_mask=None,\n","            token_type_ids=None,\n","            #position_ids=None,\n","            inputs_embeds=None,\n","            labels=None,\n","            output_attentions=None,\n","            output_hidden_states=None,\n","            return_dict=None):\n","        \n","        outputs = self.model(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            #position_ids=position_ids,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,)\n","        \n","        sequence_output = outputs[0]\n","        prediction_scores = self.lm_head(sequence_output)\n","\n","        masked_lm_loss = None\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss()\n","            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n","\n","        return MaskedLMOutput(loss=masked_lm_loss,\n","                              logits=prediction_scores,\n","                              hidden_states=outputs.hidden_states,\n","                              attentions=outputs.attentions)"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":573,"status":"ok","timestamp":1650798089683,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"waiting-balance"},"outputs":[],"source":["class CustomModel(nn.Module):\n","    def __init__(self, cfg, model_config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        if model_config_path is None:\n","            self.model_config = AutoConfig.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                output_hidden_states=True,\n","            )\n","        else:\n","            self.model_config = torch.load(model_config_path)\n","\n","        if pretrained:\n","            self.backbone = AutoModel.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                config=self.model_config,\n","            )\n","            print(f\"Load weight from pretrained\")\n","        else:\n","            #self.backbone = AutoModel.from_config(self.model_config)\n","            # itpt = AutoModelForMaskedLM.from_config(self.model_config)\n","            #path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp010/checkpoint-130170/pytorch_model.bin\")\n","            # path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n","            # state_dict = torch.load(path)\n","            # itpt.load_state_dict(state_dict)\n","            path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\")\n","            #path = str(Path(\"../output\") / CFG.competition_name /  \"nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\")\n","            masked_model = MaskedModel(CFG, config_path=None, pretrained=True)\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            masked_model.load_state_dict(state)\n","            self.backbone = masked_model.model\n","            print(f\"Load weight from {path}\")\n","            del state, masked_model; gc.collect()\n","\n","        self.lstm = nn.GRU(\n","            input_size=self.model_config.hidden_size,\n","            bidirectional=True,\n","            hidden_size=self.model_config.hidden_size // 2,\n","            num_layers=4,\n","            dropout=self.cfg.dropout,\n","            batch_first=True,\n","        )\n","        self.fc = nn.Sequential(\n","            nn.Dropout(self.cfg.dropout),\n","            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n","        )\n","\n","    def forward(self, inputs, mappings_from_token_to_char):\n","        h = self.backbone(**inputs)[\"last_hidden_state\"]  # [batch, seq_len, d_model]\n","        mappings_from_token_to_char = mappings_from_token_to_char.unsqueeze(2).expand(-1, -1, self.model_config.hidden_size)\n","        h = torch.gather(h, 1, mappings_from_token_to_char)    # [batch, seq_len, d_model]\n","        h, _ = self.lstm(h)\n","        output = self.fc(h)\n","\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"passive-genealogy"},"source":["## Training"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1650798089684,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"wTOMVCkAguWN"},"outputs":[],"source":["class FocalLoss(nn.Module):\n","    def __init__(self, reduction='none', alpha=1, gamma=2):\n","        super().__init__()\n","        self.reduction = reduction\n","        self.alpha = alpha\n","        self.gamma = gamma\n","\n","    def forward(self, inputs, targets):\n","        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n","        pt = torch.exp(-bce_loss)\n","        loss = self.alpha * (1. - pt)**self.gamma * bce_loss\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss\n","\n","\n","class SmoothFocalLoss(nn.Module):\n","    def __init__(self, reduction='none', alpha=1, gamma=2, smoothing=0.0):\n","        super().__init__()\n","        self.reduction = reduction\n","        self.focal_loss = FocalLoss(reduction='none', alpha=alpha, gamma=gamma)\n","        self.smoothing = smoothing\n","\n","    @staticmethod\n","    def _smooth(targets:torch.Tensor, smoothing=0.0):\n","        assert 0 \u003c= smoothing \u003c 1\n","        with torch.no_grad():\n","            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n","        return targets\n","\n","    def forward(self, inputs, targets):\n","        targets = SmoothFocalLoss._smooth(targets, self.smoothing)\n","        loss = self.focal_loss(inputs, targets)\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss\n","\n","    \n","class CEFocalLoss(nn.Module):\n","    def __init__(self, reduction='none', alpha=1, gamma=2):\n","        super(CEFocalLoss, self).__init__()\n","        self.reduction = reduction\n","        self.alpha = alpha\n","        self.gamma = gamma\n","\n","    def forward(self, inputs, targets):\n","        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n","        pt = torch.exp(-ce_loss)\n","        loss = self.alpha * (1. - pt)**self.gamma * ce_loss\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss\n","\n","    \n","class SmoothCEFocalLoss(nn.Module):\n","    def __init__(self, reduction='none', alpha=1, gamma=2, smoothing=0.0):\n","        super(SmoothCEFocalLoss, self).__init__()\n","        self.reduction = reduction\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.smoothing = smoothing\n","\n","    def forward(self, inputs, targets):\n","        ce_loss = F.cross_entropy(inputs, targets, reduction='none', label_smoothing=self.smoothing) # torch \u003e= 1.10.0\n","        pt = torch.exp(-ce_loss)\n","        loss = self.alpha * (1. - pt)**self.gamma * ce_loss\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1650798089684,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"supposed-bernard"},"outputs":[],"source":["def train_fn(\n","    train_dataloader,\n","    model,\n","    criterion,\n","    optimizer,\n","    epoch,\n","    scheduler,\n","    device,\n","):\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels, mappings_from_token_to_char) in enumerate(train_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device) \n","        batch_size = labels.size(0)\n","        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            output = model(inputs, mappings_from_token_to_char)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","        loss = loss.mean()\n","\n","        if CFG.gradient_accumulation_steps \u003e 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","\n","        if CFG.batch_scheduler:\n","            scheduler.step()\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n","            print(\n","                \"Epoch: [{0}][{1}/{2}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                \"Grad: {grad_norm:.4f}  \"\n","                \"LR: {lr:.6f}  \"\n","                .format(\n","                    epoch+1,\n","                    step,\n","                    len(train_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n","                    loss=losses,\n","                     grad_norm=grad_norm,\n","                     lr=scheduler.get_lr()[0],\n","                )\n","            )\n","    del output, loss, inputs, labels, mappings_from_token_to_char, scaler, grad_norm; gc.collect()\n","    torch.cuda.empty_cache()\n","    return losses.avg"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1650798089685,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"immediate-break"},"outputs":[],"source":["def valid_fn(\n","    val_dataloader,\n","    model,\n","    criterion,\n","    device,\n","):\n","    model.eval()\n","    preds = []\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels, mappings_from_token_to_char) in enumerate(val_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device) \n","        batch_size = labels.size(0)\n","        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            output = model(inputs, mappings_from_token_to_char)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","        loss = loss.mean()\n","    \n","        if CFG.gradient_accumulation_steps \u003e 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n","            print(\n","                \"EVAL: [{0}/{1}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                .format(\n","                    step, len(val_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n","                    loss=losses,\n","                )\n","            )\n","    preds = np.concatenate(preds)\n","    return losses.avg, preds"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1650798089685,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"oriented-arizona"},"outputs":[],"source":["def inference_fn(test_dataloader, model, device):\n","    model.eval()\n","    model.to(device)\n","    preds = []\n","    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n","    for (inputs, mappings_from_token_to_char) in tk0:\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n","\n","        with torch.no_grad():\n","            output = model(inputs, mappings_from_token_to_char)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","    preds = np.concatenate(preds)\n","    return preds"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1650798089686,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"polyphonic-astrology"},"outputs":[],"source":["def train_loop(df, i_fold, device):\n","    print(f\"========== fold: {i_fold} training ==========\")\n","    train_idx = df[df[\"fold\"] != i_fold].index\n","    val_idx = df[df[\"fold\"] == i_fold].index\n","\n","    train_folds = df.loc[train_idx].reset_index(drop=True)\n","    val_folds = df.loc[val_idx].reset_index(drop=True)\n","\n","    if CFG.pseudo_plain_path is not None:\n","        pseudo_plain = pd.read_pickle(CFG.pseudo_plain_path)\n","        print(f\"get pseudo plain from {CFG.pseudo_plain_path}\")\n","        pseudo_label_list = []\n","        weights = [0.4433659049657008, 0.20859987143371844, 0.3480342236005807]\n","        for exp_name in [\"nbme-exp060\", \"nbme-exp067\", \"nbme-exp083\"]:\n","            pseudo_label_path = f'./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/{exp_name}/pseudo_labels_{i_fold}.npy'\n","            #pseudo_label_path = f'../output/nbme-score-clinical-patient-notes/{exp_name}/pseudo_labels_{i_fold}.npy'\n","            pseudo_label = np.load(pseudo_label_path)\n","            print(f\"get pseudo labels from {pseudo_label_path}\")\n","            pseudo_label_list.append(pseudo_label)\n","\n","        pseudo_label = weights[0] * pseudo_label_list[0] + weights[1] * pseudo_label_list[1] + weights[2] * pseudo_label_list[2]\n","        pseudo_label = trunc_pred(pseudo_plain[\"pn_history\"].values, pseudo_label)\n","        predicted_location_str = get_predicted_location_str(pseudo_label, th=0.5)\n","        preds = get_predictions(predicted_location_str)\n","        results_postprocess = postprocess(pseudo_plain[\"pn_history\"].values, preds)\n","        #results_postprocess = get_results_from_preds_list(results_postprocess)\n","        pseudo_label = get_preds_from_results(results_postprocess, pseudo_plain[\"pn_history\"].values, pseudo_label.shape[1])\n","        print(pseudo_plain.shape, pseudo_label.shape)\n","\n","        pseudo_plain['feature_text'] = pseudo_plain['feature_text'].str.lower()\n","        pseudo_plain['pn_history'] = pseudo_plain['pn_history'].str.lower()\n","\n","        pseudo_plain[\"pseudo_idx\"] = np.arange(len(pseudo_plain))\n","        pseudo_plain = pseudo_plain.sample(n=CFG.n_pseudo_labels)\n","        print(pseudo_plain.shape)\n","        train_folds = pd.concat([train_folds, pseudo_plain], axis=0, ignore_index=True)\n","        print(train_folds.shape)\n","\n","    train_dataset = TrainingDataset(CFG, train_folds, pseudo_label)\n","    val_dataset = TrainingDataset(CFG, val_folds)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=True,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=True,\n","    )\n","    val_dataloader = DataLoader(\n","        val_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=False,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=False,\n","    )\n","\n","    # model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","    model = CustomModel(CFG, model_config_path=None, pretrained=False)   # itptを使うため\n","    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n","    model.to(device)\n","\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\"params\": [p for n, p in param_optimizer if not any(\n","            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n","        {\"params\": [p for n, p in param_optimizer if any(\n","            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n","    ]\n","    optimizer = AdamW(\n","        optimizer_grouped_parameters,\n","        lr=CFG.lr,\n","        betas=CFG.betas,\n","        weight_decay=CFG.weight_decay,\n","    )\n","    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n","    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        num_training_steps=num_train_optimization_steps,\n","    )\n","\n","    criterion = SmoothFocalLoss(reduction='none', alpha=CFG.alpha, gamma=CFG.gamma, smoothing=CFG.smoothing)\n","    #criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    best_score = -1 * np.inf\n","\n","    for epoch in range(CFG.epochs):\n","        start_time = time.time()\n","        avg_loss = train_fn(\n","            train_dataloader,\n","            model,\n","            criterion,\n","            optimizer,\n","            epoch,\n","            scheduler,\n","            device,\n","        )\n","        avg_val_loss, val_preds = valid_fn(\n","            val_dataloader,\n","            model,\n","            criterion,\n","            device,\n","        )\n","\n","        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n","            scheduler.step()\n","\n","        # scoring\n","        val_folds[[str(i) for i in range(CFG.max_char_len)]] = val_preds\n","        score = scoring(val_folds, th=0.5, use_token_prob=False)\n","\n","        elapsed = time.time() - start_time\n","\n","        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n","        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n","        if score \u003e best_score:\n","            best_score = score\n","            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n","            torch.save({\n","                \"model\": model.state_dict(),\n","                \"predictions\": val_preds,\n","                },\n","                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","            )\n","\n","    predictions = torch.load(\n","        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","        map_location=torch.device(\"cpu\"),\n","    )[\"predictions\"]\n","    val_folds[[str(i) for i in range(CFG.max_char_len)]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return val_folds"]},{"cell_type":"markdown","metadata":{"id":"superior-globe"},"source":["## Main"]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1650798089687,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"recreational-association"},"outputs":[],"source":["def main():\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    if CFG.train:\n","        oof_df = pd.DataFrame()\n","        for i_fold in range(CFG.n_fold):\n","            if i_fold in CFG.train_fold:\n","                _oof_df = train_loop(train, i_fold, device)\n","                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n","        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    if CFG.submission:\n","        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n","    else:\n","        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    best_thres = 0.5\n","    best_score = 0.\n","    for th in np.arange(0.45, 0.55, 0.01):\n","        th = np.round(th, 2)\n","        score = scoring(oof_df, th=th, use_token_prob=False)\n","        if best_score \u003c score:\n","            best_thres = th\n","            best_score = score\n","    print(f\"best_thres: {best_thres}  score: {best_score:.5f}\")\n","\n","    if CFG.inference:\n","        test_dataset = TestDataset(CFG, test)\n","        test_dataloader = DataLoader(\n","            test_dataset,\n","            batch_size=CFG.batch_size,\n","            shuffle=False,\n","            num_workers=CFG.num_workers,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","        predictions = []\n","        for i_fold in CFG.train_fold:\n","            if CFG.submission:\n","                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n","                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n","            else:\n","                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n","\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            model.load_state_dict(state[\"model\"])\n","            print(f\"load weights from {path}\")\n","            test_char_probs = inference_fn(test_dataloader, model, device)\n","            predictions.append(test_char_probs)\n","\n","            del state, test_char_probs, model; gc.collect()\n","            torch.cuda.empty_cache()\n","\n","        predictions = np.mean(predictions, axis=0)\n","        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n","        test[CFG.target_col] = predicted_location_str\n","        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n","        test[[CFG.id_col, CFG.target_col]].to_csv(\n","            CFG.output_dir / \"submission.csv\", index=False\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"committed-express"},"outputs":[{"name":"stdout","output_type":"stream","text":["========== fold: 0 training ==========\n","get pseudo plain from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n","get pseudo labels from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_0.npy\n","get pseudo labels from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_0.npy\n","get pseudo labels from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp083/pseudo_labels_0.npy\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f94a6bd8355241ad912d728a297ec6fa","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/612602 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8eee5e4a5a934b208f18dad0a17d5038","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/612602 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["(612602, 950)\n","(612602, 6) (612602, 950)\n","(100000, 7)\n","(110725, 11)\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\n","Epoch: [1][0/36908] Elapsed 0m 1s (remain 740m 15s) Loss: 0.0860(0.0860) Grad: 81389.8906  LR: 0.000000  \n","Epoch: [1][100/36908] Elapsed 1m 4s (remain 392m 50s) Loss: 0.0684(0.0810) Grad: 65753.2969  LR: 0.000001  \n","Epoch: [1][200/36908] Elapsed 2m 8s (remain 391m 38s) Loss: 0.0338(0.0662) Grad: 32102.8203  LR: 0.000001  \n","Epoch: [1][300/36908] Elapsed 3m 12s (remain 391m 3s) Loss: 0.0164(0.0513) Grad: 4301.4058  LR: 0.000002  \n","Epoch: [1][400/36908] Elapsed 4m 17s (remain 389m 58s) Loss: 0.0130(0.0419) Grad: 3926.3936  LR: 0.000002  \n","Epoch: [1][500/36908] Elapsed 5m 21s (remain 388m 53s) Loss: 0.0299(0.0361) Grad: 14480.1758  LR: 0.000003  \n","Epoch: [1][600/36908] Elapsed 6m 25s (remain 388m 19s) Loss: 0.0100(0.0322) Grad: 3048.4497  LR: 0.000003  \n","Epoch: [1][700/36908] Elapsed 7m 30s (remain 387m 47s) Loss: 0.0079(0.0294) Grad: 3465.6860  LR: 0.000004  \n","Epoch: [1][800/36908] Elapsed 8m 34s (remain 386m 48s) Loss: 0.0096(0.0271) Grad: 3245.2825  LR: 0.000004  \n","Epoch: [1][900/36908] Elapsed 9m 39s (remain 385m 49s) Loss: 0.0176(0.0253) Grad: 8620.5850  LR: 0.000005  \n","Epoch: [1][1000/36908] Elapsed 10m 43s (remain 384m 31s) Loss: 0.0099(0.0240) Grad: 3977.7161  LR: 0.000005  \n","Epoch: [1][1100/36908] Elapsed 11m 47s (remain 383m 30s) Loss: 0.0072(0.0228) Grad: 4719.3931  LR: 0.000006  \n","Epoch: [1][1200/36908] Elapsed 12m 52s (remain 382m 48s) Loss: 0.0032(0.0217) Grad: 5300.8950  LR: 0.000007  \n","Epoch: [1][1300/36908] Elapsed 13m 57s (remain 381m 58s) Loss: 0.0052(0.0204) Grad: 18683.5566  LR: 0.000007  \n","Epoch: [1][1400/36908] Elapsed 15m 1s (remain 380m 52s) Loss: 0.0007(0.0194) Grad: 1539.1777  LR: 0.000008  \n","Epoch: [1][1500/36908] Elapsed 16m 5s (remain 379m 40s) Loss: 0.0054(0.0185) Grad: 26219.6719  LR: 0.000008  \n","Epoch: [1][1600/36908] Elapsed 17m 10s (remain 378m 37s) Loss: 0.0010(0.0176) Grad: 2666.4705  LR: 0.000009  \n","Epoch: [1][1700/36908] Elapsed 18m 14s (remain 377m 28s) Loss: 0.0021(0.0168) Grad: 8725.2080  LR: 0.000009  \n","Epoch: [1][1800/36908] Elapsed 19m 19s (remain 376m 35s) Loss: 0.0091(0.0161) Grad: 29691.5898  LR: 0.000010  \n","Epoch: [1][1900/36908] Elapsed 20m 22s (remain 375m 15s) Loss: 0.0020(0.0154) Grad: 14937.1699  LR: 0.000010  \n","Epoch: [1][2000/36908] Elapsed 21m 26s (remain 373m 59s) Loss: 0.0029(0.0148) Grad: 7611.4561  LR: 0.000011  \n","Epoch: [1][2100/36908] Elapsed 22m 30s (remain 372m 52s) Loss: 0.0007(0.0143) Grad: 1764.6860  LR: 0.000011  \n","Epoch: [1][2200/36908] Elapsed 23m 34s (remain 371m 48s) Loss: 0.0010(0.0138) Grad: 6816.6260  LR: 0.000012  \n","Epoch: [1][2300/36908] Elapsed 24m 38s (remain 370m 37s) Loss: 0.0004(0.0134) Grad: 1190.4297  LR: 0.000012  \n","Epoch: [1][2400/36908] Elapsed 25m 42s (remain 369m 24s) Loss: 0.0006(0.0129) Grad: 1435.7522  LR: 0.000013  \n","Epoch: [1][2500/36908] Elapsed 26m 46s (remain 368m 17s) Loss: 0.0086(0.0125) Grad: 23918.2012  LR: 0.000014  \n","Epoch: [1][2600/36908] Elapsed 27m 50s (remain 367m 8s) Loss: 0.0105(0.0122) Grad: 19541.2285  LR: 0.000014  \n","Epoch: [1][2700/36908] Elapsed 28m 53s (remain 365m 59s) Loss: 0.0026(0.0118) Grad: 9304.0908  LR: 0.000015  \n","Epoch: [1][2800/36908] Elapsed 29m 56s (remain 364m 41s) Loss: 0.0024(0.0115) Grad: 11146.2969  LR: 0.000015  \n","Epoch: [1][2900/36908] Elapsed 31m 0s (remain 363m 28s) Loss: 0.0024(0.0112) Grad: 6114.1392  LR: 0.000016  \n","Epoch: [1][3000/36908] Elapsed 32m 4s (remain 362m 21s) Loss: 0.0013(0.0109) Grad: 5106.5415  LR: 0.000016  \n","Epoch: [1][3100/36908] Elapsed 33m 8s (remain 361m 18s) Loss: 0.0003(0.0107) Grad: 517.8190  LR: 0.000017  \n","Epoch: [1][3200/36908] Elapsed 34m 12s (remain 360m 9s) Loss: 0.0003(0.0105) Grad: 488.8037  LR: 0.000017  \n","Epoch: [1][3300/36908] Elapsed 35m 15s (remain 358m 59s) Loss: 0.0005(0.0102) Grad: 7278.3794  LR: 0.000018  \n","Epoch: [1][3400/36908] Elapsed 36m 19s (remain 357m 56s) Loss: 0.0004(0.0100) Grad: 699.3036  LR: 0.000018  \n","Epoch: [1][3500/36908] Elapsed 37m 24s (remain 356m 53s) Loss: 0.0016(0.0098) Grad: 18140.0469  LR: 0.000019  \n","Epoch: [1][3600/36908] Elapsed 38m 28s (remain 355m 50s) Loss: 0.0039(0.0096) Grad: 29199.8984  LR: 0.000020  \n","Epoch: [1][3700/36908] Elapsed 39m 32s (remain 354m 45s) Loss: 0.0001(0.0094) Grad: 66.8390  LR: 0.000020  \n","Epoch: [1][3800/36908] Elapsed 40m 36s (remain 353m 39s) Loss: 0.0032(0.0092) Grad: 3998.3555  LR: 0.000020  \n","Epoch: [1][3900/36908] Elapsed 41m 40s (remain 352m 36s) Loss: 0.0025(0.0090) Grad: 3222.6125  LR: 0.000020  \n","Epoch: [1][4000/36908] Elapsed 42m 44s (remain 351m 35s) Loss: 0.0065(0.0089) Grad: 23945.5508  LR: 0.000020  \n","Epoch: [1][4100/36908] Elapsed 43m 48s (remain 350m 29s) Loss: 0.0035(0.0087) Grad: 24550.9531  LR: 0.000020  \n","Epoch: [1][4200/36908] Elapsed 44m 52s (remain 349m 19s) Loss: 0.0001(0.0086) Grad: 281.1193  LR: 0.000020  \n","Epoch: [1][4300/36908] Elapsed 45m 55s (remain 348m 10s) Loss: 0.0013(0.0084) Grad: 3078.2615  LR: 0.000020  \n","Epoch: [1][4400/36908] Elapsed 46m 59s (remain 347m 4s) Loss: 0.0024(0.0083) Grad: 12036.5791  LR: 0.000020  \n","Epoch: [1][4500/36908] Elapsed 48m 3s (remain 346m 0s) Loss: 0.0037(0.0082) Grad: 14195.8701  LR: 0.000020  \n","Epoch: [1][4600/36908] Elapsed 49m 7s (remain 344m 53s) Loss: 0.0017(0.0080) Grad: 13488.2949  LR: 0.000019  \n","Epoch: [1][4700/36908] Elapsed 50m 10s (remain 343m 46s) Loss: 0.0003(0.0079) Grad: 2798.3779  LR: 0.000019  \n","Epoch: [1][4800/36908] Elapsed 51m 14s (remain 342m 42s) Loss: 0.0004(0.0078) Grad: 3686.8062  LR: 0.000019  \n","Epoch: [1][4900/36908] Elapsed 52m 19s (remain 341m 40s) Loss: 0.0009(0.0077) Grad: 11734.1934  LR: 0.000019  \n","Epoch: [1][5000/36908] Elapsed 53m 24s (remain 340m 45s) Loss: 0.0007(0.0076) Grad: 46701.5625  LR: 0.000019  \n","Epoch: [1][5100/36908] Elapsed 54m 29s (remain 339m 45s) Loss: 0.0012(0.0075) Grad: 8172.0693  LR: 0.000019  \n","Epoch: [1][5200/36908] Elapsed 55m 34s (remain 338m 48s) Loss: 0.0001(0.0074) Grad: 829.2990  LR: 0.000019  \n","Epoch: [1][5300/36908] Elapsed 56m 38s (remain 337m 45s) Loss: 0.0047(0.0073) Grad: 106129.4922  LR: 0.000019  \n","Epoch: [1][5400/36908] Elapsed 57m 44s (remain 336m 50s) Loss: 0.0001(0.0072) Grad: 98.1786  LR: 0.000019  \n","Epoch: [1][5500/36908] Elapsed 58m 48s (remain 335m 46s) Loss: 0.0009(0.0071) Grad: 5497.4771  LR: 0.000019  \n","Epoch: [1][5600/36908] Elapsed 59m 52s (remain 334m 39s) Loss: 0.0013(0.0070) Grad: 7664.5288  LR: 0.000019  \n","Epoch: [1][5700/36908] Elapsed 60m 56s (remain 333m 33s) Loss: 0.0001(0.0069) Grad: 119.4993  LR: 0.000019  \n","Epoch: [1][5800/36908] Elapsed 62m 0s (remain 332m 28s) Loss: 0.0022(0.0068) Grad: 22520.8398  LR: 0.000019  \n","Epoch: [1][5900/36908] Elapsed 63m 4s (remain 331m 25s) Loss: 0.0001(0.0067) Grad: 173.1660  LR: 0.000019  \n","Epoch: [1][6000/36908] Elapsed 64m 8s (remain 330m 20s) Loss: 0.0020(0.0066) Grad: 114025.5625  LR: 0.000019  \n","Epoch: [1][6100/36908] Elapsed 65m 13s (remain 329m 21s) Loss: 0.0001(0.0066) Grad: 1261.6907  LR: 0.000019  \n","Epoch: [1][6200/36908] Elapsed 66m 17s (remain 328m 18s) Loss: 0.0019(0.0065) Grad: 10398.1250  LR: 0.000018  \n","Epoch: [1][6300/36908] Elapsed 67m 22s (remain 327m 15s) Loss: 0.0001(0.0064) Grad: 415.0481  LR: 0.000018  \n","Epoch: [1][6400/36908] Elapsed 68m 26s (remain 326m 10s) Loss: 0.0005(0.0063) Grad: 9135.2871  LR: 0.000018  \n","Epoch: [1][6500/36908] Elapsed 69m 31s (remain 325m 10s) Loss: 0.0014(0.0063) Grad: 16942.0078  LR: 0.000018  \n","Epoch: [1][6600/36908] Elapsed 70m 35s (remain 324m 8s) Loss: 0.0001(0.0062) Grad: 343.2646  LR: 0.000018  \n","Epoch: [1][6700/36908] Elapsed 71m 40s (remain 323m 6s) Loss: 0.0009(0.0061) Grad: 10670.8389  LR: 0.000018  \n","Epoch: [1][6800/36908] Elapsed 72m 44s (remain 322m 1s) Loss: 0.0010(0.0061) Grad: 16237.4922  LR: 0.000018  \n","Epoch: [1][6900/36908] Elapsed 73m 48s (remain 320m 57s) Loss: 0.0098(0.0060) Grad: 19472.0059  LR: 0.000018  \n","Epoch: [1][7000/36908] Elapsed 74m 53s (remain 319m 54s) Loss: 0.0040(0.0060) Grad: 15768.3887  LR: 0.000018  \n","Epoch: [1][7100/36908] Elapsed 75m 56s (remain 318m 47s) Loss: 0.0001(0.0059) Grad: 132.2141  LR: 0.000018  \n","Epoch: [1][7200/36908] Elapsed 76m 59s (remain 317m 38s) Loss: 0.0001(0.0059) Grad: 153.0696  LR: 0.000018  \n","Epoch: [1][7300/36908] Elapsed 78m 2s (remain 316m 28s) Loss: 0.0000(0.0058) Grad: 233.1576  LR: 0.000018  \n","Epoch: [1][7400/36908] Elapsed 79m 5s (remain 315m 19s) Loss: 0.0115(0.0057) Grad: 8930.2881  LR: 0.000018  \n","Epoch: [1][7500/36908] Elapsed 80m 9s (remain 314m 13s) Loss: 0.0001(0.0057) Grad: 1951.5352  LR: 0.000018  \n","Epoch: [1][7600/36908] Elapsed 81m 13s (remain 313m 9s) Loss: 0.0003(0.0056) Grad: 10141.4053  LR: 0.000018  \n","Epoch: [1][7700/36908] Elapsed 82m 16s (remain 312m 4s) Loss: 0.0024(0.0056) Grad: 13968.4209  LR: 0.000018  \n","Epoch: [1][7800/36908] Elapsed 83m 20s (remain 310m 59s) Loss: 0.0071(0.0055) Grad: 46315.2383  LR: 0.000018  \n","Epoch: [1][7900/36908] Elapsed 84m 25s (remain 309m 57s) Loss: 0.0020(0.0055) Grad: 16690.6855  LR: 0.000017  \n","Epoch: [1][8000/36908] Elapsed 85m 29s (remain 308m 52s) Loss: 0.0067(0.0054) Grad: 32653.9727  LR: 0.000017  \n","Epoch: [1][8100/36908] Elapsed 86m 33s (remain 307m 47s) Loss: 0.0000(0.0054) Grad: 829.9882  LR: 0.000017  \n","Epoch: [1][8200/36908] Elapsed 87m 36s (remain 306m 40s) Loss: 0.0004(0.0053) Grad: 8568.9277  LR: 0.000017  \n","Epoch: [1][8300/36908] Elapsed 88m 40s (remain 305m 34s) Loss: 0.0004(0.0053) Grad: 10223.8242  LR: 0.000017  \n","Epoch: [1][8400/36908] Elapsed 89m 45s (remain 304m 32s) Loss: 0.0004(0.0052) Grad: 29556.5020  LR: 0.000017  \n","Epoch: [1][8500/36908] Elapsed 90m 49s (remain 303m 31s) Loss: 0.0000(0.0052) Grad: 700.4584  LR: 0.000017  \n","Epoch: [1][8600/36908] Elapsed 91m 54s (remain 302m 27s) Loss: 0.0000(0.0052) Grad: 227.9018  LR: 0.000017  \n","Epoch: [1][8700/36908] Elapsed 92m 58s (remain 301m 24s) Loss: 0.0000(0.0051) Grad: 1251.5104  LR: 0.000017  \n","Epoch: [1][8800/36908] Elapsed 94m 3s (remain 300m 22s) Loss: 0.0006(0.0051) Grad: 13741.9883  LR: 0.000017  \n","Epoch: [1][8900/36908] Elapsed 95m 8s (remain 299m 21s) Loss: 0.0005(0.0050) Grad: 12000.5596  LR: 0.000017  \n","Epoch: [1][9000/36908] Elapsed 96m 13s (remain 298m 19s) Loss: 0.0004(0.0050) Grad: 17832.5586  LR: 0.000017  \n","Epoch: [1][9100/36908] Elapsed 97m 17s (remain 297m 16s) Loss: 0.0070(0.0050) Grad: 11489.8369  LR: 0.000017  \n","Epoch: [1][9200/36908] Elapsed 98m 22s (remain 296m 13s) Loss: 0.0033(0.0049) Grad: 61317.8477  LR: 0.000017  \n","Epoch: [1][9300/36908] Elapsed 99m 27s (remain 295m 12s) Loss: 0.0002(0.0049) Grad: 1667.8956  LR: 0.000017  \n","Epoch: [1][9400/36908] Elapsed 100m 31s (remain 294m 8s) Loss: 0.0032(0.0049) Grad: 26242.1582  LR: 0.000017  \n","Epoch: [1][9500/36908] Elapsed 101m 36s (remain 293m 4s) Loss: 0.0022(0.0048) Grad: 18863.6426  LR: 0.000017  \n","Epoch: [1][9600/36908] Elapsed 102m 40s (remain 292m 0s) Loss: 0.0024(0.0048) Grad: 61444.3945  LR: 0.000016  \n","Epoch: [1][9700/36908] Elapsed 103m 44s (remain 290m 56s) Loss: 0.0000(0.0048) Grad: 24.8040  LR: 0.000016  \n","Epoch: [1][9800/36908] Elapsed 104m 48s (remain 289m 52s) Loss: 0.0100(0.0047) Grad: 58925.4766  LR: 0.000016  \n","Epoch: [1][9900/36908] Elapsed 105m 52s (remain 288m 47s) Loss: 0.0001(0.0047) Grad: 1066.8436  LR: 0.000016  \n","Epoch: [1][10000/36908] Elapsed 106m 56s (remain 287m 41s) Loss: 0.0094(0.0047) Grad: 43153.5586  LR: 0.000016  \n","Epoch: [1][10100/36908] Elapsed 108m 0s (remain 286m 37s) Loss: 0.0000(0.0046) Grad: 86.0769  LR: 0.000016  \n","Epoch: [1][10200/36908] Elapsed 109m 4s (remain 285m 34s) Loss: 0.0029(0.0046) Grad: 12977.6807  LR: 0.000016  \n","Epoch: [1][10300/36908] Elapsed 110m 9s (remain 284m 31s) Loss: 0.0000(0.0046) Grad: 296.4940  LR: 0.000016  \n","Epoch: [1][10400/36908] Elapsed 111m 13s (remain 283m 27s) Loss: 0.0010(0.0046) Grad: 12823.2285  LR: 0.000016  \n","Epoch: [1][10500/36908] Elapsed 112m 17s (remain 282m 24s) Loss: 0.0001(0.0045) Grad: 284.8568  LR: 0.000016  \n","Epoch: [1][10600/36908] Elapsed 113m 22s (remain 281m 21s) Loss: 0.0002(0.0045) Grad: 1615.4938  LR: 0.000016  \n","Epoch: [1][10700/36908] Elapsed 114m 27s (remain 280m 19s) Loss: 0.0000(0.0045) Grad: 143.7192  LR: 0.000016  \n","Epoch: [1][10800/36908] Elapsed 115m 32s (remain 279m 16s) Loss: 0.0062(0.0044) Grad: 116362.2812  LR: 0.000016  \n","Epoch: [1][10900/36908] Elapsed 116m 36s (remain 278m 11s) Loss: 0.0001(0.0044) Grad: 1331.5820  LR: 0.000016  \n","Epoch: [1][11000/36908] Elapsed 117m 41s (remain 277m 8s) Loss: 0.0101(0.0044) Grad: 49547.3789  LR: 0.000016  \n","Epoch: [1][11100/36908] Elapsed 118m 45s (remain 276m 4s) Loss: 0.0002(0.0044) Grad: 9187.8682  LR: 0.000016  \n","Epoch: [1][11200/36908] Elapsed 119m 49s (remain 275m 0s) Loss: 0.0017(0.0043) Grad: 36443.8711  LR: 0.000015  \n","Epoch: [1][11300/36908] Elapsed 120m 54s (remain 273m 57s) Loss: 0.0018(0.0043) Grad: 27639.8613  LR: 0.000015  \n","Epoch: [1][11400/36908] Elapsed 121m 58s (remain 272m 52s) Loss: 0.0002(0.0043) Grad: 1902.2700  LR: 0.000015  \n","Epoch: [1][11500/36908] Elapsed 123m 2s (remain 271m 48s) Loss: 0.0006(0.0043) Grad: 8222.9072  LR: 0.000015  \n","Epoch: [1][11600/36908] Elapsed 124m 7s (remain 270m 45s) Loss: 0.0000(0.0043) Grad: 114.9946  LR: 0.000015  \n","Epoch: [1][11700/36908] Elapsed 125m 11s (remain 269m 40s) Loss: 0.0064(0.0042) Grad: 54993.8125  LR: 0.000015  \n","Epoch: [1][11800/36908] Elapsed 126m 14s (remain 268m 34s) Loss: 0.0000(0.0042) Grad: 604.2393  LR: 0.000015  \n","Epoch: [1][11900/36908] Elapsed 127m 18s (remain 267m 31s) Loss: 0.0004(0.0042) Grad: 15411.8086  LR: 0.000015  \n","Epoch: [1][12000/36908] Elapsed 128m 23s (remain 266m 27s) Loss: 0.0000(0.0042) Grad: 127.5686  LR: 0.000015  \n","Epoch: [1][12100/36908] Elapsed 129m 27s (remain 265m 22s) Loss: 0.0008(0.0041) Grad: 4505.8755  LR: 0.000015  \n","Epoch: [1][12200/36908] Elapsed 130m 31s (remain 264m 17s) Loss: 0.0002(0.0041) Grad: 2351.8889  LR: 0.000015  \n","Epoch: [1][12300/36908] Elapsed 131m 35s (remain 263m 13s) Loss: 0.0001(0.0041) Grad: 368.1574  LR: 0.000015  \n","Epoch: [1][12400/36908] Elapsed 132m 39s (remain 262m 10s) Loss: 0.0001(0.0041) Grad: 4568.6411  LR: 0.000015  \n","Epoch: [1][12500/36908] Elapsed 133m 44s (remain 261m 7s) Loss: 0.0094(0.0041) Grad: 59546.2656  LR: 0.000015  \n","Epoch: [1][12600/36908] Elapsed 134m 49s (remain 260m 3s) Loss: 0.0061(0.0040) Grad: 27639.6055  LR: 0.000015  \n","Epoch: [1][12700/36908] Elapsed 135m 53s (remain 258m 59s) Loss: 0.0002(0.0040) Grad: 880.6141  LR: 0.000015  \n","Epoch: [1][12800/36908] Elapsed 136m 58s (remain 257m 56s) Loss: 0.0001(0.0040) Grad: 799.6550  LR: 0.000015  \n","Epoch: [1][12900/36908] Elapsed 138m 2s (remain 256m 53s) Loss: 0.0003(0.0040) Grad: 4219.3081  LR: 0.000014  \n","Epoch: [1][13000/36908] Elapsed 139m 8s (remain 255m 50s) Loss: 0.0045(0.0040) Grad: 55122.4883  LR: 0.000014  \n","Epoch: [1][13100/36908] Elapsed 140m 12s (remain 254m 47s) Loss: 0.0000(0.0039) Grad: 41.3645  LR: 0.000014  \n","Epoch: [1][13200/36908] Elapsed 141m 17s (remain 253m 44s) Loss: 0.0046(0.0039) Grad: 33472.0820  LR: 0.000014  \n","Epoch: [1][13300/36908] Elapsed 142m 21s (remain 252m 40s) Loss: 0.0006(0.0039) Grad: 3430.5977  LR: 0.000014  \n","Epoch: [1][13400/36908] Elapsed 143m 26s (remain 251m 36s) Loss: 0.0202(0.0039) Grad: 180714.9531  LR: 0.000014  \n","Epoch: [1][13500/36908] Elapsed 144m 30s (remain 250m 31s) Loss: 0.0001(0.0039) Grad: 5372.4258  LR: 0.000014  \n","Epoch: [1][13600/36908] Elapsed 145m 34s (remain 249m 27s) Loss: 0.0123(0.0038) Grad: 254690.9688  LR: 0.000014  \n","Epoch: [1][13700/36908] Elapsed 146m 38s (remain 248m 23s) Loss: 0.0005(0.0038) Grad: 10735.4619  LR: 0.000014  \n","Epoch: [1][13800/36908] Elapsed 147m 43s (remain 247m 19s) Loss: 0.0010(0.0038) Grad: 51464.3203  LR: 0.000014  \n","Epoch: [1][13900/36908] Elapsed 148m 47s (remain 246m 15s) Loss: 0.0046(0.0038) Grad: 44748.6016  LR: 0.000014  \n","Epoch: [1][14000/36908] Elapsed 149m 51s (remain 245m 11s) Loss: 0.0007(0.0038) Grad: 11907.1230  LR: 0.000014  \n","Epoch: [1][14100/36908] Elapsed 150m 55s (remain 244m 5s) Loss: 0.0000(0.0038) Grad: 1322.9508  LR: 0.000014  \n","Epoch: [1][14200/36908] Elapsed 151m 59s (remain 243m 1s) Loss: 0.0000(0.0037) Grad: 189.8582  LR: 0.000014  \n","Epoch: [1][14300/36908] Elapsed 153m 4s (remain 241m 58s) Loss: 0.0026(0.0037) Grad: 152558.3438  LR: 0.000014  \n","Epoch: [1][14400/36908] Elapsed 154m 8s (remain 240m 54s) Loss: 0.0004(0.0037) Grad: 33925.3281  LR: 0.000014  \n","Epoch: [1][14500/36908] Elapsed 155m 12s (remain 239m 49s) Loss: 0.0000(0.0037) Grad: 129.1971  LR: 0.000013  \n","Epoch: [1][14600/36908] Elapsed 156m 16s (remain 238m 45s) Loss: 0.0002(0.0037) Grad: 10293.0518  LR: 0.000013  \n","Epoch: [1][14700/36908] Elapsed 157m 20s (remain 237m 40s) Loss: 0.0000(0.0037) Grad: 64.2710  LR: 0.000013  \n","Epoch: [1][14800/36908] Elapsed 158m 24s (remain 236m 36s) Loss: 0.0000(0.0036) Grad: 87.3694  LR: 0.000013  \n","Epoch: [1][14900/36908] Elapsed 159m 29s (remain 235m 32s) Loss: 0.0002(0.0036) Grad: 7989.0303  LR: 0.000013  \n","Epoch: [1][15000/36908] Elapsed 160m 33s (remain 234m 28s) Loss: 0.0000(0.0036) Grad: 156.3130  LR: 0.000013  \n","Epoch: [1][15100/36908] Elapsed 161m 38s (remain 233m 24s) Loss: 0.0003(0.0036) Grad: 7463.1855  LR: 0.000013  \n","Epoch: [1][15200/36908] Elapsed 162m 43s (remain 232m 21s) Loss: 0.0000(0.0036) Grad: 127.4247  LR: 0.000013  \n","Epoch: [1][15300/36908] Elapsed 163m 48s (remain 231m 19s) Loss: 0.0001(0.0036) Grad: 4127.4238  LR: 0.000013  \n","Epoch: [1][15400/36908] Elapsed 164m 53s (remain 230m 15s) Loss: 0.0000(0.0036) Grad: 76.9827  LR: 0.000013  \n","Epoch: [1][15500/36908] Elapsed 165m 58s (remain 229m 12s) Loss: 0.0026(0.0035) Grad: 18317.8398  LR: 0.000013  \n","Epoch: [1][15600/36908] Elapsed 167m 2s (remain 228m 8s) Loss: 0.0000(0.0035) Grad: 37.9985  LR: 0.000013  \n","Epoch: [1][15700/36908] Elapsed 168m 7s (remain 227m 5s) Loss: 0.0000(0.0035) Grad: 231.8146  LR: 0.000013  \n","Epoch: [1][15800/36908] Elapsed 169m 12s (remain 226m 1s) Loss: 0.0000(0.0035) Grad: 64.0971  LR: 0.000013  \n","Epoch: [1][15900/36908] Elapsed 170m 16s (remain 224m 57s) Loss: 0.0001(0.0035) Grad: 1679.2273  LR: 0.000013  \n","Epoch: [1][16000/36908] Elapsed 171m 21s (remain 223m 53s) Loss: 0.0000(0.0035) Grad: 108.6064  LR: 0.000013  \n","Epoch: [1][16100/36908] Elapsed 172m 26s (remain 222m 50s) Loss: 0.0001(0.0035) Grad: 505.1801  LR: 0.000013  \n","Epoch: [1][16200/36908] Elapsed 173m 31s (remain 221m 47s) Loss: 0.0002(0.0034) Grad: 14320.8350  LR: 0.000012  \n","Epoch: [1][16300/36908] Elapsed 174m 36s (remain 220m 43s) Loss: 0.0000(0.0034) Grad: 4.0865  LR: 0.000012  \n","Epoch: [1][16400/36908] Elapsed 175m 40s (remain 219m 39s) Loss: 0.0005(0.0034) Grad: 6475.3008  LR: 0.000012  \n","Epoch: [1][16500/36908] Elapsed 176m 45s (remain 218m 36s) Loss: 0.0117(0.0034) Grad: 44180.2422  LR: 0.000012  \n","Epoch: [1][16600/36908] Elapsed 177m 50s (remain 217m 32s) Loss: 0.0001(0.0034) Grad: 1511.0809  LR: 0.000012  \n","Epoch: [1][16700/36908] Elapsed 178m 54s (remain 216m 27s) Loss: 0.0004(0.0034) Grad: 6373.0898  LR: 0.000012  \n","Epoch: [1][16800/36908] Elapsed 179m 58s (remain 215m 23s) Loss: 0.0001(0.0034) Grad: 2792.2825  LR: 0.000012  \n","Epoch: [1][16900/36908] Elapsed 181m 3s (remain 214m 19s) Loss: 0.0000(0.0034) Grad: 45.5228  LR: 0.000012  \n","Epoch: [1][17000/36908] Elapsed 182m 8s (remain 213m 17s) Loss: 0.0066(0.0033) Grad: 36566.4570  LR: 0.000012  \n","Epoch: [1][17100/36908] Elapsed 183m 14s (remain 212m 13s) Loss: 0.0000(0.0033) Grad: 44.6716  LR: 0.000012  \n","Epoch: [1][17200/36908] Elapsed 184m 18s (remain 211m 9s) Loss: 0.0000(0.0033) Grad: 145.6184  LR: 0.000012  \n","Epoch: [1][17300/36908] Elapsed 185m 23s (remain 210m 6s) Loss: 0.0000(0.0033) Grad: 456.3520  LR: 0.000012  \n","Epoch: [1][17400/36908] Elapsed 186m 29s (remain 209m 3s) Loss: 0.0009(0.0033) Grad: 5925.2617  LR: 0.000012  \n","Epoch: [1][17500/36908] Elapsed 187m 34s (remain 208m 0s) Loss: 0.0000(0.0033) Grad: 170.7303  LR: 0.000012  \n","Epoch: [1][17600/36908] Elapsed 188m 39s (remain 206m 56s) Loss: 0.0000(0.0033) Grad: 352.5629  LR: 0.000012  \n","Epoch: [1][17700/36908] Elapsed 189m 44s (remain 205m 53s) Loss: 0.0000(0.0033) Grad: 78.1347  LR: 0.000012  \n","Epoch: [1][17800/36908] Elapsed 190m 50s (remain 204m 50s) Loss: 0.0042(0.0032) Grad: 14668.3633  LR: 0.000012  \n","Epoch: [1][17900/36908] Elapsed 191m 55s (remain 203m 46s) Loss: 0.0005(0.0032) Grad: 5202.2241  LR: 0.000011  \n","Epoch: [1][18000/36908] Elapsed 193m 0s (remain 202m 43s) Loss: 0.0000(0.0032) Grad: 60.6056  LR: 0.000011  \n","Epoch: [1][18100/36908] Elapsed 194m 3s (remain 201m 38s) Loss: 0.0000(0.0032) Grad: 70.1817  LR: 0.000011  \n","Epoch: [1][18200/36908] Elapsed 195m 8s (remain 200m 33s) Loss: 0.0004(0.0032) Grad: 10382.7783  LR: 0.000011  \n","Epoch: [1][18300/36908] Elapsed 196m 12s (remain 199m 29s) Loss: 0.0002(0.0032) Grad: 4315.6094  LR: 0.000011  \n","Epoch: [1][18400/36908] Elapsed 197m 16s (remain 198m 24s) Loss: 0.0003(0.0032) Grad: 819.3524  LR: 0.000011  \n","Epoch: [1][18500/36908] Elapsed 198m 20s (remain 197m 19s) Loss: 0.0001(0.0032) Grad: 1118.3488  LR: 0.000011  \n","Epoch: [1][18600/36908] Elapsed 199m 24s (remain 196m 15s) Loss: 0.0027(0.0032) Grad: 10970.7598  LR: 0.000011  \n","Epoch: [1][18700/36908] Elapsed 200m 28s (remain 195m 10s) Loss: 0.0000(0.0031) Grad: 54.9535  LR: 0.000011  \n","Epoch: [1][18800/36908] Elapsed 201m 32s (remain 194m 6s) Loss: 0.0004(0.0031) Grad: 8108.4990  LR: 0.000011  \n","Epoch: [1][18900/36908] Elapsed 202m 36s (remain 193m 1s) Loss: 0.0009(0.0031) Grad: 5685.9541  LR: 0.000011  \n","Epoch: [1][19000/36908] Elapsed 203m 40s (remain 191m 56s) Loss: 0.0000(0.0031) Grad: 795.5547  LR: 0.000011  \n","Epoch: [1][19100/36908] Elapsed 204m 44s (remain 190m 52s) Loss: 0.0100(0.0031) Grad: 46301.6836  LR: 0.000011  \n","Epoch: [1][19200/36908] Elapsed 205m 49s (remain 189m 48s) Loss: 0.0000(0.0031) Grad: 138.2426  LR: 0.000011  \n","Epoch: [1][19300/36908] Elapsed 206m 53s (remain 188m 43s) Loss: 0.0040(0.0031) Grad: 21449.0195  LR: 0.000011  \n","Epoch: [1][19400/36908] Elapsed 207m 57s (remain 187m 39s) Loss: 0.0070(0.0031) Grad: 108196.8203  LR: 0.000011  \n","Epoch: [1][19500/36908] Elapsed 209m 0s (remain 186m 34s) Loss: 0.0044(0.0031) Grad: 54443.8398  LR: 0.000010  \n","Epoch: [1][19600/36908] Elapsed 210m 4s (remain 185m 29s) Loss: 0.0005(0.0031) Grad: 13575.6113  LR: 0.000010  \n","Epoch: [1][19700/36908] Elapsed 211m 8s (remain 184m 25s) Loss: 0.0133(0.0031) Grad: 43792.7148  LR: 0.000010  \n","Epoch: [1][19800/36908] Elapsed 212m 13s (remain 183m 20s) Loss: 0.0038(0.0030) Grad: 82872.8516  LR: 0.000010  \n","Epoch: [1][19900/36908] Elapsed 213m 16s (remain 182m 15s) Loss: 0.0009(0.0030) Grad: 22685.6445  LR: 0.000010  \n","Epoch: [1][20000/36908] Elapsed 214m 20s (remain 181m 11s) Loss: 0.0000(0.0030) Grad: 298.0244  LR: 0.000010  \n","Epoch: [1][20100/36908] Elapsed 215m 24s (remain 180m 6s) Loss: 0.0000(0.0030) Grad: 70.4666  LR: 0.000010  \n","Epoch: [1][20200/36908] Elapsed 216m 29s (remain 179m 2s) Loss: 0.0018(0.0030) Grad: 23519.9238  LR: 0.000010  \n","Epoch: [1][20300/36908] Elapsed 217m 34s (remain 177m 59s) Loss: 0.0000(0.0030) Grad: 82.4958  LR: 0.000010  \n","Epoch: [1][20400/36908] Elapsed 218m 39s (remain 176m 55s) Loss: 0.0000(0.0030) Grad: 113.8884  LR: 0.000010  \n","Epoch: [1][20500/36908] Elapsed 219m 43s (remain 175m 50s) Loss: 0.0000(0.0030) Grad: 51.6551  LR: 0.000010  \n","Epoch: [1][20600/36908] Elapsed 220m 47s (remain 174m 46s) Loss: 0.0000(0.0030) Grad: 473.3693  LR: 0.000010  \n","Epoch: [1][20700/36908] Elapsed 221m 52s (remain 173m 42s) Loss: 0.0000(0.0030) Grad: 34.4649  LR: 0.000010  \n","Epoch: [1][20800/36908] Elapsed 222m 56s (remain 172m 38s) Loss: 0.0001(0.0030) Grad: 2102.9971  LR: 0.000010  \n","Epoch: [1][20900/36908] Elapsed 224m 1s (remain 171m 34s) Loss: 0.0000(0.0029) Grad: 307.6941  LR: 0.000010  \n","Epoch: [1][21000/36908] Elapsed 225m 6s (remain 170m 30s) Loss: 0.0021(0.0029) Grad: 23484.5312  LR: 0.000010  \n","Epoch: [1][21100/36908] Elapsed 226m 11s (remain 169m 26s) Loss: 0.0000(0.0029) Grad: 35.8447  LR: 0.000010  \n","Epoch: [1][21200/36908] Elapsed 227m 16s (remain 168m 22s) Loss: 0.0012(0.0029) Grad: 25521.5430  LR: 0.000009  \n","Epoch: [1][21300/36908] Elapsed 228m 20s (remain 167m 18s) Loss: 0.0009(0.0029) Grad: 4889.0820  LR: 0.000009  \n","Epoch: [1][21400/36908] Elapsed 229m 26s (remain 166m 14s) Loss: 0.0007(0.0029) Grad: 33895.4609  LR: 0.000009  \n","Epoch: [1][21500/36908] Elapsed 230m 31s (remain 165m 11s) Loss: 0.0010(0.0029) Grad: 22359.0430  LR: 0.000009  \n","Epoch: [1][21600/36908] Elapsed 231m 36s (remain 164m 7s) Loss: 0.0006(0.0029) Grad: 57421.7695  LR: 0.000009  \n","Epoch: [1][21700/36908] Elapsed 232m 41s (remain 163m 3s) Loss: 0.0000(0.0029) Grad: 136.7822  LR: 0.000009  \n","Epoch: [1][21800/36908] Elapsed 233m 46s (remain 161m 59s) Loss: 0.0012(0.0029) Grad: 27447.6777  LR: 0.000009  \n","Epoch: [1][21900/36908] Elapsed 234m 51s (remain 160m 55s) Loss: 0.0001(0.0029) Grad: 3520.7915  LR: 0.000009  \n","Epoch: [1][22000/36908] Elapsed 235m 56s (remain 159m 52s) Loss: 0.0001(0.0029) Grad: 1535.4603  LR: 0.000009  \n","Epoch: [1][22100/36908] Elapsed 237m 1s (remain 158m 48s) Loss: 0.0034(0.0028) Grad: 9584.5371  LR: 0.000009  \n","Epoch: [1][22200/36908] Elapsed 238m 6s (remain 157m 44s) Loss: 0.0050(0.0028) Grad: 346673.1562  LR: 0.000009  \n","Epoch: [1][22300/36908] Elapsed 239m 11s (remain 156m 40s) Loss: 0.0000(0.0028) Grad: 1192.9541  LR: 0.000009  \n","Epoch: [1][22400/36908] Elapsed 240m 16s (remain 155m 36s) Loss: 0.0000(0.0028) Grad: 797.9199  LR: 0.000009  \n","Epoch: [1][22500/36908] Elapsed 241m 22s (remain 154m 32s) Loss: 0.0026(0.0028) Grad: 53515.7734  LR: 0.000009  \n","Epoch: [1][22600/36908] Elapsed 242m 26s (remain 153m 28s) Loss: 0.0000(0.0028) Grad: 950.1286  LR: 0.000009  \n","Epoch: [1][22700/36908] Elapsed 243m 31s (remain 152m 24s) Loss: 0.0022(0.0028) Grad: 36970.8750  LR: 0.000009  \n","Epoch: [1][22800/36908] Elapsed 244m 35s (remain 151m 19s) Loss: 0.0001(0.0028) Grad: 909.0536  LR: 0.000008  \n","Epoch: [1][22900/36908] Elapsed 245m 40s (remain 150m 15s) Loss: 0.0002(0.0028) Grad: 4228.3179  LR: 0.000008  \n","Epoch: [1][23000/36908] Elapsed 246m 45s (remain 149m 11s) Loss: 0.0015(0.0028) Grad: 74383.2422  LR: 0.000008  \n","Epoch: [1][23100/36908] Elapsed 247m 50s (remain 148m 7s) Loss: 0.0000(0.0028) Grad: 48.8400  LR: 0.000008  \n","Epoch: [1][23200/36908] Elapsed 248m 56s (remain 147m 4s) Loss: 0.0000(0.0028) Grad: 31.0979  LR: 0.000008  \n","Epoch: [1][23300/36908] Elapsed 250m 1s (remain 146m 0s) Loss: 0.0004(0.0028) Grad: 15909.8750  LR: 0.000008  \n","Epoch: [1][23400/36908] Elapsed 251m 6s (remain 144m 56s) Loss: 0.0181(0.0028) Grad: 47969.8828  LR: 0.000008  \n","Epoch: [1][23500/36908] Elapsed 252m 10s (remain 143m 52s) Loss: 0.0002(0.0027) Grad: 7717.1592  LR: 0.000008  \n","Epoch: [1][23600/36908] Elapsed 253m 15s (remain 142m 47s) Loss: 0.0000(0.0027) Grad: 98.2350  LR: 0.000008  \n","Epoch: [1][23700/36908] Elapsed 254m 20s (remain 141m 43s) Loss: 0.0012(0.0027) Grad: 16116.7539  LR: 0.000008  \n","Epoch: [1][23800/36908] Elapsed 255m 25s (remain 140m 39s) Loss: 0.0000(0.0027) Grad: 368.1837  LR: 0.000008  \n","Epoch: [1][23900/36908] Elapsed 256m 30s (remain 139m 35s) Loss: 0.0000(0.0027) Grad: 97.6593  LR: 0.000008  \n","Epoch: [1][24000/36908] Elapsed 257m 34s (remain 138m 31s) Loss: 0.0000(0.0027) Grad: 49.6797  LR: 0.000008  \n","Epoch: [1][24100/36908] Elapsed 258m 39s (remain 137m 27s) Loss: 0.0000(0.0027) Grad: 120.1751  LR: 0.000008  \n","Epoch: [1][24200/36908] Elapsed 259m 44s (remain 136m 23s) Loss: 0.0056(0.0027) Grad: 525662.6875  LR: 0.000008  \n","Epoch: [1][24300/36908] Elapsed 260m 50s (remain 135m 18s) Loss: 0.0004(0.0027) Grad: 141212.2031  LR: 0.000008  \n","Epoch: [1][24400/36908] Elapsed 261m 54s (remain 134m 14s) Loss: 0.0013(0.0027) Grad: 151576.7344  LR: 0.000008  \n","Epoch: [1][24500/36908] Elapsed 262m 59s (remain 133m 10s) Loss: 0.0000(0.0027) Grad: 2526.8938  LR: 0.000007  \n","Epoch: [1][24600/36908] Elapsed 264m 3s (remain 132m 6s) Loss: 0.0032(0.0027) Grad: 72703.0234  LR: 0.000007  \n","Epoch: [1][24700/36908] Elapsed 265m 8s (remain 131m 1s) Loss: 0.0047(0.0027) Grad: 168389.1406  LR: 0.000007  \n","Epoch: [1][24800/36908] Elapsed 266m 12s (remain 129m 57s) Loss: 0.0001(0.0027) Grad: 8298.6963  LR: 0.000007  \n","Epoch: [1][24900/36908] Elapsed 267m 15s (remain 128m 52s) Loss: 0.0000(0.0027) Grad: 22.4203  LR: 0.000007  \n","Epoch: [1][25000/36908] Elapsed 268m 20s (remain 127m 47s) Loss: 0.0000(0.0027) Grad: 302.0360  LR: 0.000007  \n","Epoch: [1][25100/36908] Elapsed 269m 25s (remain 126m 43s) Loss: 0.0000(0.0026) Grad: 73.1044  LR: 0.000007  \n","Epoch: [1][25200/36908] Elapsed 270m 29s (remain 125m 39s) Loss: 0.0056(0.0026) Grad: 121851.4531  LR: 0.000007  \n","Epoch: [1][25300/36908] Elapsed 271m 32s (remain 124m 34s) Loss: 0.0000(0.0026) Grad: 180.6026  LR: 0.000007  \n","Epoch: [1][25400/36908] Elapsed 272m 36s (remain 123m 29s) Loss: 0.0013(0.0026) Grad: 116164.7500  LR: 0.000007  \n","Epoch: [1][25500/36908] Elapsed 273m 41s (remain 122m 25s) Loss: 0.0000(0.0026) Grad: 44.9633  LR: 0.000007  \n","Epoch: [1][25600/36908] Elapsed 274m 46s (remain 121m 21s) Loss: 0.0002(0.0026) Grad: 10261.7822  LR: 0.000007  \n","Epoch: [1][25700/36908] Elapsed 275m 52s (remain 120m 17s) Loss: 0.0021(0.0026) Grad: 165724.5781  LR: 0.000007  \n","Epoch: [1][25800/36908] Elapsed 276m 55s (remain 119m 12s) Loss: 0.0097(0.0026) Grad: 237029.2500  LR: 0.000007  \n","Epoch: [1][25900/36908] Elapsed 277m 59s (remain 118m 8s) Loss: 0.0009(0.0026) Grad: 80782.2109  LR: 0.000007  \n","Epoch: [1][26000/36908] Elapsed 279m 4s (remain 117m 4s) Loss: 0.0000(0.0026) Grad: 84.6681  LR: 0.000007  \n","Epoch: [1][26100/36908] Elapsed 280m 10s (remain 116m 0s) Loss: 0.0004(0.0026) Grad: 14773.1426  LR: 0.000007  \n","Epoch: [1][26200/36908] Elapsed 281m 16s (remain 114m 56s) Loss: 0.0016(0.0026) Grad: 42574.4180  LR: 0.000006  \n","Epoch: [1][26300/36908] Elapsed 282m 20s (remain 113m 51s) Loss: 0.0038(0.0026) Grad: 104278.5078  LR: 0.000006  \n","Epoch: [1][26400/36908] Elapsed 283m 24s (remain 112m 47s) Loss: 0.0084(0.0026) Grad: 91280.5312  LR: 0.000006  \n","Epoch: [1][26500/36908] Elapsed 284m 28s (remain 111m 42s) Loss: 0.0000(0.0026) Grad: 16.0151  LR: 0.000006  \n","Epoch: [1][26600/36908] Elapsed 285m 31s (remain 110m 37s) Loss: 0.0000(0.0026) Grad: 21.5873  LR: 0.000006  \n","Epoch: [1][26700/36908] Elapsed 286m 35s (remain 109m 33s) Loss: 0.0000(0.0026) Grad: 29.1446  LR: 0.000006  \n","Epoch: [1][26800/36908] Elapsed 287m 39s (remain 108m 28s) Loss: 0.0038(0.0026) Grad: 112963.6094  LR: 0.000006  \n","Epoch: [1][26900/36908] Elapsed 288m 43s (remain 107m 24s) Loss: 0.0021(0.0026) Grad: 68476.8203  LR: 0.000006  \n","Epoch: [1][27000/36908] Elapsed 289m 47s (remain 106m 19s) Loss: 0.0000(0.0025) Grad: 485.6591  LR: 0.000006  \n","Epoch: [1][27100/36908] Elapsed 290m 50s (remain 105m 14s) Loss: 0.0064(0.0025) Grad: 529283.8750  LR: 0.000006  \n","Epoch: [1][27200/36908] Elapsed 291m 56s (remain 104m 10s) Loss: 0.0001(0.0025) Grad: 17998.3594  LR: 0.000006  \n","Epoch: [1][27300/36908] Elapsed 293m 1s (remain 103m 6s) Loss: 0.0001(0.0025) Grad: 13213.6973  LR: 0.000006  \n","Epoch: [1][27400/36908] Elapsed 294m 6s (remain 102m 2s) Loss: 0.0006(0.0025) Grad: 52814.4766  LR: 0.000006  \n","Epoch: [1][27500/36908] Elapsed 295m 11s (remain 100m 58s) Loss: 0.0011(0.0025) Grad: 170449.1875  LR: 0.000006  \n","Epoch: [1][27600/36908] Elapsed 296m 16s (remain 99m 54s) Loss: 0.0000(0.0025) Grad: 76.7297  LR: 0.000006  \n","Epoch: [1][27700/36908] Elapsed 297m 21s (remain 98m 49s) Loss: 0.0000(0.0025) Grad: 73.0263  LR: 0.000006  \n","Epoch: [1][27800/36908] Elapsed 298m 26s (remain 97m 45s) Loss: 0.0000(0.0025) Grad: 36.2490  LR: 0.000005  \n","Epoch: [1][27900/36908] Elapsed 299m 30s (remain 96m 41s) Loss: 0.0006(0.0025) Grad: 41618.7031  LR: 0.000005  \n","Epoch: [1][28000/36908] Elapsed 300m 35s (remain 95m 37s) Loss: 0.0025(0.0025) Grad: 61611.4883  LR: 0.000005  \n","Epoch: [1][28100/36908] Elapsed 301m 41s (remain 94m 32s) Loss: 0.0007(0.0025) Grad: 153071.9531  LR: 0.000005  \n","Epoch: [1][28200/36908] Elapsed 302m 46s (remain 93m 28s) Loss: 0.0000(0.0025) Grad: 49.7483  LR: 0.000005  \n","Epoch: [1][28300/36908] Elapsed 303m 51s (remain 92m 24s) Loss: 0.0000(0.0025) Grad: 166.9288  LR: 0.000005  \n","Epoch: [1][28400/36908] Elapsed 304m 55s (remain 91m 20s) Loss: 0.0000(0.0025) Grad: 46.3423  LR: 0.000005  \n","Epoch: [1][28500/36908] Elapsed 305m 59s (remain 90m 15s) Loss: 0.0001(0.0025) Grad: 3787.2324  LR: 0.000005  \n","Epoch: [1][28600/36908] Elapsed 307m 4s (remain 89m 11s) Loss: 0.0001(0.0025) Grad: 3230.1848  LR: 0.000005  \n","Epoch: [1][28700/36908] Elapsed 308m 9s (remain 88m 6s) Loss: 0.0000(0.0025) Grad: 72.2245  LR: 0.000005  \n","Epoch: [1][28800/36908] Elapsed 309m 13s (remain 87m 2s) Loss: 0.0000(0.0025) Grad: 41.3576  LR: 0.000005  \n","Epoch: [1][28900/36908] Elapsed 310m 18s (remain 85m 58s) Loss: 0.0002(0.0025) Grad: 2745.3035  LR: 0.000005  \n","Epoch: [1][29000/36908] Elapsed 311m 22s (remain 84m 53s) Loss: 0.0004(0.0025) Grad: 19009.7988  LR: 0.000005  \n","Epoch: [1][29100/36908] Elapsed 312m 26s (remain 83m 49s) Loss: 0.0002(0.0025) Grad: 1339.7921  LR: 0.000005  \n","Epoch: [1][29200/36908] Elapsed 313m 30s (remain 82m 44s) Loss: 0.0000(0.0025) Grad: 68.1676  LR: 0.000005  \n","Epoch: [1][29300/36908] Elapsed 314m 33s (remain 81m 39s) Loss: 0.0000(0.0024) Grad: 221.5266  LR: 0.000005  \n","Epoch: [1][29400/36908] Elapsed 315m 38s (remain 80m 35s) Loss: 0.0074(0.0024) Grad: 112484.6406  LR: 0.000005  \n","Epoch: [1][29500/36908] Elapsed 316m 42s (remain 79m 31s) Loss: 0.0004(0.0024) Grad: 13744.3320  LR: 0.000004  \n","Epoch: [1][29600/36908] Elapsed 317m 47s (remain 78m 26s) Loss: 0.0000(0.0024) Grad: 1327.6459  LR: 0.000004  \n","Epoch: [1][29700/36908] Elapsed 318m 50s (remain 77m 22s) Loss: 0.0005(0.0024) Grad: 12463.2314  LR: 0.000004  \n","Epoch: [1][29800/36908] Elapsed 319m 54s (remain 76m 17s) Loss: 0.0002(0.0024) Grad: 5089.8462  LR: 0.000004  \n","Epoch: [1][29900/36908] Elapsed 320m 58s (remain 75m 13s) Loss: 0.0001(0.0024) Grad: 3893.8918  LR: 0.000004  \n","Epoch: [1][30000/36908] Elapsed 322m 2s (remain 74m 8s) Loss: 0.0000(0.0024) Grad: 43.0421  LR: 0.000004  \n","Epoch: [1][30100/36908] Elapsed 323m 6s (remain 73m 4s) Loss: 0.0002(0.0024) Grad: 10573.7266  LR: 0.000004  \n","Epoch: [1][30200/36908] Elapsed 324m 10s (remain 71m 59s) Loss: 0.0032(0.0024) Grad: 103521.4766  LR: 0.000004  \n","Epoch: [1][30300/36908] Elapsed 325m 13s (remain 70m 54s) Loss: 0.0030(0.0024) Grad: 134992.1562  LR: 0.000004  \n","Epoch: [1][30400/36908] Elapsed 326m 17s (remain 69m 50s) Loss: 0.0000(0.0024) Grad: 21.0722  LR: 0.000004  \n","Epoch: [1][30500/36908] Elapsed 327m 21s (remain 68m 45s) Loss: 0.0023(0.0024) Grad: 81442.2344  LR: 0.000004  \n","Epoch: [1][30600/36908] Elapsed 328m 24s (remain 67m 41s) Loss: 0.0029(0.0024) Grad: 26541.3164  LR: 0.000004  \n","Epoch: [1][30700/36908] Elapsed 329m 28s (remain 66m 36s) Loss: 0.0000(0.0024) Grad: 235.4898  LR: 0.000004  \n","Epoch: [1][30800/36908] Elapsed 330m 32s (remain 65m 32s) Loss: 0.0001(0.0024) Grad: 4928.6606  LR: 0.000004  \n","Epoch: [1][30900/36908] Elapsed 331m 37s (remain 64m 27s) Loss: 0.0011(0.0024) Grad: 62437.3906  LR: 0.000004  \n","Epoch: [1][31000/36908] Elapsed 332m 41s (remain 63m 23s) Loss: 0.0005(0.0024) Grad: 22984.5059  LR: 0.000004  \n","Epoch: [1][31100/36908] Elapsed 333m 45s (remain 62m 18s) Loss: 0.0007(0.0024) Grad: 83136.7188  LR: 0.000003  \n","Epoch: [1][31200/36908] Elapsed 334m 48s (remain 61m 14s) Loss: 0.0019(0.0024) Grad: 46557.9062  LR: 0.000003  \n","Epoch: [1][31300/36908] Elapsed 335m 52s (remain 60m 9s) Loss: 0.0001(0.0024) Grad: 6369.9185  LR: 0.000003  \n","Epoch: [1][31400/36908] Elapsed 336m 57s (remain 59m 5s) Loss: 0.0042(0.0024) Grad: 315483.4375  LR: 0.000003  \n","Epoch: [1][31500/36908] Elapsed 338m 1s (remain 58m 1s) Loss: 0.0002(0.0024) Grad: 47983.2812  LR: 0.000003  \n","Epoch: [1][31600/36908] Elapsed 339m 5s (remain 56m 56s) Loss: 0.0000(0.0024) Grad: 5.8084  LR: 0.000003  \n","Epoch: [1][31700/36908] Elapsed 340m 9s (remain 55m 52s) Loss: 0.0000(0.0024) Grad: 63.4361  LR: 0.000003  \n","Epoch: [1][31800/36908] Elapsed 341m 14s (remain 54m 48s) Loss: 0.0055(0.0023) Grad: 143995.0156  LR: 0.000003  \n","Epoch: [1][31900/36908] Elapsed 342m 18s (remain 53m 43s) Loss: 0.0000(0.0023) Grad: 242.3231  LR: 0.000003  \n","Epoch: [1][32000/36908] Elapsed 343m 22s (remain 52m 39s) Loss: 0.0000(0.0023) Grad: 298.5291  LR: 0.000003  \n","Epoch: [1][32100/36908] Elapsed 344m 27s (remain 51m 34s) Loss: 0.0002(0.0023) Grad: 38999.0938  LR: 0.000003  \n","Epoch: [1][32200/36908] Elapsed 345m 32s (remain 50m 30s) Loss: 0.0192(0.0023) Grad: 846819.6250  LR: 0.000003  \n","Epoch: [1][32300/36908] Elapsed 346m 36s (remain 49m 26s) Loss: 0.0000(0.0023) Grad: 434.7951  LR: 0.000003  \n","Epoch: [1][32400/36908] Elapsed 347m 39s (remain 48m 21s) Loss: 0.0021(0.0023) Grad: 138326.7656  LR: 0.000003  \n","Epoch: [1][32500/36908] Elapsed 348m 42s (remain 47m 17s) Loss: 0.0000(0.0023) Grad: 2292.3706  LR: 0.000003  \n","Epoch: [1][32600/36908] Elapsed 349m 46s (remain 46m 12s) Loss: 0.0000(0.0023) Grad: 94.1768  LR: 0.000003  \n","Epoch: [1][32700/36908] Elapsed 350m 49s (remain 45m 8s) Loss: 0.0037(0.0023) Grad: 170072.5312  LR: 0.000003  \n","Epoch: [1][32800/36908] Elapsed 351m 53s (remain 44m 3s) Loss: 0.0000(0.0023) Grad: 2005.9906  LR: 0.000002  \n","Epoch: [1][32900/36908] Elapsed 352m 56s (remain 42m 59s) Loss: 0.0000(0.0023) Grad: 47.7969  LR: 0.000002  \n","Epoch: [1][33000/36908] Elapsed 353m 59s (remain 41m 54s) Loss: 0.0015(0.0023) Grad: 66934.6172  LR: 0.000002  \n","Epoch: [1][33100/36908] Elapsed 355m 3s (remain 40m 50s) Loss: 0.0000(0.0023) Grad: 59.2561  LR: 0.000002  \n","Epoch: [1][33200/36908] Elapsed 356m 7s (remain 39m 45s) Loss: 0.0001(0.0023) Grad: 3108.9810  LR: 0.000002  \n","Epoch: [1][33300/36908] Elapsed 357m 10s (remain 38m 41s) Loss: 0.0001(0.0023) Grad: 2515.3391  LR: 0.000002  \n","Epoch: [1][33400/36908] Elapsed 358m 13s (remain 37m 36s) Loss: 0.0000(0.0023) Grad: 42.8473  LR: 0.000002  \n","Epoch: [1][33500/36908] Elapsed 359m 17s (remain 36m 32s) Loss: 0.0022(0.0023) Grad: 144497.7500  LR: 0.000002  \n","Epoch: [1][33600/36908] Elapsed 360m 21s (remain 35m 27s) Loss: 0.0004(0.0023) Grad: 1777.8998  LR: 0.000002  \n","Epoch: [1][33700/36908] Elapsed 361m 24s (remain 34m 23s) Loss: 0.0000(0.0023) Grad: 72.9713  LR: 0.000002  \n","Epoch: [1][33800/36908] Elapsed 362m 28s (remain 33m 19s) Loss: 0.0001(0.0023) Grad: 6208.8618  LR: 0.000002  \n","Epoch: [1][33900/36908] Elapsed 363m 31s (remain 32m 14s) Loss: 0.0003(0.0023) Grad: 7732.6392  LR: 0.000002  \n","Epoch: [1][34000/36908] Elapsed 364m 36s (remain 31m 10s) Loss: 0.0003(0.0023) Grad: 2259.2900  LR: 0.000002  \n","Epoch: [1][34100/36908] Elapsed 365m 39s (remain 30m 5s) Loss: 0.0079(0.0023) Grad: 86434.0234  LR: 0.000002  \n","Epoch: [1][34200/36908] Elapsed 366m 42s (remain 29m 1s) Loss: 0.0004(0.0023) Grad: 12273.9561  LR: 0.000002  \n","Epoch: [1][34300/36908] Elapsed 367m 44s (remain 27m 57s) Loss: 0.0014(0.0023) Grad: 54375.0469  LR: 0.000002  \n","Epoch: [1][34400/36908] Elapsed 368m 48s (remain 26m 52s) Loss: 0.0044(0.0023) Grad: 40458.4727  LR: 0.000002  \n","Epoch: [1][34500/36908] Elapsed 369m 51s (remain 25m 48s) Loss: 0.0004(0.0023) Grad: 23084.4688  LR: 0.000001  \n","Epoch: [1][34600/36908] Elapsed 370m 55s (remain 24m 43s) Loss: 0.0007(0.0023) Grad: 32096.9805  LR: 0.000001  \n","Epoch: [1][34700/36908] Elapsed 371m 58s (remain 23m 39s) Loss: 0.0000(0.0022) Grad: 319.8414  LR: 0.000001  \n","Epoch: [1][34800/36908] Elapsed 373m 2s (remain 22m 35s) Loss: 0.0001(0.0022) Grad: 4622.5752  LR: 0.000001  \n","Epoch: [1][34900/36908] Elapsed 374m 6s (remain 21m 30s) Loss: 0.0000(0.0022) Grad: 53.9288  LR: 0.000001  \n","Epoch: [1][35000/36908] Elapsed 375m 10s (remain 20m 26s) Loss: 0.0000(0.0022) Grad: 325.6693  LR: 0.000001  \n","Epoch: [1][35100/36908] Elapsed 376m 14s (remain 19m 22s) Loss: 0.0000(0.0022) Grad: 28.3663  LR: 0.000001  \n","Epoch: [1][35200/36908] Elapsed 377m 17s (remain 18m 17s) Loss: 0.0006(0.0022) Grad: 43041.6211  LR: 0.000001  \n","Epoch: [1][35300/36908] Elapsed 378m 21s (remain 17m 13s) Loss: 0.0053(0.0022) Grad: 52269.1875  LR: 0.000001  \n","Epoch: [1][35400/36908] Elapsed 379m 26s (remain 16m 9s) Loss: 0.0032(0.0022) Grad: 10854.8613  LR: 0.000001  \n","Epoch: [1][35500/36908] Elapsed 380m 31s (remain 15m 4s) Loss: 0.0026(0.0022) Grad: 29053.6445  LR: 0.000001  \n","Epoch: [1][35600/36908] Elapsed 381m 35s (remain 14m 0s) Loss: 0.0000(0.0022) Grad: 151.5011  LR: 0.000001  \n","Epoch: [1][35700/36908] Elapsed 382m 39s (remain 12m 56s) Loss: 0.0002(0.0022) Grad: 6111.5005  LR: 0.000001  \n","Epoch: [1][35800/36908] Elapsed 383m 44s (remain 11m 51s) Loss: 0.0000(0.0022) Grad: 57.0750  LR: 0.000001  \n","Epoch: [1][35900/36908] Elapsed 384m 49s (remain 10m 47s) Loss: 0.0000(0.0022) Grad: 168.4231  LR: 0.000001  \n","Epoch: [1][36000/36908] Elapsed 385m 53s (remain 9m 43s) Loss: 0.0000(0.0022) Grad: 8.8063  LR: 0.000001  \n","Epoch: [1][36100/36908] Elapsed 386m 58s (remain 8m 39s) Loss: 0.0002(0.0022) Grad: 3312.5161  LR: 0.000000  \n","Epoch: [1][36200/36908] Elapsed 388m 2s (remain 7m 34s) Loss: 0.0000(0.0022) Grad: 347.5728  LR: 0.000000  \n","Epoch: [1][36300/36908] Elapsed 389m 7s (remain 6m 30s) Loss: 0.0050(0.0022) Grad: 85129.7344  LR: 0.000000  \n","Epoch: [1][36400/36908] Elapsed 390m 12s (remain 5m 26s) Loss: 0.0001(0.0022) Grad: 2095.0259  LR: 0.000000  \n","Epoch: [1][36500/36908] Elapsed 391m 16s (remain 4m 21s) Loss: 0.0012(0.0022) Grad: 14735.9697  LR: 0.000000  \n","Epoch: [1][36600/36908] Elapsed 392m 20s (remain 3m 17s) Loss: 0.0003(0.0022) Grad: 7395.3579  LR: 0.000000  \n","Epoch: [1][36700/36908] Elapsed 393m 24s (remain 2m 13s) Loss: 0.0000(0.0022) Grad: 53.0938  LR: 0.000000  \n","Epoch: [1][36800/36908] Elapsed 394m 30s (remain 1m 8s) Loss: 0.0000(0.0022) Grad: 16.2404  LR: 0.000000  \n","Epoch: [1][36900/36908] Elapsed 395m 34s (remain 0m 4s) Loss: 0.0000(0.0022) Grad: 96.0831  LR: 0.000000  \n","Epoch: [1][36907/36908] Elapsed 395m 39s (remain 0m 0s) Loss: 0.0000(0.0022) Grad: 197.5037  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 12m 51s) Loss: 0.0001(0.0001) \n","EVAL: [100/1192] Elapsed 0m 24s (remain 4m 28s) Loss: 0.0053(0.0025) \n","EVAL: [200/1192] Elapsed 0m 48s (remain 3m 58s) Loss: 0.0036(0.0029) \n","EVAL: [300/1192] Elapsed 1m 11s (remain 3m 32s) Loss: 0.0021(0.0033) \n","EVAL: [400/1192] Elapsed 1m 35s (remain 3m 7s) Loss: 0.0039(0.0035) \n","EVAL: [500/1192] Elapsed 1m 58s (remain 2m 43s) Loss: 0.0027(0.0032) \n","EVAL: [600/1192] Elapsed 2m 22s (remain 2m 20s) Loss: 0.0000(0.0034) \n","EVAL: [700/1192] Elapsed 2m 46s (remain 1m 56s) Loss: 0.0364(0.0040) \n","EVAL: [800/1192] Elapsed 3m 9s (remain 1m 32s) Loss: 0.0013(0.0041) \n","EVAL: [900/1192] Elapsed 3m 33s (remain 1m 8s) Loss: 0.0002(0.0041) \n","EVAL: [1000/1192] Elapsed 3m 57s (remain 0m 45s) Loss: 0.0000(0.0040) \n","EVAL: [1100/1192] Elapsed 4m 20s (remain 0m 21s) Loss: 0.0005(0.0039) \n","EVAL: [1191/1192] Elapsed 4m 42s (remain 0m 0s) Loss: 0.0000(0.0037) \n","Epoch 1 - avg_train_loss: 0.0022  avg_val_loss: 0.0037  time: 24024s\n","Epoch 1 - Score: 0.8824\n","Epoch 1 - Save Best Score: 0.8824 Model\n","========== fold: 1 training ==========\n","get pseudo plain from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n","get pseudo labels from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_1.npy\n","get pseudo labels from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_1.npy\n","get pseudo labels from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp083/pseudo_labels_1.npy\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"84419842ca8743908bf405a638f6cf45","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/612602 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a0517900da2a4ef69b59b1b0078b6164","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/612602 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["(612602, 950)\n","(612602, 6) (612602, 950)\n","(100000, 7)\n","(110725, 11)\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\n","Epoch: [1][0/36908] Elapsed 0m 1s (remain 856m 12s) Loss: 0.0702(0.0702) Grad: 68483.4453  LR: 0.000000  \n","Epoch: [1][100/36908] Elapsed 1m 6s (remain 402m 40s) Loss: 0.0520(0.0634) Grad: 53005.6445  LR: 0.000001  \n","Epoch: [1][200/36908] Elapsed 2m 11s (remain 398m 44s) Loss: 0.0279(0.0518) Grad: 24213.1719  LR: 0.000001  \n","Epoch: [1][300/36908] Elapsed 3m 15s (remain 395m 53s) Loss: 0.0249(0.0407) Grad: 8569.5146  LR: 0.000002  \n","Epoch: [1][400/36908] Elapsed 4m 19s (remain 394m 13s) Loss: 0.0114(0.0339) Grad: 3000.4729  LR: 0.000002  \n","Epoch: [1][500/36908] Elapsed 5m 24s (remain 393m 6s) Loss: 0.0199(0.0298) Grad: 7129.7993  LR: 0.000003  \n","Epoch: [1][600/36908] Elapsed 6m 29s (remain 391m 46s) Loss: 0.0123(0.0267) Grad: 3749.8811  LR: 0.000003  \n","Epoch: [1][700/36908] Elapsed 7m 33s (remain 390m 10s) Loss: 0.0222(0.0247) Grad: 10543.9502  LR: 0.000004  \n","Epoch: [1][800/36908] Elapsed 8m 37s (remain 389m 5s) Loss: 0.0126(0.0232) Grad: 4410.2925  LR: 0.000004  \n","Epoch: [1][900/36908] Elapsed 9m 42s (remain 387m 48s) Loss: 0.0056(0.0219) Grad: 3253.5510  LR: 0.000005  \n","Epoch: [1][1000/36908] Elapsed 10m 46s (remain 386m 29s) Loss: 0.0112(0.0209) Grad: 7711.7212  LR: 0.000005  \n","Epoch: [1][1100/36908] Elapsed 11m 50s (remain 385m 8s) Loss: 0.0066(0.0200) Grad: 4435.1084  LR: 0.000006  \n","Epoch: [1][1200/36908] Elapsed 12m 54s (remain 383m 38s) Loss: 0.0033(0.0191) Grad: 2889.3132  LR: 0.000007  \n","Epoch: [1][1300/36908] Elapsed 13m 58s (remain 382m 17s) Loss: 0.0040(0.0182) Grad: 11348.2949  LR: 0.000007  \n","Epoch: [1][1400/36908] Elapsed 15m 2s (remain 381m 0s) Loss: 0.0023(0.0173) Grad: 9702.0762  LR: 0.000008  \n","Epoch: [1][1500/36908] Elapsed 16m 5s (remain 379m 40s) Loss: 0.0079(0.0164) Grad: 13281.5830  LR: 0.000008  \n","Epoch: [1][1600/36908] Elapsed 17m 9s (remain 378m 13s) Loss: 0.0009(0.0157) Grad: 2507.5938  LR: 0.000009  \n","Epoch: [1][1700/36908] Elapsed 18m 12s (remain 376m 48s) Loss: 0.0032(0.0150) Grad: 18559.2344  LR: 0.000009  \n","Epoch: [1][1800/36908] Elapsed 19m 16s (remain 375m 38s) Loss: 0.0018(0.0144) Grad: 4560.3984  LR: 0.000010  \n","Epoch: [1][1900/36908] Elapsed 20m 20s (remain 374m 26s) Loss: 0.0051(0.0138) Grad: 21870.6270  LR: 0.000010  \n","Epoch: [1][2000/36908] Elapsed 21m 23s (remain 373m 7s) Loss: 0.0025(0.0133) Grad: 9964.8418  LR: 0.000011  \n","Epoch: [1][2100/36908] Elapsed 22m 26s (remain 371m 50s) Loss: 0.0006(0.0128) Grad: 2437.4036  LR: 0.000011  \n","Epoch: [1][2200/36908] Elapsed 23m 30s (remain 370m 36s) Loss: 0.0002(0.0124) Grad: 352.8282  LR: 0.000012  \n","Epoch: [1][2300/36908] Elapsed 24m 34s (remain 369m 32s) Loss: 0.0005(0.0120) Grad: 2625.3391  LR: 0.000012  \n","Epoch: [1][2400/36908] Elapsed 25m 38s (remain 368m 37s) Loss: 0.0028(0.0116) Grad: 19738.2383  LR: 0.000013  \n","Epoch: [1][2500/36908] Elapsed 26m 42s (remain 367m 22s) Loss: 0.0041(0.0113) Grad: 23369.4414  LR: 0.000014  \n","Epoch: [1][2600/36908] Elapsed 27m 45s (remain 366m 9s) Loss: 0.0005(0.0109) Grad: 656.4813  LR: 0.000014  \n","Epoch: [1][2700/36908] Elapsed 28m 49s (remain 365m 2s) Loss: 0.0020(0.0106) Grad: 10605.8662  LR: 0.000015  \n","Epoch: [1][2800/36908] Elapsed 29m 53s (remain 363m 56s) Loss: 0.0012(0.0104) Grad: 8162.5835  LR: 0.000015  \n","Epoch: [1][2900/36908] Elapsed 30m 56s (remain 362m 48s) Loss: 0.0078(0.0101) Grad: 28847.1992  LR: 0.000016  \n","Epoch: [1][3000/36908] Elapsed 32m 0s (remain 361m 35s) Loss: 0.0084(0.0098) Grad: 14203.2656  LR: 0.000016  \n","Epoch: [1][3100/36908] Elapsed 33m 3s (remain 360m 26s) Loss: 0.0002(0.0096) Grad: 253.7577  LR: 0.000017  \n","Epoch: [1][3200/36908] Elapsed 34m 8s (remain 359m 28s) Loss: 0.0026(0.0094) Grad: 3802.4956  LR: 0.000017  \n","Epoch: [1][3300/36908] Elapsed 35m 13s (remain 358m 32s) Loss: 0.0015(0.0092) Grad: 11612.7334  LR: 0.000018  \n","Epoch: [1][3400/36908] Elapsed 36m 17s (remain 357m 30s) Loss: 0.0016(0.0090) Grad: 8175.7524  LR: 0.000018  \n","Epoch: [1][3500/36908] Elapsed 37m 21s (remain 356m 25s) Loss: 0.0016(0.0088) Grad: 6065.6440  LR: 0.000019  \n","Epoch: [1][3600/36908] Elapsed 38m 25s (remain 355m 25s) Loss: 0.0001(0.0086) Grad: 150.8896  LR: 0.000020  \n","Epoch: [1][3700/36908] Elapsed 39m 29s (remain 354m 23s) Loss: 0.0002(0.0084) Grad: 945.0003  LR: 0.000020  \n","Epoch: [1][3800/36908] Elapsed 40m 34s (remain 353m 23s) Loss: 0.0001(0.0083) Grad: 221.2445  LR: 0.000020  \n","Epoch: [1][3900/36908] Elapsed 41m 38s (remain 352m 17s) Loss: 0.0036(0.0081) Grad: 10568.5576  LR: 0.000020  \n","Epoch: [1][4000/36908] Elapsed 42m 41s (remain 351m 11s) Loss: 0.0155(0.0080) Grad: 45982.1367  LR: 0.000020  \n","Epoch: [1][4100/36908] Elapsed 43m 46s (remain 350m 7s) Loss: 0.0081(0.0078) Grad: 60529.7539  LR: 0.000020  \n","Epoch: [1][4200/36908] Elapsed 44m 50s (remain 349m 6s) Loss: 0.0106(0.0077) Grad: 14101.0449  LR: 0.000020  \n","Epoch: [1][4300/36908] Elapsed 45m 54s (remain 348m 2s) Loss: 0.0001(0.0076) Grad: 352.7769  LR: 0.000020  \n","Epoch: [1][4400/36908] Elapsed 46m 57s (remain 346m 53s) Loss: 0.0019(0.0075) Grad: 20686.3242  LR: 0.000020  \n","Epoch: [1][4500/36908] Elapsed 48m 1s (remain 345m 49s) Loss: 0.0002(0.0074) Grad: 554.0828  LR: 0.000020  \n","Epoch: [1][4600/36908] Elapsed 49m 5s (remain 344m 43s) Loss: 0.0001(0.0073) Grad: 128.4361  LR: 0.000019  \n","Epoch: [1][4700/36908] Elapsed 50m 9s (remain 343m 37s) Loss: 0.0001(0.0072) Grad: 261.0856  LR: 0.000019  \n","Epoch: [1][4800/36908] Elapsed 51m 12s (remain 342m 29s) Loss: 0.0053(0.0071) Grad: 71534.3125  LR: 0.000019  \n","Epoch: [1][4900/36908] Elapsed 52m 17s (remain 341m 27s) Loss: 0.0000(0.0070) Grad: 75.6893  LR: 0.000019  \n","Epoch: [1][5000/36908] Elapsed 53m 22s (remain 340m 32s) Loss: 0.0014(0.0069) Grad: 21384.7422  LR: 0.000019  \n","Epoch: [1][5100/36908] Elapsed 54m 26s (remain 339m 28s) Loss: 0.0000(0.0068) Grad: 141.7407  LR: 0.000019  \n","Epoch: [1][5200/36908] Elapsed 55m 30s (remain 338m 21s) Loss: 0.0000(0.0067) Grad: 129.0844  LR: 0.000019  \n","Epoch: [1][5300/36908] Elapsed 56m 34s (remain 337m 18s) Loss: 0.0002(0.0066) Grad: 2588.3286  LR: 0.000019  \n","Epoch: [1][5400/36908] Elapsed 57m 38s (remain 336m 15s) Loss: 0.0004(0.0065) Grad: 5857.7886  LR: 0.000019  \n","Epoch: [1][5500/36908] Elapsed 58m 43s (remain 335m 15s) Loss: 0.0005(0.0065) Grad: 8199.2480  LR: 0.000019  \n","Epoch: [1][5600/36908] Elapsed 59m 47s (remain 334m 12s) Loss: 0.0006(0.0064) Grad: 8086.1538  LR: 0.000019  \n","Epoch: [1][5700/36908] Elapsed 60m 51s (remain 333m 8s) Loss: 0.0007(0.0063) Grad: 6307.8613  LR: 0.000019  \n","Epoch: [1][5800/36908] Elapsed 61m 55s (remain 332m 4s) Loss: 0.0039(0.0062) Grad: 17501.6797  LR: 0.000019  \n","Epoch: [1][5900/36908] Elapsed 63m 0s (remain 331m 2s) Loss: 0.0001(0.0061) Grad: 121.9662  LR: 0.000019  \n","Epoch: [1][6000/36908] Elapsed 64m 4s (remain 329m 59s) Loss: 0.0032(0.0061) Grad: 42356.7461  LR: 0.000019  \n","Epoch: [1][6100/36908] Elapsed 65m 8s (remain 328m 56s) Loss: 0.0003(0.0060) Grad: 2056.6145  LR: 0.000019  \n","Epoch: [1][6200/36908] Elapsed 66m 12s (remain 327m 52s) Loss: 0.0000(0.0059) Grad: 352.7082  LR: 0.000018  \n","Epoch: [1][6300/36908] Elapsed 67m 16s (remain 326m 49s) Loss: 0.0009(0.0059) Grad: 13287.7529  LR: 0.000018  \n","Epoch: [1][6400/36908] Elapsed 68m 21s (remain 325m 47s) Loss: 0.0000(0.0058) Grad: 93.7360  LR: 0.000018  \n","Epoch: [1][6500/36908] Elapsed 69m 26s (remain 324m 46s) Loss: 0.0024(0.0058) Grad: 18957.6250  LR: 0.000018  \n","Epoch: [1][6600/36908] Elapsed 70m 30s (remain 323m 42s) Loss: 0.0006(0.0057) Grad: 5654.9727  LR: 0.000018  \n","Epoch: [1][6700/36908] Elapsed 71m 35s (remain 322m 43s) Loss: 0.0044(0.0056) Grad: 51481.4297  LR: 0.000018  \n","Epoch: [1][6800/36908] Elapsed 72m 40s (remain 321m 43s) Loss: 0.0006(0.0056) Grad: 13013.6104  LR: 0.000018  \n","Epoch: [1][6900/36908] Elapsed 73m 44s (remain 320m 39s) Loss: 0.0004(0.0055) Grad: 6772.5146  LR: 0.000018  \n","Epoch: [1][7000/36908] Elapsed 74m 48s (remain 319m 35s) Loss: 0.0014(0.0055) Grad: 51680.8516  LR: 0.000018  \n","Epoch: [1][7100/36908] Elapsed 75m 53s (remain 318m 31s) Loss: 0.0054(0.0054) Grad: 32532.0879  LR: 0.000018  \n","Epoch: [1][7200/36908] Elapsed 76m 57s (remain 317m 29s) Loss: 0.0045(0.0054) Grad: 24632.6660  LR: 0.000018  \n","Epoch: [1][7300/36908] Elapsed 78m 1s (remain 316m 26s) Loss: 0.0079(0.0053) Grad: 15077.4053  LR: 0.000018  \n","Epoch: [1][7400/36908] Elapsed 79m 5s (remain 315m 21s) Loss: 0.0041(0.0053) Grad: 27504.3223  LR: 0.000018  \n","Epoch: [1][7500/36908] Elapsed 80m 9s (remain 314m 16s) Loss: 0.0017(0.0052) Grad: 25394.4199  LR: 0.000018  \n","Epoch: [1][7600/36908] Elapsed 81m 14s (remain 313m 13s) Loss: 0.0000(0.0052) Grad: 88.1070  LR: 0.000018  \n","Epoch: [1][7700/36908] Elapsed 82m 18s (remain 312m 9s) Loss: 0.0017(0.0051) Grad: 17391.1230  LR: 0.000018  \n","Epoch: [1][7800/36908] Elapsed 83m 22s (remain 311m 5s) Loss: 0.0028(0.0051) Grad: 8840.1826  LR: 0.000018  \n","Epoch: [1][7900/36908] Elapsed 84m 26s (remain 310m 0s) Loss: 0.0032(0.0051) Grad: 5873.8882  LR: 0.000017  \n","Epoch: [1][8000/36908] Elapsed 85m 30s (remain 308m 56s) Loss: 0.0001(0.0050) Grad: 2013.6995  LR: 0.000017  \n","Epoch: [1][8100/36908] Elapsed 86m 35s (remain 307m 53s) Loss: 0.0082(0.0050) Grad: 77898.7188  LR: 0.000017  \n","Epoch: [1][8200/36908] Elapsed 87m 39s (remain 306m 50s) Loss: 0.0021(0.0049) Grad: 27985.3965  LR: 0.000017  \n","Epoch: [1][8300/36908] Elapsed 88m 43s (remain 305m 45s) Loss: 0.0008(0.0049) Grad: 20736.0469  LR: 0.000017  \n","Epoch: [1][8400/36908] Elapsed 89m 46s (remain 304m 39s) Loss: 0.0000(0.0048) Grad: 168.9328  LR: 0.000017  \n","Epoch: [1][8500/36908] Elapsed 90m 51s (remain 303m 35s) Loss: 0.0014(0.0048) Grad: 25986.1914  LR: 0.000017  \n","Epoch: [1][8600/36908] Elapsed 91m 55s (remain 302m 33s) Loss: 0.0000(0.0048) Grad: 172.5584  LR: 0.000017  \n","Epoch: [1][8700/36908] Elapsed 92m 59s (remain 301m 29s) Loss: 0.0000(0.0047) Grad: 54.1715  LR: 0.000017  \n","Epoch: [1][8800/36908] Elapsed 94m 3s (remain 300m 22s) Loss: 0.0001(0.0047) Grad: 1585.5597  LR: 0.000017  \n","Epoch: [1][8900/36908] Elapsed 95m 7s (remain 299m 17s) Loss: 0.0009(0.0047) Grad: 11964.8457  LR: 0.000017  \n","Epoch: [1][9000/36908] Elapsed 96m 11s (remain 298m 13s) Loss: 0.0171(0.0046) Grad: 124094.1484  LR: 0.000017  \n","Epoch: [1][9100/36908] Elapsed 97m 15s (remain 297m 8s) Loss: 0.0018(0.0046) Grad: 63856.0312  LR: 0.000017  \n","Epoch: [1][9200/36908] Elapsed 98m 18s (remain 296m 3s) Loss: 0.0002(0.0046) Grad: 9474.7246  LR: 0.000017  \n","Epoch: [1][9300/36908] Elapsed 99m 22s (remain 294m 57s) Loss: 0.0111(0.0045) Grad: 129714.1172  LR: 0.000017  \n","Epoch: [1][9400/36908] Elapsed 100m 26s (remain 293m 54s) Loss: 0.0007(0.0045) Grad: 13786.2979  LR: 0.000017  \n","Epoch: [1][9500/36908] Elapsed 101m 31s (remain 292m 53s) Loss: 0.0034(0.0045) Grad: 29549.0645  LR: 0.000017  \n","Epoch: [1][9600/36908] Elapsed 102m 36s (remain 291m 50s) Loss: 0.0001(0.0045) Grad: 2181.7625  LR: 0.000016  \n","Epoch: [1][9700/36908] Elapsed 103m 40s (remain 290m 45s) Loss: 0.0000(0.0044) Grad: 459.7867  LR: 0.000016  \n","Epoch: [1][9800/36908] Elapsed 104m 44s (remain 289m 41s) Loss: 0.0021(0.0044) Grad: 43169.7383  LR: 0.000016  \n","Epoch: [1][9900/36908] Elapsed 105m 49s (remain 288m 40s) Loss: 0.0000(0.0044) Grad: 136.3044  LR: 0.000016  \n","Epoch: [1][10000/36908] Elapsed 106m 54s (remain 287m 36s) Loss: 0.0002(0.0043) Grad: 3882.3499  LR: 0.000016  \n","Epoch: [1][10100/36908] Elapsed 107m 57s (remain 286m 31s) Loss: 0.0143(0.0043) Grad: 351093.5000  LR: 0.000016  \n","Epoch: [1][10200/36908] Elapsed 109m 1s (remain 285m 26s) Loss: 0.0003(0.0043) Grad: 9341.1523  LR: 0.000016  \n","Epoch: [1][10300/36908] Elapsed 110m 5s (remain 284m 22s) Loss: 0.0008(0.0043) Grad: 13251.5410  LR: 0.000016  \n","Epoch: [1][10400/36908] Elapsed 111m 9s (remain 283m 18s) Loss: 0.0000(0.0042) Grad: 250.2132  LR: 0.000016  \n","Epoch: [1][10500/36908] Elapsed 112m 13s (remain 282m 13s) Loss: 0.0000(0.0042) Grad: 1017.9323  LR: 0.000016  \n","Epoch: [1][10600/36908] Elapsed 113m 18s (remain 281m 10s) Loss: 0.0017(0.0042) Grad: 28085.3164  LR: 0.000016  \n","Epoch: [1][10700/36908] Elapsed 114m 23s (remain 280m 8s) Loss: 0.0000(0.0041) Grad: 137.1167  LR: 0.000016  \n","Epoch: [1][10800/36908] Elapsed 115m 27s (remain 279m 4s) Loss: 0.0001(0.0041) Grad: 5806.5703  LR: 0.000016  \n","Epoch: [1][10900/36908] Elapsed 116m 31s (remain 277m 59s) Loss: 0.0004(0.0041) Grad: 6026.4658  LR: 0.000016  \n","Epoch: [1][11000/36908] Elapsed 117m 35s (remain 276m 54s) Loss: 0.0012(0.0041) Grad: 32558.0762  LR: 0.000016  \n","Epoch: [1][11100/36908] Elapsed 118m 38s (remain 275m 48s) Loss: 0.0123(0.0041) Grad: 287383.2188  LR: 0.000016  \n","Epoch: [1][11200/36908] Elapsed 119m 42s (remain 274m 43s) Loss: 0.0000(0.0040) Grad: 11.5700  LR: 0.000015  \n","Epoch: [1][11300/36908] Elapsed 120m 46s (remain 273m 39s) Loss: 0.0076(0.0040) Grad: 85520.4453  LR: 0.000015  \n","Epoch: [1][11400/36908] Elapsed 121m 50s (remain 272m 34s) Loss: 0.0000(0.0040) Grad: 214.5773  LR: 0.000015  \n","Epoch: [1][11500/36908] Elapsed 122m 53s (remain 271m 28s) Loss: 0.0001(0.0040) Grad: 2015.7736  LR: 0.000015  \n","Epoch: [1][11600/36908] Elapsed 123m 56s (remain 270m 23s) Loss: 0.0014(0.0039) Grad: 23324.6992  LR: 0.000015  \n","Epoch: [1][11700/36908] Elapsed 125m 0s (remain 269m 19s) Loss: 0.0081(0.0039) Grad: 68879.8750  LR: 0.000015  \n","Epoch: [1][11800/36908] Elapsed 126m 4s (remain 268m 14s) Loss: 0.0000(0.0039) Grad: 29.7015  LR: 0.000015  \n","Epoch: [1][11900/36908] Elapsed 127m 8s (remain 267m 8s) Loss: 0.0002(0.0039) Grad: 6481.3984  LR: 0.000015  \n","Epoch: [1][12000/36908] Elapsed 128m 11s (remain 266m 2s) Loss: 0.0000(0.0039) Grad: 2504.6055  LR: 0.000015  \n","Epoch: [1][12100/36908] Elapsed 129m 15s (remain 264m 57s) Loss: 0.0000(0.0038) Grad: 260.3642  LR: 0.000015  \n","Epoch: [1][12200/36908] Elapsed 130m 18s (remain 263m 52s) Loss: 0.0004(0.0038) Grad: 18852.4512  LR: 0.000015  \n","Epoch: [1][12300/36908] Elapsed 131m 22s (remain 262m 48s) Loss: 0.0009(0.0038) Grad: 33828.7852  LR: 0.000015  \n","Epoch: [1][12400/36908] Elapsed 132m 26s (remain 261m 43s) Loss: 0.0000(0.0038) Grad: 296.1816  LR: 0.000015  \n","Epoch: [1][12500/36908] Elapsed 133m 30s (remain 260m 40s) Loss: 0.0000(0.0038) Grad: 18882.3164  LR: 0.000015  \n","Epoch: [1][12600/36908] Elapsed 134m 35s (remain 259m 36s) Loss: 0.0002(0.0037) Grad: 4036.4392  LR: 0.000015  \n","Epoch: [1][12700/36908] Elapsed 135m 39s (remain 258m 33s) Loss: 0.0001(0.0037) Grad: 1716.1886  LR: 0.000015  \n","Epoch: [1][12800/36908] Elapsed 136m 44s (remain 257m 29s) Loss: 0.0003(0.0037) Grad: 21098.5703  LR: 0.000015  \n","Epoch: [1][12900/36908] Elapsed 137m 48s (remain 256m 25s) Loss: 0.0000(0.0037) Grad: 5890.3950  LR: 0.000014  \n","Epoch: [1][13000/36908] Elapsed 138m 51s (remain 255m 21s) Loss: 0.0000(0.0037) Grad: 69.5233  LR: 0.000014  \n","Epoch: [1][13100/36908] Elapsed 139m 56s (remain 254m 17s) Loss: 0.0004(0.0037) Grad: 28059.1113  LR: 0.000014  \n","Epoch: [1][13200/36908] Elapsed 141m 0s (remain 253m 13s) Loss: 0.0006(0.0037) Grad: 41250.6484  LR: 0.000014  \n","Epoch: [1][13300/36908] Elapsed 142m 4s (remain 252m 9s) Loss: 0.0031(0.0036) Grad: 146809.2344  LR: 0.000014  \n","Epoch: [1][13400/36908] Elapsed 143m 8s (remain 251m 4s) Loss: 0.0000(0.0036) Grad: 2062.0520  LR: 0.000014  \n","Epoch: [1][13500/36908] Elapsed 144m 12s (remain 250m 0s) Loss: 0.0000(0.0036) Grad: 185.3889  LR: 0.000014  \n","Epoch: [1][13600/36908] Elapsed 145m 17s (remain 248m 58s) Loss: 0.0001(0.0036) Grad: 1722.2892  LR: 0.000014  \n","Epoch: [1][13700/36908] Elapsed 146m 21s (remain 247m 54s) Loss: 0.0036(0.0036) Grad: 104997.2891  LR: 0.000014  \n","Epoch: [1][13800/36908] Elapsed 147m 25s (remain 246m 50s) Loss: 0.0000(0.0036) Grad: 5.7564  LR: 0.000014  \n","Epoch: [1][13900/36908] Elapsed 148m 30s (remain 245m 46s) Loss: 0.0000(0.0036) Grad: 635.1535  LR: 0.000014  \n","Epoch: [1][14000/36908] Elapsed 149m 34s (remain 244m 43s) Loss: 0.0002(0.0035) Grad: 11151.1143  LR: 0.000014  \n","Epoch: [1][14100/36908] Elapsed 150m 39s (remain 243m 40s) Loss: 0.0034(0.0035) Grad: 109536.7656  LR: 0.000014  \n","Epoch: [1][14200/36908] Elapsed 151m 43s (remain 242m 35s) Loss: 0.0000(0.0035) Grad: 51.2949  LR: 0.000014  \n","Epoch: [1][14300/36908] Elapsed 152m 47s (remain 241m 31s) Loss: 0.0092(0.0035) Grad: 92479.9141  LR: 0.000014  \n","Epoch: [1][14400/36908] Elapsed 153m 50s (remain 240m 26s) Loss: 0.0000(0.0035) Grad: 545.0341  LR: 0.000014  \n","Epoch: [1][14500/36908] Elapsed 154m 54s (remain 239m 22s) Loss: 0.0000(0.0035) Grad: 361.0022  LR: 0.000013  \n","Epoch: [1][14600/36908] Elapsed 155m 58s (remain 238m 18s) Loss: 0.0011(0.0034) Grad: 349427.0000  LR: 0.000013  \n","Epoch: [1][14700/36908] Elapsed 157m 2s (remain 237m 13s) Loss: 0.0000(0.0034) Grad: 51.5061  LR: 0.000013  \n","Epoch: [1][14800/36908] Elapsed 158m 6s (remain 236m 8s) Loss: 0.0001(0.0034) Grad: 2205.9023  LR: 0.000013  \n","Epoch: [1][14900/36908] Elapsed 159m 9s (remain 235m 4s) Loss: 0.0000(0.0034) Grad: 39.1044  LR: 0.000013  \n","Epoch: [1][15000/36908] Elapsed 160m 13s (remain 233m 59s) Loss: 0.0004(0.0034) Grad: 16849.0918  LR: 0.000013  \n","Epoch: [1][15100/36908] Elapsed 161m 16s (remain 232m 53s) Loss: 0.0000(0.0034) Grad: 558.2231  LR: 0.000013  \n","Epoch: [1][15200/36908] Elapsed 162m 20s (remain 231m 48s) Loss: 0.0008(0.0034) Grad: 11628.6299  LR: 0.000013  \n","Epoch: [1][15300/36908] Elapsed 163m 24s (remain 230m 44s) Loss: 0.0001(0.0034) Grad: 2797.4980  LR: 0.000013  \n","Epoch: [1][15400/36908] Elapsed 164m 28s (remain 229m 40s) Loss: 0.0000(0.0033) Grad: 2103.1445  LR: 0.000013  \n","Epoch: [1][15500/36908] Elapsed 165m 31s (remain 228m 35s) Loss: 0.0001(0.0033) Grad: 2409.1606  LR: 0.000013  \n","Epoch: [1][15600/36908] Elapsed 166m 35s (remain 227m 31s) Loss: 0.0000(0.0033) Grad: 24.6406  LR: 0.000013  \n","Epoch: [1][15700/36908] Elapsed 167m 40s (remain 226m 28s) Loss: 0.0000(0.0033) Grad: 48.2929  LR: 0.000013  \n","Epoch: [1][15800/36908] Elapsed 168m 44s (remain 225m 24s) Loss: 0.0039(0.0033) Grad: 230842.2969  LR: 0.000013  \n","Epoch: [1][15900/36908] Elapsed 169m 48s (remain 224m 20s) Loss: 0.0011(0.0033) Grad: 54446.3555  LR: 0.000013  \n","Epoch: [1][16000/36908] Elapsed 170m 52s (remain 223m 15s) Loss: 0.0006(0.0033) Grad: 12388.3418  LR: 0.000013  \n","Epoch: [1][16100/36908] Elapsed 171m 56s (remain 222m 11s) Loss: 0.0006(0.0033) Grad: 19003.3613  LR: 0.000013  \n","Epoch: [1][16200/36908] Elapsed 173m 1s (remain 221m 8s) Loss: 0.0000(0.0032) Grad: 358.6786  LR: 0.000012  \n","Epoch: [1][16300/36908] Elapsed 174m 4s (remain 220m 3s) Loss: 0.0102(0.0032) Grad: 52706.8867  LR: 0.000012  \n","Epoch: [1][16400/36908] Elapsed 175m 8s (remain 218m 59s) Loss: 0.0013(0.0032) Grad: 44688.9062  LR: 0.000012  \n","Epoch: [1][16500/36908] Elapsed 176m 11s (remain 217m 54s) Loss: 0.0007(0.0032) Grad: 18238.4531  LR: 0.000012  \n","Epoch: [1][16600/36908] Elapsed 177m 16s (remain 216m 50s) Loss: 0.0002(0.0032) Grad: 6291.2710  LR: 0.000012  \n","Epoch: [1][16700/36908] Elapsed 178m 21s (remain 215m 47s) Loss: 0.0050(0.0032) Grad: 572961.5000  LR: 0.000012  \n","Epoch: [1][16800/36908] Elapsed 179m 24s (remain 214m 43s) Loss: 0.0005(0.0032) Grad: 5174.3345  LR: 0.000012  \n","Epoch: [1][16900/36908] Elapsed 180m 28s (remain 213m 38s) Loss: 0.0009(0.0032) Grad: 38112.8164  LR: 0.000012  \n","Epoch: [1][17000/36908] Elapsed 181m 33s (remain 212m 35s) Loss: 0.0000(0.0032) Grad: 27.9936  LR: 0.000012  \n","Epoch: [1][17100/36908] Elapsed 182m 37s (remain 211m 30s) Loss: 0.0014(0.0032) Grad: 47338.8398  LR: 0.000012  \n","Epoch: [1][17200/36908] Elapsed 183m 40s (remain 210m 26s) Loss: 0.0000(0.0031) Grad: 124.9110  LR: 0.000012  \n","Epoch: [1][17300/36908] Elapsed 184m 45s (remain 209m 22s) Loss: 0.0009(0.0031) Grad: 20097.2344  LR: 0.000012  \n","Epoch: [1][17400/36908] Elapsed 185m 49s (remain 208m 19s) Loss: 0.0000(0.0031) Grad: 1282.6753  LR: 0.000012  \n","Epoch: [1][17500/36908] Elapsed 186m 53s (remain 207m 15s) Loss: 0.0000(0.0031) Grad: 194.3654  LR: 0.000012  \n","Epoch: [1][17600/36908] Elapsed 187m 57s (remain 206m 10s) Loss: 0.0000(0.0031) Grad: 474.7278  LR: 0.000012  \n","Epoch: [1][17700/36908] Elapsed 189m 1s (remain 205m 6s) Loss: 0.0000(0.0031) Grad: 3942.6528  LR: 0.000012  \n","Epoch: [1][17800/36908] Elapsed 190m 5s (remain 204m 1s) Loss: 0.0001(0.0031) Grad: 6513.7578  LR: 0.000012  \n","Epoch: [1][17900/36908] Elapsed 191m 9s (remain 202m 58s) Loss: 0.0003(0.0031) Grad: 7314.7993  LR: 0.000011  \n","Epoch: [1][18000/36908] Elapsed 192m 14s (remain 201m 55s) Loss: 0.0018(0.0031) Grad: 53691.9258  LR: 0.000011  \n","Epoch: [1][18100/36908] Elapsed 193m 19s (remain 200m 52s) Loss: 0.0001(0.0031) Grad: 2909.3462  LR: 0.000011  \n","Epoch: [1][18200/36908] Elapsed 194m 24s (remain 199m 49s) Loss: 0.0007(0.0030) Grad: 9303.2148  LR: 0.000011  \n","Epoch: [1][18300/36908] Elapsed 195m 29s (remain 198m 45s) Loss: 0.0030(0.0030) Grad: 44014.0586  LR: 0.000011  \n","Epoch: [1][18400/36908] Elapsed 196m 34s (remain 197m 42s) Loss: 0.0029(0.0030) Grad: 108617.1172  LR: 0.000011  \n","Epoch: [1][18500/36908] Elapsed 197m 40s (remain 196m 39s) Loss: 0.0016(0.0030) Grad: 55067.5820  LR: 0.000011  \n","Epoch: [1][18600/36908] Elapsed 198m 44s (remain 195m 36s) Loss: 0.0028(0.0030) Grad: 58259.1914  LR: 0.000011  \n","Epoch: [1][18700/36908] Elapsed 199m 47s (remain 194m 30s) Loss: 0.0001(0.0030) Grad: 3894.8003  LR: 0.000011  \n","Epoch: [1][18800/36908] Elapsed 200m 51s (remain 193m 27s) Loss: 0.0001(0.0030) Grad: 8787.8965  LR: 0.000011  \n","Epoch: [1][18900/36908] Elapsed 201m 55s (remain 192m 22s) Loss: 0.0044(0.0030) Grad: 183046.5469  LR: 0.000011  \n","Epoch: [1][19000/36908] Elapsed 202m 59s (remain 191m 18s) Loss: 0.0002(0.0030) Grad: 14639.7295  LR: 0.000011  \n","Epoch: [1][19100/36908] Elapsed 204m 3s (remain 190m 14s) Loss: 0.0013(0.0030) Grad: 259661.3125  LR: 0.000011  \n","Epoch: [1][19200/36908] Elapsed 205m 7s (remain 189m 9s) Loss: 0.0001(0.0030) Grad: 3871.8379  LR: 0.000011  \n","Epoch: [1][19300/36908] Elapsed 206m 11s (remain 188m 5s) Loss: 0.0000(0.0029) Grad: 219.8658  LR: 0.000011  \n","Epoch: [1][19400/36908] Elapsed 207m 16s (remain 187m 2s) Loss: 0.0000(0.0029) Grad: 54.0592  LR: 0.000011  \n","Epoch: [1][19500/36908] Elapsed 208m 21s (remain 185m 59s) Loss: 0.0000(0.0029) Grad: 72.8457  LR: 0.000010  \n","Epoch: [1][19600/36908] Elapsed 209m 24s (remain 184m 54s) Loss: 0.0046(0.0029) Grad: 90089.9297  LR: 0.000010  \n","Epoch: [1][19700/36908] Elapsed 210m 28s (remain 183m 50s) Loss: 0.0000(0.0029) Grad: 398.6424  LR: 0.000010  \n","Epoch: [1][19800/36908] Elapsed 211m 33s (remain 182m 46s) Loss: 0.0004(0.0029) Grad: 16378.3779  LR: 0.000010  \n","Epoch: [1][19900/36908] Elapsed 212m 37s (remain 181m 42s) Loss: 0.0000(0.0029) Grad: 8.8351  LR: 0.000010  \n","Epoch: [1][20000/36908] Elapsed 213m 42s (remain 180m 38s) Loss: 0.0000(0.0029) Grad: 304.3685  LR: 0.000010  \n","Epoch: [1][20100/36908] Elapsed 214m 45s (remain 179m 33s) Loss: 0.0001(0.0029) Grad: 47592.5938  LR: 0.000010  \n","Epoch: [1][20200/36908] Elapsed 215m 50s (remain 178m 30s) Loss: 0.0000(0.0029) Grad: 143.1893  LR: 0.000010  \n","Epoch: [1][20300/36908] Elapsed 216m 54s (remain 177m 26s) Loss: 0.0001(0.0029) Grad: 3353.9727  LR: 0.000010  \n","Epoch: [1][20400/36908] Elapsed 217m 58s (remain 176m 22s) Loss: 0.0000(0.0029) Grad: 112.2286  LR: 0.000010  \n","Epoch: [1][20500/36908] Elapsed 219m 2s (remain 175m 18s) Loss: 0.0027(0.0028) Grad: 40129.9336  LR: 0.000010  \n","Epoch: [1][20600/36908] Elapsed 220m 6s (remain 174m 13s) Loss: 0.0001(0.0028) Grad: 3037.2598  LR: 0.000010  \n","Epoch: [1][20700/36908] Elapsed 221m 10s (remain 173m 9s) Loss: 0.0000(0.0028) Grad: 17.6121  LR: 0.000010  \n","Epoch: [1][20800/36908] Elapsed 222m 13s (remain 172m 4s) Loss: 0.0040(0.0028) Grad: 32142.1602  LR: 0.000010  \n","Epoch: [1][20900/36908] Elapsed 223m 16s (remain 170m 59s) Loss: 0.0000(0.0028) Grad: 29.4625  LR: 0.000010  \n","Epoch: [1][21000/36908] Elapsed 224m 20s (remain 169m 55s) Loss: 0.0000(0.0028) Grad: 1311.2247  LR: 0.000010  \n","Epoch: [1][21100/36908] Elapsed 225m 25s (remain 168m 52s) Loss: 0.0025(0.0028) Grad: 12771.5420  LR: 0.000010  \n","Epoch: [1][21200/36908] Elapsed 226m 29s (remain 167m 48s) Loss: 0.0001(0.0028) Grad: 4300.0767  LR: 0.000009  \n","Epoch: [1][21300/36908] Elapsed 227m 34s (remain 166m 44s) Loss: 0.0000(0.0028) Grad: 101.4475  LR: 0.000009  \n","Epoch: [1][21400/36908] Elapsed 228m 38s (remain 165m 40s) Loss: 0.0000(0.0028) Grad: 128.1537  LR: 0.000009  \n","Epoch: [1][21500/36908] Elapsed 229m 42s (remain 164m 35s) Loss: 0.0000(0.0028) Grad: 265.2578  LR: 0.000009  \n","Epoch: [1][21600/36908] Elapsed 230m 45s (remain 163m 31s) Loss: 0.0026(0.0028) Grad: 9004.8887  LR: 0.000009  \n","Epoch: [1][21700/36908] Elapsed 231m 49s (remain 162m 27s) Loss: 0.0000(0.0027) Grad: 43.5392  LR: 0.000009  \n","Epoch: [1][21800/36908] Elapsed 232m 52s (remain 161m 22s) Loss: 0.0010(0.0027) Grad: 14763.3740  LR: 0.000009  \n","Epoch: [1][21900/36908] Elapsed 233m 56s (remain 160m 17s) Loss: 0.0000(0.0027) Grad: 37.1788  LR: 0.000009  \n","Epoch: [1][22000/36908] Elapsed 234m 59s (remain 159m 13s) Loss: 0.0001(0.0027) Grad: 1820.6965  LR: 0.000009  \n","Epoch: [1][22100/36908] Elapsed 236m 3s (remain 158m 9s) Loss: 0.0001(0.0027) Grad: 1861.1266  LR: 0.000009  \n","Epoch: [1][22200/36908] Elapsed 237m 8s (remain 157m 5s) Loss: 0.0000(0.0027) Grad: 1415.6149  LR: 0.000009  \n","Epoch: [1][22300/36908] Elapsed 238m 12s (remain 156m 1s) Loss: 0.0004(0.0027) Grad: 11422.0195  LR: 0.000009  \n","Epoch: [1][22400/36908] Elapsed 239m 17s (remain 154m 57s) Loss: 0.0000(0.0027) Grad: 1492.9391  LR: 0.000009  \n","Epoch: [1][22500/36908] Elapsed 240m 21s (remain 153m 53s) Loss: 0.0000(0.0027) Grad: 117.2127  LR: 0.000009  \n","Epoch: [1][22600/36908] Elapsed 241m 25s (remain 152m 49s) Loss: 0.0011(0.0027) Grad: 17072.8242  LR: 0.000009  \n","Epoch: [1][22700/36908] Elapsed 242m 30s (remain 151m 46s) Loss: 0.0000(0.0027) Grad: 331.6424  LR: 0.000009  \n","Epoch: [1][22800/36908] Elapsed 243m 34s (remain 150m 42s) Loss: 0.0000(0.0027) Grad: 57.9827  LR: 0.000008  \n","Epoch: [1][22900/36908] Elapsed 244m 38s (remain 149m 38s) Loss: 0.0000(0.0027) Grad: 44.2132  LR: 0.000008  \n","Epoch: [1][23000/36908] Elapsed 245m 43s (remain 148m 34s) Loss: 0.0001(0.0027) Grad: 9480.9775  LR: 0.000008  \n","Epoch: [1][23100/36908] Elapsed 246m 47s (remain 147m 29s) Loss: 0.0000(0.0027) Grad: 20.6776  LR: 0.000008  \n","Epoch: [1][23200/36908] Elapsed 247m 51s (remain 146m 25s) Loss: 0.0007(0.0026) Grad: 28813.2891  LR: 0.000008  \n","Epoch: [1][23300/36908] Elapsed 248m 55s (remain 145m 22s) Loss: 0.0000(0.0026) Grad: 301.6058  LR: 0.000008  \n","Epoch: [1][23400/36908] Elapsed 250m 0s (remain 144m 18s) Loss: 0.0000(0.0026) Grad: 67.6660  LR: 0.000008  \n","Epoch: [1][23500/36908] Elapsed 251m 5s (remain 143m 14s) Loss: 0.0000(0.0026) Grad: 373.2580  LR: 0.000008  \n","Epoch: [1][23600/36908] Elapsed 252m 10s (remain 142m 10s) Loss: 0.0007(0.0026) Grad: 28003.9570  LR: 0.000008  \n","Epoch: [1][23700/36908] Elapsed 253m 15s (remain 141m 7s) Loss: 0.0000(0.0026) Grad: 66.3164  LR: 0.000008  \n","Epoch: [1][23800/36908] Elapsed 254m 20s (remain 140m 3s) Loss: 0.0001(0.0026) Grad: 3299.9111  LR: 0.000008  \n","Epoch: [1][23900/36908] Elapsed 255m 25s (remain 139m 0s) Loss: 0.0016(0.0026) Grad: 36591.1875  LR: 0.000008  \n","Epoch: [1][24000/36908] Elapsed 256m 28s (remain 137m 55s) Loss: 0.0007(0.0026) Grad: 7894.6328  LR: 0.000008  \n","Epoch: [1][24100/36908] Elapsed 257m 31s (remain 136m 50s) Loss: 0.0000(0.0026) Grad: 291.5932  LR: 0.000008  \n","Epoch: [1][24200/36908] Elapsed 258m 35s (remain 135m 46s) Loss: 0.0000(0.0026) Grad: 641.4610  LR: 0.000008  \n","Epoch: [1][24300/36908] Elapsed 259m 38s (remain 134m 42s) Loss: 0.0000(0.0026) Grad: 34.5616  LR: 0.000008  \n","Epoch: [1][24400/36908] Elapsed 260m 42s (remain 133m 37s) Loss: 0.0000(0.0026) Grad: 1131.0889  LR: 0.000008  \n","Epoch: [1][24500/36908] Elapsed 261m 44s (remain 132m 32s) Loss: 0.0009(0.0026) Grad: 9519.0947  LR: 0.000007  \n","Epoch: [1][24600/36908] Elapsed 262m 49s (remain 131m 28s) Loss: 0.0000(0.0026) Grad: 322.3607  LR: 0.000007  \n","Epoch: [1][24700/36908] Elapsed 263m 54s (remain 130m 25s) Loss: 0.0023(0.0026) Grad: 122984.7734  LR: 0.000007  \n","Epoch: [1][24800/36908] Elapsed 264m 59s (remain 129m 21s) Loss: 0.0000(0.0026) Grad: 640.6004  LR: 0.000007  \n","Epoch: [1][24900/36908] Elapsed 266m 4s (remain 128m 17s) Loss: 0.0057(0.0025) Grad: 153382.3125  LR: 0.000007  \n","Epoch: [1][25000/36908] Elapsed 267m 8s (remain 127m 13s) Loss: 0.0020(0.0025) Grad: 51500.3008  LR: 0.000007  \n","Epoch: [1][25100/36908] Elapsed 268m 13s (remain 126m 10s) Loss: 0.0002(0.0025) Grad: 22926.0547  LR: 0.000007  \n","Epoch: [1][25200/36908] Elapsed 269m 18s (remain 125m 6s) Loss: 0.0002(0.0025) Grad: 13440.9951  LR: 0.000007  \n","Epoch: [1][25300/36908] Elapsed 270m 22s (remain 124m 2s) Loss: 0.0007(0.0025) Grad: 36694.3828  LR: 0.000007  \n","Epoch: [1][25400/36908] Elapsed 271m 27s (remain 122m 58s) Loss: 0.0000(0.0025) Grad: 66.0601  LR: 0.000007  \n","Epoch: [1][25500/36908] Elapsed 272m 31s (remain 121m 54s) Loss: 0.0006(0.0025) Grad: 26510.2129  LR: 0.000007  \n","Epoch: [1][25600/36908] Elapsed 273m 34s (remain 120m 49s) Loss: 0.0000(0.0025) Grad: 44.1671  LR: 0.000007  \n","Epoch: [1][25700/36908] Elapsed 274m 39s (remain 119m 45s) Loss: 0.0000(0.0025) Grad: 76.8674  LR: 0.000007  \n","Epoch: [1][25800/36908] Elapsed 275m 44s (remain 118m 42s) Loss: 0.0000(0.0025) Grad: 1722.3710  LR: 0.000007  \n","Epoch: [1][25900/36908] Elapsed 276m 49s (remain 117m 38s) Loss: 0.0000(0.0025) Grad: 2335.8474  LR: 0.000007  \n","Epoch: [1][26000/36908] Elapsed 277m 54s (remain 116m 34s) Loss: 0.0000(0.0025) Grad: 1200.0868  LR: 0.000007  \n","Epoch: [1][26100/36908] Elapsed 278m 59s (remain 115m 31s) Loss: 0.0000(0.0025) Grad: 94.8420  LR: 0.000007  \n","Epoch: [1][26200/36908] Elapsed 280m 5s (remain 114m 27s) Loss: 0.0007(0.0025) Grad: 44537.1523  LR: 0.000006  \n","Epoch: [1][26300/36908] Elapsed 281m 10s (remain 113m 23s) Loss: 0.0000(0.0025) Grad: 2926.2583  LR: 0.000006  \n","Epoch: [1][26400/36908] Elapsed 282m 15s (remain 112m 20s) Loss: 0.0001(0.0025) Grad: 4686.7207  LR: 0.000006  \n","Epoch: [1][26500/36908] Elapsed 283m 20s (remain 111m 16s) Loss: 0.0004(0.0025) Grad: 16151.2002  LR: 0.000006  \n","Epoch: [1][26600/36908] Elapsed 284m 24s (remain 110m 12s) Loss: 0.0002(0.0025) Grad: 23691.8809  LR: 0.000006  \n","Epoch: [1][26700/36908] Elapsed 285m 28s (remain 109m 7s) Loss: 0.0001(0.0025) Grad: 16082.8867  LR: 0.000006  \n","Epoch: [1][26800/36908] Elapsed 286m 34s (remain 108m 4s) Loss: 0.0018(0.0024) Grad: 64557.8359  LR: 0.000006  \n","Epoch: [1][26900/36908] Elapsed 287m 40s (remain 107m 0s) Loss: 0.0010(0.0024) Grad: 34761.9922  LR: 0.000006  \n","Epoch: [1][27000/36908] Elapsed 288m 46s (remain 105m 57s) Loss: 0.0000(0.0024) Grad: 51.9387  LR: 0.000006  \n","Epoch: [1][27100/36908] Elapsed 289m 51s (remain 104m 53s) Loss: 0.0000(0.0024) Grad: 1218.4370  LR: 0.000006  \n","Epoch: [1][27200/36908] Elapsed 290m 57s (remain 103m 49s) Loss: 0.0000(0.0024) Grad: 197.4724  LR: 0.000006  \n","Epoch: [1][27300/36908] Elapsed 292m 3s (remain 102m 46s) Loss: 0.0012(0.0024) Grad: 71308.6406  LR: 0.000006  \n","Epoch: [1][27400/36908] Elapsed 293m 8s (remain 101m 42s) Loss: 0.0257(0.0024) Grad: 563168.0625  LR: 0.000006  \n","Epoch: [1][27500/36908] Elapsed 294m 13s (remain 100m 38s) Loss: 0.0000(0.0024) Grad: 51.4633  LR: 0.000006  \n","Epoch: [1][27600/36908] Elapsed 295m 17s (remain 99m 34s) Loss: 0.0000(0.0024) Grad: 99.9202  LR: 0.000006  \n","Epoch: [1][27700/36908] Elapsed 296m 21s (remain 98m 30s) Loss: 0.0000(0.0024) Grad: 13.2768  LR: 0.000006  \n","Epoch: [1][27800/36908] Elapsed 297m 25s (remain 97m 25s) Loss: 0.0001(0.0024) Grad: 1431.3553  LR: 0.000005  \n","Epoch: [1][27900/36908] Elapsed 298m 30s (remain 96m 21s) Loss: 0.0000(0.0024) Grad: 122.5617  LR: 0.000005  \n","Epoch: [1][28000/36908] Elapsed 299m 35s (remain 95m 17s) Loss: 0.0001(0.0024) Grad: 947.8459  LR: 0.000005  \n","Epoch: [1][28100/36908] Elapsed 300m 40s (remain 94m 13s) Loss: 0.0006(0.0024) Grad: 14555.2158  LR: 0.000005  \n","Epoch: [1][28200/36908] Elapsed 301m 45s (remain 93m 10s) Loss: 0.0130(0.0024) Grad: 86097.5000  LR: 0.000005  \n","Epoch: [1][28300/36908] Elapsed 302m 51s (remain 92m 6s) Loss: 0.0008(0.0024) Grad: 33476.9180  LR: 0.000005  \n","Epoch: [1][28400/36908] Elapsed 303m 55s (remain 91m 2s) Loss: 0.0001(0.0024) Grad: 1952.0537  LR: 0.000005  \n","Epoch: [1][28500/36908] Elapsed 305m 0s (remain 89m 58s) Loss: 0.0000(0.0024) Grad: 187.7278  LR: 0.000005  \n","Epoch: [1][28600/36908] Elapsed 306m 5s (remain 88m 54s) Loss: 0.0038(0.0024) Grad: 35924.3203  LR: 0.000005  \n","Epoch: [1][28700/36908] Elapsed 307m 10s (remain 87m 50s) Loss: 0.0001(0.0024) Grad: 5909.1484  LR: 0.000005  \n","Epoch: [1][28800/36908] Elapsed 308m 15s (remain 86m 46s) Loss: 0.0000(0.0024) Grad: 4476.8135  LR: 0.000005  \n","Epoch: [1][28900/36908] Elapsed 309m 19s (remain 85m 41s) Loss: 0.0000(0.0024) Grad: 352.7993  LR: 0.000005  \n","Epoch: [1][29000/36908] Elapsed 310m 24s (remain 84m 38s) Loss: 0.0004(0.0024) Grad: 19347.1895  LR: 0.000005  \n","Epoch: [1][29100/36908] Elapsed 311m 30s (remain 83m 34s) Loss: 0.0009(0.0023) Grad: 11849.3799  LR: 0.000005  \n","Epoch: [1][29200/36908] Elapsed 312m 35s (remain 82m 30s) Loss: 0.0001(0.0023) Grad: 3724.9795  LR: 0.000005  \n","Epoch: [1][29300/36908] Elapsed 313m 39s (remain 81m 25s) Loss: 0.0000(0.0023) Grad: 217.2619  LR: 0.000005  \n","Epoch: [1][29400/36908] Elapsed 314m 45s (remain 80m 22s) Loss: 0.0002(0.0023) Grad: 3457.4058  LR: 0.000005  \n","Epoch: [1][29500/36908] Elapsed 315m 51s (remain 79m 18s) Loss: 0.0000(0.0023) Grad: 113.4743  LR: 0.000004  \n","Epoch: [1][29600/36908] Elapsed 316m 57s (remain 78m 14s) Loss: 0.0001(0.0023) Grad: 1389.1346  LR: 0.000004  \n","Epoch: [1][29700/36908] Elapsed 318m 1s (remain 77m 10s) Loss: 0.0004(0.0023) Grad: 11357.1523  LR: 0.000004  \n","Epoch: [1][29800/36908] Elapsed 319m 5s (remain 76m 5s) Loss: 0.0000(0.0023) Grad: 63.0696  LR: 0.000004  \n","Epoch: [1][29900/36908] Elapsed 320m 10s (remain 75m 1s) Loss: 0.0016(0.0023) Grad: 52093.2070  LR: 0.000004  \n","Epoch: [1][30000/36908] Elapsed 321m 14s (remain 73m 57s) Loss: 0.0000(0.0023) Grad: 103.5954  LR: 0.000004  \n","Epoch: [1][30100/36908] Elapsed 322m 19s (remain 72m 53s) Loss: 0.0003(0.0023) Grad: 10645.2754  LR: 0.000004  \n","Epoch: [1][30200/36908] Elapsed 323m 22s (remain 71m 48s) Loss: 0.0000(0.0023) Grad: 43.1315  LR: 0.000004  \n","Epoch: [1][30300/36908] Elapsed 324m 27s (remain 70m 44s) Loss: 0.0000(0.0023) Grad: 133.0298  LR: 0.000004  \n","Epoch: [1][30400/36908] Elapsed 325m 32s (remain 69m 40s) Loss: 0.0057(0.0023) Grad: 40163.0508  LR: 0.000004  \n","Epoch: [1][30500/36908] Elapsed 326m 36s (remain 68m 36s) Loss: 0.0000(0.0023) Grad: 76.2538  LR: 0.000004  \n","Epoch: [1][30600/36908] Elapsed 327m 40s (remain 67m 32s) Loss: 0.0007(0.0023) Grad: 30334.3535  LR: 0.000004  \n","Epoch: [1][30700/36908] Elapsed 328m 45s (remain 66m 27s) Loss: 0.0000(0.0023) Grad: 507.0058  LR: 0.000004  \n","Epoch: [1][30800/36908] Elapsed 329m 49s (remain 65m 23s) Loss: 0.0000(0.0023) Grad: 15.1351  LR: 0.000004  \n","Epoch: [1][30900/36908] Elapsed 330m 53s (remain 64m 19s) Loss: 0.0014(0.0023) Grad: 12461.2080  LR: 0.000004  \n","Epoch: [1][31000/36908] Elapsed 331m 57s (remain 63m 15s) Loss: 0.0001(0.0023) Grad: 1212.8921  LR: 0.000004  \n","Epoch: [1][31100/36908] Elapsed 333m 0s (remain 62m 10s) Loss: 0.0002(0.0023) Grad: 2382.1267  LR: 0.000003  \n","Epoch: [1][31200/36908] Elapsed 334m 4s (remain 61m 6s) Loss: 0.0001(0.0023) Grad: 4638.6035  LR: 0.000003  \n","Epoch: [1][31300/36908] Elapsed 335m 9s (remain 60m 2s) Loss: 0.0000(0.0023) Grad: 19.4087  LR: 0.000003  \n","Epoch: [1][31400/36908] Elapsed 336m 13s (remain 58m 57s) Loss: 0.0001(0.0023) Grad: 1697.9038  LR: 0.000003  \n","Epoch: [1][31500/36908] Elapsed 337m 17s (remain 57m 53s) Loss: 0.0001(0.0023) Grad: 3819.5259  LR: 0.000003  \n","Epoch: [1][31600/36908] Elapsed 338m 21s (remain 56m 49s) Loss: 0.0000(0.0023) Grad: 75.4490  LR: 0.000003  \n","Epoch: [1][31700/36908] Elapsed 339m 25s (remain 55m 45s) Loss: 0.0000(0.0023) Grad: 102.5135  LR: 0.000003  \n","Epoch: [1][31800/36908] Elapsed 340m 29s (remain 54m 40s) Loss: 0.0186(0.0022) Grad: 194803.4531  LR: 0.000003  \n","Epoch: [1][31900/36908] Elapsed 341m 33s (remain 53m 36s) Loss: 0.0001(0.0022) Grad: 13813.7275  LR: 0.000003  \n","Epoch: [1][32000/36908] Elapsed 342m 37s (remain 52m 32s) Loss: 0.0000(0.0022) Grad: 221.9267  LR: 0.000003  \n","Epoch: [1][32100/36908] Elapsed 343m 41s (remain 51m 28s) Loss: 0.0001(0.0022) Grad: 4982.5576  LR: 0.000003  \n","Epoch: [1][32200/36908] Elapsed 344m 45s (remain 50m 23s) Loss: 0.0033(0.0022) Grad: 256882.9062  LR: 0.000003  \n","Epoch: [1][32300/36908] Elapsed 345m 49s (remain 49m 19s) Loss: 0.0032(0.0022) Grad: 91194.8359  LR: 0.000003  \n","Epoch: [1][32400/36908] Elapsed 346m 53s (remain 48m 15s) Loss: 0.0020(0.0022) Grad: 133994.2656  LR: 0.000003  \n","Epoch: [1][32500/36908] Elapsed 347m 57s (remain 47m 10s) Loss: 0.0000(0.0022) Grad: 53.9227  LR: 0.000003  \n","Epoch: [1][32600/36908] Elapsed 349m 1s (remain 46m 6s) Loss: 0.0000(0.0022) Grad: 938.6559  LR: 0.000003  \n","Epoch: [1][32700/36908] Elapsed 350m 5s (remain 45m 2s) Loss: 0.0001(0.0022) Grad: 5835.5029  LR: 0.000003  \n","Epoch: [1][32800/36908] Elapsed 351m 9s (remain 43m 58s) Loss: 0.0066(0.0022) Grad: 258224.7969  LR: 0.000002  \n","Epoch: [1][32900/36908] Elapsed 352m 13s (remain 42m 53s) Loss: 0.0067(0.0022) Grad: 73812.5625  LR: 0.000002  \n","Epoch: [1][33000/36908] Elapsed 353m 17s (remain 41m 49s) Loss: 0.0000(0.0022) Grad: 1241.6542  LR: 0.000002  \n","Epoch: [1][33100/36908] Elapsed 354m 21s (remain 40m 45s) Loss: 0.0050(0.0022) Grad: 553368.9375  LR: 0.000002  \n","Epoch: [1][33200/36908] Elapsed 355m 25s (remain 39m 41s) Loss: 0.0001(0.0022) Grad: 11279.4756  LR: 0.000002  \n","Epoch: [1][33300/36908] Elapsed 356m 29s (remain 38m 36s) Loss: 0.0002(0.0022) Grad: 19020.2812  LR: 0.000002  \n","Epoch: [1][33400/36908] Elapsed 357m 33s (remain 37m 32s) Loss: 0.0000(0.0022) Grad: 160.5955  LR: 0.000002  \n","Epoch: [1][33500/36908] Elapsed 358m 37s (remain 36m 28s) Loss: 0.0001(0.0022) Grad: 2986.6526  LR: 0.000002  \n","Epoch: [1][33600/36908] Elapsed 359m 41s (remain 35m 24s) Loss: 0.0000(0.0022) Grad: 46.6092  LR: 0.000002  \n","Epoch: [1][33700/36908] Elapsed 360m 45s (remain 34m 19s) Loss: 0.0005(0.0022) Grad: 30512.2285  LR: 0.000002  \n","Epoch: [1][33800/36908] Elapsed 361m 49s (remain 33m 15s) Loss: 0.0000(0.0022) Grad: 299.0883  LR: 0.000002  \n","Epoch: [1][33900/36908] Elapsed 362m 54s (remain 32m 11s) Loss: 0.0001(0.0022) Grad: 2768.0532  LR: 0.000002  \n","Epoch: [1][34000/36908] Elapsed 363m 58s (remain 31m 7s) Loss: 0.0000(0.0022) Grad: 871.9969  LR: 0.000002  \n","Epoch: [1][34100/36908] Elapsed 365m 2s (remain 30m 2s) Loss: 0.0081(0.0022) Grad: 146000.8594  LR: 0.000002  \n","Epoch: [1][34200/36908] Elapsed 366m 5s (remain 28m 58s) Loss: 0.0005(0.0022) Grad: 11495.3594  LR: 0.000002  \n","Epoch: [1][34300/36908] Elapsed 367m 10s (remain 27m 54s) Loss: 0.0000(0.0022) Grad: 113.9801  LR: 0.000002  \n","Epoch: [1][34400/36908] Elapsed 368m 14s (remain 26m 50s) Loss: 0.0013(0.0022) Grad: 29109.3203  LR: 0.000002  \n","Epoch: [1][34500/36908] Elapsed 369m 18s (remain 25m 45s) Loss: 0.0000(0.0022) Grad: 235.6461  LR: 0.000001  \n","Epoch: [1][34600/36908] Elapsed 370m 21s (remain 24m 41s) Loss: 0.0003(0.0022) Grad: 7066.8350  LR: 0.000001  \n","Epoch: [1][34700/36908] Elapsed 371m 25s (remain 23m 37s) Loss: 0.0001(0.0022) Grad: 3962.3945  LR: 0.000001  \n","Epoch: [1][34800/36908] Elapsed 372m 29s (remain 22m 33s) Loss: 0.0000(0.0021) Grad: 31.6201  LR: 0.000001  \n","Epoch: [1][34900/36908] Elapsed 373m 34s (remain 21m 28s) Loss: 0.0000(0.0021) Grad: 1291.3344  LR: 0.000001  \n","Epoch: [1][35000/36908] Elapsed 374m 38s (remain 20m 24s) Loss: 0.0000(0.0021) Grad: 480.9157  LR: 0.000001  \n","Epoch: [1][35100/36908] Elapsed 375m 42s (remain 19m 20s) Loss: 0.0000(0.0021) Grad: 24.1991  LR: 0.000001  \n","Epoch: [1][35200/36908] Elapsed 376m 46s (remain 18m 16s) Loss: 0.0012(0.0021) Grad: 32172.9043  LR: 0.000001  \n","Epoch: [1][35300/36908] Elapsed 377m 50s (remain 17m 12s) Loss: 0.0058(0.0021) Grad: 36604.4648  LR: 0.000001  \n","Epoch: [1][35400/36908] Elapsed 378m 54s (remain 16m 7s) Loss: 0.0023(0.0021) Grad: 52207.4414  LR: 0.000001  \n","Epoch: [1][35500/36908] Elapsed 379m 58s (remain 15m 3s) Loss: 0.0000(0.0021) Grad: 439.5222  LR: 0.000001  \n","Epoch: [1][35600/36908] Elapsed 381m 3s (remain 13m 59s) Loss: 0.0000(0.0021) Grad: 4.9447  LR: 0.000001  \n","Epoch: [1][35700/36908] Elapsed 382m 7s (remain 12m 55s) Loss: 0.0000(0.0021) Grad: 202.7883  LR: 0.000001  \n","Epoch: [1][35800/36908] Elapsed 383m 12s (remain 11m 50s) Loss: 0.0023(0.0021) Grad: 77592.0859  LR: 0.000001  \n","Epoch: [1][35900/36908] Elapsed 384m 16s (remain 10m 46s) Loss: 0.0001(0.0021) Grad: 2490.2334  LR: 0.000001  \n","Epoch: [1][36000/36908] Elapsed 385m 20s (remain 9m 42s) Loss: 0.0000(0.0021) Grad: 61.5106  LR: 0.000001  \n","Epoch: [1][36100/36908] Elapsed 386m 25s (remain 8m 38s) Loss: 0.0002(0.0021) Grad: 4832.1025  LR: 0.000000  \n","Epoch: [1][36200/36908] Elapsed 387m 29s (remain 7m 34s) Loss: 0.0000(0.0021) Grad: 960.3864  LR: 0.000000  \n","Epoch: [1][36300/36908] Elapsed 388m 34s (remain 6m 29s) Loss: 0.0000(0.0021) Grad: 436.6314  LR: 0.000000  \n","Epoch: [1][36400/36908] Elapsed 389m 39s (remain 5m 25s) Loss: 0.0000(0.0021) Grad: 101.3187  LR: 0.000000  \n","Epoch: [1][36500/36908] Elapsed 390m 43s (remain 4m 21s) Loss: 0.0000(0.0021) Grad: 37.6747  LR: 0.000000  \n","Epoch: [1][36600/36908] Elapsed 391m 47s (remain 3m 17s) Loss: 0.0002(0.0021) Grad: 6179.5601  LR: 0.000000  \n","Epoch: [1][36700/36908] Elapsed 392m 51s (remain 2m 12s) Loss: 0.0006(0.0021) Grad: 15480.9873  LR: 0.000000  \n","Epoch: [1][36800/36908] Elapsed 393m 54s (remain 1m 8s) Loss: 0.0005(0.0021) Grad: 11347.9570  LR: 0.000000  \n","Epoch: [1][36900/36908] Elapsed 394m 57s (remain 0m 4s) Loss: 0.0000(0.0021) Grad: 47.3615  LR: 0.000000  \n","Epoch: [1][36907/36908] Elapsed 395m 2s (remain 0m 0s) Loss: 0.0000(0.0021) Grad: 997.8170  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 19m 10s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 23s (remain 4m 18s) Loss: 0.0002(0.0031) \n","EVAL: [200/1192] Elapsed 0m 47s (remain 3m 51s) Loss: 0.0000(0.0034) \n","EVAL: [300/1192] Elapsed 1m 9s (remain 3m 27s) Loss: 0.0005(0.0053) \n","EVAL: [400/1192] Elapsed 1m 32s (remain 3m 2s) Loss: 0.0094(0.0053) \n","EVAL: [500/1192] Elapsed 1m 55s (remain 2m 39s) Loss: 0.0155(0.0048) \n","EVAL: [600/1192] Elapsed 2m 18s (remain 2m 16s) Loss: 0.0704(0.0049) \n","EVAL: [700/1192] Elapsed 2m 41s (remain 1m 52s) Loss: 0.0037(0.0056) \n","EVAL: [800/1192] Elapsed 3m 4s (remain 1m 29s) Loss: 0.0013(0.0055) \n","EVAL: [900/1192] Elapsed 3m 27s (remain 1m 6s) Loss: 0.0001(0.0053) \n","EVAL: [1000/1192] Elapsed 3m 49s (remain 0m 43s) Loss: 0.0013(0.0050) \n","EVAL: [1100/1192] Elapsed 4m 12s (remain 0m 20s) Loss: 0.0035(0.0048) \n","EVAL: [1191/1192] Elapsed 4m 33s (remain 0m 0s) Loss: 0.0048(0.0046) \n","Epoch 1 - avg_train_loss: 0.0021  avg_val_loss: 0.0046  time: 23978s\n","Epoch 1 - Score: 0.8796\n","Epoch 1 - Save Best Score: 0.8796 Model\n","========== fold: 2 training ==========\n","get pseudo plain from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n","get pseudo labels from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_2.npy\n","get pseudo labels from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_2.npy\n","get pseudo labels from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp083/pseudo_labels_2.npy\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e0bd90cddc7942108e6d93d2ac2b82ce","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/612602 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"17ada097d7904de7bfdf407bd7a46434","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/612602 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["(612602, 950)\n","(612602, 6) (612602, 950)\n","(100000, 7)\n","(110725, 11)\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\n","Epoch: [1][0/36908] Elapsed 0m 1s (remain 992m 1s) Loss: 0.0851(0.0851) Grad: 74909.3984  LR: 0.000000  \n","Epoch: [1][100/36908] Elapsed 1m 5s (remain 395m 34s) Loss: 0.0663(0.0788) Grad: 64577.2852  LR: 0.000001  \n","Epoch: [1][200/36908] Elapsed 2m 8s (remain 391m 12s) Loss: 0.0301(0.0649) Grad: 34872.2578  LR: 0.000001  \n","Epoch: [1][300/36908] Elapsed 3m 11s (remain 388m 40s) Loss: 0.0158(0.0503) Grad: 4097.6338  LR: 0.000002  \n","Epoch: [1][400/36908] Elapsed 4m 14s (remain 386m 53s) Loss: 0.0202(0.0408) Grad: 8213.8027  LR: 0.000002  \n","Epoch: [1][500/36908] Elapsed 5m 17s (remain 384m 11s) Loss: 0.0099(0.0350) Grad: 2644.7234  LR: 0.000003  \n","Epoch: [1][600/36908] Elapsed 6m 19s (remain 382m 8s) Loss: 0.0123(0.0312) Grad: 3901.3474  LR: 0.000003  \n","Epoch: [1][700/36908] Elapsed 7m 21s (remain 380m 11s) Loss: 0.0114(0.0284) Grad: 3704.8105  LR: 0.000004  \n","Epoch: [1][800/36908] Elapsed 8m 24s (remain 378m 53s) Loss: 0.0107(0.0262) Grad: 4803.9365  LR: 0.000004  \n","Epoch: [1][900/36908] Elapsed 9m 27s (remain 378m 17s) Loss: 0.0071(0.0246) Grad: 3234.1936  LR: 0.000005  \n","Epoch: [1][1000/36908] Elapsed 10m 30s (remain 377m 3s) Loss: 0.0171(0.0232) Grad: 9273.3027  LR: 0.000005  \n","Epoch: [1][1100/36908] Elapsed 11m 33s (remain 376m 8s) Loss: 0.0022(0.0219) Grad: 3641.1777  LR: 0.000006  \n","Epoch: [1][1200/36908] Elapsed 12m 37s (remain 375m 8s) Loss: 0.0047(0.0208) Grad: 6156.7744  LR: 0.000007  \n","Epoch: [1][1300/36908] Elapsed 13m 39s (remain 374m 2s) Loss: 0.0030(0.0197) Grad: 6907.3945  LR: 0.000007  \n","Epoch: [1][1400/36908] Elapsed 14m 42s (remain 372m 53s) Loss: 0.0021(0.0187) Grad: 4854.6460  LR: 0.000008  \n","Epoch: [1][1500/36908] Elapsed 15m 45s (remain 371m 54s) Loss: 0.0006(0.0178) Grad: 1392.6254  LR: 0.000008  \n","Epoch: [1][1600/36908] Elapsed 16m 48s (remain 370m 39s) Loss: 0.0021(0.0171) Grad: 4517.1123  LR: 0.000009  \n","Epoch: [1][1700/36908] Elapsed 17m 51s (remain 369m 28s) Loss: 0.0176(0.0163) Grad: 48713.5039  LR: 0.000009  \n","Epoch: [1][1800/36908] Elapsed 18m 54s (remain 368m 30s) Loss: 0.0294(0.0157) Grad: 57069.2969  LR: 0.000010  \n","Epoch: [1][1900/36908] Elapsed 19m 58s (remain 367m 49s) Loss: 0.0008(0.0151) Grad: 1986.5079  LR: 0.000010  \n","Epoch: [1][2000/36908] Elapsed 21m 2s (remain 366m 59s) Loss: 0.0028(0.0145) Grad: 8458.6934  LR: 0.000011  \n","Epoch: [1][2100/36908] Elapsed 22m 7s (remain 366m 24s) Loss: 0.0005(0.0140) Grad: 1027.2487  LR: 0.000011  \n","Epoch: [1][2200/36908] Elapsed 23m 11s (remain 365m 41s) Loss: 0.0122(0.0136) Grad: 23411.7480  LR: 0.000012  \n","Epoch: [1][2300/36908] Elapsed 24m 16s (remain 365m 2s) Loss: 0.0123(0.0131) Grad: 13777.1562  LR: 0.000012  \n","Epoch: [1][2400/36908] Elapsed 25m 20s (remain 364m 9s) Loss: 0.0042(0.0127) Grad: 33475.9062  LR: 0.000013  \n","Epoch: [1][2500/36908] Elapsed 26m 24s (remain 363m 17s) Loss: 0.0045(0.0124) Grad: 13569.5664  LR: 0.000014  \n","Epoch: [1][2600/36908] Elapsed 27m 28s (remain 362m 23s) Loss: 0.0010(0.0120) Grad: 4270.6094  LR: 0.000014  \n","Epoch: [1][2700/36908] Elapsed 28m 32s (remain 361m 24s) Loss: 0.0017(0.0117) Grad: 5216.7939  LR: 0.000015  \n","Epoch: [1][2800/36908] Elapsed 29m 36s (remain 360m 27s) Loss: 0.0000(0.0114) Grad: 53.9660  LR: 0.000015  \n","Epoch: [1][2900/36908] Elapsed 30m 39s (remain 359m 29s) Loss: 0.0078(0.0111) Grad: 11555.5869  LR: 0.000016  \n","Epoch: [1][3000/36908] Elapsed 31m 44s (remain 358m 32s) Loss: 0.0023(0.0108) Grad: 5707.6084  LR: 0.000016  \n","Epoch: [1][3100/36908] Elapsed 32m 48s (remain 357m 42s) Loss: 0.0014(0.0106) Grad: 19637.0312  LR: 0.000017  \n","Epoch: [1][3200/36908] Elapsed 33m 53s (remain 356m 51s) Loss: 0.0016(0.0103) Grad: 9427.2773  LR: 0.000017  \n","Epoch: [1][3300/36908] Elapsed 34m 57s (remain 355m 56s) Loss: 0.0003(0.0101) Grad: 740.0035  LR: 0.000018  \n","Epoch: [1][3400/36908] Elapsed 36m 1s (remain 354m 54s) Loss: 0.0198(0.0099) Grad: 38342.1289  LR: 0.000018  \n","Epoch: [1][3500/36908] Elapsed 37m 5s (remain 353m 55s) Loss: 0.0103(0.0097) Grad: 22144.3965  LR: 0.000019  \n","Epoch: [1][3600/36908] Elapsed 38m 10s (remain 353m 5s) Loss: 0.0005(0.0095) Grad: 2970.2356  LR: 0.000020  \n","Epoch: [1][3700/36908] Elapsed 39m 14s (remain 352m 4s) Loss: 0.0002(0.0093) Grad: 400.5612  LR: 0.000020  \n","Epoch: [1][3800/36908] Elapsed 40m 17s (remain 351m 0s) Loss: 0.0003(0.0091) Grad: 284.1223  LR: 0.000020  \n","Epoch: [1][3900/36908] Elapsed 41m 21s (remain 349m 56s) Loss: 0.0002(0.0090) Grad: 275.1859  LR: 0.000020  \n","Epoch: [1][4000/36908] Elapsed 42m 24s (remain 348m 50s) Loss: 0.0002(0.0088) Grad: 251.1341  LR: 0.000020  \n","Epoch: [1][4100/36908] Elapsed 43m 28s (remain 347m 45s) Loss: 0.0015(0.0087) Grad: 5391.4009  LR: 0.000020  \n","Epoch: [1][4200/36908] Elapsed 44m 31s (remain 346m 36s) Loss: 0.0024(0.0085) Grad: 24485.7227  LR: 0.000020  \n","Epoch: [1][4300/36908] Elapsed 45m 34s (remain 345m 28s) Loss: 0.0028(0.0084) Grad: 37133.6953  LR: 0.000020  \n","Epoch: [1][4400/36908] Elapsed 46m 37s (remain 344m 20s) Loss: 0.0021(0.0083) Grad: 17080.1016  LR: 0.000020  \n","Epoch: [1][4500/36908] Elapsed 47m 40s (remain 343m 16s) Loss: 0.0009(0.0081) Grad: 3170.9392  LR: 0.000020  \n","Epoch: [1][4600/36908] Elapsed 48m 43s (remain 342m 11s) Loss: 0.0001(0.0080) Grad: 140.4497  LR: 0.000019  \n","Epoch: [1][4700/36908] Elapsed 49m 47s (remain 341m 6s) Loss: 0.0041(0.0079) Grad: 23505.5195  LR: 0.000019  \n","Epoch: [1][4800/36908] Elapsed 50m 50s (remain 340m 1s) Loss: 0.0019(0.0078) Grad: 24708.4434  LR: 0.000019  \n","Epoch: [1][4900/36908] Elapsed 51m 53s (remain 338m 55s) Loss: 0.0008(0.0076) Grad: 41016.0039  LR: 0.000019  \n","Epoch: [1][5000/36908] Elapsed 52m 59s (remain 338m 3s) Loss: 0.0027(0.0075) Grad: 39324.9609  LR: 0.000019  \n","Epoch: [1][5100/36908] Elapsed 54m 2s (remain 336m 58s) Loss: 0.0055(0.0074) Grad: 31469.4961  LR: 0.000019  \n","Epoch: [1][5200/36908] Elapsed 55m 5s (remain 335m 53s) Loss: 0.0001(0.0073) Grad: 277.7940  LR: 0.000019  \n","Epoch: [1][5300/36908] Elapsed 56m 9s (remain 334m 49s) Loss: 0.0008(0.0072) Grad: 9065.2725  LR: 0.000019  \n","Epoch: [1][5400/36908] Elapsed 57m 12s (remain 333m 43s) Loss: 0.0013(0.0071) Grad: 7166.3462  LR: 0.000019  \n","Epoch: [1][5500/36908] Elapsed 58m 15s (remain 332m 38s) Loss: 0.0004(0.0070) Grad: 6331.0869  LR: 0.000019  \n","Epoch: [1][5600/36908] Elapsed 59m 18s (remain 331m 32s) Loss: 0.0014(0.0070) Grad: 11248.9160  LR: 0.000019  \n","Epoch: [1][5700/36908] Elapsed 60m 22s (remain 330m 30s) Loss: 0.0005(0.0069) Grad: 5291.0186  LR: 0.000019  \n","Epoch: [1][5800/36908] Elapsed 61m 25s (remain 329m 25s) Loss: 0.0010(0.0068) Grad: 14849.9414  LR: 0.000019  \n","Epoch: [1][5900/36908] Elapsed 62m 29s (remain 328m 20s) Loss: 0.0018(0.0067) Grad: 6609.9136  LR: 0.000019  \n","Epoch: [1][6000/36908] Elapsed 63m 33s (remain 327m 19s) Loss: 0.0001(0.0066) Grad: 1795.0095  LR: 0.000019  \n","Epoch: [1][6100/36908] Elapsed 64m 37s (remain 326m 18s) Loss: 0.0002(0.0066) Grad: 1556.4233  LR: 0.000019  \n","Epoch: [1][6200/36908] Elapsed 65m 41s (remain 325m 16s) Loss: 0.0002(0.0065) Grad: 1399.2991  LR: 0.000018  \n","Epoch: [1][6300/36908] Elapsed 66m 44s (remain 324m 12s) Loss: 0.0001(0.0064) Grad: 434.8714  LR: 0.000018  \n","Epoch: [1][6400/36908] Elapsed 67m 48s (remain 323m 9s) Loss: 0.0001(0.0063) Grad: 208.9984  LR: 0.000018  \n","Epoch: [1][6500/36908] Elapsed 68m 52s (remain 322m 8s) Loss: 0.0033(0.0063) Grad: 24425.4902  LR: 0.000018  \n","Epoch: [1][6600/36908] Elapsed 69m 56s (remain 321m 5s) Loss: 0.0003(0.0062) Grad: 1982.1481  LR: 0.000018  \n","Epoch: [1][6700/36908] Elapsed 71m 0s (remain 320m 5s) Loss: 0.0007(0.0061) Grad: 7765.6216  LR: 0.000018  \n","Epoch: [1][6800/36908] Elapsed 72m 4s (remain 319m 3s) Loss: 0.0005(0.0061) Grad: 1415.9902  LR: 0.000018  \n","Epoch: [1][6900/36908] Elapsed 73m 8s (remain 318m 2s) Loss: 0.0011(0.0060) Grad: 23690.5039  LR: 0.000018  \n","Epoch: [1][7000/36908] Elapsed 74m 12s (remain 317m 1s) Loss: 0.0000(0.0059) Grad: 126.1907  LR: 0.000018  \n","Epoch: [1][7100/36908] Elapsed 75m 16s (remain 315m 57s) Loss: 0.0066(0.0059) Grad: 42436.1406  LR: 0.000018  \n","Epoch: [1][7200/36908] Elapsed 76m 20s (remain 314m 54s) Loss: 0.0000(0.0058) Grad: 175.7040  LR: 0.000018  \n","Epoch: [1][7300/36908] Elapsed 77m 23s (remain 313m 52s) Loss: 0.0015(0.0058) Grad: 35603.2539  LR: 0.000018  \n","Epoch: [1][7400/36908] Elapsed 78m 27s (remain 312m 48s) Loss: 0.0001(0.0057) Grad: 156.1961  LR: 0.000018  \n","Epoch: [1][7500/36908] Elapsed 79m 31s (remain 311m 46s) Loss: 0.0025(0.0057) Grad: 22518.2109  LR: 0.000018  \n","Epoch: [1][7600/36908] Elapsed 80m 35s (remain 310m 45s) Loss: 0.0000(0.0056) Grad: 85.3097  LR: 0.000018  \n","Epoch: [1][7700/36908] Elapsed 81m 40s (remain 309m 43s) Loss: 0.0000(0.0056) Grad: 60.2379  LR: 0.000018  \n","Epoch: [1][7800/36908] Elapsed 82m 43s (remain 308m 40s) Loss: 0.0028(0.0055) Grad: 59821.3203  LR: 0.000018  \n","Epoch: [1][7900/36908] Elapsed 83m 47s (remain 307m 38s) Loss: 0.0001(0.0055) Grad: 1189.7655  LR: 0.000017  \n","Epoch: [1][8000/36908] Elapsed 84m 51s (remain 306m 36s) Loss: 0.0001(0.0054) Grad: 1856.5315  LR: 0.000017  \n","Epoch: [1][8100/36908] Elapsed 85m 55s (remain 305m 33s) Loss: 0.0066(0.0054) Grad: 89331.5469  LR: 0.000017  \n","Epoch: [1][8200/36908] Elapsed 86m 58s (remain 304m 27s) Loss: 0.0004(0.0053) Grad: 25608.0645  LR: 0.000017  \n","Epoch: [1][8300/36908] Elapsed 88m 3s (remain 303m 27s) Loss: 0.0009(0.0053) Grad: 20608.7383  LR: 0.000017  \n","Epoch: [1][8400/36908] Elapsed 89m 7s (remain 302m 25s) Loss: 0.0001(0.0052) Grad: 344.4623  LR: 0.000017  \n","Epoch: [1][8500/36908] Elapsed 90m 11s (remain 301m 22s) Loss: 0.0000(0.0052) Grad: 107.7876  LR: 0.000017  \n","Epoch: [1][8600/36908] Elapsed 91m 15s (remain 300m 18s) Loss: 0.0009(0.0051) Grad: 10215.0840  LR: 0.000017  \n","Epoch: [1][8700/36908] Elapsed 92m 18s (remain 299m 15s) Loss: 0.0009(0.0051) Grad: 88161.9453  LR: 0.000017  \n","Epoch: [1][8800/36908] Elapsed 93m 22s (remain 298m 12s) Loss: 0.0031(0.0051) Grad: 185886.7344  LR: 0.000017  \n","Epoch: [1][8900/36908] Elapsed 94m 26s (remain 297m 10s) Loss: 0.0003(0.0050) Grad: 6055.3638  LR: 0.000017  \n","Epoch: [1][9000/36908] Elapsed 95m 30s (remain 296m 8s) Loss: 0.0000(0.0050) Grad: 157.9335  LR: 0.000017  \n","Epoch: [1][9100/36908] Elapsed 96m 35s (remain 295m 7s) Loss: 0.0023(0.0050) Grad: 104748.9609  LR: 0.000017  \n","Epoch: [1][9200/36908] Elapsed 97m 39s (remain 294m 3s) Loss: 0.0001(0.0049) Grad: 8118.4463  LR: 0.000017  \n","Epoch: [1][9300/36908] Elapsed 98m 42s (remain 292m 59s) Loss: 0.0008(0.0049) Grad: 4904.7979  LR: 0.000017  \n","Epoch: [1][9400/36908] Elapsed 99m 46s (remain 291m 57s) Loss: 0.0021(0.0049) Grad: 28657.8711  LR: 0.000017  \n","Epoch: [1][9500/36908] Elapsed 100m 50s (remain 290m 53s) Loss: 0.0001(0.0048) Grad: 3991.2410  LR: 0.000017  \n","Epoch: [1][9600/36908] Elapsed 101m 53s (remain 289m 49s) Loss: 0.0078(0.0048) Grad: 13395.9717  LR: 0.000016  \n","Epoch: [1][9700/36908] Elapsed 102m 57s (remain 288m 44s) Loss: 0.0000(0.0047) Grad: 170.2579  LR: 0.000016  \n","Epoch: [1][9800/36908] Elapsed 104m 0s (remain 287m 39s) Loss: 0.0004(0.0047) Grad: 5372.7632  LR: 0.000016  \n","Epoch: [1][9900/36908] Elapsed 105m 4s (remain 286m 35s) Loss: 0.0031(0.0047) Grad: 62436.5195  LR: 0.000016  \n","Epoch: [1][10000/36908] Elapsed 106m 8s (remain 285m 32s) Loss: 0.0028(0.0047) Grad: 36007.9336  LR: 0.000016  \n","Epoch: [1][10100/36908] Elapsed 107m 12s (remain 284m 31s) Loss: 0.0038(0.0046) Grad: 18038.8594  LR: 0.000016  \n","Epoch: [1][10200/36908] Elapsed 108m 16s (remain 283m 29s) Loss: 0.0044(0.0046) Grad: 11562.5566  LR: 0.000016  \n","Epoch: [1][10300/36908] Elapsed 109m 21s (remain 282m 28s) Loss: 0.0000(0.0046) Grad: 205.6503  LR: 0.000016  \n","Epoch: [1][10400/36908] Elapsed 110m 25s (remain 281m 25s) Loss: 0.0020(0.0046) Grad: 5989.5601  LR: 0.000016  \n","Epoch: [1][10500/36908] Elapsed 111m 29s (remain 280m 22s) Loss: 0.0011(0.0045) Grad: 14143.1973  LR: 0.000016  \n","Epoch: [1][10600/36908] Elapsed 112m 34s (remain 279m 20s) Loss: 0.0070(0.0045) Grad: 17820.0586  LR: 0.000016  \n","Epoch: [1][10700/36908] Elapsed 113m 38s (remain 278m 17s) Loss: 0.0013(0.0045) Grad: 28630.1777  LR: 0.000016  \n","Epoch: [1][10800/36908] Elapsed 114m 41s (remain 277m 13s) Loss: 0.0000(0.0044) Grad: 62.5094  LR: 0.000016  \n","Epoch: [1][10900/36908] Elapsed 115m 45s (remain 276m 9s) Loss: 0.0015(0.0044) Grad: 4502.8242  LR: 0.000016  \n","Epoch: [1][11000/36908] Elapsed 116m 49s (remain 275m 6s) Loss: 0.0127(0.0044) Grad: 76347.7188  LR: 0.000016  \n","Epoch: [1][11100/36908] Elapsed 117m 52s (remain 274m 2s) Loss: 0.0000(0.0044) Grad: 41.6756  LR: 0.000016  \n","Epoch: [1][11200/36908] Elapsed 118m 56s (remain 272m 59s) Loss: 0.0021(0.0043) Grad: 9299.8613  LR: 0.000015  \n","Epoch: [1][11300/36908] Elapsed 120m 1s (remain 271m 57s) Loss: 0.0001(0.0043) Grad: 615.9151  LR: 0.000015  \n","Epoch: [1][11400/36908] Elapsed 121m 5s (remain 270m 54s) Loss: 0.0038(0.0043) Grad: 11117.5635  LR: 0.000015  \n","Epoch: [1][11500/36908] Elapsed 122m 9s (remain 269m 52s) Loss: 0.0001(0.0043) Grad: 68.4707  LR: 0.000015  \n","Epoch: [1][11600/36908] Elapsed 123m 13s (remain 268m 49s) Loss: 0.0000(0.0042) Grad: 106.6123  LR: 0.000015  \n","Epoch: [1][11700/36908] Elapsed 124m 17s (remain 267m 45s) Loss: 0.0000(0.0042) Grad: 29.2054  LR: 0.000015  \n","Epoch: [1][11800/36908] Elapsed 125m 21s (remain 266m 42s) Loss: 0.0041(0.0042) Grad: 10341.5098  LR: 0.000015  \n","Epoch: [1][11900/36908] Elapsed 126m 25s (remain 265m 39s) Loss: 0.0000(0.0042) Grad: 29.0993  LR: 0.000015  \n","Epoch: [1][12000/36908] Elapsed 127m 29s (remain 264m 36s) Loss: 0.0000(0.0041) Grad: 83.3664  LR: 0.000015  \n","Epoch: [1][12100/36908] Elapsed 128m 33s (remain 263m 33s) Loss: 0.0007(0.0041) Grad: 3605.5776  LR: 0.000015  \n","Epoch: [1][12200/36908] Elapsed 129m 37s (remain 262m 29s) Loss: 0.0000(0.0041) Grad: 70.7781  LR: 0.000015  \n","Epoch: [1][12300/36908] Elapsed 130m 41s (remain 261m 25s) Loss: 0.0000(0.0041) Grad: 23.3057  LR: 0.000015  \n","Epoch: [1][12400/36908] Elapsed 131m 44s (remain 260m 21s) Loss: 0.0004(0.0041) Grad: 2492.9028  LR: 0.000015  \n","Epoch: [1][12500/36908] Elapsed 132m 48s (remain 259m 17s) Loss: 0.0001(0.0040) Grad: 242.7851  LR: 0.000015  \n","Epoch: [1][12600/36908] Elapsed 133m 52s (remain 258m 14s) Loss: 0.0006(0.0040) Grad: 1585.2208  LR: 0.000015  \n","Epoch: [1][12700/36908] Elapsed 134m 57s (remain 257m 12s) Loss: 0.0001(0.0040) Grad: 371.0202  LR: 0.000015  \n","Epoch: [1][12800/36908] Elapsed 136m 1s (remain 256m 9s) Loss: 0.0004(0.0040) Grad: 2793.5823  LR: 0.000015  \n","Epoch: [1][12900/36908] Elapsed 137m 5s (remain 255m 5s) Loss: 0.0041(0.0039) Grad: 4827.8101  LR: 0.000014  \n","Epoch: [1][13000/36908] Elapsed 138m 8s (remain 254m 2s) Loss: 0.0010(0.0039) Grad: 3928.8281  LR: 0.000014  \n","Epoch: [1][13100/36908] Elapsed 139m 12s (remain 252m 58s) Loss: 0.0028(0.0039) Grad: 27854.9238  LR: 0.000014  \n","Epoch: [1][13200/36908] Elapsed 140m 16s (remain 251m 55s) Loss: 0.0001(0.0039) Grad: 718.4548  LR: 0.000014  \n","Epoch: [1][13300/36908] Elapsed 141m 20s (remain 250m 51s) Loss: 0.0044(0.0039) Grad: 15053.1670  LR: 0.000014  \n","Epoch: [1][13400/36908] Elapsed 142m 24s (remain 249m 47s) Loss: 0.0044(0.0038) Grad: 15028.6523  LR: 0.000014  \n","Epoch: [1][13500/36908] Elapsed 143m 27s (remain 248m 43s) Loss: 0.0018(0.0038) Grad: 5301.9067  LR: 0.000014  \n","Epoch: [1][13600/36908] Elapsed 144m 31s (remain 247m 39s) Loss: 0.0052(0.0038) Grad: 48176.0664  LR: 0.000014  \n","Epoch: [1][13700/36908] Elapsed 145m 35s (remain 246m 36s) Loss: 0.0000(0.0038) Grad: 136.7502  LR: 0.000014  \n","Epoch: [1][13800/36908] Elapsed 146m 39s (remain 245m 32s) Loss: 0.0057(0.0038) Grad: 15390.0654  LR: 0.000014  \n","Epoch: [1][13900/36908] Elapsed 147m 42s (remain 244m 28s) Loss: 0.0000(0.0038) Grad: 240.3247  LR: 0.000014  \n","Epoch: [1][14000/36908] Elapsed 148m 46s (remain 243m 24s) Loss: 0.0007(0.0037) Grad: 18380.9766  LR: 0.000014  \n","Epoch: [1][14100/36908] Elapsed 149m 50s (remain 242m 21s) Loss: 0.0000(0.0037) Grad: 59.3543  LR: 0.000014  \n","Epoch: [1][14200/36908] Elapsed 150m 54s (remain 241m 18s) Loss: 0.0000(0.0037) Grad: 424.8524  LR: 0.000014  \n","Epoch: [1][14300/36908] Elapsed 151m 58s (remain 240m 15s) Loss: 0.0056(0.0037) Grad: 48556.8438  LR: 0.000014  \n","Epoch: [1][14400/36908] Elapsed 153m 3s (remain 239m 12s) Loss: 0.0000(0.0037) Grad: 8.3165  LR: 0.000014  \n","Epoch: [1][14500/36908] Elapsed 154m 7s (remain 238m 9s) Loss: 0.0001(0.0037) Grad: 3885.6311  LR: 0.000013  \n","Epoch: [1][14600/36908] Elapsed 155m 11s (remain 237m 5s) Loss: 0.0000(0.0036) Grad: 56.7748  LR: 0.000013  \n","Epoch: [1][14700/36908] Elapsed 156m 14s (remain 236m 1s) Loss: 0.0000(0.0036) Grad: 16.6482  LR: 0.000013  \n","Epoch: [1][14800/36908] Elapsed 157m 18s (remain 234m 56s) Loss: 0.0000(0.0036) Grad: 65.5978  LR: 0.000013  \n","Epoch: [1][14900/36908] Elapsed 158m 21s (remain 233m 53s) Loss: 0.0000(0.0036) Grad: 123.6768  LR: 0.000013  \n","Epoch: [1][15000/36908] Elapsed 159m 25s (remain 232m 49s) Loss: 0.0004(0.0036) Grad: 3735.3726  LR: 0.000013  \n","Epoch: [1][15100/36908] Elapsed 160m 29s (remain 231m 45s) Loss: 0.0060(0.0036) Grad: 27483.9922  LR: 0.000013  \n","Epoch: [1][15200/36908] Elapsed 161m 33s (remain 230m 42s) Loss: 0.0000(0.0035) Grad: 602.5533  LR: 0.000013  \n","Epoch: [1][15300/36908] Elapsed 162m 37s (remain 229m 38s) Loss: 0.0000(0.0035) Grad: 151.8111  LR: 0.000013  \n","Epoch: [1][15400/36908] Elapsed 163m 41s (remain 228m 34s) Loss: 0.0001(0.0035) Grad: 1120.2373  LR: 0.000013  \n","Epoch: [1][15500/36908] Elapsed 164m 45s (remain 227m 31s) Loss: 0.0000(0.0035) Grad: 184.8700  LR: 0.000013  \n","Epoch: [1][15600/36908] Elapsed 165m 49s (remain 226m 28s) Loss: 0.0015(0.0035) Grad: 14935.7246  LR: 0.000013  \n","Epoch: [1][15700/36908] Elapsed 166m 54s (remain 225m 25s) Loss: 0.0000(0.0035) Grad: 1632.6527  LR: 0.000013  \n","Epoch: [1][15800/36908] Elapsed 167m 59s (remain 224m 23s) Loss: 0.0002(0.0035) Grad: 2874.9521  LR: 0.000013  \n","Epoch: [1][15900/36908] Elapsed 169m 3s (remain 223m 20s) Loss: 0.0001(0.0034) Grad: 280.0350  LR: 0.000013  \n","Epoch: [1][16000/36908] Elapsed 170m 7s (remain 222m 17s) Loss: 0.0000(0.0034) Grad: 35.7306  LR: 0.000013  \n","Epoch: [1][16100/36908] Elapsed 171m 11s (remain 221m 13s) Loss: 0.0007(0.0034) Grad: 8000.4658  LR: 0.000013  \n","Epoch: [1][16200/36908] Elapsed 172m 14s (remain 220m 9s) Loss: 0.0032(0.0034) Grad: 15265.6348  LR: 0.000012  \n","Epoch: [1][16300/36908] Elapsed 173m 19s (remain 219m 7s) Loss: 0.0000(0.0034) Grad: 194.4193  LR: 0.000012  \n","Epoch: [1][16400/36908] Elapsed 174m 24s (remain 218m 3s) Loss: 0.0000(0.0034) Grad: 31.9683  LR: 0.000012  \n","Epoch: [1][16500/36908] Elapsed 175m 27s (remain 216m 59s) Loss: 0.0000(0.0034) Grad: 205.3503  LR: 0.000012  \n","Epoch: [1][16600/36908] Elapsed 176m 31s (remain 215m 55s) Loss: 0.0047(0.0033) Grad: 34599.9141  LR: 0.000012  \n","Epoch: [1][16700/36908] Elapsed 177m 34s (remain 214m 51s) Loss: 0.0003(0.0033) Grad: 3730.5640  LR: 0.000012  \n","Epoch: [1][16800/36908] Elapsed 178m 38s (remain 213m 47s) Loss: 0.0155(0.0033) Grad: 56952.0742  LR: 0.000012  \n","Epoch: [1][16900/36908] Elapsed 179m 42s (remain 212m 44s) Loss: 0.0000(0.0033) Grad: 185.2632  LR: 0.000012  \n","Epoch: [1][17000/36908] Elapsed 180m 47s (remain 211m 41s) Loss: 0.0001(0.0033) Grad: 619.6157  LR: 0.000012  \n","Epoch: [1][17100/36908] Elapsed 181m 52s (remain 210m 39s) Loss: 0.0002(0.0033) Grad: 1008.5941  LR: 0.000012  \n","Epoch: [1][17200/36908] Elapsed 182m 57s (remain 209m 37s) Loss: 0.0008(0.0033) Grad: 4497.6030  LR: 0.000012  \n","Epoch: [1][17300/36908] Elapsed 184m 2s (remain 208m 34s) Loss: 0.0000(0.0033) Grad: 24.8419  LR: 0.000012  \n","Epoch: [1][17400/36908] Elapsed 185m 7s (remain 207m 31s) Loss: 0.0003(0.0033) Grad: 4236.2217  LR: 0.000012  \n","Epoch: [1][17500/36908] Elapsed 186m 11s (remain 206m 28s) Loss: 0.0001(0.0032) Grad: 1143.7659  LR: 0.000012  \n","Epoch: [1][17600/36908] Elapsed 187m 15s (remain 205m 24s) Loss: 0.0000(0.0032) Grad: 18.5423  LR: 0.000012  \n","Epoch: [1][17700/36908] Elapsed 188m 20s (remain 204m 21s) Loss: 0.0000(0.0032) Grad: 1010.6535  LR: 0.000012  \n","Epoch: [1][17800/36908] Elapsed 189m 24s (remain 203m 18s) Loss: 0.0000(0.0032) Grad: 88.3657  LR: 0.000012  \n","Epoch: [1][17900/36908] Elapsed 190m 29s (remain 202m 15s) Loss: 0.0000(0.0032) Grad: 394.5270  LR: 0.000011  \n","Epoch: [1][18000/36908] Elapsed 191m 34s (remain 201m 12s) Loss: 0.0000(0.0032) Grad: 117.5030  LR: 0.000011  \n","Epoch: [1][18100/36908] Elapsed 192m 38s (remain 200m 9s) Loss: 0.0011(0.0032) Grad: 20690.9668  LR: 0.000011  \n","Epoch: [1][18200/36908] Elapsed 193m 42s (remain 199m 5s) Loss: 0.0054(0.0032) Grad: 35611.9883  LR: 0.000011  \n","Epoch: [1][18300/36908] Elapsed 194m 46s (remain 198m 2s) Loss: 0.0002(0.0032) Grad: 4278.2329  LR: 0.000011  \n","Epoch: [1][18400/36908] Elapsed 195m 51s (remain 196m 59s) Loss: 0.0000(0.0031) Grad: 402.4331  LR: 0.000011  \n","Epoch: [1][18500/36908] Elapsed 196m 56s (remain 195m 56s) Loss: 0.0007(0.0031) Grad: 26285.3613  LR: 0.000011  \n","Epoch: [1][18600/36908] Elapsed 198m 0s (remain 194m 53s) Loss: 0.0020(0.0031) Grad: 6038.3926  LR: 0.000011  \n","Epoch: [1][18700/36908] Elapsed 199m 5s (remain 193m 49s) Loss: 0.0002(0.0031) Grad: 6908.3843  LR: 0.000011  \n","Epoch: [1][18800/36908] Elapsed 200m 9s (remain 192m 46s) Loss: 0.0002(0.0031) Grad: 5266.6602  LR: 0.000011  \n","Epoch: [1][18900/36908] Elapsed 201m 13s (remain 191m 42s) Loss: 0.0005(0.0031) Grad: 16814.2793  LR: 0.000011  \n","Epoch: [1][19000/36908] Elapsed 202m 17s (remain 190m 38s) Loss: 0.0006(0.0031) Grad: 20198.3262  LR: 0.000011  \n","Epoch: [1][19100/36908] Elapsed 203m 22s (remain 189m 35s) Loss: 0.0011(0.0031) Grad: 13773.8652  LR: 0.000011  \n","Epoch: [1][19200/36908] Elapsed 204m 26s (remain 188m 31s) Loss: 0.0000(0.0031) Grad: 224.8721  LR: 0.000011  \n","Epoch: [1][19300/36908] Elapsed 205m 29s (remain 187m 27s) Loss: 0.0001(0.0031) Grad: 1263.3243  LR: 0.000011  \n","Epoch: [1][19400/36908] Elapsed 206m 33s (remain 186m 23s) Loss: 0.0000(0.0031) Grad: 53.5081  LR: 0.000011  \n","Epoch: [1][19500/36908] Elapsed 207m 37s (remain 185m 19s) Loss: 0.0000(0.0030) Grad: 643.5683  LR: 0.000010  \n","Epoch: [1][19600/36908] Elapsed 208m 41s (remain 184m 16s) Loss: 0.0085(0.0030) Grad: 39498.5664  LR: 0.000010  \n","Epoch: [1][19700/36908] Elapsed 209m 47s (remain 183m 13s) Loss: 0.0037(0.0030) Grad: 70186.9453  LR: 0.000010  \n","Epoch: [1][19800/36908] Elapsed 210m 51s (remain 182m 10s) Loss: 0.0007(0.0030) Grad: 13788.4551  LR: 0.000010  \n","Epoch: [1][19900/36908] Elapsed 211m 56s (remain 181m 6s) Loss: 0.0002(0.0030) Grad: 4559.6567  LR: 0.000010  \n","Epoch: [1][20000/36908] Elapsed 213m 0s (remain 180m 3s) Loss: 0.0009(0.0030) Grad: 19862.3945  LR: 0.000010  \n","Epoch: [1][20100/36908] Elapsed 214m 3s (remain 178m 58s) Loss: 0.0000(0.0030) Grad: 274.1353  LR: 0.000010  \n","Epoch: [1][20200/36908] Elapsed 215m 7s (remain 177m 54s) Loss: 0.0012(0.0030) Grad: 26240.1055  LR: 0.000010  \n","Epoch: [1][20300/36908] Elapsed 216m 11s (remain 176m 51s) Loss: 0.0000(0.0030) Grad: 64.9759  LR: 0.000010  \n","Epoch: [1][20400/36908] Elapsed 217m 15s (remain 175m 47s) Loss: 0.0000(0.0030) Grad: 162.4902  LR: 0.000010  \n","Epoch: [1][20500/36908] Elapsed 218m 19s (remain 174m 43s) Loss: 0.0000(0.0030) Grad: 338.2412  LR: 0.000010  \n","Epoch: [1][20600/36908] Elapsed 219m 23s (remain 173m 39s) Loss: 0.0035(0.0029) Grad: 51873.7969  LR: 0.000010  \n","Epoch: [1][20700/36908] Elapsed 220m 28s (remain 172m 36s) Loss: 0.0039(0.0029) Grad: 142557.6875  LR: 0.000010  \n","Epoch: [1][20800/36908] Elapsed 221m 32s (remain 171m 33s) Loss: 0.0039(0.0029) Grad: 33979.0273  LR: 0.000010  \n","Epoch: [1][20900/36908] Elapsed 222m 37s (remain 170m 29s) Loss: 0.0013(0.0029) Grad: 8485.2656  LR: 0.000010  \n","Epoch: [1][21000/36908] Elapsed 223m 42s (remain 169m 26s) Loss: 0.0008(0.0029) Grad: 10169.6729  LR: 0.000010  \n","Epoch: [1][21100/36908] Elapsed 224m 46s (remain 168m 22s) Loss: 0.0000(0.0029) Grad: 21.3068  LR: 0.000010  \n","Epoch: [1][21200/36908] Elapsed 225m 50s (remain 167m 19s) Loss: 0.0000(0.0029) Grad: 1505.4225  LR: 0.000009  \n","Epoch: [1][21300/36908] Elapsed 226m 55s (remain 166m 15s) Loss: 0.0000(0.0029) Grad: 208.1134  LR: 0.000009  \n","Epoch: [1][21400/36908] Elapsed 227m 59s (remain 165m 12s) Loss: 0.0032(0.0029) Grad: 6042.6802  LR: 0.000009  \n","Epoch: [1][21500/36908] Elapsed 229m 4s (remain 164m 8s) Loss: 0.0000(0.0029) Grad: 259.0697  LR: 0.000009  \n","Epoch: [1][21600/36908] Elapsed 230m 8s (remain 163m 4s) Loss: 0.0000(0.0029) Grad: 318.4525  LR: 0.000009  \n","Epoch: [1][21700/36908] Elapsed 231m 12s (remain 162m 0s) Loss: 0.0061(0.0029) Grad: 199186.8438  LR: 0.000009  \n","Epoch: [1][21800/36908] Elapsed 232m 16s (remain 160m 57s) Loss: 0.0023(0.0028) Grad: 16675.8867  LR: 0.000009  \n","Epoch: [1][21900/36908] Elapsed 233m 22s (remain 159m 54s) Loss: 0.0003(0.0028) Grad: 2702.5085  LR: 0.000009  \n","Epoch: [1][22000/36908] Elapsed 234m 26s (remain 158m 50s) Loss: 0.0155(0.0028) Grad: 124283.8516  LR: 0.000009  \n","Epoch: [1][22100/36908] Elapsed 235m 29s (remain 157m 46s) Loss: 0.0006(0.0028) Grad: 72108.5234  LR: 0.000009  \n","Epoch: [1][22200/36908] Elapsed 236m 32s (remain 156m 41s) Loss: 0.0032(0.0028) Grad: 73534.1562  LR: 0.000009  \n","Epoch: [1][22300/36908] Elapsed 237m 35s (remain 155m 37s) Loss: 0.0000(0.0028) Grad: 131.5674  LR: 0.000009  \n","Epoch: [1][22400/36908] Elapsed 238m 40s (remain 154m 33s) Loss: 0.0000(0.0028) Grad: 27.3716  LR: 0.000009  \n","Epoch: [1][22500/36908] Elapsed 239m 45s (remain 153m 30s) Loss: 0.0018(0.0028) Grad: 56635.9648  LR: 0.000009  \n","Epoch: [1][22600/36908] Elapsed 240m 48s (remain 152m 26s) Loss: 0.0031(0.0028) Grad: 66451.3828  LR: 0.000009  \n","Epoch: [1][22700/36908] Elapsed 241m 52s (remain 151m 22s) Loss: 0.0000(0.0028) Grad: 53.2548  LR: 0.000009  \n","Epoch: [1][22800/36908] Elapsed 242m 55s (remain 150m 17s) Loss: 0.0000(0.0028) Grad: 2758.5930  LR: 0.000008  \n","Epoch: [1][22900/36908] Elapsed 243m 59s (remain 149m 13s) Loss: 0.0022(0.0028) Grad: 53280.4414  LR: 0.000008  \n","Epoch: [1][23000/36908] Elapsed 245m 3s (remain 148m 9s) Loss: 0.0002(0.0028) Grad: 28759.5547  LR: 0.000008  \n","Epoch: [1][23100/36908] Elapsed 246m 6s (remain 147m 5s) Loss: 0.0000(0.0028) Grad: 255.7152  LR: 0.000008  \n","Epoch: [1][23200/36908] Elapsed 247m 10s (remain 146m 1s) Loss: 0.0000(0.0028) Grad: 50.7812  LR: 0.000008  \n","Epoch: [1][23300/36908] Elapsed 248m 13s (remain 144m 57s) Loss: 0.0025(0.0028) Grad: 159741.8281  LR: 0.000008  \n","Epoch: [1][23400/36908] Elapsed 249m 16s (remain 143m 52s) Loss: 0.0002(0.0027) Grad: 8127.3501  LR: 0.000008  \n","Epoch: [1][23500/36908] Elapsed 250m 19s (remain 142m 48s) Loss: 0.0000(0.0027) Grad: 27.1655  LR: 0.000008  \n","Epoch: [1][23600/36908] Elapsed 251m 22s (remain 141m 44s) Loss: 0.0000(0.0027) Grad: 543.6478  LR: 0.000008  \n","Epoch: [1][23700/36908] Elapsed 252m 25s (remain 140m 39s) Loss: 0.0003(0.0027) Grad: 43150.2734  LR: 0.000008  \n","Epoch: [1][23800/36908] Elapsed 253m 29s (remain 139m 35s) Loss: 0.0000(0.0027) Grad: 34.6927  LR: 0.000008  \n","Epoch: [1][23900/36908] Elapsed 254m 32s (remain 138m 31s) Loss: 0.0000(0.0027) Grad: 71.6891  LR: 0.000008  \n","Epoch: [1][24000/36908] Elapsed 255m 35s (remain 137m 27s) Loss: 0.0000(0.0027) Grad: 212.6402  LR: 0.000008  \n","Epoch: [1][24100/36908] Elapsed 256m 39s (remain 136m 23s) Loss: 0.0002(0.0027) Grad: 7521.7705  LR: 0.000008  \n","Epoch: [1][24200/36908] Elapsed 257m 43s (remain 135m 19s) Loss: 0.0000(0.0027) Grad: 887.3365  LR: 0.000008  \n","Epoch: [1][24300/36908] Elapsed 258m 47s (remain 134m 15s) Loss: 0.0013(0.0027) Grad: 31047.5605  LR: 0.000008  \n","Epoch: [1][24400/36908] Elapsed 259m 51s (remain 133m 11s) Loss: 0.0143(0.0027) Grad: 89251.2969  LR: 0.000008  \n","Epoch: [1][24500/36908] Elapsed 260m 55s (remain 132m 7s) Loss: 0.0000(0.0027) Grad: 304.5672  LR: 0.000007  \n","Epoch: [1][24600/36908] Elapsed 261m 59s (remain 131m 3s) Loss: 0.0000(0.0027) Grad: 3190.9438  LR: 0.000007  \n","Epoch: [1][24700/36908] Elapsed 263m 3s (remain 129m 59s) Loss: 0.0017(0.0027) Grad: 13844.6768  LR: 0.000007  \n","Epoch: [1][24800/36908] Elapsed 264m 6s (remain 128m 55s) Loss: 0.0000(0.0027) Grad: 32.3229  LR: 0.000007  \n","Epoch: [1][24900/36908] Elapsed 265m 9s (remain 127m 51s) Loss: 0.0009(0.0026) Grad: 14757.0117  LR: 0.000007  \n","Epoch: [1][25000/36908] Elapsed 266m 11s (remain 126m 46s) Loss: 0.0011(0.0026) Grad: 48749.9688  LR: 0.000007  \n","Epoch: [1][25100/36908] Elapsed 267m 15s (remain 125m 42s) Loss: 0.0000(0.0026) Grad: 86.7782  LR: 0.000007  \n","Epoch: [1][25200/36908] Elapsed 268m 18s (remain 124m 38s) Loss: 0.0000(0.0026) Grad: 60.0985  LR: 0.000007  \n","Epoch: [1][25300/36908] Elapsed 269m 21s (remain 123m 34s) Loss: 0.0008(0.0026) Grad: 18498.2266  LR: 0.000007  \n","Epoch: [1][25400/36908] Elapsed 270m 24s (remain 122m 29s) Loss: 0.0000(0.0026) Grad: 65.3100  LR: 0.000007  \n","Epoch: [1][25500/36908] Elapsed 271m 27s (remain 121m 25s) Loss: 0.0001(0.0026) Grad: 1862.7900  LR: 0.000007  \n","Epoch: [1][25600/36908] Elapsed 272m 31s (remain 120m 21s) Loss: 0.0038(0.0026) Grad: 46823.7930  LR: 0.000007  \n","Epoch: [1][25700/36908] Elapsed 273m 33s (remain 119m 17s) Loss: 0.0002(0.0026) Grad: 11879.5820  LR: 0.000007  \n","Epoch: [1][25800/36908] Elapsed 274m 36s (remain 118m 12s) Loss: 0.0028(0.0026) Grad: 29112.1777  LR: 0.000007  \n","Epoch: [1][25900/36908] Elapsed 275m 39s (remain 117m 8s) Loss: 0.0002(0.0026) Grad: 6304.6870  LR: 0.000007  \n","Epoch: [1][26000/36908] Elapsed 276m 41s (remain 116m 4s) Loss: 0.0003(0.0026) Grad: 48095.2773  LR: 0.000007  \n","Epoch: [1][26100/36908] Elapsed 277m 45s (remain 115m 0s) Loss: 0.0000(0.0026) Grad: 74.8795  LR: 0.000007  \n","Epoch: [1][26200/36908] Elapsed 278m 48s (remain 113m 56s) Loss: 0.0000(0.0026) Grad: 162.2912  LR: 0.000006  \n","Epoch: [1][26300/36908] Elapsed 279m 52s (remain 112m 52s) Loss: 0.0008(0.0026) Grad: 20033.0430  LR: 0.000006  \n","Epoch: [1][26400/36908] Elapsed 280m 55s (remain 111m 48s) Loss: 0.0000(0.0026) Grad: 8.2422  LR: 0.000006  \n","Epoch: [1][26500/36908] Elapsed 281m 59s (remain 110m 44s) Loss: 0.0007(0.0026) Grad: 7104.2891  LR: 0.000006  \n","Epoch: [1][26600/36908] Elapsed 283m 4s (remain 109m 41s) Loss: 0.0000(0.0026) Grad: 74.7060  LR: 0.000006  \n","Epoch: [1][26700/36908] Elapsed 284m 8s (remain 108m 37s) Loss: 0.0000(0.0026) Grad: 23.2627  LR: 0.000006  \n","Epoch: [1][26800/36908] Elapsed 285m 12s (remain 107m 33s) Loss: 0.0003(0.0025) Grad: 14458.8730  LR: 0.000006  \n","Epoch: [1][26900/36908] Elapsed 286m 17s (remain 106m 29s) Loss: 0.0000(0.0025) Grad: 11.9823  LR: 0.000006  \n","Epoch: [1][27000/36908] Elapsed 287m 21s (remain 105m 26s) Loss: 0.0010(0.0025) Grad: 13101.0879  LR: 0.000006  \n","Epoch: [1][27100/36908] Elapsed 288m 25s (remain 104m 22s) Loss: 0.0001(0.0025) Grad: 6665.5410  LR: 0.000006  \n","Epoch: [1][27200/36908] Elapsed 289m 29s (remain 103m 18s) Loss: 0.0020(0.0025) Grad: 36691.2031  LR: 0.000006  \n","Epoch: [1][27300/36908] Elapsed 290m 34s (remain 102m 14s) Loss: 0.0003(0.0025) Grad: 10018.1885  LR: 0.000006  \n","Epoch: [1][27400/36908] Elapsed 291m 38s (remain 101m 11s) Loss: 0.0000(0.0025) Grad: 184.4059  LR: 0.000006  \n","Epoch: [1][27500/36908] Elapsed 292m 42s (remain 100m 7s) Loss: 0.0053(0.0025) Grad: 94513.6094  LR: 0.000006  \n","Epoch: [1][27600/36908] Elapsed 293m 46s (remain 99m 3s) Loss: 0.0006(0.0025) Grad: 5380.5547  LR: 0.000006  \n","Epoch: [1][27700/36908] Elapsed 294m 49s (remain 97m 59s) Loss: 0.0016(0.0025) Grad: 48316.3672  LR: 0.000006  \n","Epoch: [1][27800/36908] Elapsed 295m 53s (remain 96m 55s) Loss: 0.0014(0.0025) Grad: 71672.0859  LR: 0.000005  \n","Epoch: [1][27900/36908] Elapsed 296m 56s (remain 95m 51s) Loss: 0.0000(0.0025) Grad: 43.9139  LR: 0.000005  \n","Epoch: [1][28000/36908] Elapsed 298m 0s (remain 94m 47s) Loss: 0.0000(0.0025) Grad: 368.1313  LR: 0.000005  \n","Epoch: [1][28100/36908] Elapsed 299m 4s (remain 93m 43s) Loss: 0.0000(0.0025) Grad: 1078.4513  LR: 0.000005  \n","Epoch: [1][28200/36908] Elapsed 300m 7s (remain 92m 39s) Loss: 0.0064(0.0025) Grad: 64212.3359  LR: 0.000005  \n","Epoch: [1][28300/36908] Elapsed 301m 10s (remain 91m 35s) Loss: 0.0006(0.0025) Grad: 10948.4209  LR: 0.000005  \n","Epoch: [1][28400/36908] Elapsed 302m 13s (remain 90m 31s) Loss: 0.0014(0.0025) Grad: 40405.3867  LR: 0.000005  \n","Epoch: [1][28500/36908] Elapsed 303m 16s (remain 89m 27s) Loss: 0.0000(0.0025) Grad: 18.7396  LR: 0.000005  \n","Epoch: [1][28600/36908] Elapsed 304m 19s (remain 88m 23s) Loss: 0.0020(0.0025) Grad: 36659.7188  LR: 0.000005  \n","Epoch: [1][28700/36908] Elapsed 305m 22s (remain 87m 19s) Loss: 0.0000(0.0025) Grad: 48.7328  LR: 0.000005  \n","Epoch: [1][28800/36908] Elapsed 306m 26s (remain 86m 15s) Loss: 0.0000(0.0024) Grad: 90.5337  LR: 0.000005  \n","Epoch: [1][28900/36908] Elapsed 307m 29s (remain 85m 11s) Loss: 0.0013(0.0024) Grad: 46330.0898  LR: 0.000005  \n","Epoch: [1][29000/36908] Elapsed 308m 32s (remain 84m 7s) Loss: 0.0006(0.0024) Grad: 13211.4746  LR: 0.000005  \n","Epoch: [1][29100/36908] Elapsed 309m 36s (remain 83m 3s) Loss: 0.0000(0.0024) Grad: 51.8073  LR: 0.000005  \n","Epoch: [1][29200/36908] Elapsed 310m 39s (remain 81m 59s) Loss: 0.0015(0.0024) Grad: 67475.3906  LR: 0.000005  \n","Epoch: [1][29300/36908] Elapsed 311m 42s (remain 80m 55s) Loss: 0.0000(0.0024) Grad: 140.0918  LR: 0.000005  \n","Epoch: [1][29400/36908] Elapsed 312m 46s (remain 79m 51s) Loss: 0.0004(0.0024) Grad: 12779.4600  LR: 0.000005  \n","Epoch: [1][29500/36908] Elapsed 313m 49s (remain 78m 47s) Loss: 0.0000(0.0024) Grad: 123.7732  LR: 0.000004  \n","Epoch: [1][29600/36908] Elapsed 314m 52s (remain 77m 43s) Loss: 0.0000(0.0024) Grad: 33.1939  LR: 0.000004  \n","Epoch: [1][29700/36908] Elapsed 315m 56s (remain 76m 39s) Loss: 0.0003(0.0024) Grad: 6676.0962  LR: 0.000004  \n","Epoch: [1][29800/36908] Elapsed 317m 0s (remain 75m 35s) Loss: 0.0000(0.0024) Grad: 42.0485  LR: 0.000004  \n","Epoch: [1][29900/36908] Elapsed 318m 4s (remain 74m 32s) Loss: 0.0000(0.0024) Grad: 239.0232  LR: 0.000004  \n","Epoch: [1][30000/36908] Elapsed 319m 8s (remain 73m 28s) Loss: 0.0011(0.0024) Grad: 13232.9277  LR: 0.000004  \n","Epoch: [1][30100/36908] Elapsed 320m 11s (remain 72m 24s) Loss: 0.0034(0.0024) Grad: 177380.6719  LR: 0.000004  \n","Epoch: [1][30200/36908] Elapsed 321m 15s (remain 71m 20s) Loss: 0.0052(0.0024) Grad: 37461.0273  LR: 0.000004  \n","Epoch: [1][30300/36908] Elapsed 322m 19s (remain 70m 16s) Loss: 0.0003(0.0024) Grad: 6374.5815  LR: 0.000004  \n","Epoch: [1][30400/36908] Elapsed 323m 23s (remain 69m 13s) Loss: 0.0001(0.0024) Grad: 4599.7715  LR: 0.000004  \n","Epoch: [1][30500/36908] Elapsed 324m 27s (remain 68m 9s) Loss: 0.0000(0.0024) Grad: 737.4974  LR: 0.000004  \n","Epoch: [1][30600/36908] Elapsed 325m 30s (remain 67m 5s) Loss: 0.0005(0.0024) Grad: 10072.5596  LR: 0.000004  \n","Epoch: [1][30700/36908] Elapsed 326m 34s (remain 66m 1s) Loss: 0.0000(0.0024) Grad: 6.3531  LR: 0.000004  \n","Epoch: [1][30800/36908] Elapsed 327m 37s (remain 64m 57s) Loss: 0.0000(0.0024) Grad: 2293.3091  LR: 0.000004  \n","Epoch: [1][30900/36908] Elapsed 328m 41s (remain 63m 53s) Loss: 0.0000(0.0024) Grad: 8.3814  LR: 0.000004  \n","Epoch: [1][31000/36908] Elapsed 329m 45s (remain 62m 49s) Loss: 0.0046(0.0024) Grad: 30775.7168  LR: 0.000004  \n","Epoch: [1][31100/36908] Elapsed 330m 49s (remain 61m 46s) Loss: 0.0000(0.0024) Grad: 306.3568  LR: 0.000003  \n","Epoch: [1][31200/36908] Elapsed 331m 53s (remain 60m 42s) Loss: 0.0105(0.0024) Grad: 57021.3867  LR: 0.000003  \n","Epoch: [1][31300/36908] Elapsed 332m 56s (remain 59m 38s) Loss: 0.0000(0.0023) Grad: 41.3228  LR: 0.000003  \n","Epoch: [1][31400/36908] Elapsed 334m 0s (remain 58m 34s) Loss: 0.0008(0.0023) Grad: 19661.5508  LR: 0.000003  \n","Epoch: [1][31500/36908] Elapsed 335m 3s (remain 57m 30s) Loss: 0.0000(0.0023) Grad: 210.7407  LR: 0.000003  \n","Epoch: [1][31600/36908] Elapsed 336m 7s (remain 56m 26s) Loss: 0.0000(0.0023) Grad: 18.8608  LR: 0.000003  \n","Epoch: [1][31700/36908] Elapsed 337m 10s (remain 55m 22s) Loss: 0.0000(0.0023) Grad: 17.9487  LR: 0.000003  \n","Epoch: [1][31800/36908] Elapsed 338m 14s (remain 54m 19s) Loss: 0.0000(0.0023) Grad: 24.7107  LR: 0.000003  \n","Epoch: [1][31900/36908] Elapsed 339m 17s (remain 53m 15s) Loss: 0.0000(0.0023) Grad: 40.0765  LR: 0.000003  \n","Epoch: [1][32000/36908] Elapsed 340m 21s (remain 52m 11s) Loss: 0.0001(0.0023) Grad: 3468.3220  LR: 0.000003  \n","Epoch: [1][32100/36908] Elapsed 341m 25s (remain 51m 7s) Loss: 0.0087(0.0023) Grad: 321305.3125  LR: 0.000003  \n","Epoch: [1][32200/36908] Elapsed 342m 31s (remain 50m 4s) Loss: 0.0000(0.0023) Grad: 11.6564  LR: 0.000003  \n","Epoch: [1][32300/36908] Elapsed 343m 34s (remain 49m 0s) Loss: 0.0002(0.0023) Grad: 14091.9023  LR: 0.000003  \n","Epoch: [1][32400/36908] Elapsed 344m 38s (remain 47m 56s) Loss: 0.0000(0.0023) Grad: 119.6759  LR: 0.000003  \n","Epoch: [1][32500/36908] Elapsed 345m 42s (remain 46m 52s) Loss: 0.0044(0.0023) Grad: 234965.8906  LR: 0.000003  \n","Epoch: [1][32600/36908] Elapsed 346m 45s (remain 45m 48s) Loss: 0.0000(0.0023) Grad: 1678.9894  LR: 0.000003  \n","Epoch: [1][32700/36908] Elapsed 347m 48s (remain 44m 44s) Loss: 0.0025(0.0023) Grad: 57734.8594  LR: 0.000003  \n","Epoch: [1][32800/36908] Elapsed 348m 51s (remain 43m 40s) Loss: 0.0000(0.0023) Grad: 37.0502  LR: 0.000002  \n","Epoch: [1][32900/36908] Elapsed 349m 55s (remain 42m 36s) Loss: 0.0001(0.0023) Grad: 4730.3638  LR: 0.000002  \n","Epoch: [1][33000/36908] Elapsed 350m 57s (remain 41m 33s) Loss: 0.0000(0.0023) Grad: 22.2333  LR: 0.000002  \n","Epoch: [1][33100/36908] Elapsed 352m 0s (remain 40m 29s) Loss: 0.0045(0.0023) Grad: 144231.4062  LR: 0.000002  \n","Epoch: [1][33200/36908] Elapsed 353m 4s (remain 39m 25s) Loss: 0.0000(0.0023) Grad: 1121.5310  LR: 0.000002  \n","Epoch: [1][33300/36908] Elapsed 354m 8s (remain 38m 21s) Loss: 0.0000(0.0023) Grad: 1189.4246  LR: 0.000002  \n","Epoch: [1][33400/36908] Elapsed 355m 11s (remain 37m 17s) Loss: 0.0001(0.0023) Grad: 5719.0981  LR: 0.000002  \n","Epoch: [1][33500/36908] Elapsed 356m 14s (remain 36m 13s) Loss: 0.0000(0.0023) Grad: 574.0679  LR: 0.000002  \n","Epoch: [1][33600/36908] Elapsed 357m 18s (remain 35m 9s) Loss: 0.0000(0.0023) Grad: 168.0289  LR: 0.000002  \n","Epoch: [1][33700/36908] Elapsed 358m 21s (remain 34m 6s) Loss: 0.0000(0.0023) Grad: 160.7226  LR: 0.000002  \n","Epoch: [1][33800/36908] Elapsed 359m 24s (remain 33m 2s) Loss: 0.0045(0.0022) Grad: 136639.1094  LR: 0.000002  \n","Epoch: [1][33900/36908] Elapsed 360m 27s (remain 31m 58s) Loss: 0.0031(0.0022) Grad: 93077.3438  LR: 0.000002  \n","Epoch: [1][34000/36908] Elapsed 361m 31s (remain 30m 54s) Loss: 0.0000(0.0022) Grad: 21.5107  LR: 0.000002  \n","Epoch: [1][34100/36908] Elapsed 362m 34s (remain 29m 50s) Loss: 0.0004(0.0022) Grad: 27782.2715  LR: 0.000002  \n","Epoch: [1][34200/36908] Elapsed 363m 38s (remain 28m 46s) Loss: 0.0000(0.0022) Grad: 10812.2314  LR: 0.000002  \n","Epoch: [1][34300/36908] Elapsed 364m 41s (remain 27m 43s) Loss: 0.0000(0.0022) Grad: 2248.5405  LR: 0.000002  \n","Epoch: [1][34400/36908] Elapsed 365m 45s (remain 26m 39s) Loss: 0.0000(0.0022) Grad: 43.3290  LR: 0.000002  \n","Epoch: [1][34500/36908] Elapsed 366m 48s (remain 25m 35s) Loss: 0.0000(0.0022) Grad: 2611.4341  LR: 0.000001  \n","Epoch: [1][34600/36908] Elapsed 367m 52s (remain 24m 31s) Loss: 0.0000(0.0022) Grad: 2348.8286  LR: 0.000001  \n","Epoch: [1][34700/36908] Elapsed 368m 54s (remain 23m 27s) Loss: 0.0000(0.0022) Grad: 2158.0342  LR: 0.000001  \n","Epoch: [1][34800/36908] Elapsed 369m 58s (remain 22m 23s) Loss: 0.0000(0.0022) Grad: 12.9628  LR: 0.000001  \n","Epoch: [1][34900/36908] Elapsed 371m 1s (remain 21m 20s) Loss: 0.0041(0.0022) Grad: 48039.1406  LR: 0.000001  \n","Epoch: [1][35000/36908] Elapsed 372m 5s (remain 20m 16s) Loss: 0.0000(0.0022) Grad: 319.3589  LR: 0.000001  \n","Epoch: [1][35100/36908] Elapsed 373m 8s (remain 19m 12s) Loss: 0.0000(0.0022) Grad: 16.3808  LR: 0.000001  \n","Epoch: [1][35200/36908] Elapsed 374m 11s (remain 18m 8s) Loss: 0.0002(0.0022) Grad: 20756.4141  LR: 0.000001  \n","Epoch: [1][35300/36908] Elapsed 375m 14s (remain 17m 4s) Loss: 0.0003(0.0022) Grad: 40268.6953  LR: 0.000001  \n","Epoch: [1][35400/36908] Elapsed 376m 17s (remain 16m 1s) Loss: 0.0000(0.0022) Grad: 691.8608  LR: 0.000001  \n","Epoch: [1][35500/36908] Elapsed 377m 21s (remain 14m 57s) Loss: 0.0003(0.0022) Grad: 5855.3257  LR: 0.000001  \n","Epoch: [1][35600/36908] Elapsed 378m 24s (remain 13m 53s) Loss: 0.0000(0.0022) Grad: 20.6261  LR: 0.000001  \n","Epoch: [1][35700/36908] Elapsed 379m 27s (remain 12m 49s) Loss: 0.0000(0.0022) Grad: 1218.4376  LR: 0.000001  \n","Epoch: [1][35800/36908] Elapsed 380m 30s (remain 11m 45s) Loss: 0.0000(0.0022) Grad: 1787.5225  LR: 0.000001  \n","Epoch: [1][35900/36908] Elapsed 381m 33s (remain 10m 42s) Loss: 0.0002(0.0022) Grad: 58313.0625  LR: 0.000001  \n","Epoch: [1][36000/36908] Elapsed 382m 36s (remain 9m 38s) Loss: 0.0018(0.0022) Grad: 295647.9062  LR: 0.000001  \n","Epoch: [1][36100/36908] Elapsed 383m 39s (remain 8m 34s) Loss: 0.0002(0.0022) Grad: 45567.7539  LR: 0.000000  \n","Epoch: [1][36200/36908] Elapsed 384m 44s (remain 7m 30s) Loss: 0.0000(0.0022) Grad: 794.3337  LR: 0.000000  \n","Epoch: [1][36300/36908] Elapsed 385m 48s (remain 6m 27s) Loss: 0.0000(0.0022) Grad: 484.0341  LR: 0.000000  \n","Epoch: [1][36400/36908] Elapsed 386m 52s (remain 5m 23s) Loss: 0.0000(0.0022) Grad: 442.6466  LR: 0.000000  \n","Epoch: [1][36500/36908] Elapsed 387m 56s (remain 4m 19s) Loss: 0.0084(0.0022) Grad: 112704.1953  LR: 0.000000  \n","Epoch: [1][36600/36908] Elapsed 389m 0s (remain 3m 15s) Loss: 0.0000(0.0022) Grad: 836.2369  LR: 0.000000  \n","Epoch: [1][36700/36908] Elapsed 390m 4s (remain 2m 12s) Loss: 0.0000(0.0022) Grad: 12.2917  LR: 0.000000  \n","Epoch: [1][36800/36908] Elapsed 391m 7s (remain 1m 8s) Loss: 0.0000(0.0022) Grad: 27.4774  LR: 0.000000  \n","Epoch: [1][36900/36908] Elapsed 392m 11s (remain 0m 4s) Loss: 0.0000(0.0022) Grad: 139.5533  LR: 0.000000  \n","Epoch: [1][36907/36908] Elapsed 392m 15s (remain 0m 0s) Loss: 0.0003(0.0022) Grad: 60784.8125  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 1s (remain 23m 24s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 24s (remain 4m 26s) Loss: 0.0229(0.0043) \n","EVAL: [200/1192] Elapsed 0m 47s (remain 3m 56s) Loss: 0.0015(0.0041) \n","EVAL: [300/1192] Elapsed 1m 10s (remain 3m 30s) Loss: 0.0059(0.0040) \n","EVAL: [400/1192] Elapsed 1m 34s (remain 3m 5s) Loss: 0.0000(0.0042) \n","EVAL: [500/1192] Elapsed 1m 57s (remain 2m 41s) Loss: 0.0000(0.0039) \n","EVAL: [600/1192] Elapsed 2m 20s (remain 2m 18s) Loss: 0.0000(0.0040) \n","EVAL: [700/1192] Elapsed 2m 43s (remain 1m 54s) Loss: 0.0076(0.0046) \n","EVAL: [800/1192] Elapsed 3m 7s (remain 1m 31s) Loss: 0.0000(0.0046) \n","EVAL: [900/1192] Elapsed 3m 30s (remain 1m 7s) Loss: 0.0029(0.0045) \n","EVAL: [1000/1192] Elapsed 3m 53s (remain 0m 44s) Loss: 0.0021(0.0045) \n","EVAL: [1100/1192] Elapsed 4m 16s (remain 0m 21s) Loss: 0.0138(0.0043) \n","EVAL: [1191/1192] Elapsed 4m 37s (remain 0m 0s) Loss: 0.0000(0.0041) \n","Epoch 1 - avg_train_loss: 0.0022  avg_val_loss: 0.0041  time: 23816s\n","Epoch 1 - Score: 0.8865\n","Epoch 1 - Save Best Score: 0.8865 Model\n","========== fold: 3 training ==========\n","get pseudo plain from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n","get pseudo labels from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_3.npy\n","get pseudo labels from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_3.npy\n","get pseudo labels from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp083/pseudo_labels_3.npy\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"053364184fc54251906b47b1d846a378","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/612602 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"29caafc560064e1586e84554865c9bad","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/612602 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["(612602, 950)\n","(612602, 6) (612602, 950)\n","(100000, 7)\n","(110725, 11)\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\n","Epoch: [1][0/36908] Elapsed 0m 1s (remain 1017m 4s) Loss: 0.0751(0.0751) Grad: 68713.4688  LR: 0.000000  \n","Epoch: [1][100/36908] Elapsed 1m 5s (remain 398m 47s) Loss: 0.0608(0.0720) Grad: 58263.2617  LR: 0.000001  \n","Epoch: [1][200/36908] Elapsed 2m 9s (remain 393m 49s) Loss: 0.0293(0.0593) Grad: 29868.2812  LR: 0.000001  \n","Epoch: [1][300/36908] Elapsed 3m 13s (remain 391m 28s) Loss: 0.0050(0.0460) Grad: 7708.4980  LR: 0.000002  \n","Epoch: [1][400/36908] Elapsed 4m 16s (remain 389m 23s) Loss: 0.0053(0.0381) Grad: 3401.1326  LR: 0.000002  \n","Epoch: [1][500/36908] Elapsed 5m 19s (remain 387m 21s) Loss: 0.0200(0.0331) Grad: 5394.5464  LR: 0.000003  \n","Epoch: [1][600/36908] Elapsed 6m 23s (remain 386m 19s) Loss: 0.0114(0.0297) Grad: 3771.9961  LR: 0.000003  \n","Epoch: [1][700/36908] Elapsed 7m 27s (remain 384m 54s) Loss: 0.0102(0.0273) Grad: 4215.2383  LR: 0.000004  \n","Epoch: [1][800/36908] Elapsed 8m 30s (remain 383m 27s) Loss: 0.0035(0.0252) Grad: 3870.0269  LR: 0.000004  \n","Epoch: [1][900/36908] Elapsed 9m 33s (remain 381m 59s) Loss: 0.0063(0.0238) Grad: 3102.9517  LR: 0.000005  \n","Epoch: [1][1000/36908] Elapsed 10m 36s (remain 380m 37s) Loss: 0.0049(0.0225) Grad: 6040.2051  LR: 0.000005  \n","Epoch: [1][1100/36908] Elapsed 11m 40s (remain 379m 30s) Loss: 0.0084(0.0214) Grad: 7303.9951  LR: 0.000006  \n","Epoch: [1][1200/36908] Elapsed 12m 43s (remain 378m 25s) Loss: 0.0105(0.0203) Grad: 11562.9600  LR: 0.000007  \n","Epoch: [1][1300/36908] Elapsed 13m 46s (remain 377m 8s) Loss: 0.0007(0.0192) Grad: 1338.4117  LR: 0.000007  \n","Epoch: [1][1400/36908] Elapsed 14m 50s (remain 375m 57s) Loss: 0.0066(0.0182) Grad: 25520.2227  LR: 0.000008  \n","Epoch: [1][1500/36908] Elapsed 15m 53s (remain 374m 44s) Loss: 0.0031(0.0173) Grad: 2634.5288  LR: 0.000008  \n","Epoch: [1][1600/36908] Elapsed 16m 56s (remain 373m 34s) Loss: 0.0013(0.0165) Grad: 2974.6418  LR: 0.000009  \n","Epoch: [1][1700/36908] Elapsed 17m 59s (remain 372m 26s) Loss: 0.0018(0.0158) Grad: 20934.7168  LR: 0.000009  \n","Epoch: [1][1800/36908] Elapsed 19m 2s (remain 371m 17s) Loss: 0.0060(0.0151) Grad: 18011.5918  LR: 0.000010  \n","Epoch: [1][1900/36908] Elapsed 20m 5s (remain 370m 6s) Loss: 0.0038(0.0145) Grad: 12848.0830  LR: 0.000010  \n","Epoch: [1][2000/36908] Elapsed 21m 8s (remain 368m 57s) Loss: 0.0001(0.0140) Grad: 289.4045  LR: 0.000011  \n","Epoch: [1][2100/36908] Elapsed 22m 12s (remain 367m 52s) Loss: 0.0014(0.0135) Grad: 2560.6086  LR: 0.000011  \n","Epoch: [1][2200/36908] Elapsed 23m 15s (remain 366m 43s) Loss: 0.0001(0.0130) Grad: 2730.9712  LR: 0.000012  \n","Epoch: [1][2300/36908] Elapsed 24m 19s (remain 365m 46s) Loss: 0.0037(0.0126) Grad: 13997.3926  LR: 0.000012  \n","Epoch: [1][2400/36908] Elapsed 25m 23s (remain 364m 48s) Loss: 0.0012(0.0122) Grad: 6301.7412  LR: 0.000013  \n","Epoch: [1][2500/36908] Elapsed 26m 27s (remain 363m 56s) Loss: 0.0034(0.0118) Grad: 6870.0762  LR: 0.000014  \n","Epoch: [1][2600/36908] Elapsed 27m 31s (remain 363m 7s) Loss: 0.0003(0.0115) Grad: 9857.4482  LR: 0.000014  \n","Epoch: [1][2700/36908] Elapsed 28m 37s (remain 362m 27s) Loss: 0.0020(0.0111) Grad: 5934.8198  LR: 0.000015  \n","Epoch: [1][2800/36908] Elapsed 29m 42s (remain 361m 43s) Loss: 0.0055(0.0109) Grad: 18423.0703  LR: 0.000015  \n","Epoch: [1][2900/36908] Elapsed 30m 48s (remain 361m 10s) Loss: 0.0010(0.0106) Grad: 1981.6786  LR: 0.000016  \n","Epoch: [1][3000/36908] Elapsed 31m 53s (remain 360m 19s) Loss: 0.0003(0.0103) Grad: 1041.1327  LR: 0.000016  \n","Epoch: [1][3100/36908] Elapsed 32m 57s (remain 359m 19s) Loss: 0.0009(0.0101) Grad: 1940.4050  LR: 0.000017  \n","Epoch: [1][3200/36908] Elapsed 34m 1s (remain 358m 16s) Loss: 0.0098(0.0099) Grad: 35016.3086  LR: 0.000017  \n","Epoch: [1][3300/36908] Elapsed 35m 5s (remain 357m 18s) Loss: 0.0006(0.0097) Grad: 1519.4080  LR: 0.000018  \n","Epoch: [1][3400/36908] Elapsed 36m 10s (remain 356m 24s) Loss: 0.0002(0.0095) Grad: 1283.0869  LR: 0.000018  \n","Epoch: [1][3500/36908] Elapsed 37m 14s (remain 355m 25s) Loss: 0.0014(0.0093) Grad: 3289.5369  LR: 0.000019  \n","Epoch: [1][3600/36908] Elapsed 38m 20s (remain 354m 37s) Loss: 0.0023(0.0091) Grad: 5288.7554  LR: 0.000020  \n","Epoch: [1][3700/36908] Elapsed 39m 25s (remain 353m 47s) Loss: 0.0152(0.0090) Grad: 31187.8770  LR: 0.000020  \n","Epoch: [1][3800/36908] Elapsed 40m 30s (remain 352m 51s) Loss: 0.0027(0.0088) Grad: 6535.2471  LR: 0.000020  \n","Epoch: [1][3900/36908] Elapsed 41m 35s (remain 351m 53s) Loss: 0.0023(0.0086) Grad: 2440.3606  LR: 0.000020  \n","Epoch: [1][4000/36908] Elapsed 42m 41s (remain 351m 3s) Loss: 0.0032(0.0085) Grad: 35161.1484  LR: 0.000020  \n","Epoch: [1][4100/36908] Elapsed 43m 46s (remain 350m 15s) Loss: 0.0019(0.0083) Grad: 14813.6611  LR: 0.000020  \n","Epoch: [1][4200/36908] Elapsed 44m 52s (remain 349m 24s) Loss: 0.0007(0.0082) Grad: 10899.7451  LR: 0.000020  \n","Epoch: [1][4300/36908] Elapsed 45m 56s (remain 348m 18s) Loss: 0.0007(0.0080) Grad: 1921.8741  LR: 0.000020  \n","Epoch: [1][4400/36908] Elapsed 47m 0s (remain 347m 10s) Loss: 0.0006(0.0079) Grad: 5503.4033  LR: 0.000020  \n","Epoch: [1][4500/36908] Elapsed 48m 3s (remain 346m 3s) Loss: 0.0004(0.0078) Grad: 3641.2668  LR: 0.000020  \n","Epoch: [1][4600/36908] Elapsed 49m 7s (remain 344m 57s) Loss: 0.0005(0.0077) Grad: 901.0717  LR: 0.000019  \n","Epoch: [1][4700/36908] Elapsed 50m 11s (remain 343m 50s) Loss: 0.0013(0.0075) Grad: 9668.7988  LR: 0.000019  \n","Epoch: [1][4800/36908] Elapsed 51m 15s (remain 342m 50s) Loss: 0.0031(0.0074) Grad: 6841.9800  LR: 0.000019  \n","Epoch: [1][4900/36908] Elapsed 52m 20s (remain 341m 48s) Loss: 0.0009(0.0073) Grad: 14664.8711  LR: 0.000019  \n","Epoch: [1][5000/36908] Elapsed 53m 25s (remain 340m 49s) Loss: 0.0024(0.0072) Grad: 15814.7393  LR: 0.000019  \n","Epoch: [1][5100/36908] Elapsed 54m 29s (remain 339m 44s) Loss: 0.0070(0.0071) Grad: 36302.4805  LR: 0.000019  \n","Epoch: [1][5200/36908] Elapsed 55m 34s (remain 338m 49s) Loss: 0.0001(0.0070) Grad: 5179.3315  LR: 0.000019  \n","Epoch: [1][5300/36908] Elapsed 56m 41s (remain 337m 58s) Loss: 0.0008(0.0069) Grad: 14115.7793  LR: 0.000019  \n","Epoch: [1][5400/36908] Elapsed 57m 46s (remain 337m 2s) Loss: 0.0001(0.0068) Grad: 301.3954  LR: 0.000019  \n","Epoch: [1][5500/36908] Elapsed 58m 52s (remain 336m 6s) Loss: 0.0001(0.0068) Grad: 191.9325  LR: 0.000019  \n","Epoch: [1][5600/36908] Elapsed 59m 57s (remain 335m 7s) Loss: 0.0039(0.0067) Grad: 26690.4961  LR: 0.000019  \n","Epoch: [1][5700/36908] Elapsed 61m 2s (remain 334m 7s) Loss: 0.0003(0.0066) Grad: 2204.2283  LR: 0.000019  \n","Epoch: [1][5800/36908] Elapsed 62m 7s (remain 333m 8s) Loss: 0.0004(0.0065) Grad: 17138.6973  LR: 0.000019  \n","Epoch: [1][5900/36908] Elapsed 63m 13s (remain 332m 11s) Loss: 0.0034(0.0065) Grad: 26041.3828  LR: 0.000019  \n","Epoch: [1][6000/36908] Elapsed 64m 18s (remain 331m 11s) Loss: 0.0002(0.0064) Grad: 1222.8164  LR: 0.000019  \n","Epoch: [1][6100/36908] Elapsed 65m 22s (remain 330m 8s) Loss: 0.0001(0.0063) Grad: 394.5046  LR: 0.000019  \n","Epoch: [1][6200/36908] Elapsed 66m 27s (remain 329m 5s) Loss: 0.0010(0.0062) Grad: 16985.6270  LR: 0.000018  \n","Epoch: [1][6300/36908] Elapsed 67m 32s (remain 328m 3s) Loss: 0.0015(0.0062) Grad: 111204.1406  LR: 0.000018  \n","Epoch: [1][6400/36908] Elapsed 68m 36s (remain 327m 0s) Loss: 0.0000(0.0061) Grad: 161.7464  LR: 0.000018  \n","Epoch: [1][6500/36908] Elapsed 69m 42s (remain 326m 1s) Loss: 0.0000(0.0060) Grad: 10.6994  LR: 0.000018  \n","Epoch: [1][6600/36908] Elapsed 70m 48s (remain 325m 5s) Loss: 0.0003(0.0060) Grad: 2872.7273  LR: 0.000018  \n","Epoch: [1][6700/36908] Elapsed 71m 54s (remain 324m 7s) Loss: 0.0000(0.0059) Grad: 58.0008  LR: 0.000018  \n","Epoch: [1][6800/36908] Elapsed 73m 0s (remain 323m 9s) Loss: 0.0001(0.0059) Grad: 205.0068  LR: 0.000018  \n","Epoch: [1][6900/36908] Elapsed 74m 6s (remain 322m 12s) Loss: 0.0027(0.0058) Grad: 17206.2793  LR: 0.000018  \n","Epoch: [1][7000/36908] Elapsed 75m 12s (remain 321m 14s) Loss: 0.0064(0.0057) Grad: 93814.1016  LR: 0.000018  \n","Epoch: [1][7100/36908] Elapsed 76m 17s (remain 320m 15s) Loss: 0.0003(0.0057) Grad: 3795.6323  LR: 0.000018  \n","Epoch: [1][7200/36908] Elapsed 77m 23s (remain 319m 17s) Loss: 0.0026(0.0056) Grad: 35767.1133  LR: 0.000018  \n","Epoch: [1][7300/36908] Elapsed 78m 29s (remain 318m 19s) Loss: 0.0012(0.0056) Grad: 92906.3594  LR: 0.000018  \n","Epoch: [1][7400/36908] Elapsed 79m 36s (remain 317m 21s) Loss: 0.0058(0.0055) Grad: 34603.9102  LR: 0.000018  \n","Epoch: [1][7500/36908] Elapsed 80m 42s (remain 316m 23s) Loss: 0.0002(0.0055) Grad: 1932.9288  LR: 0.000018  \n","Epoch: [1][7600/36908] Elapsed 81m 48s (remain 315m 23s) Loss: 0.0003(0.0054) Grad: 1983.6030  LR: 0.000018  \n","Epoch: [1][7700/36908] Elapsed 82m 54s (remain 314m 24s) Loss: 0.0017(0.0054) Grad: 10081.1104  LR: 0.000018  \n","Epoch: [1][7800/36908] Elapsed 84m 0s (remain 313m 26s) Loss: 0.0006(0.0053) Grad: 3239.7759  LR: 0.000018  \n","Epoch: [1][7900/36908] Elapsed 85m 6s (remain 312m 28s) Loss: 0.0003(0.0053) Grad: 9970.1533  LR: 0.000017  \n","Epoch: [1][8000/36908] Elapsed 86m 12s (remain 311m 28s) Loss: 0.0011(0.0053) Grad: 10286.0664  LR: 0.000017  \n","Epoch: [1][8100/36908] Elapsed 87m 18s (remain 310m 26s) Loss: 0.0007(0.0052) Grad: 10155.2080  LR: 0.000017  \n","Epoch: [1][8200/36908] Elapsed 88m 23s (remain 309m 24s) Loss: 0.0136(0.0052) Grad: 79345.7891  LR: 0.000017  \n","Epoch: [1][8300/36908] Elapsed 89m 30s (remain 308m 27s) Loss: 0.0075(0.0051) Grad: 45215.5391  LR: 0.000017  \n","Epoch: [1][8400/36908] Elapsed 90m 36s (remain 307m 26s) Loss: 0.0046(0.0051) Grad: 28693.5352  LR: 0.000017  \n","Epoch: [1][8500/36908] Elapsed 91m 41s (remain 306m 25s) Loss: 0.0001(0.0050) Grad: 1329.7897  LR: 0.000017  \n","Epoch: [1][8600/36908] Elapsed 92m 47s (remain 305m 24s) Loss: 0.0011(0.0050) Grad: 251792.3281  LR: 0.000017  \n","Epoch: [1][8700/36908] Elapsed 93m 53s (remain 304m 24s) Loss: 0.0018(0.0050) Grad: 44529.2852  LR: 0.000017  \n","Epoch: [1][8800/36908] Elapsed 94m 59s (remain 303m 23s) Loss: 0.0000(0.0049) Grad: 77.9500  LR: 0.000017  \n","Epoch: [1][8900/36908] Elapsed 96m 6s (remain 302m 23s) Loss: 0.0044(0.0049) Grad: 26034.6309  LR: 0.000017  \n","Epoch: [1][9000/36908] Elapsed 97m 12s (remain 301m 22s) Loss: 0.0014(0.0049) Grad: 32875.5977  LR: 0.000017  \n","Epoch: [1][9100/36908] Elapsed 98m 18s (remain 300m 20s) Loss: 0.0001(0.0048) Grad: 830.1068  LR: 0.000017  \n","Epoch: [1][9200/36908] Elapsed 99m 23s (remain 299m 18s) Loss: 0.0011(0.0048) Grad: 18514.4590  LR: 0.000017  \n","Epoch: [1][9300/36908] Elapsed 100m 29s (remain 298m 16s) Loss: 0.0003(0.0048) Grad: 11888.7734  LR: 0.000017  \n","Epoch: [1][9400/36908] Elapsed 101m 35s (remain 297m 14s) Loss: 0.0017(0.0047) Grad: 87489.9453  LR: 0.000017  \n","Epoch: [1][9500/36908] Elapsed 102m 41s (remain 296m 13s) Loss: 0.0000(0.0047) Grad: 275.0963  LR: 0.000017  \n","Epoch: [1][9600/36908] Elapsed 103m 47s (remain 295m 11s) Loss: 0.0005(0.0046) Grad: 6491.7427  LR: 0.000016  \n","Epoch: [1][9700/36908] Elapsed 104m 53s (remain 294m 9s) Loss: 0.0008(0.0046) Grad: 20430.8711  LR: 0.000016  \n","Epoch: [1][9800/36908] Elapsed 105m 58s (remain 293m 5s) Loss: 0.0024(0.0046) Grad: 23740.8809  LR: 0.000016  \n","Epoch: [1][9900/36908] Elapsed 107m 4s (remain 292m 3s) Loss: 0.0024(0.0046) Grad: 167811.2656  LR: 0.000016  \n","Epoch: [1][10000/36908] Elapsed 108m 9s (remain 291m 0s) Loss: 0.0021(0.0045) Grad: 39127.7812  LR: 0.000016  \n","Epoch: [1][10100/36908] Elapsed 109m 15s (remain 289m 58s) Loss: 0.0005(0.0045) Grad: 7977.0361  LR: 0.000016  \n","Epoch: [1][10200/36908] Elapsed 110m 21s (remain 288m 55s) Loss: 0.0002(0.0045) Grad: 4856.7949  LR: 0.000016  \n","Epoch: [1][10300/36908] Elapsed 111m 27s (remain 287m 53s) Loss: 0.0021(0.0044) Grad: 29841.5449  LR: 0.000016  \n","Epoch: [1][10400/36908] Elapsed 112m 33s (remain 286m 50s) Loss: 0.0003(0.0044) Grad: 15333.4736  LR: 0.000016  \n","Epoch: [1][10500/36908] Elapsed 113m 38s (remain 285m 47s) Loss: 0.0008(0.0044) Grad: 3716.9192  LR: 0.000016  \n","Epoch: [1][10600/36908] Elapsed 114m 44s (remain 284m 44s) Loss: 0.0027(0.0044) Grad: 82909.8750  LR: 0.000016  \n","Epoch: [1][10700/36908] Elapsed 115m 50s (remain 283m 41s) Loss: 0.0031(0.0043) Grad: 36238.3750  LR: 0.000016  \n","Epoch: [1][10800/36908] Elapsed 116m 56s (remain 282m 39s) Loss: 0.0022(0.0043) Grad: 23063.7168  LR: 0.000016  \n","Epoch: [1][10900/36908] Elapsed 118m 2s (remain 281m 36s) Loss: 0.0055(0.0043) Grad: 34618.9922  LR: 0.000016  \n","Epoch: [1][11000/36908] Elapsed 119m 8s (remain 280m 33s) Loss: 0.0014(0.0043) Grad: 83840.9219  LR: 0.000016  \n","Epoch: [1][11100/36908] Elapsed 120m 14s (remain 279m 30s) Loss: 0.0000(0.0042) Grad: 29.2371  LR: 0.000016  \n","Epoch: [1][11200/36908] Elapsed 121m 19s (remain 278m 27s) Loss: 0.0000(0.0042) Grad: 104.6611  LR: 0.000015  \n","Epoch: [1][11300/36908] Elapsed 122m 25s (remain 277m 23s) Loss: 0.0000(0.0042) Grad: 86.5682  LR: 0.000015  \n","Epoch: [1][11400/36908] Elapsed 123m 30s (remain 276m 18s) Loss: 0.0000(0.0042) Grad: 487.6289  LR: 0.000015  \n","Epoch: [1][11500/36908] Elapsed 124m 35s (remain 275m 14s) Loss: 0.0006(0.0041) Grad: 25168.1484  LR: 0.000015  \n","Epoch: [1][11600/36908] Elapsed 125m 41s (remain 274m 10s) Loss: 0.0000(0.0041) Grad: 833.1244  LR: 0.000015  \n","Epoch: [1][11700/36908] Elapsed 126m 46s (remain 273m 6s) Loss: 0.0000(0.0041) Grad: 133.0289  LR: 0.000015  \n","Epoch: [1][11800/36908] Elapsed 127m 51s (remain 272m 2s) Loss: 0.0000(0.0041) Grad: 158.0926  LR: 0.000015  \n","Epoch: [1][11900/36908] Elapsed 128m 56s (remain 270m 55s) Loss: 0.0052(0.0040) Grad: 75609.2344  LR: 0.000015  \n","Epoch: [1][12000/36908] Elapsed 129m 59s (remain 269m 46s) Loss: 0.0000(0.0040) Grad: 481.6242  LR: 0.000015  \n","Epoch: [1][12100/36908] Elapsed 131m 2s (remain 268m 38s) Loss: 0.0003(0.0040) Grad: 25030.4238  LR: 0.000015  \n","Epoch: [1][12200/36908] Elapsed 132m 6s (remain 267m 30s) Loss: 0.0004(0.0040) Grad: 30014.9863  LR: 0.000015  \n","Epoch: [1][12300/36908] Elapsed 133m 9s (remain 266m 22s) Loss: 0.0002(0.0040) Grad: 5141.9087  LR: 0.000015  \n","Epoch: [1][12400/36908] Elapsed 134m 12s (remain 265m 14s) Loss: 0.0004(0.0039) Grad: 6356.4600  LR: 0.000015  \n","Epoch: [1][12500/36908] Elapsed 135m 16s (remain 264m 6s) Loss: 0.0000(0.0039) Grad: 1131.5895  LR: 0.000015  \n","Epoch: [1][12600/36908] Elapsed 136m 20s (remain 262m 59s) Loss: 0.0000(0.0039) Grad: 327.8872  LR: 0.000015  \n","Epoch: [1][12700/36908] Elapsed 137m 24s (remain 261m 53s) Loss: 0.0004(0.0039) Grad: 29635.6426  LR: 0.000015  \n","Epoch: [1][12800/36908] Elapsed 138m 28s (remain 260m 46s) Loss: 0.0001(0.0039) Grad: 1542.9265  LR: 0.000015  \n","Epoch: [1][12900/36908] Elapsed 139m 31s (remain 259m 38s) Loss: 0.0000(0.0038) Grad: 201.1349  LR: 0.000014  \n","Epoch: [1][13000/36908] Elapsed 140m 35s (remain 258m 31s) Loss: 0.0000(0.0038) Grad: 3795.0327  LR: 0.000014  \n","Epoch: [1][13100/36908] Elapsed 141m 38s (remain 257m 23s) Loss: 0.0000(0.0038) Grad: 618.6066  LR: 0.000014  \n","Epoch: [1][13200/36908] Elapsed 142m 42s (remain 256m 16s) Loss: 0.0000(0.0038) Grad: 3950.0427  LR: 0.000014  \n","Epoch: [1][13300/36908] Elapsed 143m 45s (remain 255m 8s) Loss: 0.0013(0.0038) Grad: 94223.3281  LR: 0.000014  \n","Epoch: [1][13400/36908] Elapsed 144m 48s (remain 254m 0s) Loss: 0.0055(0.0038) Grad: 562325.8125  LR: 0.000014  \n","Epoch: [1][13500/36908] Elapsed 145m 51s (remain 252m 53s) Loss: 0.0002(0.0038) Grad: 29695.5898  LR: 0.000014  \n","Epoch: [1][13600/36908] Elapsed 146m 55s (remain 251m 46s) Loss: 0.0000(0.0037) Grad: 1238.1312  LR: 0.000014  \n","Epoch: [1][13700/36908] Elapsed 147m 58s (remain 250m 38s) Loss: 0.0007(0.0037) Grad: 12197.1299  LR: 0.000014  \n","Epoch: [1][13800/36908] Elapsed 149m 2s (remain 249m 32s) Loss: 0.0034(0.0037) Grad: 41939.0781  LR: 0.000014  \n","Epoch: [1][13900/36908] Elapsed 150m 6s (remain 248m 26s) Loss: 0.0000(0.0037) Grad: 1862.7993  LR: 0.000014  \n","Epoch: [1][14000/36908] Elapsed 151m 11s (remain 247m 21s) Loss: 0.0070(0.0037) Grad: 155604.2344  LR: 0.000014  \n","Epoch: [1][14100/36908] Elapsed 152m 16s (remain 246m 17s) Loss: 0.0023(0.0037) Grad: 104193.1953  LR: 0.000014  \n","Epoch: [1][14200/36908] Elapsed 153m 21s (remain 245m 12s) Loss: 0.0000(0.0037) Grad: 2400.4922  LR: 0.000014  \n","Epoch: [1][14300/36908] Elapsed 154m 25s (remain 244m 7s) Loss: 0.0000(0.0036) Grad: 1532.9059  LR: 0.000014  \n","Epoch: [1][14400/36908] Elapsed 155m 31s (remain 243m 3s) Loss: 0.0000(0.0036) Grad: 156.4031  LR: 0.000014  \n","Epoch: [1][14500/36908] Elapsed 156m 36s (remain 241m 59s) Loss: 0.0014(0.0036) Grad: 126905.9219  LR: 0.000013  \n","Epoch: [1][14600/36908] Elapsed 157m 41s (remain 240m 54s) Loss: 0.0000(0.0036) Grad: 50.9240  LR: 0.000013  \n","Epoch: [1][14700/36908] Elapsed 158m 45s (remain 239m 48s) Loss: 0.0000(0.0036) Grad: 1264.7839  LR: 0.000013  \n","Epoch: [1][14800/36908] Elapsed 159m 49s (remain 238m 42s) Loss: 0.0020(0.0036) Grad: 16000.3525  LR: 0.000013  \n","Epoch: [1][14900/36908] Elapsed 160m 53s (remain 237m 36s) Loss: 0.0000(0.0036) Grad: 79.0537  LR: 0.000013  \n","Epoch: [1][15000/36908] Elapsed 161m 57s (remain 236m 31s) Loss: 0.0000(0.0036) Grad: 59.5065  LR: 0.000013  \n","Epoch: [1][15100/36908] Elapsed 163m 1s (remain 235m 25s) Loss: 0.0000(0.0035) Grad: 106.1925  LR: 0.000013  \n","Epoch: [1][15200/36908] Elapsed 164m 5s (remain 234m 19s) Loss: 0.0010(0.0035) Grad: 28350.1270  LR: 0.000013  \n","Epoch: [1][15300/36908] Elapsed 165m 9s (remain 233m 13s) Loss: 0.0038(0.0035) Grad: 349028.1562  LR: 0.000013  \n","Epoch: [1][15400/36908] Elapsed 166m 12s (remain 232m 6s) Loss: 0.0011(0.0035) Grad: 19605.1387  LR: 0.000013  \n","Epoch: [1][15500/36908] Elapsed 167m 16s (remain 231m 0s) Loss: 0.0016(0.0035) Grad: 85233.7500  LR: 0.000013  \n","Epoch: [1][15600/36908] Elapsed 168m 19s (remain 229m 53s) Loss: 0.0002(0.0035) Grad: 3451.1106  LR: 0.000013  \n","Epoch: [1][15700/36908] Elapsed 169m 21s (remain 228m 45s) Loss: 0.0000(0.0035) Grad: 186.7529  LR: 0.000013  \n","Epoch: [1][15800/36908] Elapsed 170m 24s (remain 227m 37s) Loss: 0.0038(0.0034) Grad: 101331.9219  LR: 0.000013  \n","Epoch: [1][15900/36908] Elapsed 171m 26s (remain 226m 30s) Loss: 0.0191(0.0034) Grad: 100402.0859  LR: 0.000013  \n","Epoch: [1][16000/36908] Elapsed 172m 29s (remain 225m 22s) Loss: 0.0009(0.0034) Grad: 73222.2500  LR: 0.000013  \n","Epoch: [1][16100/36908] Elapsed 173m 32s (remain 224m 15s) Loss: 0.0002(0.0034) Grad: 35483.8906  LR: 0.000013  \n","Epoch: [1][16200/36908] Elapsed 174m 35s (remain 223m 9s) Loss: 0.0001(0.0034) Grad: 14706.5742  LR: 0.000012  \n","Epoch: [1][16300/36908] Elapsed 175m 38s (remain 222m 2s) Loss: 0.0000(0.0034) Grad: 511.5908  LR: 0.000012  \n","Epoch: [1][16400/36908] Elapsed 176m 41s (remain 220m 55s) Loss: 0.0000(0.0034) Grad: 158.1427  LR: 0.000012  \n","Epoch: [1][16500/36908] Elapsed 177m 44s (remain 219m 48s) Loss: 0.0133(0.0034) Grad: inf  LR: 0.000012  \n","Epoch: [1][16600/36908] Elapsed 178m 46s (remain 218m 41s) Loss: 0.0024(0.0034) Grad: 152364.0938  LR: 0.000012  \n","Epoch: [1][16700/36908] Elapsed 179m 49s (remain 217m 34s) Loss: 0.0006(0.0033) Grad: 113508.6172  LR: 0.000012  \n","Epoch: [1][16800/36908] Elapsed 180m 52s (remain 216m 27s) Loss: 0.0003(0.0033) Grad: 10153.0928  LR: 0.000012  \n","Epoch: [1][16900/36908] Elapsed 181m 55s (remain 215m 21s) Loss: 0.0000(0.0033) Grad: 648.7809  LR: 0.000012  \n","Epoch: [1][17000/36908] Elapsed 182m 58s (remain 214m 14s) Loss: 0.0054(0.0033) Grad: 61522.2266  LR: 0.000012  \n"]}],"source":["if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"nbme-exp094.ipynb","version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"papermill":{"default_parameters":{},"duration":641.572862,"end_time":"2022-02-27T11:39:50.972497","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-02-27T11:29:09.399635","version":"2.3.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"00c61771eded4991a4c8baff52705e19":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0163acad7bed45ceb77ae913902ec8d0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01ad766657e641acb7938c21ef8252df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc522c4748024725ac8a3b0c4b55b44d","placeholder":"​","style":"IPY_MODEL_ebd71d81ff1147209db0839f45132d37","value":"100%"}},"02dd1a014b19403cbc38cdb80fd0fdf7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48431fe80be642e385a29c9a1f79dcca","placeholder":"​","style":"IPY_MODEL_6821eaa30acf4e2b882d59602bcc93b4","value":" 612602/612602 [00:00\u0026lt;00:00, 947950.84it/s]"}},"04c665f50cc24796900396f595420d37":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"053364184fc54251906b47b1d846a378":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_13813725dcfd4257ae630cd53ec97ff7","IPY_MODEL_c746ab1c693c4a3a9071d1c35b2a148e","IPY_MODEL_02dd1a014b19403cbc38cdb80fd0fdf7"],"layout":"IPY_MODEL_159d89ef5ecc49a0b57cc940fa0b0da3"}},"082271089cfb47efb80b2bc36084b53b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"09247eb76c794085a944eadb6e5d2351":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c8907f23b1440c6819d6cad3a212346":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d35b70d278b471eb9edde7467abc292":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d00c1c0f59d943e4bd945549d59d280b","placeholder":"​","style":"IPY_MODEL_498537a9b5fd4ad1827f755ecac9785d","value":" 42146/42146 [00:23\u0026lt;00:00, 1940.50it/s]"}},"1115446da7414840b365b18da28949e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c2b9f72e8bc46cfb119852b061b549b","placeholder":"​","style":"IPY_MODEL_8b958541335f443486d2e9da389f3237","value":" 612602/612602 [00:29\u0026lt;00:00, 24179.15it/s]"}},"11618cec11864dc09e0467143ca35070":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13813725dcfd4257ae630cd53ec97ff7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_953b56e7810a479b9d6faebff19cf914","placeholder":"​","style":"IPY_MODEL_fb4d13679b8f4c738b7b9691644c00c2","value":"100%"}},"1458c8ab5a854f7fac8ff5d3fea15065":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9eef15db4a1a41f6913deeaaf035739a","IPY_MODEL_cf68986a0ab948c6815e6b60775c24e5","IPY_MODEL_8b9a3f7080ef466ea8149946e891f3d0"],"layout":"IPY_MODEL_3812300290154b158f45bd9a99f3e599"}},"15209ae8fea64c3fbbc2e9ce2cea8632":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"159d89ef5ecc49a0b57cc940fa0b0da3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15a2fe7fa4b847de8609358348323ec2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d85b307b22f44f8a5c46920ac4a2c38","placeholder":"​","style":"IPY_MODEL_eec24051b1e84ce99eb32769f338d65c","value":"100%"}},"17ada097d7904de7bfdf407bd7a46434":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d495bfec46ce4f7d93888074a6b133f0","IPY_MODEL_c42cdc5799874d56b3eb256d4ac5b287","IPY_MODEL_aa851460b80542d088b934593f930b4b"],"layout":"IPY_MODEL_59cef2153eb740dbad00f270b6adc557"}},"1b6b5506c9b54edfb53f204bb1c106c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ddd708343f04e719d0ca961bd110dee","placeholder":"​","style":"IPY_MODEL_8228daf3c31b4f50b43c56913d2a84a1","value":" 42146/42146 [00:00\u0026lt;00:00, 690816.90it/s]"}},"1e1367ae75bb4fb8a78e07c9c0a6e4dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"22f1ea873a3746708c9bd64b2c4d1fec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"242513234cbc42fdae123052603ebdc1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d39c48e8e0bc46d890abe09c2fdd6765","max":612602,"min":0,"orientation":"horizontal","style":"IPY_MODEL_04c665f50cc24796900396f595420d37","value":612602}},"24f2d6c297dc470091e20057ae39c5b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_50d4c5f6835447479247a261d4a5fc10","max":612602,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b5e6fcd882834c7da752ba4228e243d0","value":612602}},"254bca9de2d34d44b17839c620f676c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0212009e58b49f08cc00e6c09811ed4","placeholder":"​","style":"IPY_MODEL_65adc58e74db45748eb4b1d4307a0749","value":" 612602/612602 [00:00\u0026lt;00:00, 976100.55it/s]"}},"26a57a293861433e867e0faf466b1cfd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"274fe987c5e241dbaa3e3c48a758cd19":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"29caafc560064e1586e84554865c9bad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f43b48faad8446d2831c730b93809a95","IPY_MODEL_5b3f50f56e8247d3a5a78ffbc3598a3a","IPY_MODEL_1115446da7414840b365b18da28949e6"],"layout":"IPY_MODEL_8d697655ad164ca0b0d8f9604616e079"}},"2aac9b18cf544960aaf07a385308588c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"35af474496924fcaa3fb00d02a0d26c5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3812300290154b158f45bd9a99f3e599":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3adf4785a4354deca2cc262ed03a56c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"48431fe80be642e385a29c9a1f79dcca":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"486dc2396f7f454782ac62bef2727921":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f4ea91546e1e4fe1b5afb2b9dfedbd94","IPY_MODEL_4e342a721b9b4cbe997afd225362d516","IPY_MODEL_1b6b5506c9b54edfb53f204bb1c106c3"],"layout":"IPY_MODEL_bbdfdb7366774e44b5200c62281ebee0"}},"498537a9b5fd4ad1827f755ecac9785d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ae374b4bbd649fcaf89c30833693135":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d85b307b22f44f8a5c46920ac4a2c38":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ddd708343f04e719d0ca961bd110dee":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e342a721b9b4cbe997afd225362d516":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_15209ae8fea64c3fbbc2e9ce2cea8632","max":42146,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7adaa361948c4941b95469ccec13d848","value":42146}},"4f8b75efec3a4a9d8550a730cca6abc2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"50d4c5f6835447479247a261d4a5fc10":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"512c4f8900b7411ea54df4540af859c1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59355104db1649abbc90d5434a19627a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"59cef2153eb740dbad00f270b6adc557":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b1a058572f44f1798c0dac02647d868":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b3f50f56e8247d3a5a78ffbc3598a3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_09247eb76c794085a944eadb6e5d2351","max":612602,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c7bb69a161824b8ebc788384c85fcc30","value":612602}},"5c2b9f72e8bc46cfb119852b061b549b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d170b701e47454694267444bcc20272":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2d040eeeb8948fbbac56c3b312eab70","max":42146,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e2933e910c904d8aa4523b4435b14810","value":42146}},"5ed13e64b5e54a389515831a734d32e5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f9bfca84ea54c9b830f3650ae4ac116":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_774f1d7b814643c2a1cc1cc0ef5f2266","placeholder":"​","style":"IPY_MODEL_a33fff1a8f484ae1a428dc27a7b62a30","value":"100%"}},"65adc58e74db45748eb4b1d4307a0749":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"665d8b418fde4c61b61cc4f92cb2656e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6821eaa30acf4e2b882d59602bcc93b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7080ac0e76844cb6bc69d98f2b44e2f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"713d2e0c64b84d27a1a7b469b0fd944c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"774f1d7b814643c2a1cc1cc0ef5f2266":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7adaa361948c4941b95469ccec13d848":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7cadff28eb7a498a97d51ea1beb1fcbd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7e3303f01099441c96a1323c211a900d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f58fdb90b8ee482e89f5d6757f826a84","placeholder":"​","style":"IPY_MODEL_082271089cfb47efb80b2bc36084b53b","value":"100%"}},"8228daf3c31b4f50b43c56913d2a84a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"83f91daa69b0427bb99b0ea72e99e91e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84419842ca8743908bf405a638f6cf45":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_15a2fe7fa4b847de8609358348323ec2","IPY_MODEL_242513234cbc42fdae123052603ebdc1","IPY_MODEL_254bca9de2d34d44b17839c620f676c1"],"layout":"IPY_MODEL_5b1a058572f44f1798c0dac02647d868"}},"86dfb74338454225b3f09f235964d631":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8709be5d02c7425e9476bbbfb08d4b02":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_35af474496924fcaa3fb00d02a0d26c5","max":612602,"min":0,"orientation":"horizontal","style":"IPY_MODEL_59355104db1649abbc90d5434a19627a","value":612602}},"8a4991ccec854d6489d4792ebd9d7dba":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a64f5c36fb740fe970f2fb0c4c4d550":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef77b0969f3a489e8a980298d932d844","placeholder":"​","style":"IPY_MODEL_c7f35728bdb643d58f6e95dac3a7a09f","value":" 612602/612602 [00:29\u0026lt;00:00, 24124.49it/s]"}},"8b958541335f443486d2e9da389f3237":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8b9a3f7080ef466ea8149946e891f3d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86dfb74338454225b3f09f235964d631","placeholder":"​","style":"IPY_MODEL_0c8907f23b1440c6819d6cad3a212346","value":" 143/143 [00:00\u0026lt;00:00, 3323.91it/s]"}},"8cee85fdacc846b2b0a2b38cce81bf1b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_713d2e0c64b84d27a1a7b469b0fd944c","placeholder":"​","style":"IPY_MODEL_4f8b75efec3a4a9d8550a730cca6abc2","value":" 612602/612602 [00:00\u0026lt;00:00, 1011829.84it/s]"}},"8d697655ad164ca0b0d8f9604616e079":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d8dcaf5ca3f4ac39dfcc01c4f4105e2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8eee5e4a5a934b208f18dad0a17d5038":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ac5a1c54138b465bad38d56c222c3bc2","IPY_MODEL_8709be5d02c7425e9476bbbfb08d4b02","IPY_MODEL_e8cded13834b40c98da54400d9e4a335"],"layout":"IPY_MODEL_26a57a293861433e867e0faf466b1cfd"}},"953b56e7810a479b9d6faebff19cf914":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9622e83a04574bd7bb336e96dd1c7401":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9d12f7755bfe4e9c95c64113d3fe3bb6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9eef15db4a1a41f6913deeaaf035739a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d9c091bc31fb45bcaba1c2567bf6a6eb","placeholder":"​","style":"IPY_MODEL_f8a8bf71373d40238a8b24089abd0a5d","value":"100%"}},"a0517900da2a4ef69b59b1b0078b6164":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5f9bfca84ea54c9b830f3650ae4ac116","IPY_MODEL_e9eb142a31a447e3aae56d99fa2f8fbf","IPY_MODEL_8a64f5c36fb740fe970f2fb0c4c4d550"],"layout":"IPY_MODEL_11618cec11864dc09e0467143ca35070"}},"a33fff1a8f484ae1a428dc27a7b62a30":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aa851460b80542d088b934593f930b4b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a4991ccec854d6489d4792ebd9d7dba","placeholder":"​","style":"IPY_MODEL_d52102695ac941d9a4bb43f09b15f7c2","value":" 612602/612602 [00:29\u0026lt;00:00, 24103.20it/s]"}},"ac5a1c54138b465bad38d56c222c3bc2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_665d8b418fde4c61b61cc4f92cb2656e","placeholder":"​","style":"IPY_MODEL_274fe987c5e241dbaa3e3c48a758cd19","value":"100%"}},"ad0fde1178af4d2fa2e51b2432760942":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b305245f34c44486a05828bac4d2b307":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4b5881a571e4f04be84cbe8fe8ff2e3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7e3303f01099441c96a1323c211a900d","IPY_MODEL_5d170b701e47454694267444bcc20272","IPY_MODEL_0d35b70d278b471eb9edde7467abc292"],"layout":"IPY_MODEL_5ed13e64b5e54a389515831a734d32e5"}},"b5e6fcd882834c7da752ba4228e243d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bbdfdb7366774e44b5200c62281ebee0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0c51ffad2ff47bf8f899a159ea54685":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2d040eeeb8948fbbac56c3b312eab70":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3cd57dd0e2541f892db73334bdd0401":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c42cdc5799874d56b3eb256d4ac5b287":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d8dcaf5ca3f4ac39dfcc01c4f4105e2","max":612602,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3adf4785a4354deca2cc262ed03a56c2","value":612602}},"c746ab1c693c4a3a9071d1c35b2a148e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0c51ffad2ff47bf8f899a159ea54685","max":612602,"min":0,"orientation":"horizontal","style":"IPY_MODEL_22f1ea873a3746708c9bd64b2c4d1fec","value":612602}},"c7bb69a161824b8ebc788384c85fcc30":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c7f35728bdb643d58f6e95dac3a7a09f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc522c4748024725ac8a3b0c4b55b44d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf68986a0ab948c6815e6b60775c24e5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0163acad7bed45ceb77ae913902ec8d0","max":143,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7cadff28eb7a498a97d51ea1beb1fcbd","value":143}},"d00c1c0f59d943e4bd945549d59d280b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0212009e58b49f08cc00e6c09811ed4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0a61cc4e08c484fb622c97b21c507e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3cd57dd0e2541f892db73334bdd0401","placeholder":"​","style":"IPY_MODEL_7080ac0e76844cb6bc69d98f2b44e2f5","value":"100%"}},"d39c48e8e0bc46d890abe09c2fdd6765":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3a4f5c5a8c04d97bb8913c7fb2d0e0d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_83f91daa69b0427bb99b0ea72e99e91e","max":612602,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9622e83a04574bd7bb336e96dd1c7401","value":612602}},"d495bfec46ce4f7d93888074a6b133f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ae374b4bbd649fcaf89c30833693135","placeholder":"​","style":"IPY_MODEL_efdb9098c0c04e57849d0b17151a84cc","value":"100%"}},"d52102695ac941d9a4bb43f09b15f7c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d9c091bc31fb45bcaba1c2567bf6a6eb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0bd90cddc7942108e6d93d2ac2b82ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d0a61cc4e08c484fb622c97b21c507e0","IPY_MODEL_d3a4f5c5a8c04d97bb8913c7fb2d0e0d","IPY_MODEL_8cee85fdacc846b2b0a2b38cce81bf1b"],"layout":"IPY_MODEL_ec49fcd4f9644659b393b619a3729e62"}},"e2933e910c904d8aa4523b4435b14810":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e6adb3ecf4034f32bef523ea3b629fa1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e78b0987c51a4ec7b835b2bcfe928427":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8c8e12a3d2d4d2881dcc08f5a5168a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_512c4f8900b7411ea54df4540af859c1","placeholder":"​","style":"IPY_MODEL_e78b0987c51a4ec7b835b2bcfe928427","value":" 612602/612602 [00:00\u0026lt;00:00, 899059.60it/s]"}},"e8cded13834b40c98da54400d9e4a335":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b305245f34c44486a05828bac4d2b307","placeholder":"​","style":"IPY_MODEL_1e1367ae75bb4fb8a78e07c9c0a6e4dd","value":" 612602/612602 [00:28\u0026lt;00:00, 25978.37it/s]"}},"e9eb142a31a447e3aae56d99fa2f8fbf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed6c10475b4544d8835262cad7be13c6","max":612602,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2aac9b18cf544960aaf07a385308588c","value":612602}},"ebd71d81ff1147209db0839f45132d37":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec49fcd4f9644659b393b619a3729e62":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed6c10475b4544d8835262cad7be13c6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eec24051b1e84ce99eb32769f338d65c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef77b0969f3a489e8a980298d932d844":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efdb9098c0c04e57849d0b17151a84cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f43b48faad8446d2831c730b93809a95":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6adb3ecf4034f32bef523ea3b629fa1","placeholder":"​","style":"IPY_MODEL_00c61771eded4991a4c8baff52705e19","value":"100%"}},"f4ea91546e1e4fe1b5afb2b9dfedbd94":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad0fde1178af4d2fa2e51b2432760942","placeholder":"​","style":"IPY_MODEL_fba6f53acaee45dd88aff72672361e0b","value":"100%"}},"f58fdb90b8ee482e89f5d6757f826a84":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8a8bf71373d40238a8b24089abd0a5d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f94a6bd8355241ad912d728a297ec6fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_01ad766657e641acb7938c21ef8252df","IPY_MODEL_24f2d6c297dc470091e20057ae39c5b9","IPY_MODEL_e8c8e12a3d2d4d2881dcc08f5a5168a1"],"layout":"IPY_MODEL_9d12f7755bfe4e9c95c64113d3fe3bb6"}},"fb4d13679b8f4c738b7b9691644c00c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fba6f53acaee45dd88aff72672361e0b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}