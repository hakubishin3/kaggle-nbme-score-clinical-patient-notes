{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "blind-kingdom",
   "metadata": {
    "id": "aa1f8e80"
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-glenn",
   "metadata": {
    "id": "c0138fac"
   },
   "source": [
    "- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-ministry",
   "metadata": {
    "id": "cf1dfda9"
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deadly-confidence",
   "metadata": {
    "id": "a7a78d25"
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"nbme-exp019\"\n",
    "ENV = \"local\"\n",
    "DEBUG_MODE = False\n",
    "SUBMISSION_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aware-worcester",
   "metadata": {
    "id": "4ecc4e4d"
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    env=ENV\n",
    "    exp_name=EXP_NAME\n",
    "    debug=DEBUG_MODE\n",
    "    submission=SUBMISSION_MODE\n",
    "    apex=True\n",
    "    input_dir=None\n",
    "    output_dir=None\n",
    "    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n",
    "    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n",
    "    competition_name=\"nbme-score-clinical-patient-notes\"\n",
    "    id_col=\"id\"\n",
    "    target_col=\"location\"\n",
    "    pretrained_model_name=\"microsoft/deberta-large\"\n",
    "    tokenizer=None\n",
    "    max_len=None\n",
    "    output_dim=1\n",
    "    dropout=0.2\n",
    "    num_workers=4\n",
    "    batch_size=4\n",
    "    lr=2e-5\n",
    "    betas=(0.9, 0.98)\n",
    "    weight_decay=0.1\n",
    "    num_warmup_steps_rate=0.1\n",
    "    batch_scheduler=True\n",
    "    epochs=5\n",
    "    n_fold=5\n",
    "    train_fold=[0, 1, 2, 3, 4]\n",
    "    seed=71\n",
    "    gradient_accumulation_steps=2\n",
    "    max_grad_norm=1000\n",
    "    print_freq=100\n",
    "    train=True\n",
    "    inference=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "personalized-death",
   "metadata": {
    "id": "3894c88b"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.train_fold = [0, 1]\n",
    "\n",
    "if CFG.submission:\n",
    "    CFG.train = False\n",
    "    CFG.inference = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-neutral",
   "metadata": {
    "id": "31768c85"
   },
   "source": [
    "## Directory Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "checked-boards",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4693,
     "status": "ok",
     "timestamp": 1646023773081,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "00e7d967",
    "outputId": "d56a483d-9171-44e6-856a-a90dfe8e0ac3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "print(CFG.env)\n",
    "if CFG.env == \"colab\":\n",
    "    # colab環境\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "    # install packages\n",
    "    !pip install transformers\n",
    "\n",
    "elif CFG.env == \"local\":\n",
    "    # ローカルサーバ\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "\n",
    "elif CFG.env == \"kaggle\":\n",
    "    # kaggle環境\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vital-mexico",
   "metadata": {
    "id": "d726b7d9"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import BartModel,BertModel,BertTokenizer\n",
    "from transformers import DebertaModel,DebertaTokenizer\n",
    "from transformers import RobertaModel,RobertaTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-ladder",
   "metadata": {
    "id": "b6d82f71"
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "desperate-keyboard",
   "metadata": {
    "id": "95abbe2c"
   },
   "outputs": [],
   "source": [
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "flexible-wednesday",
   "metadata": {
    "id": "832ee36d"
   },
   "outputs": [],
   "source": [
    "def create_labels_for_scoring(df):\n",
    "    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n",
    "    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, \"location\"]\n",
    "        if lst:\n",
    "            new_lst = \";\".join(lst)\n",
    "            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n",
    "\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df[\"location_for_create_labels\"].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, token_probs, tokenizer):\n",
    "    res = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n",
    "        encoded = tokenizer(\n",
    "            text=text,\n",
    "            max_length=CFG.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n",
    "            start, end = offset_mapping\n",
    "            res[i][start:end] = pred\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_predicted_location_str(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def scoring(df, th=0.5):\n",
    "    labels = create_labels_for_scoring(df)\n",
    "\n",
    "    token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n",
    "    char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n",
    "    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n",
    "    preds = get_predictions(predicted_location_str)\n",
    "\n",
    "    score = get_score(labels, preds)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_best_thres(oof_df):\n",
    "    def f1_opt(x):\n",
    "        return -1 * scoring(oof_df, th=x)\n",
    "\n",
    "    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n",
    "    return best_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "logical-chemistry",
   "metadata": {
    "id": "918828a7"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "gorgeous-record",
   "metadata": {
    "id": "d02a78e1"
   },
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-africa",
   "metadata": {
    "id": "47266f39"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "shaped-metallic",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 815,
     "status": "ok",
     "timestamp": 1646023777557,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "20fed6da",
    "outputId": "64d3e7ad-0986-4799-f9df-f0242c1977a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 6), (143, 3), (42146, 3), (5, 4))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(CFG.input_dir / \"train.csv\")\n",
    "features = pd.read_csv(CFG.input_dir / \"features.csv\")\n",
    "patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n",
    "test = pd.read_csv(CFG.input_dir / \"test.csv\")\n",
    "\n",
    "train.shape, features.shape, patient_notes.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "visible-australia",
   "metadata": {
    "id": "e67d0132"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n",
    "    print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-gibson",
   "metadata": {
    "id": "47bca11a"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "interpreted-northeast",
   "metadata": {
    "id": "d9c8e9ba"
   },
   "outputs": [],
   "source": [
    "def preprocess_features(features):\n",
    "    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "\n",
    "\n",
    "features = preprocess_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "martial-blind",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1646023777558,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "7ef41e18",
    "outputId": "31edaa7d-c088-495a-95ee-f0d56f97074c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 8), (5, 6))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "electoral-favor",
   "metadata": {
    "id": "8233df16"
   },
   "outputs": [],
   "source": [
    "train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n",
    "train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "reported-parade",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1646023778018,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "e9143e61",
    "outputId": "cf45e2d7-5f66-4d96-c6e2-da79c888bcc8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4399\n",
       "1    8181\n",
       "2    1296\n",
       "3     287\n",
       "4      99\n",
       "5      27\n",
       "6       9\n",
       "7       1\n",
       "8       1\n",
       "Name: annotation_length, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n",
    "display(train['annotation_length'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-relevance",
   "metadata": {
    "id": "6bdc7949"
   },
   "source": [
    "## CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "mature-coalition",
   "metadata": {
    "id": "c4acf61d"
   },
   "outputs": [],
   "source": [
    "def get_groupkfold(df, group_name):\n",
    "    groups = df[group_name].unique()\n",
    "\n",
    "    kf = KFold(\n",
    "        n_splits=CFG.n_fold,\n",
    "        shuffle=True,\n",
    "        random_state=CFG.seed,\n",
    "    )\n",
    "    folds_ids = []\n",
    "    for i_fold, (_, val_group_idx) in enumerate(kf.split(groups)):\n",
    "        val_group = groups[val_group_idx]\n",
    "        is_val = df[group_name].isin(val_group)\n",
    "        val_idx = df[is_val].index\n",
    "        df.loc[val_idx, \"fold\"] = int(i_fold)\n",
    "\n",
    "    df[\"fold\"] = df[\"fold\"].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "every-minutes",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1646023778018,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "2ca0c08e",
    "outputId": "cfc9c06e-e30c-4cb5-a072-d0cfcfa5fdc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    2902\n",
       "1    2894\n",
       "2    2813\n",
       "3    2791\n",
       "4    2900\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = get_groupkfold(train, \"pn_num\")\n",
    "display(train.groupby(\"fold\").size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-entrance",
   "metadata": {
    "id": "a8560070"
   },
   "source": [
    "## Setup tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dramatic-afghanistan",
   "metadata": {
    "id": "c316b13f"
   },
   "outputs": [],
   "source": [
    "if CFG.submission:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG.pretrained_model_name)\n",
    "    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n",
    "\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-arrow",
   "metadata": {
    "id": "e689a7fc"
   },
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "immune-campbell",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "a31c60ff4dab48e08d2ef9293d85df6d",
      "c40d970496ff447a8c0b80d787b07a4d",
      "40e6583408c447199ff5b94d23601936",
      "1141ae38bc6f473aab89db14fa4eeacf",
      "47b8a7f3d0544d79b30ad02e4222082e",
      "0260998578564385a0b5b9425a0a5ca1",
      "428ca357bd284d199e2558b1f577d79a",
      "2e32fee744ef42e0aaa89a7b03e82427",
      "d51d3aa414db4aa8b0ccae896e671152",
      "ad49cbf6b6e84ccaab873458182f22a1",
      "5375de82ce3a41a8b5550e0a6b4316c1"
     ]
    },
    "executionInfo": {
     "elapsed": 37449,
     "status": "ok",
     "timestamp": 1646023819498,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "df31758e",
    "outputId": "e3ee6910-2896-413b-9bb7-1e1dd630166c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb90d2fce9214a4fa1d020b160b2257d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 433\n"
     ]
    }
   ],
   "source": [
    "pn_history_lengths = []\n",
    "tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    pn_history_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(pn_history_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "northern-branch",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "8a503d1abd884514a1e23101e03c6781",
      "e06e5e9eb0414b6fad63bdc99b44a313",
      "6b04b019813e458080f02bc9111433a6",
      "2e3818222bab4603a896be5976cb8409",
      "eeb468dbb94943fcb30219d4dd98fcab",
      "5e66444e9c714134bd2765cb3b6d1f15",
      "220f78b6119042af8729543465e1234e",
      "7f1d7796e2174485a0d1b1e9a71d7ade",
      "e72cad76f875451a8e2479e2df237575",
      "9754a5f1e61d49c8972df40ee9290375",
      "25bf78e432e641e0a435dc3626c3ee8a"
     ]
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1646023819500,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "3caff24a",
    "outputId": "09841871-9f3a-4e70-a528-07122a0ebba2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63970e450fdd47adb63ee4d115cdb334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 30\n"
     ]
    }
   ],
   "source": [
    "feature_text_lengths = []\n",
    "tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    feature_text_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(feature_text_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "oriental-jacksonville",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1646023819500,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "756d83ff",
    "outputId": "02d1e748-4ce8-4d68-f2a2-175f08316e9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 466\n"
     ]
    }
   ],
   "source": [
    "CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n",
    "\n",
    "print(\"max length:\", CFG.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "flexible-trainer",
   "metadata": {
    "id": "054b899a"
   },
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "        self.annotation_lengths = self.df[\"annotation_length\"].values\n",
    "        self.locations = self.df[\"location\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def _create_label(self, pn_history, annotation_length, location_list):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        offset_mapping = encoded[\"offset_mapping\"]\n",
    "        ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n",
    "        label = np.zeros(len(offset_mapping))\n",
    "        label[ignore_idxes] = -1\n",
    "\n",
    "        if annotation_length > 0:\n",
    "            for location in location_list:\n",
    "                for loc in [s.split() for s in location.split(\";\")]:\n",
    "                    start, end = int(loc[0]), int(loc[1])\n",
    "                    start_idx = -1\n",
    "                    end_idx = -1\n",
    "                    for idx in range(len(offset_mapping)):\n",
    "                        if (start_idx == -1) & (start < offset_mapping[idx][0]):\n",
    "                            start_idx = idx - 1\n",
    "                        if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n",
    "                            end_idx = idx + 1\n",
    "                    if start_idx == -1:\n",
    "                        start_idx = end_idx\n",
    "                    if (start_idx != -1) & (end_idx != -1):\n",
    "                        label[start_idx:end_idx] = 1\n",
    "\n",
    "        return torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n",
    "        return input_, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "stock-robertson",
   "metadata": {
    "id": "1d58367c"
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        return input_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-lucas",
   "metadata": {
    "id": "8c57abef"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "animated-array",
   "metadata": {
    "id": "54f92d89"
   },
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, model_config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if model_config_path is None:\n",
    "            self.model_config = AutoConfig.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "        else:\n",
    "            self.model_config = torch.load(model_config_path)\n",
    "\n",
    "        if pretrained:\n",
    "            self.backbone = AutoModel.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                config=self.model_config,\n",
    "            )\n",
    "            print(f\"Load weight from pretrained\")\n",
    "        else:\n",
    "            #self.backbone = AutoModel.from_config(self.model_config)\n",
    "            itpt = AutoModelForMaskedLM.from_config(self.model_config)\n",
    "            #path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp009/checkpoint-129000/pytorch_model.bin\")\n",
    "            path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n",
    "            state_dict = torch.load(path)\n",
    "            itpt.load_state_dict(state_dict)\n",
    "            self.backbone = itpt.deberta\n",
    "            print(f\"Load weight from {path}\")\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(self.cfg.dropout),\n",
    "            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h = self.backbone(**inputs)[\"last_hidden_state\"]\n",
    "        output = self.fc(h)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-bristol",
   "metadata": {
    "id": "91401041"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "talented-quantity",
   "metadata": {
    "id": "eda8175d"
   },
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    train_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    epoch,\n",
    "    scheduler,\n",
    "    device,\n",
    "):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels) in enumerate(train_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            output = model(inputs)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "\n",
    "        pos_nums = (labels == 1).sum(axis=1)\n",
    "        pos_nums = torch.tensor([[pos_num] * CFG.max_len for pos_num in pos_nums]).view(-1, 1).to(device)\n",
    "        pos_nums = torch.masked_select(pos_nums, labels.view(-1, 1) != -1)\n",
    "        weight = []\n",
    "        for pos_num in pos_nums:\n",
    "            if pos_num == 0:\n",
    "                weight.append(3.0)\n",
    "            else:\n",
    "                weight.append(1.0)\n",
    "        weight = torch.tensor(weight).to(device)\n",
    "        loss = loss * weight\n",
    "\n",
    "        loss = loss.mean()\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if CFG.batch_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n",
    "            print(\n",
    "                \"Epoch: [{0}][{1}/{2}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                \"Grad: {grad_norm:.4f}  \"\n",
    "                \"LR: {lr:.6f}  \"\n",
    "                .format(\n",
    "                    epoch+1,\n",
    "                    step,\n",
    "                    len(train_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n",
    "                    loss=losses,\n",
    "                     grad_norm=grad_norm,\n",
    "                     lr=scheduler.get_lr()[0],\n",
    "                )\n",
    "            )\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "figured-cooperative",
   "metadata": {
    "id": "c44b63a7"
   },
   "outputs": [],
   "source": [
    "def valid_fn(\n",
    "    val_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    device,\n",
    "):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels) in enumerate(val_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "\n",
    "        pos_nums = (labels == 1).sum(axis=1)\n",
    "        pos_nums = torch.tensor([[pos_num] * CFG.max_len for pos_num in pos_nums]).view(-1, 1).to(device)\n",
    "        pos_nums = torch.masked_select(pos_nums, labels.view(-1, 1) != -1)\n",
    "        weight = []\n",
    "        for pos_num in pos_nums:\n",
    "            if pos_num == 0:\n",
    "                weight.append(3.0)\n",
    "            else:\n",
    "                weight.append(1.0)\n",
    "        weight = torch.tensor(weight).to(device)\n",
    "        loss = loss * weight\n",
    "\n",
    "        loss = loss.mean()\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n",
    "            print(\n",
    "                \"EVAL: [{0}/{1}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                .format(\n",
    "                    step, len(val_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n",
    "                    loss=losses,\n",
    "                )\n",
    "            )\n",
    "    preds = np.concatenate(preds)\n",
    "    return losses.avg, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "played-pointer",
   "metadata": {
    "id": "4219ac38"
   },
   "outputs": [],
   "source": [
    "def inference_fn(test_dataloader, model, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preds = []\n",
    "    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "brazilian-nigeria",
   "metadata": {
    "id": "014a76b7"
   },
   "outputs": [],
   "source": [
    "def train_loop(df, i_fold, device):\n",
    "    print(f\"========== fold: {i_fold} training ==========\")\n",
    "    train_idx = df[df[\"fold\"] != i_fold].index\n",
    "    val_idx = df[df[\"fold\"] == i_fold].index\n",
    "\n",
    "    train_folds = df.loc[train_idx].reset_index(drop=True)\n",
    "    val_folds = df.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = TrainingDataset(CFG, train_folds)\n",
    "    val_dataset = TrainingDataset(CFG, val_folds)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "    model = CustomModel(CFG, model_config_path=None, pretrained=False)   # itptを使うため\n",
    "    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n",
    "    model.to(device)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in param_optimizer if not any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n",
    "        {\"params\": [p for n, p in param_optimizer if any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=CFG.lr,\n",
    "        betas=CFG.betas,\n",
    "        weight_decay=CFG.weight_decay,\n",
    "    )\n",
    "    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n",
    "    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_train_optimization_steps,\n",
    "    )\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    best_score = -1 * np.inf\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "        avg_loss = train_fn(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            scheduler,\n",
    "            device,\n",
    "        )\n",
    "        avg_val_loss, val_preds = valid_fn(\n",
    "            val_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "\n",
    "        # scoring\n",
    "        val_folds[[str(i) for i in range(CFG.max_len)]] = val_preds\n",
    "        score = scoring(val_folds, th=0.5)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n",
    "        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"predictions\": val_preds,\n",
    "                },\n",
    "                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "            )\n",
    "\n",
    "    predictions = torch.load(\n",
    "        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )[\"predictions\"]\n",
    "    val_folds[[str(i) for i in range(CFG.max_len)]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return val_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-switch",
   "metadata": {
    "id": "c38fb834"
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "desperate-crime",
   "metadata": {
    "id": "62d677cd"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for i_fold in range(CFG.n_fold):\n",
    "            if i_fold in CFG.train_fold:\n",
    "                _oof_df = train_loop(train, i_fold, device)\n",
    "                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n",
    "        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    if CFG.submission:\n",
    "        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n",
    "    else:\n",
    "        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    score = scoring(oof_df, th=0.5)\n",
    "    print(f\"Best thres: 0.5, Score: {score:.4f}\")\n",
    "    best_thres = get_best_thres(oof_df)\n",
    "    score = scoring(oof_df, th=best_thres)\n",
    "    print(f\"Best thres: {best_thres}, Score: {score:.4f}\")\n",
    "\n",
    "    if CFG.inference:\n",
    "        test_dataset = TestDataset(CFG, test)\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=CFG.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=CFG.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        predictions = []\n",
    "        for i_fold in CFG.train_fold:\n",
    "            if CFG.submission:\n",
    "                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n",
    "                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n",
    "            else:\n",
    "                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n",
    "\n",
    "            state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "            model.load_state_dict(state[\"model\"])\n",
    "            test_token_probs = inference_fn(test_dataloader, model, device)\n",
    "            test[[f\"fold{i_fold}_{i}\" for i in range(CFG.max_len)]] = test_token_probs\n",
    "            test_char_probs = get_char_probs(test[\"pn_history\"].values, test_token_probs, CFG.tokenizer)\n",
    "            predictions.append(test_char_probs)\n",
    "\n",
    "            del state, test_token_probs, model; gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        predictions = np.mean(predictions, axis=0)\n",
    "        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n",
    "        test[CFG.target_col] = predicted_location_str\n",
    "        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n",
    "        test[[CFG.id_col, CFG.target_col]].to_csv(\n",
    "            CFG.output_dir / \"submission.csv\", index=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "graduate-vision",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "5a00641beddc46eb8da430f2d9999490",
      "04966868d2974cb8b3215a50572c2c94",
      "5a5c41748cba4234a6a6f9aabddfa861",
      "72044a7dbe7d4b5f839f38f6e827ec63",
      "bed6a691643a46d5bd25e03cdc5b73f7",
      "b4d0bd0dea5341a9b03f0092fe3cba39"
     ]
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1646034258180,
     "user": {
      "displayName": "Shuhei Goda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08246931244224045522"
     },
     "user_tz": -540
    },
    "id": "1d4fcf7c",
    "outputId": "1362d223-3d70-4ba7-daa5-14b7300eef5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2849] Elapsed 0m 1s (remain 58m 45s) Loss: 0.4680(0.4680) Grad: inf  LR: 0.000000  \n",
      "Epoch: [1][100/2849] Elapsed 0m 50s (remain 23m 4s) Loss: 0.3190(0.4386) Grad: 54221.9375  LR: 0.000001  \n",
      "Epoch: [1][200/2849] Elapsed 1m 40s (remain 22m 5s) Loss: 0.0869(0.3411) Grad: 6981.6616  LR: 0.000003  \n",
      "Epoch: [1][300/2849] Elapsed 2m 29s (remain 21m 9s) Loss: 0.0374(0.2434) Grad: 852.8734  LR: 0.000004  \n",
      "Epoch: [1][400/2849] Elapsed 3m 20s (remain 20m 21s) Loss: 0.0816(0.1933) Grad: 1578.8778  LR: 0.000006  \n",
      "Epoch: [1][500/2849] Elapsed 4m 10s (remain 19m 32s) Loss: 0.0309(0.1619) Grad: 937.6583  LR: 0.000007  \n",
      "Epoch: [1][600/2849] Elapsed 5m 0s (remain 18m 42s) Loss: 0.0368(0.1387) Grad: 8138.3501  LR: 0.000008  \n",
      "Epoch: [1][700/2849] Elapsed 5m 50s (remain 17m 54s) Loss: 0.0699(0.1217) Grad: 6198.0088  LR: 0.000010  \n",
      "Epoch: [1][800/2849] Elapsed 6m 41s (remain 17m 5s) Loss: 0.0060(0.1084) Grad: 807.5145  LR: 0.000011  \n",
      "Epoch: [1][900/2849] Elapsed 7m 31s (remain 16m 16s) Loss: 0.0054(0.0979) Grad: 630.7219  LR: 0.000013  \n",
      "Epoch: [1][1000/2849] Elapsed 8m 21s (remain 15m 25s) Loss: 0.0082(0.0893) Grad: 1308.4385  LR: 0.000014  \n",
      "Epoch: [1][1100/2849] Elapsed 9m 10s (remain 14m 34s) Loss: 0.0092(0.0822) Grad: 2203.1423  LR: 0.000015  \n",
      "Epoch: [1][1200/2849] Elapsed 10m 0s (remain 13m 44s) Loss: 0.0054(0.0763) Grad: 599.2184  LR: 0.000017  \n",
      "Epoch: [1][1300/2849] Elapsed 10m 50s (remain 12m 54s) Loss: 0.0073(0.0711) Grad: 693.2661  LR: 0.000018  \n",
      "Epoch: [1][1400/2849] Elapsed 11m 41s (remain 12m 4s) Loss: 0.0132(0.0667) Grad: 1828.4723  LR: 0.000020  \n",
      "Epoch: [1][1500/2849] Elapsed 12m 30s (remain 11m 14s) Loss: 0.0039(0.0628) Grad: 363.3955  LR: 0.000020  \n",
      "Epoch: [1][1600/2849] Elapsed 13m 20s (remain 10m 23s) Loss: 0.0072(0.0596) Grad: 585.2568  LR: 0.000020  \n",
      "Epoch: [1][1700/2849] Elapsed 14m 10s (remain 9m 33s) Loss: 0.0067(0.0567) Grad: 813.5896  LR: 0.000020  \n",
      "Epoch: [1][1800/2849] Elapsed 14m 59s (remain 8m 43s) Loss: 0.0020(0.0539) Grad: 261.2776  LR: 0.000019  \n",
      "Epoch: [1][1900/2849] Elapsed 15m 49s (remain 7m 53s) Loss: 0.0001(0.0516) Grad: 16.2247  LR: 0.000019  \n",
      "Epoch: [1][2000/2849] Elapsed 16m 39s (remain 7m 3s) Loss: 0.0030(0.0494) Grad: 384.2924  LR: 0.000019  \n",
      "Epoch: [1][2100/2849] Elapsed 17m 29s (remain 6m 13s) Loss: 0.0137(0.0474) Grad: 1963.9025  LR: 0.000019  \n",
      "Epoch: [1][2200/2849] Elapsed 18m 19s (remain 5m 23s) Loss: 0.0001(0.0456) Grad: 25.7079  LR: 0.000019  \n",
      "Epoch: [1][2300/2849] Elapsed 19m 8s (remain 4m 33s) Loss: 0.0804(0.0439) Grad: 1369.2407  LR: 0.000019  \n",
      "Epoch: [1][2400/2849] Elapsed 19m 58s (remain 3m 43s) Loss: 0.0032(0.0424) Grad: 159.9443  LR: 0.000018  \n",
      "Epoch: [1][2500/2849] Elapsed 20m 47s (remain 2m 53s) Loss: 0.0031(0.0410) Grad: 228.3253  LR: 0.000018  \n",
      "Epoch: [1][2600/2849] Elapsed 21m 36s (remain 2m 3s) Loss: 0.0014(0.0397) Grad: 66.0907  LR: 0.000018  \n",
      "Epoch: [1][2700/2849] Elapsed 22m 25s (remain 1m 13s) Loss: 0.0039(0.0385) Grad: 280.1574  LR: 0.000018  \n",
      "Epoch: [1][2800/2849] Elapsed 23m 14s (remain 0m 23s) Loss: 0.0026(0.0375) Grad: 106.3157  LR: 0.000018  \n",
      "Epoch: [1][2848/2849] Elapsed 23m 38s (remain 0m 0s) Loss: 0.0049(0.0370) Grad: 402.2635  LR: 0.000018  \n",
      "EVAL: [0/726] Elapsed 0m 0s (remain 6m 5s) Loss: 0.0024(0.0024) \n",
      "EVAL: [100/726] Elapsed 0m 27s (remain 2m 52s) Loss: 0.0030(0.0063) \n",
      "EVAL: [200/726] Elapsed 0m 55s (remain 2m 24s) Loss: 0.0005(0.0071) \n",
      "EVAL: [300/726] Elapsed 1m 22s (remain 1m 56s) Loss: 0.0002(0.0066) \n",
      "EVAL: [400/726] Elapsed 1m 50s (remain 1m 29s) Loss: 0.0054(0.0082) \n",
      "EVAL: [500/726] Elapsed 2m 17s (remain 1m 1s) Loss: 0.0139(0.0081) \n",
      "EVAL: [600/726] Elapsed 2m 44s (remain 0m 34s) Loss: 0.0032(0.0076) \n",
      "EVAL: [700/726] Elapsed 3m 12s (remain 0m 6s) Loss: 0.0021(0.0073) \n",
      "EVAL: [725/726] Elapsed 3m 18s (remain 0m 0s) Loss: 0.0005(0.0072) \n",
      "Epoch 1 - avg_train_loss: 0.0370  avg_val_loss: 0.0072  time: 1623s\n",
      "Epoch 1 - Score: 0.8601\n",
      "Epoch 1 - Save Best Score: 0.8601 Model\n",
      "Epoch: [2][0/2849] Elapsed 0m 0s (remain 36m 6s) Loss: 0.0065(0.0065) Grad: 5447.1030  LR: 0.000018  \n",
      "Epoch: [2][100/2849] Elapsed 0m 51s (remain 23m 27s) Loss: 0.0042(0.0068) Grad: 11439.7852  LR: 0.000018  \n",
      "Epoch: [2][200/2849] Elapsed 1m 41s (remain 22m 21s) Loss: 0.0004(0.0072) Grad: 1160.4572  LR: 0.000017  \n",
      "Epoch: [2][300/2849] Elapsed 2m 31s (remain 21m 25s) Loss: 0.0043(0.0065) Grad: 4548.4243  LR: 0.000017  \n",
      "Epoch: [2][400/2849] Elapsed 3m 21s (remain 20m 32s) Loss: 0.0074(0.0061) Grad: 34142.0664  LR: 0.000017  \n",
      "Epoch: [2][500/2849] Elapsed 4m 11s (remain 19m 40s) Loss: 0.0021(0.0061) Grad: 2389.0862  LR: 0.000017  \n",
      "Epoch: [2][600/2849] Elapsed 5m 2s (remain 18m 51s) Loss: 0.0006(0.0060) Grad: 917.8663  LR: 0.000017  \n",
      "Epoch: [2][700/2849] Elapsed 5m 52s (remain 18m 0s) Loss: 0.0000(0.0059) Grad: 156.5087  LR: 0.000017  \n",
      "Epoch: [2][800/2849] Elapsed 6m 42s (remain 17m 10s) Loss: 0.0010(0.0059) Grad: 12788.2539  LR: 0.000017  \n",
      "Epoch: [2][900/2849] Elapsed 7m 33s (remain 16m 19s) Loss: 0.0077(0.0060) Grad: 10908.5820  LR: 0.000016  \n",
      "Epoch: [2][1000/2849] Elapsed 8m 23s (remain 15m 29s) Loss: 0.0136(0.0060) Grad: 7590.4395  LR: 0.000016  \n",
      "Epoch: [2][1100/2849] Elapsed 9m 14s (remain 14m 39s) Loss: 0.0012(0.0059) Grad: 1929.1815  LR: 0.000016  \n",
      "Epoch: [2][1200/2849] Elapsed 10m 3s (remain 13m 48s) Loss: 0.0196(0.0059) Grad: 27364.0859  LR: 0.000016  \n",
      "Epoch: [2][1300/2849] Elapsed 10m 53s (remain 12m 58s) Loss: 0.0032(0.0060) Grad: 6095.2124  LR: 0.000016  \n",
      "Epoch: [2][1400/2849] Elapsed 11m 44s (remain 12m 7s) Loss: 0.0001(0.0060) Grad: 208.8000  LR: 0.000016  \n",
      "Epoch: [2][1500/2849] Elapsed 12m 34s (remain 11m 18s) Loss: 0.0030(0.0061) Grad: 2875.9353  LR: 0.000015  \n",
      "Epoch: [2][1600/2849] Elapsed 13m 25s (remain 10m 27s) Loss: 0.0294(0.0060) Grad: 61794.0781  LR: 0.000015  \n",
      "Epoch: [2][1700/2849] Elapsed 14m 15s (remain 9m 37s) Loss: 0.0049(0.0061) Grad: 4343.4277  LR: 0.000015  \n",
      "Epoch: [2][1800/2849] Elapsed 15m 6s (remain 8m 47s) Loss: 0.0044(0.0061) Grad: 4103.5620  LR: 0.000015  \n",
      "Epoch: [2][1900/2849] Elapsed 15m 56s (remain 7m 56s) Loss: 0.0017(0.0061) Grad: 2349.4270  LR: 0.000015  \n",
      "Epoch: [2][2000/2849] Elapsed 16m 46s (remain 7m 6s) Loss: 0.0016(0.0061) Grad: 1707.2762  LR: 0.000015  \n",
      "Epoch: [2][2100/2849] Elapsed 17m 37s (remain 6m 16s) Loss: 0.0004(0.0061) Grad: 669.1428  LR: 0.000014  \n",
      "Epoch: [2][2200/2849] Elapsed 18m 27s (remain 5m 26s) Loss: 0.0241(0.0060) Grad: 24350.5664  LR: 0.000014  \n",
      "Epoch: [2][2300/2849] Elapsed 19m 17s (remain 4m 35s) Loss: 0.0029(0.0060) Grad: 4060.5388  LR: 0.000014  \n",
      "Epoch: [2][2400/2849] Elapsed 20m 7s (remain 3m 45s) Loss: 0.0049(0.0060) Grad: 6143.8384  LR: 0.000014  \n",
      "Epoch: [2][2500/2849] Elapsed 20m 57s (remain 2m 55s) Loss: 0.0003(0.0060) Grad: 601.2382  LR: 0.000014  \n",
      "Epoch: [2][2600/2849] Elapsed 21m 48s (remain 2m 4s) Loss: 0.0209(0.0060) Grad: 13459.1348  LR: 0.000014  \n",
      "Epoch: [2][2700/2849] Elapsed 22m 39s (remain 1m 14s) Loss: 0.0027(0.0060) Grad: 2602.1848  LR: 0.000014  \n",
      "Epoch: [2][2800/2849] Elapsed 23m 30s (remain 0m 24s) Loss: 0.0002(0.0059) Grad: 553.4805  LR: 0.000013  \n",
      "Epoch: [2][2848/2849] Elapsed 23m 54s (remain 0m 0s) Loss: 0.0002(0.0060) Grad: 888.4745  LR: 0.000013  \n",
      "EVAL: [0/726] Elapsed 0m 0s (remain 5m 58s) Loss: 0.0013(0.0013) \n",
      "EVAL: [100/726] Elapsed 0m 27s (remain 2m 50s) Loss: 0.0030(0.0081) \n",
      "EVAL: [200/726] Elapsed 0m 55s (remain 2m 23s) Loss: 0.0001(0.0074) \n",
      "EVAL: [300/726] Elapsed 1m 22s (remain 1m 55s) Loss: 0.0002(0.0071) \n",
      "EVAL: [400/726] Elapsed 1m 49s (remain 1m 28s) Loss: 0.0040(0.0091) \n",
      "EVAL: [500/726] Elapsed 2m 16s (remain 1m 1s) Loss: 0.0255(0.0091) \n",
      "EVAL: [600/726] Elapsed 2m 44s (remain 0m 34s) Loss: 0.0010(0.0084) \n",
      "EVAL: [700/726] Elapsed 3m 11s (remain 0m 6s) Loss: 0.0034(0.0082) \n",
      "EVAL: [725/726] Elapsed 3m 17s (remain 0m 0s) Loss: 0.0000(0.0081) \n",
      "Epoch 2 - avg_train_loss: 0.0060  avg_val_loss: 0.0081  time: 1638s\n",
      "Epoch 2 - Score: 0.8770\n",
      "Epoch 2 - Save Best Score: 0.8770 Model\n",
      "Epoch: [3][0/2849] Elapsed 0m 0s (remain 35m 41s) Loss: 0.0010(0.0010) Grad: 33392.8906  LR: 0.000013  \n",
      "Epoch: [3][100/2849] Elapsed 0m 51s (remain 23m 19s) Loss: 0.0002(0.0032) Grad: 860.1767  LR: 0.000013  \n",
      "Epoch: [3][200/2849] Elapsed 1m 41s (remain 22m 11s) Loss: 0.0103(0.0034) Grad: 6645.3472  LR: 0.000013  \n",
      "Epoch: [3][300/2849] Elapsed 2m 30s (remain 21m 17s) Loss: 0.0000(0.0036) Grad: 128.7362  LR: 0.000013  \n",
      "Epoch: [3][400/2849] Elapsed 3m 20s (remain 20m 25s) Loss: 0.0003(0.0038) Grad: 974.0513  LR: 0.000013  \n",
      "Epoch: [3][500/2849] Elapsed 4m 11s (remain 19m 36s) Loss: 0.0055(0.0037) Grad: 6263.3423  LR: 0.000013  \n",
      "Epoch: [3][600/2849] Elapsed 5m 0s (remain 18m 45s) Loss: 0.0301(0.0040) Grad: 9805.2236  LR: 0.000012  \n",
      "Epoch: [3][700/2849] Elapsed 5m 51s (remain 17m 57s) Loss: 0.0057(0.0042) Grad: 12790.1650  LR: 0.000012  \n",
      "Epoch: [3][800/2849] Elapsed 6m 41s (remain 17m 7s) Loss: 0.0000(0.0042) Grad: 151.9216  LR: 0.000012  \n",
      "Epoch: [3][900/2849] Elapsed 7m 31s (remain 16m 16s) Loss: 0.0023(0.0043) Grad: 3455.3315  LR: 0.000012  \n",
      "Epoch: [3][1000/2849] Elapsed 8m 21s (remain 15m 26s) Loss: 0.0050(0.0043) Grad: 4050.8269  LR: 0.000012  \n",
      "Epoch: [3][1100/2849] Elapsed 9m 11s (remain 14m 36s) Loss: 0.0051(0.0044) Grad: 7272.1284  LR: 0.000012  \n",
      "Epoch: [3][1200/2849] Elapsed 10m 2s (remain 13m 46s) Loss: 0.0000(0.0044) Grad: 13.0525  LR: 0.000011  \n",
      "Epoch: [3][1300/2849] Elapsed 10m 52s (remain 12m 56s) Loss: 0.0006(0.0046) Grad: 1267.9907  LR: 0.000011  \n",
      "Epoch: [3][1400/2849] Elapsed 11m 42s (remain 12m 6s) Loss: 0.0043(0.0047) Grad: 3445.0874  LR: 0.000011  \n",
      "Epoch: [3][1500/2849] Elapsed 12m 32s (remain 11m 15s) Loss: 0.0002(0.0046) Grad: 782.2497  LR: 0.000011  \n",
      "Epoch: [3][1600/2849] Elapsed 13m 21s (remain 10m 25s) Loss: 0.0020(0.0045) Grad: 3098.0647  LR: 0.000011  \n",
      "Epoch: [3][1700/2849] Elapsed 14m 12s (remain 9m 35s) Loss: 0.0127(0.0045) Grad: 9692.0342  LR: 0.000011  \n",
      "Epoch: [3][1800/2849] Elapsed 15m 1s (remain 8m 44s) Loss: 0.0016(0.0045) Grad: 10123.1787  LR: 0.000011  \n",
      "Epoch: [3][1900/2849] Elapsed 15m 52s (remain 7m 55s) Loss: 0.0022(0.0045) Grad: 2989.7847  LR: 0.000010  \n",
      "Epoch: [3][2000/2849] Elapsed 16m 42s (remain 7m 5s) Loss: 0.0000(0.0045) Grad: 37.9997  LR: 0.000010  \n",
      "Epoch: [3][2100/2849] Elapsed 17m 33s (remain 6m 14s) Loss: 0.0001(0.0044) Grad: 161.5843  LR: 0.000010  \n",
      "Epoch: [3][2200/2849] Elapsed 18m 22s (remain 5m 24s) Loss: 0.0000(0.0044) Grad: 71.3622  LR: 0.000010  \n",
      "Epoch: [3][2300/2849] Elapsed 19m 12s (remain 4m 34s) Loss: 0.0017(0.0045) Grad: 2258.5894  LR: 0.000010  \n",
      "Epoch: [3][2400/2849] Elapsed 20m 2s (remain 3m 44s) Loss: 0.0107(0.0045) Grad: 20604.5176  LR: 0.000010  \n",
      "Epoch: [3][2500/2849] Elapsed 20m 52s (remain 2m 54s) Loss: 0.0027(0.0045) Grad: 3330.4189  LR: 0.000009  \n",
      "Epoch: [3][2600/2849] Elapsed 21m 43s (remain 2m 4s) Loss: 0.0118(0.0045) Grad: 22601.0156  LR: 0.000009  \n",
      "Epoch: [3][2700/2849] Elapsed 22m 33s (remain 1m 14s) Loss: 0.0000(0.0045) Grad: 23.6585  LR: 0.000009  \n",
      "Epoch: [3][2800/2849] Elapsed 23m 23s (remain 0m 24s) Loss: 0.0160(0.0046) Grad: 7440.5093  LR: 0.000009  \n",
      "Epoch: [3][2848/2849] Elapsed 23m 47s (remain 0m 0s) Loss: 0.0000(0.0046) Grad: 64.8253  LR: 0.000009  \n",
      "EVAL: [0/726] Elapsed 0m 0s (remain 5m 28s) Loss: 0.0006(0.0006) \n",
      "EVAL: [100/726] Elapsed 0m 27s (remain 2m 50s) Loss: 0.0022(0.0075) \n",
      "EVAL: [200/726] Elapsed 0m 55s (remain 2m 23s) Loss: 0.0001(0.0072) \n",
      "EVAL: [300/726] Elapsed 1m 22s (remain 1m 56s) Loss: 0.0000(0.0071) \n",
      "EVAL: [400/726] Elapsed 1m 49s (remain 1m 29s) Loss: 0.0031(0.0089) \n",
      "EVAL: [500/726] Elapsed 2m 17s (remain 1m 1s) Loss: 0.0200(0.0089) \n",
      "EVAL: [600/726] Elapsed 2m 44s (remain 0m 34s) Loss: 0.0049(0.0082) \n",
      "EVAL: [700/726] Elapsed 3m 11s (remain 0m 6s) Loss: 0.0042(0.0081) \n",
      "EVAL: [725/726] Elapsed 3m 18s (remain 0m 0s) Loss: 0.0000(0.0079) \n",
      "Epoch 3 - avg_train_loss: 0.0046  avg_val_loss: 0.0079  time: 1631s\n",
      "Epoch 3 - Score: 0.8827\n",
      "Epoch 3 - Save Best Score: 0.8827 Model\n",
      "Epoch: [4][0/2849] Elapsed 0m 0s (remain 36m 12s) Loss: 0.0010(0.0010) Grad: 3598.9495  LR: 0.000009  \n",
      "Epoch: [4][100/2849] Elapsed 0m 51s (remain 23m 17s) Loss: 0.0028(0.0029) Grad: 11888.0723  LR: 0.000009  \n",
      "Epoch: [4][200/2849] Elapsed 1m 41s (remain 22m 22s) Loss: 0.0067(0.0032) Grad: 12633.9785  LR: 0.000009  \n",
      "Epoch: [4][300/2849] Elapsed 2m 31s (remain 21m 26s) Loss: 0.0009(0.0031) Grad: 5256.6855  LR: 0.000008  \n",
      "Epoch: [4][400/2849] Elapsed 3m 22s (remain 20m 33s) Loss: 0.0011(0.0030) Grad: 3940.4138  LR: 0.000008  \n",
      "Epoch: [4][500/2849] Elapsed 4m 11s (remain 19m 39s) Loss: 0.0000(0.0031) Grad: 75.3200  LR: 0.000008  \n",
      "Epoch: [4][600/2849] Elapsed 5m 1s (remain 18m 49s) Loss: 0.0001(0.0036) Grad: 321.4038  LR: 0.000008  \n",
      "Epoch: [4][700/2849] Elapsed 5m 51s (remain 17m 58s) Loss: 0.0001(0.0035) Grad: 902.2421  LR: 0.000008  \n",
      "Epoch: [4][800/2849] Elapsed 6m 41s (remain 17m 6s) Loss: 0.0057(0.0034) Grad: 14935.1084  LR: 0.000008  \n",
      "Epoch: [4][900/2849] Elapsed 7m 30s (remain 16m 15s) Loss: 0.0063(0.0034) Grad: 31559.5137  LR: 0.000007  \n",
      "Epoch: [4][1000/2849] Elapsed 8m 21s (remain 15m 25s) Loss: 0.0395(0.0034) Grad: 43110.3555  LR: 0.000007  \n",
      "Epoch: [4][1100/2849] Elapsed 9m 11s (remain 14m 35s) Loss: 0.0016(0.0035) Grad: 11770.8662  LR: 0.000007  \n",
      "Epoch: [4][1200/2849] Elapsed 10m 1s (remain 13m 45s) Loss: 0.0019(0.0036) Grad: 19313.3711  LR: 0.000007  \n",
      "Epoch: [4][1300/2849] Elapsed 10m 52s (remain 12m 55s) Loss: 0.0001(0.0034) Grad: 388.3870  LR: 0.000007  \n",
      "Epoch: [4][1400/2849] Elapsed 11m 42s (remain 12m 6s) Loss: 0.0013(0.0035) Grad: 5937.6455  LR: 0.000007  \n",
      "Epoch: [4][1500/2849] Elapsed 12m 32s (remain 11m 15s) Loss: 0.0000(0.0036) Grad: 140.5688  LR: 0.000007  \n",
      "Epoch: [4][1600/2849] Elapsed 13m 22s (remain 10m 25s) Loss: 0.0004(0.0036) Grad: 2674.5085  LR: 0.000006  \n",
      "Epoch: [4][1700/2849] Elapsed 14m 12s (remain 9m 35s) Loss: 0.0050(0.0036) Grad: 13573.5459  LR: 0.000006  \n",
      "Epoch: [4][1800/2849] Elapsed 15m 2s (remain 8m 45s) Loss: 0.0002(0.0035) Grad: 1501.7291  LR: 0.000006  \n",
      "Epoch: [4][1900/2849] Elapsed 15m 53s (remain 7m 55s) Loss: 0.0001(0.0036) Grad: 387.2801  LR: 0.000006  \n",
      "Epoch: [4][2000/2849] Elapsed 16m 43s (remain 7m 5s) Loss: 0.0030(0.0036) Grad: 8117.0088  LR: 0.000006  \n",
      "Epoch: [4][2100/2849] Elapsed 17m 33s (remain 6m 14s) Loss: 0.0001(0.0036) Grad: 728.6862  LR: 0.000006  \n",
      "Epoch: [4][2200/2849] Elapsed 18m 23s (remain 5m 24s) Loss: 0.0001(0.0036) Grad: 299.7931  LR: 0.000005  \n",
      "Epoch: [4][2300/2849] Elapsed 19m 13s (remain 4m 34s) Loss: 0.0000(0.0036) Grad: 36.6309  LR: 0.000005  \n",
      "Epoch: [4][2400/2849] Elapsed 20m 3s (remain 3m 44s) Loss: 0.0273(0.0036) Grad: 43553.3711  LR: 0.000005  \n",
      "Epoch: [4][2500/2849] Elapsed 20m 53s (remain 2m 54s) Loss: 0.0000(0.0036) Grad: 70.9267  LR: 0.000005  \n",
      "Epoch: [4][2600/2849] Elapsed 21m 43s (remain 2m 4s) Loss: 0.0138(0.0036) Grad: 22952.7734  LR: 0.000005  \n",
      "Epoch: [4][2700/2849] Elapsed 22m 34s (remain 1m 14s) Loss: 0.0003(0.0036) Grad: 4143.7959  LR: 0.000005  \n",
      "Epoch: [4][2800/2849] Elapsed 23m 24s (remain 0m 24s) Loss: 0.0000(0.0036) Grad: 64.7554  LR: 0.000005  \n",
      "Epoch: [4][2848/2849] Elapsed 23m 48s (remain 0m 0s) Loss: 0.0000(0.0036) Grad: 53.9092  LR: 0.000004  \n",
      "EVAL: [0/726] Elapsed 0m 0s (remain 6m 6s) Loss: 0.0003(0.0003) \n",
      "EVAL: [100/726] Elapsed 0m 27s (remain 2m 49s) Loss: 0.0034(0.0098) \n",
      "EVAL: [200/726] Elapsed 0m 55s (remain 2m 23s) Loss: 0.0000(0.0091) \n",
      "EVAL: [300/726] Elapsed 1m 22s (remain 1m 56s) Loss: 0.0000(0.0089) \n",
      "EVAL: [400/726] Elapsed 1m 49s (remain 1m 28s) Loss: 0.0019(0.0113) \n",
      "EVAL: [500/726] Elapsed 2m 17s (remain 1m 1s) Loss: 0.0298(0.0112) \n",
      "EVAL: [600/726] Elapsed 2m 44s (remain 0m 34s) Loss: 0.0037(0.0102) \n",
      "EVAL: [700/726] Elapsed 3m 11s (remain 0m 6s) Loss: 0.0049(0.0100) \n",
      "EVAL: [725/726] Elapsed 3m 18s (remain 0m 0s) Loss: 0.0000(0.0099) \n",
      "Epoch 4 - avg_train_loss: 0.0036  avg_val_loss: 0.0099  time: 1632s\n",
      "Epoch 4 - Score: 0.8898\n",
      "Epoch 4 - Save Best Score: 0.8898 Model\n",
      "Epoch: [5][0/2849] Elapsed 0m 0s (remain 35m 46s) Loss: 0.0000(0.0000) Grad: 56.9750  LR: 0.000004  \n",
      "Epoch: [5][100/2849] Elapsed 0m 50s (remain 23m 3s) Loss: 0.0002(0.0035) Grad: 1980.9324  LR: 0.000004  \n",
      "Epoch: [5][200/2849] Elapsed 1m 40s (remain 22m 8s) Loss: 0.0000(0.0032) Grad: 274.5029  LR: 0.000004  \n",
      "Epoch: [5][300/2849] Elapsed 2m 31s (remain 21m 24s) Loss: 0.0000(0.0033) Grad: 158.5166  LR: 0.000004  \n",
      "Epoch: [5][400/2849] Elapsed 3m 21s (remain 20m 31s) Loss: 0.0000(0.0029) Grad: 15.6304  LR: 0.000004  \n",
      "Epoch: [5][500/2849] Elapsed 4m 11s (remain 19m 37s) Loss: 0.0005(0.0026) Grad: 4241.6284  LR: 0.000004  \n",
      "Epoch: [5][600/2849] Elapsed 5m 0s (remain 18m 45s) Loss: 0.0000(0.0029) Grad: 12.8810  LR: 0.000004  \n",
      "Epoch: [5][700/2849] Elapsed 5m 51s (remain 17m 56s) Loss: 0.0042(0.0027) Grad: 9834.7197  LR: 0.000003  \n",
      "Epoch: [5][800/2849] Elapsed 6m 41s (remain 17m 7s) Loss: 0.0002(0.0027) Grad: 1659.0437  LR: 0.000003  \n",
      "Epoch: [5][900/2849] Elapsed 7m 32s (remain 16m 17s) Loss: 0.0078(0.0028) Grad: 35262.6367  LR: 0.000003  \n",
      "Epoch: [5][1000/2849] Elapsed 8m 21s (remain 15m 26s) Loss: 0.0082(0.0027) Grad: 16082.6084  LR: 0.000003  \n",
      "Epoch: [5][1100/2849] Elapsed 9m 11s (remain 14m 35s) Loss: 0.0002(0.0027) Grad: 806.6213  LR: 0.000003  \n",
      "Epoch: [5][1200/2849] Elapsed 10m 1s (remain 13m 45s) Loss: 0.1157(0.0028) Grad: 80812.9531  LR: 0.000003  \n",
      "Epoch: [5][1300/2849] Elapsed 10m 51s (remain 12m 55s) Loss: 0.0000(0.0028) Grad: 4.9433  LR: 0.000002  \n",
      "Epoch: [5][1400/2849] Elapsed 11m 41s (remain 12m 5s) Loss: 0.0005(0.0029) Grad: 4237.0693  LR: 0.000002  \n",
      "Epoch: [5][1500/2849] Elapsed 12m 31s (remain 11m 14s) Loss: 0.0072(0.0028) Grad: 34146.7422  LR: 0.000002  \n",
      "Epoch: [5][1600/2849] Elapsed 13m 20s (remain 10m 24s) Loss: 0.0504(0.0028) Grad: 51213.6641  LR: 0.000002  \n",
      "Epoch: [5][1700/2849] Elapsed 14m 10s (remain 9m 34s) Loss: 0.0000(0.0028) Grad: 53.9328  LR: 0.000002  \n",
      "Epoch: [5][1800/2849] Elapsed 15m 0s (remain 8m 44s) Loss: 0.0129(0.0028) Grad: 21291.2148  LR: 0.000002  \n",
      "Epoch: [5][1900/2849] Elapsed 15m 50s (remain 7m 54s) Loss: 0.0000(0.0029) Grad: 8.7777  LR: 0.000001  \n",
      "Epoch: [5][2000/2849] Elapsed 16m 41s (remain 7m 4s) Loss: 0.0000(0.0029) Grad: 19.9947  LR: 0.000001  \n",
      "Epoch: [5][2100/2849] Elapsed 17m 31s (remain 6m 14s) Loss: 0.0026(0.0029) Grad: 14780.4922  LR: 0.000001  \n",
      "Epoch: [5][2200/2849] Elapsed 18m 21s (remain 5m 24s) Loss: 0.0000(0.0030) Grad: 14.0863  LR: 0.000001  \n",
      "Epoch: [5][2300/2849] Elapsed 19m 11s (remain 4m 34s) Loss: 0.0002(0.0030) Grad: 3825.5044  LR: 0.000001  \n",
      "Epoch: [5][2400/2849] Elapsed 20m 0s (remain 3m 44s) Loss: 0.0005(0.0030) Grad: 3217.6472  LR: 0.000001  \n",
      "Epoch: [5][2500/2849] Elapsed 20m 50s (remain 2m 53s) Loss: 0.0000(0.0030) Grad: 21.1870  LR: 0.000001  \n",
      "Epoch: [5][2600/2849] Elapsed 21m 39s (remain 2m 3s) Loss: 0.0000(0.0030) Grad: 8.3170  LR: 0.000000  \n",
      "Epoch: [5][2700/2849] Elapsed 22m 30s (remain 1m 13s) Loss: 0.0000(0.0030) Grad: 7.9358  LR: 0.000000  \n",
      "Epoch: [5][2800/2849] Elapsed 23m 19s (remain 0m 23s) Loss: 0.0000(0.0029) Grad: 41.6104  LR: 0.000000  \n",
      "Epoch: [5][2848/2849] Elapsed 23m 43s (remain 0m 0s) Loss: 0.0001(0.0030) Grad: 702.7087  LR: 0.000000  \n",
      "EVAL: [0/726] Elapsed 0m 0s (remain 5m 51s) Loss: 0.0005(0.0005) \n",
      "EVAL: [100/726] Elapsed 0m 27s (remain 2m 50s) Loss: 0.0044(0.0104) \n",
      "EVAL: [200/726] Elapsed 0m 55s (remain 2m 24s) Loss: 0.0000(0.0097) \n",
      "EVAL: [300/726] Elapsed 1m 22s (remain 1m 56s) Loss: 0.0000(0.0094) \n",
      "EVAL: [400/726] Elapsed 1m 49s (remain 1m 28s) Loss: 0.0023(0.0117) \n",
      "EVAL: [500/726] Elapsed 2m 17s (remain 1m 1s) Loss: 0.0300(0.0116) \n",
      "EVAL: [600/726] Elapsed 2m 44s (remain 0m 34s) Loss: 0.0025(0.0105) \n",
      "EVAL: [700/726] Elapsed 3m 11s (remain 0m 6s) Loss: 0.0043(0.0104) \n",
      "EVAL: [725/726] Elapsed 3m 18s (remain 0m 0s) Loss: 0.0000(0.0102) \n",
      "Epoch 5 - avg_train_loss: 0.0030  avg_val_loss: 0.0102  time: 1628s\n",
      "Epoch 5 - Score: 0.8904\n",
      "Epoch 5 - Save Best Score: 0.8904 Model\n",
      "========== fold: 1 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2851] Elapsed 0m 0s (remain 32m 53s) Loss: 0.7075(0.7075) Grad: inf  LR: 0.000000  \n",
      "Epoch: [1][100/2851] Elapsed 0m 50s (remain 23m 4s) Loss: 0.5893(0.5217) Grad: 104263.4531  LR: 0.000001  \n",
      "Epoch: [1][200/2851] Elapsed 1m 41s (remain 22m 20s) Loss: 0.1478(0.4031) Grad: 13508.2773  LR: 0.000003  \n",
      "Epoch: [1][300/2851] Elapsed 2m 32s (remain 21m 27s) Loss: 0.0371(0.2913) Grad: 1257.5658  LR: 0.000004  \n",
      "Epoch: [1][400/2851] Elapsed 3m 21s (remain 20m 33s) Loss: 0.0694(0.2289) Grad: 2883.9587  LR: 0.000006  \n",
      "Epoch: [1][500/2851] Elapsed 4m 12s (remain 19m 43s) Loss: 0.0426(0.1906) Grad: 1430.2423  LR: 0.000007  \n",
      "Epoch: [1][600/2851] Elapsed 5m 2s (remain 18m 52s) Loss: 0.0193(0.1636) Grad: 4334.7568  LR: 0.000008  \n",
      "Epoch: [1][700/2851] Elapsed 5m 52s (remain 18m 1s) Loss: 0.0094(0.1427) Grad: 3571.9719  LR: 0.000010  \n",
      "Epoch: [1][800/2851] Elapsed 6m 42s (remain 17m 9s) Loss: 0.0049(0.1272) Grad: 2099.0239  LR: 0.000011  \n",
      "Epoch: [1][900/2851] Elapsed 7m 32s (remain 16m 19s) Loss: 0.0080(0.1146) Grad: 1668.4113  LR: 0.000013  \n",
      "Epoch: [1][1000/2851] Elapsed 8m 23s (remain 15m 29s) Loss: 0.0015(0.1045) Grad: 630.8103  LR: 0.000014  \n",
      "Epoch: [1][1100/2851] Elapsed 9m 13s (remain 14m 40s) Loss: 0.0030(0.0960) Grad: 1882.4629  LR: 0.000015  \n",
      "Epoch: [1][1200/2851] Elapsed 10m 3s (remain 13m 49s) Loss: 0.0210(0.0888) Grad: 2217.9731  LR: 0.000017  \n",
      "Epoch: [1][1300/2851] Elapsed 10m 53s (remain 12m 59s) Loss: 0.0011(0.0828) Grad: 316.9373  LR: 0.000018  \n",
      "Epoch: [1][1400/2851] Elapsed 11m 44s (remain 12m 9s) Loss: 0.0026(0.0778) Grad: 659.0336  LR: 0.000020  \n",
      "Epoch: [1][1500/2851] Elapsed 12m 34s (remain 11m 18s) Loss: 0.0034(0.0733) Grad: 1110.5559  LR: 0.000020  \n",
      "Epoch: [1][1600/2851] Elapsed 13m 24s (remain 10m 28s) Loss: 0.0092(0.0694) Grad: 2267.2603  LR: 0.000020  \n",
      "Epoch: [1][1700/2851] Elapsed 14m 14s (remain 9m 37s) Loss: 0.0036(0.0659) Grad: 2297.1348  LR: 0.000020  \n",
      "Epoch: [1][1800/2851] Elapsed 15m 4s (remain 8m 47s) Loss: 0.0097(0.0627) Grad: 3072.6497  LR: 0.000019  \n",
      "Epoch: [1][1900/2851] Elapsed 15m 54s (remain 7m 57s) Loss: 0.0075(0.0598) Grad: 3651.8984  LR: 0.000019  \n",
      "Epoch: [1][2000/2851] Elapsed 16m 45s (remain 7m 7s) Loss: 0.0026(0.0572) Grad: 461.3329  LR: 0.000019  \n",
      "Epoch: [1][2100/2851] Elapsed 17m 35s (remain 6m 16s) Loss: 0.0063(0.0549) Grad: 2005.7234  LR: 0.000019  \n",
      "Epoch: [1][2200/2851] Elapsed 18m 24s (remain 5m 26s) Loss: 0.0025(0.0528) Grad: 545.5583  LR: 0.000019  \n",
      "Epoch: [1][2300/2851] Elapsed 19m 14s (remain 4m 35s) Loss: 0.0045(0.0510) Grad: 2511.8735  LR: 0.000019  \n",
      "Epoch: [1][2400/2851] Elapsed 20m 5s (remain 3m 45s) Loss: 0.0119(0.0492) Grad: 1437.2697  LR: 0.000018  \n",
      "Epoch: [1][2500/2851] Elapsed 20m 54s (remain 2m 55s) Loss: 0.0013(0.0476) Grad: 260.0315  LR: 0.000018  \n",
      "Epoch: [1][2600/2851] Elapsed 21m 44s (remain 2m 5s) Loss: 0.0031(0.0460) Grad: 492.0229  LR: 0.000018  \n",
      "Epoch: [1][2700/2851] Elapsed 22m 34s (remain 1m 15s) Loss: 0.0069(0.0446) Grad: 1541.1666  LR: 0.000018  \n",
      "Epoch: [1][2800/2851] Elapsed 23m 23s (remain 0m 25s) Loss: 0.0088(0.0432) Grad: 1421.0416  LR: 0.000018  \n",
      "Epoch: [1][2850/2851] Elapsed 23m 48s (remain 0m 0s) Loss: 0.0235(0.0425) Grad: 8656.0527  LR: 0.000018  \n",
      "EVAL: [0/724] Elapsed 0m 0s (remain 6m 14s) Loss: 0.0012(0.0012) \n",
      "EVAL: [100/724] Elapsed 0m 28s (remain 2m 53s) Loss: 0.0025(0.0069) \n",
      "EVAL: [200/724] Elapsed 0m 56s (remain 2m 26s) Loss: 0.0005(0.0081) \n",
      "EVAL: [300/724] Elapsed 1m 23s (remain 1m 57s) Loss: 0.0004(0.0085) \n",
      "EVAL: [400/724] Elapsed 1m 50s (remain 1m 29s) Loss: 0.0001(0.0086) \n",
      "EVAL: [500/724] Elapsed 2m 18s (remain 1m 1s) Loss: 0.0112(0.0096) \n",
      "EVAL: [600/724] Elapsed 2m 45s (remain 0m 33s) Loss: 0.0034(0.0093) \n",
      "EVAL: [700/724] Elapsed 3m 12s (remain 0m 6s) Loss: 0.0001(0.0088) \n",
      "EVAL: [723/724] Elapsed 3m 18s (remain 0m 0s) Loss: 0.0006(0.0087) \n",
      "Epoch 1 - avg_train_loss: 0.0425  avg_val_loss: 0.0087  time: 1633s\n",
      "Epoch 1 - Score: 0.8347\n",
      "Epoch 1 - Save Best Score: 0.8347 Model\n",
      "Epoch: [2][0/2851] Elapsed 0m 0s (remain 35m 30s) Loss: 0.0007(0.0007) Grad: 1690.1714  LR: 0.000018  \n",
      "Epoch: [2][100/2851] Elapsed 0m 51s (remain 23m 19s) Loss: 0.0033(0.0063) Grad: 5984.9937  LR: 0.000018  \n",
      "Epoch: [2][200/2851] Elapsed 1m 41s (remain 22m 17s) Loss: 0.0000(0.0061) Grad: 538.6644  LR: 0.000017  \n",
      "Epoch: [2][300/2851] Elapsed 2m 32s (remain 21m 27s) Loss: 0.0027(0.0067) Grad: 6439.3218  LR: 0.000017  \n",
      "Epoch: [2][400/2851] Elapsed 3m 23s (remain 20m 42s) Loss: 0.0007(0.0064) Grad: 2183.5029  LR: 0.000017  \n",
      "Epoch: [2][500/2851] Elapsed 4m 13s (remain 19m 50s) Loss: 0.0126(0.0067) Grad: 10765.1094  LR: 0.000017  \n",
      "Epoch: [2][600/2851] Elapsed 5m 4s (remain 18m 58s) Loss: 0.0016(0.0064) Grad: 4893.7197  LR: 0.000017  \n",
      "Epoch: [2][700/2851] Elapsed 5m 55s (remain 18m 10s) Loss: 0.0004(0.0065) Grad: 1387.1985  LR: 0.000017  \n",
      "Epoch: [2][800/2851] Elapsed 6m 46s (remain 17m 20s) Loss: 0.0001(0.0065) Grad: 335.3214  LR: 0.000017  \n",
      "Epoch: [2][900/2851] Elapsed 7m 37s (remain 16m 29s) Loss: 0.0003(0.0066) Grad: 1701.4608  LR: 0.000016  \n",
      "Epoch: [2][1000/2851] Elapsed 8m 28s (remain 15m 39s) Loss: 0.0018(0.0066) Grad: 7533.8789  LR: 0.000016  \n",
      "Epoch: [2][1100/2851] Elapsed 9m 18s (remain 14m 48s) Loss: 0.0003(0.0066) Grad: 2079.3167  LR: 0.000016  \n",
      "Epoch: [2][1200/2851] Elapsed 10m 9s (remain 13m 56s) Loss: 0.0025(0.0066) Grad: 10057.2139  LR: 0.000016  \n",
      "Epoch: [2][1300/2851] Elapsed 10m 59s (remain 13m 6s) Loss: 0.0001(0.0066) Grad: 1484.3993  LR: 0.000016  \n",
      "Epoch: [2][1400/2851] Elapsed 11m 51s (remain 12m 16s) Loss: 0.0114(0.0065) Grad: 33786.5938  LR: 0.000016  \n",
      "Epoch: [2][1500/2851] Elapsed 12m 42s (remain 11m 26s) Loss: 0.0004(0.0066) Grad: 1578.1797  LR: 0.000015  \n",
      "Epoch: [2][1600/2851] Elapsed 13m 33s (remain 10m 34s) Loss: 0.0006(0.0066) Grad: 2648.3494  LR: 0.000015  \n",
      "Epoch: [2][1700/2851] Elapsed 14m 23s (remain 9m 43s) Loss: 0.0005(0.0066) Grad: 1855.3726  LR: 0.000015  \n",
      "Epoch: [2][1800/2851] Elapsed 15m 14s (remain 8m 52s) Loss: 0.0001(0.0067) Grad: 371.4232  LR: 0.000015  \n",
      "Epoch: [2][1900/2851] Elapsed 16m 5s (remain 8m 2s) Loss: 0.0100(0.0066) Grad: 16884.4688  LR: 0.000015  \n",
      "Epoch: [2][2000/2851] Elapsed 16m 56s (remain 7m 11s) Loss: 0.0067(0.0066) Grad: 17567.6758  LR: 0.000015  \n",
      "Epoch: [2][2100/2851] Elapsed 17m 46s (remain 6m 20s) Loss: 0.0000(0.0064) Grad: 5.9588  LR: 0.000015  \n",
      "Epoch: [2][2200/2851] Elapsed 18m 37s (remain 5m 29s) Loss: 0.0018(0.0064) Grad: 6099.8071  LR: 0.000014  \n",
      "Epoch: [2][2300/2851] Elapsed 19m 27s (remain 4m 39s) Loss: 0.0027(0.0063) Grad: 6536.1191  LR: 0.000014  \n",
      "Epoch: [2][2400/2851] Elapsed 20m 17s (remain 3m 48s) Loss: 0.0000(0.0063) Grad: 41.7514  LR: 0.000014  \n",
      "Epoch: [2][2500/2851] Elapsed 21m 10s (remain 2m 57s) Loss: 0.0053(0.0064) Grad: 17725.9844  LR: 0.000014  \n",
      "Epoch: [2][2600/2851] Elapsed 22m 1s (remain 2m 6s) Loss: 0.0013(0.0064) Grad: 3329.3628  LR: 0.000014  \n",
      "Epoch: [2][2700/2851] Elapsed 22m 51s (remain 1m 16s) Loss: 0.0001(0.0064) Grad: 313.5630  LR: 0.000014  \n",
      "Epoch: [2][2800/2851] Elapsed 23m 41s (remain 0m 25s) Loss: 0.0008(0.0064) Grad: 2214.9656  LR: 0.000013  \n",
      "Epoch: [2][2850/2851] Elapsed 24m 6s (remain 0m 0s) Loss: 0.0000(0.0064) Grad: 18.5353  LR: 0.000013  \n",
      "EVAL: [0/724] Elapsed 0m 0s (remain 6m 59s) Loss: 0.0011(0.0011) \n",
      "EVAL: [100/724] Elapsed 0m 28s (remain 2m 57s) Loss: 0.0025(0.0064) \n",
      "EVAL: [200/724] Elapsed 0m 56s (remain 2m 28s) Loss: 0.0000(0.0081) \n",
      "EVAL: [300/724] Elapsed 1m 24s (remain 1m 58s) Loss: 0.0002(0.0078) \n",
      "EVAL: [400/724] Elapsed 1m 52s (remain 1m 30s) Loss: 0.0000(0.0076) \n",
      "EVAL: [500/724] Elapsed 2m 19s (remain 1m 2s) Loss: 0.0122(0.0090) \n",
      "EVAL: [600/724] Elapsed 2m 47s (remain 0m 34s) Loss: 0.0020(0.0086) \n",
      "EVAL: [700/724] Elapsed 3m 14s (remain 0m 6s) Loss: 0.0000(0.0079) \n",
      "EVAL: [723/724] Elapsed 3m 20s (remain 0m 0s) Loss: 0.0001(0.0078) \n",
      "Epoch 2 - avg_train_loss: 0.0064  avg_val_loss: 0.0078  time: 1653s\n",
      "Epoch 2 - Score: 0.8694\n",
      "Epoch 2 - Save Best Score: 0.8694 Model\n",
      "Epoch: [3][0/2851] Elapsed 0m 0s (remain 36m 25s) Loss: 0.0001(0.0001) Grad: 1597.1453  LR: 0.000013  \n",
      "Epoch: [3][100/2851] Elapsed 0m 51s (remain 23m 22s) Loss: 0.0060(0.0048) Grad: 12846.5342  LR: 0.000013  \n",
      "Epoch: [3][200/2851] Elapsed 1m 42s (remain 22m 25s) Loss: 0.0009(0.0052) Grad: 4008.9458  LR: 0.000013  \n",
      "Epoch: [3][300/2851] Elapsed 2m 32s (remain 21m 31s) Loss: 0.0013(0.0054) Grad: 13390.3848  LR: 0.000013  \n",
      "Epoch: [3][400/2851] Elapsed 3m 22s (remain 20m 38s) Loss: 0.0000(0.0055) Grad: 11.1291  LR: 0.000013  \n",
      "Epoch: [3][500/2851] Elapsed 4m 14s (remain 19m 52s) Loss: 0.0005(0.0052) Grad: 1784.6052  LR: 0.000013  \n",
      "Epoch: [3][600/2851] Elapsed 5m 6s (remain 19m 8s) Loss: 0.0002(0.0050) Grad: 2755.0193  LR: 0.000012  \n",
      "Epoch: [3][700/2851] Elapsed 5m 57s (remain 18m 17s) Loss: 0.0000(0.0048) Grad: 403.3839  LR: 0.000012  \n",
      "Epoch: [3][800/2851] Elapsed 6m 47s (remain 17m 24s) Loss: 0.0000(0.0051) Grad: 51.5807  LR: 0.000012  \n",
      "Epoch: [3][900/2851] Elapsed 7m 38s (remain 16m 32s) Loss: 0.0000(0.0050) Grad: 34.0727  LR: 0.000012  \n",
      "Epoch: [3][1000/2851] Elapsed 8m 28s (remain 15m 40s) Loss: 0.0046(0.0049) Grad: 8113.7388  LR: 0.000012  \n",
      "Epoch: [3][1100/2851] Elapsed 9m 19s (remain 14m 48s) Loss: 0.0000(0.0050) Grad: 202.0167  LR: 0.000012  \n",
      "Epoch: [3][1200/2851] Elapsed 10m 9s (remain 13m 57s) Loss: 0.0000(0.0050) Grad: 13.7840  LR: 0.000011  \n",
      "Epoch: [3][1300/2851] Elapsed 11m 0s (remain 13m 6s) Loss: 0.0002(0.0050) Grad: 739.0372  LR: 0.000011  \n",
      "Epoch: [3][1400/2851] Elapsed 11m 51s (remain 12m 16s) Loss: 0.0109(0.0053) Grad: 13362.2988  LR: 0.000011  \n",
      "Epoch: [3][1500/2851] Elapsed 12m 41s (remain 11m 25s) Loss: 0.0049(0.0052) Grad: 5323.9927  LR: 0.000011  \n",
      "Epoch: [3][1600/2851] Elapsed 13m 31s (remain 10m 33s) Loss: 0.0000(0.0052) Grad: 140.4794  LR: 0.000011  \n",
      "Epoch: [3][1700/2851] Elapsed 14m 22s (remain 9m 43s) Loss: 0.0029(0.0052) Grad: 5037.5781  LR: 0.000011  \n",
      "Epoch: [3][1800/2851] Elapsed 15m 13s (remain 8m 52s) Loss: 0.0003(0.0052) Grad: 910.8983  LR: 0.000011  \n",
      "Epoch: [3][1900/2851] Elapsed 16m 3s (remain 8m 1s) Loss: 0.0268(0.0052) Grad: 28267.5879  LR: 0.000010  \n",
      "Epoch: [3][2000/2851] Elapsed 16m 54s (remain 7m 10s) Loss: 0.0002(0.0052) Grad: 624.1973  LR: 0.000010  \n",
      "Epoch: [3][2100/2851] Elapsed 17m 45s (remain 6m 20s) Loss: 0.0007(0.0052) Grad: 1402.9733  LR: 0.000010  \n",
      "Epoch: [3][2200/2851] Elapsed 18m 35s (remain 5m 29s) Loss: 0.0046(0.0052) Grad: 4654.6724  LR: 0.000010  \n",
      "Epoch: [3][2300/2851] Elapsed 19m 26s (remain 4m 38s) Loss: 0.0292(0.0051) Grad: 15358.8242  LR: 0.000010  \n",
      "Epoch: [3][2400/2851] Elapsed 20m 16s (remain 3m 47s) Loss: 0.0121(0.0052) Grad: 9499.5898  LR: 0.000010  \n",
      "Epoch: [3][2500/2851] Elapsed 21m 6s (remain 2m 57s) Loss: 0.0040(0.0051) Grad: 7021.1494  LR: 0.000009  \n",
      "Epoch: [3][2600/2851] Elapsed 21m 56s (remain 2m 6s) Loss: 0.0013(0.0050) Grad: 3909.9819  LR: 0.000009  \n",
      "Epoch: [3][2700/2851] Elapsed 22m 47s (remain 1m 15s) Loss: 0.0113(0.0050) Grad: 13237.9160  LR: 0.000009  \n",
      "Epoch: [3][2800/2851] Elapsed 23m 37s (remain 0m 25s) Loss: 0.0091(0.0049) Grad: 25642.4980  LR: 0.000009  \n",
      "Epoch: [3][2850/2851] Elapsed 24m 3s (remain 0m 0s) Loss: 0.0034(0.0050) Grad: 5090.4141  LR: 0.000009  \n",
      "EVAL: [0/724] Elapsed 0m 0s (remain 6m 39s) Loss: 0.0012(0.0012) \n",
      "EVAL: [100/724] Elapsed 0m 28s (remain 2m 54s) Loss: 0.0047(0.0073) \n",
      "EVAL: [200/724] Elapsed 0m 56s (remain 2m 26s) Loss: 0.0000(0.0084) \n",
      "EVAL: [300/724] Elapsed 1m 23s (remain 1m 57s) Loss: 0.0002(0.0080) \n",
      "EVAL: [400/724] Elapsed 1m 51s (remain 1m 29s) Loss: 0.0000(0.0077) \n",
      "EVAL: [500/724] Elapsed 2m 20s (remain 1m 2s) Loss: 0.0109(0.0092) \n",
      "EVAL: [600/724] Elapsed 2m 49s (remain 0m 34s) Loss: 0.0038(0.0088) \n",
      "EVAL: [700/724] Elapsed 3m 16s (remain 0m 6s) Loss: 0.0000(0.0080) \n",
      "EVAL: [723/724] Elapsed 3m 22s (remain 0m 0s) Loss: 0.0000(0.0079) \n",
      "Epoch 3 - avg_train_loss: 0.0050  avg_val_loss: 0.0079  time: 1651s\n",
      "Epoch 3 - Score: 0.8815\n",
      "Epoch 3 - Save Best Score: 0.8815 Model\n",
      "Epoch: [4][0/2851] Elapsed 0m 0s (remain 40m 30s) Loss: 0.0005(0.0005) Grad: 2572.2542  LR: 0.000009  \n",
      "Epoch: [4][100/2851] Elapsed 0m 51s (remain 23m 12s) Loss: 0.0008(0.0036) Grad: 2288.5671  LR: 0.000009  \n",
      "Epoch: [4][200/2851] Elapsed 1m 41s (remain 22m 19s) Loss: 0.0059(0.0036) Grad: 11989.1855  LR: 0.000009  \n",
      "Epoch: [4][300/2851] Elapsed 2m 31s (remain 21m 27s) Loss: 0.0094(0.0044) Grad: 50097.4062  LR: 0.000008  \n",
      "Epoch: [4][400/2851] Elapsed 3m 23s (remain 20m 40s) Loss: 0.0160(0.0042) Grad: 20703.5078  LR: 0.000008  \n",
      "Epoch: [4][500/2851] Elapsed 4m 13s (remain 19m 49s) Loss: 0.0012(0.0044) Grad: 16409.6836  LR: 0.000008  \n",
      "Epoch: [4][600/2851] Elapsed 5m 3s (remain 18m 57s) Loss: 0.0019(0.0042) Grad: 9057.7217  LR: 0.000008  \n",
      "Epoch: [4][700/2851] Elapsed 5m 54s (remain 18m 8s) Loss: 0.0000(0.0044) Grad: 166.1057  LR: 0.000008  \n",
      "Epoch: [4][800/2851] Elapsed 6m 45s (remain 17m 18s) Loss: 0.0000(0.0042) Grad: 22.9834  LR: 0.000008  \n",
      "Epoch: [4][900/2851] Elapsed 7m 36s (remain 16m 27s) Loss: 0.0166(0.0041) Grad: 31913.6758  LR: 0.000007  \n",
      "Epoch: [4][1000/2851] Elapsed 8m 26s (remain 15m 35s) Loss: 0.0011(0.0043) Grad: 4219.4009  LR: 0.000007  \n",
      "Epoch: [4][1100/2851] Elapsed 9m 17s (remain 14m 46s) Loss: 0.0710(0.0043) Grad: 62166.7070  LR: 0.000007  \n",
      "Epoch: [4][1200/2851] Elapsed 10m 8s (remain 13m 56s) Loss: 0.0004(0.0043) Grad: 608.5483  LR: 0.000007  \n",
      "Epoch: [4][1300/2851] Elapsed 10m 58s (remain 13m 5s) Loss: 0.0027(0.0044) Grad: 5141.6489  LR: 0.000007  \n",
      "Epoch: [4][1400/2851] Elapsed 11m 49s (remain 12m 13s) Loss: 0.0014(0.0043) Grad: 3187.7854  LR: 0.000007  \n",
      "Epoch: [4][1500/2851] Elapsed 12m 39s (remain 11m 23s) Loss: 0.0000(0.0044) Grad: 16.2476  LR: 0.000007  \n",
      "Epoch: [4][1600/2851] Elapsed 13m 30s (remain 10m 32s) Loss: 0.0001(0.0043) Grad: 542.5617  LR: 0.000006  \n",
      "Epoch: [4][1700/2851] Elapsed 14m 22s (remain 9m 42s) Loss: 0.0000(0.0043) Grad: 190.0896  LR: 0.000006  \n",
      "Epoch: [4][1800/2851] Elapsed 15m 13s (remain 8m 52s) Loss: 0.0050(0.0042) Grad: 5550.5112  LR: 0.000006  \n",
      "Epoch: [4][1900/2851] Elapsed 16m 3s (remain 8m 1s) Loss: 0.0013(0.0044) Grad: 1856.1926  LR: 0.000006  \n",
      "Epoch: [4][2000/2851] Elapsed 16m 53s (remain 7m 10s) Loss: 0.0105(0.0043) Grad: 11378.7588  LR: 0.000006  \n",
      "Epoch: [4][2100/2851] Elapsed 17m 44s (remain 6m 19s) Loss: 0.0010(0.0043) Grad: 8675.0068  LR: 0.000006  \n",
      "Epoch: [4][2200/2851] Elapsed 18m 35s (remain 5m 29s) Loss: 0.0003(0.0042) Grad: 774.3781  LR: 0.000005  \n",
      "Epoch: [4][2300/2851] Elapsed 19m 27s (remain 4m 38s) Loss: 0.0001(0.0042) Grad: 114.5096  LR: 0.000005  \n",
      "Epoch: [4][2400/2851] Elapsed 20m 17s (remain 3m 48s) Loss: 0.0309(0.0041) Grad: 14782.2783  LR: 0.000005  \n",
      "Epoch: [4][2500/2851] Elapsed 21m 7s (remain 2m 57s) Loss: 0.0053(0.0041) Grad: 4317.2783  LR: 0.000005  \n",
      "Epoch: [4][2600/2851] Elapsed 21m 58s (remain 2m 6s) Loss: 0.0022(0.0041) Grad: 7185.4521  LR: 0.000005  \n",
      "Epoch: [4][2700/2851] Elapsed 22m 50s (remain 1m 16s) Loss: 0.0049(0.0041) Grad: 3779.1909  LR: 0.000005  \n",
      "Epoch: [4][2800/2851] Elapsed 23m 40s (remain 0m 25s) Loss: 0.0317(0.0040) Grad: 12976.2773  LR: 0.000005  \n",
      "Epoch: [4][2850/2851] Elapsed 24m 5s (remain 0m 0s) Loss: 0.0000(0.0041) Grad: 48.6180  LR: 0.000004  \n",
      "EVAL: [0/724] Elapsed 0m 0s (remain 7m 10s) Loss: 0.0006(0.0006) \n",
      "EVAL: [100/724] Elapsed 0m 28s (remain 2m 55s) Loss: 0.0033(0.0076) \n",
      "EVAL: [200/724] Elapsed 0m 56s (remain 2m 27s) Loss: 0.0000(0.0091) \n",
      "EVAL: [300/724] Elapsed 1m 24s (remain 1m 58s) Loss: 0.0003(0.0088) \n",
      "EVAL: [400/724] Elapsed 1m 52s (remain 1m 30s) Loss: 0.0000(0.0085) \n",
      "EVAL: [500/724] Elapsed 2m 19s (remain 1m 2s) Loss: 0.0099(0.0101) \n",
      "EVAL: [600/724] Elapsed 2m 48s (remain 0m 34s) Loss: 0.0028(0.0098) \n",
      "EVAL: [700/724] Elapsed 3m 15s (remain 0m 6s) Loss: 0.0000(0.0089) \n",
      "EVAL: [723/724] Elapsed 3m 21s (remain 0m 0s) Loss: 0.0000(0.0088) \n",
      "Epoch 4 - avg_train_loss: 0.0041  avg_val_loss: 0.0088  time: 1652s\n",
      "Epoch 4 - Score: 0.8827\n",
      "Epoch 4 - Save Best Score: 0.8827 Model\n",
      "Epoch: [5][0/2851] Elapsed 0m 0s (remain 38m 24s) Loss: 0.0221(0.0221) Grad: 47884.9023  LR: 0.000004  \n",
      "Epoch: [5][100/2851] Elapsed 0m 51s (remain 23m 25s) Loss: 0.0039(0.0032) Grad: 32674.3828  LR: 0.000004  \n",
      "Epoch: [5][200/2851] Elapsed 1m 42s (remain 22m 26s) Loss: 0.0004(0.0029) Grad: 4350.4370  LR: 0.000004  \n",
      "Epoch: [5][300/2851] Elapsed 2m 32s (remain 21m 29s) Loss: 0.0000(0.0028) Grad: 149.8888  LR: 0.000004  \n",
      "Epoch: [5][400/2851] Elapsed 3m 22s (remain 20m 35s) Loss: 0.0146(0.0029) Grad: 19909.8613  LR: 0.000004  \n",
      "Epoch: [5][500/2851] Elapsed 4m 12s (remain 19m 43s) Loss: 0.0032(0.0029) Grad: 11601.5713  LR: 0.000004  \n",
      "Epoch: [5][600/2851] Elapsed 5m 2s (remain 18m 53s) Loss: 0.0001(0.0031) Grad: 593.8757  LR: 0.000004  \n",
      "Epoch: [5][700/2851] Elapsed 5m 53s (remain 18m 5s) Loss: 0.0161(0.0031) Grad: 28718.7871  LR: 0.000003  \n",
      "Epoch: [5][800/2851] Elapsed 6m 45s (remain 17m 17s) Loss: 0.0053(0.0031) Grad: 8800.4756  LR: 0.000003  \n",
      "Epoch: [5][900/2851] Elapsed 7m 35s (remain 16m 26s) Loss: 0.0010(0.0031) Grad: 4999.9932  LR: 0.000003  \n",
      "Epoch: [5][1000/2851] Elapsed 8m 25s (remain 15m 34s) Loss: 0.0005(0.0030) Grad: 3724.5991  LR: 0.000003  \n",
      "Epoch: [5][1100/2851] Elapsed 9m 15s (remain 14m 43s) Loss: 0.0056(0.0031) Grad: 19455.0586  LR: 0.000003  \n",
      "Epoch: [5][1200/2851] Elapsed 10m 6s (remain 13m 53s) Loss: 0.0021(0.0031) Grad: 7020.3262  LR: 0.000003  \n",
      "Epoch: [5][1300/2851] Elapsed 10m 57s (remain 13m 3s) Loss: 0.0000(0.0031) Grad: 63.0700  LR: 0.000002  \n",
      "Epoch: [5][1400/2851] Elapsed 11m 47s (remain 12m 12s) Loss: 0.0000(0.0031) Grad: 30.1744  LR: 0.000002  \n",
      "Epoch: [5][1500/2851] Elapsed 12m 38s (remain 11m 22s) Loss: 0.0000(0.0032) Grad: 173.2499  LR: 0.000002  \n",
      "Epoch: [5][1600/2851] Elapsed 13m 28s (remain 10m 31s) Loss: 0.0109(0.0032) Grad: 42712.0664  LR: 0.000002  \n",
      "Epoch: [5][1700/2851] Elapsed 14m 19s (remain 9m 41s) Loss: 0.0000(0.0032) Grad: 72.5420  LR: 0.000002  \n",
      "Epoch: [5][1800/2851] Elapsed 15m 10s (remain 8m 50s) Loss: 0.0000(0.0031) Grad: 31.5939  LR: 0.000002  \n",
      "Epoch: [5][1900/2851] Elapsed 16m 1s (remain 8m 0s) Loss: 0.0000(0.0032) Grad: 7.6905  LR: 0.000001  \n",
      "Epoch: [5][2000/2851] Elapsed 16m 51s (remain 7m 9s) Loss: 0.0001(0.0032) Grad: 966.7061  LR: 0.000001  \n",
      "Epoch: [5][2100/2851] Elapsed 17m 41s (remain 6m 19s) Loss: 0.0064(0.0032) Grad: 13028.7227  LR: 0.000001  \n",
      "Epoch: [5][2200/2851] Elapsed 18m 32s (remain 5m 28s) Loss: 0.0000(0.0032) Grad: 21.4458  LR: 0.000001  \n",
      "Epoch: [5][2300/2851] Elapsed 19m 23s (remain 4m 38s) Loss: 0.0059(0.0032) Grad: 18736.0938  LR: 0.000001  \n",
      "Epoch: [5][2400/2851] Elapsed 20m 13s (remain 3m 47s) Loss: 0.0001(0.0033) Grad: 850.0435  LR: 0.000001  \n",
      "Epoch: [5][2500/2851] Elapsed 21m 4s (remain 2m 56s) Loss: 0.0534(0.0033) Grad: 27415.8652  LR: 0.000001  \n",
      "Epoch: [5][2600/2851] Elapsed 21m 55s (remain 2m 6s) Loss: 0.0008(0.0033) Grad: 4680.2803  LR: 0.000000  \n",
      "Epoch: [5][2700/2851] Elapsed 22m 45s (remain 1m 15s) Loss: 0.0001(0.0033) Grad: 238.4537  LR: 0.000000  \n",
      "Epoch: [5][2800/2851] Elapsed 23m 36s (remain 0m 25s) Loss: 0.0000(0.0032) Grad: 105.2971  LR: 0.000000  \n",
      "Epoch: [5][2850/2851] Elapsed 24m 2s (remain 0m 0s) Loss: 0.0009(0.0032) Grad: 2238.9463  LR: 0.000000  \n",
      "EVAL: [0/724] Elapsed 0m 0s (remain 7m 16s) Loss: 0.0006(0.0006) \n",
      "EVAL: [100/724] Elapsed 0m 28s (remain 2m 58s) Loss: 0.0036(0.0080) \n",
      "EVAL: [200/724] Elapsed 0m 57s (remain 2m 29s) Loss: 0.0000(0.0096) \n",
      "EVAL: [300/724] Elapsed 1m 24s (remain 1m 59s) Loss: 0.0006(0.0094) \n",
      "EVAL: [400/724] Elapsed 1m 52s (remain 1m 30s) Loss: 0.0000(0.0094) \n",
      "EVAL: [500/724] Elapsed 2m 20s (remain 1m 2s) Loss: 0.0135(0.0115) \n",
      "EVAL: [600/724] Elapsed 2m 49s (remain 0m 34s) Loss: 0.0016(0.0111) \n",
      "EVAL: [700/724] Elapsed 3m 16s (remain 0m 6s) Loss: 0.0000(0.0101) \n",
      "EVAL: [723/724] Elapsed 3m 22s (remain 0m 0s) Loss: 0.0000(0.0100) \n",
      "Epoch 5 - avg_train_loss: 0.0032  avg_val_loss: 0.0100  time: 1650s\n",
      "Epoch 5 - Score: 0.8856\n",
      "Epoch 5 - Save Best Score: 0.8856 Model\n",
      "========== fold: 2 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2871] Elapsed 0m 0s (remain 40m 54s) Loss: 0.3458(0.3458) Grad: 204017.2969  LR: 0.000000  \n",
      "Epoch: [1][100/2871] Elapsed 0m 51s (remain 23m 21s) Loss: 0.2916(0.4959) Grad: 12092.4521  LR: 0.000001  \n",
      "Epoch: [1][200/2871] Elapsed 1m 41s (remain 22m 24s) Loss: 0.0691(0.3467) Grad: 4892.7920  LR: 0.000003  \n",
      "Epoch: [1][300/2871] Elapsed 2m 31s (remain 21m 33s) Loss: 0.0324(0.2452) Grad: 442.7823  LR: 0.000004  \n",
      "Epoch: [1][400/2871] Elapsed 3m 21s (remain 20m 41s) Loss: 0.0412(0.1932) Grad: 763.4689  LR: 0.000006  \n",
      "Epoch: [1][500/2871] Elapsed 4m 11s (remain 19m 51s) Loss: 0.0234(0.1626) Grad: 1279.8525  LR: 0.000007  \n",
      "Epoch: [1][600/2871] Elapsed 5m 1s (remain 18m 58s) Loss: 0.0251(0.1405) Grad: 1186.7655  LR: 0.000008  \n",
      "Epoch: [1][700/2871] Elapsed 5m 51s (remain 18m 8s) Loss: 0.0121(0.1231) Grad: 1494.7047  LR: 0.000010  \n",
      "Epoch: [1][800/2871] Elapsed 6m 41s (remain 17m 18s) Loss: 0.0103(0.1096) Grad: 602.7847  LR: 0.000011  \n",
      "Epoch: [1][900/2871] Elapsed 7m 31s (remain 16m 27s) Loss: 0.0116(0.0987) Grad: 1297.7050  LR: 0.000013  \n",
      "Epoch: [1][1000/2871] Elapsed 8m 21s (remain 15m 37s) Loss: 0.0095(0.0903) Grad: 2481.4092  LR: 0.000014  \n",
      "Epoch: [1][1100/2871] Elapsed 9m 12s (remain 14m 47s) Loss: 0.0137(0.0832) Grad: 1565.5156  LR: 0.000015  \n",
      "Epoch: [1][1200/2871] Elapsed 10m 2s (remain 13m 57s) Loss: 0.0063(0.0770) Grad: 1362.4957  LR: 0.000017  \n",
      "Epoch: [1][1300/2871] Elapsed 10m 52s (remain 13m 7s) Loss: 0.0060(0.0718) Grad: 762.4417  LR: 0.000018  \n",
      "Epoch: [1][1400/2871] Elapsed 11m 41s (remain 12m 16s) Loss: 0.0032(0.0675) Grad: 471.4208  LR: 0.000020  \n",
      "Epoch: [1][1500/2871] Elapsed 12m 32s (remain 11m 26s) Loss: 0.0080(0.0637) Grad: 749.4076  LR: 0.000020  \n",
      "Epoch: [1][1600/2871] Elapsed 13m 21s (remain 10m 35s) Loss: 0.0418(0.0603) Grad: 4767.1172  LR: 0.000020  \n",
      "Epoch: [1][1700/2871] Elapsed 14m 11s (remain 9m 45s) Loss: 0.0128(0.0573) Grad: 1187.6406  LR: 0.000020  \n",
      "Epoch: [1][1800/2871] Elapsed 15m 1s (remain 8m 55s) Loss: 0.0097(0.0546) Grad: 859.0907  LR: 0.000019  \n",
      "Epoch: [1][1900/2871] Elapsed 15m 51s (remain 8m 5s) Loss: 0.0035(0.0522) Grad: 382.2367  LR: 0.000019  \n",
      "Epoch: [1][2000/2871] Elapsed 16m 40s (remain 7m 15s) Loss: 0.0041(0.0500) Grad: 637.7184  LR: 0.000019  \n",
      "Epoch: [1][2100/2871] Elapsed 17m 30s (remain 6m 24s) Loss: 0.0025(0.0479) Grad: 583.0992  LR: 0.000019  \n",
      "Epoch: [1][2200/2871] Elapsed 18m 20s (remain 5m 34s) Loss: 0.0020(0.0462) Grad: 416.8811  LR: 0.000019  \n",
      "Epoch: [1][2300/2871] Elapsed 19m 10s (remain 4m 44s) Loss: 0.0022(0.0446) Grad: 834.4047  LR: 0.000019  \n",
      "Epoch: [1][2400/2871] Elapsed 19m 59s (remain 3m 54s) Loss: 0.0041(0.0431) Grad: 599.3624  LR: 0.000019  \n",
      "Epoch: [1][2500/2871] Elapsed 20m 48s (remain 3m 4s) Loss: 0.0021(0.0416) Grad: 517.1616  LR: 0.000018  \n",
      "Epoch: [1][2600/2871] Elapsed 21m 39s (remain 2m 14s) Loss: 0.0032(0.0403) Grad: 688.7821  LR: 0.000018  \n",
      "Epoch: [1][2700/2871] Elapsed 22m 29s (remain 1m 24s) Loss: 0.0010(0.0391) Grad: 117.5020  LR: 0.000018  \n",
      "Epoch: [1][2800/2871] Elapsed 23m 18s (remain 0m 34s) Loss: 0.0183(0.0380) Grad: 1686.4248  LR: 0.000018  \n",
      "Epoch: [1][2870/2871] Elapsed 23m 53s (remain 0m 0s) Loss: 0.0009(0.0373) Grad: 126.6934  LR: 0.000018  \n",
      "EVAL: [0/704] Elapsed 0m 0s (remain 6m 19s) Loss: 0.0024(0.0024) \n",
      "EVAL: [100/704] Elapsed 0m 27s (remain 2m 47s) Loss: 0.0044(0.0072) \n",
      "EVAL: [200/704] Elapsed 0m 56s (remain 2m 20s) Loss: 0.0001(0.0063) \n",
      "EVAL: [300/704] Elapsed 1m 23s (remain 1m 51s) Loss: 0.0004(0.0062) \n",
      "EVAL: [400/704] Elapsed 1m 50s (remain 1m 23s) Loss: 0.0103(0.0067) \n",
      "EVAL: [500/704] Elapsed 2m 18s (remain 0m 56s) Loss: 0.0075(0.0072) \n",
      "EVAL: [600/704] Elapsed 2m 46s (remain 0m 28s) Loss: 0.0004(0.0074) \n",
      "EVAL: [700/704] Elapsed 3m 13s (remain 0m 0s) Loss: 0.0001(0.0071) \n",
      "EVAL: [703/704] Elapsed 3m 14s (remain 0m 0s) Loss: 0.0000(0.0070) \n",
      "Epoch 1 - avg_train_loss: 0.0373  avg_val_loss: 0.0070  time: 1633s\n",
      "Epoch 1 - Score: 0.8445\n",
      "Epoch 1 - Save Best Score: 0.8445 Model\n",
      "Epoch: [2][0/2871] Elapsed 0m 0s (remain 36m 11s) Loss: 0.0004(0.0004) Grad: 1206.9116  LR: 0.000018  \n",
      "Epoch: [2][100/2871] Elapsed 0m 50s (remain 23m 16s) Loss: 0.0049(0.0040) Grad: 7509.4180  LR: 0.000018  \n",
      "Epoch: [2][200/2871] Elapsed 1m 41s (remain 22m 29s) Loss: 0.0321(0.0049) Grad: 75782.8203  LR: 0.000017  \n",
      "Epoch: [2][300/2871] Elapsed 2m 32s (remain 21m 43s) Loss: 0.0028(0.0053) Grad: 3337.6436  LR: 0.000017  \n",
      "Epoch: [2][400/2871] Elapsed 3m 23s (remain 20m 54s) Loss: 0.0026(0.0053) Grad: 5143.5430  LR: 0.000017  \n",
      "Epoch: [2][500/2871] Elapsed 4m 14s (remain 20m 3s) Loss: 0.0038(0.0056) Grad: 5551.0630  LR: 0.000017  \n",
      "Epoch: [2][600/2871] Elapsed 5m 5s (remain 19m 13s) Loss: 0.0011(0.0055) Grad: 4326.1685  LR: 0.000017  \n",
      "Epoch: [2][700/2871] Elapsed 5m 56s (remain 18m 23s) Loss: 0.0013(0.0054) Grad: 3336.9136  LR: 0.000017  \n",
      "Epoch: [2][800/2871] Elapsed 6m 47s (remain 17m 32s) Loss: 0.0007(0.0054) Grad: 2389.9539  LR: 0.000017  \n",
      "Epoch: [2][900/2871] Elapsed 7m 37s (remain 16m 41s) Loss: 0.0050(0.0054) Grad: 35186.9414  LR: 0.000016  \n",
      "Epoch: [2][1000/2871] Elapsed 8m 28s (remain 15m 50s) Loss: 0.0001(0.0054) Grad: 1472.4436  LR: 0.000016  \n",
      "Epoch: [2][1100/2871] Elapsed 9m 19s (remain 14m 58s) Loss: 0.0078(0.0056) Grad: 9703.6689  LR: 0.000016  \n",
      "Epoch: [2][1200/2871] Elapsed 10m 10s (remain 14m 8s) Loss: 0.0041(0.0055) Grad: 10467.7305  LR: 0.000016  \n",
      "Epoch: [2][1300/2871] Elapsed 11m 1s (remain 13m 18s) Loss: 0.0109(0.0056) Grad: 36560.1523  LR: 0.000016  \n",
      "Epoch: [2][1400/2871] Elapsed 11m 54s (remain 12m 30s) Loss: 0.0054(0.0057) Grad: 5927.6113  LR: 0.000016  \n",
      "Epoch: [2][1500/2871] Elapsed 12m 45s (remain 11m 38s) Loss: 0.0140(0.0056) Grad: 17848.7969  LR: 0.000015  \n",
      "Epoch: [2][1600/2871] Elapsed 13m 36s (remain 10m 47s) Loss: 0.0004(0.0057) Grad: 2505.1995  LR: 0.000015  \n",
      "Epoch: [2][1700/2871] Elapsed 14m 26s (remain 9m 56s) Loss: 0.0366(0.0057) Grad: 24658.6934  LR: 0.000015  \n",
      "Epoch: [2][1800/2871] Elapsed 15m 17s (remain 9m 5s) Loss: 0.0005(0.0057) Grad: 2961.2915  LR: 0.000015  \n",
      "Epoch: [2][1900/2871] Elapsed 16m 9s (remain 8m 14s) Loss: 0.0042(0.0058) Grad: 8849.4502  LR: 0.000015  \n",
      "Epoch: [2][2000/2871] Elapsed 17m 0s (remain 7m 23s) Loss: 0.0106(0.0058) Grad: 13414.5537  LR: 0.000015  \n",
      "Epoch: [2][2100/2871] Elapsed 17m 50s (remain 6m 32s) Loss: 0.0003(0.0058) Grad: 1387.1072  LR: 0.000015  \n",
      "Epoch: [2][2200/2871] Elapsed 18m 41s (remain 5m 41s) Loss: 0.0060(0.0058) Grad: 30900.2285  LR: 0.000014  \n",
      "Epoch: [2][2300/2871] Elapsed 19m 31s (remain 4m 50s) Loss: 0.0554(0.0057) Grad: 75542.8047  LR: 0.000014  \n",
      "Epoch: [2][2400/2871] Elapsed 20m 22s (remain 3m 59s) Loss: 0.0001(0.0058) Grad: 365.1594  LR: 0.000014  \n",
      "Epoch: [2][2500/2871] Elapsed 21m 13s (remain 3m 8s) Loss: 0.0002(0.0059) Grad: 476.9738  LR: 0.000014  \n",
      "Epoch: [2][2600/2871] Elapsed 22m 4s (remain 2m 17s) Loss: 0.0022(0.0059) Grad: 5720.3760  LR: 0.000014  \n",
      "Epoch: [2][2700/2871] Elapsed 22m 54s (remain 1m 26s) Loss: 0.0000(0.0059) Grad: 150.1539  LR: 0.000014  \n",
      "Epoch: [2][2800/2871] Elapsed 23m 44s (remain 0m 35s) Loss: 0.0084(0.0059) Grad: 9511.7852  LR: 0.000013  \n",
      "Epoch: [2][2870/2871] Elapsed 24m 20s (remain 0m 0s) Loss: 0.0000(0.0059) Grad: 39.0048  LR: 0.000013  \n",
      "EVAL: [0/704] Elapsed 0m 0s (remain 7m 15s) Loss: 0.0007(0.0007) \n",
      "EVAL: [100/704] Elapsed 0m 28s (remain 2m 50s) Loss: 0.0026(0.0084) \n",
      "EVAL: [200/704] Elapsed 0m 56s (remain 2m 20s) Loss: 0.0000(0.0072) \n",
      "EVAL: [300/704] Elapsed 1m 23s (remain 1m 52s) Loss: 0.0001(0.0069) \n",
      "EVAL: [400/704] Elapsed 1m 51s (remain 1m 23s) Loss: 0.0094(0.0077) \n",
      "EVAL: [500/704] Elapsed 2m 18s (remain 0m 56s) Loss: 0.0062(0.0083) \n",
      "EVAL: [600/704] Elapsed 2m 47s (remain 0m 28s) Loss: 0.0000(0.0083) \n",
      "EVAL: [700/704] Elapsed 3m 15s (remain 0m 0s) Loss: 0.0000(0.0078) \n",
      "EVAL: [703/704] Elapsed 3m 16s (remain 0m 0s) Loss: 0.0000(0.0078) \n",
      "Epoch 2 - avg_train_loss: 0.0059  avg_val_loss: 0.0078  time: 1662s\n",
      "Epoch 2 - Score: 0.8692\n",
      "Epoch 2 - Save Best Score: 0.8692 Model\n",
      "Epoch: [3][0/2871] Elapsed 0m 0s (remain 38m 56s) Loss: 0.0001(0.0001) Grad: 344.1596  LR: 0.000013  \n",
      "Epoch: [3][100/2871] Elapsed 0m 51s (remain 23m 39s) Loss: 0.0002(0.0035) Grad: 1207.9785  LR: 0.000013  \n",
      "Epoch: [3][200/2871] Elapsed 1m 42s (remain 22m 38s) Loss: 0.0003(0.0045) Grad: 1090.9712  LR: 0.000013  \n",
      "Epoch: [3][300/2871] Elapsed 2m 33s (remain 21m 50s) Loss: 0.0040(0.0045) Grad: 6820.6611  LR: 0.000013  \n",
      "Epoch: [3][400/2871] Elapsed 3m 26s (remain 21m 10s) Loss: 0.0000(0.0043) Grad: 193.3957  LR: 0.000013  \n",
      "Epoch: [3][500/2871] Elapsed 4m 17s (remain 20m 17s) Loss: 0.0001(0.0045) Grad: 236.9936  LR: 0.000013  \n",
      "Epoch: [3][600/2871] Elapsed 5m 8s (remain 19m 23s) Loss: 0.0000(0.0048) Grad: 142.8712  LR: 0.000012  \n",
      "Epoch: [3][700/2871] Elapsed 5m 58s (remain 18m 29s) Loss: 0.0108(0.0048) Grad: 45849.7227  LR: 0.000012  \n",
      "Epoch: [3][800/2871] Elapsed 6m 48s (remain 17m 36s) Loss: 0.0038(0.0046) Grad: 6870.4424  LR: 0.000012  \n",
      "Epoch: [3][900/2871] Elapsed 7m 39s (remain 16m 43s) Loss: 0.0088(0.0045) Grad: 57183.3477  LR: 0.000012  \n",
      "Epoch: [3][1000/2871] Elapsed 8m 29s (remain 15m 51s) Loss: 0.0109(0.0045) Grad: 25748.9570  LR: 0.000012  \n",
      "Epoch: [3][1100/2871] Elapsed 9m 19s (remain 14m 59s) Loss: 0.0000(0.0048) Grad: 82.3939  LR: 0.000012  \n",
      "Epoch: [3][1200/2871] Elapsed 10m 10s (remain 14m 9s) Loss: 0.0032(0.0047) Grad: 8581.9678  LR: 0.000011  \n",
      "Epoch: [3][1300/2871] Elapsed 11m 2s (remain 13m 19s) Loss: 0.0002(0.0047) Grad: 804.2525  LR: 0.000011  \n",
      "Epoch: [3][1400/2871] Elapsed 11m 52s (remain 12m 27s) Loss: 0.0070(0.0047) Grad: 28220.2383  LR: 0.000011  \n",
      "Epoch: [3][1500/2871] Elapsed 12m 43s (remain 11m 36s) Loss: 0.0058(0.0047) Grad: 14615.5938  LR: 0.000011  \n",
      "Epoch: [3][1600/2871] Elapsed 13m 35s (remain 10m 47s) Loss: 0.0020(0.0048) Grad: 5663.7529  LR: 0.000011  \n",
      "Epoch: [3][1700/2871] Elapsed 14m 26s (remain 9m 56s) Loss: 0.0033(0.0048) Grad: 60878.4453  LR: 0.000011  \n",
      "Epoch: [3][1800/2871] Elapsed 15m 16s (remain 9m 4s) Loss: 0.0001(0.0047) Grad: 320.3818  LR: 0.000011  \n",
      "Epoch: [3][1900/2871] Elapsed 16m 6s (remain 8m 13s) Loss: 0.0000(0.0047) Grad: 328.9787  LR: 0.000010  \n",
      "Epoch: [3][2000/2871] Elapsed 16m 56s (remain 7m 22s) Loss: 0.0002(0.0048) Grad: 1146.8951  LR: 0.000010  \n",
      "Epoch: [3][2100/2871] Elapsed 17m 47s (remain 6m 31s) Loss: 0.0074(0.0048) Grad: 32689.5371  LR: 0.000010  \n",
      "Epoch: [3][2200/2871] Elapsed 18m 40s (remain 5m 41s) Loss: 0.0019(0.0047) Grad: 5206.1538  LR: 0.000010  \n",
      "Epoch: [3][2300/2871] Elapsed 19m 33s (remain 4m 50s) Loss: 0.0000(0.0048) Grad: 100.6425  LR: 0.000010  \n",
      "Epoch: [3][2400/2871] Elapsed 20m 24s (remain 3m 59s) Loss: 0.0001(0.0048) Grad: 200.3809  LR: 0.000010  \n",
      "Epoch: [3][2500/2871] Elapsed 21m 14s (remain 3m 8s) Loss: 0.0123(0.0047) Grad: 16338.1191  LR: 0.000009  \n",
      "Epoch: [3][2600/2871] Elapsed 22m 4s (remain 2m 17s) Loss: 0.0005(0.0047) Grad: 3039.4075  LR: 0.000009  \n",
      "Epoch: [3][2700/2871] Elapsed 22m 55s (remain 1m 26s) Loss: 0.0072(0.0049) Grad: 11157.1270  LR: 0.000009  \n",
      "Epoch: [3][2800/2871] Elapsed 23m 46s (remain 0m 35s) Loss: 0.0000(0.0049) Grad: 123.3228  LR: 0.000009  \n",
      "Epoch: [3][2870/2871] Elapsed 24m 22s (remain 0m 0s) Loss: 0.0001(0.0049) Grad: 386.4916  LR: 0.000009  \n",
      "EVAL: [0/704] Elapsed 0m 0s (remain 7m 16s) Loss: 0.0007(0.0007) \n",
      "EVAL: [100/704] Elapsed 0m 28s (remain 2m 47s) Loss: 0.0036(0.0080) \n",
      "EVAL: [200/704] Elapsed 0m 55s (remain 2m 19s) Loss: 0.0000(0.0069) \n",
      "EVAL: [300/704] Elapsed 1m 23s (remain 1m 51s) Loss: 0.0002(0.0065) \n",
      "EVAL: [400/704] Elapsed 1m 51s (remain 1m 24s) Loss: 0.0090(0.0075) \n",
      "EVAL: [500/704] Elapsed 2m 19s (remain 0m 56s) Loss: 0.0068(0.0083) \n",
      "EVAL: [600/704] Elapsed 2m 48s (remain 0m 28s) Loss: 0.0001(0.0085) \n",
      "EVAL: [700/704] Elapsed 3m 15s (remain 0m 0s) Loss: 0.0000(0.0080) \n",
      "EVAL: [703/704] Elapsed 3m 16s (remain 0m 0s) Loss: 0.0000(0.0079) \n",
      "Epoch 3 - avg_train_loss: 0.0049  avg_val_loss: 0.0079  time: 1664s\n",
      "Epoch 3 - Score: 0.8720\n",
      "Epoch 3 - Save Best Score: 0.8720 Model\n",
      "Epoch: [4][0/2871] Elapsed 0m 0s (remain 37m 49s) Loss: 0.0086(0.0086) Grad: 10954.8730  LR: 0.000009  \n",
      "Epoch: [4][100/2871] Elapsed 0m 50s (remain 23m 18s) Loss: 0.0000(0.0030) Grad: 40.8028  LR: 0.000009  \n",
      "Epoch: [4][200/2871] Elapsed 1m 41s (remain 22m 26s) Loss: 0.0000(0.0038) Grad: 49.1368  LR: 0.000009  \n",
      "Epoch: [4][300/2871] Elapsed 2m 32s (remain 21m 43s) Loss: 0.0001(0.0036) Grad: 583.5991  LR: 0.000008  \n",
      "Epoch: [4][400/2871] Elapsed 3m 23s (remain 20m 52s) Loss: 0.0042(0.0043) Grad: 39298.3008  LR: 0.000008  \n",
      "Epoch: [4][500/2871] Elapsed 4m 13s (remain 19m 59s) Loss: 0.0004(0.0043) Grad: 2873.6125  LR: 0.000008  \n",
      "Epoch: [4][600/2871] Elapsed 5m 4s (remain 19m 9s) Loss: 0.0097(0.0045) Grad: 27754.3652  LR: 0.000008  \n",
      "Epoch: [4][700/2871] Elapsed 5m 55s (remain 18m 19s) Loss: 0.0004(0.0043) Grad: 2206.4834  LR: 0.000008  \n",
      "Epoch: [4][800/2871] Elapsed 6m 45s (remain 17m 28s) Loss: 0.0002(0.0044) Grad: 1380.2655  LR: 0.000008  \n",
      "Epoch: [4][900/2871] Elapsed 7m 35s (remain 16m 36s) Loss: 0.0022(0.0044) Grad: 9399.9111  LR: 0.000007  \n",
      "Epoch: [4][1000/2871] Elapsed 8m 25s (remain 15m 45s) Loss: 0.0031(0.0043) Grad: 9055.0664  LR: 0.000007  \n",
      "Epoch: [4][1100/2871] Elapsed 9m 16s (remain 14m 53s) Loss: 0.0035(0.0042) Grad: 13052.0693  LR: 0.000007  \n",
      "Epoch: [4][1200/2871] Elapsed 10m 6s (remain 14m 2s) Loss: 0.0026(0.0041) Grad: 9108.1348  LR: 0.000007  \n",
      "Epoch: [4][1300/2871] Elapsed 10m 56s (remain 13m 11s) Loss: 0.0001(0.0042) Grad: 658.9839  LR: 0.000007  \n",
      "Epoch: [4][1400/2871] Elapsed 11m 46s (remain 12m 20s) Loss: 0.0026(0.0042) Grad: 5606.0996  LR: 0.000007  \n",
      "Epoch: [4][1500/2871] Elapsed 12m 36s (remain 11m 30s) Loss: 0.0001(0.0041) Grad: 239.7679  LR: 0.000007  \n",
      "Epoch: [4][1600/2871] Elapsed 13m 27s (remain 10m 40s) Loss: 0.0005(0.0043) Grad: 4930.3828  LR: 0.000006  \n",
      "Epoch: [4][1700/2871] Elapsed 14m 18s (remain 9m 50s) Loss: 0.0000(0.0043) Grad: 87.0953  LR: 0.000006  \n",
      "Epoch: [4][1800/2871] Elapsed 15m 8s (remain 8m 59s) Loss: 0.0019(0.0043) Grad: 13595.8320  LR: 0.000006  \n",
      "Epoch: [4][1900/2871] Elapsed 15m 59s (remain 8m 9s) Loss: 0.0000(0.0043) Grad: 14.0969  LR: 0.000006  \n",
      "Epoch: [4][2000/2871] Elapsed 16m 50s (remain 7m 19s) Loss: 0.0022(0.0043) Grad: 7094.6216  LR: 0.000006  \n",
      "Epoch: [4][2100/2871] Elapsed 17m 40s (remain 6m 28s) Loss: 0.0124(0.0042) Grad: 18846.8438  LR: 0.000006  \n",
      "Epoch: [4][2200/2871] Elapsed 18m 31s (remain 5m 38s) Loss: 0.0089(0.0042) Grad: 27357.0625  LR: 0.000005  \n",
      "Epoch: [4][2300/2871] Elapsed 19m 21s (remain 4m 47s) Loss: 0.0051(0.0041) Grad: 21531.9727  LR: 0.000005  \n",
      "Epoch: [4][2400/2871] Elapsed 20m 12s (remain 3m 57s) Loss: 0.0000(0.0041) Grad: 8.0218  LR: 0.000005  \n",
      "Epoch: [4][2500/2871] Elapsed 21m 4s (remain 3m 7s) Loss: 0.0000(0.0041) Grad: 23.0375  LR: 0.000005  \n",
      "Epoch: [4][2600/2871] Elapsed 21m 54s (remain 2m 16s) Loss: 0.0019(0.0041) Grad: 8396.6572  LR: 0.000005  \n",
      "Epoch: [4][2700/2871] Elapsed 22m 44s (remain 1m 25s) Loss: 0.0015(0.0041) Grad: 5460.3350  LR: 0.000005  \n",
      "Epoch: [4][2800/2871] Elapsed 23m 35s (remain 0m 35s) Loss: 0.0000(0.0040) Grad: 45.6337  LR: 0.000005  \n",
      "Epoch: [4][2870/2871] Elapsed 24m 11s (remain 0m 0s) Loss: 0.0001(0.0041) Grad: 231.8709  LR: 0.000004  \n",
      "EVAL: [0/704] Elapsed 0m 0s (remain 6m 57s) Loss: 0.0006(0.0006) \n",
      "EVAL: [100/704] Elapsed 0m 28s (remain 2m 49s) Loss: 0.0069(0.0083) \n",
      "EVAL: [200/704] Elapsed 0m 56s (remain 2m 20s) Loss: 0.0000(0.0074) \n",
      "EVAL: [300/704] Elapsed 1m 23s (remain 1m 52s) Loss: 0.0002(0.0071) \n",
      "EVAL: [400/704] Elapsed 1m 51s (remain 1m 24s) Loss: 0.0120(0.0084) \n",
      "EVAL: [500/704] Elapsed 2m 19s (remain 0m 56s) Loss: 0.0088(0.0094) \n",
      "EVAL: [600/704] Elapsed 2m 47s (remain 0m 28s) Loss: 0.0000(0.0098) \n",
      "EVAL: [700/704] Elapsed 3m 15s (remain 0m 0s) Loss: 0.0000(0.0092) \n",
      "EVAL: [703/704] Elapsed 3m 15s (remain 0m 0s) Loss: 0.0000(0.0092) \n",
      "Epoch 4 - avg_train_loss: 0.0041  avg_val_loss: 0.0092  time: 1652s\n",
      "Epoch 4 - Score: 0.8773\n",
      "Epoch 4 - Save Best Score: 0.8773 Model\n",
      "Epoch: [5][0/2871] Elapsed 0m 0s (remain 40m 13s) Loss: 0.0000(0.0000) Grad: 259.6436  LR: 0.000004  \n",
      "Epoch: [5][100/2871] Elapsed 0m 50s (remain 23m 12s) Loss: 0.0000(0.0029) Grad: 147.0931  LR: 0.000004  \n",
      "Epoch: [5][200/2871] Elapsed 1m 41s (remain 22m 23s) Loss: 0.0000(0.0030) Grad: 16.5106  LR: 0.000004  \n",
      "Epoch: [5][300/2871] Elapsed 2m 32s (remain 21m 39s) Loss: 0.0008(0.0032) Grad: 6882.1934  LR: 0.000004  \n",
      "Epoch: [5][400/2871] Elapsed 3m 23s (remain 20m 52s) Loss: 0.0116(0.0032) Grad: 27571.9355  LR: 0.000004  \n",
      "Epoch: [5][500/2871] Elapsed 4m 13s (remain 19m 59s) Loss: 0.0055(0.0033) Grad: 9289.1367  LR: 0.000004  \n",
      "Epoch: [5][600/2871] Elapsed 5m 3s (remain 19m 7s) Loss: 0.0000(0.0032) Grad: 17.4889  LR: 0.000004  \n",
      "Epoch: [5][700/2871] Elapsed 5m 53s (remain 18m 15s) Loss: 0.0000(0.0031) Grad: 71.3143  LR: 0.000003  \n",
      "Epoch: [5][800/2871] Elapsed 6m 43s (remain 17m 23s) Loss: 0.0000(0.0033) Grad: 65.5152  LR: 0.000003  \n",
      "Epoch: [5][900/2871] Elapsed 7m 34s (remain 16m 34s) Loss: 0.0003(0.0033) Grad: 2271.9368  LR: 0.000003  \n",
      "Epoch: [5][1000/2871] Elapsed 8m 25s (remain 15m 44s) Loss: 0.0000(0.0033) Grad: 19.7658  LR: 0.000003  \n",
      "Epoch: [5][1100/2871] Elapsed 9m 15s (remain 14m 53s) Loss: 0.0001(0.0033) Grad: 1479.5841  LR: 0.000003  \n",
      "Epoch: [5][1200/2871] Elapsed 10m 6s (remain 14m 3s) Loss: 0.0001(0.0033) Grad: 535.8121  LR: 0.000003  \n",
      "Epoch: [5][1300/2871] Elapsed 10m 58s (remain 13m 14s) Loss: 0.0000(0.0033) Grad: 11.1757  LR: 0.000002  \n",
      "Epoch: [5][1400/2871] Elapsed 11m 49s (remain 12m 23s) Loss: 0.0003(0.0035) Grad: 4629.9443  LR: 0.000002  \n",
      "Epoch: [5][1500/2871] Elapsed 12m 39s (remain 11m 33s) Loss: 0.0002(0.0034) Grad: 652.8654  LR: 0.000002  \n",
      "Epoch: [5][1600/2871] Elapsed 13m 29s (remain 10m 42s) Loss: 0.0001(0.0034) Grad: 1433.0679  LR: 0.000002  \n",
      "Epoch: [5][1700/2871] Elapsed 14m 20s (remain 9m 52s) Loss: 0.0000(0.0036) Grad: 316.5952  LR: 0.000002  \n",
      "Epoch: [5][1800/2871] Elapsed 15m 13s (remain 9m 2s) Loss: 0.0007(0.0035) Grad: 4901.1968  LR: 0.000002  \n",
      "Epoch: [5][1900/2871] Elapsed 16m 5s (remain 8m 12s) Loss: 0.0001(0.0035) Grad: 957.6162  LR: 0.000002  \n",
      "Epoch: [5][2000/2871] Elapsed 16m 55s (remain 7m 21s) Loss: 0.0045(0.0034) Grad: 7000.3613  LR: 0.000001  \n",
      "Epoch: [5][2100/2871] Elapsed 17m 45s (remain 6m 30s) Loss: 0.0017(0.0034) Grad: 7138.7734  LR: 0.000001  \n",
      "Epoch: [5][2200/2871] Elapsed 18m 36s (remain 5m 39s) Loss: 0.0000(0.0035) Grad: 434.5886  LR: 0.000001  \n",
      "Epoch: [5][2300/2871] Elapsed 19m 27s (remain 4m 49s) Loss: 0.0000(0.0034) Grad: 15.3668  LR: 0.000001  \n",
      "Epoch: [5][2400/2871] Elapsed 20m 17s (remain 3m 58s) Loss: 0.0018(0.0034) Grad: 7024.5703  LR: 0.000001  \n",
      "Epoch: [5][2500/2871] Elapsed 21m 8s (remain 3m 7s) Loss: 0.0000(0.0034) Grad: 166.8158  LR: 0.000001  \n",
      "Epoch: [5][2600/2871] Elapsed 21m 58s (remain 2m 16s) Loss: 0.0044(0.0035) Grad: 4970.0835  LR: 0.000000  \n",
      "Epoch: [5][2700/2871] Elapsed 22m 48s (remain 1m 26s) Loss: 0.0002(0.0035) Grad: 1623.1411  LR: 0.000000  \n",
      "Epoch: [5][2800/2871] Elapsed 23m 38s (remain 0m 35s) Loss: 0.0001(0.0034) Grad: 1075.1428  LR: 0.000000  \n",
      "Epoch: [5][2870/2871] Elapsed 24m 13s (remain 0m 0s) Loss: 0.0085(0.0034) Grad: 88866.0391  LR: 0.000000  \n",
      "EVAL: [0/704] Elapsed 0m 0s (remain 7m 18s) Loss: 0.0003(0.0003) \n",
      "EVAL: [100/704] Elapsed 0m 28s (remain 2m 48s) Loss: 0.0059(0.0091) \n",
      "EVAL: [200/704] Elapsed 0m 57s (remain 2m 22s) Loss: 0.0000(0.0081) \n",
      "EVAL: [300/704] Elapsed 1m 24s (remain 1m 53s) Loss: 0.0001(0.0080) \n",
      "EVAL: [400/704] Elapsed 1m 52s (remain 1m 24s) Loss: 0.0116(0.0090) \n",
      "EVAL: [500/704] Elapsed 2m 20s (remain 0m 56s) Loss: 0.0093(0.0099) \n",
      "EVAL: [600/704] Elapsed 2m 48s (remain 0m 28s) Loss: 0.0000(0.0102) \n",
      "EVAL: [700/704] Elapsed 3m 15s (remain 0m 0s) Loss: 0.0000(0.0096) \n",
      "EVAL: [703/704] Elapsed 3m 16s (remain 0m 0s) Loss: 0.0000(0.0096) \n",
      "Epoch 5 - avg_train_loss: 0.0034  avg_val_loss: 0.0096  time: 1655s\n",
      "Epoch 5 - Score: 0.8768\n",
      "========== fold: 3 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2877] Elapsed 0m 0s (remain 38m 49s) Loss: 0.5607(0.5607) Grad: inf  LR: 0.000000  \n",
      "Epoch: [1][100/2877] Elapsed 0m 52s (remain 24m 7s) Loss: 0.5110(0.5243) Grad: inf  LR: 0.000001  \n",
      "Epoch: [1][200/2877] Elapsed 1m 43s (remain 22m 55s) Loss: 0.1174(0.4009) Grad: 9952.3496  LR: 0.000003  \n",
      "Epoch: [1][300/2877] Elapsed 2m 34s (remain 21m 58s) Loss: 0.0283(0.2855) Grad: 963.2571  LR: 0.000004  \n",
      "Epoch: [1][400/2877] Elapsed 3m 24s (remain 21m 2s) Loss: 0.0263(0.2241) Grad: 930.7817  LR: 0.000006  \n",
      "Epoch: [1][500/2877] Elapsed 4m 15s (remain 20m 9s) Loss: 0.0398(0.1870) Grad: 1483.2206  LR: 0.000007  \n",
      "Epoch: [1][600/2877] Elapsed 5m 5s (remain 19m 17s) Loss: 0.0343(0.1618) Grad: 1867.9656  LR: 0.000008  \n",
      "Epoch: [1][700/2877] Elapsed 5m 57s (remain 18m 29s) Loss: 0.0072(0.1423) Grad: 1573.2526  LR: 0.000010  \n",
      "Epoch: [1][800/2877] Elapsed 6m 48s (remain 17m 37s) Loss: 0.0020(0.1268) Grad: 571.1824  LR: 0.000011  \n",
      "Epoch: [1][900/2877] Elapsed 7m 38s (remain 16m 46s) Loss: 0.0034(0.1144) Grad: 2020.1418  LR: 0.000013  \n",
      "Epoch: [1][1000/2877] Elapsed 8m 29s (remain 15m 54s) Loss: 0.0069(0.1042) Grad: 979.9125  LR: 0.000014  \n",
      "Epoch: [1][1100/2877] Elapsed 9m 20s (remain 15m 4s) Loss: 0.0097(0.0957) Grad: 6133.9326  LR: 0.000015  \n",
      "Epoch: [1][1200/2877] Elapsed 10m 13s (remain 14m 15s) Loss: 0.0131(0.0888) Grad: 5120.4102  LR: 0.000017  \n",
      "Epoch: [1][1300/2877] Elapsed 11m 4s (remain 13m 24s) Loss: 0.0231(0.0828) Grad: 3223.8054  LR: 0.000018  \n",
      "Epoch: [1][1400/2877] Elapsed 11m 55s (remain 12m 33s) Loss: 0.0139(0.0777) Grad: 1501.4844  LR: 0.000019  \n",
      "Epoch: [1][1500/2877] Elapsed 12m 48s (remain 11m 44s) Loss: 0.0017(0.0733) Grad: 847.9063  LR: 0.000020  \n",
      "Epoch: [1][1600/2877] Elapsed 13m 38s (remain 10m 52s) Loss: 0.0016(0.0693) Grad: 590.2435  LR: 0.000020  \n",
      "Epoch: [1][1700/2877] Elapsed 14m 28s (remain 10m 0s) Loss: 0.0286(0.0660) Grad: 9871.6543  LR: 0.000020  \n",
      "Epoch: [1][1800/2877] Elapsed 15m 19s (remain 9m 9s) Loss: 0.0054(0.0628) Grad: 863.0071  LR: 0.000019  \n",
      "Epoch: [1][1900/2877] Elapsed 16m 11s (remain 8m 18s) Loss: 0.0046(0.0600) Grad: 930.6978  LR: 0.000019  \n",
      "Epoch: [1][2000/2877] Elapsed 17m 1s (remain 7m 27s) Loss: 0.0055(0.0574) Grad: 1220.1887  LR: 0.000019  \n",
      "Epoch: [1][2100/2877] Elapsed 17m 52s (remain 6m 36s) Loss: 0.0050(0.0551) Grad: 650.6840  LR: 0.000019  \n",
      "Epoch: [1][2200/2877] Elapsed 18m 42s (remain 5m 44s) Loss: 0.0015(0.0530) Grad: 431.3770  LR: 0.000019  \n",
      "Epoch: [1][2300/2877] Elapsed 19m 32s (remain 4m 53s) Loss: 0.0102(0.0510) Grad: 814.5349  LR: 0.000019  \n",
      "Epoch: [1][2400/2877] Elapsed 20m 22s (remain 4m 2s) Loss: 0.0052(0.0492) Grad: 1173.0642  LR: 0.000019  \n",
      "Epoch: [1][2500/2877] Elapsed 21m 13s (remain 3m 11s) Loss: 0.0062(0.0475) Grad: 917.0492  LR: 0.000018  \n",
      "Epoch: [1][2600/2877] Elapsed 22m 4s (remain 2m 20s) Loss: 0.0002(0.0460) Grad: 75.2281  LR: 0.000018  \n",
      "Epoch: [1][2700/2877] Elapsed 22m 56s (remain 1m 29s) Loss: 0.0037(0.0446) Grad: 728.2426  LR: 0.000018  \n",
      "Epoch: [1][2800/2877] Elapsed 23m 47s (remain 0m 38s) Loss: 0.0366(0.0433) Grad: 4123.6685  LR: 0.000018  \n",
      "Epoch: [1][2876/2877] Elapsed 24m 25s (remain 0m 0s) Loss: 0.0014(0.0423) Grad: 613.7996  LR: 0.000018  \n",
      "EVAL: [0/698] Elapsed 0m 0s (remain 6m 44s) Loss: 0.0019(0.0019) \n",
      "EVAL: [100/698] Elapsed 0m 28s (remain 2m 50s) Loss: 0.0032(0.0062) \n",
      "EVAL: [200/698] Elapsed 0m 56s (remain 2m 20s) Loss: 0.0181(0.0083) \n",
      "EVAL: [300/698] Elapsed 1m 24s (remain 1m 51s) Loss: 0.0017(0.0081) \n",
      "EVAL: [400/698] Elapsed 1m 51s (remain 1m 22s) Loss: 0.0379(0.0088) \n",
      "EVAL: [500/698] Elapsed 2m 20s (remain 0m 55s) Loss: 0.0049(0.0089) \n",
      "EVAL: [600/698] Elapsed 2m 48s (remain 0m 27s) Loss: 0.0090(0.0085) \n",
      "EVAL: [697/698] Elapsed 3m 14s (remain 0m 0s) Loss: 0.0000(0.0084) \n",
      "Epoch 1 - avg_train_loss: 0.0423  avg_val_loss: 0.0084  time: 1665s\n",
      "Epoch 1 - Score: 0.8548\n",
      "Epoch 1 - Save Best Score: 0.8548 Model\n",
      "Epoch: [2][0/2877] Elapsed 0m 0s (remain 37m 5s) Loss: 0.0053(0.0053) Grad: 16241.2305  LR: 0.000018  \n",
      "Epoch: [2][100/2877] Elapsed 0m 50s (remain 23m 20s) Loss: 0.0001(0.0072) Grad: 460.7592  LR: 0.000018  \n",
      "Epoch: [2][200/2877] Elapsed 1m 42s (remain 22m 47s) Loss: 0.0001(0.0073) Grad: 612.9837  LR: 0.000017  \n",
      "Epoch: [2][300/2877] Elapsed 2m 33s (remain 21m 49s) Loss: 0.0337(0.0068) Grad: 40249.2266  LR: 0.000017  \n",
      "Epoch: [2][400/2877] Elapsed 3m 23s (remain 20m 56s) Loss: 0.0000(0.0066) Grad: 188.9160  LR: 0.000017  \n",
      "Epoch: [2][500/2877] Elapsed 4m 14s (remain 20m 7s) Loss: 0.0097(0.0065) Grad: 27847.8145  LR: 0.000017  \n",
      "Epoch: [2][600/2877] Elapsed 5m 6s (remain 19m 20s) Loss: 0.0033(0.0064) Grad: 30354.4297  LR: 0.000017  \n",
      "Epoch: [2][700/2877] Elapsed 5m 56s (remain 18m 27s) Loss: 0.0001(0.0064) Grad: 152.3243  LR: 0.000017  \n",
      "Epoch: [2][800/2877] Elapsed 6m 48s (remain 17m 38s) Loss: 0.0027(0.0066) Grad: 12051.4229  LR: 0.000017  \n",
      "Epoch: [2][900/2877] Elapsed 7m 39s (remain 16m 46s) Loss: 0.0009(0.0065) Grad: 2161.2358  LR: 0.000016  \n",
      "Epoch: [2][1000/2877] Elapsed 8m 29s (remain 15m 55s) Loss: 0.0533(0.0067) Grad: 51625.3594  LR: 0.000016  \n",
      "Epoch: [2][1100/2877] Elapsed 9m 20s (remain 15m 3s) Loss: 0.0005(0.0066) Grad: 1543.3909  LR: 0.000016  \n",
      "Epoch: [2][1200/2877] Elapsed 10m 10s (remain 14m 11s) Loss: 0.0048(0.0065) Grad: 6419.2075  LR: 0.000016  \n",
      "Epoch: [2][1300/2877] Elapsed 11m 1s (remain 13m 21s) Loss: 0.0012(0.0066) Grad: 3837.9482  LR: 0.000016  \n",
      "Epoch: [2][1400/2877] Elapsed 11m 52s (remain 12m 30s) Loss: 0.0195(0.0065) Grad: 36810.3594  LR: 0.000016  \n",
      "Epoch: [2][1500/2877] Elapsed 12m 43s (remain 11m 39s) Loss: 0.0065(0.0065) Grad: 35222.0938  LR: 0.000015  \n",
      "Epoch: [2][1600/2877] Elapsed 13m 33s (remain 10m 48s) Loss: 0.0039(0.0066) Grad: 3454.9653  LR: 0.000015  \n",
      "Epoch: [2][1700/2877] Elapsed 14m 24s (remain 9m 57s) Loss: 0.0118(0.0065) Grad: 14328.8877  LR: 0.000015  \n",
      "Epoch: [2][1800/2877] Elapsed 15m 17s (remain 9m 7s) Loss: 0.0003(0.0065) Grad: 1201.9280  LR: 0.000015  \n",
      "Epoch: [2][1900/2877] Elapsed 16m 7s (remain 8m 16s) Loss: 0.0185(0.0064) Grad: 32378.8730  LR: 0.000015  \n",
      "Epoch: [2][2000/2877] Elapsed 16m 58s (remain 7m 25s) Loss: 0.0008(0.0064) Grad: 2701.4209  LR: 0.000015  \n",
      "Epoch: [2][2100/2877] Elapsed 17m 49s (remain 6m 35s) Loss: 0.0087(0.0065) Grad: 20859.5801  LR: 0.000015  \n",
      "Epoch: [2][2200/2877] Elapsed 18m 40s (remain 5m 44s) Loss: 0.0003(0.0065) Grad: 1703.3076  LR: 0.000014  \n",
      "Epoch: [2][2300/2877] Elapsed 19m 30s (remain 4m 53s) Loss: 0.0021(0.0064) Grad: 6274.3799  LR: 0.000014  \n",
      "Epoch: [2][2400/2877] Elapsed 20m 21s (remain 4m 2s) Loss: 0.0003(0.0064) Grad: 1708.5969  LR: 0.000014  \n",
      "Epoch: [2][2500/2877] Elapsed 21m 11s (remain 3m 11s) Loss: 0.0000(0.0064) Grad: 54.5295  LR: 0.000014  \n",
      "Epoch: [2][2600/2877] Elapsed 22m 2s (remain 2m 20s) Loss: 0.0061(0.0064) Grad: 20670.4180  LR: 0.000014  \n",
      "Epoch: [2][2700/2877] Elapsed 22m 53s (remain 1m 29s) Loss: 0.0080(0.0064) Grad: 22621.3047  LR: 0.000014  \n",
      "Epoch: [2][2800/2877] Elapsed 23m 44s (remain 0m 38s) Loss: 0.0001(0.0064) Grad: 494.6836  LR: 0.000013  \n",
      "Epoch: [2][2876/2877] Elapsed 24m 22s (remain 0m 0s) Loss: 0.0259(0.0064) Grad: 32794.0898  LR: 0.000013  \n",
      "EVAL: [0/698] Elapsed 0m 0s (remain 6m 49s) Loss: 0.0009(0.0009) \n",
      "EVAL: [100/698] Elapsed 0m 28s (remain 2m 47s) Loss: 0.0036(0.0055) \n",
      "EVAL: [200/698] Elapsed 0m 55s (remain 2m 18s) Loss: 0.0008(0.0080) \n",
      "EVAL: [300/698] Elapsed 1m 23s (remain 1m 50s) Loss: 0.0043(0.0077) \n",
      "EVAL: [400/698] Elapsed 1m 51s (remain 1m 22s) Loss: 0.0448(0.0081) \n",
      "EVAL: [500/698] Elapsed 2m 19s (remain 0m 54s) Loss: 0.0078(0.0080) \n",
      "EVAL: [600/698] Elapsed 2m 47s (remain 0m 27s) Loss: 0.0060(0.0078) \n",
      "EVAL: [697/698] Elapsed 3m 14s (remain 0m 0s) Loss: 0.0000(0.0076) \n",
      "Epoch 2 - avg_train_loss: 0.0064  avg_val_loss: 0.0076  time: 1662s\n",
      "Epoch 2 - Score: 0.8736\n",
      "Epoch 2 - Save Best Score: 0.8736 Model\n",
      "Epoch: [3][0/2877] Elapsed 0m 0s (remain 39m 54s) Loss: 0.0030(0.0030) Grad: 9841.9434  LR: 0.000013  \n",
      "Epoch: [3][100/2877] Elapsed 0m 51s (remain 23m 46s) Loss: 0.0042(0.0044) Grad: 14938.5049  LR: 0.000013  \n",
      "Epoch: [3][200/2877] Elapsed 1m 42s (remain 22m 47s) Loss: 0.0028(0.0046) Grad: 4912.6660  LR: 0.000013  \n",
      "Epoch: [3][300/2877] Elapsed 2m 33s (remain 21m 50s) Loss: 0.0002(0.0049) Grad: 3663.0989  LR: 0.000013  \n",
      "Epoch: [3][400/2877] Elapsed 3m 23s (remain 20m 57s) Loss: 0.0000(0.0048) Grad: 26.6582  LR: 0.000013  \n",
      "Epoch: [3][500/2877] Elapsed 4m 14s (remain 20m 6s) Loss: 0.0066(0.0050) Grad: 85076.5156  LR: 0.000013  \n",
      "Epoch: [3][600/2877] Elapsed 5m 5s (remain 19m 16s) Loss: 0.0076(0.0052) Grad: 14014.5625  LR: 0.000012  \n",
      "Epoch: [3][700/2877] Elapsed 5m 56s (remain 18m 25s) Loss: 0.0016(0.0050) Grad: 3290.5049  LR: 0.000012  \n",
      "Epoch: [3][800/2877] Elapsed 6m 46s (remain 17m 33s) Loss: 0.0028(0.0050) Grad: 6034.7876  LR: 0.000012  \n",
      "Epoch: [3][900/2877] Elapsed 7m 37s (remain 16m 42s) Loss: 0.0051(0.0048) Grad: 18979.9473  LR: 0.000012  \n",
      "Epoch: [3][1000/2877] Elapsed 8m 28s (remain 15m 52s) Loss: 0.0002(0.0051) Grad: 1087.4966  LR: 0.000012  \n",
      "Epoch: [3][1100/2877] Elapsed 9m 19s (remain 15m 2s) Loss: 0.0001(0.0052) Grad: 615.2476  LR: 0.000012  \n",
      "Epoch: [3][1200/2877] Elapsed 10m 10s (remain 14m 11s) Loss: 0.0012(0.0051) Grad: 4130.2661  LR: 0.000011  \n",
      "Epoch: [3][1300/2877] Elapsed 11m 0s (remain 13m 20s) Loss: 0.0137(0.0049) Grad: 28058.6211  LR: 0.000011  \n",
      "Epoch: [3][1400/2877] Elapsed 11m 51s (remain 12m 29s) Loss: 0.0016(0.0050) Grad: 4546.5298  LR: 0.000011  \n",
      "Epoch: [3][1500/2877] Elapsed 12m 41s (remain 11m 38s) Loss: 0.0052(0.0051) Grad: 18052.7461  LR: 0.000011  \n",
      "Epoch: [3][1600/2877] Elapsed 13m 31s (remain 10m 47s) Loss: 0.0000(0.0051) Grad: 18.0587  LR: 0.000011  \n",
      "Epoch: [3][1700/2877] Elapsed 14m 24s (remain 9m 57s) Loss: 0.0022(0.0050) Grad: 7316.5767  LR: 0.000011  \n",
      "Epoch: [3][1800/2877] Elapsed 15m 15s (remain 9m 6s) Loss: 0.0193(0.0050) Grad: 36867.9961  LR: 0.000011  \n",
      "Epoch: [3][1900/2877] Elapsed 16m 5s (remain 8m 15s) Loss: 0.0006(0.0051) Grad: 1900.4257  LR: 0.000010  \n",
      "Epoch: [3][2000/2877] Elapsed 16m 56s (remain 7m 25s) Loss: 0.0000(0.0051) Grad: 40.6659  LR: 0.000010  \n",
      "Epoch: [3][2100/2877] Elapsed 17m 46s (remain 6m 34s) Loss: 0.0001(0.0052) Grad: 996.3578  LR: 0.000010  \n",
      "Epoch: [3][2200/2877] Elapsed 18m 37s (remain 5m 43s) Loss: 0.0169(0.0052) Grad: 53238.9648  LR: 0.000010  \n",
      "Epoch: [3][2300/2877] Elapsed 19m 27s (remain 4m 52s) Loss: 0.0225(0.0053) Grad: 32918.1562  LR: 0.000010  \n",
      "Epoch: [3][2400/2877] Elapsed 20m 18s (remain 4m 1s) Loss: 0.0401(0.0053) Grad: 53644.0039  LR: 0.000010  \n",
      "Epoch: [3][2500/2877] Elapsed 21m 9s (remain 3m 10s) Loss: 0.0057(0.0053) Grad: 15249.7305  LR: 0.000009  \n",
      "Epoch: [3][2600/2877] Elapsed 22m 0s (remain 2m 20s) Loss: 0.0066(0.0053) Grad: 8998.8906  LR: 0.000009  \n",
      "Epoch: [3][2700/2877] Elapsed 22m 50s (remain 1m 29s) Loss: 0.0110(0.0052) Grad: 70184.6250  LR: 0.000009  \n",
      "Epoch: [3][2800/2877] Elapsed 23m 40s (remain 0m 38s) Loss: 0.0150(0.0052) Grad: 26466.1953  LR: 0.000009  \n",
      "Epoch: [3][2876/2877] Elapsed 24m 19s (remain 0m 0s) Loss: 0.0016(0.0052) Grad: 4526.0464  LR: 0.000009  \n",
      "EVAL: [0/698] Elapsed 0m 0s (remain 7m 3s) Loss: 0.0028(0.0028) \n",
      "EVAL: [100/698] Elapsed 0m 28s (remain 2m 47s) Loss: 0.0015(0.0050) \n",
      "EVAL: [200/698] Elapsed 0m 55s (remain 2m 18s) Loss: 0.0008(0.0073) \n",
      "EVAL: [300/698] Elapsed 1m 23s (remain 1m 50s) Loss: 0.0022(0.0072) \n",
      "EVAL: [400/698] Elapsed 1m 50s (remain 1m 22s) Loss: 0.0268(0.0077) \n",
      "EVAL: [500/698] Elapsed 2m 18s (remain 0m 54s) Loss: 0.0109(0.0077) \n",
      "EVAL: [600/698] Elapsed 2m 46s (remain 0m 26s) Loss: 0.0024(0.0074) \n",
      "EVAL: [697/698] Elapsed 3m 13s (remain 0m 0s) Loss: 0.0000(0.0071) \n",
      "Epoch 3 - avg_train_loss: 0.0052  avg_val_loss: 0.0071  time: 1658s\n",
      "Epoch 3 - Score: 0.8856\n",
      "Epoch 3 - Save Best Score: 0.8856 Model\n",
      "Epoch: [4][0/2877] Elapsed 0m 0s (remain 41m 26s) Loss: 0.0028(0.0028) Grad: 23819.3301  LR: 0.000009  \n",
      "Epoch: [4][100/2877] Elapsed 0m 51s (remain 23m 25s) Loss: 0.0003(0.0043) Grad: 1693.4913  LR: 0.000009  \n",
      "Epoch: [4][200/2877] Elapsed 1m 41s (remain 22m 35s) Loss: 0.0079(0.0041) Grad: 13215.7178  LR: 0.000009  \n",
      "Epoch: [4][300/2877] Elapsed 2m 33s (remain 21m 49s) Loss: 0.0050(0.0039) Grad: 9824.8838  LR: 0.000008  \n",
      "Epoch: [4][400/2877] Elapsed 3m 23s (remain 20m 53s) Loss: 0.0009(0.0037) Grad: 2065.0957  LR: 0.000008  \n",
      "Epoch: [4][500/2877] Elapsed 4m 13s (remain 20m 0s) Loss: 0.0036(0.0038) Grad: 14078.8047  LR: 0.000008  \n",
      "Epoch: [4][600/2877] Elapsed 5m 3s (remain 19m 8s) Loss: 0.0071(0.0036) Grad: 27906.6523  LR: 0.000008  \n",
      "Epoch: [4][700/2877] Elapsed 5m 53s (remain 18m 18s) Loss: 0.0000(0.0038) Grad: 151.7593  LR: 0.000008  \n",
      "Epoch: [4][800/2877] Elapsed 6m 44s (remain 17m 28s) Loss: 0.0058(0.0039) Grad: 30304.4395  LR: 0.000008  \n",
      "Epoch: [4][900/2877] Elapsed 7m 35s (remain 16m 38s) Loss: 0.0002(0.0040) Grad: 1577.5884  LR: 0.000007  \n",
      "Epoch: [4][1000/2877] Elapsed 8m 25s (remain 15m 47s) Loss: 0.0402(0.0041) Grad: 55456.6953  LR: 0.000007  \n",
      "Epoch: [4][1100/2877] Elapsed 9m 15s (remain 14m 55s) Loss: 0.0066(0.0040) Grad: 7938.7144  LR: 0.000007  \n",
      "Epoch: [4][1200/2877] Elapsed 10m 5s (remain 14m 5s) Loss: 0.0000(0.0040) Grad: 676.1921  LR: 0.000007  \n",
      "Epoch: [4][1300/2877] Elapsed 10m 56s (remain 13m 15s) Loss: 0.0020(0.0042) Grad: 8729.7061  LR: 0.000007  \n",
      "Epoch: [4][1400/2877] Elapsed 11m 47s (remain 12m 25s) Loss: 0.0016(0.0041) Grad: 9418.7373  LR: 0.000007  \n",
      "Epoch: [4][1500/2877] Elapsed 12m 37s (remain 11m 34s) Loss: 0.0028(0.0041) Grad: 11725.2900  LR: 0.000007  \n",
      "Epoch: [4][1600/2877] Elapsed 13m 27s (remain 10m 43s) Loss: 0.0084(0.0040) Grad: 15975.3398  LR: 0.000006  \n",
      "Epoch: [4][1700/2877] Elapsed 14m 17s (remain 9m 53s) Loss: 0.0298(0.0041) Grad: 29421.3945  LR: 0.000006  \n",
      "Epoch: [4][1800/2877] Elapsed 15m 8s (remain 9m 2s) Loss: 0.0053(0.0041) Grad: 35263.1328  LR: 0.000006  \n",
      "Epoch: [4][1900/2877] Elapsed 15m 59s (remain 8m 12s) Loss: 0.0000(0.0041) Grad: 288.3994  LR: 0.000006  \n",
      "Epoch: [4][2000/2877] Elapsed 16m 50s (remain 7m 22s) Loss: 0.0366(0.0040) Grad: 38752.5781  LR: 0.000006  \n",
      "Epoch: [4][2100/2877] Elapsed 17m 41s (remain 6m 31s) Loss: 0.0001(0.0040) Grad: 923.2594  LR: 0.000006  \n",
      "Epoch: [4][2200/2877] Elapsed 18m 31s (remain 5m 41s) Loss: 0.0221(0.0040) Grad: 60205.0117  LR: 0.000005  \n",
      "Epoch: [4][2300/2877] Elapsed 19m 21s (remain 4m 50s) Loss: 0.0000(0.0041) Grad: 201.7835  LR: 0.000005  \n",
      "Epoch: [4][2400/2877] Elapsed 20m 12s (remain 4m 0s) Loss: 0.0577(0.0040) Grad: 35795.2734  LR: 0.000005  \n",
      "Epoch: [4][2500/2877] Elapsed 21m 4s (remain 3m 10s) Loss: 0.0021(0.0041) Grad: 5532.6479  LR: 0.000005  \n",
      "Epoch: [4][2600/2877] Elapsed 21m 54s (remain 2m 19s) Loss: 0.0002(0.0042) Grad: 1272.7646  LR: 0.000005  \n",
      "Epoch: [4][2700/2877] Elapsed 22m 45s (remain 1m 28s) Loss: 0.0062(0.0042) Grad: 14316.6523  LR: 0.000005  \n",
      "Epoch: [4][2800/2877] Elapsed 23m 37s (remain 0m 38s) Loss: 0.0027(0.0042) Grad: 11630.4893  LR: 0.000005  \n",
      "Epoch: [4][2876/2877] Elapsed 24m 16s (remain 0m 0s) Loss: 0.0041(0.0042) Grad: 7951.6177  LR: 0.000004  \n",
      "EVAL: [0/698] Elapsed 0m 0s (remain 6m 49s) Loss: 0.0008(0.0008) \n",
      "EVAL: [100/698] Elapsed 0m 28s (remain 2m 49s) Loss: 0.0010(0.0057) \n",
      "EVAL: [200/698] Elapsed 0m 57s (remain 2m 22s) Loss: 0.0015(0.0083) \n",
      "EVAL: [300/698] Elapsed 1m 25s (remain 1m 52s) Loss: 0.0033(0.0081) \n",
      "EVAL: [400/698] Elapsed 1m 52s (remain 1m 23s) Loss: 0.0277(0.0085) \n",
      "EVAL: [500/698] Elapsed 2m 20s (remain 0m 55s) Loss: 0.0069(0.0085) \n",
      "EVAL: [600/698] Elapsed 2m 47s (remain 0m 27s) Loss: 0.0050(0.0081) \n",
      "EVAL: [697/698] Elapsed 3m 14s (remain 0m 0s) Loss: 0.0000(0.0078) \n",
      "Epoch 4 - avg_train_loss: 0.0042  avg_val_loss: 0.0078  time: 1656s\n",
      "Epoch 4 - Score: 0.8888\n",
      "Epoch 4 - Save Best Score: 0.8888 Model\n",
      "Epoch: [5][0/2877] Elapsed 0m 0s (remain 40m 48s) Loss: 0.0104(0.0104) Grad: 15109.4023  LR: 0.000004  \n",
      "Epoch: [5][100/2877] Elapsed 0m 51s (remain 23m 30s) Loss: 0.0073(0.0028) Grad: 19063.9648  LR: 0.000004  \n",
      "Epoch: [5][200/2877] Elapsed 1m 42s (remain 22m 46s) Loss: 0.0015(0.0026) Grad: 6817.6006  LR: 0.000004  \n",
      "Epoch: [5][300/2877] Elapsed 2m 32s (remain 21m 48s) Loss: 0.0029(0.0028) Grad: 15684.6182  LR: 0.000004  \n",
      "Epoch: [5][400/2877] Elapsed 3m 23s (remain 20m 54s) Loss: 0.0003(0.0028) Grad: 1850.5190  LR: 0.000004  \n",
      "Epoch: [5][500/2877] Elapsed 4m 13s (remain 20m 0s) Loss: 0.0001(0.0029) Grad: 305.9181  LR: 0.000004  \n",
      "Epoch: [5][600/2877] Elapsed 5m 3s (remain 19m 10s) Loss: 0.0021(0.0031) Grad: 13189.6318  LR: 0.000004  \n",
      "Epoch: [5][700/2877] Elapsed 5m 55s (remain 18m 22s) Loss: 0.0000(0.0033) Grad: 130.9665  LR: 0.000003  \n",
      "Epoch: [5][800/2877] Elapsed 6m 45s (remain 17m 30s) Loss: 0.0006(0.0032) Grad: 4167.7832  LR: 0.000003  \n",
      "Epoch: [5][900/2877] Elapsed 7m 35s (remain 16m 38s) Loss: 0.0017(0.0032) Grad: 12338.0938  LR: 0.000003  \n",
      "Epoch: [5][1000/2877] Elapsed 8m 25s (remain 15m 48s) Loss: 0.0008(0.0032) Grad: 4419.8623  LR: 0.000003  \n",
      "Epoch: [5][1100/2877] Elapsed 9m 17s (remain 14m 58s) Loss: 0.0074(0.0032) Grad: 21177.0918  LR: 0.000003  \n",
      "Epoch: [5][1200/2877] Elapsed 10m 8s (remain 14m 8s) Loss: 0.0000(0.0033) Grad: 24.0843  LR: 0.000003  \n",
      "Epoch: [5][1300/2877] Elapsed 10m 58s (remain 13m 18s) Loss: 0.0514(0.0033) Grad: 113921.0156  LR: 0.000002  \n",
      "Epoch: [5][1400/2877] Elapsed 11m 49s (remain 12m 27s) Loss: 0.0016(0.0032) Grad: 10036.4971  LR: 0.000002  \n",
      "Epoch: [5][1500/2877] Elapsed 12m 39s (remain 11m 36s) Loss: 0.0001(0.0034) Grad: 483.0412  LR: 0.000002  \n",
      "Epoch: [5][1600/2877] Elapsed 13m 29s (remain 10m 45s) Loss: 0.0004(0.0033) Grad: 2611.1257  LR: 0.000002  \n",
      "Epoch: [5][1700/2877] Elapsed 14m 19s (remain 9m 54s) Loss: 0.0000(0.0034) Grad: 12.0651  LR: 0.000002  \n",
      "Epoch: [5][1800/2877] Elapsed 15m 10s (remain 9m 4s) Loss: 0.0000(0.0035) Grad: 234.2557  LR: 0.000002  \n",
      "Epoch: [5][1900/2877] Elapsed 16m 0s (remain 8m 13s) Loss: 0.0002(0.0035) Grad: 920.3159  LR: 0.000002  \n",
      "Epoch: [5][2000/2877] Elapsed 16m 51s (remain 7m 22s) Loss: 0.0064(0.0035) Grad: 41153.6641  LR: 0.000001  \n",
      "Epoch: [5][2100/2877] Elapsed 17m 41s (remain 6m 32s) Loss: 0.0000(0.0035) Grad: 48.0338  LR: 0.000001  \n",
      "Epoch: [5][2200/2877] Elapsed 18m 32s (remain 5m 41s) Loss: 0.0003(0.0035) Grad: 2449.4160  LR: 0.000001  \n",
      "Epoch: [5][2300/2877] Elapsed 19m 24s (remain 4m 51s) Loss: 0.0030(0.0035) Grad: 5301.1196  LR: 0.000001  \n",
      "Epoch: [5][2400/2877] Elapsed 20m 14s (remain 4m 0s) Loss: 0.0010(0.0034) Grad: 8207.2383  LR: 0.000001  \n",
      "Epoch: [5][2500/2877] Elapsed 21m 4s (remain 3m 10s) Loss: 0.0000(0.0034) Grad: 18.7437  LR: 0.000001  \n",
      "Epoch: [5][2600/2877] Elapsed 21m 55s (remain 2m 19s) Loss: 0.0000(0.0034) Grad: 396.6389  LR: 0.000000  \n",
      "Epoch: [5][2700/2877] Elapsed 22m 46s (remain 1m 29s) Loss: 0.0025(0.0034) Grad: 9535.9629  LR: 0.000000  \n",
      "Epoch: [5][2800/2877] Elapsed 23m 36s (remain 0m 38s) Loss: 0.0000(0.0034) Grad: 52.6685  LR: 0.000000  \n",
      "Epoch: [5][2876/2877] Elapsed 24m 14s (remain 0m 0s) Loss: 0.0032(0.0034) Grad: 7059.4419  LR: 0.000000  \n",
      "EVAL: [0/698] Elapsed 0m 0s (remain 6m 29s) Loss: 0.0013(0.0013) \n",
      "EVAL: [100/698] Elapsed 0m 28s (remain 2m 46s) Loss: 0.0013(0.0066) \n",
      "EVAL: [200/698] Elapsed 0m 55s (remain 2m 17s) Loss: 0.0004(0.0094) \n",
      "EVAL: [300/698] Elapsed 1m 23s (remain 1m 50s) Loss: 0.0052(0.0092) \n",
      "EVAL: [400/698] Elapsed 1m 51s (remain 1m 22s) Loss: 0.0276(0.0095) \n",
      "EVAL: [500/698] Elapsed 2m 19s (remain 0m 54s) Loss: 0.0061(0.0095) \n",
      "EVAL: [600/698] Elapsed 2m 47s (remain 0m 27s) Loss: 0.0041(0.0090) \n",
      "EVAL: [697/698] Elapsed 3m 13s (remain 0m 0s) Loss: 0.0000(0.0087) \n",
      "Epoch 5 - avg_train_loss: 0.0034  avg_val_loss: 0.0087  time: 1653s\n",
      "Epoch 5 - Score: 0.8905\n",
      "Epoch 5 - Save Best Score: 0.8905 Model\n",
      "========== fold: 4 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/2850] Elapsed 0m 0s (remain 40m 13s) Loss: 0.6354(0.6354) Grad: inf  LR: 0.000000  \n",
      "Epoch: [1][100/2850] Elapsed 0m 50s (remain 23m 7s) Loss: 0.3413(0.4965) Grad: 14548.1875  LR: 0.000001  \n",
      "Epoch: [1][200/2850] Elapsed 1m 41s (remain 22m 20s) Loss: 0.1000(0.3509) Grad: 6933.5356  LR: 0.000003  \n",
      "Epoch: [1][300/2850] Elapsed 2m 32s (remain 21m 34s) Loss: 0.0404(0.2501) Grad: 562.3069  LR: 0.000004  \n",
      "Epoch: [1][400/2850] Elapsed 3m 22s (remain 20m 38s) Loss: 0.0430(0.1973) Grad: 6325.8765  LR: 0.000006  \n",
      "Epoch: [1][500/2850] Elapsed 4m 12s (remain 19m 45s) Loss: 0.0500(0.1655) Grad: 1009.2373  LR: 0.000007  \n",
      "Epoch: [1][600/2850] Elapsed 5m 3s (remain 18m 55s) Loss: 0.0404(0.1435) Grad: 7714.2036  LR: 0.000008  \n",
      "Epoch: [1][700/2850] Elapsed 5m 54s (remain 18m 7s) Loss: 0.0314(0.1256) Grad: 4029.9856  LR: 0.000010  \n",
      "Epoch: [1][800/2850] Elapsed 6m 45s (remain 17m 18s) Loss: 0.0151(0.1120) Grad: 2186.8123  LR: 0.000011  \n",
      "Epoch: [1][900/2850] Elapsed 7m 36s (remain 16m 26s) Loss: 0.0007(0.1011) Grad: 213.4743  LR: 0.000013  \n",
      "Epoch: [1][1000/2850] Elapsed 8m 26s (remain 15m 35s) Loss: 0.0040(0.0922) Grad: 423.4719  LR: 0.000014  \n",
      "Epoch: [1][1100/2850] Elapsed 9m 17s (remain 14m 45s) Loss: 0.0035(0.0849) Grad: 445.7491  LR: 0.000015  \n",
      "Epoch: [1][1200/2850] Elapsed 10m 8s (remain 13m 56s) Loss: 0.0039(0.0786) Grad: 746.6478  LR: 0.000017  \n",
      "Epoch: [1][1300/2850] Elapsed 10m 59s (remain 13m 5s) Loss: 0.0043(0.0732) Grad: 614.5972  LR: 0.000018  \n",
      "Epoch: [1][1400/2850] Elapsed 11m 50s (remain 12m 14s) Loss: 0.0052(0.0688) Grad: 711.1902  LR: 0.000020  \n",
      "Epoch: [1][1500/2850] Elapsed 12m 40s (remain 11m 23s) Loss: 0.0402(0.0647) Grad: 4944.9575  LR: 0.000020  \n",
      "Epoch: [1][1600/2850] Elapsed 13m 31s (remain 10m 33s) Loss: 0.0190(0.0614) Grad: 1398.6908  LR: 0.000020  \n",
      "Epoch: [1][1700/2850] Elapsed 14m 21s (remain 9m 42s) Loss: 0.0042(0.0584) Grad: 463.7542  LR: 0.000020  \n",
      "Epoch: [1][1800/2850] Elapsed 15m 11s (remain 8m 50s) Loss: 0.0001(0.0557) Grad: 49.5836  LR: 0.000019  \n",
      "Epoch: [1][1900/2850] Elapsed 16m 1s (remain 7m 59s) Loss: 0.0026(0.0532) Grad: 289.1130  LR: 0.000019  \n",
      "Epoch: [1][2000/2850] Elapsed 16m 50s (remain 7m 8s) Loss: 0.0087(0.0510) Grad: 777.7863  LR: 0.000019  \n",
      "Epoch: [1][2100/2850] Elapsed 17m 41s (remain 6m 18s) Loss: 0.0004(0.0489) Grad: 55.2993  LR: 0.000019  \n",
      "Epoch: [1][2200/2850] Elapsed 18m 31s (remain 5m 27s) Loss: 0.0040(0.0471) Grad: 305.7004  LR: 0.000019  \n",
      "Epoch: [1][2300/2850] Elapsed 19m 21s (remain 4m 37s) Loss: 0.0035(0.0454) Grad: 460.0138  LR: 0.000019  \n",
      "Epoch: [1][2400/2850] Elapsed 20m 11s (remain 3m 46s) Loss: 0.0096(0.0438) Grad: 2734.4172  LR: 0.000018  \n",
      "Epoch: [1][2500/2850] Elapsed 21m 0s (remain 2m 55s) Loss: 0.0057(0.0423) Grad: 986.0294  LR: 0.000018  \n",
      "Epoch: [1][2600/2850] Elapsed 21m 50s (remain 2m 5s) Loss: 0.0145(0.0411) Grad: 992.2158  LR: 0.000018  \n",
      "Epoch: [1][2700/2850] Elapsed 22m 40s (remain 1m 15s) Loss: 0.0095(0.0398) Grad: 573.0438  LR: 0.000018  \n",
      "Epoch: [1][2800/2850] Elapsed 23m 30s (remain 0m 24s) Loss: 0.0030(0.0386) Grad: 221.7810  LR: 0.000018  \n",
      "Epoch: [1][2849/2850] Elapsed 23m 54s (remain 0m 0s) Loss: 0.0018(0.0381) Grad: 671.1459  LR: 0.000018  \n",
      "EVAL: [0/725] Elapsed 0m 0s (remain 6m 49s) Loss: 0.0080(0.0080) \n",
      "EVAL: [100/725] Elapsed 0m 28s (remain 2m 53s) Loss: 0.0007(0.0074) \n",
      "EVAL: [200/725] Elapsed 0m 55s (remain 2m 25s) Loss: 0.0124(0.0088) \n",
      "EVAL: [300/725] Elapsed 1m 23s (remain 1m 57s) Loss: 0.0019(0.0077) \n",
      "EVAL: [400/725] Elapsed 1m 51s (remain 1m 29s) Loss: 0.1149(0.0090) \n",
      "EVAL: [500/725] Elapsed 2m 18s (remain 1m 2s) Loss: 0.0168(0.0090) \n",
      "EVAL: [600/725] Elapsed 2m 47s (remain 0m 34s) Loss: 0.0023(0.0089) \n",
      "EVAL: [700/725] Elapsed 3m 14s (remain 0m 6s) Loss: 0.0006(0.0081) \n",
      "EVAL: [724/725] Elapsed 3m 21s (remain 0m 0s) Loss: 0.0109(0.0080) \n",
      "Epoch 1 - avg_train_loss: 0.0381  avg_val_loss: 0.0080  time: 1642s\n",
      "Epoch 1 - Score: 0.8309\n",
      "Epoch 1 - Save Best Score: 0.8309 Model\n",
      "Epoch: [2][0/2850] Elapsed 0m 0s (remain 34m 39s) Loss: 0.0059(0.0059) Grad: 9034.2490  LR: 0.000018  \n",
      "Epoch: [2][100/2850] Elapsed 0m 50s (remain 23m 5s) Loss: 0.0003(0.0057) Grad: 1818.1257  LR: 0.000018  \n",
      "Epoch: [2][200/2850] Elapsed 1m 41s (remain 22m 13s) Loss: 0.0011(0.0061) Grad: 3810.9412  LR: 0.000017  \n",
      "Epoch: [2][300/2850] Elapsed 2m 31s (remain 21m 23s) Loss: 0.0016(0.0062) Grad: 4967.6812  LR: 0.000017  \n",
      "Epoch: [2][400/2850] Elapsed 3m 21s (remain 20m 32s) Loss: 0.0049(0.0063) Grad: 22412.5547  LR: 0.000017  \n",
      "Epoch: [2][500/2850] Elapsed 4m 12s (remain 19m 45s) Loss: 0.0013(0.0060) Grad: 6453.1597  LR: 0.000017  \n",
      "Epoch: [2][600/2850] Elapsed 5m 3s (remain 18m 56s) Loss: 0.0031(0.0057) Grad: 12118.7393  LR: 0.000017  \n",
      "Epoch: [2][700/2850] Elapsed 5m 53s (remain 18m 4s) Loss: 0.0072(0.0060) Grad: 17244.4473  LR: 0.000017  \n",
      "Epoch: [2][800/2850] Elapsed 6m 44s (remain 17m 13s) Loss: 0.0113(0.0059) Grad: 54225.5234  LR: 0.000017  \n",
      "Epoch: [2][900/2850] Elapsed 7m 34s (remain 16m 22s) Loss: 0.0001(0.0061) Grad: 224.1047  LR: 0.000016  \n",
      "Epoch: [2][1000/2850] Elapsed 8m 25s (remain 15m 34s) Loss: 0.0015(0.0060) Grad: 2497.2166  LR: 0.000016  \n",
      "Epoch: [2][1100/2850] Elapsed 9m 16s (remain 14m 44s) Loss: 0.0002(0.0059) Grad: 520.7672  LR: 0.000016  \n",
      "Epoch: [2][1200/2850] Elapsed 10m 7s (remain 13m 53s) Loss: 0.0023(0.0061) Grad: 4986.8379  LR: 0.000016  \n",
      "Epoch: [2][1300/2850] Elapsed 10m 58s (remain 13m 3s) Loss: 0.0066(0.0060) Grad: 4187.3857  LR: 0.000016  \n",
      "Epoch: [2][1400/2850] Elapsed 11m 49s (remain 12m 13s) Loss: 0.0011(0.0060) Grad: 3582.4417  LR: 0.000016  \n",
      "Epoch: [2][1500/2850] Elapsed 12m 40s (remain 11m 23s) Loss: 0.0001(0.0059) Grad: 303.5482  LR: 0.000015  \n",
      "Epoch: [2][1600/2850] Elapsed 13m 30s (remain 10m 32s) Loss: 0.0157(0.0059) Grad: 21130.9258  LR: 0.000015  \n",
      "Epoch: [2][1700/2850] Elapsed 14m 21s (remain 9m 41s) Loss: 0.0803(0.0058) Grad: 143131.5938  LR: 0.000015  \n",
      "Epoch: [2][1800/2850] Elapsed 15m 12s (remain 8m 51s) Loss: 0.0229(0.0059) Grad: 79574.5000  LR: 0.000015  \n",
      "Epoch: [2][1900/2850] Elapsed 16m 2s (remain 8m 0s) Loss: 0.0001(0.0058) Grad: 247.6488  LR: 0.000015  \n",
      "Epoch: [2][2000/2850] Elapsed 16m 52s (remain 7m 9s) Loss: 0.0459(0.0059) Grad: 105378.6328  LR: 0.000015  \n",
      "Epoch: [2][2100/2850] Elapsed 17m 42s (remain 6m 18s) Loss: 0.0012(0.0060) Grad: 1173.8699  LR: 0.000015  \n",
      "Epoch: [2][2200/2850] Elapsed 18m 34s (remain 5m 28s) Loss: 0.0015(0.0060) Grad: 3207.1975  LR: 0.000014  \n",
      "Epoch: [2][2300/2850] Elapsed 19m 24s (remain 4m 37s) Loss: 0.0008(0.0059) Grad: 1257.5015  LR: 0.000014  \n",
      "Epoch: [2][2400/2850] Elapsed 20m 14s (remain 3m 47s) Loss: 0.0001(0.0059) Grad: 256.8921  LR: 0.000014  \n",
      "Epoch: [2][2500/2850] Elapsed 21m 5s (remain 2m 56s) Loss: 0.0014(0.0059) Grad: 1531.1594  LR: 0.000014  \n",
      "Epoch: [2][2600/2850] Elapsed 21m 56s (remain 2m 5s) Loss: 0.0188(0.0059) Grad: 6752.0854  LR: 0.000014  \n",
      "Epoch: [2][2700/2850] Elapsed 22m 46s (remain 1m 15s) Loss: 0.0063(0.0059) Grad: 4901.4395  LR: 0.000014  \n",
      "Epoch: [2][2800/2850] Elapsed 23m 36s (remain 0m 24s) Loss: 0.0301(0.0060) Grad: 12417.9434  LR: 0.000013  \n",
      "Epoch: [2][2849/2850] Elapsed 24m 1s (remain 0m 0s) Loss: 0.0100(0.0059) Grad: 24412.5781  LR: 0.000013  \n",
      "EVAL: [0/725] Elapsed 0m 0s (remain 6m 52s) Loss: 0.0056(0.0056) \n",
      "EVAL: [100/725] Elapsed 0m 28s (remain 2m 56s) Loss: 0.0016(0.0070) \n",
      "EVAL: [200/725] Elapsed 0m 57s (remain 2m 28s) Loss: 0.0100(0.0086) \n",
      "EVAL: [300/725] Elapsed 1m 24s (remain 1m 59s) Loss: 0.0006(0.0073) \n",
      "EVAL: [400/725] Elapsed 1m 52s (remain 1m 30s) Loss: 0.0860(0.0081) \n",
      "EVAL: [500/725] Elapsed 2m 19s (remain 1m 2s) Loss: 0.0071(0.0082) \n",
      "EVAL: [600/725] Elapsed 2m 48s (remain 0m 34s) Loss: 0.0035(0.0079) \n",
      "EVAL: [700/725] Elapsed 3m 15s (remain 0m 6s) Loss: 0.0005(0.0072) \n",
      "EVAL: [724/725] Elapsed 3m 22s (remain 0m 0s) Loss: 0.0140(0.0072) \n",
      "Epoch 2 - avg_train_loss: 0.0059  avg_val_loss: 0.0072  time: 1649s\n",
      "Epoch 2 - Score: 0.8787\n",
      "Epoch 2 - Save Best Score: 0.8787 Model\n",
      "Epoch: [3][0/2850] Elapsed 0m 0s (remain 39m 39s) Loss: 0.0004(0.0004) Grad: 1869.4890  LR: 0.000013  \n",
      "Epoch: [3][100/2850] Elapsed 0m 51s (remain 23m 31s) Loss: 0.0111(0.0034) Grad: 27653.4199  LR: 0.000013  \n",
      "Epoch: [3][200/2850] Elapsed 1m 42s (remain 22m 29s) Loss: 0.0001(0.0042) Grad: 1467.1466  LR: 0.000013  \n",
      "Epoch: [3][300/2850] Elapsed 2m 32s (remain 21m 33s) Loss: 0.0000(0.0044) Grad: 64.3674  LR: 0.000013  \n",
      "Epoch: [3][400/2850] Elapsed 3m 23s (remain 20m 41s) Loss: 0.0023(0.0044) Grad: 12159.6035  LR: 0.000013  \n",
      "Epoch: [3][500/2850] Elapsed 4m 14s (remain 19m 51s) Loss: 0.0049(0.0049) Grad: 9838.8154  LR: 0.000013  \n",
      "Epoch: [3][600/2850] Elapsed 5m 4s (remain 18m 59s) Loss: 0.0000(0.0049) Grad: 210.8261  LR: 0.000012  \n",
      "Epoch: [3][700/2850] Elapsed 5m 55s (remain 18m 8s) Loss: 0.0001(0.0048) Grad: 507.1217  LR: 0.000012  \n",
      "Epoch: [3][800/2850] Elapsed 6m 45s (remain 17m 16s) Loss: 0.0002(0.0049) Grad: 688.4974  LR: 0.000012  \n",
      "Epoch: [3][900/2850] Elapsed 7m 35s (remain 16m 24s) Loss: 0.0000(0.0047) Grad: 153.9189  LR: 0.000012  \n",
      "Epoch: [3][1000/2850] Elapsed 8m 25s (remain 15m 33s) Loss: 0.0001(0.0049) Grad: 304.1748  LR: 0.000012  \n",
      "Epoch: [3][1100/2850] Elapsed 9m 16s (remain 14m 43s) Loss: 0.0046(0.0048) Grad: 16499.4219  LR: 0.000012  \n",
      "Epoch: [3][1200/2850] Elapsed 10m 7s (remain 13m 53s) Loss: 0.0007(0.0048) Grad: 2027.3512  LR: 0.000011  \n",
      "Epoch: [3][1300/2850] Elapsed 10m 57s (remain 13m 3s) Loss: 0.0021(0.0048) Grad: 2167.1323  LR: 0.000011  \n",
      "Epoch: [3][1400/2850] Elapsed 11m 47s (remain 12m 12s) Loss: 0.0014(0.0049) Grad: 7632.9697  LR: 0.000011  \n",
      "Epoch: [3][1500/2850] Elapsed 12m 37s (remain 11m 21s) Loss: 0.0024(0.0049) Grad: 9110.4971  LR: 0.000011  \n",
      "Epoch: [3][1600/2850] Elapsed 13m 28s (remain 10m 30s) Loss: 0.0201(0.0050) Grad: 17783.0273  LR: 0.000011  \n",
      "Epoch: [3][1700/2850] Elapsed 14m 20s (remain 9m 41s) Loss: 0.0000(0.0050) Grad: 63.3644  LR: 0.000011  \n",
      "Epoch: [3][1800/2850] Elapsed 15m 11s (remain 8m 50s) Loss: 0.0000(0.0050) Grad: 34.8903  LR: 0.000011  \n",
      "Epoch: [3][1900/2850] Elapsed 16m 1s (remain 7m 59s) Loss: 0.0340(0.0051) Grad: 39170.3906  LR: 0.000010  \n",
      "Epoch: [3][2000/2850] Elapsed 16m 51s (remain 7m 9s) Loss: 0.0013(0.0051) Grad: 6373.8276  LR: 0.000010  \n",
      "Epoch: [3][2100/2850] Elapsed 17m 42s (remain 6m 18s) Loss: 0.0000(0.0051) Grad: 235.0192  LR: 0.000010  \n",
      "Epoch: [3][2200/2850] Elapsed 18m 33s (remain 5m 28s) Loss: 0.0000(0.0050) Grad: 149.8196  LR: 0.000010  \n",
      "Epoch: [3][2300/2850] Elapsed 19m 24s (remain 4m 37s) Loss: 0.0004(0.0050) Grad: 1857.5316  LR: 0.000010  \n",
      "Epoch: [3][2400/2850] Elapsed 20m 15s (remain 3m 47s) Loss: 0.0014(0.0050) Grad: 4780.6846  LR: 0.000010  \n",
      "Epoch: [3][2500/2850] Elapsed 21m 5s (remain 2m 56s) Loss: 0.0001(0.0049) Grad: 340.4219  LR: 0.000009  \n",
      "Epoch: [3][2600/2850] Elapsed 21m 56s (remain 2m 6s) Loss: 0.0010(0.0049) Grad: 2878.0281  LR: 0.000009  \n",
      "Epoch: [3][2700/2850] Elapsed 22m 46s (remain 1m 15s) Loss: 0.0000(0.0049) Grad: 28.6621  LR: 0.000009  \n",
      "Epoch: [3][2800/2850] Elapsed 23m 36s (remain 0m 24s) Loss: 0.0024(0.0048) Grad: 6564.8130  LR: 0.000009  \n",
      "Epoch: [3][2849/2850] Elapsed 24m 1s (remain 0m 0s) Loss: 0.0019(0.0048) Grad: 4979.6113  LR: 0.000009  \n",
      "EVAL: [0/725] Elapsed 0m 0s (remain 6m 49s) Loss: 0.0156(0.0156) \n",
      "EVAL: [100/725] Elapsed 0m 28s (remain 2m 54s) Loss: 0.0011(0.0081) \n",
      "EVAL: [200/725] Elapsed 0m 55s (remain 2m 25s) Loss: 0.0053(0.0103) \n",
      "EVAL: [300/725] Elapsed 1m 23s (remain 1m 58s) Loss: 0.0168(0.0089) \n",
      "EVAL: [400/725] Elapsed 1m 51s (remain 1m 30s) Loss: 0.1402(0.0100) \n",
      "EVAL: [500/725] Elapsed 2m 20s (remain 1m 2s) Loss: 0.0166(0.0101) \n",
      "EVAL: [600/725] Elapsed 2m 49s (remain 0m 34s) Loss: 0.0134(0.0098) \n",
      "EVAL: [700/725] Elapsed 3m 16s (remain 0m 6s) Loss: 0.0000(0.0090) \n",
      "EVAL: [724/725] Elapsed 3m 23s (remain 0m 0s) Loss: 0.0144(0.0089) \n",
      "Epoch 3 - avg_train_loss: 0.0048  avg_val_loss: 0.0089  time: 1650s\n",
      "Epoch 3 - Score: 0.8831\n",
      "Epoch 3 - Save Best Score: 0.8831 Model\n",
      "Epoch: [4][0/2850] Elapsed 0m 0s (remain 38m 26s) Loss: 0.0006(0.0006) Grad: 6059.7480  LR: 0.000009  \n",
      "Epoch: [4][100/2850] Elapsed 0m 50s (remain 23m 7s) Loss: 0.0010(0.0047) Grad: 2338.2275  LR: 0.000009  \n",
      "Epoch: [4][200/2850] Elapsed 1m 40s (remain 22m 9s) Loss: 0.0103(0.0039) Grad: 5476.4751  LR: 0.000009  \n",
      "Epoch: [4][300/2850] Elapsed 2m 31s (remain 21m 20s) Loss: 0.0003(0.0041) Grad: 1034.6365  LR: 0.000008  \n",
      "Epoch: [4][400/2850] Elapsed 3m 21s (remain 20m 32s) Loss: 0.0043(0.0039) Grad: 3442.7576  LR: 0.000008  \n",
      "Epoch: [4][500/2850] Elapsed 4m 12s (remain 19m 45s) Loss: 0.0002(0.0041) Grad: 665.4832  LR: 0.000008  \n",
      "Epoch: [4][600/2850] Elapsed 5m 5s (remain 19m 4s) Loss: 0.0000(0.0038) Grad: 18.1574  LR: 0.000008  \n",
      "Epoch: [4][700/2850] Elapsed 5m 57s (remain 18m 15s) Loss: 0.0000(0.0040) Grad: 57.7918  LR: 0.000008  \n",
      "Epoch: [4][800/2850] Elapsed 6m 47s (remain 17m 22s) Loss: 0.0002(0.0039) Grad: 655.9120  LR: 0.000008  \n",
      "Epoch: [4][900/2850] Elapsed 7m 38s (remain 16m 31s) Loss: 0.0188(0.0039) Grad: 16272.4180  LR: 0.000007  \n",
      "Epoch: [4][1000/2850] Elapsed 8m 28s (remain 15m 40s) Loss: 0.0007(0.0038) Grad: 2891.6875  LR: 0.000007  \n",
      "Epoch: [4][1100/2850] Elapsed 9m 19s (remain 14m 48s) Loss: 0.0009(0.0038) Grad: 5141.8452  LR: 0.000007  \n",
      "Epoch: [4][1200/2850] Elapsed 10m 9s (remain 13m 56s) Loss: 0.0060(0.0038) Grad: 10544.7666  LR: 0.000007  \n",
      "Epoch: [4][1300/2850] Elapsed 11m 0s (remain 13m 6s) Loss: 0.0025(0.0038) Grad: 2800.5515  LR: 0.000007  \n",
      "Epoch: [4][1400/2850] Elapsed 11m 50s (remain 12m 14s) Loss: 0.0002(0.0039) Grad: 866.1207  LR: 0.000007  \n",
      "Epoch: [4][1500/2850] Elapsed 12m 40s (remain 11m 23s) Loss: 0.0188(0.0039) Grad: 38984.4414  LR: 0.000007  \n",
      "Epoch: [4][1600/2850] Elapsed 13m 32s (remain 10m 33s) Loss: 0.0010(0.0039) Grad: 3051.2607  LR: 0.000006  \n",
      "Epoch: [4][1700/2850] Elapsed 14m 22s (remain 9m 42s) Loss: 0.0000(0.0038) Grad: 100.1427  LR: 0.000006  \n",
      "Epoch: [4][1800/2850] Elapsed 15m 12s (remain 8m 51s) Loss: 0.0000(0.0038) Grad: 43.5047  LR: 0.000006  \n",
      "Epoch: [4][1900/2850] Elapsed 16m 3s (remain 8m 0s) Loss: 0.0132(0.0038) Grad: 12808.9209  LR: 0.000006  \n",
      "Epoch: [4][2000/2850] Elapsed 16m 53s (remain 7m 9s) Loss: 0.0006(0.0038) Grad: 1345.8685  LR: 0.000006  \n",
      "Epoch: [4][2100/2850] Elapsed 17m 43s (remain 6m 19s) Loss: 0.0008(0.0038) Grad: 1494.9817  LR: 0.000006  \n",
      "Epoch: [4][2200/2850] Elapsed 18m 33s (remain 5m 28s) Loss: 0.0136(0.0038) Grad: 31107.8066  LR: 0.000005  \n",
      "Epoch: [4][2300/2850] Elapsed 19m 23s (remain 4m 37s) Loss: 0.0105(0.0038) Grad: 10428.6807  LR: 0.000005  \n",
      "Epoch: [4][2400/2850] Elapsed 20m 13s (remain 3m 46s) Loss: 0.0000(0.0038) Grad: 9.9520  LR: 0.000005  \n",
      "Epoch: [4][2500/2850] Elapsed 21m 3s (remain 2m 56s) Loss: 0.0001(0.0038) Grad: 299.3885  LR: 0.000005  \n",
      "Epoch: [4][2600/2850] Elapsed 21m 54s (remain 2m 5s) Loss: 0.0002(0.0038) Grad: 475.8388  LR: 0.000005  \n",
      "Epoch: [4][2700/2850] Elapsed 22m 45s (remain 1m 15s) Loss: 0.0002(0.0038) Grad: 1433.3439  LR: 0.000005  \n",
      "Epoch: [4][2800/2850] Elapsed 23m 36s (remain 0m 24s) Loss: 0.0001(0.0038) Grad: 373.4178  LR: 0.000005  \n",
      "Epoch: [4][2849/2850] Elapsed 24m 0s (remain 0m 0s) Loss: 0.0016(0.0038) Grad: 7127.2524  LR: 0.000004  \n",
      "EVAL: [0/725] Elapsed 0m 0s (remain 6m 44s) Loss: 0.0084(0.0084) \n",
      "EVAL: [100/725] Elapsed 0m 28s (remain 2m 53s) Loss: 0.0005(0.0075) \n",
      "EVAL: [200/725] Elapsed 0m 56s (remain 2m 26s) Loss: 0.0046(0.0097) \n",
      "EVAL: [300/725] Elapsed 1m 24s (remain 1m 58s) Loss: 0.0201(0.0084) \n",
      "EVAL: [400/725] Elapsed 1m 53s (remain 1m 31s) Loss: 0.0661(0.0093) \n",
      "EVAL: [500/725] Elapsed 2m 21s (remain 1m 3s) Loss: 0.0144(0.0096) \n",
      "EVAL: [600/725] Elapsed 2m 49s (remain 0m 34s) Loss: 0.0124(0.0093) \n",
      "EVAL: [700/725] Elapsed 3m 17s (remain 0m 6s) Loss: 0.0002(0.0085) \n",
      "EVAL: [724/725] Elapsed 3m 23s (remain 0m 0s) Loss: 0.0141(0.0085) \n",
      "Epoch 4 - avg_train_loss: 0.0038  avg_val_loss: 0.0085  time: 1650s\n",
      "Epoch 4 - Score: 0.8816\n",
      "Epoch: [5][0/2850] Elapsed 0m 0s (remain 37m 37s) Loss: 0.0053(0.0053) Grad: 34856.2305  LR: 0.000004  \n",
      "Epoch: [5][100/2850] Elapsed 0m 51s (remain 23m 8s) Loss: 0.0013(0.0025) Grad: 59431.3047  LR: 0.000004  \n",
      "Epoch: [5][200/2850] Elapsed 1m 40s (remain 22m 9s) Loss: 0.0000(0.0040) Grad: 23.8994  LR: 0.000004  \n",
      "Epoch: [5][300/2850] Elapsed 2m 31s (remain 21m 26s) Loss: 0.0000(0.0032) Grad: 31.3428  LR: 0.000004  \n",
      "Epoch: [5][400/2850] Elapsed 3m 22s (remain 20m 34s) Loss: 0.0034(0.0036) Grad: 34040.6797  LR: 0.000004  \n",
      "Epoch: [5][500/2850] Elapsed 4m 12s (remain 19m 44s) Loss: 0.0000(0.0035) Grad: 155.9857  LR: 0.000004  \n",
      "Epoch: [5][600/2850] Elapsed 5m 3s (remain 18m 56s) Loss: 0.0000(0.0035) Grad: 332.5297  LR: 0.000004  \n",
      "Epoch: [5][700/2850] Elapsed 5m 54s (remain 18m 7s) Loss: 0.0000(0.0034) Grad: 81.8794  LR: 0.000003  \n",
      "Epoch: [5][800/2850] Elapsed 6m 45s (remain 17m 16s) Loss: 0.0002(0.0034) Grad: 1138.2903  LR: 0.000003  \n",
      "Epoch: [5][900/2850] Elapsed 7m 35s (remain 16m 25s) Loss: 0.0015(0.0035) Grad: 3940.5059  LR: 0.000003  \n",
      "Epoch: [5][1000/2850] Elapsed 8m 26s (remain 15m 34s) Loss: 0.0008(0.0034) Grad: 14787.9463  LR: 0.000003  \n",
      "Epoch: [5][1100/2850] Elapsed 9m 18s (remain 14m 47s) Loss: 0.0039(0.0034) Grad: 23742.0547  LR: 0.000003  \n",
      "Epoch: [5][1200/2850] Elapsed 10m 8s (remain 13m 55s) Loss: 0.0025(0.0034) Grad: 3625.0488  LR: 0.000003  \n",
      "Epoch: [5][1300/2850] Elapsed 10m 58s (remain 13m 4s) Loss: 0.0003(0.0034) Grad: 2250.7585  LR: 0.000002  \n",
      "Epoch: [5][1400/2850] Elapsed 11m 48s (remain 12m 12s) Loss: 0.0023(0.0033) Grad: 5886.4883  LR: 0.000002  \n",
      "Epoch: [5][1500/2850] Elapsed 12m 38s (remain 11m 22s) Loss: 0.0027(0.0032) Grad: 11695.7510  LR: 0.000002  \n",
      "Epoch: [5][1600/2850] Elapsed 13m 29s (remain 10m 31s) Loss: 0.0002(0.0033) Grad: 1347.4526  LR: 0.000002  \n",
      "Epoch: [5][1700/2850] Elapsed 14m 20s (remain 9m 41s) Loss: 0.0000(0.0032) Grad: 6.5174  LR: 0.000002  \n",
      "Epoch: [5][1800/2850] Elapsed 15m 11s (remain 8m 50s) Loss: 0.0001(0.0033) Grad: 942.2531  LR: 0.000002  \n",
      "Epoch: [5][1900/2850] Elapsed 16m 1s (remain 7m 59s) Loss: 0.0002(0.0032) Grad: 2280.7012  LR: 0.000001  \n",
      "Epoch: [5][2000/2850] Elapsed 16m 51s (remain 7m 9s) Loss: 0.0002(0.0032) Grad: 2130.5713  LR: 0.000001  \n",
      "Epoch: [5][2100/2850] Elapsed 17m 41s (remain 6m 18s) Loss: 0.0008(0.0032) Grad: 2253.9983  LR: 0.000001  \n",
      "Epoch: [5][2200/2850] Elapsed 18m 32s (remain 5m 28s) Loss: 0.0000(0.0031) Grad: 11.6168  LR: 0.000001  \n",
      "Epoch: [5][2300/2850] Elapsed 19m 22s (remain 4m 37s) Loss: 0.0571(0.0032) Grad: 70703.7266  LR: 0.000001  \n",
      "Epoch: [5][2400/2850] Elapsed 20m 13s (remain 3m 46s) Loss: 0.0026(0.0031) Grad: 10517.9336  LR: 0.000001  \n",
      "Epoch: [5][2500/2850] Elapsed 21m 3s (remain 2m 56s) Loss: 0.0001(0.0031) Grad: 564.6112  LR: 0.000001  \n",
      "Epoch: [5][2600/2850] Elapsed 21m 53s (remain 2m 5s) Loss: 0.0071(0.0031) Grad: 27441.5957  LR: 0.000000  \n",
      "Epoch: [5][2700/2850] Elapsed 22m 45s (remain 1m 15s) Loss: 0.0000(0.0031) Grad: 178.2028  LR: 0.000000  \n",
      "Epoch: [5][2800/2850] Elapsed 23m 36s (remain 0m 24s) Loss: 0.0000(0.0031) Grad: 321.9850  LR: 0.000000  \n",
      "Epoch: [5][2849/2850] Elapsed 24m 0s (remain 0m 0s) Loss: 0.0060(0.0030) Grad: 28835.6445  LR: 0.000000  \n",
      "EVAL: [0/725] Elapsed 0m 0s (remain 7m 7s) Loss: 0.0214(0.0214) \n",
      "EVAL: [100/725] Elapsed 0m 28s (remain 2m 54s) Loss: 0.0010(0.0090) \n",
      "EVAL: [200/725] Elapsed 0m 56s (remain 2m 26s) Loss: 0.0035(0.0113) \n",
      "EVAL: [300/725] Elapsed 1m 23s (remain 1m 58s) Loss: 0.0266(0.0098) \n",
      "EVAL: [400/725] Elapsed 1m 51s (remain 1m 30s) Loss: 0.0649(0.0108) \n",
      "EVAL: [500/725] Elapsed 2m 20s (remain 1m 3s) Loss: 0.0153(0.0112) \n",
      "EVAL: [600/725] Elapsed 2m 49s (remain 0m 34s) Loss: 0.0158(0.0108) \n",
      "EVAL: [700/725] Elapsed 3m 16s (remain 0m 6s) Loss: 0.0001(0.0099) \n",
      "EVAL: [724/725] Elapsed 3m 23s (remain 0m 0s) Loss: 0.0155(0.0099) \n",
      "Epoch 5 - avg_train_loss: 0.0030  avg_val_loss: 0.0099  time: 1649s\n",
      "Epoch 5 - Score: 0.8822\n",
      "Best thres: 0.5, Score: 0.8853\n",
      "Best thres: 0.47812499999999997, Score: 0.8856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ffb670579e4942b8b82021729e615f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6321657400641ab94ef87d4311e4485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36848173d6ce43ca890e53383da8edf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67ba1b1a0094142be6c9ca32c8df5da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8665d1fd4a664c8f88c960630debd76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "name": "nbme-exp011.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 641.572862,
   "end_time": "2022-02-27T11:39:50.972497",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-27T11:29:09.399635",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0260998578564385a0b5b9425a0a5ca1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1141ae38bc6f473aab89db14fa4eeacf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad49cbf6b6e84ccaab873458182f22a1",
      "placeholder": "​",
      "style": "IPY_MODEL_5375de82ce3a41a8b5550e0a6b4316c1",
      "value": " 42146/42146 [00:36&lt;00:00, 2019.05it/s]"
     }
    },
    "220f78b6119042af8729543465e1234e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "25bf78e432e641e0a435dc3626c3ee8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2e32fee744ef42e0aaa89a7b03e82427": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e3818222bab4603a896be5976cb8409": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9754a5f1e61d49c8972df40ee9290375",
      "placeholder": "​",
      "style": "IPY_MODEL_25bf78e432e641e0a435dc3626c3ee8a",
      "value": " 143/143 [00:00&lt;00:00, 2166.88it/s]"
     }
    },
    "40e6583408c447199ff5b94d23601936": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e32fee744ef42e0aaa89a7b03e82427",
      "max": 42146,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d51d3aa414db4aa8b0ccae896e671152",
      "value": 42146
     }
    },
    "428ca357bd284d199e2558b1f577d79a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "47b8a7f3d0544d79b30ad02e4222082e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5375de82ce3a41a8b5550e0a6b4316c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e66444e9c714134bd2765cb3b6d1f15": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b04b019813e458080f02bc9111433a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f1d7796e2174485a0d1b1e9a71d7ade",
      "max": 143,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e72cad76f875451a8e2479e2df237575",
      "value": 143
     }
    },
    "7f1d7796e2174485a0d1b1e9a71d7ade": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a503d1abd884514a1e23101e03c6781": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e06e5e9eb0414b6fad63bdc99b44a313",
       "IPY_MODEL_6b04b019813e458080f02bc9111433a6",
       "IPY_MODEL_2e3818222bab4603a896be5976cb8409"
      ],
      "layout": "IPY_MODEL_eeb468dbb94943fcb30219d4dd98fcab"
     }
    },
    "9754a5f1e61d49c8972df40ee9290375": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a31c60ff4dab48e08d2ef9293d85df6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c40d970496ff447a8c0b80d787b07a4d",
       "IPY_MODEL_40e6583408c447199ff5b94d23601936",
       "IPY_MODEL_1141ae38bc6f473aab89db14fa4eeacf"
      ],
      "layout": "IPY_MODEL_47b8a7f3d0544d79b30ad02e4222082e"
     }
    },
    "ad49cbf6b6e84ccaab873458182f22a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c40d970496ff447a8c0b80d787b07a4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0260998578564385a0b5b9425a0a5ca1",
      "placeholder": "​",
      "style": "IPY_MODEL_428ca357bd284d199e2558b1f577d79a",
      "value": "100%"
     }
    },
    "d51d3aa414db4aa8b0ccae896e671152": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e06e5e9eb0414b6fad63bdc99b44a313": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e66444e9c714134bd2765cb3b6d1f15",
      "placeholder": "​",
      "style": "IPY_MODEL_220f78b6119042af8729543465e1234e",
      "value": "100%"
     }
    },
    "e72cad76f875451a8e2479e2df237575": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eeb468dbb94943fcb30219d4dd98fcab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
