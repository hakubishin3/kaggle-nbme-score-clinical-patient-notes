{"cells":[{"cell_type":"markdown","metadata":{"id":"blind-kingdom"},"source":["## References"]},{"cell_type":"markdown","metadata":{"id":"antique-glenn"},"source":["- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"]},{"cell_type":"markdown","metadata":{"id":"bored-ministry"},"source":["## Configurations"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1646561134774,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"deadly-confidence"},"outputs":[],"source":["EXP_NAME = \"nbme-exp025\"\n","ENV = \"colab\"\n","DEBUG_MODE = False\n","SUBMISSION_MODE = False"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1646561134775,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"aware-worcester"},"outputs":[],"source":["class CFG:\n","    env=ENV\n","    exp_name=EXP_NAME\n","    debug=DEBUG_MODE\n","    submission=SUBMISSION_MODE\n","    apex=True\n","    input_dir=None\n","    output_dir=None\n","    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n","    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n","    competition_name=\"nbme-score-clinical-patient-notes\"\n","    id_col=\"id\"\n","    target_col=\"location\"\n","    pretrained_model_name=\"microsoft/deberta-v3-large\"\n","    tokenizer=None\n","    max_len=None\n","    output_dim=1\n","    dropout=0.2\n","    num_workers=4\n","    batch_size=4\n","    lr=2e-5\n","    betas=(0.9, 0.98)\n","    weight_decay=0.1\n","    num_warmup_steps_rate=0.1\n","    batch_scheduler=True\n","    epochs=5\n","    n_fold=5\n","    train_fold=[0, 1, 2, 3, 4]\n","    seed=71\n","    gradient_accumulation_steps=2\n","    max_grad_norm=1000\n","    print_freq=100\n","    train=True\n","    inference=True"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1646561134776,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"personalized-death"},"outputs":[],"source":["if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.train_fold = [0, 1]\n","\n","if CFG.submission:\n","    CFG.train = False\n","    CFG.inference = True"]},{"cell_type":"markdown","metadata":{"id":"cardiovascular-neutral"},"source":["## Directory Settings"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37640,"status":"ok","timestamp":1646561172404,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"checked-boards","outputId":"3e129fb4-42c5-42c7-ec9f-bad3ce1b3c9d"},"outputs":[{"name":"stdout","output_type":"stream","text":["colab\n","Mounted at /content/drive\n","Collecting transformers\n","  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 13.4 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting huggingface-hub\u003c1.0,\u003e=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 6.9 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Collecting pyyaml\u003e=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 57.6 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 42.6 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,\u003e=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 51.3 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.1.0-\u003etransformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=20.0-\u003etransformers) (3.0.7)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003etransformers) (3.7.0)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (1.24.3)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (3.0.4)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2021.10.8)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers) (1.15.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.17.0\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 14.1 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n"]}],"source":["import sys\n","from pathlib import Path\n","\n","\n","print(CFG.env)\n","if CFG.env == \"colab\":\n","    # colab環境\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","    # install packages\n","    !pip install transformers\n","    !pip install sentencepiece\n","\n","elif CFG.env == \"local\":\n","    # ローカルサーバ\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","\n","elif CFG.env == \"kaggle\":\n","    # kaggle環境\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./\")"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":9457,"status":"ok","timestamp":1646561181850,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"iGai035Rvu1Z"},"outputs":[],"source":["# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n","# This must be done before importing transformers\n","import shutil\n","from pathlib import Path\n","\n","if CFG.env == \"colab\":\n","    input_dir = Path(\"./drive/MyDrive/00.kaggle/input/deberta-v2-3-fast-tokenizer\")\n","    transformers_path = Path(\"/usr/local/lib/python3.7/dist-packages/transformers\")\n","else:\n","    input_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n","    transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n","\n","convert_file = input_dir / \"convert_slow_tokenizer.py\"\n","conversion_path = transformers_path/convert_file.name\n","\n","if conversion_path.exists():\n","    conversion_path.unlink()\n","\n","shutil.copy(convert_file, transformers_path)\n","deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n","\n","for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n","    filepath = deberta_v2_path/filename\n","    if filepath.exists():\n","        filepath.unlink()\n","\n","    shutil.copy(input_dir/filename, filepath)\n","    \n","    \n","from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3365,"status":"ok","timestamp":1646561185208,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"vital-mexico"},"outputs":[],"source":["import gc\n","import os\n","import ast\n","import time\n","import math\n","import random\n","import itertools\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","from scipy.optimize import minimize\n","from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as T\n","from torchvision.io import read_image\n","from torch.utils.data import DataLoader, Dataset\n","\n","from transformers import AutoModelForMaskedLM\n","from transformers import BartModel,BertModel,BertTokenizer\n","from transformers import DebertaModel,DebertaTokenizer\n","from transformers import RobertaModel,RobertaTokenizer\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"economic-ladder"},"source":["## Utilities"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1646561185209,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"desperate-keyboard"},"outputs":[],"source":["def micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on binary arrays.\n","\n","    Args:\n","        preds (list of lists of ints): Predictions.\n","        truths (list of lists of ints): Ground truths.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    # Micro : aggregating over all instances\n","    preds = np.concatenate(preds)\n","    truths = np.concatenate(truths)\n","    return f1_score(truths, preds)\n","\n","\n","def spans_to_binary(spans, length=None):\n","    \"\"\"\n","    Converts spans to a binary array indicating whether each character is in the span.\n","\n","    Args:\n","        spans (list of lists of two ints): Spans.\n","\n","    Returns:\n","        np array [length]: Binarized spans.\n","    \"\"\"\n","    length = np.max(spans) if length is None else length\n","    binary = np.zeros(length)\n","    for start, end in spans:\n","        binary[start:end] = 1\n","    return binary\n","\n","\n","def span_micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on spans.\n","\n","    Args:\n","        preds (list of lists of two ints): Prediction spans.\n","        truths (list of lists of two ints): Ground truth spans.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    bin_preds = []\n","    bin_truths = []\n","    for pred, truth in zip(preds, truths):\n","        if not len(pred) and not len(truth):\n","            continue\n","        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n","        bin_preds.append(spans_to_binary(pred, length))\n","        bin_truths.append(spans_to_binary(truth, length))\n","    return micro_f1(bin_preds, bin_truths)\n","\n","\n","def get_score(y_true, y_pred):\n","    score = span_micro_f1(y_true, y_pred)\n","    return score"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1646561185209,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"flexible-wednesday"},"outputs":[],"source":["def create_labels_for_scoring(df):\n","    # example: ['48 61', '111 128'] -\u003e [[48, 61], [111, 128]]\n","    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n","    for i in range(len(df)):\n","        lst = df.loc[i, \"location\"]\n","        if lst:\n","            new_lst = \";\".join(lst)\n","            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n","\n","    # create labels\n","    truths = []\n","    for location_list in df[\"location_for_create_labels\"].values:\n","        truth = []\n","        if len(location_list) \u003e 0:\n","            location = location_list[0]\n","            for loc in [s.split() for s in location.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                truth.append([start, end])\n","        truths.append(truth)\n","\n","    return truths\n","\n","\n","def get_char_probs(texts, token_probs, tokenizer):\n","    res = [np.zeros(len(t)) for t in texts]\n","    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n","        encoded = tokenizer(\n","            text=text,\n","            max_length=CFG.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n","            start, end = offset_mapping\n","            res[i][start:end] = pred\n","    return res\n","\n","\n","def get_predicted_location_str(char_probs, th=0.5):\n","    results = []\n","    for char_prob in char_probs:\n","        result = np.where(char_prob \u003e= th)[0] + 1\n","        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n","        result = [f\"{min(r)} {max(r)}\" for r in result]\n","        result = \";\".join(result)\n","        results.append(result)\n","    return results\n","\n","\n","def get_predictions(results):\n","    predictions = []\n","    for result in results:\n","        prediction = []\n","        if result != \"\":\n","            for loc in [s.split() for s in result.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                prediction.append([start, end])\n","        predictions.append(prediction)\n","    return predictions\n","\n","\n","def scoring(df, th=0.5):\n","    labels = create_labels_for_scoring(df)\n","\n","    token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n","    char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n","    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n","    preds = get_predictions(predicted_location_str)\n","\n","    score = get_score(labels, preds)\n","    return score\n","\n","\n","def get_best_thres(oof_df):\n","    def f1_opt(x):\n","        return -1 * scoring(oof_df, th=x)\n","\n","    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n","    return best_thres"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1646561185210,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"logical-chemistry"},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return \"%dm %ds\" % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1646561185210,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"gorgeous-record"},"outputs":[],"source":["seed_everything()"]},{"cell_type":"markdown","metadata":{"id":"frozen-africa"},"source":["## Data Loading"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3785,"status":"ok","timestamp":1646561188985,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"shaped-metallic","outputId":"856f7794-ace6-4a8d-9af8-353bd2c6e6c8"},"outputs":[{"data":{"text/plain":["((14300, 6), (143, 3), (42146, 3), (5, 4))"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["train = pd.read_csv(CFG.input_dir / \"train.csv\")\n","features = pd.read_csv(CFG.input_dir / \"features.csv\")\n","patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n","test = pd.read_csv(CFG.input_dir / \"test.csv\")\n","\n","train.shape, features.shape, patient_notes.shape, test.shape"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1646561188985,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"visible-australia"},"outputs":[],"source":["if CFG.debug:\n","    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n","    print(train.shape)"]},{"cell_type":"markdown","metadata":{"id":"hydraulic-gibson"},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1646561188986,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"interpreted-northeast"},"outputs":[],"source":["def preprocess_features(features):\n","    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n","    return features\n","\n","\n","features = preprocess_features(features)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1646561188986,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"martial-blind","outputId":"8791b6f1-a429-4b8f-ea5a-b1a11f2e3673"},"outputs":[{"data":{"text/plain":["((14300, 8), (5, 6))"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","\n","train.shape, test.shape"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":443,"status":"ok","timestamp":1646561189422,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"electoral-favor"},"outputs":[],"source":["train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n","train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1646561189423,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"reported-parade","outputId":"1417709a-e22e-4ad8-d338-ec563abf7650"},"outputs":[{"data":{"text/plain":["0    4399\n","1    8181\n","2    1296\n","3     287\n","4      99\n","5      27\n","6       9\n","7       1\n","8       1\n","Name: annotation_length, dtype: int64"]},"metadata":{},"output_type":"display_data"}],"source":["train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n","display(train['annotation_length'].value_counts().sort_index())"]},{"cell_type":"markdown","metadata":{"id":"enabling-relevance"},"source":["## CV split"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1646561189424,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"mature-coalition"},"outputs":[],"source":["def get_groupkfold(df, group_name):\n","    groups = df[group_name].unique()\n","\n","    kf = KFold(\n","        n_splits=CFG.n_fold,\n","        shuffle=True,\n","        random_state=CFG.seed,\n","    )\n","    folds_ids = []\n","    for i_fold, (_, val_group_idx) in enumerate(kf.split(groups)):\n","        val_group = groups[val_group_idx]\n","        is_val = df[group_name].isin(val_group)\n","        val_idx = df[is_val].index\n","        df.loc[val_idx, \"fold\"] = int(i_fold)\n","\n","    df[\"fold\"] = df[\"fold\"].astype(int)\n","    return df"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1646561189424,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"every-minutes","outputId":"98c7017f-7f4d-40d4-c780-a5e96af83b35"},"outputs":[{"data":{"text/plain":["fold\n","0    2902\n","1    2894\n","2    2813\n","3    2791\n","4    2900\n","dtype: int64"]},"metadata":{},"output_type":"display_data"}],"source":["train = get_groupkfold(train, \"pn_num\")\n","display(train.groupby(\"fold\").size())"]},{"cell_type":"markdown","metadata":{"id":"subjective-entrance"},"source":["## Setup tokenizer"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":148},"executionInfo":{"elapsed":6608,"status":"ok","timestamp":1646561196023,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"dramatic-afghanistan","outputId":"74905948-0ce9-4f03-cd01-caedaab413df"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"66985436eab642de96ec07b7effd864c","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/2.35M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"342da89ce3cc4d019a74ffeb3f672761","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/52.0 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"accf134a4fad47acb5fbdf63109f59ac","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/580 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["if CFG.submission:\n","    tokenizer = DebertaV2TokenizerFast.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n","else:\n","    tokenizer = DebertaV2TokenizerFast.from_pretrained(CFG.pretrained_model_name)\n","    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n","\n","CFG.tokenizer = tokenizer"]},{"cell_type":"markdown","metadata":{"id":"divided-arrow"},"source":["## Create dataset"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"elapsed":30170,"status":"ok","timestamp":1646561226181,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"immune-campbell","outputId":"5d9f58e6-5628-455e-ce86-7c0e52a2f666"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4dc78aaceb304b0bbce94d5167d75df7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/42146 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["max length: 323\n"]}],"source":["pn_history_lengths = []\n","tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    pn_history_lengths.append(length)\n","\n","print(\"max length:\", np.max(pn_history_lengths))"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1646561226182,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"northern-branch","outputId":"c27f2a99-e86b-4d52-fdd6-b8ff4df87cc4"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e0d7789be82545e2a188433cb001cdce","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/143 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["max length: 28\n"]}],"source":["feature_text_lengths = []\n","tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    feature_text_lengths.append(length)\n","\n","print(\"max length:\", np.max(feature_text_lengths))"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1646561226183,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"oriental-jacksonville","outputId":"82e1db80-dae9-4016-8081-80e4834c3489"},"outputs":[{"name":"stdout","output_type":"stream","text":["max length: 354\n"]}],"source":["CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls \u0026 sep \u0026 sep\n","\n","print(\"max length:\", CFG.max_len)"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1646561226183,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"flexible-trainer"},"outputs":[],"source":["class TrainingDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","        self.annotation_lengths = self.df[\"annotation_length\"].values\n","        self.locations = self.df[\"location\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def _create_label(self, pn_history, annotation_length, location_list):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        offset_mapping = encoded[\"offset_mapping\"]\n","        ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n","        label = np.zeros(len(offset_mapping))\n","        label[ignore_idxes] = -1\n","\n","        if annotation_length \u003e 0:\n","            for location in location_list:\n","                for loc in [s.split() for s in location.split(\";\")]:\n","                    start, end = int(loc[0]), int(loc[1])\n","                    start_idx = -1\n","                    end_idx = -1\n","                    for idx in range(len(offset_mapping)):\n","                        if (start_idx == -1) \u0026 (start \u003c offset_mapping[idx][0]):\n","                            start_idx = idx - 1\n","                        if (end_idx == -1) \u0026 (end \u003c= offset_mapping[idx][1]):\n","                            end_idx = idx + 1\n","                    if start_idx == -1:\n","                        start_idx = end_idx\n","                    if (start_idx != -1) \u0026 (end_idx != -1):\n","                        label[start_idx:end_idx] = 1\n","\n","        return torch.tensor(label, dtype=torch.float)\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n","        return input_, label"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1646561226184,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"stock-robertson"},"outputs":[],"source":["class TestDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        return input_"]},{"cell_type":"markdown","metadata":{"id":"chemical-lucas"},"source":["## Model"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1646561226185,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"animated-array"},"outputs":[],"source":["class CustomModel(nn.Module):\n","    def __init__(self, cfg, model_config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        if model_config_path is None:\n","            self.model_config = AutoConfig.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                output_hidden_states=True,\n","            )\n","        else:\n","            self.model_config = torch.load(model_config_path)\n","\n","        if pretrained:\n","            self.backbone = AutoModel.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                config=self.model_config,\n","            )\n","            print(f\"Load weight from pretrained\")\n","        else:\n","            self.backbone = AutoModel.from_config(self.model_config)\n","\n","        \"\"\"\n","        itpt = AutoModelForMaskedLM.from_config(self.model_config)\n","        #path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp009/checkpoint-129000/pytorch_model.bin\")\n","        path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n","        state_dict = torch.load(path)\n","        itpt.load_state_dict(state_dict)\n","        self.backbone = itpt.deberta\n","        print(f\"Load weight from {path}\")\n","        \"\"\"\n","\n","        self.fc = nn.Sequential(\n","            nn.Dropout(self.cfg.dropout),\n","            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n","        )\n","\n","    def forward(self, inputs):\n","        h = self.backbone(**inputs)[\"last_hidden_state\"]\n","        output = self.fc(h)\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"thorough-bristol"},"source":["## Training"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1646561226185,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"n8Z5UnO9cCxW"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class FocalLoss(nn.Module):\n","    \"\"\"https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/65938\n","    \"\"\"\n","    def __init__(self, alpha=1, gamma=2, smooth=0.1, logits=True, reduce=True):\n","        super(FocalLoss, self).__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.smooth = smooth\n","        self.logits = logits\n","        self.reduce = reduce\n","\n","    def forward(self, inputs, targets):\n","        targets = targets * (1 - self.smooth) + self.smooth\n","\n","        if self.logits:\n","            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n","        else:\n","            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n","        pt = torch.exp(-BCE_loss)\n","        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n","\n","        if self.reduce:\n","            return torch.mean(F_loss)\n","        else:\n","            return F_loss"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":580,"status":"ok","timestamp":1646561226739,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"talented-quantity"},"outputs":[],"source":["def train_fn(\n","    train_dataloader,\n","    model,\n","    criterion,\n","    optimizer,\n","    epoch,\n","    scheduler,\n","    device,\n","):\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(train_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","\n","        pos_nums = (labels == 1).sum(axis=1)\n","        pos_nums = torch.tensor([[pos_num] * CFG.max_len for pos_num in pos_nums]).view(-1, 1).to(device)\n","        pos_nums = torch.masked_select(pos_nums, labels.view(-1, 1) != -1)\n","        weight = []\n","        for pos_num in pos_nums:\n","            if pos_num == 0:\n","                weight.append(3.0)\n","            else:\n","                weight.append(1.0)\n","        weight = torch.tensor(weight).to(device)\n","        loss = loss * weight\n","\n","        loss = loss.mean()\n","\n","        if CFG.gradient_accumulation_steps \u003e 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","\n","        if CFG.batch_scheduler:\n","            scheduler.step()\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n","            print(\n","                \"Epoch: [{0}][{1}/{2}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                \"Grad: {grad_norm:.4f}  \"\n","                \"LR: {lr:.6f}  \"\n","                .format(\n","                    epoch+1,\n","                    step,\n","                    len(train_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n","                    loss=losses,\n","                     grad_norm=grad_norm,\n","                     lr=scheduler.get_lr()[0],\n","                )\n","            )\n","    return losses.avg"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1646561226740,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"figured-cooperative"},"outputs":[],"source":["def valid_fn(\n","    val_dataloader,\n","    model,\n","    criterion,\n","    device,\n","):\n","    model.eval()\n","    preds = []\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(val_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.no_grad():\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","\n","        pos_nums = (labels == 1).sum(axis=1)\n","        pos_nums = torch.tensor([[pos_num] * CFG.max_len for pos_num in pos_nums]).view(-1, 1).to(device)\n","        pos_nums = torch.masked_select(pos_nums, labels.view(-1, 1) != -1)\n","        weight = []\n","        for pos_num in pos_nums:\n","            if pos_num == 0:\n","                weight.append(3.0)\n","            else:\n","                weight.append(1.0)\n","        weight = torch.tensor(weight).to(device)\n","        loss = loss * weight\n","\n","        loss = loss.mean()\n","\n","        if CFG.gradient_accumulation_steps \u003e 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n","            print(\n","                \"EVAL: [{0}/{1}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                .format(\n","                    step, len(val_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n","                    loss=losses,\n","                )\n","            )\n","    preds = np.concatenate(preds)\n","    return losses.avg, preds"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1646561226740,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"played-pointer"},"outputs":[],"source":["def inference_fn(test_dataloader, model, device):\n","    model.eval()\n","    model.to(device)\n","    preds = []\n","    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n","    for inputs in tk0:\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        with torch.no_grad():\n","            output = model(inputs)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","    preds = np.concatenate(preds)\n","    return preds"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1646561226741,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"brazilian-nigeria"},"outputs":[],"source":["def train_loop(df, i_fold, device):\n","    print(f\"========== fold: {i_fold} training ==========\")\n","    train_idx = df[df[\"fold\"] != i_fold].index\n","    val_idx = df[df[\"fold\"] == i_fold].index\n","\n","    train_folds = df.loc[train_idx].reset_index(drop=True)\n","    val_folds = df.loc[val_idx].reset_index(drop=True)\n","\n","    train_dataset = TrainingDataset(CFG, train_folds)\n","    val_dataset = TrainingDataset(CFG, val_folds)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=True,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=True,\n","    )\n","    val_dataloader = DataLoader(\n","        val_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=False,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=False,\n","    )\n","\n","    model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n","    model.to(device)\n","\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\"params\": [p for n, p in param_optimizer if not any(\n","            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n","        {\"params\": [p for n, p in param_optimizer if any(\n","            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n","    ]\n","    optimizer = AdamW(\n","        optimizer_grouped_parameters,\n","        lr=CFG.lr,\n","        betas=CFG.betas,\n","        weight_decay=CFG.weight_decay,\n","    )\n","    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n","    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        num_training_steps=num_train_optimization_steps,\n","    )\n","\n","    #criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    criterion = FocalLoss(reduce=False)\n","    best_score = -1 * np.inf\n","\n","    for epoch in range(CFG.epochs):\n","        start_time = time.time()\n","        avg_loss = train_fn(\n","            train_dataloader,\n","            model,\n","            criterion,\n","            optimizer,\n","            epoch,\n","            scheduler,\n","            device,\n","        )\n","        avg_val_loss, val_preds = valid_fn(\n","            val_dataloader,\n","            model,\n","            criterion,\n","            device,\n","        )\n","\n","        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n","            scheduler.step()\n","\n","        # scoring\n","        val_folds[[str(i) for i in range(CFG.max_len)]] = val_preds\n","        score = scoring(val_folds, th=0.5)\n","\n","        elapsed = time.time() - start_time\n","\n","        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n","        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n","        if score \u003e best_score:\n","            best_score = score\n","            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n","            torch.save({\n","                \"model\": model.state_dict(),\n","                \"predictions\": val_preds,\n","                },\n","                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","            )\n","\n","    predictions = torch.load(\n","        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","        map_location=torch.device(\"cpu\"),\n","    )[\"predictions\"]\n","    val_folds[[str(i) for i in range(CFG.max_len)]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return val_folds"]},{"cell_type":"markdown","metadata":{"id":"bearing-switch"},"source":["## Main"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1646561226741,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"desperate-crime"},"outputs":[],"source":["def main():\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    if CFG.train:\n","        oof_df = pd.DataFrame()\n","        for i_fold in range(CFG.n_fold):\n","            if i_fold in CFG.train_fold:\n","                _oof_df = train_loop(train, i_fold, device)\n","                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n","        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    if CFG.submission:\n","        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n","    else:\n","        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    score = scoring(oof_df, th=0.5)\n","    print(f\"Best thres: 0.5, Score: {score:.4f}\")\n","    best_thres = get_best_thres(oof_df)\n","    score = scoring(oof_df, th=best_thres)\n","    print(f\"Best thres: {best_thres}, Score: {score:.4f}\")\n","\n","    if CFG.inference:\n","        test_dataset = TestDataset(CFG, test)\n","        test_dataloader = DataLoader(\n","            test_dataset,\n","            batch_size=CFG.batch_size,\n","            shuffle=False,\n","            num_workers=CFG.num_workers,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","        predictions = []\n","        for i_fold in CFG.train_fold:\n","            if CFG.submission:\n","                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n","                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n","            else:\n","                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n","\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            model.load_state_dict(state[\"model\"])\n","            test_token_probs = inference_fn(test_dataloader, model, device)\n","            test[[f\"fold{i_fold}_{i}\" for i in range(CFG.max_len)]] = test_token_probs\n","            test_char_probs = get_char_probs(test[\"pn_history\"].values, test_token_probs, CFG.tokenizer)\n","            predictions.append(test_char_probs)\n","\n","            del state, test_token_probs, model; gc.collect()\n","            torch.cuda.empty_cache()\n","\n","        predictions = np.mean(predictions, axis=0)\n","        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n","        test[CFG.target_col] = predicted_location_str\n","        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n","        test[[CFG.id_col, CFG.target_col]].to_csv(\n","            CFG.output_dir / \"submission.csv\", index=False\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"graduate-vision"},"outputs":[{"name":"stdout","output_type":"stream","text":["========== fold: 0 training ==========\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e0ebc88dfdce4aa4bccc935b4aaccaa9","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/833M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Load weight from pretrained\n","Epoch: [1][0/2849] Elapsed 0m 0s (remain 44m 3s) Loss: 0.0847(0.0847) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/2849] Elapsed 0m 30s (remain 13m 53s) Loss: 0.0521(0.0789) Grad: 15757.5420  LR: 0.000001  \n","Epoch: [1][200/2849] Elapsed 0m 58s (remain 12m 51s) Loss: 0.0369(0.0577) Grad: 2155.9446  LR: 0.000003  \n","Epoch: [1][300/2849] Elapsed 1m 26s (remain 12m 12s) Loss: 0.0351(0.0491) Grad: 2739.9614  LR: 0.000004  \n","Epoch: [1][400/2849] Elapsed 1m 54s (remain 11m 38s) Loss: 0.0319(0.0439) Grad: 3610.5574  LR: 0.000006  \n","Epoch: [1][500/2849] Elapsed 2m 22s (remain 11m 5s) Loss: 0.0255(0.0404) Grad: 6930.3491  LR: 0.000007  \n","Epoch: [1][600/2849] Elapsed 2m 49s (remain 10m 35s) Loss: 0.0285(0.0379) Grad: 1187.9866  LR: 0.000008  \n","Epoch: [1][700/2849] Elapsed 3m 17s (remain 10m 4s) Loss: 0.0254(0.0362) Grad: 847.2330  LR: 0.000010  \n","Epoch: [1][800/2849] Elapsed 3m 45s (remain 9m 35s) Loss: 0.0259(0.0347) Grad: 9617.3242  LR: 0.000011  \n","Epoch: [1][900/2849] Elapsed 4m 12s (remain 9m 6s) Loss: 0.0198(0.0338) Grad: 1297.6677  LR: 0.000013  \n","Epoch: [1][1000/2849] Elapsed 4m 40s (remain 8m 37s) Loss: 0.0181(0.0326) Grad: 3838.6604  LR: 0.000014  \n","Epoch: [1][1100/2849] Elapsed 5m 7s (remain 8m 8s) Loss: 0.0157(0.0318) Grad: 833.4013  LR: 0.000015  \n","Epoch: [1][1200/2849] Elapsed 5m 35s (remain 7m 40s) Loss: 0.0197(0.0311) Grad: 405.7954  LR: 0.000017  \n","Epoch: [1][1300/2849] Elapsed 6m 3s (remain 7m 12s) Loss: 0.0163(0.0305) Grad: 1709.4863  LR: 0.000018  \n","Epoch: [1][1400/2849] Elapsed 6m 31s (remain 6m 44s) Loss: 0.0180(0.0300) Grad: 646.6716  LR: 0.000020  \n","Epoch: [1][1500/2849] Elapsed 6m 58s (remain 6m 16s) Loss: 0.0142(0.0295) Grad: 1347.9148  LR: 0.000020  \n","Epoch: [1][1600/2849] Elapsed 7m 26s (remain 5m 47s) Loss: 0.0255(0.0291) Grad: 898.0637  LR: 0.000020  \n","Epoch: [1][1700/2849] Elapsed 7m 54s (remain 5m 19s) Loss: 0.0201(0.0288) Grad: 530.9353  LR: 0.000020  \n","Epoch: [1][1800/2849] Elapsed 8m 21s (remain 4m 51s) Loss: 0.0199(0.0286) Grad: 884.8714  LR: 0.000019  \n","Epoch: [1][1900/2849] Elapsed 8m 49s (remain 4m 24s) Loss: 0.0147(0.0283) Grad: 1819.4539  LR: 0.000019  \n","Epoch: [1][2000/2849] Elapsed 9m 17s (remain 3m 56s) Loss: 0.0213(0.0280) Grad: 4183.3608  LR: 0.000019  \n","Epoch: [1][2100/2849] Elapsed 9m 45s (remain 3m 28s) Loss: 0.0148(0.0277) Grad: 1842.2546  LR: 0.000019  \n","Epoch: [1][2200/2849] Elapsed 10m 12s (remain 3m 0s) Loss: 0.0255(0.0274) Grad: 1273.0690  LR: 0.000019  \n","Epoch: [1][2300/2849] Elapsed 10m 40s (remain 2m 32s) Loss: 0.0134(0.0272) Grad: 514.7040  LR: 0.000019  \n","Epoch: [1][2400/2849] Elapsed 11m 8s (remain 2m 4s) Loss: 0.0126(0.0269) Grad: 97.6105  LR: 0.000018  \n","Epoch: [1][2500/2849] Elapsed 11m 36s (remain 1m 36s) Loss: 0.0242(0.0267) Grad: 537.3968  LR: 0.000018  \n","Epoch: [1][2600/2849] Elapsed 12m 3s (remain 1m 9s) Loss: 0.0225(0.0265) Grad: 2651.7683  LR: 0.000018  \n","Epoch: [1][2700/2849] Elapsed 12m 31s (remain 0m 41s) Loss: 0.0407(0.0264) Grad: 4890.9453  LR: 0.000018  \n","Epoch: [1][2800/2849] Elapsed 12m 59s (remain 0m 13s) Loss: 0.0282(0.0262) Grad: 7860.0039  LR: 0.000018  \n","Epoch: [1][2848/2849] Elapsed 13m 12s (remain 0m 0s) Loss: 0.0221(0.0262) Grad: 3007.8660  LR: 0.000018  \n","EVAL: [0/726] Elapsed 0m 0s (remain 6m 10s) Loss: 0.0193(0.0193) \n","EVAL: [100/726] Elapsed 0m 18s (remain 1m 52s) Loss: 0.0191(0.0207) \n","EVAL: [200/726] Elapsed 0m 35s (remain 1m 33s) Loss: 0.0228(0.0225) \n","EVAL: [300/726] Elapsed 0m 53s (remain 1m 15s) Loss: 0.0189(0.0219) \n","EVAL: [400/726] Elapsed 1m 11s (remain 0m 58s) Loss: 0.0256(0.0225) \n","EVAL: [500/726] Elapsed 1m 29s (remain 0m 40s) Loss: 0.0221(0.0223) \n","EVAL: [600/726] Elapsed 1m 47s (remain 0m 22s) Loss: 0.0328(0.0224) \n","EVAL: [700/726] Elapsed 2m 4s (remain 0m 4s) Loss: 0.0195(0.0227) \n","EVAL: [725/726] Elapsed 2m 8s (remain 0m 0s) Loss: 0.0252(0.0229) \n","Epoch 1 - avg_train_loss: 0.0262  avg_val_loss: 0.0229  time: 933s\n","Epoch 1 - Score: 0.8521\n","Epoch 1 - Save Best Score: 0.8521 Model\n","Epoch: [2][0/2849] Elapsed 0m 0s (remain 28m 11s) Loss: 0.0338(0.0338) Grad: 1624.8236  LR: 0.000018  \n","Epoch: [2][100/2849] Elapsed 0m 30s (remain 13m 56s) Loss: 0.0131(0.0219) Grad: 2008.3894  LR: 0.000018  \n","Epoch: [2][200/2849] Elapsed 1m 0s (remain 13m 12s) Loss: 0.0184(0.0220) Grad: 1229.1925  LR: 0.000017  \n","Epoch: [2][300/2849] Elapsed 1m 27s (remain 12m 23s) Loss: 0.0182(0.0219) Grad: 3282.5566  LR: 0.000017  \n","Epoch: [2][400/2849] Elapsed 1m 55s (remain 11m 44s) Loss: 0.0185(0.0220) Grad: 513.9209  LR: 0.000017  \n","Epoch: [2][500/2849] Elapsed 2m 23s (remain 11m 11s) Loss: 0.0183(0.0219) Grad: 253.3490  LR: 0.000017  \n","Epoch: [2][600/2849] Elapsed 2m 50s (remain 10m 39s) Loss: 0.0146(0.0219) Grad: 7930.4829  LR: 0.000017  \n","Epoch: [2][700/2849] Elapsed 3m 18s (remain 10m 8s) Loss: 0.0190(0.0220) Grad: 9120.8818  LR: 0.000017  \n","Epoch: [2][800/2849] Elapsed 3m 46s (remain 9m 38s) Loss: 0.0264(0.0220) Grad: 6586.3096  LR: 0.000017  \n","Epoch: [2][900/2849] Elapsed 4m 14s (remain 9m 9s) Loss: 0.0197(0.0220) Grad: 244.0078  LR: 0.000016  \n","Epoch: [2][1000/2849] Elapsed 4m 41s (remain 8m 40s) Loss: 0.0198(0.0221) Grad: 3359.4104  LR: 0.000016  \n","Epoch: [2][1100/2849] Elapsed 5m 9s (remain 8m 11s) Loss: 0.0126(0.0220) Grad: 874.5586  LR: 0.000016  \n","Epoch: [2][1200/2849] Elapsed 5m 37s (remain 7m 42s) Loss: 0.0290(0.0221) Grad: 12832.3115  LR: 0.000016  \n","Epoch: [2][1300/2849] Elapsed 6m 4s (remain 7m 14s) Loss: 0.0276(0.0220) Grad: 4057.8979  LR: 0.000016  \n","Epoch: [2][1400/2849] Elapsed 6m 32s (remain 6m 45s) Loss: 0.0196(0.0220) Grad: 1751.4916  LR: 0.000016  \n","Epoch: [2][1500/2849] Elapsed 7m 0s (remain 6m 17s) Loss: 0.0158(0.0220) Grad: 6439.5186  LR: 0.000015  \n","Epoch: [2][1600/2849] Elapsed 7m 28s (remain 5m 49s) Loss: 0.0202(0.0220) Grad: 13329.8691  LR: 0.000015  \n","Epoch: [2][1700/2849] Elapsed 7m 55s (remain 5m 21s) Loss: 0.0127(0.0220) Grad: 1547.6996  LR: 0.000015  \n","Epoch: [2][1800/2849] Elapsed 8m 23s (remain 4m 52s) Loss: 0.0235(0.0220) Grad: 5401.9116  LR: 0.000015  \n","Epoch: [2][1900/2849] Elapsed 8m 51s (remain 4m 24s) Loss: 0.0244(0.0220) Grad: 3130.8616  LR: 0.000015  \n","Epoch: [2][2000/2849] Elapsed 9m 18s (remain 3m 56s) Loss: 0.0248(0.0220) Grad: 719.0357  LR: 0.000015  \n","Epoch: [2][2100/2849] Elapsed 9m 46s (remain 3m 28s) Loss: 0.0202(0.0220) Grad: 2002.6357  LR: 0.000014  \n","Epoch: [2][2200/2849] Elapsed 10m 14s (remain 3m 0s) Loss: 0.0176(0.0220) Grad: 1315.1080  LR: 0.000014  \n","Epoch: [2][2300/2849] Elapsed 10m 41s (remain 2m 32s) Loss: 0.0283(0.0219) Grad: 43978.2266  LR: 0.000014  \n","Epoch: [2][2400/2849] Elapsed 11m 9s (remain 2m 4s) Loss: 0.0331(0.0219) Grad: 2284.5244  LR: 0.000014  \n","Epoch: [2][2500/2849] Elapsed 11m 37s (remain 1m 37s) Loss: 0.0255(0.0219) Grad: 635.4748  LR: 0.000014  \n","Epoch: [2][2600/2849] Elapsed 12m 4s (remain 1m 9s) Loss: 0.0137(0.0219) Grad: 5094.8149  LR: 0.000014  \n","Epoch: [2][2700/2849] Elapsed 12m 32s (remain 0m 41s) Loss: 0.0284(0.0218) Grad: 17277.2441  LR: 0.000014  \n","Epoch: [2][2800/2849] Elapsed 13m 0s (remain 0m 13s) Loss: 0.0188(0.0218) Grad: 972.5262  LR: 0.000013  \n","Epoch: [2][2848/2849] Elapsed 13m 13s (remain 0m 0s) Loss: 0.0195(0.0218) Grad: 2666.8525  LR: 0.000013  \n","EVAL: [0/726] Elapsed 0m 0s (remain 5m 34s) Loss: 0.0189(0.0189) \n","EVAL: [100/726] Elapsed 0m 18s (remain 1m 51s) Loss: 0.0188(0.0208) \n","EVAL: [200/726] Elapsed 0m 35s (remain 1m 33s) Loss: 0.0187(0.0226) \n","EVAL: [300/726] Elapsed 0m 53s (remain 1m 15s) Loss: 0.0187(0.0219) \n","EVAL: [400/726] Elapsed 1m 11s (remain 0m 57s) Loss: 0.0251(0.0227) \n","EVAL: [500/726] Elapsed 1m 28s (remain 0m 39s) Loss: 0.0243(0.0225) \n","EVAL: [600/726] Elapsed 1m 46s (remain 0m 22s) Loss: 0.0319(0.0225) \n","EVAL: [700/726] Elapsed 2m 3s (remain 0m 4s) Loss: 0.0190(0.0228) \n","EVAL: [725/726] Elapsed 2m 8s (remain 0m 0s) Loss: 0.0250(0.0229) \n","Epoch 2 - avg_train_loss: 0.0218  avg_val_loss: 0.0229  time: 926s\n","Epoch 2 - Score: 0.8669\n","Epoch 2 - Save Best Score: 0.8669 Model\n","Epoch: [3][0/2849] Elapsed 0m 0s (remain 29m 59s) Loss: 0.0213(0.0213) Grad: 9556.9561  LR: 0.000013  \n","Epoch: [3][100/2849] Elapsed 0m 32s (remain 14m 48s) Loss: 0.0153(0.0216) Grad: 4343.7695  LR: 0.000013  \n","Epoch: [3][200/2849] Elapsed 1m 1s (remain 13m 33s) Loss: 0.0130(0.0218) Grad: 4973.7056  LR: 0.000013  \n","Epoch: [3][300/2849] Elapsed 1m 30s (remain 12m 43s) Loss: 0.0242(0.0219) Grad: 365.2838  LR: 0.000013  \n","Epoch: [3][400/2849] Elapsed 1m 58s (remain 12m 3s) Loss: 0.0125(0.0218) Grad: 1503.7312  LR: 0.000013  \n","Epoch: [3][500/2849] Elapsed 2m 26s (remain 11m 28s) Loss: 0.0125(0.0218) Grad: 516.1494  LR: 0.000013  \n","Epoch: [3][600/2849] Elapsed 2m 55s (remain 10m 55s) Loss: 0.0317(0.0217) Grad: 535.5738  LR: 0.000012  \n","Epoch: [3][700/2849] Elapsed 3m 23s (remain 10m 23s) Loss: 0.0186(0.0217) Grad: 842.6025  LR: 0.000012  \n","Epoch: [3][800/2849] Elapsed 3m 52s (remain 9m 53s) Loss: 0.0226(0.0216) Grad: 994.4628  LR: 0.000012  \n","Epoch: [3][900/2849] Elapsed 4m 20s (remain 9m 22s) Loss: 0.0188(0.0217) Grad: 804.5777  LR: 0.000012  \n","Epoch: [3][1000/2849] Elapsed 4m 48s (remain 8m 52s) Loss: 0.0134(0.0217) Grad: 2490.4045  LR: 0.000012  \n","Epoch: [3][1100/2849] Elapsed 5m 16s (remain 8m 23s) Loss: 0.0128(0.0217) Grad: 2143.5205  LR: 0.000012  \n","Epoch: [3][1200/2849] Elapsed 5m 45s (remain 7m 53s) Loss: 0.0220(0.0216) Grad: 21148.8828  LR: 0.000011  \n","Epoch: [3][1300/2849] Elapsed 6m 13s (remain 7m 24s) Loss: 0.0133(0.0215) Grad: 2008.4456  LR: 0.000011  \n","Epoch: [3][1400/2849] Elapsed 6m 41s (remain 6m 55s) Loss: 0.0303(0.0215) Grad: 10174.5713  LR: 0.000011  \n","Epoch: [3][1500/2849] Elapsed 7m 9s (remain 6m 25s) Loss: 0.0232(0.0215) Grad: 5454.4507  LR: 0.000011  \n","Epoch: [3][1600/2849] Elapsed 7m 37s (remain 5m 57s) Loss: 0.0188(0.0216) Grad: 234.6836  LR: 0.000011  \n","Epoch: [3][1700/2849] Elapsed 8m 6s (remain 5m 28s) Loss: 0.0247(0.0216) Grad: 3925.6333  LR: 0.000011  \n","Epoch: [3][1800/2849] Elapsed 8m 34s (remain 4m 59s) Loss: 0.0186(0.0216) Grad: 4025.8887  LR: 0.000011  \n","Epoch: [3][1900/2849] Elapsed 9m 2s (remain 4m 30s) Loss: 0.0186(0.0215) Grad: 663.2371  LR: 0.000010  \n","Epoch: [3][2000/2849] Elapsed 9m 31s (remain 4m 2s) Loss: 0.0194(0.0215) Grad: 3708.7549  LR: 0.000010  \n","Epoch: [3][2100/2849] Elapsed 9m 59s (remain 3m 33s) Loss: 0.0245(0.0214) Grad: 223.7002  LR: 0.000010  \n","Epoch: [3][2200/2849] Elapsed 10m 27s (remain 3m 4s) Loss: 0.0128(0.0214) Grad: 1544.5831  LR: 0.000010  \n","Epoch: [3][2300/2849] Elapsed 10m 56s (remain 2m 36s) Loss: 0.0204(0.0214) Grad: 1950.0100  LR: 0.000010  \n","Epoch: [3][2400/2849] Elapsed 11m 24s (remain 2m 7s) Loss: 0.0224(0.0214) Grad: 29486.6406  LR: 0.000010  \n","Epoch: [3][2500/2849] Elapsed 11m 53s (remain 1m 39s) Loss: 0.0186(0.0214) Grad: 4172.7969  LR: 0.000009  \n","Epoch: [3][2600/2849] Elapsed 12m 21s (remain 1m 10s) Loss: 0.0263(0.0214) Grad: 8033.5850  LR: 0.000009  \n","Epoch: [3][2700/2849] Elapsed 12m 49s (remain 0m 42s) Loss: 0.0195(0.0213) Grad: 2762.5273  LR: 0.000009  \n","Epoch: [3][2800/2849] Elapsed 13m 18s (remain 0m 13s) Loss: 0.0182(0.0214) Grad: 1531.5356  LR: 0.000009  \n","Epoch: [3][2848/2849] Elapsed 13m 31s (remain 0m 0s) Loss: 0.0208(0.0213) Grad: 26938.5859  LR: 0.000009  \n","EVAL: [0/726] Elapsed 0m 0s (remain 6m 4s) Loss: 0.0190(0.0190) \n","EVAL: [100/726] Elapsed 0m 19s (remain 1m 58s) Loss: 0.0190(0.0207) \n","EVAL: [200/726] Elapsed 0m 37s (remain 1m 38s) Loss: 0.0187(0.0226) \n","EVAL: [300/726] Elapsed 0m 56s (remain 1m 19s) Loss: 0.0188(0.0219) \n","EVAL: [400/726] Elapsed 1m 14s (remain 1m 0s) Loss: 0.0253(0.0226) \n","EVAL: [500/726] Elapsed 1m 33s (remain 0m 42s) Loss: 0.0238(0.0223) \n","EVAL: [600/726] Elapsed 1m 52s (remain 0m 23s) Loss: 0.0347(0.0223) \n","EVAL: [700/726] Elapsed 2m 10s (remain 0m 4s) Loss: 0.0193(0.0226) \n","EVAL: [725/726] Elapsed 2m 15s (remain 0m 0s) Loss: 0.0251(0.0227) \n","Epoch 3 - avg_train_loss: 0.0213  avg_val_loss: 0.0227  time: 951s\n","Epoch 3 - Score: 0.8768\n","Epoch 3 - Save Best Score: 0.8768 Model\n","Epoch: [4][0/2849] Elapsed 0m 0s (remain 27m 42s) Loss: 0.0188(0.0188) Grad: 1079.7313  LR: 0.000009  \n","Epoch: [4][100/2849] Elapsed 0m 31s (remain 14m 18s) Loss: 0.0130(0.0210) Grad: 2813.8848  LR: 0.000009  \n","Epoch: [4][200/2849] Elapsed 0m 59s (remain 13m 10s) Loss: 0.0197(0.0210) Grad: 1509.7076  LR: 0.000009  \n","Epoch: [4][300/2849] Elapsed 1m 27s (remain 12m 23s) Loss: 0.0306(0.0212) Grad: 11595.0586  LR: 0.000008  \n","Epoch: [4][400/2849] Elapsed 1m 55s (remain 11m 45s) Loss: 0.0184(0.0211) Grad: 386.0695  LR: 0.000008  \n","Epoch: [4][500/2849] Elapsed 2m 23s (remain 11m 12s) Loss: 0.0184(0.0211) Grad: 1049.0978  LR: 0.000008  \n","Epoch: [4][600/2849] Elapsed 2m 51s (remain 10m 40s) Loss: 0.0337(0.0211) Grad: 25045.1289  LR: 0.000008  \n","Epoch: [4][700/2849] Elapsed 3m 19s (remain 10m 10s) Loss: 0.0250(0.0212) Grad: 12714.2871  LR: 0.000008  \n","Epoch: [4][800/2849] Elapsed 3m 46s (remain 9m 40s) Loss: 0.0188(0.0211) Grad: 1781.9592  LR: 0.000008  \n","Epoch: [4][900/2849] Elapsed 4m 14s (remain 9m 10s) Loss: 0.0184(0.0210) Grad: 401.8078  LR: 0.000007  \n","Epoch: [4][1000/2849] Elapsed 4m 42s (remain 8m 41s) Loss: 0.0169(0.0211) Grad: 492.3920  LR: 0.000007  \n","Epoch: [4][1100/2849] Elapsed 5m 10s (remain 8m 12s) Loss: 0.0187(0.0210) Grad: 2736.0659  LR: 0.000007  \n","Epoch: [4][1200/2849] Elapsed 5m 38s (remain 7m 44s) Loss: 0.0192(0.0210) Grad: 278.6209  LR: 0.000007  \n","Epoch: [4][1300/2849] Elapsed 6m 6s (remain 7m 15s) Loss: 0.0214(0.0210) Grad: 4307.5264  LR: 0.000007  \n","Epoch: [4][1400/2849] Elapsed 6m 33s (remain 6m 47s) Loss: 0.0195(0.0210) Grad: 81.5466  LR: 0.000007  \n","Epoch: [4][1500/2849] Elapsed 7m 1s (remain 6m 18s) Loss: 0.0124(0.0210) Grad: 222.2072  LR: 0.000007  \n","Epoch: [4][1600/2849] Elapsed 7m 29s (remain 5m 50s) Loss: 0.0180(0.0210) Grad: 2591.9985  LR: 0.000006  \n","Epoch: [4][1700/2849] Elapsed 7m 57s (remain 5m 22s) Loss: 0.0209(0.0210) Grad: 8084.4360  LR: 0.000006  \n","Epoch: [4][1800/2849] Elapsed 8m 25s (remain 4m 53s) Loss: 0.0160(0.0210) Grad: 4171.2900  LR: 0.000006  \n","Epoch: [4][1900/2849] Elapsed 8m 52s (remain 4m 25s) Loss: 0.0328(0.0210) Grad: 188.5095  LR: 0.000006  \n","Epoch: [4][2000/2849] Elapsed 9m 20s (remain 3m 57s) Loss: 0.0239(0.0210) Grad: 771.0464  LR: 0.000006  \n","Epoch: [4][2100/2849] Elapsed 9m 48s (remain 3m 29s) Loss: 0.0225(0.0210) Grad: 14841.5742  LR: 0.000006  \n","Epoch: [4][2200/2849] Elapsed 10m 16s (remain 3m 1s) Loss: 0.0218(0.0210) Grad: 127.5434  LR: 0.000005  \n","Epoch: [4][2300/2849] Elapsed 10m 44s (remain 2m 33s) Loss: 0.0260(0.0210) Grad: 1384.8628  LR: 0.000005  \n","Epoch: [4][2400/2849] Elapsed 11m 11s (remain 2m 5s) Loss: 0.0135(0.0210) Grad: 7156.9160  LR: 0.000005  \n","Epoch: [4][2500/2849] Elapsed 11m 39s (remain 1m 37s) Loss: 0.0177(0.0210) Grad: 2909.8657  LR: 0.000005  \n","Epoch: [4][2600/2849] Elapsed 12m 7s (remain 1m 9s) Loss: 0.0183(0.0211) Grad: 181.7779  LR: 0.000005  \n","Epoch: [4][2700/2849] Elapsed 12m 35s (remain 0m 41s) Loss: 0.0242(0.0211) Grad: 1890.6580  LR: 0.000005  \n","Epoch: [4][2800/2849] Elapsed 13m 3s (remain 0m 13s) Loss: 0.0244(0.0211) Grad: 181.5761  LR: 0.000005  \n","Epoch: [4][2848/2849] Elapsed 13m 16s (remain 0m 0s) Loss: 0.0143(0.0211) Grad: 11166.8213  LR: 0.000004  \n","EVAL: [0/726] Elapsed 0m 0s (remain 5m 26s) Loss: 0.0189(0.0189) \n","EVAL: [100/726] Elapsed 0m 18s (remain 1m 53s) Loss: 0.0188(0.0208) \n","EVAL: [200/726] Elapsed 0m 36s (remain 1m 34s) Loss: 0.0187(0.0227) \n","EVAL: [300/726] Elapsed 0m 54s (remain 1m 16s) Loss: 0.0188(0.0221) \n","EVAL: [400/726] Elapsed 1m 11s (remain 0m 58s) Loss: 0.0252(0.0228) \n","EVAL: [500/726] Elapsed 1m 29s (remain 0m 40s) Loss: 0.0222(0.0225) \n","EVAL: [600/726] Elapsed 1m 47s (remain 0m 22s) Loss: 0.0393(0.0226) \n","EVAL: [700/726] Elapsed 2m 4s (remain 0m 4s) Loss: 0.0196(0.0229) \n","EVAL: [725/726] Elapsed 2m 9s (remain 0m 0s) Loss: 0.0251(0.0230) \n","Epoch 4 - avg_train_loss: 0.0211  avg_val_loss: 0.0230  time: 931s\n","Epoch 4 - Score: 0.8812\n","Epoch 4 - Save Best Score: 0.8812 Model\n","Epoch: [5][0/2849] Elapsed 0m 0s (remain 31m 2s) Loss: 0.0250(0.0250) Grad: 1140.6007  LR: 0.000004  \n","Epoch: [5][100/2849] Elapsed 0m 34s (remain 15m 28s) Loss: 0.0152(0.0202) Grad: 1043.1741  LR: 0.000004  \n","Epoch: [5][200/2849] Elapsed 1m 4s (remain 14m 9s) Loss: 0.0252(0.0200) Grad: 437.6569  LR: 0.000004  \n","Epoch: [5][300/2849] Elapsed 1m 34s (remain 13m 19s) Loss: 0.0188(0.0204) Grad: 498.1908  LR: 0.000004  \n","Epoch: [5][400/2849] Elapsed 2m 4s (remain 12m 38s) Loss: 0.0169(0.0207) Grad: 1543.0303  LR: 0.000004  \n","Epoch: [5][500/2849] Elapsed 2m 34s (remain 12m 1s) Loss: 0.0239(0.0207) Grad: 24882.9863  LR: 0.000004  \n","Epoch: [5][600/2849] Elapsed 3m 3s (remain 11m 28s) Loss: 0.0379(0.0206) Grad: 19896.5879  LR: 0.000004  \n","Epoch: [5][700/2849] Elapsed 3m 33s (remain 10m 55s) Loss: 0.0185(0.0207) Grad: 1619.9475  LR: 0.000003  \n","Epoch: [5][800/2849] Elapsed 4m 3s (remain 10m 23s) Loss: 0.0126(0.0208) Grad: 1862.6958  LR: 0.000003  \n","Epoch: [5][900/2849] Elapsed 4m 33s (remain 9m 52s) Loss: 0.0243(0.0208) Grad: 103.1565  LR: 0.000003  \n","Epoch: [5][1000/2849] Elapsed 5m 3s (remain 9m 20s) Loss: 0.0239(0.0209) Grad: 3490.3560  LR: 0.000003  \n","Epoch: [5][1100/2849] Elapsed 5m 33s (remain 8m 49s) Loss: 0.0196(0.0209) Grad: 237.3928  LR: 0.000003  \n","Epoch: [5][1200/2849] Elapsed 6m 3s (remain 8m 19s) Loss: 0.0189(0.0209) Grad: 10888.8359  LR: 0.000003  \n","Epoch: [5][1300/2849] Elapsed 6m 33s (remain 7m 48s) Loss: 0.0124(0.0208) Grad: 4295.2603  LR: 0.000002  \n","Epoch: [5][1400/2849] Elapsed 7m 3s (remain 7m 18s) Loss: 0.0347(0.0209) Grad: 16962.7031  LR: 0.000002  \n","Epoch: [5][1500/2849] Elapsed 7m 34s (remain 6m 48s) Loss: 0.0234(0.0209) Grad: 256.2333  LR: 0.000002  \n","Epoch: [5][1600/2849] Elapsed 8m 4s (remain 6m 17s) Loss: 0.0186(0.0209) Grad: 106.0203  LR: 0.000002  \n","Epoch: [5][1700/2849] Elapsed 8m 34s (remain 5m 47s) Loss: 0.0244(0.0209) Grad: 247.0039  LR: 0.000002  \n","Epoch: [5][1800/2849] Elapsed 9m 5s (remain 5m 17s) Loss: 0.0199(0.0209) Grad: 2736.7466  LR: 0.000002  \n","Epoch: [5][1900/2849] Elapsed 9m 35s (remain 4m 46s) Loss: 0.0204(0.0209) Grad: 4313.9697  LR: 0.000001  \n","Epoch: [5][2000/2849] Elapsed 10m 5s (remain 4m 16s) Loss: 0.0186(0.0209) Grad: 1038.2477  LR: 0.000001  \n","Epoch: [5][2100/2849] Elapsed 10m 35s (remain 3m 46s) Loss: 0.0139(0.0209) Grad: 2613.0115  LR: 0.000001  \n","Epoch: [5][2200/2849] Elapsed 11m 5s (remain 3m 15s) Loss: 0.0249(0.0209) Grad: 1206.7224  LR: 0.000001  \n","Epoch: [5][2300/2849] Elapsed 11m 35s (remain 2m 45s) Loss: 0.0181(0.0209) Grad: 107.5358  LR: 0.000001  \n","Epoch: [5][2400/2849] Elapsed 12m 5s (remain 2m 15s) Loss: 0.0246(0.0209) Grad: 229.4389  LR: 0.000001  \n","Epoch: [5][2500/2849] Elapsed 12m 36s (remain 1m 45s) Loss: 0.0339(0.0209) Grad: 181.4746  LR: 0.000001  \n","Epoch: [5][2600/2849] Elapsed 13m 6s (remain 1m 14s) Loss: 0.0234(0.0209) Grad: 1273.8783  LR: 0.000000  \n","Epoch: [5][2700/2849] Elapsed 13m 36s (remain 0m 44s) Loss: 0.0290(0.0209) Grad: 183.3995  LR: 0.000000  \n","Epoch: [5][2800/2849] Elapsed 14m 7s (remain 0m 14s) Loss: 0.0124(0.0209) Grad: 696.3611  LR: 0.000000  \n","Epoch: [5][2848/2849] Elapsed 14m 22s (remain 0m 0s) Loss: 0.0127(0.0209) Grad: 2255.5247  LR: 0.000000  \n","EVAL: [0/726] Elapsed 0m 0s (remain 6m 12s) Loss: 0.0188(0.0188) \n","EVAL: [100/726] Elapsed 0m 20s (remain 2m 8s) Loss: 0.0186(0.0210) \n","EVAL: [200/726] Elapsed 0m 41s (remain 1m 47s) Loss: 0.0187(0.0228) \n","EVAL: [300/726] Elapsed 1m 1s (remain 1m 26s) Loss: 0.0188(0.0222) \n","EVAL: [400/726] Elapsed 1m 21s (remain 1m 6s) Loss: 0.0251(0.0229) \n","EVAL: [500/726] Elapsed 1m 41s (remain 0m 45s) Loss: 0.0230(0.0226) \n","EVAL: [600/726] Elapsed 2m 2s (remain 0m 25s) Loss: 0.0389(0.0226) \n","EVAL: [700/726] Elapsed 2m 22s (remain 0m 5s) Loss: 0.0194(0.0230) \n","EVAL: [725/726] Elapsed 2m 27s (remain 0m 0s) Loss: 0.0251(0.0231) \n","Epoch 5 - avg_train_loss: 0.0209  avg_val_loss: 0.0231  time: 1022s\n","Epoch 5 - Score: 0.8843\n","Epoch 5 - Save Best Score: 0.8843 Model\n","========== fold: 1 training ==========\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Load weight from pretrained\n","Epoch: [1][0/2851] Elapsed 0m 0s (remain 36m 54s) Loss: 0.1051(0.1051) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/2851] Elapsed 0m 33s (remain 15m 0s) Loss: 0.0421(0.1335) Grad: 9109.6553  LR: 0.000001  \n","Epoch: [1][200/2851] Elapsed 1m 0s (remain 13m 21s) Loss: 0.0275(0.0848) Grad: 1202.7384  LR: 0.000003  \n","Epoch: [1][300/2851] Elapsed 1m 28s (remain 12m 29s) Loss: 0.0276(0.0661) Grad: 1986.4764  LR: 0.000004  \n","Epoch: [1][400/2851] Elapsed 1m 56s (remain 11m 50s) Loss: 0.0243(0.0564) Grad: 2278.2942  LR: 0.000006  \n","Epoch: [1][500/2851] Elapsed 2m 24s (remain 11m 16s) Loss: 0.0205(0.0500) Grad: 525.4034  LR: 0.000007  \n","Epoch: [1][600/2851] Elapsed 2m 52s (remain 10m 46s) Loss: 0.0311(0.0459) Grad: 2644.0701  LR: 0.000008  \n","Epoch: [1][700/2851] Elapsed 3m 20s (remain 10m 15s) Loss: 0.0214(0.0428) Grad: 901.5607  LR: 0.000010  \n","Epoch: [1][800/2851] Elapsed 3m 48s (remain 9m 44s) Loss: 0.0338(0.0404) Grad: 1711.4282  LR: 0.000011  \n","Epoch: [1][900/2851] Elapsed 4m 16s (remain 9m 14s) Loss: 0.0194(0.0386) Grad: 703.2010  LR: 0.000013  \n","Epoch: [1][1000/2851] Elapsed 4m 43s (remain 8m 44s) Loss: 0.0214(0.0371) Grad: 809.4539  LR: 0.000014  \n","Epoch: [1][1100/2851] Elapsed 5m 11s (remain 8m 15s) Loss: 0.0188(0.0358) Grad: 291.0022  LR: 0.000015  \n","Epoch: [1][1200/2851] Elapsed 5m 39s (remain 7m 46s) Loss: 0.0273(0.0348) Grad: 1156.1783  LR: 0.000017  \n","Epoch: [1][1300/2851] Elapsed 6m 7s (remain 7m 17s) Loss: 0.0146(0.0338) Grad: 772.2358  LR: 0.000018  \n","Epoch: [1][1400/2851] Elapsed 6m 35s (remain 6m 49s) Loss: 0.0278(0.0330) Grad: 1165.3605  LR: 0.000020  \n","Epoch: [1][1500/2851] Elapsed 7m 3s (remain 6m 20s) Loss: 0.0253(0.0325) Grad: 526.5988  LR: 0.000020  \n","Epoch: [1][1600/2851] Elapsed 7m 31s (remain 5m 52s) Loss: 0.0350(0.0319) Grad: 5116.2246  LR: 0.000020  \n","Epoch: [1][1700/2851] Elapsed 7m 58s (remain 5m 23s) Loss: 0.0269(0.0313) Grad: 1366.8282  LR: 0.000020  \n","Epoch: [1][1800/2851] Elapsed 8m 26s (remain 4m 55s) Loss: 0.0369(0.0309) Grad: 1740.7506  LR: 0.000019  \n","Epoch: [1][1900/2851] Elapsed 8m 54s (remain 4m 27s) Loss: 0.0356(0.0304) Grad: 1771.7649  LR: 0.000019  \n","Epoch: [1][2000/2851] Elapsed 9m 22s (remain 3m 58s) Loss: 0.0174(0.0300) Grad: 1281.6713  LR: 0.000019  \n","Epoch: [1][2100/2851] Elapsed 9m 50s (remain 3m 30s) Loss: 0.0242(0.0297) Grad: 985.1294  LR: 0.000019  \n","Epoch: [1][2200/2851] Elapsed 10m 18s (remain 3m 2s) Loss: 0.0228(0.0294) Grad: 2180.4836  LR: 0.000019  \n","Epoch: [1][2300/2851] Elapsed 10m 46s (remain 2m 34s) Loss: 0.0196(0.0291) Grad: 166.4163  LR: 0.000019  \n","Epoch: [1][2400/2851] Elapsed 11m 14s (remain 2m 6s) Loss: 0.0191(0.0287) Grad: 75.7420  LR: 0.000018  \n","Epoch: [1][2500/2851] Elapsed 11m 41s (remain 1m 38s) Loss: 0.0291(0.0285) Grad: 1088.5492  LR: 0.000018  \n","Epoch: [1][2600/2851] Elapsed 12m 9s (remain 1m 10s) Loss: 0.0185(0.0282) Grad: 567.6516  LR: 0.000018  \n","Epoch: [1][2700/2851] Elapsed 12m 37s (remain 0m 42s) Loss: 0.0131(0.0280) Grad: 348.7737  LR: 0.000018  \n","Epoch: [1][2800/2851] Elapsed 13m 5s (remain 0m 14s) Loss: 0.0126(0.0278) Grad: 96.3030  LR: 0.000018  \n","Epoch: [1][2850/2851] Elapsed 13m 19s (remain 0m 0s) Loss: 0.0380(0.0276) Grad: 1065.1959  LR: 0.000018  \n","EVAL: [0/724] Elapsed 0m 0s (remain 6m 7s) Loss: 0.0192(0.0192) \n","EVAL: [100/724] Elapsed 0m 18s (remain 1m 52s) Loss: 0.0262(0.0202) \n","EVAL: [200/724] Elapsed 0m 36s (remain 1m 34s) Loss: 0.0314(0.0223) \n","EVAL: [300/724] Elapsed 0m 54s (remain 1m 15s) Loss: 0.0270(0.0221) \n","EVAL: [400/724] Elapsed 1m 11s (remain 0m 57s) Loss: 0.0377(0.0222) \n","EVAL: [500/724] Elapsed 1m 29s (remain 0m 39s) Loss: 0.0167(0.0221) \n","EVAL: [600/724] Elapsed 1m 47s (remain 0m 21s) Loss: 0.0256(0.0220) \n","EVAL: [700/724] Elapsed 2m 4s (remain 0m 4s) Loss: 0.0314(0.0223) \n","EVAL: [723/724] Elapsed 2m 8s (remain 0m 0s) Loss: 0.0127(0.0223) \n","Epoch 1 - avg_train_loss: 0.0276  avg_val_loss: 0.0223  time: 932s\n","Epoch 1 - Score: 0.8469\n","Epoch 1 - Save Best Score: 0.8469 Model\n","Epoch: [2][0/2851] Elapsed 0m 0s (remain 32m 45s) Loss: 0.0256(0.0256) Grad: 3034.0159  LR: 0.000018  \n","Epoch: [2][100/2851] Elapsed 0m 30s (remain 13m 53s) Loss: 0.0196(0.0214) Grad: 1397.2349  LR: 0.000018  \n","Epoch: [2][200/2851] Elapsed 1m 0s (remain 13m 12s) Loss: 0.0135(0.0214) Grad: 4775.7524  LR: 0.000017  \n","Epoch: [2][300/2851] Elapsed 1m 28s (remain 12m 27s) Loss: 0.0203(0.0213) Grad: 6723.9761  LR: 0.000017  \n","Epoch: [2][400/2851] Elapsed 1m 56s (remain 11m 49s) Loss: 0.0257(0.0212) Grad: 1980.5802  LR: 0.000017  \n","Epoch: [2][500/2851] Elapsed 2m 23s (remain 11m 14s) Loss: 0.0260(0.0212) Grad: 4766.9746  LR: 0.000017  \n","Epoch: [2][600/2851] Elapsed 2m 51s (remain 10m 43s) Loss: 0.0269(0.0211) Grad: 4318.3594  LR: 0.000017  \n","Epoch: [2][700/2851] Elapsed 3m 19s (remain 10m 12s) Loss: 0.0295(0.0212) Grad: 32335.5879  LR: 0.000017  \n","Epoch: [2][800/2851] Elapsed 3m 47s (remain 9m 42s) Loss: 0.0191(0.0212) Grad: 3673.4331  LR: 0.000017  \n","Epoch: [2][900/2851] Elapsed 4m 15s (remain 9m 12s) Loss: 0.0180(0.0212) Grad: 512.8903  LR: 0.000016  \n","Epoch: [2][1000/2851] Elapsed 4m 43s (remain 8m 43s) Loss: 0.0190(0.0213) Grad: 2985.1987  LR: 0.000016  \n","Epoch: [2][1100/2851] Elapsed 5m 11s (remain 8m 15s) Loss: 0.0356(0.0214) Grad: 30130.1484  LR: 0.000016  \n","Epoch: [2][1200/2851] Elapsed 5m 39s (remain 7m 46s) Loss: 0.0317(0.0213) Grad: 1085.3794  LR: 0.000016  \n","Epoch: [2][1300/2851] Elapsed 6m 7s (remain 7m 17s) Loss: 0.0155(0.0214) Grad: 3144.7483  LR: 0.000016  \n","Epoch: [2][1400/2851] Elapsed 6m 35s (remain 6m 49s) Loss: 0.0250(0.0214) Grad: 1336.1636  LR: 0.000016  \n","Epoch: [2][1500/2851] Elapsed 7m 3s (remain 6m 21s) Loss: 0.0189(0.0214) Grad: 1561.9651  LR: 0.000015  \n","Epoch: [2][1600/2851] Elapsed 7m 32s (remain 5m 53s) Loss: 0.0256(0.0215) Grad: 5112.6191  LR: 0.000015  \n","Epoch: [2][1700/2851] Elapsed 8m 0s (remain 5m 24s) Loss: 0.0197(0.0215) Grad: 2053.7891  LR: 0.000015  \n","Epoch: [2][1800/2851] Elapsed 8m 28s (remain 4m 56s) Loss: 0.0323(0.0214) Grad: 1410.7487  LR: 0.000015  \n","Epoch: [2][1900/2851] Elapsed 8m 56s (remain 4m 28s) Loss: 0.0244(0.0215) Grad: 2091.2878  LR: 0.000015  \n","Epoch: [2][2000/2851] Elapsed 9m 24s (remain 3m 59s) Loss: 0.0259(0.0215) Grad: 965.7496  LR: 0.000015  \n","Epoch: [2][2100/2851] Elapsed 9m 52s (remain 3m 31s) Loss: 0.0317(0.0215) Grad: 1357.9292  LR: 0.000015  \n","Epoch: [2][2200/2851] Elapsed 10m 20s (remain 3m 3s) Loss: 0.0340(0.0215) Grad: 1285.2552  LR: 0.000014  \n","Epoch: [2][2300/2851] Elapsed 10m 48s (remain 2m 34s) Loss: 0.0338(0.0215) Grad: 9707.7305  LR: 0.000014  \n","Epoch: [2][2400/2851] Elapsed 11m 16s (remain 2m 6s) Loss: 0.0325(0.0215) Grad: 95.7714  LR: 0.000014  \n","Epoch: [2][2500/2851] Elapsed 11m 44s (remain 1m 38s) Loss: 0.0341(0.0215) Grad: 38571.0078  LR: 0.000014  \n","Epoch: [2][2600/2851] Elapsed 12m 12s (remain 1m 10s) Loss: 0.0250(0.0216) Grad: 929.4634  LR: 0.000014  \n","Epoch: [2][2700/2851] Elapsed 12m 39s (remain 0m 42s) Loss: 0.0344(0.0216) Grad: 17770.1016  LR: 0.000014  \n","Epoch: [2][2800/2851] Elapsed 13m 7s (remain 0m 14s) Loss: 0.0263(0.0216) Grad: 3377.0276  LR: 0.000013  \n","Epoch: [2][2850/2851] Elapsed 13m 21s (remain 0m 0s) Loss: 0.0309(0.0216) Grad: 2328.3105  LR: 0.000013  \n","EVAL: [0/724] Elapsed 0m 0s (remain 5m 54s) Loss: 0.0189(0.0189) \n","EVAL: [100/724] Elapsed 0m 18s (remain 1m 53s) Loss: 0.0259(0.0199) \n","EVAL: [200/724] Elapsed 0m 36s (remain 1m 34s) Loss: 0.0315(0.0222) \n","EVAL: [300/724] Elapsed 0m 54s (remain 1m 15s) Loss: 0.0257(0.0221) \n","EVAL: [400/724] Elapsed 1m 11s (remain 0m 57s) Loss: 0.0376(0.0221) \n","EVAL: [500/724] Elapsed 1m 29s (remain 0m 39s) Loss: 0.0158(0.0220) \n","EVAL: [600/724] Elapsed 1m 47s (remain 0m 21s) Loss: 0.0254(0.0219) \n","EVAL: [700/724] Elapsed 2m 4s (remain 0m 4s) Loss: 0.0313(0.0222) \n","EVAL: [723/724] Elapsed 2m 8s (remain 0m 0s) Loss: 0.0125(0.0223) \n","Epoch 2 - avg_train_loss: 0.0216  avg_val_loss: 0.0223  time: 935s\n","Epoch 2 - Score: 0.8666\n","Epoch 2 - Save Best Score: 0.8666 Model\n","Epoch: [3][0/2851] Elapsed 0m 0s (remain 28m 21s) Loss: 0.0287(0.0287) Grad: 5526.4131  LR: 0.000013  \n","Epoch: [3][100/2851] Elapsed 0m 31s (remain 14m 8s) Loss: 0.0137(0.0211) Grad: 4606.0215  LR: 0.000013  \n","Epoch: [3][200/2851] Elapsed 1m 0s (remain 13m 12s) Loss: 0.0237(0.0211) Grad: 1237.2145  LR: 0.000013  \n","Epoch: [3][300/2851] Elapsed 1m 27s (remain 12m 24s) Loss: 0.0131(0.0213) Grad: 1723.2008  LR: 0.000013  \n","Epoch: [3][400/2851] Elapsed 1m 55s (remain 11m 46s) Loss: 0.0224(0.0212) Grad: 10311.3379  LR: 0.000013  \n","Epoch: [3][500/2851] Elapsed 2m 23s (remain 11m 12s) Loss: 0.0255(0.0211) Grad: 17529.3711  LR: 0.000013  \n","Epoch: [3][600/2851] Elapsed 2m 51s (remain 10m 42s) Loss: 0.0134(0.0210) Grad: 2924.2998  LR: 0.000012  \n","Epoch: [3][700/2851] Elapsed 3m 19s (remain 10m 11s) Loss: 0.0190(0.0210) Grad: 2239.4570  LR: 0.000012  \n","Epoch: [3][800/2851] Elapsed 3m 47s (remain 9m 40s) Loss: 0.0196(0.0210) Grad: 7035.2466  LR: 0.000012  \n","Epoch: [3][900/2851] Elapsed 4m 14s (remain 9m 11s) Loss: 0.0188(0.0210) Grad: 470.6994  LR: 0.000012  \n","Epoch: [3][1000/2851] Elapsed 4m 42s (remain 8m 41s) Loss: 0.0235(0.0211) Grad: 7657.6509  LR: 0.000012  \n","Epoch: [3][1100/2851] Elapsed 5m 10s (remain 8m 13s) Loss: 0.0268(0.0212) Grad: 2810.1799  LR: 0.000012  \n","Epoch: [3][1200/2851] Elapsed 5m 38s (remain 7m 44s) Loss: 0.0189(0.0211) Grad: 1020.7843  LR: 0.000011  \n","Epoch: [3][1300/2851] Elapsed 6m 5s (remain 7m 15s) Loss: 0.0182(0.0212) Grad: 623.6478  LR: 0.000011  \n","Epoch: [3][1400/2851] Elapsed 6m 33s (remain 6m 47s) Loss: 0.0325(0.0212) Grad: 8304.5488  LR: 0.000011  \n","Epoch: [3][1500/2851] Elapsed 7m 1s (remain 6m 18s) Loss: 0.0193(0.0213) Grad: 1272.7599  LR: 0.000011  \n","Epoch: [3][1600/2851] Elapsed 7m 29s (remain 5m 50s) Loss: 0.0181(0.0213) Grad: 1043.2288  LR: 0.000011  \n","Epoch: [3][1700/2851] Elapsed 7m 56s (remain 5m 22s) Loss: 0.0199(0.0213) Grad: 32531.7617  LR: 0.000011  \n","Epoch: [3][1800/2851] Elapsed 8m 24s (remain 4m 54s) Loss: 0.0124(0.0214) Grad: 604.9328  LR: 0.000011  \n","Epoch: [3][1900/2851] Elapsed 8m 52s (remain 4m 26s) Loss: 0.0140(0.0213) Grad: 10075.1562  LR: 0.000010  \n","Epoch: [3][2000/2851] Elapsed 9m 20s (remain 3m 58s) Loss: 0.0176(0.0213) Grad: 821.8132  LR: 0.000010  \n","Epoch: [3][2100/2851] Elapsed 9m 47s (remain 3m 29s) Loss: 0.0125(0.0213) Grad: 1804.4619  LR: 0.000010  \n","Epoch: [3][2200/2851] Elapsed 10m 15s (remain 3m 1s) Loss: 0.0128(0.0212) Grad: 1198.4039  LR: 0.000010  \n","Epoch: [3][2300/2851] Elapsed 10m 43s (remain 2m 33s) Loss: 0.0205(0.0212) Grad: 1297.9683  LR: 0.000010  \n","Epoch: [3][2400/2851] Elapsed 11m 11s (remain 2m 5s) Loss: 0.0186(0.0212) Grad: 1179.0317  LR: 0.000010  \n","Epoch: [3][2500/2851] Elapsed 11m 38s (remain 1m 37s) Loss: 0.0124(0.0212) Grad: 513.2402  LR: 0.000009  \n","Epoch: [3][2600/2851] Elapsed 12m 6s (remain 1m 9s) Loss: 0.0177(0.0212) Grad: 793.4832  LR: 0.000009  \n","Epoch: [3][2700/2851] Elapsed 12m 33s (remain 0m 41s) Loss: 0.0203(0.0213) Grad: 7067.9609  LR: 0.000009  \n","Epoch: [3][2800/2851] Elapsed 13m 1s (remain 0m 13s) Loss: 0.0324(0.0213) Grad: 834.4851  LR: 0.000009  \n","Epoch: [3][2850/2851] Elapsed 13m 15s (remain 0m 0s) Loss: 0.0160(0.0213) Grad: 494.5463  LR: 0.000009  \n","EVAL: [0/724] Elapsed 0m 0s (remain 5m 44s) Loss: 0.0189(0.0189) \n","EVAL: [100/724] Elapsed 0m 18s (remain 1m 52s) Loss: 0.0258(0.0209) \n","EVAL: [200/724] Elapsed 0m 36s (remain 1m 34s) Loss: 0.0313(0.0229) \n","EVAL: [300/724] Elapsed 0m 53s (remain 1m 15s) Loss: 0.0298(0.0226) \n","EVAL: [400/724] Elapsed 1m 11s (remain 0m 57s) Loss: 0.0376(0.0225) \n","EVAL: [500/724] Elapsed 1m 29s (remain 0m 39s) Loss: 0.0178(0.0226) \n","EVAL: [600/724] Elapsed 1m 47s (remain 0m 21s) Loss: 0.0251(0.0224) \n","EVAL: [700/724] Elapsed 2m 4s (remain 0m 4s) Loss: 0.0313(0.0226) \n","EVAL: [723/724] Elapsed 2m 8s (remain 0m 0s) Loss: 0.0124(0.0227) \n","Epoch 3 - avg_train_loss: 0.0213  avg_val_loss: 0.0227  time: 929s\n","Epoch 3 - Score: 0.8809\n","Epoch 3 - Save Best Score: 0.8809 Model\n","Epoch: [4][0/2851] Elapsed 0m 0s (remain 29m 44s) Loss: 0.0182(0.0182) Grad: 834.7006  LR: 0.000009  \n","Epoch: [4][100/2851] Elapsed 0m 31s (remain 14m 14s) Loss: 0.0129(0.0201) Grad: 3085.4456  LR: 0.000009  \n","Epoch: [4][200/2851] Elapsed 1m 0s (remain 13m 11s) Loss: 0.0197(0.0202) Grad: 2225.2305  LR: 0.000009  \n","Epoch: [4][300/2851] Elapsed 1m 28s (remain 12m 26s) Loss: 0.0124(0.0206) Grad: 658.1190  LR: 0.000008  \n","Epoch: [4][400/2851] Elapsed 1m 55s (remain 11m 47s) Loss: 0.0163(0.0206) Grad: 15147.8506  LR: 0.000008  \n","Epoch: [4][500/2851] Elapsed 2m 23s (remain 11m 13s) Loss: 0.0183(0.0206) Grad: 367.8715  LR: 0.000008  \n","Epoch: [4][600/2851] Elapsed 2m 51s (remain 10m 40s) Loss: 0.0196(0.0208) Grad: 715.4833  LR: 0.000008  \n","Epoch: [4][700/2851] Elapsed 3m 18s (remain 10m 10s) Loss: 0.0124(0.0207) Grad: 633.3775  LR: 0.000008  \n","Epoch: [4][800/2851] Elapsed 3m 46s (remain 9m 40s) Loss: 0.0240(0.0207) Grad: 1330.6108  LR: 0.000008  \n","Epoch: [4][900/2851] Elapsed 4m 14s (remain 9m 10s) Loss: 0.0222(0.0208) Grad: 666.9758  LR: 0.000007  \n","Epoch: [4][1000/2851] Elapsed 4m 42s (remain 8m 41s) Loss: 0.0125(0.0207) Grad: 663.0251  LR: 0.000007  \n","Epoch: [4][1100/2851] Elapsed 5m 9s (remain 8m 12s) Loss: 0.0203(0.0207) Grad: 2269.7114  LR: 0.000007  \n","Epoch: [4][1200/2851] Elapsed 5m 37s (remain 7m 43s) Loss: 0.0200(0.0208) Grad: 1488.1017  LR: 0.000007  \n","Epoch: [4][1300/2851] Elapsed 6m 5s (remain 7m 15s) Loss: 0.0139(0.0209) Grad: 11086.6211  LR: 0.000007  \n","Epoch: [4][1400/2851] Elapsed 6m 33s (remain 6m 46s) Loss: 0.0124(0.0209) Grad: 411.8271  LR: 0.000007  \n","Epoch: [4][1500/2851] Elapsed 7m 0s (remain 6m 18s) Loss: 0.0180(0.0209) Grad: 1023.7650  LR: 0.000007  \n","Epoch: [4][1600/2851] Elapsed 7m 28s (remain 5m 50s) Loss: 0.0170(0.0209) Grad: 15816.5791  LR: 0.000006  \n","Epoch: [4][1700/2851] Elapsed 7m 56s (remain 5m 21s) Loss: 0.0123(0.0209) Grad: 157.8630  LR: 0.000006  \n","Epoch: [4][1800/2851] Elapsed 8m 23s (remain 4m 53s) Loss: 0.0247(0.0210) Grad: 454.2916  LR: 0.000006  \n","Epoch: [4][1900/2851] Elapsed 8m 51s (remain 4m 25s) Loss: 0.0236(0.0210) Grad: 807.4488  LR: 0.000006  \n","Epoch: [4][2000/2851] Elapsed 9m 19s (remain 3m 57s) Loss: 0.0179(0.0210) Grad: 16095.9531  LR: 0.000006  \n","Epoch: [4][2100/2851] Elapsed 9m 46s (remain 3m 29s) Loss: 0.0124(0.0210) Grad: 704.1361  LR: 0.000006  \n","Epoch: [4][2200/2851] Elapsed 10m 14s (remain 3m 1s) Loss: 0.0250(0.0210) Grad: 1087.8486  LR: 0.000005  \n","Epoch: [4][2300/2851] Elapsed 10m 42s (remain 2m 33s) Loss: 0.0254(0.0210) Grad: 319.9258  LR: 0.000005  \n","Epoch: [4][2400/2851] Elapsed 11m 10s (remain 2m 5s) Loss: 0.0277(0.0210) Grad: 34423.1016  LR: 0.000005  \n","Epoch: [4][2500/2851] Elapsed 11m 38s (remain 1m 37s) Loss: 0.0194(0.0210) Grad: 1446.7539  LR: 0.000005  \n","Epoch: [4][2600/2851] Elapsed 12m 5s (remain 1m 9s) Loss: 0.0175(0.0210) Grad: 69500.6250  LR: 0.000005  \n","Epoch: [4][2700/2851] Elapsed 12m 33s (remain 0m 41s) Loss: 0.0257(0.0210) Grad: 548.0133  LR: 0.000005  \n","Epoch: [4][2800/2851] Elapsed 13m 1s (remain 0m 13s) Loss: 0.0247(0.0210) Grad: 107.1064  LR: 0.000005  \n","Epoch: [4][2850/2851] Elapsed 13m 14s (remain 0m 0s) Loss: 0.0188(0.0210) Grad: 140.4122  LR: 0.000004  \n","EVAL: [0/724] Elapsed 0m 0s (remain 6m 5s) Loss: 0.0188(0.0188) \n","EVAL: [100/724] Elapsed 0m 18s (remain 1m 53s) Loss: 0.0260(0.0209) \n","EVAL: [200/724] Elapsed 0m 36s (remain 1m 34s) Loss: 0.0313(0.0227) \n","EVAL: [300/724] Elapsed 0m 53s (remain 1m 15s) Loss: 0.0309(0.0226) \n","EVAL: [400/724] Elapsed 1m 11s (remain 0m 57s) Loss: 0.0376(0.0226) \n","EVAL: [500/724] Elapsed 1m 29s (remain 0m 39s) Loss: 0.0171(0.0226) \n","EVAL: [600/724] Elapsed 1m 47s (remain 0m 21s) Loss: 0.0252(0.0225) \n","EVAL: [700/724] Elapsed 2m 4s (remain 0m 4s) Loss: 0.0313(0.0226) \n","EVAL: [723/724] Elapsed 2m 8s (remain 0m 0s) Loss: 0.0124(0.0227) \n","Epoch 4 - avg_train_loss: 0.0210  avg_val_loss: 0.0227  time: 928s\n","Epoch 4 - Score: 0.8843\n","Epoch 4 - Save Best Score: 0.8843 Model\n","Epoch: [5][0/2851] Elapsed 0m 0s (remain 28m 27s) Loss: 0.0251(0.0251) Grad: 9692.4141  LR: 0.000004  \n","Epoch: [5][100/2851] Elapsed 0m 31s (remain 14m 11s) Loss: 0.0246(0.0215) Grad: 2919.3369  LR: 0.000004  \n","Epoch: [5][200/2851] Elapsed 0m 59s (remain 13m 10s) Loss: 0.0310(0.0210) Grad: 191.0462  LR: 0.000004  \n","Epoch: [5][300/2851] Elapsed 1m 27s (remain 12m 22s) Loss: 0.0123(0.0208) Grad: 342.2117  LR: 0.000004  \n","Epoch: [5][400/2851] Elapsed 1m 55s (remain 11m 44s) Loss: 0.0183(0.0208) Grad: 2878.4585  LR: 0.000004  \n","Epoch: [5][500/2851] Elapsed 2m 22s (remain 11m 10s) Loss: 0.0124(0.0209) Grad: 206.9879  LR: 0.000004  \n","Epoch: [5][600/2851] Elapsed 2m 50s (remain 10m 39s) Loss: 0.0275(0.0209) Grad: 9079.4873  LR: 0.000004  \n","Epoch: [5][700/2851] Elapsed 3m 18s (remain 10m 8s) Loss: 0.0196(0.0207) Grad: 358.1921  LR: 0.000003  \n","Epoch: [5][800/2851] Elapsed 3m 46s (remain 9m 39s) Loss: 0.0173(0.0207) Grad: 2316.8652  LR: 0.000003  \n","Epoch: [5][900/2851] Elapsed 4m 14s (remain 9m 9s) Loss: 0.0259(0.0207) Grad: 2649.7534  LR: 0.000003  \n","Epoch: [5][1000/2851] Elapsed 4m 41s (remain 8m 40s) Loss: 0.0257(0.0207) Grad: 3514.6497  LR: 0.000003  \n","Epoch: [5][1100/2851] Elapsed 5m 9s (remain 8m 11s) Loss: 0.0158(0.0208) Grad: 4265.4380  LR: 0.000003  \n","Epoch: [5][1200/2851] Elapsed 5m 37s (remain 7m 43s) Loss: 0.0309(0.0207) Grad: 590.9658  LR: 0.000003  \n","Epoch: [5][1300/2851] Elapsed 6m 4s (remain 7m 14s) Loss: 0.0183(0.0207) Grad: 254.9861  LR: 0.000002  \n","Epoch: [5][1400/2851] Elapsed 6m 32s (remain 6m 46s) Loss: 0.0189(0.0208) Grad: 1197.8540  LR: 0.000002  \n","Epoch: [5][1500/2851] Elapsed 7m 0s (remain 6m 17s) Loss: 0.0125(0.0207) Grad: 1349.6338  LR: 0.000002  \n","Epoch: [5][1600/2851] Elapsed 7m 27s (remain 5m 49s) Loss: 0.0260(0.0208) Grad: 411.7198  LR: 0.000002  \n","Epoch: [5][1700/2851] Elapsed 7m 55s (remain 5m 21s) Loss: 0.0175(0.0208) Grad: 591.0668  LR: 0.000002  \n","Epoch: [5][1800/2851] Elapsed 8m 23s (remain 4m 53s) Loss: 0.0123(0.0207) Grad: 247.7213  LR: 0.000002  \n","Epoch: [5][1900/2851] Elapsed 8m 50s (remain 4m 25s) Loss: 0.0287(0.0208) Grad: 5243.3389  LR: 0.000001  \n","Epoch: [5][2000/2851] Elapsed 9m 18s (remain 3m 57s) Loss: 0.0306(0.0208) Grad: 210.8921  LR: 0.000001  \n","Epoch: [5][2100/2851] Elapsed 9m 46s (remain 3m 29s) Loss: 0.0193(0.0208) Grad: 87.7022  LR: 0.000001  \n","Epoch: [5][2200/2851] Elapsed 10m 14s (remain 3m 1s) Loss: 0.0185(0.0208) Grad: 5610.3159  LR: 0.000001  \n","Epoch: [5][2300/2851] Elapsed 10m 41s (remain 2m 33s) Loss: 0.0196(0.0208) Grad: 258.4083  LR: 0.000001  \n","Epoch: [5][2400/2851] Elapsed 11m 9s (remain 2m 5s) Loss: 0.0124(0.0208) Grad: 570.3271  LR: 0.000001  \n","Epoch: [5][2500/2851] Elapsed 11m 37s (remain 1m 37s) Loss: 0.0194(0.0208) Grad: 146.2972  LR: 0.000001  \n","Epoch: [5][2600/2851] Elapsed 12m 4s (remain 1m 9s) Loss: 0.0183(0.0208) Grad: 961.4147  LR: 0.000000  \n","Epoch: [5][2700/2851] Elapsed 12m 32s (remain 0m 41s) Loss: 0.0263(0.0209) Grad: 16937.7188  LR: 0.000000  \n","Epoch: [5][2800/2851] Elapsed 13m 0s (remain 0m 13s) Loss: 0.0177(0.0209) Grad: 73.0578  LR: 0.000000  \n","Epoch: [5][2850/2851] Elapsed 13m 14s (remain 0m 0s) Loss: 0.0416(0.0208) Grad: 100664.2812  LR: 0.000000  \n","EVAL: [0/724] Elapsed 0m 0s (remain 5m 53s) Loss: 0.0188(0.0188) \n","EVAL: [100/724] Elapsed 0m 18s (remain 1m 52s) Loss: 0.0258(0.0212) \n","EVAL: [200/724] Elapsed 0m 36s (remain 1m 34s) Loss: 0.0313(0.0230) \n","EVAL: [300/724] Elapsed 0m 53s (remain 1m 15s) Loss: 0.0310(0.0228) \n","EVAL: [400/724] Elapsed 1m 11s (remain 0m 57s) Loss: 0.0376(0.0228) \n","EVAL: [500/724] Elapsed 1m 29s (remain 0m 39s) Loss: 0.0179(0.0229) \n","EVAL: [600/724] Elapsed 1m 46s (remain 0m 21s) Loss: 0.0263(0.0227) \n","EVAL: [700/724] Elapsed 2m 4s (remain 0m 4s) Loss: 0.0313(0.0229) \n","EVAL: [723/724] Elapsed 2m 8s (remain 0m 0s) Loss: 0.0124(0.0229) \n","Epoch 5 - avg_train_loss: 0.0208  avg_val_loss: 0.0229  time: 927s\n","Epoch 5 - Score: 0.8824\n","========== fold: 2 training ==========\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Load weight from pretrained\n","Epoch: [1][0/2871] Elapsed 0m 0s (remain 33m 21s) Loss: 0.1510(0.1510) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/2871] Elapsed 0m 28s (remain 12m 58s) Loss: 0.0609(0.0922) Grad: 13806.7686  LR: 0.000001  \n","Epoch: [1][200/2871] Elapsed 0m 55s (remain 12m 23s) Loss: 0.0269(0.0632) Grad: 780.7722  LR: 0.000003  \n","Epoch: [1][300/2871] Elapsed 1m 23s (remain 11m 53s) Loss: 0.0224(0.0520) Grad: 3628.4968  LR: 0.000004  \n","Epoch: [1][400/2871] Elapsed 1m 51s (remain 11m 24s) Loss: 0.0298(0.0458) Grad: 1083.8164  LR: 0.000006  \n","Epoch: [1][500/2871] Elapsed 2m 18s (remain 10m 56s) Loss: 0.0192(0.0417) Grad: 1701.9471  LR: 0.000007  \n","Epoch: [1][600/2871] Elapsed 2m 46s (remain 10m 30s) Loss: 0.0154(0.0390) Grad: 1373.6332  LR: 0.000008  \n","Epoch: [1][700/2871] Elapsed 3m 14s (remain 10m 1s) Loss: 0.0289(0.0370) Grad: 1377.9957  LR: 0.000010  \n","Epoch: [1][800/2871] Elapsed 3m 41s (remain 9m 33s) Loss: 0.0207(0.0354) Grad: 6451.2051  LR: 0.000011  \n","Epoch: [1][900/2871] Elapsed 4m 9s (remain 9m 5s) Loss: 0.0218(0.0341) Grad: 1545.3400  LR: 0.000013  \n","Epoch: [1][1000/2871] Elapsed 4m 36s (remain 8m 37s) Loss: 0.0242(0.0331) Grad: 965.7576  LR: 0.000014  \n","Epoch: [1][1100/2871] Elapsed 5m 4s (remain 8m 9s) Loss: 0.0280(0.0323) Grad: 2392.0806  LR: 0.000015  \n","Epoch: [1][1200/2871] Elapsed 5m 31s (remain 7m 41s) Loss: 0.0132(0.0315) Grad: 512.8243  LR: 0.000017  \n","Epoch: [1][1300/2871] Elapsed 5m 59s (remain 7m 13s) Loss: 0.0215(0.0309) Grad: 506.9412  LR: 0.000018  \n","Epoch: [1][1400/2871] Elapsed 6m 27s (remain 6m 46s) Loss: 0.0212(0.0304) Grad: 2302.8792  LR: 0.000020  \n","Epoch: [1][1500/2871] Elapsed 6m 54s (remain 6m 18s) Loss: 0.0421(0.0300) Grad: 5862.8711  LR: 0.000020  \n","Epoch: [1][1600/2871] Elapsed 7m 22s (remain 5m 50s) Loss: 0.0188(0.0295) Grad: 796.3850  LR: 0.000020  \n","Epoch: [1][1700/2871] Elapsed 7m 50s (remain 5m 23s) Loss: 0.0494(0.0291) Grad: 5591.2393  LR: 0.000020  \n","Epoch: [1][1800/2871] Elapsed 8m 17s (remain 4m 55s) Loss: 0.0228(0.0288) Grad: 899.7277  LR: 0.000019  \n","Epoch: [1][1900/2871] Elapsed 8m 45s (remain 4m 28s) Loss: 0.0210(0.0285) Grad: 236.1297  LR: 0.000019  \n","Epoch: [1][2000/2871] Elapsed 9m 12s (remain 4m 0s) Loss: 0.0218(0.0282) Grad: 1076.0105  LR: 0.000019  \n","Epoch: [1][2100/2871] Elapsed 9m 40s (remain 3m 32s) Loss: 0.0186(0.0279) Grad: 491.8992  LR: 0.000019  \n","Epoch: [1][2200/2871] Elapsed 10m 8s (remain 3m 5s) Loss: 0.0220(0.0276) Grad: 413.0864  LR: 0.000019  \n","Epoch: [1][2300/2871] Elapsed 10m 36s (remain 2m 37s) Loss: 0.0362(0.0274) Grad: 1866.0166  LR: 0.000019  \n","Epoch: [1][2400/2871] Elapsed 11m 3s (remain 2m 9s) Loss: 0.0169(0.0272) Grad: 581.9025  LR: 0.000019  \n","Epoch: [1][2500/2871] Elapsed 11m 31s (remain 1m 42s) Loss: 0.0190(0.0270) Grad: 91.5117  LR: 0.000018  \n","Epoch: [1][2600/2871] Elapsed 11m 59s (remain 1m 14s) Loss: 0.0190(0.0268) Grad: 198.7610  LR: 0.000018  \n","Epoch: [1][2700/2871] Elapsed 12m 26s (remain 0m 47s) Loss: 0.0196(0.0266) Grad: 191.2095  LR: 0.000018  \n","Epoch: [1][2800/2871] Elapsed 12m 54s (remain 0m 19s) Loss: 0.0212(0.0265) Grad: 681.4915  LR: 0.000018  \n","Epoch: [1][2870/2871] Elapsed 13m 13s (remain 0m 0s) Loss: 0.0306(0.0263) Grad: 2200.9429  LR: 0.000018  \n","EVAL: [0/704] Elapsed 0m 0s (remain 6m 36s) Loss: 0.0130(0.0130) \n","EVAL: [100/704] Elapsed 0m 18s (remain 1m 49s) Loss: 0.0147(0.0204) \n","EVAL: [200/704] Elapsed 0m 35s (remain 1m 30s) Loss: 0.0313(0.0221) \n","EVAL: [300/704] Elapsed 0m 53s (remain 1m 11s) Loss: 0.0186(0.0216) \n","EVAL: [400/704] Elapsed 1m 11s (remain 0m 53s) Loss: 0.0130(0.0221) \n","EVAL: [500/704] Elapsed 1m 29s (remain 0m 36s) Loss: 0.0209(0.0221) \n","EVAL: [600/704] Elapsed 1m 46s (remain 0m 18s) Loss: 0.0251(0.0219) \n","EVAL: [700/704] Elapsed 2m 4s (remain 0m 0s) Loss: 0.0314(0.0221) \n","EVAL: [703/704] Elapsed 2m 4s (remain 0m 0s) Loss: 0.0376(0.0221) \n","Epoch 1 - avg_train_loss: 0.0263  avg_val_loss: 0.0221  time: 923s\n","Epoch 1 - Score: 0.8347\n","Epoch 1 - Save Best Score: 0.8347 Model\n","Epoch: [2][0/2871] Elapsed 0m 0s (remain 31m 49s) Loss: 0.0177(0.0177) Grad: 1328.2941  LR: 0.000018  \n","Epoch: [2][100/2871] Elapsed 0m 32s (remain 14m 42s) Loss: 0.0271(0.0214) Grad: 4997.8032  LR: 0.000018  \n","Epoch: [2][200/2871] Elapsed 1m 0s (remain 13m 25s) Loss: 0.0139(0.0214) Grad: 4319.0327  LR: 0.000017  \n","Epoch: [2][300/2871] Elapsed 1m 28s (remain 12m 37s) Loss: 0.0201(0.0214) Grad: 9831.8701  LR: 0.000017  \n","Epoch: [2][400/2871] Elapsed 1m 56s (remain 11m 57s) Loss: 0.0204(0.0217) Grad: 3623.7866  LR: 0.000017  \n","Epoch: [2][500/2871] Elapsed 2m 24s (remain 11m 22s) Loss: 0.0296(0.0216) Grad: 14201.8760  LR: 0.000017  \n","Epoch: [2][600/2871] Elapsed 2m 52s (remain 10m 50s) Loss: 0.0259(0.0216) Grad: 7490.9575  LR: 0.000017  \n","Epoch: [2][700/2871] Elapsed 3m 20s (remain 10m 19s) Loss: 0.0214(0.0217) Grad: 5982.2983  LR: 0.000017  \n","Epoch: [2][800/2871] Elapsed 3m 47s (remain 9m 48s) Loss: 0.0136(0.0218) Grad: 2200.2942  LR: 0.000017  \n","Epoch: [2][900/2871] Elapsed 4m 15s (remain 9m 19s) Loss: 0.0185(0.0217) Grad: 2960.3889  LR: 0.000016  \n","Epoch: [2][1000/2871] Elapsed 4m 43s (remain 8m 49s) Loss: 0.0203(0.0216) Grad: 4499.5435  LR: 0.000016  \n","Epoch: [2][1100/2871] Elapsed 5m 11s (remain 8m 20s) Loss: 0.0133(0.0216) Grad: 2191.3054  LR: 0.000016  \n","Epoch: [2][1200/2871] Elapsed 5m 39s (remain 7m 51s) Loss: 0.0148(0.0215) Grad: 6034.5752  LR: 0.000016  \n","Epoch: [2][1300/2871] Elapsed 6m 6s (remain 7m 22s) Loss: 0.0126(0.0216) Grad: 678.7712  LR: 0.000016  \n","Epoch: [2][1400/2871] Elapsed 6m 34s (remain 6m 54s) Loss: 0.0251(0.0216) Grad: 484.3597  LR: 0.000016  \n","Epoch: [2][1500/2871] Elapsed 7m 2s (remain 6m 25s) Loss: 0.0250(0.0216) Grad: 737.9921  LR: 0.000015  \n","Epoch: [2][1600/2871] Elapsed 7m 30s (remain 5m 57s) Loss: 0.0253(0.0217) Grad: 2363.2156  LR: 0.000015  \n","Epoch: [2][1700/2871] Elapsed 7m 58s (remain 5m 29s) Loss: 0.0190(0.0217) Grad: 1996.9019  LR: 0.000015  \n","Epoch: [2][1800/2871] Elapsed 8m 26s (remain 5m 0s) Loss: 0.0130(0.0217) Grad: 4804.4575  LR: 0.000015  \n","Epoch: [2][1900/2871] Elapsed 8m 54s (remain 4m 32s) Loss: 0.0286(0.0217) Grad: 898.2726  LR: 0.000015  \n","Epoch: [2][2000/2871] Elapsed 9m 22s (remain 4m 4s) Loss: 0.0129(0.0217) Grad: 5056.3062  LR: 0.000015  \n","Epoch: [2][2100/2871] Elapsed 9m 49s (remain 3m 36s) Loss: 0.0257(0.0217) Grad: 670.2691  LR: 0.000015  \n","Epoch: [2][2200/2871] Elapsed 10m 17s (remain 3m 8s) Loss: 0.0186(0.0218) Grad: 992.8984  LR: 0.000014  \n","Epoch: [2][2300/2871] Elapsed 10m 45s (remain 2m 39s) Loss: 0.0197(0.0218) Grad: 2890.0752  LR: 0.000014  \n","Epoch: [2][2400/2871] Elapsed 11m 13s (remain 2m 11s) Loss: 0.0245(0.0218) Grad: 889.4723  LR: 0.000014  \n","Epoch: [2][2500/2871] Elapsed 11m 41s (remain 1m 43s) Loss: 0.0244(0.0217) Grad: 10526.1758  LR: 0.000014  \n","Epoch: [2][2600/2871] Elapsed 12m 9s (remain 1m 15s) Loss: 0.0258(0.0217) Grad: 1905.4563  LR: 0.000014  \n","Epoch: [2][2700/2871] Elapsed 12m 36s (remain 0m 47s) Loss: 0.0270(0.0217) Grad: 2571.5027  LR: 0.000014  \n","Epoch: [2][2800/2871] Elapsed 13m 4s (remain 0m 19s) Loss: 0.0256(0.0217) Grad: 13258.7158  LR: 0.000013  \n","Epoch: [2][2870/2871] Elapsed 13m 24s (remain 0m 0s) Loss: 0.0297(0.0217) Grad: 14732.9404  LR: 0.000013  \n","EVAL: [0/704] Elapsed 0m 0s (remain 6m 0s) Loss: 0.0131(0.0131) \n","EVAL: [100/704] Elapsed 0m 18s (remain 1m 49s) Loss: 0.0126(0.0204) \n","EVAL: [200/704] Elapsed 0m 36s (remain 1m 30s) Loss: 0.0313(0.0219) \n","EVAL: [300/704] Elapsed 0m 53s (remain 1m 12s) Loss: 0.0185(0.0214) \n","EVAL: [400/704] Elapsed 1m 11s (remain 0m 54s) Loss: 0.0132(0.0219) \n","EVAL: [500/704] Elapsed 1m 29s (remain 0m 36s) Loss: 0.0211(0.0218) \n","EVAL: [600/704] Elapsed 1m 47s (remain 0m 18s) Loss: 0.0250(0.0216) \n","EVAL: [700/704] Elapsed 2m 5s (remain 0m 0s) Loss: 0.0313(0.0218) \n","EVAL: [703/704] Elapsed 2m 5s (remain 0m 0s) Loss: 0.0376(0.0218) \n","Epoch 2 - avg_train_loss: 0.0217  avg_val_loss: 0.0218  time: 934s\n","Epoch 2 - Score: 0.8613\n","Epoch 2 - Save Best Score: 0.8613 Model\n","Epoch: [3][0/2871] Elapsed 0m 0s (remain 28m 28s) Loss: 0.0170(0.0170) Grad: 1279.8711  LR: 0.000013  \n","Epoch: [3][100/2871] Elapsed 0m 32s (remain 14m 46s) Loss: 0.0253(0.0214) Grad: 47822.2695  LR: 0.000013  \n","Epoch: [3][200/2871] Elapsed 1m 1s (remain 13m 30s) Loss: 0.0200(0.0216) Grad: 2310.4199  LR: 0.000013  \n","Epoch: [3][300/2871] Elapsed 1m 28s (remain 12m 38s) Loss: 0.0250(0.0216) Grad: 246.5378  LR: 0.000013  \n","Epoch: [3][400/2871] Elapsed 1m 56s (remain 11m 58s) Loss: 0.0126(0.0216) Grad: 856.3080  LR: 0.000013  \n","Epoch: [3][500/2871] Elapsed 2m 24s (remain 11m 23s) Loss: 0.0181(0.0215) Grad: 343.4158  LR: 0.000013  \n","Epoch: [3][600/2871] Elapsed 2m 52s (remain 10m 51s) Loss: 0.0319(0.0215) Grad: 1389.2198  LR: 0.000012  \n","Epoch: [3][700/2871] Elapsed 3m 20s (remain 10m 19s) Loss: 0.0194(0.0215) Grad: 11385.9404  LR: 0.000012  \n","Epoch: [3][800/2871] Elapsed 3m 47s (remain 9m 49s) Loss: 0.0258(0.0215) Grad: 1300.2439  LR: 0.000012  \n","Epoch: [3][900/2871] Elapsed 4m 15s (remain 9m 19s) Loss: 0.0125(0.0215) Grad: 674.3760  LR: 0.000012  \n","Epoch: [3][1000/2871] Elapsed 4m 43s (remain 8m 49s) Loss: 0.0255(0.0215) Grad: 3086.6152  LR: 0.000012  \n","Epoch: [3][1100/2871] Elapsed 5m 11s (remain 8m 20s) Loss: 0.0253(0.0215) Grad: 2910.8589  LR: 0.000012  \n","Epoch: [3][1200/2871] Elapsed 5m 39s (remain 7m 51s) Loss: 0.0127(0.0215) Grad: 1635.8918  LR: 0.000011  \n","Epoch: [3][1300/2871] Elapsed 6m 7s (remain 7m 22s) Loss: 0.0127(0.0215) Grad: 1060.4312  LR: 0.000011  \n","Epoch: [3][1400/2871] Elapsed 6m 34s (remain 6m 54s) Loss: 0.0186(0.0214) Grad: 14627.6055  LR: 0.000011  \n","Epoch: [3][1500/2871] Elapsed 7m 2s (remain 6m 25s) Loss: 0.0307(0.0214) Grad: 334.9405  LR: 0.000011  \n","Epoch: [3][1600/2871] Elapsed 7m 30s (remain 5m 57s) Loss: 0.0251(0.0214) Grad: 2292.6431  LR: 0.000011  \n","Epoch: [3][1700/2871] Elapsed 7m 58s (remain 5m 28s) Loss: 0.0135(0.0214) Grad: 4008.7200  LR: 0.000011  \n","Epoch: [3][1800/2871] Elapsed 8m 25s (remain 5m 0s) Loss: 0.0268(0.0214) Grad: 1907.3743  LR: 0.000011  \n","Epoch: [3][1900/2871] Elapsed 8m 53s (remain 4m 32s) Loss: 0.0124(0.0214) Grad: 204.4384  LR: 0.000010  \n","Epoch: [3][2000/2871] Elapsed 9m 21s (remain 4m 4s) Loss: 0.0153(0.0213) Grad: 956.6166  LR: 0.000010  \n","Epoch: [3][2100/2871] Elapsed 9m 49s (remain 3m 36s) Loss: 0.0132(0.0214) Grad: 3856.6304  LR: 0.000010  \n","Epoch: [3][2200/2871] Elapsed 10m 17s (remain 3m 7s) Loss: 0.0200(0.0214) Grad: 3881.9265  LR: 0.000010  \n","Epoch: [3][2300/2871] Elapsed 10m 45s (remain 2m 39s) Loss: 0.0124(0.0214) Grad: 1045.5636  LR: 0.000010  \n","Epoch: [3][2400/2871] Elapsed 11m 13s (remain 2m 11s) Loss: 0.0143(0.0214) Grad: 13180.7988  LR: 0.000010  \n","Epoch: [3][2500/2871] Elapsed 11m 40s (remain 1m 43s) Loss: 0.0313(0.0214) Grad: 1072.4639  LR: 0.000009  \n","Epoch: [3][2600/2871] Elapsed 12m 8s (remain 1m 15s) Loss: 0.0228(0.0214) Grad: 2529.9395  LR: 0.000009  \n","Epoch: [3][2700/2871] Elapsed 12m 35s (remain 0m 47s) Loss: 0.0182(0.0214) Grad: 949.4791  LR: 0.000009  \n","Epoch: [3][2800/2871] Elapsed 13m 3s (remain 0m 19s) Loss: 0.0153(0.0214) Grad: 8055.3716  LR: 0.000009  \n","Epoch: [3][2870/2871] Elapsed 13m 22s (remain 0m 0s) Loss: 0.0187(0.0214) Grad: 686.7720  LR: 0.000009  \n","EVAL: [0/704] Elapsed 0m 0s (remain 5m 41s) Loss: 0.0126(0.0126) \n","EVAL: [100/704] Elapsed 0m 18s (remain 1m 49s) Loss: 0.0132(0.0205) \n","EVAL: [200/704] Elapsed 0m 36s (remain 1m 30s) Loss: 0.0313(0.0223) \n","EVAL: [300/704] Elapsed 0m 53s (remain 1m 12s) Loss: 0.0184(0.0217) \n","EVAL: [400/704] Elapsed 1m 11s (remain 0m 54s) Loss: 0.0138(0.0223) \n","EVAL: [500/704] Elapsed 1m 29s (remain 0m 36s) Loss: 0.0212(0.0222) \n","EVAL: [600/704] Elapsed 1m 47s (remain 0m 18s) Loss: 0.0250(0.0220) \n","EVAL: [700/704] Elapsed 2m 4s (remain 0m 0s) Loss: 0.0313(0.0221) \n","EVAL: [703/704] Elapsed 2m 5s (remain 0m 0s) Loss: 0.0376(0.0221) \n","Epoch 3 - avg_train_loss: 0.0214  avg_val_loss: 0.0221  time: 932s\n","Epoch 3 - Score: 0.8735\n","Epoch 3 - Save Best Score: 0.8735 Model\n","Epoch: [4][0/2871] Elapsed 0m 0s (remain 31m 2s) Loss: 0.0203(0.0203) Grad: 1269.1189  LR: 0.000009  \n","Epoch: [4][100/2871] Elapsed 0m 32s (remain 14m 47s) Loss: 0.0207(0.0212) Grad: 9924.5430  LR: 0.000009  \n","Epoch: [4][200/2871] Elapsed 1m 0s (remain 13m 28s) Loss: 0.0187(0.0210) Grad: 198.0764  LR: 0.000009  \n","Epoch: [4][300/2871] Elapsed 1m 28s (remain 12m 37s) Loss: 0.0237(0.0212) Grad: 5078.6807  LR: 0.000008  \n","Epoch: [4][400/2871] Elapsed 1m 56s (remain 11m 59s) Loss: 0.0317(0.0212) Grad: 124.4347  LR: 0.000008  \n","Epoch: [4][500/2871] Elapsed 2m 24s (remain 11m 24s) Loss: 0.0141(0.0212) Grad: 4908.6494  LR: 0.000008  \n","Epoch: [4][600/2871] Elapsed 2m 52s (remain 10m 51s) Loss: 0.0238(0.0212) Grad: 608.8745  LR: 0.000008  \n","Epoch: [4][700/2871] Elapsed 3m 20s (remain 10m 20s) Loss: 0.0187(0.0211) Grad: 1555.3829  LR: 0.000008  \n","Epoch: [4][800/2871] Elapsed 3m 48s (remain 9m 49s) Loss: 0.0243(0.0211) Grad: 1221.0411  LR: 0.000008  \n","Epoch: [4][900/2871] Elapsed 4m 15s (remain 9m 19s) Loss: 0.0189(0.0211) Grad: 384.7638  LR: 0.000007  \n","Epoch: [4][1000/2871] Elapsed 4m 43s (remain 8m 50s) Loss: 0.0176(0.0211) Grad: 2522.0593  LR: 0.000007  \n","Epoch: [4][1100/2871] Elapsed 5m 11s (remain 8m 21s) Loss: 0.0367(0.0211) Grad: 9106.5186  LR: 0.000007  \n","Epoch: [4][1200/2871] Elapsed 5m 39s (remain 7m 52s) Loss: 0.0180(0.0211) Grad: 378.0399  LR: 0.000007  \n","Epoch: [4][1300/2871] Elapsed 6m 7s (remain 7m 23s) Loss: 0.0148(0.0211) Grad: 7975.0684  LR: 0.000007  \n","Epoch: [4][1400/2871] Elapsed 6m 35s (remain 6m 54s) Loss: 0.0303(0.0211) Grad: 1458.2527  LR: 0.000007  \n","Epoch: [4][1500/2871] Elapsed 7m 3s (remain 6m 26s) Loss: 0.0252(0.0211) Grad: 1368.2327  LR: 0.000007  \n","Epoch: [4][1600/2871] Elapsed 7m 31s (remain 5m 57s) Loss: 0.0127(0.0211) Grad: 1022.9569  LR: 0.000006  \n","Epoch: [4][1700/2871] Elapsed 7m 58s (remain 5m 29s) Loss: 0.0288(0.0211) Grad: 5649.3706  LR: 0.000006  \n","Epoch: [4][1800/2871] Elapsed 8m 26s (remain 5m 0s) Loss: 0.0139(0.0212) Grad: 3347.7256  LR: 0.000006  \n","Epoch: [4][1900/2871] Elapsed 8m 54s (remain 4m 32s) Loss: 0.0251(0.0211) Grad: 498.0483  LR: 0.000006  \n","Epoch: [4][2000/2871] Elapsed 9m 22s (remain 4m 4s) Loss: 0.0125(0.0211) Grad: 961.4733  LR: 0.000006  \n","Epoch: [4][2100/2871] Elapsed 9m 50s (remain 3m 36s) Loss: 0.0191(0.0211) Grad: 1209.3237  LR: 0.000006  \n","Epoch: [4][2200/2871] Elapsed 10m 17s (remain 3m 8s) Loss: 0.0138(0.0211) Grad: 20916.5000  LR: 0.000005  \n","Epoch: [4][2300/2871] Elapsed 10m 45s (remain 2m 39s) Loss: 0.0126(0.0211) Grad: 2712.5442  LR: 0.000005  \n","Epoch: [4][2400/2871] Elapsed 11m 13s (remain 2m 11s) Loss: 0.0330(0.0211) Grad: 987.6177  LR: 0.000005  \n","Epoch: [4][2500/2871] Elapsed 11m 41s (remain 1m 43s) Loss: 0.0196(0.0211) Grad: 1245.7936  LR: 0.000005  \n","Epoch: [4][2600/2871] Elapsed 12m 9s (remain 1m 15s) Loss: 0.0133(0.0211) Grad: 27265.1094  LR: 0.000005  \n","Epoch: [4][2700/2871] Elapsed 12m 37s (remain 0m 47s) Loss: 0.0201(0.0211) Grad: 5594.3315  LR: 0.000005  \n","Epoch: [4][2800/2871] Elapsed 13m 5s (remain 0m 19s) Loss: 0.0220(0.0211) Grad: 8522.3604  LR: 0.000005  \n","Epoch: [4][2870/2871] Elapsed 13m 24s (remain 0m 0s) Loss: 0.0124(0.0211) Grad: 387.2897  LR: 0.000004  \n","EVAL: [0/704] Elapsed 0m 0s (remain 5m 49s) Loss: 0.0127(0.0127) \n","EVAL: [100/704] Elapsed 0m 18s (remain 1m 49s) Loss: 0.0127(0.0206) \n","EVAL: [200/704] Elapsed 0m 36s (remain 1m 30s) Loss: 0.0313(0.0223) \n","EVAL: [300/704] Elapsed 0m 54s (remain 1m 12s) Loss: 0.0185(0.0218) \n","EVAL: [400/704] Elapsed 1m 11s (remain 0m 54s) Loss: 0.0140(0.0223) \n","EVAL: [500/704] Elapsed 1m 29s (remain 0m 36s) Loss: 0.0212(0.0222) \n","EVAL: [600/704] Elapsed 1m 47s (remain 0m 18s) Loss: 0.0250(0.0221) \n","EVAL: [700/704] Elapsed 2m 5s (remain 0m 0s) Loss: 0.0313(0.0222) \n","EVAL: [703/704] Elapsed 2m 5s (remain 0m 0s) Loss: 0.0376(0.0222) \n","Epoch 4 - avg_train_loss: 0.0211  avg_val_loss: 0.0222  time: 935s\n","Epoch 4 - Score: 0.8730\n","Epoch: [5][0/2871] Elapsed 0m 0s (remain 31m 23s) Loss: 0.0311(0.0311) Grad: 594.4358  LR: 0.000004  \n","Epoch: [5][100/2871] Elapsed 0m 28s (remain 13m 0s) Loss: 0.0255(0.0212) Grad: 444.7701  LR: 0.000004  \n","Epoch: [5][200/2871] Elapsed 0m 56s (remain 12m 30s) Loss: 0.0190(0.0213) Grad: 2476.9705  LR: 0.000004  \n","Epoch: [5][300/2871] Elapsed 1m 24s (remain 12m 0s) Loss: 0.0180(0.0211) Grad: 266.1070  LR: 0.000004  \n","Epoch: [5][400/2871] Elapsed 1m 52s (remain 11m 31s) Loss: 0.0238(0.0215) Grad: 2213.4790  LR: 0.000004  \n","Epoch: [5][500/2871] Elapsed 2m 20s (remain 11m 3s) Loss: 0.0201(0.0213) Grad: 2204.9963  LR: 0.000004  \n","Epoch: [5][600/2871] Elapsed 2m 48s (remain 10m 34s) Loss: 0.0165(0.0211) Grad: 298.2320  LR: 0.000004  \n","Epoch: [5][700/2871] Elapsed 3m 16s (remain 10m 6s) Loss: 0.0247(0.0210) Grad: 78.8577  LR: 0.000003  \n","Epoch: [5][800/2871] Elapsed 3m 43s (remain 9m 38s) Loss: 0.0189(0.0209) Grad: 1993.7823  LR: 0.000003  \n","Epoch: [5][900/2871] Elapsed 4m 11s (remain 9m 10s) Loss: 0.0392(0.0209) Grad: 32253.9258  LR: 0.000003  \n","Epoch: [5][1000/2871] Elapsed 4m 39s (remain 8m 42s) Loss: 0.0124(0.0208) Grad: 177.5467  LR: 0.000003  \n","Epoch: [5][1100/2871] Elapsed 5m 7s (remain 8m 14s) Loss: 0.0258(0.0208) Grad: 373.9778  LR: 0.000003  \n","Epoch: [5][1200/2871] Elapsed 5m 35s (remain 7m 46s) Loss: 0.0174(0.0207) Grad: 99.8194  LR: 0.000003  \n","Epoch: [5][1300/2871] Elapsed 6m 3s (remain 7m 19s) Loss: 0.0244(0.0207) Grad: 310.3408  LR: 0.000002  \n","Epoch: [5][1400/2871] Elapsed 6m 31s (remain 6m 51s) Loss: 0.0249(0.0207) Grad: 4612.8101  LR: 0.000002  \n","Epoch: [5][1500/2871] Elapsed 6m 59s (remain 6m 23s) Loss: 0.0185(0.0208) Grad: 749.0289  LR: 0.000002  \n","Epoch: [5][1600/2871] Elapsed 7m 27s (remain 5m 55s) Loss: 0.0165(0.0207) Grad: 10248.5371  LR: 0.000002  \n","Epoch: [5][1700/2871] Elapsed 7m 55s (remain 5m 27s) Loss: 0.0250(0.0208) Grad: 361.9198  LR: 0.000002  \n","Epoch: [5][1800/2871] Elapsed 8m 23s (remain 4m 59s) Loss: 0.0198(0.0208) Grad: 1755.2435  LR: 0.000002  \n","Epoch: [5][1900/2871] Elapsed 8m 51s (remain 4m 31s) Loss: 0.0239(0.0208) Grad: 128.9915  LR: 0.000002  \n","Epoch: [5][2000/2871] Elapsed 9m 19s (remain 4m 3s) Loss: 0.0241(0.0209) Grad: 789.9031  LR: 0.000001  \n","Epoch: [5][2100/2871] Elapsed 9m 47s (remain 3m 35s) Loss: 0.0126(0.0209) Grad: 1051.0781  LR: 0.000001  \n","Epoch: [5][2200/2871] Elapsed 10m 15s (remain 3m 7s) Loss: 0.0258(0.0209) Grad: 18417.8730  LR: 0.000001  \n","Epoch: [5][2300/2871] Elapsed 10m 43s (remain 2m 39s) Loss: 0.0170(0.0209) Grad: 210.4590  LR: 0.000001  \n","Epoch: [5][2400/2871] Elapsed 11m 10s (remain 2m 11s) Loss: 0.0195(0.0209) Grad: 7262.9014  LR: 0.000001  \n","Epoch: [5][2500/2871] Elapsed 11m 38s (remain 1m 43s) Loss: 0.0236(0.0209) Grad: 2820.6003  LR: 0.000001  \n","Epoch: [5][2600/2871] Elapsed 12m 6s (remain 1m 15s) Loss: 0.0248(0.0209) Grad: 752.7646  LR: 0.000000  \n","Epoch: [5][2700/2871] Elapsed 12m 34s (remain 0m 47s) Loss: 0.0190(0.0209) Grad: 665.4664  LR: 0.000000  \n","Epoch: [5][2800/2871] Elapsed 13m 2s (remain 0m 19s) Loss: 0.0190(0.0209) Grad: 211.7346  LR: 0.000000  \n","Epoch: [5][2870/2871] Elapsed 13m 21s (remain 0m 0s) Loss: 0.0307(0.0209) Grad: 15000.5830  LR: 0.000000  \n","EVAL: [0/704] Elapsed 0m 0s (remain 5m 52s) Loss: 0.0125(0.0125) \n","EVAL: [100/704] Elapsed 0m 18s (remain 1m 49s) Loss: 0.0127(0.0208) \n","EVAL: [200/704] Elapsed 0m 36s (remain 1m 30s) Loss: 0.0313(0.0225) \n","EVAL: [300/704] Elapsed 0m 53s (remain 1m 12s) Loss: 0.0185(0.0220) \n","EVAL: [400/704] Elapsed 1m 11s (remain 0m 54s) Loss: 0.0141(0.0225) \n","EVAL: [500/704] Elapsed 1m 29s (remain 0m 36s) Loss: 0.0213(0.0224) \n","EVAL: [600/704] Elapsed 1m 47s (remain 0m 18s) Loss: 0.0250(0.0223) \n","EVAL: [700/704] Elapsed 2m 5s (remain 0m 0s) Loss: 0.0313(0.0224) \n","EVAL: [703/704] Elapsed 2m 5s (remain 0m 0s) Loss: 0.0376(0.0224) \n","Epoch 5 - avg_train_loss: 0.0209  avg_val_loss: 0.0224  time: 931s\n","Epoch 5 - Score: 0.8712\n","========== fold: 3 training ==========\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Load weight from pretrained\n","Epoch: [1][0/2877] Elapsed 0m 0s (remain 33m 43s) Loss: 0.1929(0.1929) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/2877] Elapsed 0m 28s (remain 12m 58s) Loss: 0.0586(0.1728) Grad: 6089.2681  LR: 0.000001  \n","Epoch: [1][200/2877] Elapsed 0m 56s (remain 12m 28s) Loss: 0.0279(0.1057) Grad: 1991.9492  LR: 0.000003  \n","Epoch: [1][300/2877] Elapsed 1m 23s (remain 11m 58s) Loss: 0.0297(0.0806) Grad: 409.5205  LR: 0.000004  \n","Epoch: [1][400/2877] Elapsed 1m 51s (remain 11m 30s) Loss: 0.0353(0.0669) Grad: 893.7275  LR: 0.000006  \n","Epoch: [1][500/2877] Elapsed 2m 19s (remain 11m 2s) Loss: 0.0158(0.0583) Grad: 647.7148  LR: 0.000007  \n","Epoch: [1][600/2877] Elapsed 2m 48s (remain 10m 36s) Loss: 0.0271(0.0528) Grad: 331.1411  LR: 0.000008  \n","Epoch: [1][700/2877] Elapsed 3m 15s (remain 10m 7s) Loss: 0.0289(0.0488) Grad: 606.7651  LR: 0.000010  \n","Epoch: [1][800/2877] Elapsed 3m 43s (remain 9m 39s) Loss: 0.0260(0.0456) Grad: 563.1434  LR: 0.000011  \n","Epoch: [1][900/2877] Elapsed 4m 11s (remain 9m 11s) Loss: 0.0206(0.0431) Grad: 588.3378  LR: 0.000013  \n","Epoch: [1][1000/2877] Elapsed 4m 38s (remain 8m 42s) Loss: 0.0207(0.0412) Grad: 165.0578  LR: 0.000014  \n","Epoch: [1][1100/2877] Elapsed 5m 6s (remain 8m 14s) Loss: 0.0193(0.0396) Grad: 274.1091  LR: 0.000015  \n","Epoch: [1][1200/2877] Elapsed 5m 34s (remain 7m 46s) Loss: 0.0240(0.0382) Grad: 846.0736  LR: 0.000017  \n","Epoch: [1][1300/2877] Elapsed 6m 2s (remain 7m 19s) Loss: 0.0139(0.0369) Grad: 268.3260  LR: 0.000018  \n","Epoch: [1][1400/2877] Elapsed 6m 30s (remain 6m 51s) Loss: 0.0180(0.0360) Grad: 252.6499  LR: 0.000019  \n","Epoch: [1][1500/2877] Elapsed 6m 57s (remain 6m 23s) Loss: 0.0281(0.0352) Grad: 560.0541  LR: 0.000020  \n","Epoch: [1][1600/2877] Elapsed 7m 25s (remain 5m 55s) Loss: 0.0237(0.0344) Grad: 1231.0741  LR: 0.000020  \n","Epoch: [1][1700/2877] Elapsed 7m 53s (remain 5m 27s) Loss: 0.0196(0.0338) Grad: 256.8696  LR: 0.000020  \n","Epoch: [1][1800/2877] Elapsed 8m 21s (remain 4m 59s) Loss: 0.0247(0.0333) Grad: 127.3417  LR: 0.000019  \n","Epoch: [1][1900/2877] Elapsed 8m 49s (remain 4m 31s) Loss: 0.0133(0.0327) Grad: 207.8120  LR: 0.000019  \n","Epoch: [1][2000/2877] Elapsed 9m 16s (remain 4m 3s) Loss: 0.0219(0.0322) Grad: 337.6551  LR: 0.000019  \n","Epoch: [1][2100/2877] Elapsed 9m 44s (remain 3m 35s) Loss: 0.0274(0.0318) Grad: 452.4197  LR: 0.000019  \n","Epoch: [1][2200/2877] Elapsed 10m 12s (remain 3m 8s) Loss: 0.0276(0.0314) Grad: 352.7656  LR: 0.000019  \n","Epoch: [1][2300/2877] Elapsed 10m 40s (remain 2m 40s) Loss: 0.0297(0.0310) Grad: 511.5931  LR: 0.000019  \n","Epoch: [1][2400/2877] Elapsed 11m 8s (remain 2m 12s) Loss: 0.0141(0.0306) Grad: 384.1758  LR: 0.000019  \n","Epoch: [1][2500/2877] Elapsed 11m 36s (remain 1m 44s) Loss: 0.0278(0.0303) Grad: 249.7708  LR: 0.000018  \n","Epoch: [1][2600/2877] Elapsed 12m 3s (remain 1m 16s) Loss: 0.0279(0.0300) Grad: 586.4242  LR: 0.000018  \n","Epoch: [1][2700/2877] Elapsed 12m 31s (remain 0m 48s) Loss: 0.0211(0.0297) Grad: 281.3486  LR: 0.000018  \n","Epoch: [1][2800/2877] Elapsed 12m 59s (remain 0m 21s) Loss: 0.0186(0.0294) Grad: 300.1285  LR: 0.000018  \n","Epoch: [1][2876/2877] Elapsed 13m 20s (remain 0m 0s) Loss: 0.0277(0.0292) Grad: 387.5626  LR: 0.000018  \n","EVAL: [0/698] Elapsed 0m 0s (remain 6m 39s) Loss: 0.0196(0.0196) \n","EVAL: [100/698] Elapsed 0m 18s (remain 1m 48s) Loss: 0.0187(0.0203) \n","EVAL: [200/698] Elapsed 0m 36s (remain 1m 29s) Loss: 0.0326(0.0219) \n","EVAL: [300/698] Elapsed 0m 53s (remain 1m 11s) Loss: 0.0126(0.0218) \n","EVAL: [400/698] Elapsed 1m 11s (remain 0m 53s) Loss: 0.0302(0.0218) \n","EVAL: [500/698] Elapsed 1m 29s (remain 0m 35s) Loss: 0.0259(0.0215) \n","EVAL: [600/698] Elapsed 1m 47s (remain 0m 17s) Loss: 0.0195(0.0215) \n","EVAL: [697/698] Elapsed 2m 4s (remain 0m 0s) Loss: 0.0379(0.0217) \n","Epoch 1 - avg_train_loss: 0.0292  avg_val_loss: 0.0217  time: 929s\n","Epoch 1 - Score: 0.8576\n","Epoch 1 - Save Best Score: 0.8576 Model\n","Epoch: [2][0/2877] Elapsed 0m 0s (remain 32m 24s) Loss: 0.0195(0.0195) Grad: 5279.7529  LR: 0.000018  \n","Epoch: [2][100/2877] Elapsed 0m 32s (remain 14m 43s) Loss: 0.0246(0.0215) Grad: 1930.1583  LR: 0.000018  \n","Epoch: [2][200/2877] Elapsed 1m 0s (remain 13m 27s) Loss: 0.0134(0.0219) Grad: 1906.6248  LR: 0.000017  \n","Epoch: [2][300/2877] Elapsed 1m 28s (remain 12m 38s) Loss: 0.0210(0.0218) Grad: 8664.9893  LR: 0.000017  \n","Epoch: [2][400/2877] Elapsed 1m 56s (remain 11m 59s) Loss: 0.0176(0.0216) Grad: 21447.0234  LR: 0.000017  \n","Epoch: [2][500/2877] Elapsed 2m 24s (remain 11m 24s) Loss: 0.0250(0.0216) Grad: 3358.0479  LR: 0.000017  \n","Epoch: [2][600/2877] Elapsed 2m 52s (remain 10m 51s) Loss: 0.0261(0.0217) Grad: 1359.4482  LR: 0.000017  \n","Epoch: [2][700/2877] Elapsed 3m 19s (remain 10m 20s) Loss: 0.0203(0.0217) Grad: 9683.4561  LR: 0.000017  \n","Epoch: [2][800/2877] Elapsed 3m 47s (remain 9m 49s) Loss: 0.0229(0.0219) Grad: 7404.7417  LR: 0.000017  \n","Epoch: [2][900/2877] Elapsed 4m 15s (remain 9m 20s) Loss: 0.0211(0.0219) Grad: 12246.8154  LR: 0.000016  \n","Epoch: [2][1000/2877] Elapsed 4m 43s (remain 8m 50s) Loss: 0.0256(0.0219) Grad: 4968.2988  LR: 0.000016  \n","Epoch: [2][1100/2877] Elapsed 5m 10s (remain 8m 21s) Loss: 0.0245(0.0219) Grad: 1231.7935  LR: 0.000016  \n","Epoch: [2][1200/2877] Elapsed 5m 38s (remain 7m 52s) Loss: 0.0152(0.0219) Grad: 8581.7480  LR: 0.000016  \n","Epoch: [2][1300/2877] Elapsed 6m 6s (remain 7m 23s) Loss: 0.0202(0.0219) Grad: 5071.9751  LR: 0.000016  \n","Epoch: [2][1400/2877] Elapsed 6m 34s (remain 6m 55s) Loss: 0.0202(0.0218) Grad: 7972.2788  LR: 0.000016  \n","Epoch: [2][1500/2877] Elapsed 7m 2s (remain 6m 26s) Loss: 0.0268(0.0217) Grad: 1017.8806  LR: 0.000015  \n","Epoch: [2][1600/2877] Elapsed 7m 29s (remain 5m 58s) Loss: 0.0190(0.0218) Grad: 870.8500  LR: 0.000015  \n","Epoch: [2][1700/2877] Elapsed 7m 57s (remain 5m 30s) Loss: 0.0124(0.0218) Grad: 514.0521  LR: 0.000015  \n","Epoch: [2][1800/2877] Elapsed 8m 25s (remain 5m 1s) Loss: 0.0277(0.0218) Grad: 3858.1687  LR: 0.000015  \n","Epoch: [2][1900/2877] Elapsed 8m 53s (remain 4m 33s) Loss: 0.0306(0.0218) Grad: 4534.5073  LR: 0.000015  \n","Epoch: [2][2000/2877] Elapsed 9m 21s (remain 4m 5s) Loss: 0.0198(0.0218) Grad: 2023.6005  LR: 0.000015  \n","Epoch: [2][2100/2877] Elapsed 9m 48s (remain 3m 37s) Loss: 0.0134(0.0218) Grad: 3092.5254  LR: 0.000015  \n","Epoch: [2][2200/2877] Elapsed 10m 16s (remain 3m 9s) Loss: 0.0311(0.0218) Grad: 9269.5664  LR: 0.000014  \n","Epoch: [2][2300/2877] Elapsed 10m 44s (remain 2m 41s) Loss: 0.0128(0.0218) Grad: 2479.7695  LR: 0.000014  \n","Epoch: [2][2400/2877] Elapsed 11m 12s (remain 2m 13s) Loss: 0.0198(0.0218) Grad: 550.2255  LR: 0.000014  \n","Epoch: [2][2500/2877] Elapsed 11m 39s (remain 1m 45s) Loss: 0.0133(0.0217) Grad: 2176.2200  LR: 0.000014  \n","Epoch: [2][2600/2877] Elapsed 12m 7s (remain 1m 17s) Loss: 0.0180(0.0217) Grad: 696.1451  LR: 0.000014  \n","Epoch: [2][2700/2877] Elapsed 12m 35s (remain 0m 49s) Loss: 0.0133(0.0217) Grad: 11947.8652  LR: 0.000014  \n","Epoch: [2][2800/2877] Elapsed 13m 2s (remain 0m 21s) Loss: 0.0216(0.0217) Grad: 6098.5234  LR: 0.000013  \n","Epoch: [2][2876/2877] Elapsed 13m 23s (remain 0m 0s) Loss: 0.0287(0.0218) Grad: 17274.4785  LR: 0.000013  \n","EVAL: [0/698] Elapsed 0m 0s (remain 5m 46s) Loss: 0.0188(0.0188) \n","EVAL: [100/698] Elapsed 0m 18s (remain 1m 47s) Loss: 0.0184(0.0202) \n","EVAL: [200/698] Elapsed 0m 35s (remain 1m 28s) Loss: 0.0323(0.0218) \n","EVAL: [300/698] Elapsed 0m 53s (remain 1m 10s) Loss: 0.0128(0.0217) \n","EVAL: [400/698] Elapsed 1m 11s (remain 0m 52s) Loss: 0.0333(0.0217) \n","EVAL: [500/698] Elapsed 1m 28s (remain 0m 34s) Loss: 0.0255(0.0213) \n","EVAL: [600/698] Elapsed 1m 46s (remain 0m 17s) Loss: 0.0193(0.0213) \n","EVAL: [697/698] Elapsed 2m 3s (remain 0m 0s) Loss: 0.0376(0.0216) \n","Epoch 2 - avg_train_loss: 0.0218  avg_val_loss: 0.0216  time: 931s\n","Epoch 2 - Score: 0.8819\n","Epoch 2 - Save Best Score: 0.8819 Model\n","Epoch: [3][0/2877] Elapsed 0m 0s (remain 30m 21s) Loss: 0.0143(0.0143) Grad: 4866.1157  LR: 0.000013  \n","Epoch: [3][100/2877] Elapsed 0m 31s (remain 14m 26s) Loss: 0.0185(0.0214) Grad: 1579.2341  LR: 0.000013  \n","Epoch: [3][200/2877] Elapsed 1m 0s (remain 13m 18s) Loss: 0.0271(0.0218) Grad: 2707.3982  LR: 0.000013  \n","Epoch: [3][300/2877] Elapsed 1m 27s (remain 12m 29s) Loss: 0.0133(0.0214) Grad: 2279.0852  LR: 0.000013  \n","Epoch: [3][400/2877] Elapsed 1m 55s (remain 11m 50s) Loss: 0.0346(0.0213) Grad: 20667.6191  LR: 0.000013  \n","Epoch: [3][500/2877] Elapsed 2m 22s (remain 11m 15s) Loss: 0.0197(0.0214) Grad: 1739.5428  LR: 0.000013  \n","Epoch: [3][600/2877] Elapsed 2m 50s (remain 10m 44s) Loss: 0.0339(0.0215) Grad: 33829.3164  LR: 0.000012  \n","Epoch: [3][700/2877] Elapsed 3m 17s (remain 10m 13s) Loss: 0.0133(0.0216) Grad: 4585.3623  LR: 0.000012  \n","Epoch: [3][800/2877] Elapsed 3m 45s (remain 9m 43s) Loss: 0.0184(0.0216) Grad: 2222.7205  LR: 0.000012  \n","Epoch: [3][900/2877] Elapsed 4m 12s (remain 9m 14s) Loss: 0.0619(0.0215) Grad: 62108.7773  LR: 0.000012  \n","Epoch: [3][1000/2877] Elapsed 4m 40s (remain 8m 45s) Loss: 0.0189(0.0215) Grad: 1287.6357  LR: 0.000012  \n","Epoch: [3][1100/2877] Elapsed 5m 7s (remain 8m 16s) Loss: 0.0262(0.0215) Grad: 1695.5952  LR: 0.000012  \n","Epoch: [3][1200/2877] Elapsed 5m 35s (remain 7m 48s) Loss: 0.0137(0.0215) Grad: 5956.4785  LR: 0.000011  \n","Epoch: [3][1300/2877] Elapsed 6m 3s (remain 7m 19s) Loss: 0.0193(0.0214) Grad: 22658.1699  LR: 0.000011  \n","Epoch: [3][1400/2877] Elapsed 6m 30s (remain 6m 51s) Loss: 0.0377(0.0214) Grad: 482.5335  LR: 0.000011  \n","Epoch: [3][1500/2877] Elapsed 6m 58s (remain 6m 23s) Loss: 0.0180(0.0214) Grad: 116.4460  LR: 0.000011  \n","Epoch: [3][1600/2877] Elapsed 7m 25s (remain 5m 55s) Loss: 0.0198(0.0214) Grad: 1961.0892  LR: 0.000011  \n","Epoch: [3][1700/2877] Elapsed 7m 53s (remain 5m 27s) Loss: 0.0125(0.0215) Grad: 117.6069  LR: 0.000011  \n","Epoch: [3][1800/2877] Elapsed 8m 21s (remain 4m 59s) Loss: 0.0145(0.0214) Grad: 7067.6729  LR: 0.000011  \n","Epoch: [3][1900/2877] Elapsed 8m 48s (remain 4m 31s) Loss: 0.0183(0.0214) Grad: 3339.0764  LR: 0.000010  \n","Epoch: [3][2000/2877] Elapsed 9m 16s (remain 4m 3s) Loss: 0.0242(0.0215) Grad: 4451.1128  LR: 0.000010  \n","Epoch: [3][2100/2877] Elapsed 9m 43s (remain 3m 35s) Loss: 0.0261(0.0214) Grad: 34853.9336  LR: 0.000010  \n","Epoch: [3][2200/2877] Elapsed 10m 11s (remain 3m 7s) Loss: 0.0269(0.0214) Grad: 161.8804  LR: 0.000010  \n","Epoch: [3][2300/2877] Elapsed 10m 39s (remain 2m 40s) Loss: 0.0254(0.0214) Grad: 9719.7129  LR: 0.000010  \n","Epoch: [3][2400/2877] Elapsed 11m 6s (remain 2m 12s) Loss: 0.0128(0.0214) Grad: 1127.6421  LR: 0.000010  \n","Epoch: [3][2500/2877] Elapsed 11m 34s (remain 1m 44s) Loss: 0.0252(0.0215) Grad: 2749.2656  LR: 0.000009  \n","Epoch: [3][2600/2877] Elapsed 12m 2s (remain 1m 16s) Loss: 0.0241(0.0214) Grad: 2006.7889  LR: 0.000009  \n","Epoch: [3][2700/2877] Elapsed 12m 30s (remain 0m 48s) Loss: 0.0178(0.0214) Grad: 509.4935  LR: 0.000009  \n","Epoch: [3][2800/2877] Elapsed 12m 57s (remain 0m 21s) Loss: 0.0309(0.0214) Grad: 13656.5869  LR: 0.000009  \n","Epoch: [3][2876/2877] Elapsed 13m 18s (remain 0m 0s) Loss: 0.0126(0.0214) Grad: 864.8674  LR: 0.000009  \n","EVAL: [0/698] Elapsed 0m 0s (remain 5m 57s) Loss: 0.0189(0.0189) \n","EVAL: [100/698] Elapsed 0m 18s (remain 1m 47s) Loss: 0.0184(0.0204) \n","EVAL: [200/698] Elapsed 0m 36s (remain 1m 29s) Loss: 0.0323(0.0220) \n","EVAL: [300/698] Elapsed 0m 53s (remain 1m 11s) Loss: 0.0123(0.0220) \n","EVAL: [400/698] Elapsed 1m 11s (remain 0m 53s) Loss: 0.0322(0.0220) \n","EVAL: [500/698] Elapsed 1m 29s (remain 0m 35s) Loss: 0.0254(0.0216) \n","EVAL: [600/698] Elapsed 1m 47s (remain 0m 17s) Loss: 0.0190(0.0215) \n","EVAL: [697/698] Elapsed 2m 4s (remain 0m 0s) Loss: 0.0376(0.0218) \n","Epoch 3 - avg_train_loss: 0.0214  avg_val_loss: 0.0218  time: 927s\n","Epoch 3 - Score: 0.8809\n","Epoch: [4][0/2877] Elapsed 0m 0s (remain 29m 11s) Loss: 0.0425(0.0425) Grad: 21028.4531  LR: 0.000009  \n","Epoch: [4][100/2877] Elapsed 0m 28s (remain 12m 59s) Loss: 0.0192(0.0217) Grad: 2188.3049  LR: 0.000009  \n","Epoch: [4][200/2877] Elapsed 0m 56s (remain 12m 26s) Loss: 0.0251(0.0213) Grad: 31290.4609  LR: 0.000009  \n","Epoch: [4][300/2877] Elapsed 1m 24s (remain 11m 58s) Loss: 0.0194(0.0217) Grad: 2177.3232  LR: 0.000008  \n","Epoch: [4][400/2877] Elapsed 1m 51s (remain 11m 30s) Loss: 0.0310(0.0215) Grad: 551.6824  LR: 0.000008  \n","Epoch: [4][500/2877] Elapsed 2m 19s (remain 11m 2s) Loss: 0.0188(0.0215) Grad: 920.2985  LR: 0.000008  \n","Epoch: [4][600/2877] Elapsed 2m 47s (remain 10m 33s) Loss: 0.0139(0.0215) Grad: 3611.7305  LR: 0.000008  \n","Epoch: [4][700/2877] Elapsed 3m 15s (remain 10m 5s) Loss: 0.0256(0.0215) Grad: 1115.0208  LR: 0.000008  \n","Epoch: [4][800/2877] Elapsed 3m 42s (remain 9m 37s) Loss: 0.0220(0.0214) Grad: 1076.2814  LR: 0.000008  \n","Epoch: [4][900/2877] Elapsed 4m 10s (remain 9m 9s) Loss: 0.0192(0.0213) Grad: 2186.0281  LR: 0.000007  \n","Epoch: [4][1000/2877] Elapsed 4m 38s (remain 8m 41s) Loss: 0.0376(0.0214) Grad: 105.1757  LR: 0.000007  \n","Epoch: [4][1100/2877] Elapsed 5m 6s (remain 8m 14s) Loss: 0.0189(0.0214) Grad: 6051.3965  LR: 0.000007  \n","Epoch: [4][1200/2877] Elapsed 5m 34s (remain 7m 46s) Loss: 0.0124(0.0214) Grad: 439.7463  LR: 0.000007  \n","Epoch: [4][1300/2877] Elapsed 6m 1s (remain 7m 18s) Loss: 0.0189(0.0214) Grad: 1419.1971  LR: 0.000007  \n","Epoch: [4][1400/2877] Elapsed 6m 29s (remain 6m 50s) Loss: 0.0124(0.0213) Grad: 574.8515  LR: 0.000007  \n","Epoch: [4][1500/2877] Elapsed 7m 3s (remain 6m 28s) Loss: 0.0253(0.0213) Grad: 22094.0117  LR: 0.000007  \n","Epoch: [4][1600/2877] Elapsed 7m 31s (remain 5m 59s) Loss: 0.0126(0.0212) Grad: 744.6744  LR: 0.000006  \n","Epoch: [4][1700/2877] Elapsed 7m 58s (remain 5m 31s) Loss: 0.0186(0.0212) Grad: 2669.5469  LR: 0.000006  \n","Epoch: [4][1800/2877] Elapsed 8m 26s (remain 5m 2s) Loss: 0.0247(0.0212) Grad: 937.3028  LR: 0.000006  \n","Epoch: [4][1900/2877] Elapsed 8m 54s (remain 4m 34s) Loss: 0.0168(0.0211) Grad: 1069.0746  LR: 0.000006  \n","Epoch: [4][2000/2877] Elapsed 9m 22s (remain 4m 6s) Loss: 0.0190(0.0211) Grad: 1761.1100  LR: 0.000006  \n","Epoch: [4][2100/2877] Elapsed 9m 50s (remain 3m 37s) Loss: 0.0296(0.0212) Grad: 1220.2449  LR: 0.000006  \n","Epoch: [4][2200/2877] Elapsed 10m 18s (remain 3m 9s) Loss: 0.0248(0.0212) Grad: 604.8128  LR: 0.000005  \n","Epoch: [4][2300/2877] Elapsed 10m 45s (remain 2m 41s) Loss: 0.0267(0.0211) Grad: 3535.0120  LR: 0.000005  \n","Epoch: [4][2400/2877] Elapsed 11m 13s (remain 2m 13s) Loss: 0.0318(0.0212) Grad: 811.4272  LR: 0.000005  \n","Epoch: [4][2500/2877] Elapsed 11m 41s (remain 1m 45s) Loss: 0.0140(0.0212) Grad: 4614.0776  LR: 0.000005  \n","Epoch: [4][2600/2877] Elapsed 12m 9s (remain 1m 17s) Loss: 0.0216(0.0212) Grad: 12535.7725  LR: 0.000005  \n","Epoch: [4][2700/2877] Elapsed 12m 36s (remain 0m 49s) Loss: 0.0211(0.0212) Grad: 21370.6211  LR: 0.000005  \n","Epoch: [4][2800/2877] Elapsed 13m 4s (remain 0m 21s) Loss: 0.0257(0.0212) Grad: 519.3785  LR: 0.000005  \n","Epoch: [4][2876/2877] Elapsed 13m 25s (remain 0m 0s) Loss: 0.0191(0.0212) Grad: 1722.0155  LR: 0.000004  \n","EVAL: [0/698] Elapsed 0m 0s (remain 5m 43s) Loss: 0.0186(0.0186) \n","EVAL: [100/698] Elapsed 0m 18s (remain 1m 48s) Loss: 0.0182(0.0203) \n","EVAL: [200/698] Elapsed 0m 36s (remain 1m 29s) Loss: 0.0323(0.0220) \n","EVAL: [300/698] Elapsed 0m 53s (remain 1m 10s) Loss: 0.0124(0.0221) \n","EVAL: [400/698] Elapsed 1m 11s (remain 0m 52s) Loss: 0.0336(0.0220) \n","EVAL: [500/698] Elapsed 1m 29s (remain 0m 35s) Loss: 0.0259(0.0217) \n","EVAL: [600/698] Elapsed 1m 46s (remain 0m 17s) Loss: 0.0190(0.0216) \n","EVAL: [697/698] Elapsed 2m 3s (remain 0m 0s) Loss: 0.0376(0.0218) \n","Epoch 4 - avg_train_loss: 0.0212  avg_val_loss: 0.0218  time: 934s\n","Epoch 4 - Score: 0.8852\n","Epoch 4 - Save Best Score: 0.8852 Model\n","Epoch: [5][0/2877] Elapsed 0m 0s (remain 29m 36s) Loss: 0.0181(0.0181) Grad: 1470.0750  LR: 0.000004  \n","Epoch: [5][100/2877] Elapsed 0m 31s (remain 14m 25s) Loss: 0.0146(0.0197) Grad: 1170.7455  LR: 0.000004  \n","Epoch: [5][200/2877] Elapsed 1m 0s (remain 13m 22s) Loss: 0.0217(0.0201) Grad: 3302.0474  LR: 0.000004  \n","Epoch: [5][300/2877] Elapsed 1m 28s (remain 12m 33s) Loss: 0.0259(0.0202) Grad: 2063.0620  LR: 0.000004  \n","Epoch: [5][400/2877] Elapsed 1m 55s (remain 11m 54s) Loss: 0.0198(0.0206) Grad: 3686.2314  LR: 0.000004  \n","Epoch: [5][500/2877] Elapsed 2m 23s (remain 11m 20s) Loss: 0.0167(0.0206) Grad: 92.6829  LR: 0.000004  \n","Epoch: [5][600/2877] Elapsed 2m 50s (remain 10m 47s) Loss: 0.0281(0.0208) Grad: 397.1604  LR: 0.000004  \n","Epoch: [5][700/2877] Elapsed 3m 18s (remain 10m 15s) Loss: 0.0334(0.0208) Grad: 251.2672  LR: 0.000003  \n","Epoch: [5][800/2877] Elapsed 3m 46s (remain 9m 46s) Loss: 0.0247(0.0208) Grad: 106.5375  LR: 0.000003  \n","Epoch: [5][900/2877] Elapsed 4m 13s (remain 9m 16s) Loss: 0.0126(0.0209) Grad: 1742.2362  LR: 0.000003  \n","Epoch: [5][1000/2877] Elapsed 4m 41s (remain 8m 46s) Loss: 0.0200(0.0208) Grad: 2803.4758  LR: 0.000003  \n","Epoch: [5][1100/2877] Elapsed 5m 8s (remain 8m 18s) Loss: 0.0175(0.0208) Grad: 1296.2661  LR: 0.000003  \n","Epoch: [5][1200/2877] Elapsed 5m 36s (remain 7m 49s) Loss: 0.0265(0.0207) Grad: 339.4559  LR: 0.000003  \n","Epoch: [5][1300/2877] Elapsed 6m 4s (remain 7m 21s) Loss: 0.0184(0.0207) Grad: 4342.3564  LR: 0.000002  \n","Epoch: [5][1400/2877] Elapsed 6m 31s (remain 6m 52s) Loss: 0.0189(0.0208) Grad: 341.2375  LR: 0.000002  \n","Epoch: [5][1500/2877] Elapsed 6m 59s (remain 6m 24s) Loss: 0.0248(0.0208) Grad: 1074.1709  LR: 0.000002  \n","Epoch: [5][1600/2877] Elapsed 7m 27s (remain 5m 56s) Loss: 0.0182(0.0208) Grad: 1178.9067  LR: 0.000002  \n","Epoch: [5][1700/2877] Elapsed 7m 55s (remain 5m 28s) Loss: 0.0131(0.0208) Grad: 2908.2532  LR: 0.000002  \n","Epoch: [5][1800/2877] Elapsed 8m 22s (remain 5m 0s) Loss: 0.0181(0.0209) Grad: 121.5088  LR: 0.000002  \n","Epoch: [5][1900/2877] Elapsed 8m 50s (remain 4m 32s) Loss: 0.0251(0.0209) Grad: 1312.8031  LR: 0.000002  \n","Epoch: [5][2000/2877] Elapsed 9m 18s (remain 4m 4s) Loss: 0.0192(0.0210) Grad: 4002.2478  LR: 0.000001  \n","Epoch: [5][2100/2877] Elapsed 9m 46s (remain 3m 36s) Loss: 0.0124(0.0210) Grad: 488.5926  LR: 0.000001  \n","Epoch: [5][2200/2877] Elapsed 10m 13s (remain 3m 8s) Loss: 0.0300(0.0210) Grad: 24885.9395  LR: 0.000001  \n","Epoch: [5][2300/2877] Elapsed 10m 41s (remain 2m 40s) Loss: 0.0204(0.0210) Grad: 5500.7012  LR: 0.000001  \n","Epoch: [5][2400/2877] Elapsed 11m 9s (remain 2m 12s) Loss: 0.0184(0.0210) Grad: 498.7246  LR: 0.000001  \n","Epoch: [5][2500/2877] Elapsed 11m 37s (remain 1m 44s) Loss: 0.0174(0.0210) Grad: 2494.8740  LR: 0.000001  \n","Epoch: [5][2600/2877] Elapsed 12m 4s (remain 1m 16s) Loss: 0.0125(0.0210) Grad: 552.7496  LR: 0.000000  \n","Epoch: [5][2700/2877] Elapsed 12m 32s (remain 0m 49s) Loss: 0.0314(0.0210) Grad: 320.6632  LR: 0.000000  \n","Epoch: [5][2800/2877] Elapsed 13m 0s (remain 0m 21s) Loss: 0.0195(0.0210) Grad: 101.3243  LR: 0.000000  \n","Epoch: [5][2876/2877] Elapsed 13m 21s (remain 0m 0s) Loss: 0.0316(0.0210) Grad: 319.1992  LR: 0.000000  \n","EVAL: [0/698] Elapsed 0m 0s (remain 5m 48s) Loss: 0.0185(0.0185) \n","EVAL: [100/698] Elapsed 0m 18s (remain 1m 48s) Loss: 0.0183(0.0208) \n","EVAL: [200/698] Elapsed 0m 36s (remain 1m 29s) Loss: 0.0323(0.0225) \n","EVAL: [300/698] Elapsed 0m 53s (remain 1m 10s) Loss: 0.0124(0.0225) \n","EVAL: [400/698] Elapsed 1m 11s (remain 0m 52s) Loss: 0.0347(0.0224) \n","EVAL: [500/698] Elapsed 1m 29s (remain 0m 35s) Loss: 0.0259(0.0220) \n","EVAL: [600/698] Elapsed 1m 46s (remain 0m 17s) Loss: 0.0191(0.0219) \n","EVAL: [697/698] Elapsed 2m 4s (remain 0m 0s) Loss: 0.0376(0.0222) \n","Epoch 5 - avg_train_loss: 0.0210  avg_val_loss: 0.0222  time: 930s\n","Epoch 5 - Score: 0.8872\n","Epoch 5 - Save Best Score: 0.8872 Model\n","========== fold: 4 training ==========\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Load weight from pretrained\n","Epoch: [1][0/2850] Elapsed 0m 0s (remain 37m 36s) Loss: 0.1270(0.1270) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/2850] Elapsed 0m 32s (remain 14m 41s) Loss: 0.0523(0.0955) Grad: 10649.9551  LR: 0.000001  \n","Epoch: [1][200/2850] Elapsed 1m 0s (remain 13m 11s) Loss: 0.0249(0.0657) Grad: 1692.7018  LR: 0.000003  \n","Epoch: [1][300/2850] Elapsed 1m 27s (remain 12m 23s) Loss: 0.0358(0.0536) Grad: 1644.1573  LR: 0.000004  \n","Epoch: [1][400/2850] Elapsed 1m 55s (remain 11m 45s) Loss: 0.0292(0.0468) Grad: 2031.9253  LR: 0.000006  \n","Epoch: [1][500/2850] Elapsed 2m 23s (remain 11m 11s) Loss: 0.0314(0.0425) Grad: 1551.4143  LR: 0.000007  \n","Epoch: [1][600/2850] Elapsed 2m 51s (remain 10m 42s) Loss: 0.0239(0.0395) Grad: 2441.2100  LR: 0.000008  \n","Epoch: [1][700/2850] Elapsed 3m 19s (remain 10m 11s) Loss: 0.0271(0.0373) Grad: 1614.7266  LR: 0.000010  \n","Epoch: [1][800/2850] Elapsed 3m 47s (remain 9m 41s) Loss: 0.0336(0.0355) Grad: 415.7393  LR: 0.000011  \n","Epoch: [1][900/2850] Elapsed 4m 15s (remain 9m 12s) Loss: 0.0399(0.0342) Grad: 3093.3325  LR: 0.000013  \n","Epoch: [1][1000/2850] Elapsed 4m 43s (remain 8m 42s) Loss: 0.0422(0.0331) Grad: 4039.0735  LR: 0.000014  \n","Epoch: [1][1100/2850] Elapsed 5m 10s (remain 8m 13s) Loss: 0.0207(0.0324) Grad: 3393.5725  LR: 0.000015  \n","Epoch: [1][1200/2850] Elapsed 5m 38s (remain 7m 44s) Loss: 0.0158(0.0316) Grad: 915.3104  LR: 0.000017  \n","Epoch: [1][1300/2850] Elapsed 6m 6s (remain 7m 16s) Loss: 0.0170(0.0310) Grad: 1557.0969  LR: 0.000018  \n","Epoch: [1][1400/2850] Elapsed 6m 34s (remain 6m 47s) Loss: 0.0204(0.0305) Grad: 409.8611  LR: 0.000020  \n","Epoch: [1][1500/2850] Elapsed 7m 2s (remain 6m 19s) Loss: 0.0252(0.0300) Grad: 1282.4702  LR: 0.000020  \n","Epoch: [1][1600/2850] Elapsed 7m 29s (remain 5m 50s) Loss: 0.0202(0.0296) Grad: 411.1216  LR: 0.000020  \n","Epoch: [1][1700/2850] Elapsed 7m 57s (remain 5m 22s) Loss: 0.0192(0.0292) Grad: 731.4518  LR: 0.000020  \n","Epoch: [1][1800/2850] Elapsed 8m 25s (remain 4m 54s) Loss: 0.0136(0.0288) Grad: 346.5452  LR: 0.000019  \n","Epoch: [1][1900/2850] Elapsed 8m 53s (remain 4m 26s) Loss: 0.0208(0.0285) Grad: 463.6262  LR: 0.000019  \n","Epoch: [1][2000/2850] Elapsed 9m 21s (remain 3m 58s) Loss: 0.0210(0.0282) Grad: 388.5012  LR: 0.000019  \n","Epoch: [1][2100/2850] Elapsed 9m 48s (remain 3m 29s) Loss: 0.0251(0.0279) Grad: 109.9303  LR: 0.000019  \n","Epoch: [1][2200/2850] Elapsed 10m 16s (remain 3m 1s) Loss: 0.0200(0.0277) Grad: 522.9024  LR: 0.000019  \n","Epoch: [1][2300/2850] Elapsed 10m 44s (remain 2m 33s) Loss: 0.0207(0.0275) Grad: 1363.3248  LR: 0.000019  \n","Epoch: [1][2400/2850] Elapsed 11m 12s (remain 2m 5s) Loss: 0.0138(0.0272) Grad: 487.6121  LR: 0.000018  \n","Epoch: [1][2500/2850] Elapsed 11m 40s (remain 1m 37s) Loss: 0.0203(0.0271) Grad: 1079.7605  LR: 0.000018  \n","Epoch: [1][2600/2850] Elapsed 12m 8s (remain 1m 9s) Loss: 0.0336(0.0269) Grad: 2189.3311  LR: 0.000018  \n","Epoch: [1][2700/2850] Elapsed 12m 35s (remain 0m 41s) Loss: 0.0318(0.0268) Grad: 703.2268  LR: 0.000018  \n","Epoch: [1][2800/2850] Elapsed 13m 3s (remain 0m 13s) Loss: 0.0144(0.0266) Grad: 606.3605  LR: 0.000018  \n","Epoch: [1][2849/2850] Elapsed 13m 17s (remain 0m 0s) Loss: 0.0194(0.0266) Grad: 350.6834  LR: 0.000018  \n","EVAL: [0/725] Elapsed 0m 0s (remain 6m 26s) Loss: 0.0208(0.0208) \n","EVAL: [100/725] Elapsed 0m 18s (remain 1m 53s) Loss: 0.0167(0.0205) \n","EVAL: [200/725] Elapsed 0m 36s (remain 1m 35s) Loss: 0.0227(0.0222) \n","EVAL: [300/725] Elapsed 0m 54s (remain 1m 16s) Loss: 0.0190(0.0216) \n","EVAL: [400/725] Elapsed 1m 11s (remain 0m 58s) Loss: 0.0285(0.0218) \n","EVAL: [500/725] Elapsed 1m 29s (remain 0m 40s) Loss: 0.0205(0.0216) \n","EVAL: [600/725] Elapsed 1m 47s (remain 0m 22s) Loss: 0.0127(0.0216) \n","EVAL: [700/725] Elapsed 2m 5s (remain 0m 4s) Loss: 0.0324(0.0218) \n","EVAL: [724/725] Elapsed 2m 9s (remain 0m 0s) Loss: 0.0258(0.0219) \n","Epoch 1 - avg_train_loss: 0.0266  avg_val_loss: 0.0219  time: 931s\n","Epoch 1 - Score: 0.8363\n","Epoch 1 - Save Best Score: 0.8363 Model\n","Epoch: [2][0/2850] Elapsed 0m 0s (remain 31m 34s) Loss: 0.0240(0.0240) Grad: 1853.0400  LR: 0.000018  \n","Epoch: [2][100/2850] Elapsed 0m 31s (remain 14m 10s) Loss: 0.0190(0.0226) Grad: 7611.4561  LR: 0.000018  \n","Epoch: [2][200/2850] Elapsed 1m 0s (remain 13m 15s) Loss: 0.0195(0.0224) Grad: 4413.2256  LR: 0.000017  \n","Epoch: [2][300/2850] Elapsed 1m 28s (remain 12m 26s) Loss: 0.0254(0.0222) Grad: 1899.5381  LR: 0.000017  \n","Epoch: [2][400/2850] Elapsed 1m 56s (remain 11m 50s) Loss: 0.0190(0.0224) Grad: 1707.7386  LR: 0.000017  \n","Epoch: [2][500/2850] Elapsed 2m 24s (remain 11m 15s) Loss: 0.0181(0.0223) Grad: 1216.1497  LR: 0.000017  \n","Epoch: [2][600/2850] Elapsed 2m 51s (remain 10m 43s) Loss: 0.0257(0.0223) Grad: 6648.8516  LR: 0.000017  \n","Epoch: [2][700/2850] Elapsed 3m 19s (remain 10m 11s) Loss: 0.0136(0.0224) Grad: 2514.4753  LR: 0.000017  \n","Epoch: [2][800/2850] Elapsed 3m 47s (remain 9m 41s) Loss: 0.0231(0.0224) Grad: 15259.5352  LR: 0.000017  \n","Epoch: [2][900/2850] Elapsed 4m 15s (remain 9m 11s) Loss: 0.0183(0.0224) Grad: 873.3759  LR: 0.000016  \n","Epoch: [2][1000/2850] Elapsed 4m 42s (remain 8m 42s) Loss: 0.0380(0.0223) Grad: 25171.9121  LR: 0.000016  \n","Epoch: [2][1100/2850] Elapsed 5m 10s (remain 8m 13s) Loss: 0.0239(0.0223) Grad: 16119.4648  LR: 0.000016  \n","Epoch: [2][1200/2850] Elapsed 5m 38s (remain 7m 44s) Loss: 0.0259(0.0223) Grad: 1937.7059  LR: 0.000016  \n","Epoch: [2][1300/2850] Elapsed 6m 5s (remain 7m 15s) Loss: 0.0200(0.0223) Grad: 2109.2988  LR: 0.000016  \n","Epoch: [2][1400/2850] Elapsed 6m 33s (remain 6m 47s) Loss: 0.0199(0.0222) Grad: 652.2631  LR: 0.000016  \n","Epoch: [2][1500/2850] Elapsed 7m 1s (remain 6m 18s) Loss: 0.0166(0.0221) Grad: 22914.1484  LR: 0.000015  \n","Epoch: [2][1600/2850] Elapsed 7m 28s (remain 5m 50s) Loss: 0.0125(0.0220) Grad: 1020.3237  LR: 0.000015  \n","Epoch: [2][1700/2850] Elapsed 7m 56s (remain 5m 21s) Loss: 0.0327(0.0220) Grad: 1648.2727  LR: 0.000015  \n","Epoch: [2][1800/2850] Elapsed 8m 24s (remain 4m 53s) Loss: 0.0163(0.0220) Grad: 22473.9277  LR: 0.000015  \n","Epoch: [2][1900/2850] Elapsed 8m 51s (remain 4m 25s) Loss: 0.0315(0.0219) Grad: 1110.6340  LR: 0.000015  \n","Epoch: [2][2000/2850] Elapsed 9m 19s (remain 3m 57s) Loss: 0.0262(0.0219) Grad: 9988.7051  LR: 0.000015  \n","Epoch: [2][2100/2850] Elapsed 9m 46s (remain 3m 29s) Loss: 0.0187(0.0219) Grad: 4952.2095  LR: 0.000015  \n","Epoch: [2][2200/2850] Elapsed 10m 14s (remain 3m 1s) Loss: 0.0290(0.0219) Grad: 3327.6997  LR: 0.000014  \n","Epoch: [2][2300/2850] Elapsed 10m 42s (remain 2m 33s) Loss: 0.0136(0.0219) Grad: 3762.0862  LR: 0.000014  \n","Epoch: [2][2400/2850] Elapsed 11m 9s (remain 2m 5s) Loss: 0.0264(0.0218) Grad: 23555.7949  LR: 0.000014  \n","Epoch: [2][2500/2850] Elapsed 11m 37s (remain 1m 37s) Loss: 0.0406(0.0218) Grad: 48756.1680  LR: 0.000014  \n","Epoch: [2][2600/2850] Elapsed 12m 4s (remain 1m 9s) Loss: 0.0206(0.0218) Grad: 4415.8408  LR: 0.000014  \n","Epoch: [2][2700/2850] Elapsed 12m 32s (remain 0m 41s) Loss: 0.0183(0.0217) Grad: 1827.6342  LR: 0.000014  \n","Epoch: [2][2800/2850] Elapsed 12m 59s (remain 0m 13s) Loss: 0.0202(0.0217) Grad: 3432.5933  LR: 0.000013  \n","Epoch: [2][2849/2850] Elapsed 13m 13s (remain 0m 0s) Loss: 0.0210(0.0217) Grad: 9068.4395  LR: 0.000013  \n","EVAL: [0/725] Elapsed 0m 0s (remain 5m 51s) Loss: 0.0214(0.0214) \n","EVAL: [100/725] Elapsed 0m 18s (remain 1m 51s) Loss: 0.0166(0.0207) \n","EVAL: [200/725] Elapsed 0m 35s (remain 1m 33s) Loss: 0.0221(0.0223) \n","EVAL: [300/725] Elapsed 0m 53s (remain 1m 15s) Loss: 0.0187(0.0217) \n","EVAL: [400/725] Elapsed 1m 11s (remain 0m 57s) Loss: 0.0382(0.0220) \n","EVAL: [500/725] Elapsed 1m 28s (remain 0m 39s) Loss: 0.0217(0.0218) \n","EVAL: [600/725] Elapsed 1m 46s (remain 0m 21s) Loss: 0.0154(0.0218) \n","EVAL: [700/725] Elapsed 2m 4s (remain 0m 4s) Loss: 0.0314(0.0219) \n","EVAL: [724/725] Elapsed 2m 8s (remain 0m 0s) Loss: 0.0227(0.0220) \n","Epoch 2 - avg_train_loss: 0.0217  avg_val_loss: 0.0220  time: 926s\n","Epoch 2 - Score: 0.8720\n","Epoch 2 - Save Best Score: 0.8720 Model\n","Epoch: [3][0/2850] Elapsed 0m 0s (remain 30m 4s) Loss: 0.0337(0.0337) Grad: 7860.5454  LR: 0.000013  \n","Epoch: [3][100/2850] Elapsed 0m 31s (remain 14m 12s) Loss: 0.0128(0.0217) Grad: 1507.3799  LR: 0.000013  \n","Epoch: [3][200/2850] Elapsed 1m 0s (remain 13m 11s) Loss: 0.0198(0.0214) Grad: 3268.7271  LR: 0.000013  \n","Epoch: [3][300/2850] Elapsed 1m 27s (remain 12m 21s) Loss: 0.0184(0.0215) Grad: 1432.5736  LR: 0.000013  \n","Epoch: [3][400/2850] Elapsed 1m 55s (remain 11m 42s) Loss: 0.0220(0.0217) Grad: 9019.4482  LR: 0.000013  \n","Epoch: [3][500/2850] Elapsed 2m 22s (remain 11m 8s) Loss: 0.0311(0.0217) Grad: 2413.7283  LR: 0.000013  \n","Epoch: [3][600/2850] Elapsed 2m 50s (remain 10m 36s) Loss: 0.0133(0.0217) Grad: 3152.5344  LR: 0.000012  \n","Epoch: [3][700/2850] Elapsed 3m 17s (remain 10m 5s) Loss: 0.0159(0.0218) Grad: 2417.0186  LR: 0.000012  \n","Epoch: [3][800/2850] Elapsed 3m 45s (remain 9m 35s) Loss: 0.0125(0.0218) Grad: 393.8505  LR: 0.000012  \n","Epoch: [3][900/2850] Elapsed 4m 12s (remain 9m 6s) Loss: 0.0259(0.0216) Grad: 2938.8584  LR: 0.000012  \n","Epoch: [3][1000/2850] Elapsed 4m 40s (remain 8m 37s) Loss: 0.0180(0.0215) Grad: 2476.8491  LR: 0.000012  \n","Epoch: [3][1100/2850] Elapsed 5m 7s (remain 8m 8s) Loss: 0.0124(0.0215) Grad: 284.8060  LR: 0.000012  \n","Epoch: [3][1200/2850] Elapsed 5m 35s (remain 7m 40s) Loss: 0.0201(0.0215) Grad: 3033.1108  LR: 0.000011  \n","Epoch: [3][1300/2850] Elapsed 6m 2s (remain 7m 11s) Loss: 0.0137(0.0215) Grad: 7582.7544  LR: 0.000011  \n","Epoch: [3][1400/2850] Elapsed 6m 30s (remain 6m 43s) Loss: 0.0258(0.0215) Grad: 24127.4727  LR: 0.000011  \n","Epoch: [3][1500/2850] Elapsed 6m 57s (remain 6m 15s) Loss: 0.0249(0.0215) Grad: 1168.1293  LR: 0.000011  \n","Epoch: [3][1600/2850] Elapsed 7m 25s (remain 5m 47s) Loss: 0.0125(0.0215) Grad: 1485.9749  LR: 0.000011  \n","Epoch: [3][1700/2850] Elapsed 7m 52s (remain 5m 19s) Loss: 0.0134(0.0215) Grad: 1905.7593  LR: 0.000011  \n","Epoch: [3][1800/2850] Elapsed 8m 20s (remain 4m 51s) Loss: 0.0249(0.0215) Grad: 17251.6543  LR: 0.000011  \n","Epoch: [3][1900/2850] Elapsed 8m 48s (remain 4m 23s) Loss: 0.0241(0.0214) Grad: 656.7123  LR: 0.000010  \n","Epoch: [3][2000/2850] Elapsed 9m 15s (remain 3m 55s) Loss: 0.0236(0.0214) Grad: 1204.6365  LR: 0.000010  \n","Epoch: [3][2100/2850] Elapsed 9m 43s (remain 3m 27s) Loss: 0.0189(0.0214) Grad: 1144.6698  LR: 0.000010  \n","Epoch: [3][2200/2850] Elapsed 10m 10s (remain 3m 0s) Loss: 0.0124(0.0214) Grad: 315.9782  LR: 0.000010  \n","Epoch: [3][2300/2850] Elapsed 10m 38s (remain 2m 32s) Loss: 0.0289(0.0215) Grad: 8093.9834  LR: 0.000010  \n","Epoch: [3][2400/2850] Elapsed 11m 5s (remain 2m 4s) Loss: 0.0297(0.0215) Grad: 9518.8848  LR: 0.000010  \n","Epoch: [3][2500/2850] Elapsed 11m 33s (remain 1m 36s) Loss: 0.0336(0.0215) Grad: 12115.2920  LR: 0.000009  \n","Epoch: [3][2600/2850] Elapsed 12m 0s (remain 1m 9s) Loss: 0.0275(0.0215) Grad: 10287.9131  LR: 0.000009  \n","Epoch: [3][2700/2850] Elapsed 12m 28s (remain 0m 41s) Loss: 0.0193(0.0214) Grad: 3083.5188  LR: 0.000009  \n","Epoch: [3][2800/2850] Elapsed 12m 56s (remain 0m 13s) Loss: 0.0124(0.0214) Grad: 670.8328  LR: 0.000009  \n","Epoch: [3][2849/2850] Elapsed 13m 9s (remain 0m 0s) Loss: 0.0194(0.0214) Grad: 6468.9478  LR: 0.000009  \n","EVAL: [0/725] Elapsed 0m 0s (remain 5m 48s) Loss: 0.0201(0.0201) \n","EVAL: [100/725] Elapsed 0m 18s (remain 1m 51s) Loss: 0.0165(0.0208) \n","EVAL: [200/725] Elapsed 0m 35s (remain 1m 33s) Loss: 0.0212(0.0224) \n","EVAL: [300/725] Elapsed 0m 53s (remain 1m 15s) Loss: 0.0199(0.0218) \n","EVAL: [400/725] Elapsed 1m 11s (remain 0m 57s) Loss: 0.0332(0.0221) \n","EVAL: [500/725] Elapsed 1m 28s (remain 0m 39s) Loss: 0.0226(0.0218) \n","EVAL: [600/725] Elapsed 1m 46s (remain 0m 22s) Loss: 0.0148(0.0218) \n","EVAL: [700/725] Elapsed 2m 4s (remain 0m 4s) Loss: 0.0313(0.0220) \n","EVAL: [724/725] Elapsed 2m 8s (remain 0m 0s) Loss: 0.0244(0.0220) \n","Epoch 3 - avg_train_loss: 0.0214  avg_val_loss: 0.0220  time: 922s\n","Epoch 3 - Score: 0.8760\n","Epoch 3 - Save Best Score: 0.8760 Model\n","Epoch: [4][0/2850] Elapsed 0m 0s (remain 30m 21s) Loss: 0.0180(0.0180) Grad: 357.6933  LR: 0.000009  \n","Epoch: [4][100/2850] Elapsed 0m 30s (remain 14m 1s) Loss: 0.0245(0.0207) Grad: 4335.5669  LR: 0.000009  \n","Epoch: [4][200/2850] Elapsed 0m 59s (remain 13m 6s) Loss: 0.0378(0.0213) Grad: 10316.9414  LR: 0.000009  \n","Epoch: [4][300/2850] Elapsed 1m 27s (remain 12m 18s) Loss: 0.0194(0.0211) Grad: 632.4293  LR: 0.000008  \n","Epoch: [4][400/2850] Elapsed 1m 54s (remain 11m 40s) Loss: 0.0214(0.0214) Grad: 11689.1152  LR: 0.000008  \n","Epoch: [4][500/2850] Elapsed 2m 22s (remain 11m 7s) Loss: 0.0244(0.0212) Grad: 653.8145  LR: 0.000008  \n","Epoch: [4][600/2850] Elapsed 2m 49s (remain 10m 35s) Loss: 0.0126(0.0212) Grad: 1997.9313  LR: 0.000008  \n","Epoch: [4][700/2850] Elapsed 3m 17s (remain 10m 5s) Loss: 0.0202(0.0212) Grad: 2738.7800  LR: 0.000008  \n","Epoch: [4][800/2850] Elapsed 3m 44s (remain 9m 35s) Loss: 0.0203(0.0212) Grad: 3429.6499  LR: 0.000008  \n","Epoch: [4][900/2850] Elapsed 4m 12s (remain 9m 5s) Loss: 0.0127(0.0212) Grad: 1094.6442  LR: 0.000007  \n","Epoch: [4][1000/2850] Elapsed 4m 39s (remain 8m 36s) Loss: 0.0259(0.0211) Grad: 3168.8298  LR: 0.000007  \n","Epoch: [4][1100/2850] Elapsed 5m 7s (remain 8m 8s) Loss: 0.0229(0.0211) Grad: 1816.1436  LR: 0.000007  \n","Epoch: [4][1200/2850] Elapsed 5m 35s (remain 7m 40s) Loss: 0.0184(0.0211) Grad: 663.8350  LR: 0.000007  \n","Epoch: [4][1300/2850] Elapsed 6m 2s (remain 7m 11s) Loss: 0.0161(0.0211) Grad: 135.2621  LR: 0.000007  \n","Epoch: [4][1400/2850] Elapsed 6m 30s (remain 6m 43s) Loss: 0.0188(0.0210) Grad: 277.2767  LR: 0.000007  \n","Epoch: [4][1500/2850] Elapsed 6m 57s (remain 6m 15s) Loss: 0.0236(0.0210) Grad: 211.5778  LR: 0.000007  \n","Epoch: [4][1600/2850] Elapsed 7m 25s (remain 5m 47s) Loss: 0.0239(0.0210) Grad: 1985.7444  LR: 0.000006  \n","Epoch: [4][1700/2850] Elapsed 7m 52s (remain 5m 19s) Loss: 0.0256(0.0211) Grad: 120.6304  LR: 0.000006  \n","Epoch: [4][1800/2850] Elapsed 8m 20s (remain 4m 51s) Loss: 0.0208(0.0212) Grad: 5316.9814  LR: 0.000006  \n","Epoch: [4][1900/2850] Elapsed 8m 47s (remain 4m 23s) Loss: 0.0129(0.0211) Grad: 4805.0146  LR: 0.000006  \n","Epoch: [4][2000/2850] Elapsed 9m 15s (remain 3m 55s) Loss: 0.0132(0.0211) Grad: 3032.3555  LR: 0.000006  \n","Epoch: [4][2100/2850] Elapsed 9m 42s (remain 3m 27s) Loss: 0.0242(0.0210) Grad: 646.8483  LR: 0.000006  \n","Epoch: [4][2200/2850] Elapsed 10m 9s (remain 2m 59s) Loss: 0.0226(0.0211) Grad: 658.7473  LR: 0.000005  \n","Epoch: [4][2300/2850] Elapsed 10m 37s (remain 2m 32s) Loss: 0.0195(0.0211) Grad: 9909.9072  LR: 0.000005  \n","Epoch: [4][2400/2850] Elapsed 11m 5s (remain 2m 4s) Loss: 0.0204(0.0211) Grad: 2526.5442  LR: 0.000005  \n","Epoch: [4][2500/2850] Elapsed 11m 32s (remain 1m 36s) Loss: 0.0205(0.0211) Grad: 8806.8643  LR: 0.000005  \n","Epoch: [4][2600/2850] Elapsed 11m 59s (remain 1m 8s) Loss: 0.0200(0.0211) Grad: 2875.5466  LR: 0.000005  \n","Epoch: [4][2700/2850] Elapsed 12m 27s (remain 0m 41s) Loss: 0.0257(0.0211) Grad: 685.0278  LR: 0.000005  \n","Epoch: [4][2800/2850] Elapsed 12m 54s (remain 0m 13s) Loss: 0.0281(0.0211) Grad: 4277.6943  LR: 0.000005  \n","Epoch: [4][2849/2850] Elapsed 13m 8s (remain 0m 0s) Loss: 0.0206(0.0211) Grad: 947.1277  LR: 0.000004  \n","EVAL: [0/725] Elapsed 0m 0s (remain 5m 55s) Loss: 0.0241(0.0241) \n","EVAL: [100/725] Elapsed 0m 18s (remain 1m 51s) Loss: 0.0169(0.0211) \n","EVAL: [200/725] Elapsed 0m 35s (remain 1m 33s) Loss: 0.0216(0.0227) \n","EVAL: [300/725] Elapsed 0m 53s (remain 1m 14s) Loss: 0.0219(0.0220) \n","EVAL: [400/725] Elapsed 1m 10s (remain 0m 57s) Loss: 0.0281(0.0222) \n","EVAL: [500/725] Elapsed 1m 28s (remain 0m 39s) Loss: 0.0216(0.0220) \n","EVAL: [600/725] Elapsed 1m 46s (remain 0m 21s) Loss: 0.0152(0.0220) \n","EVAL: [700/725] Elapsed 2m 3s (remain 0m 4s) Loss: 0.0313(0.0222) \n","EVAL: [724/725] Elapsed 2m 8s (remain 0m 0s) Loss: 0.0243(0.0222) \n","Epoch 4 - avg_train_loss: 0.0211  avg_val_loss: 0.0222  time: 921s\n","Epoch 4 - Score: 0.8762\n","Epoch 4 - Save Best Score: 0.8762 Model\n","Epoch: [5][0/2850] Elapsed 0m 0s (remain 29m 23s) Loss: 0.0195(0.0195) Grad: 1413.2382  LR: 0.000004  \n","Epoch: [5][100/2850] Elapsed 0m 30s (remain 14m 2s) Loss: 0.0153(0.0203) Grad: 39620.1836  LR: 0.000004  \n","Epoch: [5][200/2850] Elapsed 0m 59s (remain 13m 1s) Loss: 0.0195(0.0202) Grad: 1339.4399  LR: 0.000004  \n","Epoch: [5][300/2850] Elapsed 1m 27s (remain 12m 17s) Loss: 0.0203(0.0203) Grad: 5623.1724  LR: 0.000004  \n","Epoch: [5][400/2850] Elapsed 1m 54s (remain 11m 39s) Loss: 0.0182(0.0204) Grad: 938.5057  LR: 0.000004  \n","Epoch: [5][500/2850] Elapsed 2m 22s (remain 11m 5s) Loss: 0.0128(0.0207) Grad: 2933.6646  LR: 0.000004  \n","Epoch: [5][600/2850] Elapsed 2m 49s (remain 10m 33s) Loss: 0.0196(0.0208) Grad: 3547.2590  LR: 0.000004  \n","Epoch: [5][700/2850] Elapsed 3m 16s (remain 10m 3s) Loss: 0.0253(0.0209) Grad: 439.9382  LR: 0.000003  \n","Epoch: [5][800/2850] Elapsed 3m 44s (remain 9m 34s) Loss: 0.0183(0.0210) Grad: 600.5049  LR: 0.000003  \n","Epoch: [5][900/2850] Elapsed 4m 12s (remain 9m 5s) Loss: 0.0193(0.0211) Grad: 249.4295  LR: 0.000003  \n","Epoch: [5][1000/2850] Elapsed 4m 39s (remain 8m 36s) Loss: 0.0123(0.0211) Grad: 92.6292  LR: 0.000003  \n","Epoch: [5][1100/2850] Elapsed 5m 6s (remain 8m 7s) Loss: 0.0257(0.0210) Grad: 50561.1133  LR: 0.000003  \n","Epoch: [5][1200/2850] Elapsed 5m 34s (remain 7m 39s) Loss: 0.0137(0.0210) Grad: 2714.6570  LR: 0.000003  \n","Epoch: [5][1300/2850] Elapsed 6m 2s (remain 7m 11s) Loss: 0.0297(0.0209) Grad: 74.0070  LR: 0.000002  \n","Epoch: [5][1400/2850] Elapsed 6m 29s (remain 6m 42s) Loss: 0.0125(0.0210) Grad: 852.8417  LR: 0.000002  \n","Epoch: [5][1500/2850] Elapsed 6m 57s (remain 6m 14s) Loss: 0.0242(0.0210) Grad: 564.4170  LR: 0.000002  \n","Epoch: [5][1600/2850] Elapsed 7m 24s (remain 5m 46s) Loss: 0.0295(0.0210) Grad: 8898.9219  LR: 0.000002  \n","Epoch: [5][1700/2850] Elapsed 7m 52s (remain 5m 18s) Loss: 0.0321(0.0211) Grad: 38467.1914  LR: 0.000002  \n","Epoch: [5][1800/2850] Elapsed 8m 19s (remain 4m 50s) Loss: 0.0253(0.0211) Grad: 40633.7148  LR: 0.000002  \n","Epoch: [5][1900/2850] Elapsed 8m 46s (remain 4m 23s) Loss: 0.0323(0.0211) Grad: 10371.5518  LR: 0.000001  \n","Epoch: [5][2000/2850] Elapsed 9m 14s (remain 3m 55s) Loss: 0.0189(0.0211) Grad: 246.8068  LR: 0.000001  \n","Epoch: [5][2100/2850] Elapsed 9m 42s (remain 3m 27s) Loss: 0.0124(0.0211) Grad: 153.3333  LR: 0.000001  \n","Epoch: [5][2200/2850] Elapsed 10m 9s (remain 2m 59s) Loss: 0.0229(0.0211) Grad: 340.9343  LR: 0.000001  \n","Epoch: [5][2300/2850] Elapsed 10m 36s (remain 2m 31s) Loss: 0.0192(0.0211) Grad: 612.4095  LR: 0.000001  \n","Epoch: [5][2400/2850] Elapsed 11m 4s (remain 2m 4s) Loss: 0.0233(0.0210) Grad: 203.3562  LR: 0.000001  \n","Epoch: [5][2500/2850] Elapsed 11m 31s (remain 1m 36s) Loss: 0.0387(0.0210) Grad: 11824.0781  LR: 0.000001  \n","Epoch: [5][2600/2850] Elapsed 11m 59s (remain 1m 8s) Loss: 0.0242(0.0210) Grad: 2928.8730  LR: 0.000000  \n","Epoch: [5][2700/2850] Elapsed 12m 26s (remain 0m 41s) Loss: 0.0299(0.0210) Grad: 227.5727  LR: 0.000000  \n","Epoch: [5][2800/2850] Elapsed 12m 54s (remain 0m 13s) Loss: 0.0128(0.0210) Grad: 3828.4260  LR: 0.000000  \n","Epoch: [5][2849/2850] Elapsed 13m 8s (remain 0m 0s) Loss: 0.0123(0.0210) Grad: 1039.0668  LR: 0.000000  \n","EVAL: [0/725] Elapsed 0m 0s (remain 5m 57s) Loss: 0.0206(0.0206) \n","EVAL: [100/725] Elapsed 0m 18s (remain 1m 52s) Loss: 0.0167(0.0213) \n","EVAL: [200/725] Elapsed 0m 36s (remain 1m 33s) Loss: 0.0214(0.0229) \n","EVAL: [300/725] Elapsed 0m 53s (remain 1m 15s) Loss: 0.0204(0.0222) \n","EVAL: [400/725] Elapsed 1m 11s (remain 0m 57s) Loss: 0.0231(0.0224) \n","EVAL: [500/725] Elapsed 1m 28s (remain 0m 39s) Loss: 0.0222(0.0222) \n","EVAL: [600/725] Elapsed 1m 46s (remain 0m 21s) Loss: 0.0153(0.0222) \n","EVAL: [700/725] Elapsed 2m 4s (remain 0m 4s) Loss: 0.0313(0.0224) \n","EVAL: [724/725] Elapsed 2m 8s (remain 0m 0s) Loss: 0.0241(0.0224) \n","Epoch 5 - avg_train_loss: 0.0210  avg_val_loss: 0.0224  time: 920s\n","Epoch 5 - Score: 0.8780\n","Epoch 5 - Save Best Score: 0.8780 Model\n","Best thres: 0.5, Score: 0.8814\n","Best thres: 0.48515625, Score: 0.8815\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Load weight from pretrained\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0bda6d6d507b48cda9e8baf73e7eccd0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Load weight from pretrained\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d15734a6fc6d45dfad17412d43a23f4f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Load weight from pretrained\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cb9564bab53541f2b87980f73b231529","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Exception ignored in: \u003cfunction _ConnectionBase.__del__ at 0x7f3240d8a3b0\u003e\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 132, in __del__\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Load weight from pretrained\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"50e113cff4c14961a63f13eac1cb16cf","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Load weight from pretrained\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f33610b29d2c438ba65aca519d2592b4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n"]}],"source":["if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"name":"nbme-exp025.ipynb","provenance":[{"file_id":"1hTEk26Dv4lh67pdHGaEsQ5C3Lpm9St9B","timestamp":1646560707666},{"file_id":"14l7vjaEJdKkFlJXP9EmrgKriPn4mqCbY","timestamp":1646539060597}],"version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"papermill":{"default_parameters":{},"duration":641.572862,"end_time":"2022-02-27T11:39:50.972497","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-02-27T11:29:09.399635","version":"2.3.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0ae152f2ba9a41e1bc6ae6e930cb6512":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7926bf222f2444aeb5bbfa93dca4f3c5","max":143,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b914edd0b2fc45d08b296d8830f83942","value":143}},"0bda6d6d507b48cda9e8baf73e7eccd0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_52ea274f58e84db3bde54d2098f10c5a","IPY_MODEL_a289e079c6da49e893dd5f4588c0cbe7","IPY_MODEL_19183a473e0d43ddbf3ebf16e8abb95d"],"layout":"IPY_MODEL_1c9ff773356b4586ac00a3573456ec3b"}},"0c2efcb84258439b9b635ea9ed2a1dc4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0c7dcdda2dbb4cfc82487cbdf47d434f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa3ba5674e414ddea193fe3ac6bcb643","placeholder":"​","style":"IPY_MODEL_99f66569599b47f5b9eac2c962222512","value":" 833M/833M [00:16\u0026lt;00:00, 55.8MB/s]"}},"13823d3db32f4048ac0f42a22fd57cc5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ab1a1b8180d484597b53d32ac29fdeb","placeholder":"​","style":"IPY_MODEL_c35ca85c68e44706b5f182cd14867610","value":"Downloading: 100%"}},"15422d03ee4c4ce6b046ca57025a6dc2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"189ef460143d4a0a99f9e50bc682e1f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2475ca87d7df4aeea635539ca349f518","placeholder":"​","style":"IPY_MODEL_fcd3ef3530dc4739a7ab4f83e9155486","value":" 2.35M/2.35M [00:00\u0026lt;00:00, 8.96MB/s]"}},"19183a473e0d43ddbf3ebf16e8abb95d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e8f40effa294ca38b895018e1814034","placeholder":"​","style":"IPY_MODEL_58e97d64329845d1a84fe22f50457776","value":" 2/2 [00:01\u0026lt;00:00,  1.24s/it]"}},"1ab1a1b8180d484597b53d32ac29fdeb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c697af8ec354926acaeb0f090dfb5a3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c9ff773356b4586ac00a3573456ec3b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ce6ee9d297b4f3c8206d29a32cb80db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4840d9f6ab47453bbe01e3f4045be0b9","max":873673253,"min":0,"orientation":"horizontal","style":"IPY_MODEL_40b9b38120d14128a96d5dd9faae75ba","value":873673253}},"1e11a8bfcb914ccab4f1ac376c149ef9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"230902df54e644aab92e67378339708a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2475ca87d7df4aeea635539ca349f518":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"286be918d66748e399f0f8ced6577a27":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29ada69b367b465d937f80b4db9e5bb7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e34205df4e5491b84fd7f38c64234d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"335114d6d8aa431aa2a672ade88a9e78":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"342da89ce3cc4d019a74ffeb3f672761":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_13823d3db32f4048ac0f42a22fd57cc5","IPY_MODEL_70eba3b0120b4d5981dba0479b6e7273","IPY_MODEL_7279208346434faca14d2e4b79c605b4"],"layout":"IPY_MODEL_82b3d171ba1c45ffa2d0e56f727ce447"}},"342e7bf10e9748b898efae0ca7d4eede":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3692a814663240dbadd291c1c9989582":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"372048c026f4425db5718341dfb514d9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37512de62138474da8a3e9a3c9ad4fa8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ea82b58452f4bbfb30b921600f20fdc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40b9b38120d14128a96d5dd9faae75ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4496907e454e4acca14ee4481ca47358":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46c6c7bc8ac5490fba6a7622c67d0a98":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4840d9f6ab47453bbe01e3f4045be0b9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48a77a6e6d9e4c5f99996610aafd1267":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ad29e70da0946bda10db0611c8cdd27":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b24e173ddc5f473f9c4befa2cf07e2c4","max":42146,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fce27e25ea724935a99ba7436ac82461","value":42146}},"4d9b67f7425341dcab6af60b3a924d8c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4dc78aaceb304b0bbce94d5167d75df7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c147f1171c7844af82462579bf874334","IPY_MODEL_4ad29e70da0946bda10db0611c8cdd27","IPY_MODEL_57308d87f47f4094b03634ad093e31d8"],"layout":"IPY_MODEL_b2b5846913bc41b489420f1a3e649fe9"}},"4e8f40effa294ca38b895018e1814034":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"516edc1d66f24695b6769fb01937a820":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52ea274f58e84db3bde54d2098f10c5a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c697af8ec354926acaeb0f090dfb5a3","placeholder":"​","style":"IPY_MODEL_15422d03ee4c4ce6b046ca57025a6dc2","value":"100%"}},"57308d87f47f4094b03634ad093e31d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4addf27161044b6bc1919716cbfa9cd","placeholder":"​","style":"IPY_MODEL_d0a449079d024483a8f3d96ceadc86eb","value":" 42146/42146 [00:30\u0026lt;00:00, 1902.96it/s]"}},"58e97d64329845d1a84fe22f50457776":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5961633b4f5c4ba0ac418eafb0b8dea1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_46c6c7bc8ac5490fba6a7622c67d0a98","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_335114d6d8aa431aa2a672ade88a9e78","value":2}},"5b84e261e46b4af5a186ff4a22f3e1db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e9080b1f5ea4b96a23fc012a6e7de05":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"61eebc9e66c642e6b16ca8c47be9dcb7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66985436eab642de96ec07b7effd864c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_752d6229bd824059b7137073c92579f0","IPY_MODEL_725f957bc1484f0da1f32e4b5419ab94","IPY_MODEL_189ef460143d4a0a99f9e50bc682e1f4"],"layout":"IPY_MODEL_230902df54e644aab92e67378339708a"}},"6a315444c1f3439e89b14e0f31787f67":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b4cc9865b894bb781ffe942410200b0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cd2c9e6d55a4a82bb9c446e45f35cb0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d86b10657c3e4fabbc1646c2d8c98ab6","placeholder":"​","style":"IPY_MODEL_0c2efcb84258439b9b635ea9ed2a1dc4","value":"Downloading: 100%"}},"70eba3b0120b4d5981dba0479b6e7273":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bbe7f92ae1ca4569a48f16c18cbc6c4d","max":52,"min":0,"orientation":"horizontal","style":"IPY_MODEL_916b33a7058e4a4aa96377ba249df4ab","value":52}},"725f957bc1484f0da1f32e4b5419ab94":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_adc9defd4f8742f6bae2281e230101b8","max":2464616,"min":0,"orientation":"horizontal","style":"IPY_MODEL_342e7bf10e9748b898efae0ca7d4eede","value":2464616}},"7279208346434faca14d2e4b79c605b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8fd048971284f77ba5e32607181e9af","placeholder":"​","style":"IPY_MODEL_5e9080b1f5ea4b96a23fc012a6e7de05","value":" 52.0/52.0 [00:00\u0026lt;00:00, 537B/s]"}},"752d6229bd824059b7137073c92579f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_61eebc9e66c642e6b16ca8c47be9dcb7","placeholder":"​","style":"IPY_MODEL_fdf5a9f3d47e4f3c804aa43edf8d52f0","value":"Downloading: 100%"}},"770b871fbf634867a41f3f3dc8ca5ff9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_37512de62138474da8a3e9a3c9ad4fa8","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_84b92e6830434bb0915377f383fe978f","value":2}},"77b8b03dcc6b442e85c61c4319bee36f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7926bf222f2444aeb5bbfa93dca4f3c5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c33450bde3e4f1cbf064dc9d35466d5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ce6ebb60bc54f37a5792deabc021f44":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8702836cefe6410eaa489e0fdcfd7248","placeholder":"​","style":"IPY_MODEL_7c33450bde3e4f1cbf064dc9d35466d5","value":" 2/2 [00:01\u0026lt;00:00,  1.27s/it]"}},"81d7cf23e7cd404581a8795e8536688a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a315444c1f3439e89b14e0f31787f67","max":580,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1e11a8bfcb914ccab4f1ac376c149ef9","value":580}},"82b3d171ba1c45ffa2d0e56f727ce447":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8319902bd2da4bb1a517f85ab8f9d0c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3692a814663240dbadd291c1c9989582","placeholder":"​","style":"IPY_MODEL_5b84e261e46b4af5a186ff4a22f3e1db","value":" 143/143 [00:00\u0026lt;00:00, 2438.28it/s]"}},"8336e465363e44ae9a576849f60a9712":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84b92e6830434bb0915377f383fe978f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8702836cefe6410eaa489e0fdcfd7248":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"916b33a7058e4a4aa96377ba249df4ab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9687342194dd4ec0ad1b85fb9fe098bd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd8d08bda9934a70a277c41d13f83f87","placeholder":"​","style":"IPY_MODEL_c59bd74ab97949a597a289f518aeb03c","value":" 2/2 [00:01\u0026lt;00:00,  1.14s/it]"}},"97bf30b915b2480caf6c7fc439751926":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b81d6dd27646460f9c9a2efc540784ca","placeholder":"​","style":"IPY_MODEL_f635f884b8ed475eb5ad1cb6b83b99e8","value":"100%"}},"99f66569599b47f5b9eac2c962222512":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a289e079c6da49e893dd5f4588c0cbe7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_286be918d66748e399f0f8ced6577a27","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e7bb04ea4c50452fbe8f01aa00b20464","value":2}},"accf134a4fad47acb5fbdf63109f59ac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6cd2c9e6d55a4a82bb9c446e45f35cb0","IPY_MODEL_81d7cf23e7cd404581a8795e8536688a","IPY_MODEL_c10c052bd430457ebd83ac1b5b73a63d"],"layout":"IPY_MODEL_516edc1d66f24695b6769fb01937a820"}},"adc9defd4f8742f6bae2281e230101b8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b24e173ddc5f473f9c4befa2cf07e2c4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2b5846913bc41b489420f1a3e649fe9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3df35e6712b4f49aba827af044e96f7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b4cc9865b894bb781ffe942410200b0","placeholder":"​","style":"IPY_MODEL_4d9b67f7425341dcab6af60b3a924d8c","value":"Downloading: 100%"}},"b7833c41896d4600b002ffff40ce253f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b81d6dd27646460f9c9a2efc540784ca":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b914edd0b2fc45d08b296d8830f83942":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bbe7f92ae1ca4569a48f16c18cbc6c4d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c10c052bd430457ebd83ac1b5b73a63d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_29ada69b367b465d937f80b4db9e5bb7","placeholder":"​","style":"IPY_MODEL_48a77a6e6d9e4c5f99996610aafd1267","value":" 580/580 [00:00\u0026lt;00:00, 7.77kB/s]"}},"c147f1171c7844af82462579bf874334":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ea82b58452f4bbfb30b921600f20fdc","placeholder":"​","style":"IPY_MODEL_b7833c41896d4600b002ffff40ce253f","value":"100%"}},"c35ca85c68e44706b5f182cd14867610":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c59bd74ab97949a597a289f518aeb03c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb9564bab53541f2b87980f73b231529":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dd39aa3b5c1a4c379bde147b47c881d5","IPY_MODEL_5961633b4f5c4ba0ac418eafb0b8dea1","IPY_MODEL_7ce6ebb60bc54f37a5792deabc021f44"],"layout":"IPY_MODEL_8336e465363e44ae9a576849f60a9712"}},"d0a449079d024483a8f3d96ceadc86eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d15734a6fc6d45dfad17412d43a23f4f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_97bf30b915b2480caf6c7fc439751926","IPY_MODEL_770b871fbf634867a41f3f3dc8ca5ff9","IPY_MODEL_9687342194dd4ec0ad1b85fb9fe098bd"],"layout":"IPY_MODEL_4496907e454e4acca14ee4481ca47358"}},"d4addf27161044b6bc1919716cbfa9cd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d86b10657c3e4fabbc1646c2d8c98ab6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd39aa3b5c1a4c379bde147b47c881d5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8c7579b937348a18c1977cf321e96aa","placeholder":"​","style":"IPY_MODEL_77b8b03dcc6b442e85c61c4319bee36f","value":"100%"}},"e0d7789be82545e2a188433cb001cdce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ec441b8d06504361ba7a04c252a16f62","IPY_MODEL_0ae152f2ba9a41e1bc6ae6e930cb6512","IPY_MODEL_8319902bd2da4bb1a517f85ab8f9d0c3"],"layout":"IPY_MODEL_f24f65e7bc2349f48de24414a3b8fe05"}},"e0ebc88dfdce4aa4bccc935b4aaccaa9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b3df35e6712b4f49aba827af044e96f7","IPY_MODEL_1ce6ee9d297b4f3c8206d29a32cb80db","IPY_MODEL_0c7dcdda2dbb4cfc82487cbdf47d434f"],"layout":"IPY_MODEL_372048c026f4425db5718341dfb514d9"}},"e7bb04ea4c50452fbe8f01aa00b20464":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e8fd048971284f77ba5e32607181e9af":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec441b8d06504361ba7a04c252a16f62":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa88839d5bbc431d8f6879c06a923030","placeholder":"​","style":"IPY_MODEL_2e34205df4e5491b84fd7f38c64234d3","value":"100%"}},"f24f65e7bc2349f48de24414a3b8fe05":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f635f884b8ed475eb5ad1cb6b83b99e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f8c7579b937348a18c1977cf321e96aa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa3ba5674e414ddea193fe3ac6bcb643":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa88839d5bbc431d8f6879c06a923030":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcd3ef3530dc4739a7ab4f83e9155486":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fce27e25ea724935a99ba7436ac82461":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fd8d08bda9934a70a277c41d13f83f87":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdf5a9f3d47e4f3c804aa43edf8d52f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}