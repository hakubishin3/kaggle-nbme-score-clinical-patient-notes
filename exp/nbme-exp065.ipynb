{"cells":[{"cell_type":"markdown","id":"colored-security","metadata":{"id":"colored-security"},"source":["## References"]},{"cell_type":"markdown","id":"educational-operator","metadata":{"id":"educational-operator"},"source":["- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"]},{"cell_type":"markdown","id":"incorrect-greek","metadata":{"id":"incorrect-greek"},"source":["## Configurations"]},{"cell_type":"code","execution_count":1,"id":"alive-granny","metadata":{"id":"alive-granny","executionInfo":{"status":"ok","timestamp":1648219861478,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["EXP_NAME = \"nbme-exp065\"\n","ENV = \"colab\"\n","DEBUG_MODE = False\n","SUBMISSION_MODE = False"]},{"cell_type":"code","execution_count":2,"id":"heavy-prophet","metadata":{"id":"heavy-prophet","executionInfo":{"status":"ok","timestamp":1648219861479,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class CFG:\n","    env=ENV\n","    exp_name=EXP_NAME\n","    debug=DEBUG_MODE\n","    submission=SUBMISSION_MODE\n","    apex=True\n","    input_dir=None\n","    output_dir=None\n","    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n","    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n","    competition_name=\"nbme-score-clinical-patient-notes\"\n","    id_col=\"id\"\n","    target_col=\"location\"\n","    pretrained_model_name=\"microsoft/deberta-large\"\n","    tokenizer=None\n","    max_len=None\n","    output_dim=1\n","    dropout=0.2\n","    num_workers=4\n","    batch_size=3\n","    lr=2e-5\n","    betas=(0.9, 0.98)\n","    weight_decay=0.1\n","    num_warmup_steps_rate=0.1\n","    batch_scheduler=True\n","    epochs=5\n","    n_fold=4\n","    train_fold=[0, 1, 2, 3]\n","    seed=71\n","    gradient_accumulation_steps=1\n","    max_grad_norm=1000\n","    print_freq=100\n","    train=True\n","    inference=True"]},{"cell_type":"code","execution_count":3,"id":"vocational-coating","metadata":{"id":"vocational-coating","executionInfo":{"status":"ok","timestamp":1648219861479,"user_tz":-540,"elapsed":8,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.train_fold = [0, 1]\n","\n","if CFG.submission:\n","    CFG.train = False\n","    CFG.inference = True"]},{"cell_type":"markdown","id":"private-moderator","metadata":{"id":"private-moderator"},"source":["## Directory Settings"]},{"cell_type":"code","execution_count":4,"id":"married-tokyo","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"married-tokyo","outputId":"76fbb6f0-3c75-43f7-bc8d-c3053a6950d0","executionInfo":{"status":"ok","timestamp":1648219913167,"user_tz":-540,"elapsed":51696,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["colab\n","Mounted at /content/drive\n","Collecting transformers==4.16.2\n","  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 16.0 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 6.3 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 69.5 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.11.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.63.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (1.21.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (3.6.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 58.5 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,>=0.10.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 49.7 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2019.12.20)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.16.2) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.16.2) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (1.15.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.16.2\n"]}],"source":["import sys\n","from pathlib import Path\n","\n","\n","print(CFG.env)\n","if CFG.env == \"colab\":\n","    # colab環境\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","    # install packages\n","    !pip install transformers==4.16.2\n","\n","elif CFG.env == \"local\":\n","    # ローカルサーバ\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","\n","elif CFG.env == \"kaggle\":\n","    # kaggle環境\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./\")"]},{"cell_type":"code","execution_count":5,"id":"blank-pierre","metadata":{"id":"blank-pierre","executionInfo":{"status":"ok","timestamp":1648219924777,"user_tz":-540,"elapsed":11624,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["import gc\n","import os\n","import ast\n","import time\n","import math\n","import random\n","import itertools\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","from scipy.optimize import minimize\n","from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as T\n","from torchvision.io import read_image\n","from torch.utils.data import DataLoader, Dataset\n","\n","from transformers import AutoModelForMaskedLM\n","from transformers import BartModel,BertModel,BertTokenizer\n","from transformers import DebertaModel,DebertaTokenizer\n","from transformers import RobertaModel,RobertaTokenizer\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","id":"sound-still","metadata":{"id":"sound-still"},"source":["## Utilities"]},{"cell_type":"code","execution_count":6,"id":"surprised-commercial","metadata":{"id":"surprised-commercial","executionInfo":{"status":"ok","timestamp":1648219924778,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on binary arrays.\n","\n","    Args:\n","        preds (list of lists of ints): Predictions.\n","        truths (list of lists of ints): Ground truths.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    # Micro : aggregating over all instances\n","    preds = np.concatenate(preds)\n","    truths = np.concatenate(truths)\n","    return f1_score(truths, preds)\n","\n","\n","def spans_to_binary(spans, length=None):\n","    \"\"\"\n","    Converts spans to a binary array indicating whether each character is in the span.\n","\n","    Args:\n","        spans (list of lists of two ints): Spans.\n","\n","    Returns:\n","        np array [length]: Binarized spans.\n","    \"\"\"\n","    length = np.max(spans) if length is None else length\n","    binary = np.zeros(length)\n","    for start, end in spans:\n","        binary[start:end] = 1\n","    return binary\n","\n","\n","def span_micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on spans.\n","\n","    Args:\n","        preds (list of lists of two ints): Prediction spans.\n","        truths (list of lists of two ints): Ground truth spans.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    bin_preds = []\n","    bin_truths = []\n","    for pred, truth in zip(preds, truths):\n","        if not len(pred) and not len(truth):\n","            continue\n","        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n","        bin_preds.append(spans_to_binary(pred, length))\n","        bin_truths.append(spans_to_binary(truth, length))\n","    return micro_f1(bin_preds, bin_truths)\n","\n","\n","def get_score(y_true, y_pred):\n","    score = span_micro_f1(y_true, y_pred)\n","    return score"]},{"cell_type":"code","execution_count":7,"id":"interstate-accident","metadata":{"id":"interstate-accident","executionInfo":{"status":"ok","timestamp":1648219924778,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def create_labels_for_scoring(df):\n","    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n","    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n","    for i in range(len(df)):\n","        lst = df.loc[i, \"location\"]\n","        if lst:\n","            new_lst = \";\".join(lst)\n","            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n","\n","    # create labels\n","    truths = []\n","    for location_list in df[\"location_for_create_labels\"].values:\n","        truth = []\n","        if len(location_list) > 0:\n","            location = location_list[0]\n","            for loc in [s.split() for s in location.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                truth.append([start, end])\n","        truths.append(truth)\n","\n","    return truths\n","\n","\n","def get_char_probs(texts, token_probs, tokenizer):\n","    res = [np.zeros(len(t)) for t in texts]\n","    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n","        encoded = tokenizer(\n","            text=text,\n","            max_length=CFG.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n","            start, end = offset_mapping\n","            res[i][start:end] = pred\n","    return res\n","\n","\n","def get_predicted_location_str(char_probs, th=0.5):\n","    results = []\n","    for char_prob in char_probs:\n","        result = np.where(char_prob >= th)[0] + 1\n","        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n","        result = [f\"{min(r)} {max(r)}\" for r in result]\n","        result = \";\".join(result)\n","        results.append(result)\n","    return results\n","\n","\n","def get_predictions(results):\n","    predictions = []\n","    for result in results:\n","        prediction = []\n","        if result != \"\":\n","            for loc in [s.split() for s in result.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                prediction.append([start, end])\n","        predictions.append(prediction)\n","    return predictions\n","\n","\n","def scoring(df, th=0.5):\n","    labels = create_labels_for_scoring(df)\n","\n","    token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n","    char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n","    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n","    preds = get_predictions(predicted_location_str)\n","\n","    score = get_score(labels, preds)\n","    return score\n","\n","\n","def get_best_thres(oof_df):\n","    def f1_opt(x):\n","        return -1 * scoring(oof_df, th=x)\n","\n","    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n","    return best_thres"]},{"cell_type":"code","execution_count":8,"id":"coated-pioneer","metadata":{"id":"coated-pioneer","executionInfo":{"status":"ok","timestamp":1648219924779,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return \"%dm %ds\" % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":9,"id":"nervous-delaware","metadata":{"id":"nervous-delaware","executionInfo":{"status":"ok","timestamp":1648219924779,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["seed_everything()"]},{"cell_type":"markdown","id":"functioning-destruction","metadata":{"id":"functioning-destruction"},"source":["## Data Loading"]},{"cell_type":"code","execution_count":10,"id":"global-monte","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"global-monte","outputId":"d76e66bc-5a19-4e5b-fc64-7aadf49a860d","executionInfo":{"status":"ok","timestamp":1648219928718,"user_tz":-540,"elapsed":3944,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14300, 6), (143, 3), (42146, 3), (5, 4))"]},"metadata":{},"execution_count":10}],"source":["train = pd.read_csv(CFG.input_dir / \"train.csv\")\n","features = pd.read_csv(CFG.input_dir / \"features.csv\")\n","patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n","test = pd.read_csv(CFG.input_dir / \"test.csv\")\n","\n","train.shape, features.shape, patient_notes.shape, test.shape"]},{"cell_type":"code","execution_count":11,"id":"independent-airfare","metadata":{"id":"independent-airfare","executionInfo":{"status":"ok","timestamp":1648219928719,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["if CFG.debug:\n","    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n","    print(train.shape)"]},{"cell_type":"markdown","id":"silent-locator","metadata":{"id":"silent-locator"},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":12,"id":"unusual-fifty","metadata":{"id":"unusual-fifty","executionInfo":{"status":"ok","timestamp":1648219928719,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def preprocess_features(features):\n","    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n","    return features\n","\n","\n","features = preprocess_features(features)"]},{"cell_type":"code","source":["# lower case\n","features['feature_text'] = features['feature_text'].str.lower()\n","patient_notes['pn_history'] = patient_notes['pn_history'].str.lower()"],"metadata":{"id":"bAWBG0LKjTES","executionInfo":{"status":"ok","timestamp":1648219928720,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"id":"bAWBG0LKjTES","execution_count":13,"outputs":[]},{"cell_type":"code","execution_count":14,"id":"decreased-mustang","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"decreased-mustang","outputId":"56c38a2f-baa6-469a-babe-b6d42b4b9a89","executionInfo":{"status":"ok","timestamp":1648219928720,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14300, 8), (5, 6))"]},"metadata":{},"execution_count":14}],"source":["train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","\n","train.shape, test.shape"]},{"cell_type":"code","execution_count":15,"id":"boolean-trade","metadata":{"id":"boolean-trade","executionInfo":{"status":"ok","timestamp":1648219929290,"user_tz":-540,"elapsed":575,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n","train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"]},{"cell_type":"code","execution_count":16,"id":"accomplished-dakota","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"id":"accomplished-dakota","outputId":"6286fd76-f4d0-46f8-e48e-8f8a2bee5232","executionInfo":{"status":"ok","timestamp":1648219929291,"user_tz":-540,"elapsed":11,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["0    4399\n","1    8181\n","2    1296\n","3     287\n","4      99\n","5      27\n","6       9\n","7       1\n","8       1\n","Name: annotation_length, dtype: int64"]},"metadata":{}}],"source":["train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n","display(train['annotation_length'].value_counts().sort_index())"]},{"cell_type":"markdown","id":"funded-elizabeth","metadata":{"id":"funded-elizabeth"},"source":["## CV split"]},{"cell_type":"code","execution_count":17,"id":"unexpected-columbia","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":124},"id":"unexpected-columbia","outputId":"e84c9b70-0650-4a45-c325-c197cde53964","executionInfo":{"status":"ok","timestamp":1648219929292,"user_tz":-540,"elapsed":10,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["fold\n","0    3575\n","1    3575\n","2    3575\n","3    3575\n","dtype: int64"]},"metadata":{}}],"source":["Fold = GroupKFold(n_splits=CFG.n_fold)\n","groups = train['pn_num'].values\n","for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n","    train.loc[val_index, 'fold'] = int(n)\n","train['fold'] = train['fold'].astype(int)\n","display(train.groupby('fold').size())"]},{"cell_type":"markdown","id":"critical-archive","metadata":{"id":"critical-archive"},"source":["## Setup tokenizer"]},{"cell_type":"code","execution_count":18,"id":"broken-generator","metadata":{"id":"broken-generator","colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["93396849796641e38fe598fbb80f39bf","ca48dfaaed0c456c965f6d5c936f39ac","856f62ca528f4a68b3d75762c75cdc11","c22b89da452b4343ba57863751510473","cd324da6811f4ce58dd7d6e83f4e850d","5440de7a54de4fa8babb13b22b4a7a0f","3e5b9a91da7246d981cdf332c73be293","579f85189b7f49ee875c67e82a671b17","683970fb94df4782b325365e560aa3fa","3c8c4bf8c12145feadc330210e40ae99","76a0515276e54e27ad7c47f13c00fc46","81aefc91729449d3998d6412dfb1bbb2","3c29f0e0c2af4b1e986d5f59189ddbb6","79de7803649d43f4abeeaaf2bf0f4eac","242c6cad4d424d3d9af60f40d4dce8a6","824e9cfab84b44e4b985faea1c67c8b6","c8dcc71ae64c42529bd722ce19c3f390","e90df5ec49ac4d8aab8b3d2fac166e65","236e6f8a4e7f48ba9e71e1437969d736","030adae8feab402c92f0ffb9ee0213d3","eff3d2de34164d4abc26f1322ed6a67a","4d668caff0b84b51a0c5d339bce3b0b9","18f08745ea4545c1ab6b405a502d2216","e5f90f81f0d94bf0901974ce6bf633c7","2317ee8e32c14e31aaf8413c6a68bca0","6807c2e3067e450883292543b9118239","1d26abd18a0c4e5d96dd01cab2c174b9","1c6a204bcd664429ab7f085f1febd9b2","025a482d60d242cfa249367c104e045d","030cca9bec1944fc9ef0514fff7d9a8b","f50fb4f6649b46a485eb453a4042d3f6","0e4226d9cd754c76b8cc73332a7263cf","8a4d322962cd44c4a98c358c592c229f","a88dcdd54db5497f920b3fa52dc1bebb","04812347a714437e9ef329124fefefe5","907e0494746f4b9d9df8cbf8a7650c31","d3d55721adc94e2c9912fad0fe6331b3","2fa1ca10c77d4093a344e0a15d9d6549","944437ffdd884dda89468e2c5e32a425","8a37882246ab4f2fa4d9027f8a825102","3944b26ad2c549feb50af10ecab5abe7","c2f67e0f521d4ff3b82ad92f7963b8d9","8219735e55e242d4ab8b57baa3915b14","3ea2ca6055804c6198187e30af45c4a7"]},"executionInfo":{"status":"ok","timestamp":1648219936790,"user_tz":-540,"elapsed":7507,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}},"outputId":"44000a3d-1660-471f-9549-e01cc7d8ede8"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93396849796641e38fe598fbb80f39bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/475 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81aefc91729449d3998d6412dfb1bbb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18f08745ea4545c1ab6b405a502d2216"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a88dcdd54db5497f920b3fa52dc1bebb"}},"metadata":{}}],"source":["if CFG.submission:\n","    tokenizer = AutoTokenizer.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n","else:\n","    tokenizer = AutoTokenizer.from_pretrained(CFG.pretrained_model_name)\n","    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n","\n","CFG.tokenizer = tokenizer"]},{"cell_type":"markdown","id":"compatible-lincoln","metadata":{"id":"compatible-lincoln"},"source":["## Create dataset"]},{"cell_type":"code","execution_count":19,"id":"fluid-nancy","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["492692e8f13c4f92ace440121366e6d4","9ff86c5e3715480b868af385905a1271","f52efa19f8f84c66b2eaafaae633c680","e3c97b5dd1ac4f2d8b0774d97656846b","5eec03c6eda14d6085279feb1e9dd49c","0d662b9dc4524e0bb309e5523bb9a161","43a14631d837429795298cdd7ac8bf70","4a42ddd9f43a4719adc48d1ebf20dbb4","a6773a1438974d54a340ae6e8fe1de89","6ee34c7c90c34299ac27975d7ed02ce8","d9a9bb811a9944148592b846ff3d78f5"]},"id":"fluid-nancy","outputId":"a6ba2ac4-bce9-41e1-c3f7-0aa135a86daf","executionInfo":{"status":"ok","timestamp":1648219965406,"user_tz":-540,"elapsed":28623,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/42146 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"492692e8f13c4f92ace440121366e6d4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 325\n"]}],"source":["pn_history_lengths = []\n","tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    pn_history_lengths.append(length)\n","\n","print(\"max length:\", np.max(pn_history_lengths))"]},{"cell_type":"code","execution_count":20,"id":"posted-humidity","metadata":{"id":"posted-humidity","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["b5026118027a4e32bfa47fa07b3cb107","a23445a8eb094a86934c659f12340b71","633d1efd643c4a7e8549defeb08b908c","8e4336fbae44412da80d666ecd00a425","f68159ff7f3f48e889a5aba9a9587220","48e66789c51742018bd7fe97ffd5f9f3","3d13408ab7a84711a60458529f1dd32d","58df78091e134c009151c2b9ff857e02","d8c996b15d9e46a3b2eceaf7d0f49c1d","5036963bdbd043808e6922143454f306","45da851b555b451195033d8fd5d7dc48"]},"executionInfo":{"status":"ok","timestamp":1648219965406,"user_tz":-540,"elapsed":31,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}},"outputId":"a3a999d6-9818-4f0c-df8f-c33cd262893e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/143 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5026118027a4e32bfa47fa07b3cb107"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 30\n"]}],"source":["feature_text_lengths = []\n","tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    feature_text_lengths.append(length)\n","\n","print(\"max length:\", np.max(feature_text_lengths))"]},{"cell_type":"code","execution_count":21,"id":"resistant-amount","metadata":{"id":"resistant-amount","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648219965407,"user_tz":-540,"elapsed":29,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}},"outputId":"7cfdb8d7-30e5-4657-af77-1c25e082e669"},"outputs":[{"output_type":"stream","name":"stdout","text":["max length: 358\n"]}],"source":["CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n","\n","print(\"max length:\", CFG.max_len)"]},{"cell_type":"code","execution_count":22,"id":"august-equity","metadata":{"id":"august-equity","executionInfo":{"status":"ok","timestamp":1648219965408,"user_tz":-540,"elapsed":27,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class TrainingDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","        self.annotation_lengths = self.df[\"annotation_length\"].values\n","        self.locations = self.df[\"location\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def _create_label(self, pn_history, annotation_length, location_list):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        offset_mapping = encoded[\"offset_mapping\"]\n","        ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n","        label = np.zeros(len(offset_mapping))\n","        label[ignore_idxes] = -1\n","\n","        if annotation_length > 0:\n","            for location in location_list:\n","                for loc in [s.split() for s in location.split(\";\")]:\n","                    start, end = int(loc[0]), int(loc[1])\n","                    start_idx = -1\n","                    end_idx = -1\n","                    for idx in range(len(offset_mapping)):\n","                        if (start_idx == -1) & (start < offset_mapping[idx][0]):\n","                            start_idx = idx - 1\n","                        if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n","                            end_idx = idx + 1\n","                    if start_idx == -1:\n","                        start_idx = end_idx\n","                    if (start_idx != -1) & (end_idx != -1):\n","                        label[start_idx:end_idx] = 1\n","\n","        return torch.tensor(label, dtype=torch.float)\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n","        return input_, label"]},{"cell_type":"code","execution_count":23,"id":"weird-interaction","metadata":{"id":"weird-interaction","executionInfo":{"status":"ok","timestamp":1648219965408,"user_tz":-540,"elapsed":27,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class TestDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        return input_"]},{"cell_type":"markdown","id":"upper-mobility","metadata":{"id":"upper-mobility"},"source":["## Model"]},{"cell_type":"code","execution_count":24,"id":"spanish-destruction","metadata":{"id":"spanish-destruction","executionInfo":{"status":"ok","timestamp":1648219965409,"user_tz":-540,"elapsed":26,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class CustomModel(nn.Module):\n","    def __init__(self, cfg, model_config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        if model_config_path is None:\n","            self.model_config = AutoConfig.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                output_hidden_states=True,\n","            )\n","        else:\n","            self.model_config = torch.load(model_config_path)\n","\n","        if pretrained:\n","            self.backbone = AutoModel.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                config=self.model_config,\n","            )\n","            print(f\"Load weight from pretrained\")\n","        else:\n","            #self.backbone = AutoModel.from_config(self.model_config)\n","            itpt = AutoModelForMaskedLM.from_config(self.model_config)\n","            path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp010/checkpoint-130170/pytorch_model.bin\")\n","            # path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n","            state_dict = torch.load(path)\n","            itpt.load_state_dict(state_dict)\n","            self.backbone = itpt.deberta\n","            print(f\"Load weight from {path}\")\n","\n","        self.fc = nn.Sequential(\n","            nn.Dropout(self.cfg.dropout),\n","            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n","        )\n","\n","    def forward(self, inputs):\n","        h = self.backbone(**inputs)[\"last_hidden_state\"]\n","        output = self.fc(h)\n","        return output"]},{"cell_type":"markdown","id":"chronic-bullet","metadata":{"id":"chronic-bullet"},"source":["## Training"]},{"cell_type":"code","execution_count":25,"id":"biological-hunger","metadata":{"id":"biological-hunger","executionInfo":{"status":"ok","timestamp":1648219965409,"user_tz":-540,"elapsed":26,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def train_fn(\n","    train_dataloader,\n","    model,\n","    criterion,\n","    optimizer,\n","    epoch,\n","    scheduler,\n","    device,\n","):\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(train_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","        loss = loss.mean()\n","\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","\n","        if CFG.batch_scheduler:\n","            scheduler.step()\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n","            print(\n","                \"Epoch: [{0}][{1}/{2}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                \"Grad: {grad_norm:.4f}  \"\n","                \"LR: {lr:.6f}  \"\n","                .format(\n","                    epoch+1,\n","                    step,\n","                    len(train_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n","                    loss=losses,\n","                     grad_norm=grad_norm,\n","                     lr=scheduler.get_lr()[0],\n","                )\n","            )\n","    return losses.avg"]},{"cell_type":"code","execution_count":26,"id":"satisfied-sterling","metadata":{"id":"satisfied-sterling","executionInfo":{"status":"ok","timestamp":1648219965410,"user_tz":-540,"elapsed":26,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def valid_fn(\n","    val_dataloader,\n","    model,\n","    criterion,\n","    device,\n","):\n","    model.eval()\n","    preds = []\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(val_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.no_grad():\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","        loss = loss.mean()\n","\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n","            print(\n","                \"EVAL: [{0}/{1}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                .format(\n","                    step, len(val_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n","                    loss=losses,\n","                )\n","            )\n","    preds = np.concatenate(preds)\n","    return losses.avg, preds"]},{"cell_type":"code","execution_count":27,"id":"incorporate-viking","metadata":{"id":"incorporate-viking","executionInfo":{"status":"ok","timestamp":1648219965410,"user_tz":-540,"elapsed":25,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def inference_fn(test_dataloader, model, device):\n","    model.eval()\n","    model.to(device)\n","    preds = []\n","    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n","    for inputs in tk0:\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        with torch.no_grad():\n","            output = model(inputs)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","    preds = np.concatenate(preds)\n","    return preds"]},{"cell_type":"code","execution_count":28,"id":"dental-sunset","metadata":{"id":"dental-sunset","executionInfo":{"status":"ok","timestamp":1648219965847,"user_tz":-540,"elapsed":461,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def train_loop(df, i_fold, device):\n","    print(f\"========== fold: {i_fold} training ==========\")\n","    train_idx = df[df[\"fold\"] != i_fold].index\n","    val_idx = df[df[\"fold\"] == i_fold].index\n","\n","    train_folds = df.loc[train_idx].reset_index(drop=True)\n","    val_folds = df.loc[val_idx].reset_index(drop=True)\n","\n","    train_dataset = TrainingDataset(CFG, train_folds)\n","    val_dataset = TrainingDataset(CFG, val_folds)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=True,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=True,\n","    )\n","    val_dataloader = DataLoader(\n","        val_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=False,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=False,\n","    )\n","\n","    # model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","    model = CustomModel(CFG, model_config_path=None, pretrained=False)   # itptを使うため\n","    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n","    model.to(device)\n","\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\"params\": [p for n, p in param_optimizer if not any(\n","            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n","        {\"params\": [p for n, p in param_optimizer if any(\n","            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n","    ]\n","    optimizer = AdamW(\n","        optimizer_grouped_parameters,\n","        lr=CFG.lr,\n","        betas=CFG.betas,\n","        weight_decay=CFG.weight_decay,\n","    )\n","    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n","    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        num_training_steps=num_train_optimization_steps,\n","    )\n","\n","    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    best_score = -1 * np.inf\n","\n","    for epoch in range(CFG.epochs):\n","        start_time = time.time()\n","        avg_loss = train_fn(\n","            train_dataloader,\n","            model,\n","            criterion,\n","            optimizer,\n","            epoch,\n","            scheduler,\n","            device,\n","        )\n","        avg_val_loss, val_preds = valid_fn(\n","            val_dataloader,\n","            model,\n","            criterion,\n","            device,\n","        )\n","\n","        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n","            scheduler.step()\n","\n","        # scoring\n","        val_folds[[str(i) for i in range(CFG.max_len)]] = val_preds\n","        score = scoring(val_folds, th=0.5)\n","\n","        elapsed = time.time() - start_time\n","\n","        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n","        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n","        if score > best_score:\n","            best_score = score\n","            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n","            torch.save({\n","                \"model\": model.state_dict(),\n","                \"predictions\": val_preds,\n","                },\n","                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","            )\n","\n","    predictions = torch.load(\n","        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","        map_location=torch.device(\"cpu\"),\n","    )[\"predictions\"]\n","    val_folds[[str(i) for i in range(CFG.max_len)]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return val_folds"]},{"cell_type":"markdown","id":"brazilian-graphics","metadata":{"id":"brazilian-graphics"},"source":["## Main"]},{"cell_type":"code","execution_count":29,"id":"connected-protein","metadata":{"id":"connected-protein","executionInfo":{"status":"ok","timestamp":1648219965848,"user_tz":-540,"elapsed":8,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def main():\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    if CFG.train:\n","        oof_df = pd.DataFrame()\n","        for i_fold in range(CFG.n_fold):\n","            if i_fold in CFG.train_fold:\n","                _oof_df = train_loop(train, i_fold, device)\n","                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n","        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    if CFG.submission:\n","        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n","    else:\n","        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    score = scoring(oof_df, th=0.5)\n","    print(f\"Best thres: 0.5, Score: {score:.4f}\")\n","    best_thres = get_best_thres(oof_df)\n","    score = scoring(oof_df, th=best_thres)\n","    print(f\"Best thres: {best_thres}, Score: {score:.4f}\")\n","\n","    if CFG.inference:\n","        test_dataset = TestDataset(CFG, test)\n","        test_dataloader = DataLoader(\n","            test_dataset,\n","            batch_size=CFG.batch_size,\n","            shuffle=False,\n","            num_workers=CFG.num_workers,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","        predictions = []\n","        for i_fold in CFG.train_fold:\n","            if CFG.submission:\n","                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n","                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n","            else:\n","                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n","\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            model.load_state_dict(state[\"model\"])\n","            test_token_probs = inference_fn(test_dataloader, model, device)\n","            test[[f\"fold{i_fold}_{i}\" for i in range(CFG.max_len)]] = test_token_probs\n","            test_char_probs = get_char_probs(test[\"pn_history\"].values, test_token_probs, CFG.tokenizer)\n","            predictions.append(test_char_probs)\n","\n","            del state, test_token_probs, model; gc.collect()\n","            torch.cuda.empty_cache()\n","\n","        predictions = np.mean(predictions, axis=0)\n","        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n","        test[CFG.target_col] = predicted_location_str\n","        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n","        test[[CFG.id_col, CFG.target_col]].to_csv(\n","            CFG.output_dir / \"submission.csv\", index=False\n","        )"]},{"cell_type":"code","execution_count":null,"id":"serious-bunny","metadata":{"id":"serious-bunny","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["6eaa45a79eea49d7bd751c6966052957","0fbb7f0aa7e84edda0ea106c31aaeaa2","61a9f8d985184f69833f2d0f4825696a","af3da1fafb6a491c84edc0d64ba9963c","0689cd965f424a48b7de97c828ca7c62","70ea62aa13a94fbdb25e26258709c7fd","d91866807e3d444d939854bee32646ad","6b6134e91eb44f5f8246d6d55de5eb4c","bd1cae6c5bef4a14bf34d0a3c5f1c3b9","946e6388966b4d3c8b85f0cef3217903","7b9e89790a2741abac2e472b199648c3","761393f8e6964b58b103436b804bf60e","c5d01a08aeba4fc09ba820609a806a19","6d34c4b357254a048ce183d85df3f115","7cb47d100bf24c24a2ace444fdb975f4","5bff9704782246629da79145ce490ce1","4bf220f024d848f6abfe731bd8269593","5c549562112648d085066bbc3ab0a05c","cd1b847c670841ed986455b17b7f2ab5","a584170b609c4eefa75f3f0d65275acf","ac5b24b572e54624a52fab3d7b757aca","7908b5b887b64959b444a120ce88d8d8","fb1ade356f954e94a432a4f4088d7694","026df47e46f44b0e9c730254d80a76d8","deaf9e54ba72486d8780936f3ca2b337","6ecaf3103af44b689cbefc755d7dbb5c","77593f6828e7414681926fd2d769cbaa","9d9dc0d2d8d94ede8a08a5d6b301667a","21b6896ce6cd4b538c952775fc03b949","9ac80c48e99e48258dcc83b23ef32f59","ced03b069ba24671a9ef114a9eeb4e78","534d2e53904d44629743000efd223692","c9bfd7aac5d04be788e36da18ed6b6aa"]},"outputId":"e8836cfb-b259-421d-bacd-d507000ffef8"},"outputs":[{"output_type":"stream","name":"stdout","text":["========== fold: 0 training ==========\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n","Epoch: [1][0/3575] Elapsed 0m 1s (remain 74m 52s) Loss: 0.6053(0.6053) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/3575] Elapsed 0m 26s (remain 15m 4s) Loss: 0.3885(0.5259) Grad: 32863.8398  LR: 0.000001  \n","Epoch: [1][200/3575] Elapsed 0m 52s (remain 14m 45s) Loss: 0.0527(0.3389) Grad: 913.5522  LR: 0.000002  \n","Epoch: [1][300/3575] Elapsed 1m 17s (remain 14m 5s) Loss: 0.0880(0.2498) Grad: 1738.5060  LR: 0.000003  \n","Epoch: [1][400/3575] Elapsed 1m 42s (remain 13m 33s) Loss: 0.0356(0.2044) Grad: 1948.7107  LR: 0.000004  \n","Epoch: [1][500/3575] Elapsed 2m 8s (remain 13m 5s) Loss: 0.0254(0.1722) Grad: 6612.4224  LR: 0.000006  \n","Epoch: [1][600/3575] Elapsed 2m 33s (remain 12m 37s) Loss: 0.0083(0.1483) Grad: 1845.9480  LR: 0.000007  \n","Epoch: [1][700/3575] Elapsed 2m 58s (remain 12m 9s) Loss: 0.0010(0.1309) Grad: 200.7860  LR: 0.000008  \n","Epoch: [1][800/3575] Elapsed 3m 23s (remain 11m 43s) Loss: 0.0930(0.1173) Grad: 4303.4048  LR: 0.000009  \n","Epoch: [1][900/3575] Elapsed 3m 48s (remain 11m 16s) Loss: 0.0057(0.1071) Grad: 3060.2095  LR: 0.000010  \n","Epoch: [1][1000/3575] Elapsed 4m 12s (remain 10m 50s) Loss: 0.0361(0.0983) Grad: 5880.8682  LR: 0.000011  \n","Epoch: [1][1100/3575] Elapsed 4m 38s (remain 10m 24s) Loss: 0.0001(0.0912) Grad: 39.8471  LR: 0.000012  \n","Epoch: [1][1200/3575] Elapsed 5m 3s (remain 9m 58s) Loss: 0.0023(0.0851) Grad: 740.9465  LR: 0.000013  \n","Epoch: [1][1300/3575] Elapsed 5m 27s (remain 9m 33s) Loss: 0.0151(0.0802) Grad: 1498.4427  LR: 0.000015  \n","Epoch: [1][1400/3575] Elapsed 5m 52s (remain 9m 7s) Loss: 0.0671(0.0758) Grad: 7446.6025  LR: 0.000016  \n","Epoch: [1][1500/3575] Elapsed 6m 17s (remain 8m 41s) Loss: 0.0068(0.0721) Grad: 3588.7029  LR: 0.000017  \n","Epoch: [1][1600/3575] Elapsed 6m 42s (remain 8m 16s) Loss: 0.0057(0.0687) Grad: 6036.7271  LR: 0.000018  \n","Epoch: [1][1700/3575] Elapsed 7m 7s (remain 7m 51s) Loss: 0.0056(0.0656) Grad: 1524.9181  LR: 0.000019  \n","Epoch: [1][1800/3575] Elapsed 7m 32s (remain 7m 25s) Loss: 0.0333(0.0630) Grad: 7304.4668  LR: 0.000020  \n","Epoch: [1][1900/3575] Elapsed 7m 57s (remain 7m 0s) Loss: 0.0122(0.0605) Grad: 1257.7814  LR: 0.000020  \n","Epoch: [1][2000/3575] Elapsed 8m 22s (remain 6m 35s) Loss: 0.0063(0.0582) Grad: 883.6592  LR: 0.000020  \n","Epoch: [1][2100/3575] Elapsed 8m 47s (remain 6m 10s) Loss: 0.0090(0.0560) Grad: 1848.2009  LR: 0.000020  \n","Epoch: [1][2200/3575] Elapsed 9m 12s (remain 5m 44s) Loss: 0.0006(0.0541) Grad: 256.5839  LR: 0.000019  \n","Epoch: [1][2300/3575] Elapsed 9m 37s (remain 5m 19s) Loss: 0.0181(0.0524) Grad: 8407.9004  LR: 0.000019  \n","Epoch: [1][2400/3575] Elapsed 10m 2s (remain 4m 54s) Loss: 0.0005(0.0509) Grad: 395.6518  LR: 0.000019  \n","Epoch: [1][2500/3575] Elapsed 10m 27s (remain 4m 29s) Loss: 0.0005(0.0497) Grad: 202.7834  LR: 0.000019  \n","Epoch: [1][2600/3575] Elapsed 10m 52s (remain 4m 4s) Loss: 0.0006(0.0483) Grad: 354.4513  LR: 0.000019  \n","Epoch: [1][2700/3575] Elapsed 11m 16s (remain 3m 39s) Loss: 0.0064(0.0471) Grad: 2626.4482  LR: 0.000019  \n","Epoch: [1][2800/3575] Elapsed 11m 41s (remain 3m 13s) Loss: 0.0079(0.0460) Grad: 2509.4321  LR: 0.000019  \n","Epoch: [1][2900/3575] Elapsed 12m 6s (remain 2m 48s) Loss: 0.0347(0.0449) Grad: 5414.2285  LR: 0.000019  \n","Epoch: [1][3000/3575] Elapsed 12m 31s (remain 2m 23s) Loss: 0.0015(0.0438) Grad: 1480.7552  LR: 0.000018  \n","Epoch: [1][3100/3575] Elapsed 12m 56s (remain 1m 58s) Loss: 0.0002(0.0429) Grad: 63.3712  LR: 0.000018  \n","Epoch: [1][3200/3575] Elapsed 13m 20s (remain 1m 33s) Loss: 0.0884(0.0420) Grad: 12159.7754  LR: 0.000018  \n","Epoch: [1][3300/3575] Elapsed 13m 45s (remain 1m 8s) Loss: 0.0051(0.0410) Grad: 3498.3213  LR: 0.000018  \n","Epoch: [1][3400/3575] Elapsed 14m 10s (remain 0m 43s) Loss: 0.0234(0.0403) Grad: 6647.1641  LR: 0.000018  \n","Epoch: [1][3500/3575] Elapsed 14m 35s (remain 0m 18s) Loss: 0.0025(0.0395) Grad: 1472.0842  LR: 0.000018  \n","Epoch: [1][3574/3575] Elapsed 14m 53s (remain 0m 0s) Loss: 0.0208(0.0389) Grad: 19912.8301  LR: 0.000018  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 7m 17s) Loss: 0.0014(0.0014) \n","EVAL: [100/1192] Elapsed 0m 10s (remain 1m 57s) Loss: 0.0207(0.0093) \n","EVAL: [200/1192] Elapsed 0m 21s (remain 1m 45s) Loss: 0.0465(0.0118) \n","EVAL: [300/1192] Elapsed 0m 31s (remain 1m 34s) Loss: 0.0094(0.0140) \n","EVAL: [400/1192] Elapsed 0m 42s (remain 1m 23s) Loss: 0.0120(0.0144) \n","EVAL: [500/1192] Elapsed 0m 53s (remain 1m 13s) Loss: 0.0133(0.0133) \n","EVAL: [600/1192] Elapsed 1m 3s (remain 1m 2s) Loss: 0.0239(0.0137) \n","EVAL: [700/1192] Elapsed 1m 14s (remain 0m 51s) Loss: 0.1803(0.0161) \n","EVAL: [800/1192] Elapsed 1m 24s (remain 0m 41s) Loss: 0.0095(0.0164) \n","EVAL: [900/1192] Elapsed 1m 35s (remain 0m 30s) Loss: 0.0115(0.0162) \n","EVAL: [1000/1192] Elapsed 1m 45s (remain 0m 20s) Loss: 0.0001(0.0159) \n","EVAL: [1100/1192] Elapsed 1m 56s (remain 0m 9s) Loss: 0.0027(0.0151) \n","EVAL: [1191/1192] Elapsed 2m 5s (remain 0m 0s) Loss: 0.0001(0.0147) \n","Epoch 1 - avg_train_loss: 0.0389  avg_val_loss: 0.0147  time: 1025s\n","Epoch 1 - Score: 0.8516\n","Epoch 1 - Save Best Score: 0.8516 Model\n","Epoch: [2][0/3575] Elapsed 0m 0s (remain 31m 18s) Loss: 0.0016(0.0016) Grad: 3293.8667  LR: 0.000018  \n","Epoch: [2][100/3575] Elapsed 0m 27s (remain 15m 49s) Loss: 0.0081(0.0108) Grad: 46298.2656  LR: 0.000018  \n","Epoch: [2][200/3575] Elapsed 0m 54s (remain 15m 10s) Loss: 0.0000(0.0095) Grad: 160.2502  LR: 0.000018  \n","Epoch: [2][300/3575] Elapsed 1m 19s (remain 14m 20s) Loss: 0.0081(0.0105) Grad: 19787.4902  LR: 0.000017  \n","Epoch: [2][400/3575] Elapsed 1m 44s (remain 13m 43s) Loss: 0.0740(0.0100) Grad: 25367.4785  LR: 0.000017  \n","Epoch: [2][500/3575] Elapsed 2m 8s (remain 13m 11s) Loss: 0.0017(0.0106) Grad: 11626.0254  LR: 0.000017  \n","Epoch: [2][600/3575] Elapsed 2m 34s (remain 12m 42s) Loss: 0.0314(0.0114) Grad: 49586.6914  LR: 0.000017  \n","Epoch: [2][700/3575] Elapsed 2m 59s (remain 12m 15s) Loss: 0.0112(0.0116) Grad: 13527.2070  LR: 0.000017  \n","Epoch: [2][800/3575] Elapsed 3m 24s (remain 11m 48s) Loss: 0.0092(0.0113) Grad: 25102.2520  LR: 0.000017  \n","Epoch: [2][900/3575] Elapsed 3m 49s (remain 11m 21s) Loss: 0.0028(0.0109) Grad: 19175.5137  LR: 0.000017  \n","Epoch: [2][1000/3575] Elapsed 4m 15s (remain 10m 56s) Loss: 0.0000(0.0108) Grad: 174.0647  LR: 0.000017  \n","Epoch: [2][1100/3575] Elapsed 4m 40s (remain 10m 30s) Loss: 0.0268(0.0107) Grad: 74320.6797  LR: 0.000016  \n","Epoch: [2][1200/3575] Elapsed 5m 5s (remain 10m 4s) Loss: 0.0006(0.0109) Grad: 6105.6919  LR: 0.000016  \n","Epoch: [2][1300/3575] Elapsed 5m 31s (remain 9m 39s) Loss: 0.0002(0.0112) Grad: 1079.1997  LR: 0.000016  \n","Epoch: [2][1400/3575] Elapsed 5m 56s (remain 9m 13s) Loss: 0.0000(0.0111) Grad: 41.8917  LR: 0.000016  \n","Epoch: [2][1500/3575] Elapsed 6m 21s (remain 8m 47s) Loss: 0.0020(0.0111) Grad: 4052.0332  LR: 0.000016  \n","Epoch: [2][1600/3575] Elapsed 6m 47s (remain 8m 22s) Loss: 0.0002(0.0109) Grad: 383.0895  LR: 0.000016  \n","Epoch: [2][1700/3575] Elapsed 7m 12s (remain 7m 56s) Loss: 0.0245(0.0110) Grad: 14350.2383  LR: 0.000016  \n","Epoch: [2][1800/3575] Elapsed 7m 37s (remain 7m 30s) Loss: 0.0001(0.0109) Grad: 136.1037  LR: 0.000016  \n","Epoch: [2][1900/3575] Elapsed 8m 2s (remain 7m 4s) Loss: 0.0008(0.0109) Grad: 2089.5068  LR: 0.000015  \n","Epoch: [2][2000/3575] Elapsed 8m 27s (remain 6m 39s) Loss: 0.0050(0.0107) Grad: 9534.6289  LR: 0.000015  \n","Epoch: [2][2100/3575] Elapsed 8m 52s (remain 6m 13s) Loss: 0.0444(0.0108) Grad: 8509.4033  LR: 0.000015  \n","Epoch: [2][2200/3575] Elapsed 9m 17s (remain 5m 48s) Loss: 0.0133(0.0108) Grad: 5878.0078  LR: 0.000015  \n","Epoch: [2][2300/3575] Elapsed 9m 43s (remain 5m 22s) Loss: 0.0002(0.0106) Grad: 320.1029  LR: 0.000015  \n","Epoch: [2][2400/3575] Elapsed 10m 8s (remain 4m 57s) Loss: 0.0061(0.0105) Grad: 10568.6396  LR: 0.000015  \n","Epoch: [2][2500/3575] Elapsed 10m 33s (remain 4m 32s) Loss: 0.0275(0.0105) Grad: 15206.7773  LR: 0.000015  \n","Epoch: [2][2600/3575] Elapsed 10m 58s (remain 4m 6s) Loss: 0.0000(0.0105) Grad: 42.2086  LR: 0.000015  \n","Epoch: [2][2700/3575] Elapsed 11m 23s (remain 3m 41s) Loss: 0.0019(0.0105) Grad: 3918.5679  LR: 0.000014  \n","Epoch: [2][2800/3575] Elapsed 11m 48s (remain 3m 15s) Loss: 0.0000(0.0105) Grad: 56.2616  LR: 0.000014  \n","Epoch: [2][2900/3575] Elapsed 12m 14s (remain 2m 50s) Loss: 0.0034(0.0105) Grad: 5897.0601  LR: 0.000014  \n","Epoch: [2][3000/3575] Elapsed 12m 39s (remain 2m 25s) Loss: 0.0001(0.0104) Grad: 118.8635  LR: 0.000014  \n","Epoch: [2][3100/3575] Elapsed 13m 4s (remain 1m 59s) Loss: 0.0017(0.0104) Grad: 9324.4189  LR: 0.000014  \n","Epoch: [2][3200/3575] Elapsed 13m 29s (remain 1m 34s) Loss: 0.0000(0.0104) Grad: 53.4334  LR: 0.000014  \n","Epoch: [2][3300/3575] Elapsed 13m 54s (remain 1m 9s) Loss: 0.0116(0.0105) Grad: 25838.8984  LR: 0.000014  \n","Epoch: [2][3400/3575] Elapsed 14m 19s (remain 0m 43s) Loss: 0.0000(0.0104) Grad: 16.2501  LR: 0.000014  \n","Epoch: [2][3500/3575] Elapsed 14m 44s (remain 0m 18s) Loss: 0.0003(0.0105) Grad: 980.9041  LR: 0.000013  \n","Epoch: [2][3574/3575] Elapsed 15m 3s (remain 0m 0s) Loss: 0.0015(0.0105) Grad: 6322.5845  LR: 0.000013  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 6m 47s) Loss: 0.0001(0.0001) \n","EVAL: [100/1192] Elapsed 0m 10s (remain 1m 57s) Loss: 0.0228(0.0089) \n","EVAL: [200/1192] Elapsed 0m 21s (remain 1m 45s) Loss: 0.0204(0.0109) \n","EVAL: [300/1192] Elapsed 0m 31s (remain 1m 34s) Loss: 0.0115(0.0111) \n","EVAL: [400/1192] Elapsed 0m 42s (remain 1m 23s) Loss: 0.0062(0.0112) \n","EVAL: [500/1192] Elapsed 0m 53s (remain 1m 13s) Loss: 0.0123(0.0103) \n","EVAL: [600/1192] Elapsed 1m 3s (remain 1m 2s) Loss: 0.0136(0.0109) \n","EVAL: [700/1192] Elapsed 1m 14s (remain 0m 52s) Loss: 0.1734(0.0133) \n","EVAL: [800/1192] Elapsed 1m 24s (remain 0m 41s) Loss: 0.0020(0.0137) \n","EVAL: [900/1192] Elapsed 1m 35s (remain 0m 30s) Loss: 0.0086(0.0136) \n","EVAL: [1000/1192] Elapsed 1m 46s (remain 0m 20s) Loss: 0.0000(0.0134) \n","EVAL: [1100/1192] Elapsed 1m 56s (remain 0m 9s) Loss: 0.0046(0.0129) \n","EVAL: [1191/1192] Elapsed 2m 6s (remain 0m 0s) Loss: 0.0000(0.0127) \n","Epoch 2 - avg_train_loss: 0.0105  avg_val_loss: 0.0127  time: 1035s\n","Epoch 2 - Score: 0.8750\n","Epoch 2 - Save Best Score: 0.8750 Model\n","Epoch: [3][0/3575] Elapsed 0m 0s (remain 34m 47s) Loss: 0.0036(0.0036) Grad: 6051.1816  LR: 0.000013  \n","Epoch: [3][100/3575] Elapsed 0m 28s (remain 16m 10s) Loss: 0.0002(0.0066) Grad: 905.8636  LR: 0.000013  \n","Epoch: [3][200/3575] Elapsed 0m 54s (remain 15m 19s) Loss: 0.0135(0.0079) Grad: 12035.4229  LR: 0.000013  \n","Epoch: [3][300/3575] Elapsed 1m 19s (remain 14m 28s) Loss: 0.0031(0.0075) Grad: 14864.4775  LR: 0.000013  \n","Epoch: [3][400/3575] Elapsed 1m 44s (remain 13m 49s) Loss: 0.0306(0.0077) Grad: 86923.2031  LR: 0.000013  \n","Epoch: [3][500/3575] Elapsed 2m 9s (remain 13m 16s) Loss: 0.0000(0.0078) Grad: 27.8759  LR: 0.000013  \n","Epoch: [3][600/3575] Elapsed 2m 35s (remain 12m 47s) Loss: 0.1306(0.0077) Grad: 127362.6641  LR: 0.000013  \n","Epoch: [3][700/3575] Elapsed 3m 0s (remain 12m 18s) Loss: 0.0036(0.0080) Grad: 14208.7861  LR: 0.000012  \n","Epoch: [3][800/3575] Elapsed 3m 25s (remain 11m 50s) Loss: 0.0000(0.0081) Grad: 19.1997  LR: 0.000012  \n","Epoch: [3][900/3575] Elapsed 3m 50s (remain 11m 23s) Loss: 0.0000(0.0080) Grad: 46.8052  LR: 0.000012  \n","Epoch: [3][1000/3575] Elapsed 4m 15s (remain 10m 56s) Loss: 0.0002(0.0078) Grad: 440.7494  LR: 0.000012  \n","Epoch: [3][1100/3575] Elapsed 4m 40s (remain 10m 29s) Loss: 0.0000(0.0076) Grad: 92.7830  LR: 0.000012  \n","Epoch: [3][1200/3575] Elapsed 5m 5s (remain 10m 3s) Loss: 0.0163(0.0078) Grad: 22167.5098  LR: 0.000012  \n","Epoch: [3][1300/3575] Elapsed 5m 30s (remain 9m 37s) Loss: 0.0000(0.0078) Grad: 16.8538  LR: 0.000012  \n","Epoch: [3][1400/3575] Elapsed 5m 55s (remain 9m 11s) Loss: 0.0226(0.0078) Grad: 41933.3633  LR: 0.000012  \n","Epoch: [3][1500/3575] Elapsed 6m 20s (remain 8m 46s) Loss: 0.0000(0.0080) Grad: 9.5987  LR: 0.000011  \n","Epoch: [3][1600/3575] Elapsed 6m 45s (remain 8m 20s) Loss: 0.0000(0.0077) Grad: 286.3874  LR: 0.000011  \n","Epoch: [3][1700/3575] Elapsed 7m 11s (remain 7m 54s) Loss: 0.0001(0.0076) Grad: 238.9598  LR: 0.000011  \n","Epoch: [3][1800/3575] Elapsed 7m 36s (remain 7m 29s) Loss: 0.0092(0.0075) Grad: 20561.1621  LR: 0.000011  \n","Epoch: [3][1900/3575] Elapsed 8m 1s (remain 7m 3s) Loss: 0.0788(0.0077) Grad: 92014.2266  LR: 0.000011  \n","Epoch: [3][2000/3575] Elapsed 8m 26s (remain 6m 38s) Loss: 0.0098(0.0076) Grad: 99388.0938  LR: 0.000011  \n","Epoch: [3][2100/3575] Elapsed 8m 51s (remain 6m 12s) Loss: 0.0000(0.0077) Grad: 135.8722  LR: 0.000011  \n","Epoch: [3][2200/3575] Elapsed 9m 16s (remain 5m 47s) Loss: 0.0068(0.0076) Grad: 102978.6250  LR: 0.000011  \n","Epoch: [3][2300/3575] Elapsed 9m 41s (remain 5m 21s) Loss: 0.0106(0.0075) Grad: 96528.0703  LR: 0.000010  \n","Epoch: [3][2400/3575] Elapsed 10m 6s (remain 4m 56s) Loss: 0.0009(0.0076) Grad: 19640.1973  LR: 0.000010  \n","Epoch: [3][2500/3575] Elapsed 10m 31s (remain 4m 31s) Loss: 0.0000(0.0076) Grad: 7.7926  LR: 0.000010  \n","Epoch: [3][2600/3575] Elapsed 10m 56s (remain 4m 5s) Loss: 0.0000(0.0077) Grad: 241.2624  LR: 0.000010  \n","Epoch: [3][2700/3575] Elapsed 11m 21s (remain 3m 40s) Loss: 0.0261(0.0076) Grad: 71544.2500  LR: 0.000010  \n","Epoch: [3][2800/3575] Elapsed 11m 45s (remain 3m 15s) Loss: 0.0036(0.0078) Grad: 40970.0859  LR: 0.000010  \n","Epoch: [3][2900/3575] Elapsed 12m 10s (remain 2m 49s) Loss: 0.0079(0.0077) Grad: 65728.2266  LR: 0.000010  \n","Epoch: [3][3000/3575] Elapsed 12m 35s (remain 2m 24s) Loss: 0.0000(0.0078) Grad: 69.4238  LR: 0.000010  \n","Epoch: [3][3100/3575] Elapsed 13m 0s (remain 1m 59s) Loss: 0.0000(0.0078) Grad: 160.2253  LR: 0.000009  \n","Epoch: [3][3200/3575] Elapsed 13m 25s (remain 1m 34s) Loss: 0.0011(0.0077) Grad: 7471.4463  LR: 0.000009  \n","Epoch: [3][3300/3575] Elapsed 13m 50s (remain 1m 8s) Loss: 0.0000(0.0077) Grad: 7.9928  LR: 0.000009  \n","Epoch: [3][3400/3575] Elapsed 14m 16s (remain 0m 43s) Loss: 0.0000(0.0077) Grad: 490.2837  LR: 0.000009  \n","Epoch: [3][3500/3575] Elapsed 14m 41s (remain 0m 18s) Loss: 0.0000(0.0078) Grad: 22.7775  LR: 0.000009  \n","Epoch: [3][3574/3575] Elapsed 14m 59s (remain 0m 0s) Loss: 0.0053(0.0078) Grad: 28913.2070  LR: 0.000009  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 6m 42s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 10s (remain 1m 56s) Loss: 0.0189(0.0125) \n","EVAL: [200/1192] Elapsed 0m 21s (remain 1m 45s) Loss: 0.0311(0.0142) \n","EVAL: [300/1192] Elapsed 0m 31s (remain 1m 34s) Loss: 0.0078(0.0145) \n","EVAL: [400/1192] Elapsed 0m 42s (remain 1m 23s) Loss: 0.0139(0.0150) \n","EVAL: [500/1192] Elapsed 0m 52s (remain 1m 13s) Loss: 0.0142(0.0140) \n","EVAL: [600/1192] Elapsed 1m 3s (remain 1m 2s) Loss: 0.0006(0.0146) \n","EVAL: [700/1192] Elapsed 1m 14s (remain 0m 51s) Loss: 0.2052(0.0181) \n","EVAL: [800/1192] Elapsed 1m 24s (remain 0m 41s) Loss: 0.0005(0.0186) \n","EVAL: [900/1192] Elapsed 1m 35s (remain 0m 30s) Loss: 0.0007(0.0186) \n","EVAL: [1000/1192] Elapsed 1m 45s (remain 0m 20s) Loss: 0.0000(0.0185) \n","EVAL: [1100/1192] Elapsed 1m 56s (remain 0m 9s) Loss: 0.0005(0.0178) \n","EVAL: [1191/1192] Elapsed 2m 5s (remain 0m 0s) Loss: 0.0000(0.0174) \n","Epoch 3 - avg_train_loss: 0.0078  avg_val_loss: 0.0174  time: 1030s\n","Epoch 3 - Score: 0.8772\n","Epoch 3 - Save Best Score: 0.8772 Model\n","Epoch: [4][0/3575] Elapsed 0m 0s (remain 33m 20s) Loss: 0.0001(0.0001) Grad: 282.0440  LR: 0.000009  \n","Epoch: [4][100/3575] Elapsed 0m 27s (remain 16m 2s) Loss: 0.0014(0.0082) Grad: 4859.6743  LR: 0.000009  \n","Epoch: [4][200/3575] Elapsed 0m 54s (remain 15m 22s) Loss: 0.0000(0.0081) Grad: 83.9210  LR: 0.000009  \n","Epoch: [4][300/3575] Elapsed 1m 20s (remain 14m 31s) Loss: 0.0000(0.0074) Grad: 26.4325  LR: 0.000009  \n","Epoch: [4][400/3575] Elapsed 1m 45s (remain 13m 52s) Loss: 0.0030(0.0068) Grad: 17496.2109  LR: 0.000008  \n","Epoch: [4][500/3575] Elapsed 2m 10s (remain 13m 20s) Loss: 0.0001(0.0067) Grad: 1578.6703  LR: 0.000008  \n","Epoch: [4][600/3575] Elapsed 2m 35s (remain 12m 49s) Loss: 0.0000(0.0069) Grad: 36.1712  LR: 0.000008  \n","Epoch: [4][700/3575] Elapsed 3m 0s (remain 12m 20s) Loss: 0.0022(0.0067) Grad: 15278.4521  LR: 0.000008  \n","Epoch: [4][800/3575] Elapsed 3m 25s (remain 11m 53s) Loss: 0.0000(0.0065) Grad: 23.5431  LR: 0.000008  \n","Epoch: [4][900/3575] Elapsed 3m 51s (remain 11m 25s) Loss: 0.0000(0.0061) Grad: 44.2829  LR: 0.000008  \n","Epoch: [4][1000/3575] Elapsed 4m 16s (remain 10m 58s) Loss: 0.0000(0.0060) Grad: 37.0945  LR: 0.000008  \n","Epoch: [4][1100/3575] Elapsed 4m 41s (remain 10m 32s) Loss: 0.0000(0.0061) Grad: 10.2176  LR: 0.000008  \n","Epoch: [4][1200/3575] Elapsed 5m 6s (remain 10m 5s) Loss: 0.0000(0.0059) Grad: 35.1024  LR: 0.000007  \n","Epoch: [4][1300/3575] Elapsed 5m 31s (remain 9m 39s) Loss: 0.0004(0.0061) Grad: 2176.9604  LR: 0.000007  \n","Epoch: [4][1400/3575] Elapsed 5m 56s (remain 9m 13s) Loss: 0.0057(0.0061) Grad: 60716.4727  LR: 0.000007  \n","Epoch: [4][1500/3575] Elapsed 6m 21s (remain 8m 47s) Loss: 0.0001(0.0060) Grad: 1053.3726  LR: 0.000007  \n","Epoch: [4][1600/3575] Elapsed 6m 46s (remain 8m 21s) Loss: 0.0000(0.0061) Grad: 255.9065  LR: 0.000007  \n","Epoch: [4][1700/3575] Elapsed 7m 11s (remain 7m 55s) Loss: 0.0000(0.0062) Grad: 3.3030  LR: 0.000007  \n","Epoch: [4][1800/3575] Elapsed 7m 37s (remain 7m 30s) Loss: 0.0000(0.0060) Grad: 46.2175  LR: 0.000007  \n","Epoch: [4][1900/3575] Elapsed 8m 2s (remain 7m 5s) Loss: 0.0000(0.0060) Grad: 465.3320  LR: 0.000007  \n","Epoch: [4][2000/3575] Elapsed 8m 28s (remain 6m 39s) Loss: 0.0002(0.0060) Grad: 4655.6587  LR: 0.000006  \n","Epoch: [4][2100/3575] Elapsed 8m 53s (remain 6m 14s) Loss: 0.0084(0.0060) Grad: 113068.0469  LR: 0.000006  \n","Epoch: [4][2200/3575] Elapsed 9m 18s (remain 5m 48s) Loss: 0.0057(0.0060) Grad: 49122.7930  LR: 0.000006  \n","Epoch: [4][2300/3575] Elapsed 9m 44s (remain 5m 23s) Loss: 0.0389(0.0061) Grad: 63538.9727  LR: 0.000006  \n","Epoch: [4][2400/3575] Elapsed 10m 9s (remain 4m 57s) Loss: 0.0000(0.0061) Grad: 13.2711  LR: 0.000006  \n","Epoch: [4][2500/3575] Elapsed 10m 34s (remain 4m 32s) Loss: 0.0000(0.0061) Grad: 76.6746  LR: 0.000006  \n","Epoch: [4][2600/3575] Elapsed 10m 59s (remain 4m 7s) Loss: 0.0000(0.0061) Grad: 15.5443  LR: 0.000006  \n","Epoch: [4][2700/3575] Elapsed 11m 25s (remain 3m 41s) Loss: 0.0549(0.0061) Grad: 66038.6719  LR: 0.000006  \n","Epoch: [4][2800/3575] Elapsed 11m 50s (remain 3m 16s) Loss: 0.0059(0.0061) Grad: 29666.0391  LR: 0.000005  \n","Epoch: [4][2900/3575] Elapsed 12m 16s (remain 2m 51s) Loss: 0.0068(0.0061) Grad: 183109.2188  LR: 0.000005  \n","Epoch: [4][3000/3575] Elapsed 12m 41s (remain 2m 25s) Loss: 0.0000(0.0060) Grad: 4.9729  LR: 0.000005  \n","Epoch: [4][3100/3575] Elapsed 13m 6s (remain 2m 0s) Loss: 0.0080(0.0062) Grad: 18802.6035  LR: 0.000005  \n","Epoch: [4][3200/3575] Elapsed 13m 32s (remain 1m 34s) Loss: 0.0000(0.0061) Grad: 99.8633  LR: 0.000005  \n","Epoch: [4][3300/3575] Elapsed 13m 57s (remain 1m 9s) Loss: 0.0000(0.0060) Grad: 19.5955  LR: 0.000005  \n","Epoch: [4][3400/3575] Elapsed 14m 22s (remain 0m 44s) Loss: 0.0011(0.0060) Grad: 15663.2920  LR: 0.000005  \n","Epoch: [4][3500/3575] Elapsed 14m 47s (remain 0m 18s) Loss: 0.0000(0.0059) Grad: 81.8250  LR: 0.000005  \n","Epoch: [4][3574/3575] Elapsed 15m 6s (remain 0m 0s) Loss: 0.0000(0.0058) Grad: 77.1890  LR: 0.000004  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 6m 51s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 10s (remain 1m 57s) Loss: 0.0218(0.0145) \n","EVAL: [200/1192] Elapsed 0m 21s (remain 1m 45s) Loss: 0.0515(0.0175) \n","EVAL: [300/1192] Elapsed 0m 31s (remain 1m 34s) Loss: 0.0038(0.0184) \n","EVAL: [400/1192] Elapsed 0m 42s (remain 1m 23s) Loss: 0.0181(0.0189) \n","EVAL: [500/1192] Elapsed 0m 53s (remain 1m 13s) Loss: 0.0097(0.0175) \n","EVAL: [600/1192] Elapsed 1m 3s (remain 1m 2s) Loss: 0.0022(0.0182) \n","EVAL: [700/1192] Elapsed 1m 14s (remain 0m 52s) Loss: 0.2327(0.0222) \n","EVAL: [800/1192] Elapsed 1m 24s (remain 0m 41s) Loss: 0.0001(0.0228) \n","EVAL: [900/1192] Elapsed 1m 35s (remain 0m 30s) Loss: 0.0062(0.0226) \n","EVAL: [1000/1192] Elapsed 1m 46s (remain 0m 20s) Loss: 0.0000(0.0223) \n","EVAL: [1100/1192] Elapsed 1m 56s (remain 0m 9s) Loss: 0.0001(0.0214) \n","EVAL: [1191/1192] Elapsed 2m 6s (remain 0m 0s) Loss: 0.0000(0.0208) \n","Epoch 4 - avg_train_loss: 0.0058  avg_val_loss: 0.0208  time: 1038s\n","Epoch 4 - Score: 0.8786\n","Epoch 4 - Save Best Score: 0.8786 Model\n","Epoch: [5][0/3575] Elapsed 0m 0s (remain 36m 37s) Loss: 0.0000(0.0000) Grad: 27.9837  LR: 0.000004  \n","Epoch: [5][100/3575] Elapsed 0m 29s (remain 16m 40s) Loss: 0.0000(0.0032) Grad: 14.2510  LR: 0.000004  \n","Epoch: [5][200/3575] Elapsed 0m 55s (remain 15m 33s) Loss: 0.0000(0.0032) Grad: 3.7679  LR: 0.000004  \n","Epoch: [5][300/3575] Elapsed 1m 20s (remain 14m 39s) Loss: 0.0000(0.0038) Grad: 1.5668  LR: 0.000004  \n","Epoch: [5][400/3575] Elapsed 1m 46s (remain 13m 59s) Loss: 0.0044(0.0034) Grad: 54834.0352  LR: 0.000004  \n","Epoch: [5][500/3575] Elapsed 2m 11s (remain 13m 26s) Loss: 0.0000(0.0033) Grad: 26.0899  LR: 0.000004  \n","Epoch: [5][600/3575] Elapsed 2m 36s (remain 12m 55s) Loss: 0.0089(0.0035) Grad: 16679.9199  LR: 0.000004  \n","Epoch: [5][700/3575] Elapsed 3m 2s (remain 12m 26s) Loss: 0.0000(0.0033) Grad: 0.9914  LR: 0.000004  \n","Epoch: [5][800/3575] Elapsed 3m 27s (remain 11m 58s) Loss: 0.0104(0.0035) Grad: 49634.1914  LR: 0.000003  \n","Epoch: [5][900/3575] Elapsed 3m 52s (remain 11m 29s) Loss: 0.0000(0.0038) Grad: 3.5231  LR: 0.000003  \n","Epoch: [5][1000/3575] Elapsed 4m 17s (remain 11m 3s) Loss: 0.2596(0.0042) Grad: 157032.6250  LR: 0.000003  \n","Epoch: [5][1100/3575] Elapsed 4m 43s (remain 10m 36s) Loss: 0.0081(0.0042) Grad: 68611.2266  LR: 0.000003  \n","Epoch: [5][1200/3575] Elapsed 5m 8s (remain 10m 9s) Loss: 0.0000(0.0041) Grad: 1.2727  LR: 0.000003  \n","Epoch: [5][1300/3575] Elapsed 5m 33s (remain 9m 43s) Loss: 0.0000(0.0041) Grad: 207.0542  LR: 0.000003  \n","Epoch: [5][1400/3575] Elapsed 5m 58s (remain 9m 16s) Loss: 0.0007(0.0044) Grad: 9370.1221  LR: 0.000003  \n","Epoch: [5][1500/3575] Elapsed 6m 23s (remain 8m 50s) Loss: 0.0000(0.0045) Grad: 411.3254  LR: 0.000003  \n","Epoch: [5][1600/3575] Elapsed 6m 49s (remain 8m 24s) Loss: 0.0000(0.0043) Grad: 88.0174  LR: 0.000002  \n","Epoch: [5][1700/3575] Elapsed 7m 14s (remain 7m 58s) Loss: 0.0000(0.0045) Grad: 10.2112  LR: 0.000002  \n","Epoch: [5][1800/3575] Elapsed 7m 39s (remain 7m 32s) Loss: 0.0080(0.0044) Grad: 14828.7598  LR: 0.000002  \n","Epoch: [5][1900/3575] Elapsed 8m 4s (remain 7m 6s) Loss: 0.0000(0.0044) Grad: 1.1723  LR: 0.000002  \n","Epoch: [5][2000/3575] Elapsed 8m 29s (remain 6m 41s) Loss: 0.0000(0.0043) Grad: 231.1073  LR: 0.000002  \n","Epoch: [5][2100/3575] Elapsed 8m 55s (remain 6m 15s) Loss: 0.0122(0.0043) Grad: 172600.7656  LR: 0.000002  \n","Epoch: [5][2200/3575] Elapsed 9m 20s (remain 5m 49s) Loss: 0.0105(0.0042) Grad: 26075.2168  LR: 0.000002  \n","Epoch: [5][2300/3575] Elapsed 9m 45s (remain 5m 24s) Loss: 0.0000(0.0043) Grad: 13.6824  LR: 0.000002  \n","Epoch: [5][2400/3575] Elapsed 10m 11s (remain 4m 58s) Loss: 0.0000(0.0044) Grad: 234.8713  LR: 0.000001  \n","Epoch: [5][2500/3575] Elapsed 10m 36s (remain 4m 33s) Loss: 0.0000(0.0044) Grad: 15.1818  LR: 0.000001  \n","Epoch: [5][2600/3575] Elapsed 11m 1s (remain 4m 7s) Loss: 0.0000(0.0043) Grad: 358.2542  LR: 0.000001  \n","Epoch: [5][2700/3575] Elapsed 11m 26s (remain 3m 42s) Loss: 0.0027(0.0044) Grad: 151218.4844  LR: 0.000001  \n","Epoch: [5][2800/3575] Elapsed 11m 51s (remain 3m 16s) Loss: 0.0018(0.0043) Grad: 73487.1797  LR: 0.000001  \n","Epoch: [5][2900/3575] Elapsed 12m 16s (remain 2m 51s) Loss: 0.0122(0.0043) Grad: 45408.7539  LR: 0.000001  \n","Epoch: [5][3000/3575] Elapsed 12m 41s (remain 2m 25s) Loss: 0.0000(0.0042) Grad: 108.2665  LR: 0.000001  \n","Epoch: [5][3100/3575] Elapsed 13m 6s (remain 2m 0s) Loss: 0.0084(0.0042) Grad: 187642.7344  LR: 0.000001  \n","Epoch: [5][3200/3575] Elapsed 13m 32s (remain 1m 34s) Loss: 0.0055(0.0043) Grad: 8903.4805  LR: 0.000000  \n","Epoch: [5][3300/3575] Elapsed 13m 57s (remain 1m 9s) Loss: 0.0000(0.0044) Grad: 44.7121  LR: 0.000000  \n","Epoch: [5][3400/3575] Elapsed 14m 22s (remain 0m 44s) Loss: 0.0000(0.0044) Grad: 568.6872  LR: 0.000000  \n","Epoch: [5][3500/3575] Elapsed 14m 47s (remain 0m 18s) Loss: 0.0000(0.0044) Grad: 20.6096  LR: 0.000000  \n","Epoch: [5][3574/3575] Elapsed 15m 5s (remain 0m 0s) Loss: 0.0074(0.0044) Grad: 16486.9941  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 7m 0s) Loss: 0.0001(0.0001) \n","EVAL: [100/1192] Elapsed 0m 10s (remain 1m 56s) Loss: 0.0241(0.0155) \n","EVAL: [200/1192] Elapsed 0m 21s (remain 1m 45s) Loss: 0.0452(0.0180) \n","EVAL: [300/1192] Elapsed 0m 31s (remain 1m 34s) Loss: 0.0046(0.0177) \n","EVAL: [400/1192] Elapsed 0m 42s (remain 1m 23s) Loss: 0.0189(0.0182) \n","EVAL: [500/1192] Elapsed 0m 53s (remain 1m 13s) Loss: 0.0148(0.0171) \n","EVAL: [600/1192] Elapsed 1m 3s (remain 1m 2s) Loss: 0.0303(0.0177) \n","EVAL: [700/1192] Elapsed 1m 14s (remain 0m 51s) Loss: 0.2350(0.0212) \n","EVAL: [800/1192] Elapsed 1m 24s (remain 0m 41s) Loss: 0.0001(0.0219) \n","EVAL: [900/1192] Elapsed 1m 35s (remain 0m 30s) Loss: 0.0103(0.0217) \n","EVAL: [1000/1192] Elapsed 1m 45s (remain 0m 20s) Loss: 0.0000(0.0214) \n","EVAL: [1100/1192] Elapsed 1m 56s (remain 0m 9s) Loss: 0.0001(0.0205) \n","EVAL: [1191/1192] Elapsed 2m 6s (remain 0m 0s) Loss: 0.0000(0.0200) \n","Epoch 5 - avg_train_loss: 0.0044  avg_val_loss: 0.0200  time: 1036s\n","Epoch 5 - Score: 0.8807\n","Epoch 5 - Save Best Score: 0.8807 Model\n","========== fold: 1 training ==========\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n","Epoch: [1][0/3575] Elapsed 0m 0s (remain 41m 20s) Loss: 0.7639(0.7639) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/3575] Elapsed 0m 29s (remain 16m 59s) Loss: 0.4525(0.6817) Grad: 76103.4609  LR: 0.000001  \n","Epoch: [1][200/3575] Elapsed 0m 54s (remain 15m 20s) Loss: 0.1557(0.4522) Grad: 8832.6104  LR: 0.000002  \n","Epoch: [1][300/3575] Elapsed 1m 20s (remain 14m 33s) Loss: 0.0382(0.3261) Grad: 4813.7144  LR: 0.000003  \n","Epoch: [1][400/3575] Elapsed 1m 45s (remain 13m 57s) Loss: 0.0416(0.2618) Grad: 3921.1021  LR: 0.000004  \n","Epoch: [1][500/3575] Elapsed 2m 11s (remain 13m 25s) Loss: 0.0299(0.2215) Grad: 4470.0410  LR: 0.000006  \n","Epoch: [1][600/3575] Elapsed 2m 36s (remain 12m 53s) Loss: 0.0191(0.1920) Grad: 3143.7700  LR: 0.000007  \n","Epoch: [1][700/3575] Elapsed 3m 1s (remain 12m 22s) Loss: 0.0113(0.1700) Grad: 3299.2634  LR: 0.000008  \n","Epoch: [1][800/3575] Elapsed 3m 26s (remain 11m 53s) Loss: 0.2819(0.1528) Grad: 22846.6484  LR: 0.000009  \n","Epoch: [1][900/3575] Elapsed 3m 51s (remain 11m 25s) Loss: 0.0397(0.1389) Grad: 6911.5562  LR: 0.000010  \n","Epoch: [1][1000/3575] Elapsed 4m 15s (remain 10m 58s) Loss: 0.0052(0.1270) Grad: 2723.6670  LR: 0.000011  \n","Epoch: [1][1100/3575] Elapsed 4m 40s (remain 10m 31s) Loss: 0.0040(0.1176) Grad: 3876.9724  LR: 0.000012  \n","Epoch: [1][1200/3575] Elapsed 5m 5s (remain 10m 4s) Loss: 0.0089(0.1092) Grad: 10057.8945  LR: 0.000013  \n","Epoch: [1][1300/3575] Elapsed 5m 31s (remain 9m 38s) Loss: 0.0003(0.1024) Grad: 245.2482  LR: 0.000015  \n","Epoch: [1][1400/3575] Elapsed 5m 55s (remain 9m 12s) Loss: 0.0103(0.0965) Grad: 4120.8198  LR: 0.000016  \n","Epoch: [1][1500/3575] Elapsed 6m 20s (remain 8m 46s) Loss: 0.0005(0.0914) Grad: 247.1536  LR: 0.000017  \n","Epoch: [1][1600/3575] Elapsed 6m 45s (remain 8m 20s) Loss: 0.0041(0.0870) Grad: 1530.2986  LR: 0.000018  \n","Epoch: [1][1700/3575] Elapsed 7m 10s (remain 7m 54s) Loss: 0.0043(0.0827) Grad: 2560.6318  LR: 0.000019  \n","Epoch: [1][1800/3575] Elapsed 7m 35s (remain 7m 28s) Loss: 0.0173(0.0790) Grad: 3306.8831  LR: 0.000020  \n","Epoch: [1][1900/3575] Elapsed 8m 0s (remain 7m 2s) Loss: 0.0025(0.0756) Grad: 1045.2264  LR: 0.000020  \n","Epoch: [1][2000/3575] Elapsed 8m 25s (remain 6m 37s) Loss: 0.0012(0.0727) Grad: 564.9401  LR: 0.000020  \n","Epoch: [1][2100/3575] Elapsed 8m 49s (remain 6m 11s) Loss: 0.0040(0.0700) Grad: 1301.8887  LR: 0.000020  \n","Epoch: [1][2200/3575] Elapsed 9m 14s (remain 5m 46s) Loss: 0.0026(0.0675) Grad: 2028.3474  LR: 0.000019  \n","Epoch: [1][2300/3575] Elapsed 9m 40s (remain 5m 21s) Loss: 0.0051(0.0652) Grad: 1437.6646  LR: 0.000019  \n","Epoch: [1][2400/3575] Elapsed 10m 5s (remain 4m 56s) Loss: 0.0024(0.0630) Grad: 1401.1959  LR: 0.000019  \n","Epoch: [1][2500/3575] Elapsed 10m 30s (remain 4m 30s) Loss: 0.0510(0.0612) Grad: 9247.7041  LR: 0.000019  \n","Epoch: [1][2600/3575] Elapsed 10m 55s (remain 4m 5s) Loss: 0.0002(0.0594) Grad: 281.2973  LR: 0.000019  \n","Epoch: [1][2700/3575] Elapsed 11m 20s (remain 3m 40s) Loss: 0.0408(0.0577) Grad: 10351.6230  LR: 0.000019  \n","Epoch: [1][2800/3575] Elapsed 11m 45s (remain 3m 14s) Loss: 0.0182(0.0561) Grad: 3913.1768  LR: 0.000019  \n","Epoch: [1][2900/3575] Elapsed 12m 10s (remain 2m 49s) Loss: 0.0243(0.0547) Grad: 18783.4004  LR: 0.000019  \n","Epoch: [1][3000/3575] Elapsed 12m 35s (remain 2m 24s) Loss: 0.0018(0.0534) Grad: 1629.4115  LR: 0.000018  \n","Epoch: [1][3100/3575] Elapsed 12m 59s (remain 1m 59s) Loss: 0.0000(0.0522) Grad: 43.1756  LR: 0.000018  \n","Epoch: [1][3200/3575] Elapsed 13m 24s (remain 1m 34s) Loss: 0.0184(0.0510) Grad: 4891.6001  LR: 0.000018  \n","Epoch: [1][3300/3575] Elapsed 13m 49s (remain 1m 8s) Loss: 0.0041(0.0498) Grad: 2677.2905  LR: 0.000018  \n","Epoch: [1][3400/3575] Elapsed 14m 14s (remain 0m 43s) Loss: 0.0072(0.0489) Grad: 3168.6362  LR: 0.000018  \n","Epoch: [1][3500/3575] Elapsed 14m 38s (remain 0m 18s) Loss: 0.0000(0.0479) Grad: 20.2201  LR: 0.000018  \n","Epoch: [1][3574/3575] Elapsed 14m 57s (remain 0m 0s) Loss: 0.0000(0.0470) Grad: 9.3223  LR: 0.000018  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 0s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 10s (remain 1m 58s) Loss: 0.0000(0.0113) \n","EVAL: [200/1192] Elapsed 0m 21s (remain 1m 45s) Loss: 0.0004(0.0151) \n","EVAL: [300/1192] Elapsed 0m 31s (remain 1m 34s) Loss: 0.0014(0.0221) \n","EVAL: [400/1192] Elapsed 0m 42s (remain 1m 23s) Loss: 0.0410(0.0222) \n","EVAL: [500/1192] Elapsed 0m 53s (remain 1m 13s) Loss: 0.0249(0.0211) \n","EVAL: [600/1192] Elapsed 1m 3s (remain 1m 2s) Loss: 0.2834(0.0213) \n","EVAL: [700/1192] Elapsed 1m 14s (remain 0m 51s) Loss: 0.0348(0.0237) \n","EVAL: [800/1192] Elapsed 1m 24s (remain 0m 41s) Loss: 0.0033(0.0228) \n","EVAL: [900/1192] Elapsed 1m 35s (remain 0m 30s) Loss: 0.0093(0.0233) \n","EVAL: [1000/1192] Elapsed 1m 45s (remain 0m 20s) Loss: 0.0000(0.0223) \n","EVAL: [1100/1192] Elapsed 1m 56s (remain 0m 9s) Loss: 0.0121(0.0210) \n","EVAL: [1191/1192] Elapsed 2m 5s (remain 0m 0s) Loss: 0.0205(0.0198) \n","Epoch 1 - avg_train_loss: 0.0470  avg_val_loss: 0.0198  time: 1028s\n","Epoch 1 - Score: 0.8584\n","Epoch 1 - Save Best Score: 0.8584 Model\n","Epoch: [2][0/3575] Elapsed 0m 0s (remain 38m 12s) Loss: 0.0604(0.0604) Grad: 210563.6406  LR: 0.000018  \n","Epoch: [2][100/3575] Elapsed 0m 28s (remain 16m 15s) Loss: 0.0184(0.0154) Grad: 15987.0049  LR: 0.000018  \n","Epoch: [2][200/3575] Elapsed 0m 54s (remain 15m 18s) Loss: 0.1962(0.0151) Grad: 58471.3672  LR: 0.000018  \n","Epoch: [2][300/3575] Elapsed 1m 19s (remain 14m 28s) Loss: 0.0001(0.0136) Grad: 864.6882  LR: 0.000017  \n","Epoch: [2][400/3575] Elapsed 1m 44s (remain 13m 49s) Loss: 0.0023(0.0134) Grad: 18436.9121  LR: 0.000017  \n","Epoch: [2][500/3575] Elapsed 2m 9s (remain 13m 16s) Loss: 0.0046(0.0133) Grad: 41667.6289  LR: 0.000017  \n","Epoch: [2][600/3575] Elapsed 2m 34s (remain 12m 44s) Loss: 0.0001(0.0130) Grad: 575.7168  LR: 0.000017  \n","Epoch: [2][700/3575] Elapsed 2m 59s (remain 12m 15s) Loss: 0.0000(0.0124) Grad: 64.9485  LR: 0.000017  \n","Epoch: [2][800/3575] Elapsed 3m 24s (remain 11m 47s) Loss: 0.0132(0.0119) Grad: 18923.1465  LR: 0.000017  \n","Epoch: [2][900/3575] Elapsed 3m 49s (remain 11m 19s) Loss: 0.0084(0.0124) Grad: 9847.9199  LR: 0.000017  \n","Epoch: [2][1000/3575] Elapsed 4m 13s (remain 10m 53s) Loss: 0.0160(0.0123) Grad: 24945.9316  LR: 0.000017  \n","Epoch: [2][1100/3575] Elapsed 4m 38s (remain 10m 26s) Loss: 0.0193(0.0121) Grad: 25352.0977  LR: 0.000016  \n","Epoch: [2][1200/3575] Elapsed 5m 3s (remain 10m 0s) Loss: 0.0000(0.0120) Grad: 13.0648  LR: 0.000016  \n","Epoch: [2][1300/3575] Elapsed 5m 28s (remain 9m 34s) Loss: 0.0001(0.0119) Grad: 1149.4067  LR: 0.000016  \n","Epoch: [2][1400/3575] Elapsed 5m 53s (remain 9m 8s) Loss: 0.0000(0.0119) Grad: 66.4423  LR: 0.000016  \n","Epoch: [2][1500/3575] Elapsed 6m 18s (remain 8m 42s) Loss: 0.0001(0.0121) Grad: 454.8453  LR: 0.000016  \n","Epoch: [2][1600/3575] Elapsed 6m 43s (remain 8m 16s) Loss: 0.0001(0.0118) Grad: 484.1637  LR: 0.000016  \n","Epoch: [2][1700/3575] Elapsed 7m 7s (remain 7m 51s) Loss: 0.0113(0.0118) Grad: 15026.5098  LR: 0.000016  \n","Epoch: [2][1800/3575] Elapsed 7m 32s (remain 7m 26s) Loss: 0.0000(0.0117) Grad: 22.8303  LR: 0.000016  \n","Epoch: [2][1900/3575] Elapsed 7m 57s (remain 7m 0s) Loss: 0.0049(0.0116) Grad: 18167.3223  LR: 0.000015  \n","Epoch: [2][2000/3575] Elapsed 8m 22s (remain 6m 35s) Loss: 0.0637(0.0116) Grad: 187689.5625  LR: 0.000015  \n","Epoch: [2][2100/3575] Elapsed 8m 47s (remain 6m 9s) Loss: 0.0000(0.0116) Grad: 99.2727  LR: 0.000015  \n","Epoch: [2][2200/3575] Elapsed 9m 12s (remain 5m 44s) Loss: 0.0214(0.0117) Grad: 67386.1562  LR: 0.000015  \n","Epoch: [2][2300/3575] Elapsed 9m 36s (remain 5m 19s) Loss: 0.0000(0.0115) Grad: 44.4260  LR: 0.000015  \n","Epoch: [2][2400/3575] Elapsed 10m 1s (remain 4m 54s) Loss: 0.0036(0.0114) Grad: 7898.4102  LR: 0.000015  \n","Epoch: [2][2500/3575] Elapsed 10m 26s (remain 4m 28s) Loss: 0.0007(0.0112) Grad: 1272.1985  LR: 0.000015  \n","Epoch: [2][2600/3575] Elapsed 10m 51s (remain 4m 3s) Loss: 0.0158(0.0112) Grad: 13359.2852  LR: 0.000015  \n","Epoch: [2][2700/3575] Elapsed 11m 16s (remain 3m 39s) Loss: 0.0003(0.0111) Grad: 506.7659  LR: 0.000014  \n","Epoch: [2][2800/3575] Elapsed 11m 42s (remain 3m 14s) Loss: 0.0001(0.0111) Grad: 102.6914  LR: 0.000014  \n","Epoch: [2][2900/3575] Elapsed 12m 7s (remain 2m 48s) Loss: 0.0188(0.0110) Grad: 14477.2930  LR: 0.000014  \n","Epoch: [2][3000/3575] Elapsed 12m 32s (remain 2m 23s) Loss: 0.0039(0.0110) Grad: 1959.0466  LR: 0.000014  \n","Epoch: [2][3100/3575] Elapsed 12m 57s (remain 1m 58s) Loss: 0.0014(0.0109) Grad: 3764.2949  LR: 0.000014  \n","Epoch: [2][3200/3575] Elapsed 13m 22s (remain 1m 33s) Loss: 0.0023(0.0108) Grad: 6015.6660  LR: 0.000014  \n","Epoch: [2][3300/3575] Elapsed 13m 47s (remain 1m 8s) Loss: 0.0004(0.0107) Grad: 243.4303  LR: 0.000014  \n","Epoch: [2][3400/3575] Elapsed 14m 12s (remain 0m 43s) Loss: 0.0170(0.0107) Grad: 8385.6016  LR: 0.000014  \n","Epoch: [2][3500/3575] Elapsed 14m 37s (remain 0m 18s) Loss: 0.0247(0.0106) Grad: 14255.8936  LR: 0.000013  \n","Epoch: [2][3574/3575] Elapsed 14m 56s (remain 0m 0s) Loss: 0.0000(0.0106) Grad: 28.2392  LR: 0.000013  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 7m 13s) Loss: 0.0001(0.0001) \n","EVAL: [100/1192] Elapsed 0m 10s (remain 1m 57s) Loss: 0.0001(0.0108) \n","EVAL: [200/1192] Elapsed 0m 21s (remain 1m 45s) Loss: 0.0004(0.0120) \n","EVAL: [300/1192] Elapsed 0m 31s (remain 1m 34s) Loss: 0.0011(0.0171) \n","EVAL: [400/1192] Elapsed 0m 42s (remain 1m 23s) Loss: 0.0345(0.0176) \n","EVAL: [500/1192] Elapsed 0m 53s (remain 1m 13s) Loss: 0.0333(0.0170) \n","EVAL: [600/1192] Elapsed 1m 3s (remain 1m 2s) Loss: 0.2056(0.0172) \n","EVAL: [700/1192] Elapsed 1m 14s (remain 0m 51s) Loss: 0.0179(0.0191) \n","EVAL: [800/1192] Elapsed 1m 24s (remain 0m 41s) Loss: 0.0484(0.0181) \n","EVAL: [900/1192] Elapsed 1m 35s (remain 0m 30s) Loss: 0.0063(0.0175) \n","EVAL: [1000/1192] Elapsed 1m 45s (remain 0m 20s) Loss: 0.0001(0.0169) \n","EVAL: [1100/1192] Elapsed 1m 56s (remain 0m 9s) Loss: 0.0074(0.0159) \n","EVAL: [1191/1192] Elapsed 2m 5s (remain 0m 0s) Loss: 0.0129(0.0150) \n","Epoch 2 - avg_train_loss: 0.0106  avg_val_loss: 0.0150  time: 1036s\n","Epoch 2 - Score: 0.8742\n","Epoch 2 - Save Best Score: 0.8742 Model\n","Epoch: [3][0/3575] Elapsed 0m 0s (remain 35m 9s) Loss: 0.0000(0.0000) Grad: 3.9058  LR: 0.000013  \n","Epoch: [3][100/3575] Elapsed 0m 28s (remain 16m 14s) Loss: 0.0047(0.0046) Grad: 12932.6553  LR: 0.000013  \n","Epoch: [3][200/3575] Elapsed 0m 54s (remain 15m 19s) Loss: 0.0000(0.0056) Grad: 16.8230  LR: 0.000013  \n","Epoch: [3][300/3575] Elapsed 1m 19s (remain 14m 26s) Loss: 0.0010(0.0080) Grad: 1172.5520  LR: 0.000013  \n","Epoch: [3][400/3575] Elapsed 1m 44s (remain 13m 48s) Loss: 0.0001(0.0078) Grad: 152.9169  LR: 0.000013  \n","Epoch: [3][500/3575] Elapsed 2m 9s (remain 13m 15s) Loss: 0.0019(0.0079) Grad: 4331.1104  LR: 0.000013  \n","Epoch: [3][600/3575] Elapsed 2m 34s (remain 12m 44s) Loss: 0.0000(0.0078) Grad: 15.9102  LR: 0.000013  \n","Epoch: [3][700/3575] Elapsed 2m 59s (remain 12m 15s) Loss: 0.0003(0.0079) Grad: 507.9085  LR: 0.000012  \n","Epoch: [3][800/3575] Elapsed 3m 24s (remain 11m 47s) Loss: 0.0026(0.0079) Grad: 5796.8369  LR: 0.000012  \n","Epoch: [3][900/3575] Elapsed 3m 49s (remain 11m 20s) Loss: 0.0000(0.0077) Grad: 50.3112  LR: 0.000012  \n","Epoch: [3][1000/3575] Elapsed 4m 14s (remain 10m 53s) Loss: 0.0000(0.0075) Grad: 25.5459  LR: 0.000012  \n","Epoch: [3][1100/3575] Elapsed 4m 39s (remain 10m 27s) Loss: 0.0016(0.0075) Grad: 2946.9570  LR: 0.000012  \n","Epoch: [3][1200/3575] Elapsed 5m 3s (remain 10m 0s) Loss: 0.0076(0.0074) Grad: 24112.0527  LR: 0.000012  \n","Epoch: [3][1300/3575] Elapsed 5m 28s (remain 9m 34s) Loss: 0.0000(0.0076) Grad: 61.5508  LR: 0.000012  \n","Epoch: [3][1400/3575] Elapsed 5m 53s (remain 9m 9s) Loss: 0.0003(0.0076) Grad: 426.6711  LR: 0.000012  \n","Epoch: [3][1500/3575] Elapsed 6m 18s (remain 8m 43s) Loss: 0.0411(0.0077) Grad: 24506.7949  LR: 0.000011  \n","Epoch: [3][1600/3575] Elapsed 6m 43s (remain 8m 17s) Loss: 0.0016(0.0076) Grad: 6865.0215  LR: 0.000011  \n","Epoch: [3][1700/3575] Elapsed 7m 8s (remain 7m 52s) Loss: 0.0000(0.0075) Grad: 58.5311  LR: 0.000011  \n","Epoch: [3][1800/3575] Elapsed 7m 33s (remain 7m 26s) Loss: 0.0137(0.0075) Grad: 18715.4746  LR: 0.000011  \n","Epoch: [3][1900/3575] Elapsed 7m 58s (remain 7m 1s) Loss: 0.0000(0.0076) Grad: 26.7740  LR: 0.000011  \n","Epoch: [3][2000/3575] Elapsed 8m 23s (remain 6m 35s) Loss: 0.0000(0.0075) Grad: 19.4201  LR: 0.000011  \n","Epoch: [3][2100/3575] Elapsed 8m 48s (remain 6m 10s) Loss: 0.0228(0.0075) Grad: 10984.8438  LR: 0.000011  \n","Epoch: [3][2200/3575] Elapsed 9m 13s (remain 5m 45s) Loss: 0.0001(0.0077) Grad: 1038.1924  LR: 0.000011  \n","Epoch: [3][2300/3575] Elapsed 9m 38s (remain 5m 20s) Loss: 0.0000(0.0076) Grad: 123.7460  LR: 0.000010  \n","Epoch: [3][2400/3575] Elapsed 10m 3s (remain 4m 54s) Loss: 0.0014(0.0078) Grad: 9129.8740  LR: 0.000010  \n","Epoch: [3][2500/3575] Elapsed 10m 27s (remain 4m 29s) Loss: 0.0020(0.0077) Grad: 28924.1582  LR: 0.000010  \n","Epoch: [3][2600/3575] Elapsed 10m 52s (remain 4m 4s) Loss: 0.0003(0.0078) Grad: 5691.5874  LR: 0.000010  \n","Epoch: [3][2700/3575] Elapsed 11m 17s (remain 3m 39s) Loss: 0.0010(0.0078) Grad: 24913.7266  LR: 0.000010  \n","Epoch: [3][2800/3575] Elapsed 11m 42s (remain 3m 14s) Loss: 0.0060(0.0079) Grad: 10905.1533  LR: 0.000010  \n","Epoch: [3][2900/3575] Elapsed 12m 7s (remain 2m 49s) Loss: 0.0048(0.0079) Grad: 20865.2188  LR: 0.000010  \n","Epoch: [3][3000/3575] Elapsed 12m 32s (remain 2m 23s) Loss: 0.0000(0.0079) Grad: 163.8993  LR: 0.000010  \n","Epoch: [3][3100/3575] Elapsed 12m 57s (remain 1m 58s) Loss: 0.0200(0.0078) Grad: 47683.3164  LR: 0.000009  \n","Epoch: [3][3200/3575] Elapsed 13m 22s (remain 1m 33s) Loss: 0.0027(0.0078) Grad: 14961.6729  LR: 0.000009  \n","Epoch: [3][3300/3575] Elapsed 13m 47s (remain 1m 8s) Loss: 0.0000(0.0078) Grad: 6.4694  LR: 0.000009  \n","Epoch: [3][3400/3575] Elapsed 14m 12s (remain 0m 43s) Loss: 0.0156(0.0078) Grad: 83860.7109  LR: 0.000009  \n","Epoch: [3][3500/3575] Elapsed 14m 36s (remain 0m 18s) Loss: 0.0005(0.0077) Grad: 1376.2942  LR: 0.000009  \n","Epoch: [3][3574/3575] Elapsed 14m 55s (remain 0m 0s) Loss: 0.0032(0.0078) Grad: 24534.6289  LR: 0.000009  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 7m 9s) Loss: 0.0001(0.0001) \n","EVAL: [100/1192] Elapsed 0m 10s (remain 1m 57s) Loss: 0.0001(0.0099) \n","EVAL: [200/1192] Elapsed 0m 21s (remain 1m 45s) Loss: 0.0015(0.0116) \n","EVAL: [300/1192] Elapsed 0m 31s (remain 1m 34s) Loss: 0.0048(0.0181) \n","EVAL: [400/1192] Elapsed 0m 42s (remain 1m 23s) Loss: 0.0339(0.0191) \n","EVAL: [500/1192] Elapsed 0m 52s (remain 1m 13s) Loss: 0.0420(0.0184) \n","EVAL: [600/1192] Elapsed 1m 3s (remain 1m 2s) Loss: 0.2318(0.0186) \n","EVAL: [700/1192] Elapsed 1m 14s (remain 0m 51s) Loss: 0.0298(0.0206) \n","EVAL: [800/1192] Elapsed 1m 24s (remain 0m 41s) Loss: 0.0728(0.0197) \n","EVAL: [900/1192] Elapsed 1m 35s (remain 0m 30s) Loss: 0.0004(0.0192) \n","EVAL: [1000/1192] Elapsed 1m 45s (remain 0m 20s) Loss: 0.0000(0.0186) \n","EVAL: [1100/1192] Elapsed 1m 56s (remain 0m 9s) Loss: 0.0097(0.0176) \n","EVAL: [1191/1192] Elapsed 2m 5s (remain 0m 0s) Loss: 0.0158(0.0166) \n","Epoch 3 - avg_train_loss: 0.0078  avg_val_loss: 0.0166  time: 1034s\n","Epoch 3 - Score: 0.8784\n","Epoch 3 - Save Best Score: 0.8784 Model\n","Epoch: [4][0/3575] Elapsed 0m 0s (remain 33m 32s) Loss: 0.0002(0.0002) Grad: 650.1974  LR: 0.000009  \n","Epoch: [4][100/3575] Elapsed 0m 28s (remain 16m 18s) Loss: 0.0000(0.0070) Grad: 19.2544  LR: 0.000009  \n","Epoch: [4][200/3575] Elapsed 0m 54s (remain 15m 21s) Loss: 0.0009(0.0069) Grad: 23150.9824  LR: 0.000009  \n","Epoch: [4][300/3575] Elapsed 1m 19s (remain 14m 28s) Loss: 0.0011(0.0065) Grad: 11229.4951  LR: 0.000009  \n","Epoch: [4][400/3575] Elapsed 1m 44s (remain 13m 49s) Loss: 0.0000(0.0059) Grad: 7.3042  LR: 0.000008  \n","Epoch: [4][500/3575] Elapsed 2m 9s (remain 13m 15s) Loss: 0.0334(0.0059) Grad: 27110.6777  LR: 0.000008  \n","Epoch: [4][600/3575] Elapsed 2m 34s (remain 12m 45s) Loss: 0.0000(0.0055) Grad: 1.5419  LR: 0.000008  \n","Epoch: [4][700/3575] Elapsed 2m 59s (remain 12m 16s) Loss: 0.0000(0.0056) Grad: 12.7824  LR: 0.000008  \n","Epoch: [4][800/3575] Elapsed 3m 24s (remain 11m 48s) Loss: 0.0000(0.0057) Grad: 11.9769  LR: 0.000008  \n","Epoch: [4][900/3575] Elapsed 3m 49s (remain 11m 20s) Loss: 0.0000(0.0055) Grad: 258.8888  LR: 0.000008  \n","Epoch: [4][1000/3575] Elapsed 4m 14s (remain 10m 53s) Loss: 0.0000(0.0056) Grad: 46.8991  LR: 0.000008  \n","Epoch: [4][1100/3575] Elapsed 4m 39s (remain 10m 27s) Loss: 0.0000(0.0054) Grad: 64.0057  LR: 0.000008  \n","Epoch: [4][1200/3575] Elapsed 5m 4s (remain 10m 0s) Loss: 0.0373(0.0056) Grad: 36788.0156  LR: 0.000007  \n","Epoch: [4][1300/3575] Elapsed 5m 28s (remain 9m 34s) Loss: 0.0000(0.0056) Grad: 21.5575  LR: 0.000007  \n","Epoch: [4][1400/3575] Elapsed 5m 53s (remain 9m 8s) Loss: 0.0000(0.0056) Grad: 100.7968  LR: 0.000007  \n","Epoch: [4][1500/3575] Elapsed 6m 18s (remain 8m 43s) Loss: 0.0000(0.0056) Grad: 96.6275  LR: 0.000007  \n","Epoch: [4][1600/3575] Elapsed 6m 43s (remain 8m 17s) Loss: 0.0039(0.0057) Grad: 26006.0957  LR: 0.000007  \n","Epoch: [4][1700/3575] Elapsed 7m 8s (remain 7m 52s) Loss: 0.0022(0.0055) Grad: 11920.5078  LR: 0.000007  \n","Epoch: [4][1800/3575] Elapsed 7m 33s (remain 7m 26s) Loss: 0.0000(0.0055) Grad: 7.6562  LR: 0.000007  \n","Epoch: [4][1900/3575] Elapsed 7m 58s (remain 7m 1s) Loss: 0.0000(0.0055) Grad: 2.9907  LR: 0.000007  \n","Epoch: [4][2000/3575] Elapsed 8m 24s (remain 6m 36s) Loss: 0.0311(0.0055) Grad: 112070.0078  LR: 0.000006  \n","Epoch: [4][2100/3575] Elapsed 8m 49s (remain 6m 11s) Loss: 0.0181(0.0055) Grad: 144481.9531  LR: 0.000006  \n","Epoch: [4][2200/3575] Elapsed 9m 14s (remain 5m 45s) Loss: 0.0003(0.0055) Grad: 6665.2334  LR: 0.000006  \n","Epoch: [4][2300/3575] Elapsed 9m 38s (remain 5m 20s) Loss: 0.0054(0.0054) Grad: 202341.2344  LR: 0.000006  \n","Epoch: [4][2400/3575] Elapsed 10m 3s (remain 4m 55s) Loss: 0.0000(0.0055) Grad: 36.4456  LR: 0.000006  \n","Epoch: [4][2500/3575] Elapsed 10m 28s (remain 4m 29s) Loss: 0.0001(0.0055) Grad: 7719.5410  LR: 0.000006  \n","Epoch: [4][2600/3575] Elapsed 10m 53s (remain 4m 4s) Loss: 0.0000(0.0055) Grad: 428.4294  LR: 0.000006  \n","Epoch: [4][2700/3575] Elapsed 11m 18s (remain 3m 39s) Loss: 0.0320(0.0055) Grad: 195768.2500  LR: 0.000006  \n","Epoch: [4][2800/3575] Elapsed 11m 43s (remain 3m 14s) Loss: 0.0092(0.0055) Grad: 133283.4844  LR: 0.000005  \n","Epoch: [4][2900/3575] Elapsed 12m 7s (remain 2m 49s) Loss: 0.0000(0.0056) Grad: 2.5191  LR: 0.000005  \n","Epoch: [4][3000/3575] Elapsed 12m 32s (remain 2m 24s) Loss: 0.0000(0.0057) Grad: 31.4624  LR: 0.000005  \n","Epoch: [4][3100/3575] Elapsed 12m 57s (remain 1m 58s) Loss: 0.0000(0.0057) Grad: 125.1317  LR: 0.000005  \n","Epoch: [4][3200/3575] Elapsed 13m 22s (remain 1m 33s) Loss: 0.0177(0.0057) Grad: 166532.1406  LR: 0.000005  \n","Epoch: [4][3300/3575] Elapsed 13m 47s (remain 1m 8s) Loss: 0.0024(0.0056) Grad: 62707.2734  LR: 0.000005  \n","Epoch: [4][3400/3575] Elapsed 14m 12s (remain 0m 43s) Loss: 0.0002(0.0056) Grad: 4918.2471  LR: 0.000005  \n","Epoch: [4][3500/3575] Elapsed 14m 37s (remain 0m 18s) Loss: 0.0897(0.0056) Grad: 228074.0156  LR: 0.000005  \n","Epoch: [4][3574/3575] Elapsed 14m 55s (remain 0m 0s) Loss: 0.0000(0.0056) Grad: 345.3208  LR: 0.000004  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 6m 56s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 10s (remain 1m 56s) Loss: 0.0000(0.0147) \n","EVAL: [200/1192] Elapsed 0m 21s (remain 1m 45s) Loss: 0.0000(0.0152) \n","EVAL: [300/1192] Elapsed 0m 31s (remain 1m 34s) Loss: 0.0003(0.0237) \n","EVAL: [400/1192] Elapsed 0m 42s (remain 1m 23s) Loss: 0.0581(0.0252) \n","EVAL: [500/1192] Elapsed 0m 52s (remain 1m 12s) Loss: 0.0536(0.0237) \n","EVAL: [600/1192] Elapsed 1m 3s (remain 1m 2s) Loss: 0.2769(0.0239) \n","EVAL: [700/1192] Elapsed 1m 14s (remain 0m 51s) Loss: 0.0519(0.0270) \n","EVAL: [800/1192] Elapsed 1m 24s (remain 0m 41s) Loss: 0.0598(0.0258) \n","EVAL: [900/1192] Elapsed 1m 35s (remain 0m 30s) Loss: 0.0002(0.0254) \n","EVAL: [1000/1192] Elapsed 1m 45s (remain 0m 20s) Loss: 0.0000(0.0246) \n","EVAL: [1100/1192] Elapsed 1m 56s (remain 0m 9s) Loss: 0.0115(0.0233) \n","EVAL: [1191/1192] Elapsed 2m 5s (remain 0m 0s) Loss: 0.0201(0.0220) \n","Epoch 4 - avg_train_loss: 0.0056  avg_val_loss: 0.0220  time: 1038s\n","Epoch 4 - Score: 0.8809\n","Epoch 4 - Save Best Score: 0.8809 Model\n","Epoch: [5][0/3575] Elapsed 0m 0s (remain 31m 48s) Loss: 0.0000(0.0000) Grad: 14.7807  LR: 0.000004  \n","Epoch: [5][100/3575] Elapsed 0m 27s (remain 15m 50s) Loss: 0.0000(0.0044) Grad: 5.3597  LR: 0.000004  \n","Epoch: [5][200/3575] Elapsed 0m 54s (remain 15m 11s) Loss: 0.0000(0.0043) Grad: 19.6748  LR: 0.000004  \n","Epoch: [5][300/3575] Elapsed 1m 19s (remain 14m 27s) Loss: 0.0003(0.0038) Grad: 13844.3516  LR: 0.000004  \n","Epoch: [5][400/3575] Elapsed 1m 45s (remain 13m 52s) Loss: 0.0000(0.0038) Grad: 115.0193  LR: 0.000004  \n","Epoch: [5][500/3575] Elapsed 2m 10s (remain 13m 17s) Loss: 0.0700(0.0043) Grad: 50546.6523  LR: 0.000004  \n","Epoch: [5][600/3575] Elapsed 2m 35s (remain 12m 48s) Loss: 0.0000(0.0041) Grad: 0.8520  LR: 0.000004  \n","Epoch: [5][700/3575] Elapsed 3m 0s (remain 12m 18s) Loss: 0.0000(0.0042) Grad: 2.6780  LR: 0.000004  \n","Epoch: [5][800/3575] Elapsed 3m 25s (remain 11m 50s) Loss: 0.0091(0.0041) Grad: 32679.4473  LR: 0.000003  \n","Epoch: [5][900/3575] Elapsed 3m 50s (remain 11m 23s) Loss: 0.0046(0.0041) Grad: 10896.9219  LR: 0.000003  \n","Epoch: [5][1000/3575] Elapsed 4m 15s (remain 10m 56s) Loss: 0.1291(0.0045) Grad: 56418.2578  LR: 0.000003  \n","Epoch: [5][1100/3575] Elapsed 4m 39s (remain 10m 29s) Loss: 0.0000(0.0044) Grad: 21.0736  LR: 0.000003  \n","Epoch: [5][1200/3575] Elapsed 5m 4s (remain 10m 2s) Loss: 0.0019(0.0044) Grad: 10385.8760  LR: 0.000003  \n","Epoch: [5][1300/3575] Elapsed 5m 29s (remain 9m 36s) Loss: 0.0000(0.0043) Grad: 0.9555  LR: 0.000003  \n","Epoch: [5][1400/3575] Elapsed 5m 54s (remain 9m 10s) Loss: 0.0001(0.0042) Grad: 364.4143  LR: 0.000003  \n","Epoch: [5][1500/3575] Elapsed 6m 19s (remain 8m 44s) Loss: 0.0056(0.0041) Grad: 3564.7419  LR: 0.000003  \n","Epoch: [5][1600/3575] Elapsed 6m 44s (remain 8m 18s) Loss: 0.0000(0.0041) Grad: 10.3575  LR: 0.000002  \n","Epoch: [5][1700/3575] Elapsed 7m 9s (remain 7m 53s) Loss: 0.0000(0.0043) Grad: 62.3523  LR: 0.000002  \n","Epoch: [5][1800/3575] Elapsed 7m 34s (remain 7m 27s) Loss: 0.0000(0.0042) Grad: 33.2805  LR: 0.000002  \n","Epoch: [5][1900/3575] Elapsed 7m 59s (remain 7m 1s) Loss: 0.0000(0.0042) Grad: 14.7049  LR: 0.000002  \n","Epoch: [5][2000/3575] Elapsed 8m 23s (remain 6m 36s) Loss: 0.0000(0.0042) Grad: 18.1828  LR: 0.000002  \n","Epoch: [5][2100/3575] Elapsed 8m 48s (remain 6m 10s) Loss: 0.0000(0.0041) Grad: 52.2116  LR: 0.000002  \n","Epoch: [5][2200/3575] Elapsed 9m 13s (remain 5m 45s) Loss: 0.0000(0.0040) Grad: 21.2036  LR: 0.000002  \n","Epoch: [5][2300/3575] Elapsed 9m 38s (remain 5m 20s) Loss: 0.0622(0.0040) Grad: 12727.0430  LR: 0.000002  \n","Epoch: [5][2400/3575] Elapsed 10m 3s (remain 4m 54s) Loss: 0.0154(0.0039) Grad: 14786.1328  LR: 0.000001  \n","Epoch: [5][2500/3575] Elapsed 10m 28s (remain 4m 29s) Loss: 0.0010(0.0039) Grad: 7379.2178  LR: 0.000001  \n","Epoch: [5][2600/3575] Elapsed 10m 52s (remain 4m 4s) Loss: 0.0004(0.0039) Grad: 904.2933  LR: 0.000001  \n","Epoch: [5][2700/3575] Elapsed 11m 17s (remain 3m 39s) Loss: 0.0000(0.0039) Grad: 12.8825  LR: 0.000001  \n","Epoch: [5][2800/3575] Elapsed 11m 42s (remain 3m 14s) Loss: 0.0000(0.0039) Grad: 30.4610  LR: 0.000001  \n","Epoch: [5][2900/3575] Elapsed 12m 7s (remain 2m 49s) Loss: 0.0005(0.0039) Grad: 1375.5852  LR: 0.000001  \n","Epoch: [5][3000/3575] Elapsed 12m 32s (remain 2m 23s) Loss: 0.0136(0.0039) Grad: 428093.1250  LR: 0.000001  \n","Epoch: [5][3100/3575] Elapsed 12m 57s (remain 1m 58s) Loss: 0.0000(0.0039) Grad: 27.0041  LR: 0.000001  \n","Epoch: [5][3200/3575] Elapsed 13m 22s (remain 1m 33s) Loss: 0.0000(0.0039) Grad: 39.7546  LR: 0.000000  \n","Epoch: [5][3300/3575] Elapsed 13m 46s (remain 1m 8s) Loss: 0.0003(0.0039) Grad: 7590.4561  LR: 0.000000  \n","Epoch: [5][3400/3575] Elapsed 14m 11s (remain 0m 43s) Loss: 0.0017(0.0039) Grad: 31654.5098  LR: 0.000000  \n","Epoch: [5][3500/3575] Elapsed 14m 36s (remain 0m 18s) Loss: 0.0000(0.0040) Grad: 48.0063  LR: 0.000000  \n","Epoch: [5][3574/3575] Elapsed 14m 54s (remain 0m 0s) Loss: 0.0000(0.0040) Grad: 86.1287  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 6m 45s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 10s (remain 1m 56s) Loss: 0.0000(0.0143) \n","EVAL: [200/1192] Elapsed 0m 21s (remain 1m 45s) Loss: 0.0000(0.0154) \n","EVAL: [300/1192] Elapsed 0m 31s (remain 1m 34s) Loss: 0.0006(0.0231) \n","EVAL: [400/1192] Elapsed 0m 42s (remain 1m 23s) Loss: 0.0548(0.0243) \n","EVAL: [500/1192] Elapsed 0m 52s (remain 1m 12s) Loss: 0.0500(0.0229) \n","EVAL: [600/1192] Elapsed 1m 3s (remain 1m 2s) Loss: 0.2482(0.0231) \n","EVAL: [700/1192] Elapsed 1m 14s (remain 0m 51s) Loss: 0.0458(0.0264) \n","EVAL: [800/1192] Elapsed 1m 24s (remain 0m 41s) Loss: 0.0826(0.0254) \n","EVAL: [900/1192] Elapsed 1m 35s (remain 0m 30s) Loss: 0.0007(0.0250) \n","EVAL: [1000/1192] Elapsed 1m 45s (remain 0m 20s) Loss: 0.0000(0.0243) \n","EVAL: [1100/1192] Elapsed 1m 56s (remain 0m 9s) Loss: 0.0116(0.0231) \n","EVAL: [1191/1192] Elapsed 2m 5s (remain 0m 0s) Loss: 0.0195(0.0219) \n","Epoch 5 - avg_train_loss: 0.0040  avg_val_loss: 0.0219  time: 1025s\n","Epoch 5 - Score: 0.8793\n","========== fold: 2 training ==========\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n","Epoch: [1][0/3575] Elapsed 0m 0s (remain 37m 25s) Loss: 0.5796(0.5796) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/3575] Elapsed 0m 25s (remain 14m 43s) Loss: 0.3829(0.5389) Grad: 68036.3047  LR: 0.000001  \n","Epoch: [1][200/3575] Elapsed 0m 50s (remain 14m 11s) Loss: 0.1025(0.3628) Grad: 6721.5000  LR: 0.000002  \n","Epoch: [1][300/3575] Elapsed 1m 15s (remain 13m 44s) Loss: 0.0649(0.2679) Grad: 2046.1560  LR: 0.000003  \n","Epoch: [1][400/3575] Elapsed 1m 40s (remain 13m 16s) Loss: 0.1483(0.2172) Grad: 8144.3867  LR: 0.000004  \n","Epoch: [1][500/3575] Elapsed 2m 5s (remain 12m 51s) Loss: 0.0492(0.1845) Grad: 5871.5195  LR: 0.000006  \n","Epoch: [1][600/3575] Elapsed 2m 30s (remain 12m 25s) Loss: 0.0195(0.1597) Grad: 4977.7080  LR: 0.000007  \n","Epoch: [1][700/3575] Elapsed 2m 55s (remain 11m 59s) Loss: 0.0060(0.1411) Grad: 2427.2039  LR: 0.000008  \n","Epoch: [1][800/3575] Elapsed 3m 20s (remain 11m 33s) Loss: 0.0177(0.1266) Grad: 3597.5596  LR: 0.000009  \n","Epoch: [1][900/3575] Elapsed 3m 45s (remain 11m 8s) Loss: 0.0038(0.1150) Grad: 1438.3818  LR: 0.000010  \n","Epoch: [1][1000/3575] Elapsed 4m 10s (remain 10m 43s) Loss: 0.0468(0.1056) Grad: 5002.1421  LR: 0.000011  \n","Epoch: [1][1100/3575] Elapsed 4m 35s (remain 10m 18s) Loss: 0.0026(0.0976) Grad: 1521.8110  LR: 0.000012  \n","Epoch: [1][1200/3575] Elapsed 5m 0s (remain 9m 53s) Loss: 0.0003(0.0910) Grad: 590.5865  LR: 0.000013  \n","Epoch: [1][1300/3575] Elapsed 5m 25s (remain 9m 28s) Loss: 0.0003(0.0855) Grad: 222.2172  LR: 0.000015  \n","Epoch: [1][1400/3575] Elapsed 5m 50s (remain 9m 3s) Loss: 0.0044(0.0807) Grad: 3388.3394  LR: 0.000016  \n","Epoch: [1][1500/3575] Elapsed 6m 15s (remain 8m 38s) Loss: 0.0012(0.0766) Grad: 529.9586  LR: 0.000017  \n","Epoch: [1][1600/3575] Elapsed 6m 40s (remain 8m 14s) Loss: 0.0052(0.0730) Grad: 1217.1761  LR: 0.000018  \n","Epoch: [1][1700/3575] Elapsed 7m 5s (remain 7m 49s) Loss: 0.0000(0.0697) Grad: 39.5525  LR: 0.000019  \n","Epoch: [1][1800/3575] Elapsed 7m 31s (remain 7m 24s) Loss: 0.0512(0.0672) Grad: 14312.1533  LR: 0.000020  \n","Epoch: [1][1900/3575] Elapsed 7m 56s (remain 6m 59s) Loss: 0.0224(0.0644) Grad: 4814.9971  LR: 0.000020  \n","Epoch: [1][2000/3575] Elapsed 8m 22s (remain 6m 34s) Loss: 0.0230(0.0620) Grad: 2813.2905  LR: 0.000020  \n","Epoch: [1][2100/3575] Elapsed 8m 47s (remain 6m 9s) Loss: 0.0144(0.0598) Grad: 7184.6880  LR: 0.000020  \n","Epoch: [1][2200/3575] Elapsed 9m 12s (remain 5m 45s) Loss: 0.0166(0.0582) Grad: 10050.0352  LR: 0.000019  \n","Epoch: [1][2300/3575] Elapsed 9m 37s (remain 5m 20s) Loss: 0.0058(0.0562) Grad: 28337.9297  LR: 0.000019  \n","Epoch: [1][2400/3575] Elapsed 10m 3s (remain 4m 54s) Loss: 0.0024(0.0546) Grad: 2145.8301  LR: 0.000019  \n","Epoch: [1][2500/3575] Elapsed 10m 28s (remain 4m 29s) Loss: 0.0190(0.0531) Grad: 7152.0449  LR: 0.000019  \n","Epoch: [1][2600/3575] Elapsed 10m 53s (remain 4m 4s) Loss: 0.0000(0.0516) Grad: 79.2394  LR: 0.000019  \n","Epoch: [1][2700/3575] Elapsed 11m 18s (remain 3m 39s) Loss: 0.1370(0.0504) Grad: 39798.4297  LR: 0.000019  \n","Epoch: [1][2800/3575] Elapsed 11m 43s (remain 3m 14s) Loss: 0.0021(0.0492) Grad: 1550.9830  LR: 0.000019  \n","Epoch: [1][2900/3575] Elapsed 12m 9s (remain 2m 49s) Loss: 0.0071(0.0480) Grad: 2885.8892  LR: 0.000019  \n","Epoch: [1][3000/3575] Elapsed 12m 34s (remain 2m 24s) Loss: 0.0133(0.0468) Grad: 10724.6104  LR: 0.000018  \n","Epoch: [1][3100/3575] Elapsed 12m 59s (remain 1m 59s) Loss: 0.0475(0.0458) Grad: 16174.0859  LR: 0.000018  \n","Epoch: [1][3200/3575] Elapsed 13m 24s (remain 1m 34s) Loss: 0.0002(0.0448) Grad: 183.1799  LR: 0.000018  \n","Epoch: [1][3300/3575] Elapsed 13m 49s (remain 1m 8s) Loss: 0.0031(0.0439) Grad: 3833.6138  LR: 0.000018  \n","Epoch: [1][3400/3575] Elapsed 14m 14s (remain 0m 43s) Loss: 0.0273(0.0430) Grad: 14534.4277  LR: 0.000018  \n","Epoch: [1][3500/3575] Elapsed 14m 39s (remain 0m 18s) Loss: 0.0030(0.0422) Grad: 2508.9480  LR: 0.000018  \n","Epoch: [1][3574/3575] Elapsed 14m 58s (remain 0m 0s) Loss: 0.0082(0.0416) Grad: 16460.2090  LR: 0.000018  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 6s) Loss: 0.0001(0.0001) \n","EVAL: [100/1192] Elapsed 0m 10s (remain 1m 57s) Loss: 0.0456(0.0124) \n","EVAL: [200/1192] Elapsed 0m 21s (remain 1m 45s) Loss: 0.0255(0.0130) \n","EVAL: [300/1192] Elapsed 0m 31s (remain 1m 34s) Loss: 0.0417(0.0133) \n","EVAL: [400/1192] Elapsed 0m 42s (remain 1m 23s) Loss: 0.0051(0.0143) \n","EVAL: [500/1192] Elapsed 0m 53s (remain 1m 13s) Loss: 0.0011(0.0131) \n","EVAL: [600/1192] Elapsed 1m 3s (remain 1m 2s) Loss: 0.0351(0.0136) \n","EVAL: [700/1192] Elapsed 1m 14s (remain 0m 51s) Loss: 0.0136(0.0156) \n","EVAL: [800/1192] Elapsed 1m 24s (remain 0m 41s) Loss: 0.0001(0.0154) \n","EVAL: [900/1192] Elapsed 1m 35s (remain 0m 30s) Loss: 0.0093(0.0158) \n","EVAL: [1000/1192] Elapsed 1m 45s (remain 0m 20s) Loss: 0.0179(0.0154) \n","EVAL: [1100/1192] Elapsed 1m 56s (remain 0m 9s) Loss: 0.0291(0.0148) \n","EVAL: [1191/1192] Elapsed 2m 5s (remain 0m 0s) Loss: 0.0001(0.0141) \n","Epoch 1 - avg_train_loss: 0.0416  avg_val_loss: 0.0141  time: 1029s\n","Epoch 1 - Score: 0.8592\n","Epoch 1 - Save Best Score: 0.8592 Model\n","Epoch: [2][0/3575] Elapsed 0m 0s (remain 38m 58s) Loss: 0.0001(0.0001) Grad: 1250.9128  LR: 0.000018  \n","Epoch: [2][100/3575] Elapsed 0m 29s (remain 17m 11s) Loss: 0.0163(0.0107) Grad: 16841.8965  LR: 0.000018  \n","Epoch: [2][200/3575] Elapsed 0m 55s (remain 15m 34s) Loss: 0.0041(0.0095) Grad: 7507.1919  LR: 0.000018  \n","Epoch: [2][300/3575] Elapsed 1m 20s (remain 14m 40s) Loss: 0.0048(0.0095) Grad: 12721.2236  LR: 0.000017  \n","Epoch: [2][400/3575] Elapsed 1m 46s (remain 14m 0s) Loss: 0.0001(0.0102) Grad: 637.7357  LR: 0.000017  \n","Epoch: [2][500/3575] Elapsed 2m 11s (remain 13m 26s) Loss: 0.0009(0.0102) Grad: 14342.9658  LR: 0.000017  \n","Epoch: [2][600/3575] Elapsed 2m 36s (remain 12m 55s) Loss: 0.0000(0.0105) Grad: 40.0078  LR: 0.000017  \n","Epoch: [2][700/3575] Elapsed 3m 1s (remain 12m 26s) Loss: 0.0001(0.0108) Grad: 261.5266  LR: 0.000017  \n","Epoch: [2][800/3575] Elapsed 3m 27s (remain 11m 57s) Loss: 0.0612(0.0107) Grad: 279077.4062  LR: 0.000017  \n","Epoch: [2][900/3575] Elapsed 3m 52s (remain 11m 29s) Loss: 0.0001(0.0110) Grad: 360.2436  LR: 0.000017  \n","Epoch: [2][1000/3575] Elapsed 4m 17s (remain 11m 1s) Loss: 0.0006(0.0112) Grad: 3933.2571  LR: 0.000017  \n","Epoch: [2][1100/3575] Elapsed 4m 42s (remain 10m 34s) Loss: 0.0012(0.0112) Grad: 9663.3008  LR: 0.000016  \n","Epoch: [2][1200/3575] Elapsed 5m 7s (remain 10m 7s) Loss: 0.0059(0.0114) Grad: 18851.0938  LR: 0.000016  \n","Epoch: [2][1300/3575] Elapsed 5m 32s (remain 9m 41s) Loss: 0.0003(0.0115) Grad: 1238.0631  LR: 0.000016  \n","Epoch: [2][1400/3575] Elapsed 5m 57s (remain 9m 15s) Loss: 0.0559(0.0113) Grad: 74525.1641  LR: 0.000016  \n","Epoch: [2][1500/3575] Elapsed 6m 22s (remain 8m 48s) Loss: 0.0000(0.0112) Grad: 277.7308  LR: 0.000016  \n","Epoch: [2][1600/3575] Elapsed 6m 47s (remain 8m 22s) Loss: 0.0008(0.0112) Grad: 15642.0234  LR: 0.000016  \n","Epoch: [2][1700/3575] Elapsed 7m 12s (remain 7m 56s) Loss: 0.0000(0.0112) Grad: 278.2835  LR: 0.000016  \n","Epoch: [2][1800/3575] Elapsed 7m 38s (remain 7m 31s) Loss: 0.0000(0.0111) Grad: 31.1217  LR: 0.000016  \n","Epoch: [2][1900/3575] Elapsed 8m 3s (remain 7m 5s) Loss: 0.0001(0.0113) Grad: 435.5986  LR: 0.000015  \n","Epoch: [2][2000/3575] Elapsed 8m 28s (remain 6m 39s) Loss: 0.0000(0.0111) Grad: 171.6778  LR: 0.000015  \n","Epoch: [2][2100/3575] Elapsed 8m 53s (remain 6m 14s) Loss: 0.0276(0.0110) Grad: 82778.1641  LR: 0.000015  \n","Epoch: [2][2200/3575] Elapsed 9m 18s (remain 5m 48s) Loss: 0.0000(0.0110) Grad: 127.9371  LR: 0.000015  \n","Epoch: [2][2300/3575] Elapsed 9m 43s (remain 5m 23s) Loss: 0.0006(0.0113) Grad: 6738.4258  LR: 0.000015  \n","Epoch: [2][2400/3575] Elapsed 10m 8s (remain 4m 57s) Loss: 0.0156(0.0113) Grad: 73482.0625  LR: 0.000015  \n","Epoch: [2][2500/3575] Elapsed 10m 33s (remain 4m 32s) Loss: 0.0171(0.0111) Grad: 37592.6680  LR: 0.000015  \n","Epoch: [2][2600/3575] Elapsed 10m 58s (remain 4m 6s) Loss: 0.0004(0.0112) Grad: 4796.8696  LR: 0.000015  \n","Epoch: [2][2700/3575] Elapsed 11m 23s (remain 3m 41s) Loss: 0.1237(0.0111) Grad: 243337.7812  LR: 0.000014  \n","Epoch: [2][2800/3575] Elapsed 11m 48s (remain 3m 15s) Loss: 0.0000(0.0111) Grad: 568.0900  LR: 0.000014  \n","Epoch: [2][2900/3575] Elapsed 12m 14s (remain 2m 50s) Loss: 0.0050(0.0111) Grad: 143283.3281  LR: 0.000014  \n","Epoch: [2][3000/3575] Elapsed 12m 39s (remain 2m 25s) Loss: 0.0003(0.0111) Grad: 1858.8019  LR: 0.000014  \n","Epoch: [2][3100/3575] Elapsed 13m 4s (remain 1m 59s) Loss: 0.0029(0.0111) Grad: 32033.9414  LR: 0.000014  \n","Epoch: [2][3200/3575] Elapsed 13m 29s (remain 1m 34s) Loss: 0.0017(0.0110) Grad: 9186.9619  LR: 0.000014  \n","Epoch: [2][3300/3575] Elapsed 13m 54s (remain 1m 9s) Loss: 0.0027(0.0110) Grad: 28757.9883  LR: 0.000014  \n","Epoch: [2][3400/3575] Elapsed 14m 19s (remain 0m 43s) Loss: 0.0279(0.0110) Grad: 184598.2969  LR: 0.000014  \n","Epoch: [2][3500/3575] Elapsed 14m 44s (remain 0m 18s) Loss: 0.0001(0.0110) Grad: 1009.7701  LR: 0.000013  \n","Epoch: [2][3574/3575] Elapsed 15m 3s (remain 0m 0s) Loss: 0.0000(0.0110) Grad: 383.9027  LR: 0.000013  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 7m 24s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 10s (remain 1m 57s) Loss: 0.0636(0.0154) \n","EVAL: [200/1192] Elapsed 0m 21s (remain 1m 45s) Loss: 0.0173(0.0137) \n","EVAL: [300/1192] Elapsed 0m 31s (remain 1m 34s) Loss: 0.0344(0.0133) \n","EVAL: [400/1192] Elapsed 0m 42s (remain 1m 23s) Loss: 0.0001(0.0138) \n","EVAL: [500/1192] Elapsed 0m 53s (remain 1m 13s) Loss: 0.0000(0.0129) \n","EVAL: [600/1192] Elapsed 1m 3s (remain 1m 2s) Loss: 0.0136(0.0131) \n","EVAL: [700/1192] Elapsed 1m 14s (remain 0m 51s) Loss: 0.0115(0.0144) \n","EVAL: [800/1192] Elapsed 1m 24s (remain 0m 41s) Loss: 0.0000(0.0144) \n","EVAL: [900/1192] Elapsed 1m 35s (remain 0m 30s) Loss: 0.0111(0.0146) \n","EVAL: [1000/1192] Elapsed 1m 45s (remain 0m 20s) Loss: 0.0100(0.0141) \n","EVAL: [1100/1192] Elapsed 1m 56s (remain 0m 9s) Loss: 0.0300(0.0134) \n","EVAL: [1191/1192] Elapsed 2m 6s (remain 0m 0s) Loss: 0.0000(0.0131) \n","Epoch 2 - avg_train_loss: 0.0110  avg_val_loss: 0.0131  time: 1034s\n","Epoch 2 - Score: 0.8853\n","Epoch 2 - Save Best Score: 0.8853 Model\n","Epoch: [3][0/3575] Elapsed 0m 0s (remain 36m 49s) Loss: 0.0001(0.0001) Grad: 589.6915  LR: 0.000013  \n","Epoch: [3][100/3575] Elapsed 0m 28s (remain 16m 35s) Loss: 0.0112(0.0063) Grad: 58840.8906  LR: 0.000013  \n","Epoch: [3][200/3575] Elapsed 0m 55s (remain 15m 37s) Loss: 0.0499(0.0068) Grad: 114964.4688  LR: 0.000013  \n","Epoch: [3][300/3575] Elapsed 1m 21s (remain 14m 43s) Loss: 0.0000(0.0072) Grad: 151.4543  LR: 0.000013  \n","Epoch: [3][400/3575] Elapsed 1m 46s (remain 14m 3s) Loss: 0.0001(0.0077) Grad: 1287.8660  LR: 0.000013  \n","Epoch: [3][500/3575] Elapsed 2m 11s (remain 13m 29s) Loss: 0.0125(0.0081) Grad: 153849.9844  LR: 0.000013  \n","Epoch: [3][600/3575] Elapsed 2m 37s (remain 12m 59s) Loss: 0.0000(0.0082) Grad: 59.2440  LR: 0.000013  \n","Epoch: [3][700/3575] Elapsed 3m 2s (remain 12m 30s) Loss: 0.0313(0.0084) Grad: 47435.3125  LR: 0.000012  \n","Epoch: [3][800/3575] Elapsed 3m 28s (remain 12m 2s) Loss: 0.0000(0.0082) Grad: 92.0016  LR: 0.000012  \n","Epoch: [3][900/3575] Elapsed 3m 53s (remain 11m 34s) Loss: 0.0019(0.0084) Grad: 12711.9609  LR: 0.000012  \n","Epoch: [3][1000/3575] Elapsed 4m 19s (remain 11m 7s) Loss: 0.0000(0.0081) Grad: 116.3535  LR: 0.000012  \n","Epoch: [3][1100/3575] Elapsed 4m 44s (remain 10m 39s) Loss: 0.0001(0.0082) Grad: 866.9534  LR: 0.000012  \n","Epoch: [3][1200/3575] Elapsed 5m 9s (remain 10m 12s) Loss: 0.0000(0.0081) Grad: 28.7234  LR: 0.000012  \n","Epoch: [3][1300/3575] Elapsed 5m 35s (remain 9m 46s) Loss: 0.0006(0.0080) Grad: 5757.4253  LR: 0.000012  \n","Epoch: [3][1400/3575] Elapsed 6m 0s (remain 9m 20s) Loss: 0.0006(0.0080) Grad: 4307.7344  LR: 0.000012  \n","Epoch: [3][1500/3575] Elapsed 6m 26s (remain 8m 53s) Loss: 0.0015(0.0079) Grad: 6267.2544  LR: 0.000011  \n","Epoch: [3][1600/3575] Elapsed 6m 51s (remain 8m 27s) Loss: 0.0014(0.0080) Grad: 6619.6328  LR: 0.000011  \n","Epoch: [3][1700/3575] Elapsed 7m 16s (remain 8m 1s) Loss: 0.0222(0.0078) Grad: 13100.7471  LR: 0.000011  \n","Epoch: [3][1800/3575] Elapsed 7m 42s (remain 7m 35s) Loss: 0.0000(0.0078) Grad: 40.9291  LR: 0.000011  \n","Epoch: [3][1900/3575] Elapsed 8m 7s (remain 7m 9s) Loss: 0.0194(0.0078) Grad: 13161.7012  LR: 0.000011  \n","Epoch: [3][2000/3575] Elapsed 8m 33s (remain 6m 43s) Loss: 0.0005(0.0078) Grad: 2992.1487  LR: 0.000011  \n","Epoch: [3][2100/3575] Elapsed 8m 58s (remain 6m 17s) Loss: 0.0305(0.0078) Grad: 15236.4414  LR: 0.000011  \n","Epoch: [3][2200/3575] Elapsed 9m 23s (remain 5m 51s) Loss: 0.0092(0.0078) Grad: 13524.9697  LR: 0.000011  \n","Epoch: [3][2300/3575] Elapsed 9m 48s (remain 5m 25s) Loss: 0.0274(0.0078) Grad: 34846.3594  LR: 0.000010  \n","Epoch: [3][2400/3575] Elapsed 10m 14s (remain 5m 0s) Loss: 0.0004(0.0078) Grad: 1154.8932  LR: 0.000010  \n","Epoch: [3][2500/3575] Elapsed 10m 39s (remain 4m 34s) Loss: 0.0105(0.0078) Grad: 22568.6973  LR: 0.000010  \n","Epoch: [3][2600/3575] Elapsed 11m 4s (remain 4m 8s) Loss: 0.0004(0.0078) Grad: 1001.7398  LR: 0.000010  \n","Epoch: [3][2700/3575] Elapsed 11m 29s (remain 3m 43s) Loss: 0.0001(0.0079) Grad: 110.8291  LR: 0.000010  \n","Epoch: [3][2800/3575] Elapsed 11m 55s (remain 3m 17s) Loss: 0.0393(0.0080) Grad: 17995.4121  LR: 0.000010  \n","Epoch: [3][2900/3575] Elapsed 12m 20s (remain 2m 52s) Loss: 0.0066(0.0080) Grad: 4246.0210  LR: 0.000010  \n","Epoch: [3][3000/3575] Elapsed 12m 45s (remain 2m 26s) Loss: 0.0000(0.0080) Grad: 2.6548  LR: 0.000010  \n","Epoch: [3][3100/3575] Elapsed 13m 10s (remain 2m 0s) Loss: 0.0097(0.0080) Grad: 5045.3818  LR: 0.000009  \n","Epoch: [3][3200/3575] Elapsed 13m 35s (remain 1m 35s) Loss: 0.0608(0.0080) Grad: 25605.8770  LR: 0.000009  \n","Epoch: [3][3300/3575] Elapsed 14m 1s (remain 1m 9s) Loss: 0.0000(0.0080) Grad: 18.9063  LR: 0.000009  \n","Epoch: [3][3400/3575] Elapsed 14m 26s (remain 0m 44s) Loss: 0.0001(0.0080) Grad: 260.1535  LR: 0.000009  \n","Epoch: [3][3500/3575] Elapsed 14m 51s (remain 0m 18s) Loss: 0.0001(0.0081) Grad: 135.6804  LR: 0.000009  \n","Epoch: [3][3574/3575] Elapsed 15m 10s (remain 0m 0s) Loss: 0.0072(0.0081) Grad: 7858.0464  LR: 0.000009  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 9s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 10s (remain 1m 58s) Loss: 0.0796(0.0148) \n","EVAL: [200/1192] Elapsed 0m 21s (remain 1m 46s) Loss: 0.0250(0.0141) \n","EVAL: [300/1192] Elapsed 0m 32s (remain 1m 35s) Loss: 0.0196(0.0129) \n","EVAL: [400/1192] Elapsed 0m 42s (remain 1m 24s) Loss: 0.0003(0.0134) \n","EVAL: [500/1192] Elapsed 0m 53s (remain 1m 13s) Loss: 0.0003(0.0123) \n","EVAL: [600/1192] Elapsed 1m 3s (remain 1m 2s) Loss: 0.0119(0.0128) \n","EVAL: [700/1192] Elapsed 1m 14s (remain 0m 52s) Loss: 0.0126(0.0143) \n","EVAL: [800/1192] Elapsed 1m 25s (remain 0m 41s) Loss: 0.0000(0.0142) \n","EVAL: [900/1192] Elapsed 1m 35s (remain 0m 30s) Loss: 0.0108(0.0142) \n","EVAL: [1000/1192] Elapsed 1m 46s (remain 0m 20s) Loss: 0.0003(0.0139) \n","EVAL: [1100/1192] Elapsed 1m 56s (remain 0m 9s) Loss: 0.0534(0.0133) \n","EVAL: [1191/1192] Elapsed 2m 6s (remain 0m 0s) Loss: 0.0001(0.0128) \n","Epoch 3 - avg_train_loss: 0.0081  avg_val_loss: 0.0128  time: 1042s\n","Epoch 3 - Score: 0.8880\n","Epoch 3 - Save Best Score: 0.8880 Model\n","Epoch: [4][0/3575] Elapsed 0m 0s (remain 34m 48s) Loss: 0.0032(0.0032) Grad: 13473.9258  LR: 0.000009  \n","Epoch: [4][100/3575] Elapsed 0m 29s (remain 16m 46s) Loss: 0.0002(0.0046) Grad: 3405.3369  LR: 0.000009  \n","Epoch: [4][200/3575] Elapsed 0m 56s (remain 15m 45s) Loss: 0.0044(0.0047) Grad: 16844.0957  LR: 0.000009  \n","Epoch: [4][300/3575] Elapsed 1m 21s (remain 14m 50s) Loss: 0.0012(0.0049) Grad: 11152.2158  LR: 0.000009  \n","Epoch: [4][400/3575] Elapsed 1m 47s (remain 14m 8s) Loss: 0.0005(0.0046) Grad: 3524.0642  LR: 0.000008  \n","Epoch: [4][500/3575] Elapsed 2m 13s (remain 13m 36s) Loss: 0.0017(0.0052) Grad: 34012.6523  LR: 0.000008  \n","Epoch: [4][600/3575] Elapsed 2m 38s (remain 13m 3s) Loss: 0.0002(0.0051) Grad: 736.3150  LR: 0.000008  \n","Epoch: [4][700/3575] Elapsed 3m 3s (remain 12m 33s) Loss: 0.0182(0.0056) Grad: 16223.3057  LR: 0.000008  \n","Epoch: [4][800/3575] Elapsed 3m 29s (remain 12m 3s) Loss: 0.0060(0.0056) Grad: 13905.7656  LR: 0.000008  \n","Epoch: [4][900/3575] Elapsed 3m 54s (remain 11m 35s) Loss: 0.0000(0.0057) Grad: 15.9455  LR: 0.000008  \n","Epoch: [4][1000/3575] Elapsed 4m 19s (remain 11m 7s) Loss: 0.0000(0.0057) Grad: 94.2811  LR: 0.000008  \n","Epoch: [4][1100/3575] Elapsed 4m 44s (remain 10m 39s) Loss: 0.0002(0.0059) Grad: 1583.4087  LR: 0.000008  \n","Epoch: [4][1200/3575] Elapsed 5m 10s (remain 10m 12s) Loss: 0.0189(0.0059) Grad: 45127.7422  LR: 0.000007  \n","Epoch: [4][1300/3575] Elapsed 5m 35s (remain 9m 46s) Loss: 0.0002(0.0059) Grad: 915.3323  LR: 0.000007  \n","Epoch: [4][1400/3575] Elapsed 6m 0s (remain 9m 19s) Loss: 0.0002(0.0060) Grad: 1740.7946  LR: 0.000007  \n","Epoch: [4][1500/3575] Elapsed 6m 25s (remain 8m 53s) Loss: 0.0297(0.0059) Grad: 40332.0156  LR: 0.000007  \n","Epoch: [4][1600/3575] Elapsed 6m 51s (remain 8m 27s) Loss: 0.0003(0.0058) Grad: 508.3194  LR: 0.000007  \n","Epoch: [4][1700/3575] Elapsed 7m 16s (remain 8m 0s) Loss: 0.0000(0.0059) Grad: 1.0205  LR: 0.000007  \n","Epoch: [4][1800/3575] Elapsed 7m 41s (remain 7m 34s) Loss: 0.0028(0.0058) Grad: 22690.7324  LR: 0.000007  \n","Epoch: [4][1900/3575] Elapsed 8m 7s (remain 7m 8s) Loss: 0.0175(0.0058) Grad: 27331.0254  LR: 0.000007  \n","Epoch: [4][2000/3575] Elapsed 8m 32s (remain 6m 42s) Loss: 0.0077(0.0058) Grad: 4208.0181  LR: 0.000006  \n","Epoch: [4][2100/3575] Elapsed 8m 57s (remain 6m 17s) Loss: 0.0001(0.0059) Grad: 121.1602  LR: 0.000006  \n","Epoch: [4][2200/3575] Elapsed 9m 22s (remain 5m 51s) Loss: 0.0019(0.0060) Grad: 4382.5830  LR: 0.000006  \n","Epoch: [4][2300/3575] Elapsed 9m 48s (remain 5m 25s) Loss: 0.0046(0.0060) Grad: 16473.0703  LR: 0.000006  \n","Epoch: [4][2400/3575] Elapsed 10m 13s (remain 4m 59s) Loss: 0.0000(0.0059) Grad: 83.4225  LR: 0.000006  \n","Epoch: [4][2500/3575] Elapsed 10m 38s (remain 4m 34s) Loss: 0.0001(0.0059) Grad: 483.9774  LR: 0.000006  \n","Epoch: [4][2600/3575] Elapsed 11m 4s (remain 4m 8s) Loss: 0.0000(0.0059) Grad: 0.9891  LR: 0.000006  \n","Epoch: [4][2700/3575] Elapsed 11m 29s (remain 3m 43s) Loss: 0.0023(0.0059) Grad: 30920.3633  LR: 0.000006  \n","Epoch: [4][2800/3575] Elapsed 11m 54s (remain 3m 17s) Loss: 0.0619(0.0059) Grad: 35651.4492  LR: 0.000005  \n","Epoch: [4][2900/3575] Elapsed 12m 20s (remain 2m 51s) Loss: 0.0091(0.0059) Grad: 7982.2896  LR: 0.000005  \n","Epoch: [4][3000/3575] Elapsed 12m 45s (remain 2m 26s) Loss: 0.0012(0.0059) Grad: 7967.0298  LR: 0.000005  \n","Epoch: [4][3100/3575] Elapsed 13m 11s (remain 2m 0s) Loss: 0.0001(0.0060) Grad: 342.6360  LR: 0.000005  \n","Epoch: [4][3200/3575] Elapsed 13m 36s (remain 1m 35s) Loss: 0.0000(0.0059) Grad: 128.1241  LR: 0.000005  \n","Epoch: [4][3300/3575] Elapsed 14m 1s (remain 1m 9s) Loss: 0.0003(0.0059) Grad: 3071.9077  LR: 0.000005  \n","Epoch: [4][3400/3575] Elapsed 14m 27s (remain 0m 44s) Loss: 0.0117(0.0060) Grad: 18404.1836  LR: 0.000005  \n","Epoch: [4][3500/3575] Elapsed 14m 52s (remain 0m 18s) Loss: 0.0058(0.0059) Grad: 63727.4961  LR: 0.000005  \n","Epoch: [4][3574/3575] Elapsed 15m 11s (remain 0m 0s) Loss: 0.0150(0.0059) Grad: 14501.8135  LR: 0.000004  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 7m 8s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 10s (remain 1m 57s) Loss: 0.0943(0.0196) \n","EVAL: [200/1192] Elapsed 0m 21s (remain 1m 45s) Loss: 0.0345(0.0183) \n","EVAL: [300/1192] Elapsed 0m 31s (remain 1m 34s) Loss: 0.0441(0.0170) \n","EVAL: [400/1192] Elapsed 0m 42s (remain 1m 23s) Loss: 0.0001(0.0174) \n","EVAL: [500/1192] Elapsed 0m 53s (remain 1m 13s) Loss: 0.0000(0.0160) \n","EVAL: [600/1192] Elapsed 1m 3s (remain 1m 2s) Loss: 0.0247(0.0160) \n","EVAL: [700/1192] Elapsed 1m 14s (remain 0m 52s) Loss: 0.0137(0.0176) \n","EVAL: [800/1192] Elapsed 1m 24s (remain 0m 41s) Loss: 0.0000(0.0174) \n","EVAL: [900/1192] Elapsed 1m 35s (remain 0m 30s) Loss: 0.0221(0.0174) \n","EVAL: [1000/1192] Elapsed 1m 46s (remain 0m 20s) Loss: 0.0001(0.0172) \n","EVAL: [1100/1192] Elapsed 1m 56s (remain 0m 9s) Loss: 0.0435(0.0165) \n","EVAL: [1191/1192] Elapsed 2m 6s (remain 0m 0s) Loss: 0.0000(0.0159) \n","Epoch 4 - avg_train_loss: 0.0059  avg_val_loss: 0.0159  time: 1047s\n","Epoch 4 - Score: 0.8881\n","Epoch 4 - Save Best Score: 0.8881 Model\n","Epoch: [5][0/3575] Elapsed 0m 0s (remain 35m 35s) Loss: 0.0251(0.0251) Grad: 35059.8789  LR: 0.000004  \n","Epoch: [5][100/3575] Elapsed 0m 28s (remain 16m 27s) Loss: 0.0014(0.0040) Grad: 9229.0361  LR: 0.000004  \n","Epoch: [5][200/3575] Elapsed 0m 55s (remain 15m 39s) Loss: 0.0023(0.0044) Grad: 5401.4116  LR: 0.000004  \n","Epoch: [5][300/3575] Elapsed 1m 21s (remain 14m 45s) Loss: 0.0161(0.0043) Grad: 97798.1328  LR: 0.000004  \n","Epoch: [5][400/3575] Elapsed 1m 46s (remain 14m 5s) Loss: 0.0001(0.0040) Grad: 1455.5978  LR: 0.000004  \n","Epoch: [5][500/3575] Elapsed 2m 12s (remain 13m 32s) Loss: 0.0000(0.0039) Grad: 1.1052  LR: 0.000004  \n","Epoch: [5][600/3575] Elapsed 2m 38s (remain 13m 5s) Loss: 0.0003(0.0040) Grad: 2953.5090  LR: 0.000004  \n","Epoch: [5][700/3575] Elapsed 3m 4s (remain 12m 35s) Loss: 0.0001(0.0042) Grad: 313.1857  LR: 0.000004  \n","Epoch: [5][800/3575] Elapsed 3m 29s (remain 12m 6s) Loss: 0.0000(0.0041) Grad: 2.3442  LR: 0.000003  \n","Epoch: [5][900/3575] Elapsed 3m 55s (remain 11m 38s) Loss: 0.0072(0.0040) Grad: 42991.7109  LR: 0.000003  \n","Epoch: [5][1000/3575] Elapsed 4m 20s (remain 11m 10s) Loss: 0.0000(0.0045) Grad: 1.8942  LR: 0.000003  \n","Epoch: [5][1100/3575] Elapsed 4m 46s (remain 10m 42s) Loss: 0.0000(0.0044) Grad: 11.3767  LR: 0.000003  \n","Epoch: [5][1200/3575] Elapsed 5m 11s (remain 10m 15s) Loss: 0.0000(0.0044) Grad: 29.7797  LR: 0.000003  \n","Epoch: [5][1300/3575] Elapsed 5m 36s (remain 9m 48s) Loss: 0.0066(0.0044) Grad: 65920.6641  LR: 0.000003  \n","Epoch: [5][1400/3575] Elapsed 6m 2s (remain 9m 22s) Loss: 0.0111(0.0044) Grad: 87393.3203  LR: 0.000003  \n","Epoch: [5][1500/3575] Elapsed 6m 27s (remain 8m 55s) Loss: 0.0000(0.0043) Grad: 4.2992  LR: 0.000003  \n","Epoch: [5][1600/3575] Elapsed 6m 53s (remain 8m 29s) Loss: 0.0000(0.0043) Grad: 1.5578  LR: 0.000002  \n","Epoch: [5][1700/3575] Elapsed 7m 19s (remain 8m 3s) Loss: 0.0000(0.0043) Grad: 2.7391  LR: 0.000002  \n","Epoch: [5][1800/3575] Elapsed 7m 44s (remain 7m 37s) Loss: 0.0797(0.0043) Grad: 44487.3477  LR: 0.000002  \n","Epoch: [5][1900/3575] Elapsed 8m 10s (remain 7m 11s) Loss: 0.0000(0.0043) Grad: 19.0384  LR: 0.000002  \n","Epoch: [5][2000/3575] Elapsed 8m 35s (remain 6m 45s) Loss: 0.0119(0.0042) Grad: 478682.8125  LR: 0.000002  \n","Epoch: [5][2100/3575] Elapsed 9m 0s (remain 6m 19s) Loss: 0.0595(0.0042) Grad: 284405.9375  LR: 0.000002  \n","Epoch: [5][2200/3575] Elapsed 9m 26s (remain 5m 53s) Loss: 0.0001(0.0043) Grad: 1751.9811  LR: 0.000002  \n","Epoch: [5][2300/3575] Elapsed 9m 51s (remain 5m 27s) Loss: 0.0000(0.0043) Grad: 44.0288  LR: 0.000002  \n","Epoch: [5][2400/3575] Elapsed 10m 17s (remain 5m 1s) Loss: 0.0000(0.0043) Grad: 2.1943  LR: 0.000001  \n","Epoch: [5][2500/3575] Elapsed 10m 42s (remain 4m 36s) Loss: 0.0000(0.0043) Grad: 56.2601  LR: 0.000001  \n","Epoch: [5][2600/3575] Elapsed 11m 8s (remain 4m 10s) Loss: 0.0000(0.0042) Grad: 1.6351  LR: 0.000001  \n","Epoch: [5][2700/3575] Elapsed 11m 33s (remain 3m 44s) Loss: 0.0053(0.0043) Grad: 15047.6689  LR: 0.000001  \n","Epoch: [5][2800/3575] Elapsed 11m 59s (remain 3m 18s) Loss: 0.0000(0.0043) Grad: 13.8952  LR: 0.000001  \n","Epoch: [5][2900/3575] Elapsed 12m 24s (remain 2m 53s) Loss: 0.0000(0.0043) Grad: 1.2713  LR: 0.000001  \n","Epoch: [5][3000/3575] Elapsed 12m 50s (remain 2m 27s) Loss: 0.0000(0.0042) Grad: 2.2587  LR: 0.000001  \n","Epoch: [5][3100/3575] Elapsed 13m 15s (remain 2m 1s) Loss: 0.0004(0.0042) Grad: 4627.2300  LR: 0.000001  \n","Epoch: [5][3200/3575] Elapsed 13m 41s (remain 1m 35s) Loss: 0.0000(0.0042) Grad: 27.9963  LR: 0.000000  \n","Epoch: [5][3300/3575] Elapsed 14m 7s (remain 1m 10s) Loss: 0.0078(0.0043) Grad: 13282.7578  LR: 0.000000  \n","Epoch: [5][3400/3575] Elapsed 14m 32s (remain 0m 44s) Loss: 0.0036(0.0043) Grad: 3672.5881  LR: 0.000000  \n","Epoch: [5][3500/3575] Elapsed 14m 58s (remain 0m 18s) Loss: 0.0000(0.0043) Grad: 2.5574  LR: 0.000000  \n","Epoch: [5][3574/3575] Elapsed 15m 17s (remain 0m 0s) Loss: 0.0000(0.0043) Grad: 3.1975  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 7m 9s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 10s (remain 1m 57s) Loss: 0.1034(0.0226) \n","EVAL: [200/1192] Elapsed 0m 21s (remain 1m 45s) Loss: 0.0327(0.0200) \n","EVAL: [300/1192] Elapsed 0m 32s (remain 1m 34s) Loss: 0.0368(0.0186) \n","EVAL: [400/1192] Elapsed 0m 42s (remain 1m 24s) Loss: 0.0000(0.0190) \n","EVAL: [500/1192] Elapsed 0m 53s (remain 1m 13s) Loss: 0.0000(0.0175) \n","EVAL: [600/1192] Elapsed 1m 3s (remain 1m 2s) Loss: 0.0354(0.0178) \n","EVAL: [700/1192] Elapsed 1m 14s (remain 0m 52s) Loss: 0.0139(0.0196) \n","EVAL: [800/1192] Elapsed 1m 24s (remain 0m 41s) Loss: 0.0000(0.0195) \n","EVAL: [900/1192] Elapsed 1m 35s (remain 0m 30s) Loss: 0.0301(0.0196) \n","EVAL: [1000/1192] Elapsed 1m 46s (remain 0m 20s) Loss: 0.0000(0.0193) \n","EVAL: [1100/1192] Elapsed 1m 56s (remain 0m 9s) Loss: 0.0663(0.0185) \n","EVAL: [1191/1192] Elapsed 2m 6s (remain 0m 0s) Loss: 0.0000(0.0178) \n","Epoch 5 - avg_train_loss: 0.0043  avg_val_loss: 0.0178  time: 1052s\n","Epoch 5 - Score: 0.8850\n","========== fold: 3 training ==========\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n","Epoch: [1][0/3575] Elapsed 0m 0s (remain 34m 36s) Loss: 0.9128(0.9128) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/3575] Elapsed 0m 25s (remain 14m 53s) Loss: 0.5877(0.8121) Grad: 92846.7266  LR: 0.000001  \n","Epoch: [1][200/3575] Elapsed 0m 51s (remain 14m 19s) Loss: 0.1163(0.5295) Grad: 2681.4958  LR: 0.000002  \n","Epoch: [1][300/3575] Elapsed 1m 16s (remain 13m 53s) Loss: 0.0504(0.3789) Grad: 799.7103  LR: 0.000003  \n","Epoch: [1][400/3575] Elapsed 1m 41s (remain 13m 27s) Loss: 0.0281(0.3022) Grad: 1188.7480  LR: 0.000004  \n","Epoch: [1][500/3575] Elapsed 2m 7s (remain 13m 1s) Loss: 0.0837(0.2519) Grad: 3804.4475  LR: 0.000006  \n","Epoch: [1][600/3575] Elapsed 2m 32s (remain 12m 35s) Loss: 0.0091(0.2159) Grad: 985.7546  LR: 0.000007  \n","Epoch: [1][700/3575] Elapsed 2m 57s (remain 12m 9s) Loss: 0.0013(0.1892) Grad: 241.4822  LR: 0.000008  \n","Epoch: [1][800/3575] Elapsed 3m 23s (remain 11m 43s) Loss: 0.0285(0.1685) Grad: 5427.2715  LR: 0.000009  \n","Epoch: [1][900/3575] Elapsed 3m 48s (remain 11m 18s) Loss: 0.0053(0.1520) Grad: 733.5159  LR: 0.000010  \n","Epoch: [1][1000/3575] Elapsed 4m 13s (remain 10m 52s) Loss: 0.0008(0.1388) Grad: 544.4250  LR: 0.000011  \n","Epoch: [1][1100/3575] Elapsed 4m 39s (remain 10m 27s) Loss: 0.0149(0.1287) Grad: 1600.1302  LR: 0.000012  \n","Epoch: [1][1200/3575] Elapsed 5m 4s (remain 10m 2s) Loss: 0.0025(0.1198) Grad: 735.6554  LR: 0.000013  \n","Epoch: [1][1300/3575] Elapsed 5m 30s (remain 9m 37s) Loss: 0.0010(0.1120) Grad: 274.0169  LR: 0.000015  \n","Epoch: [1][1400/3575] Elapsed 5m 55s (remain 9m 11s) Loss: 0.0106(0.1052) Grad: 2037.4330  LR: 0.000016  \n","Epoch: [1][1500/3575] Elapsed 6m 21s (remain 8m 46s) Loss: 0.0337(0.0994) Grad: 5014.4248  LR: 0.000017  \n","Epoch: [1][1600/3575] Elapsed 6m 46s (remain 8m 21s) Loss: 0.0549(0.0941) Grad: 2991.7329  LR: 0.000018  \n","Epoch: [1][1700/3575] Elapsed 7m 11s (remain 7m 55s) Loss: 0.0183(0.0896) Grad: 2186.8718  LR: 0.000019  \n","Epoch: [1][1800/3575] Elapsed 7m 36s (remain 7m 30s) Loss: 0.0150(0.0856) Grad: 3175.7502  LR: 0.000020  \n","Epoch: [1][1900/3575] Elapsed 8m 2s (remain 7m 4s) Loss: 0.0036(0.0819) Grad: 1237.3901  LR: 0.000020  \n","Epoch: [1][2000/3575] Elapsed 8m 27s (remain 6m 39s) Loss: 0.0045(0.0785) Grad: 626.4385  LR: 0.000020  \n","Epoch: [1][2100/3575] Elapsed 8m 52s (remain 6m 13s) Loss: 0.0000(0.0757) Grad: 15.3546  LR: 0.000020  \n","Epoch: [1][2200/3575] Elapsed 9m 18s (remain 5m 48s) Loss: 0.0004(0.0730) Grad: 288.5398  LR: 0.000019  \n","Epoch: [1][2300/3575] Elapsed 9m 43s (remain 5m 23s) Loss: 0.0073(0.0705) Grad: 4151.8457  LR: 0.000019  \n","Epoch: [1][2400/3575] Elapsed 10m 8s (remain 4m 57s) Loss: 0.0055(0.0680) Grad: 1106.2798  LR: 0.000019  \n","Epoch: [1][2500/3575] Elapsed 10m 34s (remain 4m 32s) Loss: 0.0001(0.0658) Grad: 42.8763  LR: 0.000019  \n","Epoch: [1][2600/3575] Elapsed 10m 59s (remain 4m 6s) Loss: 0.0174(0.0640) Grad: 1876.7719  LR: 0.000019  \n","Epoch: [1][2700/3575] Elapsed 11m 24s (remain 3m 41s) Loss: 0.0422(0.0622) Grad: 11807.2275  LR: 0.000019  \n","Epoch: [1][2800/3575] Elapsed 11m 49s (remain 3m 16s) Loss: 0.0207(0.0605) Grad: 1640.2506  LR: 0.000019  \n","Epoch: [1][2900/3575] Elapsed 12m 15s (remain 2m 50s) Loss: 0.0018(0.0591) Grad: 398.3023  LR: 0.000019  \n","Epoch: [1][3000/3575] Elapsed 12m 40s (remain 2m 25s) Loss: 0.0858(0.0576) Grad: 13906.8301  LR: 0.000018  \n","Epoch: [1][3100/3575] Elapsed 13m 5s (remain 2m 0s) Loss: 0.0028(0.0562) Grad: 988.6276  LR: 0.000018  \n","Epoch: [1][3200/3575] Elapsed 13m 30s (remain 1m 34s) Loss: 0.0000(0.0548) Grad: 10.9314  LR: 0.000018  \n","Epoch: [1][3300/3575] Elapsed 13m 56s (remain 1m 9s) Loss: 0.0018(0.0535) Grad: 3971.7759  LR: 0.000018  \n","Epoch: [1][3400/3575] Elapsed 14m 21s (remain 0m 44s) Loss: 0.0092(0.0523) Grad: 3016.0613  LR: 0.000018  \n","Epoch: [1][3500/3575] Elapsed 14m 46s (remain 0m 18s) Loss: 0.0018(0.0512) Grad: 636.5104  LR: 0.000018  \n","Epoch: [1][3574/3575] Elapsed 15m 5s (remain 0m 0s) Loss: 0.0071(0.0504) Grad: 2321.5840  LR: 0.000018  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 27s) Loss: 0.0009(0.0009) \n","EVAL: [100/1192] Elapsed 0m 10s (remain 1m 58s) Loss: 0.0626(0.0103) \n","EVAL: [200/1192] Elapsed 0m 21s (remain 1m 45s) Loss: 0.0323(0.0116) \n","EVAL: [300/1192] Elapsed 0m 32s (remain 1m 34s) Loss: 0.0189(0.0126) \n","EVAL: [400/1192] Elapsed 0m 42s (remain 1m 24s) Loss: 0.0006(0.0124) \n","EVAL: [500/1192] Elapsed 0m 53s (remain 1m 13s) Loss: 0.0704(0.0115) \n","EVAL: [600/1192] Elapsed 1m 3s (remain 1m 2s) Loss: 0.0082(0.0124) \n","EVAL: [700/1192] Elapsed 1m 14s (remain 0m 52s) Loss: 0.0017(0.0138) \n","EVAL: [800/1192] Elapsed 1m 24s (remain 0m 41s) Loss: 0.0257(0.0137) \n","EVAL: [900/1192] Elapsed 1m 35s (remain 0m 30s) Loss: 0.0168(0.0143) \n","EVAL: [1000/1192] Elapsed 1m 46s (remain 0m 20s) Loss: 0.0089(0.0139) \n","EVAL: [1100/1192] Elapsed 1m 56s (remain 0m 9s) Loss: 0.0531(0.0134) \n","EVAL: [1191/1192] Elapsed 2m 6s (remain 0m 0s) Loss: 0.0001(0.0130) \n","Epoch 1 - avg_train_loss: 0.0504  avg_val_loss: 0.0130  time: 1036s\n","Epoch 1 - Score: 0.8575\n","Epoch 1 - Save Best Score: 0.8575 Model\n","Epoch: [2][0/3575] Elapsed 0m 0s (remain 39m 28s) Loss: 0.0129(0.0129) Grad: 43487.6094  LR: 0.000018  \n","Epoch: [2][100/3575] Elapsed 0m 28s (remain 16m 26s) Loss: 0.0141(0.0124) Grad: 35455.9766  LR: 0.000018  \n","Epoch: [2][200/3575] Elapsed 0m 55s (remain 15m 36s) Loss: 0.0005(0.0113) Grad: 1868.6918  LR: 0.000018  \n","Epoch: [2][300/3575] Elapsed 1m 20s (remain 14m 40s) Loss: 0.0196(0.0117) Grad: 34460.4297  LR: 0.000017  \n","Epoch: [2][400/3575] Elapsed 1m 46s (remain 14m 2s) Loss: 0.0076(0.0109) Grad: 33124.0430  LR: 0.000017  \n","Epoch: [2][500/3575] Elapsed 2m 11s (remain 13m 27s) Loss: 0.0035(0.0104) Grad: 14851.4297  LR: 0.000017  \n","Epoch: [2][600/3575] Elapsed 2m 36s (remain 12m 56s) Loss: 0.0098(0.0101) Grad: 19381.3613  LR: 0.000017  \n","Epoch: [2][700/3575] Elapsed 3m 2s (remain 12m 26s) Loss: 0.0100(0.0102) Grad: 17796.8320  LR: 0.000017  \n","Epoch: [2][800/3575] Elapsed 3m 27s (remain 11m 57s) Loss: 0.0541(0.0104) Grad: 174212.0469  LR: 0.000017  \n","Epoch: [2][900/3575] Elapsed 3m 52s (remain 11m 29s) Loss: 0.0000(0.0104) Grad: 38.3901  LR: 0.000017  \n","Epoch: [2][1000/3575] Elapsed 4m 17s (remain 11m 2s) Loss: 0.0013(0.0105) Grad: 13445.2275  LR: 0.000017  \n","Epoch: [2][1100/3575] Elapsed 4m 42s (remain 10m 35s) Loss: 0.0000(0.0104) Grad: 51.1717  LR: 0.000016  \n","Epoch: [2][1200/3575] Elapsed 5m 7s (remain 10m 8s) Loss: 0.0348(0.0102) Grad: 44615.3984  LR: 0.000016  \n","Epoch: [2][1300/3575] Elapsed 5m 33s (remain 9m 42s) Loss: 0.0047(0.0103) Grad: 23566.6445  LR: 0.000016  \n","Epoch: [2][1400/3575] Elapsed 5m 58s (remain 9m 16s) Loss: 0.0000(0.0103) Grad: 250.1966  LR: 0.000016  \n","Epoch: [2][1500/3575] Elapsed 6m 23s (remain 8m 50s) Loss: 0.0000(0.0102) Grad: 154.9525  LR: 0.000016  \n","Epoch: [2][1600/3575] Elapsed 6m 48s (remain 8m 24s) Loss: 0.0442(0.0102) Grad: 44730.1172  LR: 0.000016  \n","Epoch: [2][1700/3575] Elapsed 7m 14s (remain 7m 58s) Loss: 0.0015(0.0102) Grad: 6214.7153  LR: 0.000016  \n","Epoch: [2][1800/3575] Elapsed 7m 39s (remain 7m 32s) Loss: 0.0000(0.0103) Grad: 29.3237  LR: 0.000016  \n","Epoch: [2][1900/3575] Elapsed 8m 4s (remain 7m 6s) Loss: 0.0000(0.0101) Grad: 74.7979  LR: 0.000015  \n","Epoch: [2][2000/3575] Elapsed 8m 29s (remain 6m 41s) Loss: 0.0274(0.0102) Grad: 37151.9219  LR: 0.000015  \n","Epoch: [2][2100/3575] Elapsed 8m 54s (remain 6m 15s) Loss: 0.0000(0.0100) Grad: 118.6501  LR: 0.000015  \n","Epoch: [2][2200/3575] Elapsed 9m 20s (remain 5m 49s) Loss: 0.0007(0.0102) Grad: 10202.0264  LR: 0.000015  \n","Epoch: [2][2300/3575] Elapsed 9m 45s (remain 5m 24s) Loss: 0.0022(0.0102) Grad: 41197.7930  LR: 0.000015  \n","Epoch: [2][2400/3575] Elapsed 10m 10s (remain 4m 58s) Loss: 0.0000(0.0102) Grad: 48.7430  LR: 0.000015  \n","Epoch: [2][2500/3575] Elapsed 10m 35s (remain 4m 33s) Loss: 0.0747(0.0104) Grad: 231704.0312  LR: 0.000015  \n","Epoch: [2][2600/3575] Elapsed 11m 1s (remain 4m 7s) Loss: 0.0001(0.0103) Grad: 641.2715  LR: 0.000015  \n","Epoch: [2][2700/3575] Elapsed 11m 26s (remain 3m 42s) Loss: 0.0065(0.0106) Grad: 34852.5117  LR: 0.000014  \n","Epoch: [2][2800/3575] Elapsed 11m 51s (remain 3m 16s) Loss: 0.0666(0.0107) Grad: 137399.6406  LR: 0.000014  \n","Epoch: [2][2900/3575] Elapsed 12m 17s (remain 2m 51s) Loss: 0.1401(0.0108) Grad: 207058.7188  LR: 0.000014  \n","Epoch: [2][3000/3575] Elapsed 12m 42s (remain 2m 25s) Loss: 0.0158(0.0107) Grad: 34494.5664  LR: 0.000014  \n","Epoch: [2][3100/3575] Elapsed 13m 7s (remain 2m 0s) Loss: 0.0072(0.0108) Grad: 33011.2344  LR: 0.000014  \n","Epoch: [2][3200/3575] Elapsed 13m 33s (remain 1m 35s) Loss: 0.0061(0.0107) Grad: 18896.0391  LR: 0.000014  \n","Epoch: [2][3300/3575] Elapsed 13m 58s (remain 1m 9s) Loss: 0.0374(0.0107) Grad: 354435.6250  LR: 0.000014  \n","Epoch: [2][3400/3575] Elapsed 14m 23s (remain 0m 44s) Loss: 0.0001(0.0107) Grad: 1052.5679  LR: 0.000014  \n","Epoch: [2][3500/3575] Elapsed 14m 48s (remain 0m 18s) Loss: 0.0009(0.0106) Grad: 12173.1582  LR: 0.000013  \n","Epoch: [2][3574/3575] Elapsed 15m 7s (remain 0m 0s) Loss: 0.0002(0.0106) Grad: 3158.4543  LR: 0.000013  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 4s) Loss: 0.0004(0.0004) \n","EVAL: [100/1192] Elapsed 0m 10s (remain 1m 58s) Loss: 0.0690(0.0106) \n","EVAL: [200/1192] Elapsed 0m 21s (remain 1m 45s) Loss: 0.0151(0.0104) \n","EVAL: [300/1192] Elapsed 0m 32s (remain 1m 34s) Loss: 0.0181(0.0110) \n","EVAL: [400/1192] Elapsed 0m 42s (remain 1m 23s) Loss: 0.0000(0.0105) \n","EVAL: [500/1192] Elapsed 0m 53s (remain 1m 13s) Loss: 0.0588(0.0101) \n","EVAL: [600/1192] Elapsed 1m 3s (remain 1m 2s) Loss: 0.0092(0.0107) \n","EVAL: [700/1192] Elapsed 1m 14s (remain 0m 51s) Loss: 0.0055(0.0121) \n","EVAL: [800/1192] Elapsed 1m 24s (remain 0m 41s) Loss: 0.0120(0.0124) \n","EVAL: [900/1192] Elapsed 1m 35s (remain 0m 30s) Loss: 0.0232(0.0127) \n","EVAL: [1000/1192] Elapsed 1m 45s (remain 0m 20s) Loss: 0.0096(0.0124) \n","EVAL: [1100/1192] Elapsed 1m 56s (remain 0m 9s) Loss: 0.0628(0.0121) \n","EVAL: [1191/1192] Elapsed 2m 6s (remain 0m 0s) Loss: 0.0000(0.0120) \n","Epoch 2 - avg_train_loss: 0.0106  avg_val_loss: 0.0120  time: 1039s\n","Epoch 2 - Score: 0.8878\n","Epoch 2 - Save Best Score: 0.8878 Model\n","Epoch: [3][0/3575] Elapsed 0m 0s (remain 35m 18s) Loss: 0.0001(0.0001) Grad: 244.0450  LR: 0.000013  \n","Epoch: [3][100/3575] Elapsed 0m 28s (remain 16m 34s) Loss: 0.0060(0.0080) Grad: 26508.2949  LR: 0.000013  \n","Epoch: [3][200/3575] Elapsed 0m 55s (remain 15m 35s) Loss: 0.0000(0.0065) Grad: 117.2939  LR: 0.000013  \n","Epoch: [3][300/3575] Elapsed 1m 21s (remain 14m 41s) Loss: 0.0393(0.0072) Grad: 77019.8438  LR: 0.000013  \n","Epoch: [3][400/3575] Elapsed 1m 46s (remain 14m 0s) Loss: 0.0388(0.0075) Grad: 77113.2578  LR: 0.000013  \n","Epoch: [3][500/3575] Elapsed 2m 11s (remain 13m 26s) Loss: 0.0080(0.0073) Grad: 11127.1104  LR: 0.000013  \n","Epoch: [3][600/3575] Elapsed 2m 36s (remain 12m 54s) Loss: 0.0000(0.0075) Grad: 14.1286  LR: 0.000013  \n","Epoch: [3][700/3575] Elapsed 3m 1s (remain 12m 25s) Loss: 0.0001(0.0075) Grad: 176.2183  LR: 0.000012  \n","Epoch: [3][800/3575] Elapsed 3m 26s (remain 11m 56s) Loss: 0.0128(0.0075) Grad: 74880.6562  LR: 0.000012  \n","Epoch: [3][900/3575] Elapsed 3m 52s (remain 11m 29s) Loss: 0.0028(0.0079) Grad: 12904.4102  LR: 0.000012  \n","Epoch: [3][1000/3575] Elapsed 4m 17s (remain 11m 1s) Loss: 0.0012(0.0077) Grad: 10642.0029  LR: 0.000012  \n","Epoch: [3][1100/3575] Elapsed 4m 42s (remain 10m 34s) Loss: 0.0412(0.0075) Grad: 55955.1016  LR: 0.000012  \n","Epoch: [3][1200/3575] Elapsed 5m 7s (remain 10m 8s) Loss: 0.0101(0.0074) Grad: 35636.8789  LR: 0.000012  \n","Epoch: [3][1300/3575] Elapsed 5m 32s (remain 9m 41s) Loss: 0.0015(0.0077) Grad: 8111.5928  LR: 0.000012  \n","Epoch: [3][1400/3575] Elapsed 5m 57s (remain 9m 15s) Loss: 0.0000(0.0075) Grad: 6.3722  LR: 0.000012  \n","Epoch: [3][1500/3575] Elapsed 6m 22s (remain 8m 49s) Loss: 0.0021(0.0075) Grad: 17335.6816  LR: 0.000011  \n","Epoch: [3][1600/3575] Elapsed 6m 48s (remain 8m 23s) Loss: 0.0015(0.0077) Grad: 4579.9219  LR: 0.000011  \n","Epoch: [3][1700/3575] Elapsed 7m 13s (remain 7m 57s) Loss: 0.0218(0.0077) Grad: 34153.0938  LR: 0.000011  \n","Epoch: [3][1800/3575] Elapsed 7m 38s (remain 7m 31s) Loss: 0.0433(0.0077) Grad: 52408.5430  LR: 0.000011  \n","Epoch: [3][1900/3575] Elapsed 8m 3s (remain 7m 6s) Loss: 0.0229(0.0078) Grad: 100866.7031  LR: 0.000011  \n","Epoch: [3][2000/3575] Elapsed 8m 29s (remain 6m 40s) Loss: 0.0028(0.0080) Grad: 20821.2363  LR: 0.000011  \n","Epoch: [3][2100/3575] Elapsed 8m 54s (remain 6m 14s) Loss: 0.0000(0.0079) Grad: 20.6679  LR: 0.000011  \n","Epoch: [3][2200/3575] Elapsed 9m 19s (remain 5m 49s) Loss: 0.0020(0.0080) Grad: 19930.1621  LR: 0.000011  \n","Epoch: [3][2300/3575] Elapsed 9m 44s (remain 5m 23s) Loss: 0.0114(0.0080) Grad: 137541.5625  LR: 0.000010  \n","Epoch: [3][2400/3575] Elapsed 10m 10s (remain 4m 58s) Loss: 0.0020(0.0080) Grad: 37089.4141  LR: 0.000010  \n","Epoch: [3][2500/3575] Elapsed 10m 35s (remain 4m 32s) Loss: 0.0002(0.0081) Grad: 2552.3040  LR: 0.000010  \n","Epoch: [3][2600/3575] Elapsed 11m 0s (remain 4m 7s) Loss: 0.0005(0.0081) Grad: 15276.4385  LR: 0.000010  \n","Epoch: [3][2700/3575] Elapsed 11m 25s (remain 3m 41s) Loss: 0.0000(0.0081) Grad: 71.9966  LR: 0.000010  \n","Epoch: [3][2800/3575] Elapsed 11m 50s (remain 3m 16s) Loss: 0.0000(0.0081) Grad: 64.8724  LR: 0.000010  \n","Epoch: [3][2900/3575] Elapsed 12m 15s (remain 2m 50s) Loss: 0.0002(0.0081) Grad: 2988.8684  LR: 0.000010  \n","Epoch: [3][3000/3575] Elapsed 12m 41s (remain 2m 25s) Loss: 0.0000(0.0083) Grad: 66.9449  LR: 0.000010  \n","Epoch: [3][3100/3575] Elapsed 13m 6s (remain 2m 0s) Loss: 0.0007(0.0084) Grad: 5767.3857  LR: 0.000009  \n","Epoch: [3][3200/3575] Elapsed 13m 31s (remain 1m 34s) Loss: 0.0022(0.0084) Grad: 36456.0000  LR: 0.000009  \n","Epoch: [3][3300/3575] Elapsed 13m 56s (remain 1m 9s) Loss: 0.0030(0.0084) Grad: 49250.1523  LR: 0.000009  \n","Epoch: [3][3400/3575] Elapsed 14m 21s (remain 0m 44s) Loss: 0.0000(0.0084) Grad: 9.7065  LR: 0.000009  \n","Epoch: [3][3500/3575] Elapsed 14m 46s (remain 0m 18s) Loss: 0.0048(0.0084) Grad: 82512.3984  LR: 0.000009  \n","Epoch: [3][3574/3575] Elapsed 15m 5s (remain 0m 0s) Loss: 0.0000(0.0083) Grad: 1098.5981  LR: 0.000009  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 7m 46s) Loss: 0.0040(0.0040) \n","EVAL: [100/1192] Elapsed 0m 10s (remain 1m 58s) Loss: 0.1091(0.0150) \n","EVAL: [200/1192] Elapsed 0m 21s (remain 1m 45s) Loss: 0.0215(0.0130) \n","EVAL: [300/1192] Elapsed 0m 32s (remain 1m 34s) Loss: 0.0165(0.0129) \n","EVAL: [400/1192] Elapsed 0m 42s (remain 1m 24s) Loss: 0.0000(0.0130) \n","EVAL: [500/1192] Elapsed 0m 53s (remain 1m 13s) Loss: 0.0826(0.0128) \n","EVAL: [600/1192] Elapsed 1m 3s (remain 1m 2s) Loss: 0.0144(0.0134) \n","EVAL: [700/1192] Elapsed 1m 14s (remain 0m 52s) Loss: 0.0050(0.0151) \n","EVAL: [800/1192] Elapsed 1m 24s (remain 0m 41s) Loss: 0.0164(0.0153) \n","EVAL: [900/1192] Elapsed 1m 35s (remain 0m 30s) Loss: 0.0088(0.0157) \n","EVAL: [1000/1192] Elapsed 1m 46s (remain 0m 20s) Loss: 0.0019(0.0153) \n","EVAL: [1100/1192] Elapsed 1m 56s (remain 0m 9s) Loss: 0.0664(0.0150) \n","EVAL: [1191/1192] Elapsed 2m 6s (remain 0m 0s) Loss: 0.0000(0.0146) \n","Epoch 3 - avg_train_loss: 0.0083  avg_val_loss: 0.0146  time: 1037s\n","Epoch 3 - Score: 0.8894\n","Epoch 3 - Save Best Score: 0.8894 Model\n","Epoch: [4][0/3575] Elapsed 0m 0s (remain 35m 49s) Loss: 0.0000(0.0000) Grad: 180.7965  LR: 0.000009  \n","Epoch: [4][100/3575] Elapsed 0m 29s (remain 16m 53s) Loss: 0.0023(0.0073) Grad: 41658.5273  LR: 0.000009  \n","Epoch: [4][200/3575] Elapsed 0m 55s (remain 15m 38s) Loss: 0.0000(0.0058) Grad: 9.4952  LR: 0.000009  \n","Epoch: [4][300/3575] Elapsed 1m 21s (remain 14m 43s) Loss: 0.0001(0.0059) Grad: 720.1416  LR: 0.000009  \n","Epoch: [4][400/3575] Elapsed 1m 46s (remain 14m 3s) Loss: 0.0000(0.0056) Grad: 31.3664  LR: 0.000008  \n","Epoch: [4][500/3575] Elapsed 2m 11s (remain 13m 28s) Loss: 0.0000(0.0059) Grad: 676.1989  LR: 0.000008  \n","Epoch: [4][600/3575] Elapsed 2m 37s (remain 12m 57s) Loss: 0.0007(0.0064) Grad: 3048.1716  LR: 0.000008  \n","Epoch: [4][700/3575] Elapsed 3m 2s (remain 12m 28s) Loss: 0.0032(0.0063) Grad: 27370.7754  LR: 0.000008  \n","Epoch: [4][800/3575] Elapsed 3m 27s (remain 11m 59s) Loss: 0.0148(0.0063) Grad: 53574.7695  LR: 0.000008  \n","Epoch: [4][900/3575] Elapsed 3m 53s (remain 11m 31s) Loss: 0.0000(0.0066) Grad: 135.6855  LR: 0.000008  \n","Epoch: [4][1000/3575] Elapsed 4m 18s (remain 11m 4s) Loss: 0.0000(0.0064) Grad: 255.2851  LR: 0.000008  \n","Epoch: [4][1100/3575] Elapsed 4m 43s (remain 10m 37s) Loss: 0.0004(0.0063) Grad: 2433.5454  LR: 0.000008  \n","Epoch: [4][1200/3575] Elapsed 5m 8s (remain 10m 10s) Loss: 0.0043(0.0063) Grad: 11712.7471  LR: 0.000007  \n","Epoch: [4][1300/3575] Elapsed 5m 33s (remain 9m 43s) Loss: 0.0220(0.0064) Grad: 241129.3750  LR: 0.000007  \n","Epoch: [4][1400/3575] Elapsed 5m 59s (remain 9m 17s) Loss: 0.0315(0.0062) Grad: 35357.5195  LR: 0.000007  \n","Epoch: [4][1500/3575] Elapsed 6m 24s (remain 8m 51s) Loss: 0.0002(0.0060) Grad: 3520.7686  LR: 0.000007  \n","Epoch: [4][1600/3575] Elapsed 6m 49s (remain 8m 25s) Loss: 0.0000(0.0059) Grad: 2237.6091  LR: 0.000007  \n","Epoch: [4][1700/3575] Elapsed 7m 14s (remain 7m 58s) Loss: 0.0000(0.0059) Grad: 12.1640  LR: 0.000007  \n","Epoch: [4][1800/3575] Elapsed 7m 39s (remain 7m 32s) Loss: 0.0004(0.0061) Grad: 2169.8792  LR: 0.000007  \n","Epoch: [4][1900/3575] Elapsed 8m 4s (remain 7m 7s) Loss: 0.0000(0.0062) Grad: 53.5699  LR: 0.000007  \n","Epoch: [4][2000/3575] Elapsed 8m 30s (remain 6m 41s) Loss: 0.0000(0.0062) Grad: 14.7123  LR: 0.000006  \n","Epoch: [4][2100/3575] Elapsed 8m 56s (remain 6m 16s) Loss: 0.0002(0.0061) Grad: 4608.0376  LR: 0.000006  \n","Epoch: [4][2200/3575] Elapsed 9m 21s (remain 5m 50s) Loss: 0.0000(0.0062) Grad: 32.6268  LR: 0.000006  \n","Epoch: [4][2300/3575] Elapsed 9m 46s (remain 5m 24s) Loss: 0.0000(0.0062) Grad: 62.4019  LR: 0.000006  \n","Epoch: [4][2400/3575] Elapsed 10m 12s (remain 4m 59s) Loss: 0.0013(0.0062) Grad: 32912.2188  LR: 0.000006  \n","Epoch: [4][2500/3575] Elapsed 10m 37s (remain 4m 33s) Loss: 0.0000(0.0061) Grad: 4.2441  LR: 0.000006  \n","Epoch: [4][2600/3575] Elapsed 11m 2s (remain 4m 8s) Loss: 0.0036(0.0062) Grad: 38524.4336  LR: 0.000006  \n","Epoch: [4][2700/3575] Elapsed 11m 27s (remain 3m 42s) Loss: 0.0000(0.0063) Grad: 367.6018  LR: 0.000006  \n","Epoch: [4][2800/3575] Elapsed 11m 52s (remain 3m 16s) Loss: 0.0000(0.0064) Grad: 29.2938  LR: 0.000005  \n","Epoch: [4][2900/3575] Elapsed 12m 18s (remain 2m 51s) Loss: 0.0000(0.0064) Grad: 9.4340  LR: 0.000005  \n","Epoch: [4][3000/3575] Elapsed 12m 43s (remain 2m 26s) Loss: 0.0003(0.0065) Grad: 4351.4893  LR: 0.000005  \n","Epoch: [4][3100/3575] Elapsed 13m 8s (remain 2m 0s) Loss: 0.0284(0.0065) Grad: 10656.3779  LR: 0.000005  \n","Epoch: [4][3200/3575] Elapsed 13m 33s (remain 1m 35s) Loss: 0.0000(0.0065) Grad: 93.9853  LR: 0.000005  \n","Epoch: [4][3300/3575] Elapsed 13m 58s (remain 1m 9s) Loss: 0.0000(0.0065) Grad: 20.8540  LR: 0.000005  \n","Epoch: [4][3400/3575] Elapsed 14m 23s (remain 0m 44s) Loss: 0.0000(0.0066) Grad: 25.5720  LR: 0.000005  \n","Epoch: [4][3500/3575] Elapsed 14m 49s (remain 0m 18s) Loss: 0.0029(0.0065) Grad: 44009.7266  LR: 0.000005  \n","Epoch: [4][3574/3575] Elapsed 15m 7s (remain 0m 0s) Loss: 0.0074(0.0065) Grad: 24013.4766  LR: 0.000004  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 7m 16s) Loss: 0.0010(0.0010) \n","EVAL: [100/1192] Elapsed 0m 10s (remain 1m 57s) Loss: 0.0919(0.0152) \n","EVAL: [200/1192] Elapsed 0m 21s (remain 1m 45s) Loss: 0.0527(0.0146) \n","EVAL: [300/1192] Elapsed 0m 32s (remain 1m 34s) Loss: 0.0242(0.0142) \n","EVAL: [400/1192] Elapsed 0m 42s (remain 1m 24s) Loss: 0.0000(0.0142) \n","EVAL: [500/1192] Elapsed 0m 53s (remain 1m 13s) Loss: 0.0966(0.0140) \n","EVAL: [600/1192] Elapsed 1m 3s (remain 1m 2s) Loss: 0.0126(0.0144) \n","EVAL: [700/1192] Elapsed 1m 14s (remain 0m 52s) Loss: 0.0035(0.0161) \n","EVAL: [800/1192] Elapsed 1m 24s (remain 0m 41s) Loss: 0.0195(0.0164) \n","EVAL: [900/1192] Elapsed 1m 35s (remain 0m 30s) Loss: 0.0157(0.0167) \n","EVAL: [1000/1192] Elapsed 1m 46s (remain 0m 20s) Loss: 0.0003(0.0162) \n","EVAL: [1100/1192] Elapsed 1m 56s (remain 0m 9s) Loss: 0.0479(0.0158) \n","EVAL: [1191/1192] Elapsed 2m 6s (remain 0m 0s) Loss: 0.0000(0.0155) \n","Epoch 4 - avg_train_loss: 0.0065  avg_val_loss: 0.0155  time: 1039s\n","Epoch 4 - Score: 0.8867\n","Epoch: [5][0/3575] Elapsed 0m 0s (remain 32m 27s) Loss: 0.0002(0.0002) Grad: 1357.3595  LR: 0.000004  \n","Epoch: [5][100/3575] Elapsed 0m 25s (remain 14m 47s) Loss: 0.0017(0.0042) Grad: 24637.5449  LR: 0.000004  \n","Epoch: [5][200/3575] Elapsed 0m 51s (remain 14m 16s) Loss: 0.0000(0.0055) Grad: 79.3138  LR: 0.000004  \n","Epoch: [5][300/3575] Elapsed 1m 16s (remain 13m 50s) Loss: 0.0000(0.0051) Grad: 9.8920  LR: 0.000004  \n","Epoch: [5][400/3575] Elapsed 1m 41s (remain 13m 24s) Loss: 0.0000(0.0051) Grad: 12.4638  LR: 0.000004  \n","Epoch: [5][500/3575] Elapsed 2m 6s (remain 12m 57s) Loss: 0.0000(0.0055) Grad: 9.7859  LR: 0.000004  \n","Epoch: [5][600/3575] Elapsed 2m 32s (remain 12m 33s) Loss: 0.0003(0.0050) Grad: 3171.6802  LR: 0.000004  \n","Epoch: [5][700/3575] Elapsed 2m 57s (remain 12m 7s) Loss: 0.0129(0.0050) Grad: 44680.7734  LR: 0.000004  \n","Epoch: [5][800/3575] Elapsed 3m 22s (remain 11m 40s) Loss: 0.0000(0.0048) Grad: 7.4279  LR: 0.000003  \n","Epoch: [5][900/3575] Elapsed 3m 47s (remain 11m 15s) Loss: 0.0000(0.0049) Grad: 5.8221  LR: 0.000003  \n","Epoch: [5][1000/3575] Elapsed 4m 12s (remain 10m 49s) Loss: 0.0003(0.0048) Grad: 5946.2612  LR: 0.000003  \n","Epoch: [5][1100/3575] Elapsed 4m 37s (remain 10m 24s) Loss: 0.0004(0.0048) Grad: 3842.6436  LR: 0.000003  \n","Epoch: [5][1200/3575] Elapsed 5m 3s (remain 9m 58s) Loss: 0.0001(0.0048) Grad: 388.6114  LR: 0.000003  \n","Epoch: [5][1300/3575] Elapsed 5m 28s (remain 9m 33s) Loss: 0.0001(0.0049) Grad: 729.7247  LR: 0.000003  \n","Epoch: [5][1400/3575] Elapsed 5m 53s (remain 9m 8s) Loss: 0.0075(0.0048) Grad: 150513.3594  LR: 0.000003  \n","Epoch: [5][1500/3575] Elapsed 6m 18s (remain 8m 42s) Loss: 0.0000(0.0047) Grad: 164.0770  LR: 0.000003  \n","Epoch: [5][1600/3575] Elapsed 6m 43s (remain 8m 17s) Loss: 0.0000(0.0050) Grad: 575.9943  LR: 0.000002  \n","Epoch: [5][1700/3575] Elapsed 7m 8s (remain 7m 52s) Loss: 0.0000(0.0050) Grad: 229.8999  LR: 0.000002  \n","Epoch: [5][1800/3575] Elapsed 7m 33s (remain 7m 26s) Loss: 0.0294(0.0050) Grad: 109019.9922  LR: 0.000002  \n","Epoch: [5][1900/3575] Elapsed 7m 58s (remain 7m 1s) Loss: 0.0000(0.0049) Grad: 23.3628  LR: 0.000002  \n","Epoch: [5][2000/3575] Elapsed 8m 24s (remain 6m 36s) Loss: 0.0000(0.0048) Grad: 93.0677  LR: 0.000002  \n","Epoch: [5][2100/3575] Elapsed 8m 49s (remain 6m 11s) Loss: 0.0000(0.0050) Grad: 111.8356  LR: 0.000002  \n","Epoch: [5][2200/3575] Elapsed 9m 14s (remain 5m 46s) Loss: 0.0001(0.0050) Grad: 5267.9736  LR: 0.000002  \n","Epoch: [5][2300/3575] Elapsed 9m 39s (remain 5m 20s) Loss: 0.0000(0.0049) Grad: 421.9225  LR: 0.000002  \n","Epoch: [5][2400/3575] Elapsed 10m 4s (remain 4m 55s) Loss: 0.0000(0.0050) Grad: 74.5294  LR: 0.000001  \n","Epoch: [5][2500/3575] Elapsed 10m 29s (remain 4m 30s) Loss: 0.0005(0.0051) Grad: 14411.1758  LR: 0.000001  \n","Epoch: [5][2600/3575] Elapsed 10m 54s (remain 4m 5s) Loss: 0.0000(0.0051) Grad: 50.0287  LR: 0.000001  \n","Epoch: [5][2700/3575] Elapsed 11m 19s (remain 3m 40s) Loss: 0.0000(0.0050) Grad: 20.5640  LR: 0.000001  \n","Epoch: [5][2800/3575] Elapsed 11m 45s (remain 3m 14s) Loss: 0.0000(0.0050) Grad: 113.9239  LR: 0.000001  \n","Epoch: [5][2900/3575] Elapsed 12m 9s (remain 2m 49s) Loss: 0.0000(0.0049) Grad: 71.0581  LR: 0.000001  \n","Epoch: [5][3000/3575] Elapsed 12m 34s (remain 2m 24s) Loss: 0.0000(0.0050) Grad: 2.4622  LR: 0.000001  \n","Epoch: [5][3100/3575] Elapsed 13m 0s (remain 1m 59s) Loss: 0.0001(0.0050) Grad: 658.9331  LR: 0.000001  \n","Epoch: [5][3200/3575] Elapsed 13m 25s (remain 1m 34s) Loss: 0.0000(0.0050) Grad: 7.0228  LR: 0.000000  \n","Epoch: [5][3300/3575] Elapsed 13m 50s (remain 1m 8s) Loss: 0.0001(0.0051) Grad: 4130.0488  LR: 0.000000  \n","Epoch: [5][3400/3575] Elapsed 14m 15s (remain 0m 43s) Loss: 0.0000(0.0051) Grad: 12.9304  LR: 0.000000  \n","Epoch: [5][3500/3575] Elapsed 14m 40s (remain 0m 18s) Loss: 0.0000(0.0051) Grad: 907.3163  LR: 0.000000  \n","Epoch: [5][3574/3575] Elapsed 14m 59s (remain 0m 0s) Loss: 0.0393(0.0050) Grad: 123265.1406  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 7m 16s) Loss: 0.0012(0.0012) \n","EVAL: [100/1192] Elapsed 0m 10s (remain 1m 57s) Loss: 0.1325(0.0200) \n","EVAL: [200/1192] Elapsed 0m 21s (remain 1m 45s) Loss: 0.0457(0.0175) \n","EVAL: [300/1192] Elapsed 0m 31s (remain 1m 34s) Loss: 0.0264(0.0172) \n","EVAL: [400/1192] Elapsed 0m 42s (remain 1m 23s) Loss: 0.0000(0.0168) \n","EVAL: [500/1192] Elapsed 0m 52s (remain 1m 13s) Loss: 0.1141(0.0168) \n","EVAL: [600/1192] Elapsed 1m 3s (remain 1m 2s) Loss: 0.0177(0.0175) \n","EVAL: [700/1192] Elapsed 1m 14s (remain 0m 51s) Loss: 0.0070(0.0199) \n","EVAL: [800/1192] Elapsed 1m 24s (remain 0m 41s) Loss: 0.0246(0.0202) \n","EVAL: [900/1192] Elapsed 1m 35s (remain 0m 30s) Loss: 0.0212(0.0206) \n","EVAL: [1000/1192] Elapsed 1m 45s (remain 0m 20s) Loss: 0.0000(0.0200) \n","EVAL: [1100/1192] Elapsed 1m 56s (remain 0m 9s) Loss: 0.0562(0.0195) \n","EVAL: [1191/1192] Elapsed 2m 5s (remain 0m 0s) Loss: 0.0000(0.0191) \n","Epoch 5 - avg_train_loss: 0.0050  avg_val_loss: 0.0191  time: 1030s\n","Epoch 5 - Score: 0.8883\n","Best thres: 0.5, Score: 0.8848\n","Best thres: 0.50625, Score: 0.8848\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6eaa45a79eea49d7bd751c6966052957"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"761393f8e6964b58b103436b804bf60e"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb1ade356f954e94a432a4f4088d7694"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]}],"source":["if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"name":"nbme-exp065.ipynb","provenance":[{"file_id":"1JLo7jIsYVj_IFPTHt-Sze0IfZvJy_iFb","timestamp":1648219833027},{"file_id":"10yG4L3_nzpdL2CDwqxa9r-KWq6jYkWfl","timestamp":1648219695953}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"papermill":{"default_parameters":{},"duration":641.572862,"end_time":"2022-02-27T11:39:50.972497","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-02-27T11:29:09.399635","version":"2.3.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"93396849796641e38fe598fbb80f39bf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ca48dfaaed0c456c965f6d5c936f39ac","IPY_MODEL_856f62ca528f4a68b3d75762c75cdc11","IPY_MODEL_c22b89da452b4343ba57863751510473"],"layout":"IPY_MODEL_cd324da6811f4ce58dd7d6e83f4e850d"}},"ca48dfaaed0c456c965f6d5c936f39ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5440de7a54de4fa8babb13b22b4a7a0f","placeholder":"​","style":"IPY_MODEL_3e5b9a91da7246d981cdf332c73be293","value":"Downloading: 100%"}},"856f62ca528f4a68b3d75762c75cdc11":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_579f85189b7f49ee875c67e82a671b17","max":52,"min":0,"orientation":"horizontal","style":"IPY_MODEL_683970fb94df4782b325365e560aa3fa","value":52}},"c22b89da452b4343ba57863751510473":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c8c4bf8c12145feadc330210e40ae99","placeholder":"​","style":"IPY_MODEL_76a0515276e54e27ad7c47f13c00fc46","value":" 52.0/52.0 [00:00&lt;00:00, 488B/s]"}},"cd324da6811f4ce58dd7d6e83f4e850d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5440de7a54de4fa8babb13b22b4a7a0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e5b9a91da7246d981cdf332c73be293":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"579f85189b7f49ee875c67e82a671b17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"683970fb94df4782b325365e560aa3fa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3c8c4bf8c12145feadc330210e40ae99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76a0515276e54e27ad7c47f13c00fc46":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"81aefc91729449d3998d6412dfb1bbb2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3c29f0e0c2af4b1e986d5f59189ddbb6","IPY_MODEL_79de7803649d43f4abeeaaf2bf0f4eac","IPY_MODEL_242c6cad4d424d3d9af60f40d4dce8a6"],"layout":"IPY_MODEL_824e9cfab84b44e4b985faea1c67c8b6"}},"3c29f0e0c2af4b1e986d5f59189ddbb6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8dcc71ae64c42529bd722ce19c3f390","placeholder":"​","style":"IPY_MODEL_e90df5ec49ac4d8aab8b3d2fac166e65","value":"Downloading: 100%"}},"79de7803649d43f4abeeaaf2bf0f4eac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_236e6f8a4e7f48ba9e71e1437969d736","max":475,"min":0,"orientation":"horizontal","style":"IPY_MODEL_030adae8feab402c92f0ffb9ee0213d3","value":475}},"242c6cad4d424d3d9af60f40d4dce8a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eff3d2de34164d4abc26f1322ed6a67a","placeholder":"​","style":"IPY_MODEL_4d668caff0b84b51a0c5d339bce3b0b9","value":" 475/475 [00:00&lt;00:00, 5.66kB/s]"}},"824e9cfab84b44e4b985faea1c67c8b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8dcc71ae64c42529bd722ce19c3f390":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e90df5ec49ac4d8aab8b3d2fac166e65":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"236e6f8a4e7f48ba9e71e1437969d736":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"030adae8feab402c92f0ffb9ee0213d3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eff3d2de34164d4abc26f1322ed6a67a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d668caff0b84b51a0c5d339bce3b0b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18f08745ea4545c1ab6b405a502d2216":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e5f90f81f0d94bf0901974ce6bf633c7","IPY_MODEL_2317ee8e32c14e31aaf8413c6a68bca0","IPY_MODEL_6807c2e3067e450883292543b9118239"],"layout":"IPY_MODEL_1d26abd18a0c4e5d96dd01cab2c174b9"}},"e5f90f81f0d94bf0901974ce6bf633c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c6a204bcd664429ab7f085f1febd9b2","placeholder":"​","style":"IPY_MODEL_025a482d60d242cfa249367c104e045d","value":"Downloading: 100%"}},"2317ee8e32c14e31aaf8413c6a68bca0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_030cca9bec1944fc9ef0514fff7d9a8b","max":898825,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f50fb4f6649b46a485eb453a4042d3f6","value":898825}},"6807c2e3067e450883292543b9118239":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e4226d9cd754c76b8cc73332a7263cf","placeholder":"​","style":"IPY_MODEL_8a4d322962cd44c4a98c358c592c229f","value":" 878k/878k [00:00&lt;00:00, 1.82MB/s]"}},"1d26abd18a0c4e5d96dd01cab2c174b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c6a204bcd664429ab7f085f1febd9b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"025a482d60d242cfa249367c104e045d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"030cca9bec1944fc9ef0514fff7d9a8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f50fb4f6649b46a485eb453a4042d3f6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0e4226d9cd754c76b8cc73332a7263cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a4d322962cd44c4a98c358c592c229f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a88dcdd54db5497f920b3fa52dc1bebb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_04812347a714437e9ef329124fefefe5","IPY_MODEL_907e0494746f4b9d9df8cbf8a7650c31","IPY_MODEL_d3d55721adc94e2c9912fad0fe6331b3"],"layout":"IPY_MODEL_2fa1ca10c77d4093a344e0a15d9d6549"}},"04812347a714437e9ef329124fefefe5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_944437ffdd884dda89468e2c5e32a425","placeholder":"​","style":"IPY_MODEL_8a37882246ab4f2fa4d9027f8a825102","value":"Downloading: 100%"}},"907e0494746f4b9d9df8cbf8a7650c31":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3944b26ad2c549feb50af10ecab5abe7","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c2f67e0f521d4ff3b82ad92f7963b8d9","value":456318}},"d3d55721adc94e2c9912fad0fe6331b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8219735e55e242d4ab8b57baa3915b14","placeholder":"​","style":"IPY_MODEL_3ea2ca6055804c6198187e30af45c4a7","value":" 446k/446k [00:00&lt;00:00, 810kB/s]"}},"2fa1ca10c77d4093a344e0a15d9d6549":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"944437ffdd884dda89468e2c5e32a425":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a37882246ab4f2fa4d9027f8a825102":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3944b26ad2c549feb50af10ecab5abe7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2f67e0f521d4ff3b82ad92f7963b8d9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8219735e55e242d4ab8b57baa3915b14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ea2ca6055804c6198187e30af45c4a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"492692e8f13c4f92ace440121366e6d4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9ff86c5e3715480b868af385905a1271","IPY_MODEL_f52efa19f8f84c66b2eaafaae633c680","IPY_MODEL_e3c97b5dd1ac4f2d8b0774d97656846b"],"layout":"IPY_MODEL_5eec03c6eda14d6085279feb1e9dd49c"}},"9ff86c5e3715480b868af385905a1271":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d662b9dc4524e0bb309e5523bb9a161","placeholder":"​","style":"IPY_MODEL_43a14631d837429795298cdd7ac8bf70","value":"100%"}},"f52efa19f8f84c66b2eaafaae633c680":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a42ddd9f43a4719adc48d1ebf20dbb4","max":42146,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a6773a1438974d54a340ae6e8fe1de89","value":42146}},"e3c97b5dd1ac4f2d8b0774d97656846b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ee34c7c90c34299ac27975d7ed02ce8","placeholder":"​","style":"IPY_MODEL_d9a9bb811a9944148592b846ff3d78f5","value":" 42146/42146 [00:28&lt;00:00, 2232.85it/s]"}},"5eec03c6eda14d6085279feb1e9dd49c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d662b9dc4524e0bb309e5523bb9a161":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43a14631d837429795298cdd7ac8bf70":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a42ddd9f43a4719adc48d1ebf20dbb4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6773a1438974d54a340ae6e8fe1de89":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6ee34c7c90c34299ac27975d7ed02ce8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9a9bb811a9944148592b846ff3d78f5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5026118027a4e32bfa47fa07b3cb107":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a23445a8eb094a86934c659f12340b71","IPY_MODEL_633d1efd643c4a7e8549defeb08b908c","IPY_MODEL_8e4336fbae44412da80d666ecd00a425"],"layout":"IPY_MODEL_f68159ff7f3f48e889a5aba9a9587220"}},"a23445a8eb094a86934c659f12340b71":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48e66789c51742018bd7fe97ffd5f9f3","placeholder":"​","style":"IPY_MODEL_3d13408ab7a84711a60458529f1dd32d","value":"100%"}},"633d1efd643c4a7e8549defeb08b908c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_58df78091e134c009151c2b9ff857e02","max":143,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d8c996b15d9e46a3b2eceaf7d0f49c1d","value":143}},"8e4336fbae44412da80d666ecd00a425":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5036963bdbd043808e6922143454f306","placeholder":"​","style":"IPY_MODEL_45da851b555b451195033d8fd5d7dc48","value":" 143/143 [00:00&lt;00:00, 2662.42it/s]"}},"f68159ff7f3f48e889a5aba9a9587220":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48e66789c51742018bd7fe97ffd5f9f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d13408ab7a84711a60458529f1dd32d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"58df78091e134c009151c2b9ff857e02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8c996b15d9e46a3b2eceaf7d0f49c1d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5036963bdbd043808e6922143454f306":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45da851b555b451195033d8fd5d7dc48":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6eaa45a79eea49d7bd751c6966052957":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0fbb7f0aa7e84edda0ea106c31aaeaa2","IPY_MODEL_61a9f8d985184f69833f2d0f4825696a","IPY_MODEL_af3da1fafb6a491c84edc0d64ba9963c"],"layout":"IPY_MODEL_0689cd965f424a48b7de97c828ca7c62"}},"0fbb7f0aa7e84edda0ea106c31aaeaa2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_70ea62aa13a94fbdb25e26258709c7fd","placeholder":"​","style":"IPY_MODEL_d91866807e3d444d939854bee32646ad","value":"Downloading: 100%"}},"61a9f8d985184f69833f2d0f4825696a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b6134e91eb44f5f8246d6d55de5eb4c","max":1627284589,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bd1cae6c5bef4a14bf34d0a3c5f1c3b9","value":1627284589}},"af3da1fafb6a491c84edc0d64ba9963c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_946e6388966b4d3c8b85f0cef3217903","placeholder":"​","style":"IPY_MODEL_7b9e89790a2741abac2e472b199648c3","value":" 1.52G/1.52G [00:32&lt;00:00, 53.3MB/s]"}},"0689cd965f424a48b7de97c828ca7c62":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70ea62aa13a94fbdb25e26258709c7fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d91866807e3d444d939854bee32646ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6b6134e91eb44f5f8246d6d55de5eb4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd1cae6c5bef4a14bf34d0a3c5f1c3b9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"946e6388966b4d3c8b85f0cef3217903":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b9e89790a2741abac2e472b199648c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"761393f8e6964b58b103436b804bf60e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c5d01a08aeba4fc09ba820609a806a19","IPY_MODEL_6d34c4b357254a048ce183d85df3f115","IPY_MODEL_7cb47d100bf24c24a2ace444fdb975f4"],"layout":"IPY_MODEL_5bff9704782246629da79145ce490ce1"}},"c5d01a08aeba4fc09ba820609a806a19":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4bf220f024d848f6abfe731bd8269593","placeholder":"​","style":"IPY_MODEL_5c549562112648d085066bbc3ab0a05c","value":"100%"}},"6d34c4b357254a048ce183d85df3f115":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd1b847c670841ed986455b17b7f2ab5","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a584170b609c4eefa75f3f0d65275acf","value":2}},"7cb47d100bf24c24a2ace444fdb975f4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac5b24b572e54624a52fab3d7b757aca","placeholder":"​","style":"IPY_MODEL_7908b5b887b64959b444a120ce88d8d8","value":" 2/2 [00:01&lt;00:00,  1.24s/it]"}},"5bff9704782246629da79145ce490ce1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bf220f024d848f6abfe731bd8269593":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c549562112648d085066bbc3ab0a05c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd1b847c670841ed986455b17b7f2ab5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a584170b609c4eefa75f3f0d65275acf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ac5b24b572e54624a52fab3d7b757aca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7908b5b887b64959b444a120ce88d8d8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb1ade356f954e94a432a4f4088d7694":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_026df47e46f44b0e9c730254d80a76d8","IPY_MODEL_deaf9e54ba72486d8780936f3ca2b337","IPY_MODEL_6ecaf3103af44b689cbefc755d7dbb5c"],"layout":"IPY_MODEL_77593f6828e7414681926fd2d769cbaa"}},"026df47e46f44b0e9c730254d80a76d8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d9dc0d2d8d94ede8a08a5d6b301667a","placeholder":"​","style":"IPY_MODEL_21b6896ce6cd4b538c952775fc03b949","value":"100%"}},"deaf9e54ba72486d8780936f3ca2b337":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ac80c48e99e48258dcc83b23ef32f59","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ced03b069ba24671a9ef114a9eeb4e78","value":2}},"6ecaf3103af44b689cbefc755d7dbb5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_534d2e53904d44629743000efd223692","placeholder":"​","style":"IPY_MODEL_c9bfd7aac5d04be788e36da18ed6b6aa","value":" 2/2 [00:02&lt;00:00,  1.54s/it]"}},"77593f6828e7414681926fd2d769cbaa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d9dc0d2d8d94ede8a08a5d6b301667a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21b6896ce6cd4b538c952775fc03b949":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9ac80c48e99e48258dcc83b23ef32f59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ced03b069ba24671a9ef114a9eeb4e78":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"534d2e53904d44629743000efd223692":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9bfd7aac5d04be788e36da18ed6b6aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}