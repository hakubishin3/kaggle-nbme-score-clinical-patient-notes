{"cells":[{"cell_type":"markdown","id":"colored-security","metadata":{"id":"colored-security"},"source":["## References"]},{"cell_type":"markdown","id":"educational-operator","metadata":{"id":"educational-operator"},"source":["- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"]},{"cell_type":"markdown","id":"incorrect-greek","metadata":{"id":"incorrect-greek"},"source":["## Configurations"]},{"cell_type":"code","execution_count":1,"id":"alive-granny","metadata":{"id":"alive-granny","executionInfo":{"status":"ok","timestamp":1650528620255,"user_tz":-540,"elapsed":380,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["EXP_NAME = \"nbme-exp090\"\n","ENV = \"colab\"\n","DEBUG_MODE = False\n","SUBMISSION_MODE = False"]},{"cell_type":"code","execution_count":2,"id":"heavy-prophet","metadata":{"id":"heavy-prophet","executionInfo":{"status":"ok","timestamp":1650528620969,"user_tz":-540,"elapsed":276,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["class CFG:\n","    env=ENV\n","    exp_name=EXP_NAME\n","    debug=DEBUG_MODE\n","    submission=SUBMISSION_MODE\n","    apex=True\n","    input_dir=None\n","    output_dir=None\n","    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n","    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n","    competition_name=\"nbme-score-clinical-patient-notes\"\n","    id_col=\"id\"\n","    target_col=\"location\"\n","    pretrained_model_name=\"microsoft/deberta-v3-large\"\n","    tokenizer=None\n","    max_len=None\n","    #pseudo_plain_path='../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl'\n","    pseudo_plain_path=\"./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\"\n","    n_pseudo_labels=100000\n","    output_dim=1\n","    dropout=0.2\n","    num_workers=4\n","    batch_size=3\n","    lr=2e-5\n","    betas=(0.9, 0.98)\n","    weight_decay=0.1\n","    alpha=1\n","    gamma=2\n","    smoothing=0.0001\n","    num_warmup_steps_rate=0.1\n","    batch_scheduler=True\n","    epochs=1\n","    n_fold=4\n","    train_fold=[0, 1, 2, 3]\n","    seed=71\n","    gradient_accumulation_steps=2\n","    max_grad_norm=1000\n","    print_freq=100\n","    train=True\n","    inference=True"]},{"cell_type":"code","execution_count":3,"id":"vocational-coating","metadata":{"id":"vocational-coating","executionInfo":{"status":"ok","timestamp":1650528620969,"user_tz":-540,"elapsed":4,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.train_fold = [0, 1]\n","\n","if CFG.submission:\n","    CFG.train = False\n","    CFG.inference = True"]},{"cell_type":"markdown","id":"private-moderator","metadata":{"id":"private-moderator"},"source":["## Directory Settings"]},{"cell_type":"code","execution_count":4,"id":"married-tokyo","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"married-tokyo","outputId":"80b49fba-46da-44c9-b80a-c8cc344b91f0","executionInfo":{"status":"ok","timestamp":1650528628274,"user_tz":-540,"elapsed":7309,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["colab\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: transformers==4.16.2 in /usr/local/lib/python3.7/dist-packages (4.16.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.64.0)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (0.12.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2.23.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (0.5.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (3.6.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (6.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (0.0.49)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2019.12.20)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.11.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.16.2) (3.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.16.2) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (1.15.0)\n"]}],"source":["import sys\n","from pathlib import Path\n","\n","\n","print(CFG.env)\n","if CFG.env == \"colab\":\n","    # colab環境\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","    # install packages\n","    !pip install transformers==4.16.2\n","    !pip install -q sentencepiece==0.1.96\n","\n","elif CFG.env == \"local\":\n","    # ローカルサーバ\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","\n","elif CFG.env == \"kaggle\":\n","    # kaggle環境\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./\")"]},{"cell_type":"code","source":["# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n","# This must be done before importing transformers\n","import shutil\n","from pathlib import Path\n","\n","if CFG.env == \"colab\":\n","    input_dir = Path(\"./drive/MyDrive/00.kaggle/input/deberta-v2-3-fast-tokenizer\")\n","    transformers_path = Path(\"/usr/local/lib/python3.7/dist-packages/transformers\")\n","else:\n","    input_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n","    transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n","\n","convert_file = input_dir / \"convert_slow_tokenizer.py\"\n","conversion_path = transformers_path/convert_file.name\n","\n","if conversion_path.exists():\n","    conversion_path.unlink()\n","\n","shutil.copy(convert_file, transformers_path)\n","deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n","\n","for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n","    filepath = deberta_v2_path/filename\n","    if filepath.exists():\n","        filepath.unlink()\n","\n","    shutil.copy(input_dir/filename, filepath)\n","    \n","    \n","from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast"],"metadata":{"id":"cnGM_g9c3WJW","executionInfo":{"status":"ok","timestamp":1650528631850,"user_tz":-540,"elapsed":3590,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"id":"cnGM_g9c3WJW","execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"id":"blank-pierre","metadata":{"id":"blank-pierre","executionInfo":{"status":"ok","timestamp":1650528632457,"user_tz":-540,"elapsed":610,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["import gc\n","import os\n","import ast\n","import time\n","import math\n","import random\n","import itertools\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","from scipy.optimize import minimize\n","from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision.transforms as T\n","from torchvision.io import read_image\n","from torch.utils.data import DataLoader, Dataset\n","\n","from transformers import AutoModelForMaskedLM\n","from transformers import BartModel,BertModel,BertTokenizer\n","from transformers import DebertaModel,DebertaTokenizer\n","from transformers import RobertaModel,RobertaTokenizer\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","id":"sound-still","metadata":{"id":"sound-still"},"source":["## Utilities"]},{"cell_type":"code","execution_count":7,"id":"surprised-commercial","metadata":{"id":"surprised-commercial","executionInfo":{"status":"ok","timestamp":1650528632457,"user_tz":-540,"elapsed":4,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on binary arrays.\n","\n","    Args:\n","        preds (list of lists of ints): Predictions.\n","        truths (list of lists of ints): Ground truths.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    # Micro : aggregating over all instances\n","    preds = np.concatenate(preds)\n","    truths = np.concatenate(truths)\n","    return f1_score(truths, preds)\n","\n","\n","def spans_to_binary(spans, length=None):\n","    \"\"\"\n","    Converts spans to a binary array indicating whether each character is in the span.\n","\n","    Args:\n","        spans (list of lists of two ints): Spans.\n","\n","    Returns:\n","        np array [length]: Binarized spans.\n","    \"\"\"\n","    length = np.max(spans) if length is None else length\n","    binary = np.zeros(length)\n","    for start, end in spans:\n","        binary[start:end] = 1\n","    return binary\n","\n","\n","def span_micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on spans.\n","\n","    Args:\n","        preds (list of lists of two ints): Prediction spans.\n","        truths (list of lists of two ints): Ground truth spans.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    bin_preds = []\n","    bin_truths = []\n","    for pred, truth in zip(preds, truths):\n","        if not len(pred) and not len(truth):\n","            continue\n","        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n","        bin_preds.append(spans_to_binary(pred, length))\n","        bin_truths.append(spans_to_binary(truth, length))\n","    return micro_f1(bin_preds, bin_truths)\n","\n","\n","def get_score(y_true, y_pred):\n","    score = span_micro_f1(y_true, y_pred)\n","    return score"]},{"cell_type":"code","execution_count":8,"id":"interstate-accident","metadata":{"id":"interstate-accident","executionInfo":{"status":"ok","timestamp":1650528632458,"user_tz":-540,"elapsed":5,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def create_labels_for_scoring(df):\n","    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n","    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n","    for i in range(len(df)):\n","        lst = df.loc[i, \"location\"]\n","        if lst:\n","            new_lst = \";\".join(lst)\n","            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n","\n","    # create labels\n","    truths = []\n","    for location_list in df[\"location_for_create_labels\"].values:\n","        truth = []\n","        if len(location_list) > 0:\n","            location = location_list[0]\n","            for loc in [s.split() for s in location.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                truth.append([start, end])\n","        truths.append(truth)\n","\n","    return truths\n","\n","\n","def get_char_probs(texts, token_probs, tokenizer):\n","    res = [np.zeros(len(t)) for t in texts]\n","    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n","        encoded = tokenizer(\n","            text=text,\n","            max_length=CFG.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n","            start, end = offset_mapping\n","            res[i][start:end] = pred\n","    return res\n","\n","\n","def get_predicted_location_str(char_probs, th=0.5):\n","    results = []\n","    for char_prob in char_probs:\n","        result = np.where(char_prob >= th)[0] + 1\n","        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n","        result = [f\"{min(r)} {max(r)}\" for r in result]\n","        result = \";\".join(result)\n","        results.append(result)\n","    return results\n","\n","\n","def get_predictions(results):\n","    predictions = []\n","    for result in results:\n","        prediction = []\n","        if result != \"\":\n","            for loc in [s.split() for s in result.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                prediction.append([start, end])\n","        predictions.append(prediction)\n","    return predictions\n","\n","\n","def scoring(df, th=0.5):\n","    labels = create_labels_for_scoring(df)\n","\n","    token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n","    char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n","    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n","    preds = get_predictions(predicted_location_str)\n","\n","    score = get_score(labels, preds)\n","    return score\n","\n","\n","def get_best_thres(oof_df):\n","    def f1_opt(x):\n","        return -1 * scoring(oof_df, th=x)\n","\n","    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n","    return best_thres"]},{"cell_type":"code","execution_count":9,"id":"coated-pioneer","metadata":{"id":"coated-pioneer","executionInfo":{"status":"ok","timestamp":1650528632458,"user_tz":-540,"elapsed":5,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return \"%dm %ds\" % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":10,"id":"nervous-delaware","metadata":{"id":"nervous-delaware","executionInfo":{"status":"ok","timestamp":1650528632459,"user_tz":-540,"elapsed":5,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["seed_everything()"]},{"cell_type":"code","source":["def postprocess(texts, preds):\n","    fix_tokenize_dict = {\n","        'heart': ['h', 'eart'],\n","        'hair': ['h', 'air'],\n","        'adderal': ['a', 'dderal'],\n","        'mother': ['m', 'other'],\n","        'intermittent': ['i', 'ntermittent'],\n","        'temperature': ['t', 'emperature'],\n","        'episodes': ['e', 'pisodes'],\n","        'no': ['n', 'o'],\n","        'has': ['h', 'as'],\n","        'LMP': ['L', 'MP'],\n","        '10': ['1', '0'],\n","        'blood': ['b', 'lood'],\n","        'recurrent': ['r', 'ecurrent'],\n","        'denies': ['d', 'enies'],\n","        'sudden': ['s', 'udden'],\n","        'Sexually': ['S', 'exually'],\n","        'up': ['u', 'p'],\n","        'wakes': ['w', 'akes'],\n","        'sweats': ['s', 'weats'],\n","        'hot': ['h', 'ot'],\n","        'drenched': ['d', 'renched'],\n","        'gnawing': ['g', 'nawing'],\n","        'Uses': ['U', 'ses'],\n","        'Begin': ['B', 'egin'],\n","        'Nausea': ['N', 'ausea'],\n","        'Burning': ['B', 'urning'],\n","        'Started': ['S', 'tarted'],\n","        'neurvousness': ['n', 'eurvousness'],\n","        'constipation': ['c', 'onstipation'],\n","        'nervousness': ['n', 'ervousness'],\n","        'cold': ['c', 'old'],\n","        'loss': ['l', 'oss'],\n","        'CBC': ['C', 'BC'],\n","        'Hx': ['H', 'x'],\n","        'tingling': ['t', 'ingling'],\n","        'feels': ['f', 'eels'],\n","        'Lost': ['L', 'ost'],\n","        'she': ['s', 'he'],\n","        'racing': ['r', 'acing'],\n","        'throat': ['t', 'hroat'],\n","        'PATIENT': ['P', 'ATIENT'],\n","        'recreational': ['r', 'ecreational'],\n","        'clammy': ['c', 'lammy'],\n","        'numbness': ['n', 'umbness'],\n","        'like': ['l', 'ike'],\n","        'reports': ['r', 'eports'],\n","        'exercise': ['e', 'xercise'],\n","        'started': ['s', 'tarted'],\n","        'brough': ['b', 'rough'],\n","        'Associated': ['A', 'ssociated'],\n","        'exacerbated': ['e', 'xacerbated'],\n","        'sharp': ['s', 'harp'],\n","        'cannot': ['c', 'annot'],\n","        'heavy': ['h', 'eavy'],\n","        'fatigue': ['f', 'atigue'],\n","        'trouble': ['t', 'rouble'],\n","        'hearing': ['h', 'earing'],\n","        'reduced': ['r', 'educed'],\n","        'lack': ['l', 'ack'],\n","        'vomiting': ['v', 'omiting'],\n","        'generalized': ['g', 'eneralized'],\n","        'body': ['b', 'ody'],\n","        'all': ['a', 'll'],\n","        'scratchy': ['s', 'cratchy'],\n","        'mom': ['m', 'om'],\n","        'discomfort': ['d', 'iscomfort'],\n","        'CAD': ['C', 'AD'],\n","        'Thyroid': ['T', 'hyroid'],\n","        'BLADDER': ['B', 'LADDER'],\n","        'diarrhea': ['d', 'iarrhea'],\n","        'Started': ['S', 'tarted'],\n","        'Vaginal': ['V', 'aginal'],\n","        'sleeping': ['s', 'leeping'],\n","        'UNCLE': ['U', 'NCLE'],\n","        'USING': ['U', 'SING'],\n","        'BURNING': ['B', 'URNING'],\n","        'GETTING': ['G', 'ETTING'],\n","        'ETOH': ['E', 'TOH'],\n","        'ON': ['O', 'N'],\n","        'INITIALLY': ['I', 'NITIALLY'],\n","        'epigastric': ['e', 'pigastric'],\n","        'occurs': ['o', 'ccurs'],\n","        'began': ['b', 'egan'],\n","        'alleviated': ['a', 'lleviated'],\n","        'overwhelmed': ['o', 'verwhelmed'],\n","        'clamminess': ['c', 'lamminess'],\n","        'strongly': ['s', 'trongly'],\n","        'lump': ['l', 'ump'],\n","        'drugs': ['d', 'rugs'],\n","        'chest': ['c', 'hest'],\n","        'stuffy': ['s', 'tuffy'],\n","        'changes': ['c', 'hanges'],\n","        'trouble': ['t', 'rouble'],\n","        'takes': ['t', 'akes'],\n","        'tossing': ['t', 'ossing'],\n","        'Fam': ['F', 'am'],\n","        'sweating': ['s', 'weating'],\n","        'dyspareunia': ['d', 'yspareunia'],\n","        'irregular': ['i', 'rregular'],\n","        'time': ['t', 'ime'],\n","        'unpredictable': ['u', 'npredictable'],\n","        'darkened': ['d', 'arkened'],\n","        'anxiety': ['a', 'nxiety'],\n","        'nervous': ['n', 'ervous'],\n","        'TAKING': ['T', 'AKING'],\n","        'losing': ['l', 'osing'],\n","        'Difficulyt': ['D', 'ifficulyt'],\n","        'Appetite': ['A', 'ppetite'],\n","        'increased': ['i', 'ncreased'],\n","        'fingers': ['f', 'ingers'],\n","        'illicit': ['i', 'llicit'],\n","        'claminess': ['c', 'laminess'],\n","        'clamy': ['c', 'lamy'],\n","        'Recently': ['R', 'ecently'],\n","        'feeling': ['f', 'eeling'],\n","        'aggrav': ['a', 'ggrav'],\n","        'changing': ['c', 'hanging'],\n","        'unable': ['u', 'nable'],\n","        'SEEING': ['S', 'EEING'],\n","        'staying': ['s', 'taying'],\n","        'lightheadedness': ['l', 'ightheadedness'],\n","        'lighheadeness': ['l', 'ighheadeness'],\n","        'nail': ['n', 'ail'],\n","        'pounding': ['p', 'ounding'],\n","        'My': ['M', 'y'],\n","        'Father': ['F', 'ather'],\n","        'urinary': ['u', 'rinary'],\n","        'pain': ['p', 'ain'],\n","        'not': ['n', 'ot'],\n","        'lower': ['l', 'ower'],\n","        'menses': ['m', 'enses'],\n","        'at': ['a', 't'],\n","        'takes': ['t', 'akes'],\n","        'initally': ['i', 'nitally'],\n","        'melena': ['m', 'elena'],\n","        'BOWEL': ['B', 'OWEL'],\n","        'WEIGHT': ['W', 'EIGHT'],\n","        'difficulty': ['d', 'ifficulty'],\n","        'condo': ['c', 'ondo'],\n","        'experiences': ['e', 'xperiences'],\n","        'stuffy': ['s', 'tuffy'],\n","        'rhinorrhea': ['r', 'hinorrhea'],\n","        'felt': ['f', 'elt'],\n","        'feverish': ['f', 'everish'],\n","        'CYCLE': ['C', 'YCLE'],\n","        'tampon': ['t', 'ampon'],\n","        'Last': ['L', 'ast'],\n","        'Son': ['S', 'on'],\n","        'saw': ['s', 'aw'],\n","        'tightness': ['t', 'ightness'],\n","        'rash': ['r', 'ash'],\n","        'ibuprofen': ['i', 'buprofen'],\n","        'SCRATHY': ['S', 'CRATHY'],\n","        'PHOTOPHOBIA': ['P', 'HOTOPHOBIA'],\n","    }\n","    preds_pp = preds.copy()\n","    tk0 = tqdm(range(len(preds_pp)), total=len(preds_pp))\n","    for raw_idx in tk0:\n","        pred = preds[raw_idx]\n","        text = texts[raw_idx]\n","        if len(pred) != 0:\n","            # pp1: indexが1から始まる予測値は0から始まるように修正 ## 0.88579 -> 0.88702\n","            if pred[0][0] == 1:\n","                preds_pp[raw_idx][0][0] = 0\n","            for p_index, pp in enumerate(pred):\n","                start, end = pred[p_index]\n","                # pp2: startとendが同じ予測値はstartを前に１ずらす ## 0.88702 -> 0.88714\n","                if start == end:\n","                    preds_pp[raw_idx][p_index][0] = start - 1\n","                    start = start - 1\n","                # pp3: 始点が改行の場合始点を1つ後ろにずらす ## 0.88714 -> 0.88746\n","                if text[start] == '\\n':\n","                    preds_pp[raw_idx][p_index][0] = start + 1\n","                    start = start + 1\n","                # pp4: 1-2などは-2で予測されることがあるので修正 ## 0.88746 -> 0.88747\n","                if text[start-1].isdigit() and text[start] == '-' and text[start+1].isdigit():\n","                    preds_pp[raw_idx][p_index][0] = start - 1\n","                    start = start - 1\n","                if text[start-1].isdigit() and text[start] == '/' and text[start+1].isdigit():\n","                    preds_pp[raw_idx][p_index][0] = start - 1\n","                    start = start - 1\n","                # pp5: 67などは7で予測されることがあるので修正 ## 0.88747 -> 0.88748\n","                if text[start-1].isdigit() and text[start].isdigit():\n","                    preds_pp[raw_idx][p_index][0] = start - 1\n","                    start = start - 1\n","                # pp6: 文頭が大文字で始まるものは大文字部分が除かれて予測されることがあるので修正 ## 0.88748 -> 0.88761\n","                if text[start-2] == '.' and text[start-1].isupper():\n","                    preds_pp[raw_idx][p_index][0] = start - 1\n","                    start = start - 1\n","                if text[start-2] == ',' and text[start-1].isupper():\n","                    preds_pp[raw_idx][p_index][0] = start - 1\n","                    start = start - 1\n","                if text[start-2] == ':' and text[start-1].isupper():\n","                    preds_pp[raw_idx][p_index][0] = start - 1\n","                    start = start - 1\n","                if text[start-2] == '-' and text[start-1].isupper():\n","                    preds_pp[raw_idx][p_index][0] = start - 1\n","                    start = start - 1\n","                # pp7: heart -> h + eart となっているようなものを修正する ## 0.88761 -> 0.88806\n","                for key, fix_tokenize in fix_tokenize_dict.items():\n","                    _s, s = fix_tokenize[0], fix_tokenize[1]\n","                    if text[start-1].lower() == _s.lower() and text[start:start+len(s)].lower() == s.lower():\n","                        preds_pp[raw_idx][p_index][0] = start - 1\n","                        start = start - 1\n","    return preds_pp"],"metadata":{"id":"PMFcPy4zPVGs","executionInfo":{"status":"ok","timestamp":1650528632703,"user_tz":-540,"elapsed":249,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"id":"PMFcPy4zPVGs","execution_count":11,"outputs":[]},{"cell_type":"code","source":["def get_results_from_preds_list(preds):\n","    results = []\n","    for pred in preds:\n","        s = []\n","        for p in pred:\n","            s.append(' '.join(list(map(str, p))))\n","        s = ';'.join(s)\n","        results.append(s)\n","    return results"],"metadata":{"id":"K_ml_nAFPVJt","executionInfo":{"status":"ok","timestamp":1650528632703,"user_tz":-540,"elapsed":3,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"id":"K_ml_nAFPVJt","execution_count":12,"outputs":[]},{"cell_type":"code","source":["def trunc_pred(texts, preds):\n","    preds_pp = preds.copy()\n","    tk0 = tqdm(range(len(preds_pp)), total=len(preds_pp))\n","    for raw_idx in tk0:\n","        text = texts[raw_idx]\n","        num_text = len(text)\n","        preds_pp[raw_idx, num_text:] = 0\n","    return preds_pp"],"metadata":{"id":"DeUSVrAlPRum","executionInfo":{"status":"ok","timestamp":1650528632703,"user_tz":-540,"elapsed":3,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"id":"DeUSVrAlPRum","execution_count":13,"outputs":[]},{"cell_type":"code","source":["def create_label(pn_history, location_list, max_char_len):\n","    label = np.zeros(max_char_len)\n","    label[len(pn_history):] = -1\n","    if len(location_list) > 0:\n","        for location in location_list:\n","            start, end = int(location[0]), int(location[1])\n","            label[start:end] = 1\n","    return label\n","\n","def get_preds_from_results(results, texts, max_char_len):\n","    labels = []\n","    for idx, result in enumerate(results):\n","        label = create_label(texts[idx], result, max_char_len)\n","        labels.append(label)\n","    labels = np.stack(labels)\n","    print(labels.shape)\n","    return labels"],"metadata":{"id":"2Mp2v2Z2PTJh","executionInfo":{"status":"ok","timestamp":1650528632704,"user_tz":-540,"elapsed":3,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"id":"2Mp2v2Z2PTJh","execution_count":14,"outputs":[]},{"cell_type":"markdown","id":"functioning-destruction","metadata":{"id":"functioning-destruction"},"source":["## Data Loading"]},{"cell_type":"code","execution_count":15,"id":"global-monte","metadata":{"id":"global-monte","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650528633355,"user_tz":-540,"elapsed":654,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"0d2b75c5-94f3-474d-d777-77523f226c61"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14300, 6), (143, 3), (42146, 3), (5, 4))"]},"metadata":{},"execution_count":15}],"source":["train = pd.read_csv(CFG.input_dir / \"train.csv\")\n","features = pd.read_csv(CFG.input_dir / \"features.csv\")\n","patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n","test = pd.read_csv(CFG.input_dir / \"test.csv\")\n","\n","train.shape, features.shape, patient_notes.shape, test.shape"]},{"cell_type":"code","execution_count":16,"id":"independent-airfare","metadata":{"id":"independent-airfare","executionInfo":{"status":"ok","timestamp":1650528633355,"user_tz":-540,"elapsed":5,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["if CFG.debug:\n","    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n","    print(train.shape)"]},{"cell_type":"markdown","id":"silent-locator","metadata":{"id":"silent-locator"},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":17,"id":"unusual-fifty","metadata":{"id":"unusual-fifty","executionInfo":{"status":"ok","timestamp":1650528633355,"user_tz":-540,"elapsed":4,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def preprocess_features(features):\n","    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n","    return features\n","\n","\n","features = preprocess_features(features)"]},{"cell_type":"code","source":["features['feature_text'] = features['feature_text'].str.lower()\n","patient_notes['pn_history'] = patient_notes['pn_history'].str.lower()"],"metadata":{"id":"gGzAPm7Img02","executionInfo":{"status":"ok","timestamp":1650528633356,"user_tz":-540,"elapsed":5,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"id":"gGzAPm7Img02","execution_count":18,"outputs":[]},{"cell_type":"code","execution_count":19,"id":"decreased-mustang","metadata":{"id":"decreased-mustang","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650528633356,"user_tz":-540,"elapsed":4,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"07b227fa-acb5-4454-e89a-81c8c74f49d1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14300, 8), (5, 6))"]},"metadata":{},"execution_count":19}],"source":["train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","\n","train.shape, test.shape"]},{"cell_type":"code","execution_count":20,"id":"boolean-trade","metadata":{"id":"boolean-trade","executionInfo":{"status":"ok","timestamp":1650528633669,"user_tz":-540,"elapsed":317,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n","train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"]},{"cell_type":"code","execution_count":21,"id":"accomplished-dakota","metadata":{"id":"accomplished-dakota","colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"status":"ok","timestamp":1650528633670,"user_tz":-540,"elapsed":8,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"4deeae9c-e8a3-446f-d72d-93169d820464"},"outputs":[{"output_type":"display_data","data":{"text/plain":["0    4399\n","1    8181\n","2    1296\n","3     287\n","4      99\n","5      27\n","6       9\n","7       1\n","8       1\n","Name: annotation_length, dtype: int64"]},"metadata":{}}],"source":["train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n","display(train['annotation_length'].value_counts().sort_index())"]},{"cell_type":"markdown","id":"funded-elizabeth","metadata":{"id":"funded-elizabeth"},"source":["## CV split"]},{"cell_type":"code","execution_count":22,"id":"unexpected-columbia","metadata":{"id":"unexpected-columbia","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1650528633670,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"f7533840-b57f-4aff-cbbd-a7f8adcb4503"},"outputs":[{"output_type":"display_data","data":{"text/plain":["fold\n","0    3575\n","1    3575\n","2    3575\n","3    3575\n","dtype: int64"]},"metadata":{}}],"source":["Fold = GroupKFold(n_splits=CFG.n_fold)\n","groups = train['pn_num'].values\n","for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n","    train.loc[val_index, 'fold'] = int(n)\n","train['fold'] = train['fold'].astype(int)\n","display(train.groupby('fold').size())"]},{"cell_type":"markdown","id":"critical-archive","metadata":{"id":"critical-archive"},"source":["## Setup tokenizer"]},{"cell_type":"code","execution_count":23,"id":"broken-generator","metadata":{"id":"broken-generator","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650528635568,"user_tz":-540,"elapsed":1903,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"366e8fc1-0dee-4903-e74c-e9fa0475d6ee"},"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["if CFG.submission:\n","    tokenizer = DebertaV2TokenizerFast.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n","else:\n","    tokenizer = DebertaV2TokenizerFast.from_pretrained(CFG.pretrained_model_name)\n","    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n","\n","CFG.tokenizer = tokenizer"]},{"cell_type":"markdown","id":"compatible-lincoln","metadata":{"id":"compatible-lincoln"},"source":["## Create dataset"]},{"cell_type":"code","execution_count":24,"id":"fluid-nancy","metadata":{"id":"fluid-nancy","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["b6614359d578492bb5573280c915b6c4","ea430aaa256e49eb96079a74f3b4fecc","2e7aea141d4d4b4f9df6b6ad9f02575b","fbe6adb0e2a0473d87e516e475d6e22b","e2e730a7bb444d27888c7c8b939dc2b3","6b7cb5ed67b446ada62593df7cf85732","36506a8f48034a44be169658e7fd9264","622069a10c4b4dc8a9cf9d36ae5717c3","ebcb1784deef421d904e97b7bffd1eae","d974ecce62d8481d87203c42adc9886e","1f8405cdfbed4347b4cabfd0242100f1"]},"executionInfo":{"status":"ok","timestamp":1650528658783,"user_tz":-540,"elapsed":23221,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"4b563822-aee6-46b5-a97c-90c46dccd07f"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/42146 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6614359d578492bb5573280c915b6c4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 284\n"]}],"source":["pn_history_lengths = []\n","tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    pn_history_lengths.append(length)\n","\n","print(\"max length:\", np.max(pn_history_lengths))"]},{"cell_type":"code","execution_count":25,"id":"posted-humidity","metadata":{"id":"posted-humidity","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["9a5be675403049e4b723d5d2d5f0f780","842e793e673546f1958be885102651be","7382650b39064fca99c40f773ab7a15f","1525f1d419c64db0ac150665cd091508","e64200af7c9e40af9a72b0c6979c1330","f16f832089c84e2e930ee34e7f3f7fbf","ced8a9be7a6f4f37ba1496615717eedd","f26a45f0081c41e899ddd3e45a4ce55c","6527aa2cb9a243e4ba4a9d833326d094","a275df2f1504430f963586fbcad1ddd0","e54d05fa48774c029f49472a3f9f0be2"]},"executionInfo":{"status":"ok","timestamp":1650528658784,"user_tz":-540,"elapsed":23,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"0d9002e9-1aa0-4b5a-c60a-251b5a89bb5e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/143 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a5be675403049e4b723d5d2d5f0f780"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 28\n"]}],"source":["feature_text_lengths = []\n","tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    feature_text_lengths.append(length)\n","\n","print(\"max length:\", np.max(feature_text_lengths))"]},{"cell_type":"code","execution_count":26,"id":"resistant-amount","metadata":{"id":"resistant-amount","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650528658784,"user_tz":-540,"elapsed":20,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"1305850a-9811-4c6a-afba-a42890a653f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["max length: 315\n"]}],"source":["CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n","\n","print(\"max length:\", CFG.max_len)"]},{"cell_type":"code","execution_count":27,"id":"august-equity","metadata":{"id":"august-equity","executionInfo":{"status":"ok","timestamp":1650528658785,"user_tz":-540,"elapsed":18,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["class TrainingDataset(Dataset):\n","    def __init__(self, cfg, df, pseudo_label=None):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","        self.annotation_lengths = self.df[\"annotation_length\"].values\n","        self.locations = self.df[\"location\"].values\n","        if \"pseudo_idx\" in df.columns:\n","            self.pseudo_idx = self.df[\"pseudo_idx\"].values\n","            self.pseudo_label = pseudo_label\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def _create_label(self, pn_history, annotation_length, location_list):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        offset_mapping = encoded[\"offset_mapping\"]\n","        ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n","        label = np.zeros(len(offset_mapping))\n","        label[ignore_idxes] = -1\n","\n","        if annotation_length > 0:\n","            for location in location_list:\n","                for loc in [s.split() for s in location.split(\";\")]:\n","                    start, end = int(loc[0]), int(loc[1])\n","                    start_idx = -1\n","                    end_idx = -1\n","                    for idx in range(len(offset_mapping)):\n","                        if (start_idx == -1) & (start < offset_mapping[idx][0]):\n","                            start_idx = idx - 1\n","                        if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n","                            end_idx = idx + 1\n","                    if start_idx == -1:\n","                        start_idx = end_idx\n","                    if (start_idx != -1) & (end_idx != -1):\n","                        label[start_idx:end_idx] = 1\n","\n","        return torch.tensor(label, dtype=torch.float)\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n","        return input_, label"]},{"cell_type":"code","execution_count":28,"id":"weird-interaction","metadata":{"id":"weird-interaction","executionInfo":{"status":"ok","timestamp":1650528658785,"user_tz":-540,"elapsed":18,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["class TestDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        return input_"]},{"cell_type":"markdown","id":"upper-mobility","metadata":{"id":"upper-mobility"},"source":["## Model"]},{"cell_type":"code","source":["from transformers.modeling_outputs import MaskedLMOutput\n","\n","class MaskedModel(nn.Module):\n","    def __init__(self, cfg, config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","        if config_path is None:\n","            self.config = AutoConfig.from_pretrained(\n","                cfg.pretrained_model_name,\n","                output_hidden_states=False\n","                )\n","        else:\n","            self.config = torch.load(config_path)\n","        \n","        if pretrained:\n","            self.model = AutoModel.from_pretrained(cfg.pretrained_model_name, config=self.config)\n","            self.lm_head = AutoModelForMaskedLM.from_pretrained(cfg.pretrained_model_name, config=self.config).cls # [cls, lm_head]\n","        else:\n","            self.model = AutoModel(self.config)\n","            self.lm_head = AutoModelForMaskedLM(self.config).cls # [cls, lm_head]\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def forward(\n","            self, \n","            input_ids=None,\n","            attention_mask=None,\n","            token_type_ids=None,\n","            #position_ids=None,\n","            inputs_embeds=None,\n","            labels=None,\n","            output_attentions=None,\n","            output_hidden_states=None,\n","            return_dict=None):\n","        \n","        outputs = self.model(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            #position_ids=position_ids,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,)\n","        \n","        sequence_output = outputs[0]\n","        prediction_scores = self.lm_head(sequence_output)\n","\n","        masked_lm_loss = None\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss()\n","            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n","\n","        return MaskedLMOutput(loss=masked_lm_loss,\n","                              logits=prediction_scores,\n","                              hidden_states=outputs.hidden_states,\n","                              attentions=outputs.attentions)"],"metadata":{"id":"a4HSgs6b8wQT","executionInfo":{"status":"ok","timestamp":1650528658785,"user_tz":-540,"elapsed":18,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"id":"a4HSgs6b8wQT","execution_count":29,"outputs":[]},{"cell_type":"code","execution_count":30,"id":"spanish-destruction","metadata":{"id":"spanish-destruction","executionInfo":{"status":"ok","timestamp":1650528658786,"user_tz":-540,"elapsed":18,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["class CustomModel(nn.Module):\n","    def __init__(self, cfg, model_config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        if model_config_path is None:\n","            self.model_config = AutoConfig.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                output_hidden_states=True,\n","            )\n","        else:\n","            self.model_config = torch.load(model_config_path)\n","\n","        if pretrained:\n","            self.backbone = AutoModel.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                config=self.model_config,\n","            )\n","            print(f\"Load weight from pretrained\")\n","        else:\n","            #self.backbone = AutoModel.from_config(self.model_config)\n","            # itpt = AutoModelForMaskedLM.from_config(self.model_config)\n","            #path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp010/checkpoint-130170/pytorch_model.bin\")\n","            # path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n","            # state_dict = torch.load(path)\n","            # itpt.load_state_dict(state_dict)\n","            path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\")\n","            masked_model = MaskedModel(CFG, config_path=None, pretrained=True)\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            masked_model.load_state_dict(state)\n","            self.backbone = masked_model.model\n","            print(f\"Load weight from {path}\")\n","\n","        self.fc = nn.Sequential(\n","            nn.Dropout(self.cfg.dropout),\n","            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n","        )\n","\n","    def forward(self, inputs):\n","        h = self.backbone(**inputs)[\"last_hidden_state\"]\n","        output = self.fc(h)\n","        return output"]},{"cell_type":"markdown","id":"chronic-bullet","metadata":{"id":"chronic-bullet"},"source":["## Training"]},{"cell_type":"code","source":["class FocalLoss(nn.Module):\n","    def __init__(self, reduction='none', alpha=1, gamma=2):\n","        super().__init__()\n","        self.reduction = reduction\n","        self.alpha = alpha\n","        self.gamma = gamma\n","\n","    def forward(self, inputs, targets):\n","        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n","        pt = torch.exp(-bce_loss)\n","        loss = self.alpha * (1. - pt)**self.gamma * bce_loss\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss\n","\n","\n","class SmoothFocalLoss(nn.Module):\n","    def __init__(self, reduction='none', alpha=1, gamma=2, smoothing=0.0):\n","        super().__init__()\n","        self.reduction = reduction\n","        self.focal_loss = FocalLoss(reduction='none', alpha=alpha, gamma=gamma)\n","        self.smoothing = smoothing\n","\n","    @staticmethod\n","    def _smooth(targets:torch.Tensor, smoothing=0.0):\n","        assert 0 <= smoothing < 1\n","        with torch.no_grad():\n","            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n","        return targets\n","\n","    def forward(self, inputs, targets):\n","        targets = SmoothFocalLoss._smooth(targets, self.smoothing)\n","        loss = self.focal_loss(inputs, targets)\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss\n","\n","    \n","class CEFocalLoss(nn.Module):\n","    def __init__(self, reduction='none', alpha=1, gamma=2):\n","        super(CEFocalLoss, self).__init__()\n","        self.reduction = reduction\n","        self.alpha = alpha\n","        self.gamma = gamma\n","\n","    def forward(self, inputs, targets):\n","        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n","        pt = torch.exp(-ce_loss)\n","        loss = self.alpha * (1. - pt)**self.gamma * ce_loss\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss\n","\n","    \n","class SmoothCEFocalLoss(nn.Module):\n","    def __init__(self, reduction='none', alpha=1, gamma=2, smoothing=0.0):\n","        super(SmoothCEFocalLoss, self).__init__()\n","        self.reduction = reduction\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.smoothing = smoothing\n","\n","    def forward(self, inputs, targets):\n","        ce_loss = F.cross_entropy(inputs, targets, reduction='none', label_smoothing=self.smoothing) # torch >= 1.10.0\n","        pt = torch.exp(-ce_loss)\n","        loss = self.alpha * (1. - pt)**self.gamma * ce_loss\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss"],"metadata":{"id":"jVXgmwt9nJll","executionInfo":{"status":"ok","timestamp":1650528659298,"user_tz":-540,"elapsed":530,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"id":"jVXgmwt9nJll","execution_count":31,"outputs":[]},{"cell_type":"code","execution_count":32,"id":"biological-hunger","metadata":{"id":"biological-hunger","executionInfo":{"status":"ok","timestamp":1650528659299,"user_tz":-540,"elapsed":5,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def train_fn(\n","    train_dataloader,\n","    model,\n","    criterion,\n","    optimizer,\n","    epoch,\n","    scheduler,\n","    device,\n","):\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(train_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","        loss = loss.mean()\n","\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","\n","        if CFG.batch_scheduler:\n","            scheduler.step()\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n","            print(\n","                \"Epoch: [{0}][{1}/{2}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                \"Grad: {grad_norm:.4f}  \"\n","                \"LR: {lr:.6f}  \"\n","                .format(\n","                    epoch+1,\n","                    step,\n","                    len(train_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n","                    loss=losses,\n","                     grad_norm=grad_norm,\n","                     lr=scheduler.get_lr()[0],\n","                )\n","            )\n","    return losses.avg"]},{"cell_type":"code","execution_count":33,"id":"satisfied-sterling","metadata":{"id":"satisfied-sterling","executionInfo":{"status":"ok","timestamp":1650528659299,"user_tz":-540,"elapsed":4,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def valid_fn(\n","    val_dataloader,\n","    model,\n","    criterion,\n","    device,\n","):\n","    model.eval()\n","    preds = []\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(val_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.no_grad():\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","        loss = loss.mean()\n","\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n","            print(\n","                \"EVAL: [{0}/{1}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                .format(\n","                    step, len(val_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n","                    loss=losses,\n","                )\n","            )\n","    preds = np.concatenate(preds)\n","    return losses.avg, preds"]},{"cell_type":"code","execution_count":34,"id":"incorporate-viking","metadata":{"id":"incorporate-viking","executionInfo":{"status":"ok","timestamp":1650528659300,"user_tz":-540,"elapsed":5,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def inference_fn(test_dataloader, model, device):\n","    model.eval()\n","    model.to(device)\n","    preds = []\n","    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n","    for inputs in tk0:\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        with torch.no_grad():\n","            output = model(inputs)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","    preds = np.concatenate(preds)\n","    return preds"]},{"cell_type":"code","execution_count":35,"id":"dental-sunset","metadata":{"id":"dental-sunset","executionInfo":{"status":"ok","timestamp":1650528659300,"user_tz":-540,"elapsed":5,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def train_loop(df, i_fold, device):\n","    print(f\"========== fold: {i_fold} training ==========\")\n","    train_idx = df[df[\"fold\"] != i_fold].index\n","    val_idx = df[df[\"fold\"] == i_fold].index\n","\n","    train_folds = df.loc[train_idx].reset_index(drop=True)\n","    val_folds = df.loc[val_idx].reset_index(drop=True)\n","\n","    if CFG.pseudo_plain_path is not None:\n","        pseudo_plain = pd.read_pickle(CFG.pseudo_plain_path)\n","        print(f\"get pseudo plain from {CFG.pseudo_plain_path}\")\n","        pseudo_label_list = []\n","        weights = [0.4433659049657008, 0.20859987143371844, 0.3480342236005807]\n","        #weights = [0.39072210303764265, 0.10732435236685746, 0.1766765116890754, 0.3252770329064244]\n","        #for exp_name in [\"nbme-exp060\", \"nbme-exp067\", \"nbme-exp083\", \"nbme-exp087\"]:\n","        for exp_name in [\"nbme-exp060\", \"nbme-exp067\", \"nbme-exp083\"]:\n","            pseudo_label_path = f'./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/{exp_name}/pseudo_labels_{i_fold}.npy'\n","            #pseudo_label_path = f'../output/nbme-score-clinical-patient-notes/{exp_name}/pseudo_labels_{i_fold}.npy'\n","            pseudo_label = np.load(pseudo_label_path)\n","            print(f\"get pseudo labels from {pseudo_label_path}\")\n","            pseudo_label_list.append(pseudo_label)\n","\n","        pseudo_label = weights[0] * pseudo_label_list[0] + weights[1] * pseudo_label_list[1] + weights[2] * pseudo_label_list[2]\n","        #pseudo_label = weights[0] * pseudo_label_list[0] + weights[1] * pseudo_label_list[1] + weights[2] * pseudo_label_list[2] + weights[3] * pseudo_label_list[3]\n","        pseudo_label = trunc_pred(pseudo_plain[\"pn_history\"].values, pseudo_label)\n","        best_thres = 0.46\n","        predicted_location_str = get_predicted_location_str(pseudo_label, th=best_thres)\n","        preds = get_predictions(predicted_location_str)\n","        results_postprocess = postprocess(pseudo_plain[\"pn_history\"].values, preds)\n","        results_postprocess = get_results_from_preds_list(results_postprocess)\n","        #pseudo_label = get_preds_from_results(results_postprocess, pseudo_plain[\"pn_history\"].values, pseudo_label.shape[1])\n","        pseudo_plain[\"location\"] = results_postprocess\n","        pseudo_plain[\"annotation_length\"] = pseudo_plain[\"location\"].str.len()\n","        pseudo_plain[\"location\"] = pseudo_plain[\"location\"].apply(lambda x: [x])\n","        print(pseudo_plain.shape)\n","\n","        pseudo_plain['feature_text'] = pseudo_plain['feature_text'].str.lower()\n","        pseudo_plain['pn_history'] = pseudo_plain['pn_history'].str.lower()\n","\n","        pseudo_plain[\"pseudo_idx\"] = np.arange(len(pseudo_plain))\n","        pseudo_plain = pseudo_plain.sample(n=CFG.n_pseudo_labels)\n","        print(pseudo_plain.shape)\n","        train_folds = pd.concat([train_folds, pseudo_plain], axis=0, ignore_index=True)\n","        print(train_folds.shape)\n","\n","    train_dataset = TrainingDataset(CFG, train_folds, pseudo_label)\n","    val_dataset = TrainingDataset(CFG, val_folds)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=True,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=True,\n","    )\n","    val_dataloader = DataLoader(\n","        val_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=False,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=False,\n","    )\n","\n","    # model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","    model = CustomModel(CFG, model_config_path=None, pretrained=False)   # itptを使うため\n","    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n","    model.to(device)\n","\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\"params\": [p for n, p in param_optimizer if not any(\n","            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n","        {\"params\": [p for n, p in param_optimizer if any(\n","            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n","    ]\n","    optimizer = AdamW(\n","        optimizer_grouped_parameters,\n","        lr=CFG.lr,\n","        betas=CFG.betas,\n","        weight_decay=CFG.weight_decay,\n","    )\n","    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n","    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        num_training_steps=num_train_optimization_steps,\n","    )\n","\n","    criterion = SmoothFocalLoss(reduction='none', alpha=CFG.alpha, gamma=CFG.gamma, smoothing=CFG.smoothing)\n","    #criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    best_score = -1 * np.inf\n","\n","    for epoch in range(CFG.epochs):\n","        start_time = time.time()\n","        avg_loss = train_fn(\n","            train_dataloader,\n","            model,\n","            criterion,\n","            optimizer,\n","            epoch,\n","            scheduler,\n","            device,\n","        )\n","        avg_val_loss, val_preds = valid_fn(\n","            val_dataloader,\n","            model,\n","            criterion,\n","            device,\n","        )\n","\n","        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n","            scheduler.step()\n","\n","        # scoring\n","        val_folds[[str(i) for i in range(CFG.max_len)]] = val_preds\n","        score = scoring(val_folds, th=0.5)\n","\n","        elapsed = time.time() - start_time\n","\n","        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n","        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n","        if score > best_score:\n","            best_score = score\n","            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n","            torch.save({\n","                \"model\": model.state_dict(),\n","                \"predictions\": val_preds,\n","                },\n","                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","            )\n","\n","    predictions = torch.load(\n","        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","        map_location=torch.device(\"cpu\"),\n","    )[\"predictions\"]\n","    val_folds[[str(i) for i in range(CFG.max_len)]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return val_folds"]},{"cell_type":"markdown","id":"brazilian-graphics","metadata":{"id":"brazilian-graphics"},"source":["## Main"]},{"cell_type":"code","execution_count":36,"id":"connected-protein","metadata":{"id":"connected-protein","executionInfo":{"status":"ok","timestamp":1650528659300,"user_tz":-540,"elapsed":4,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}}},"outputs":[],"source":["def main():\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    if CFG.train:\n","        oof_df = pd.DataFrame()\n","        for i_fold in range(CFG.n_fold):\n","            if i_fold in CFG.train_fold:\n","                _oof_df = train_loop(train, i_fold, device)\n","                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n","        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    if CFG.submission:\n","        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n","    else:\n","        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    score = scoring(oof_df, th=0.5)\n","    print(f\"Best thres: 0.5, Score: {score:.4f}\")\n","    best_thres = get_best_thres(oof_df)\n","    score = scoring(oof_df, th=best_thres)\n","    print(f\"Best thres: {best_thres}, Score: {score:.4f}\")\n","\n","    if CFG.inference:\n","        test_dataset = TestDataset(CFG, test)\n","        test_dataloader = DataLoader(\n","            test_dataset,\n","            batch_size=CFG.batch_size,\n","            shuffle=False,\n","            num_workers=CFG.num_workers,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","        predictions = []\n","        for i_fold in CFG.train_fold:\n","            if CFG.submission:\n","                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n","                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n","            else:\n","                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n","\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            model.load_state_dict(state[\"model\"])\n","            test_token_probs = inference_fn(test_dataloader, model, device)\n","            test[[f\"fold{i_fold}_{i}\" for i in range(CFG.max_len)]] = test_token_probs\n","            test_char_probs = get_char_probs(test[\"pn_history\"].values, test_token_probs, CFG.tokenizer)\n","            predictions.append(test_char_probs)\n","\n","            del state, test_token_probs, model; gc.collect()\n","            torch.cuda.empty_cache()\n","\n","        predictions = np.mean(predictions, axis=0)\n","        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n","        test[CFG.target_col] = predicted_location_str\n","        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n","        test[[CFG.id_col, CFG.target_col]].to_csv(\n","            CFG.output_dir / \"submission.csv\", index=False\n","        )"]},{"cell_type":"code","execution_count":37,"id":"serious-bunny","metadata":{"id":"serious-bunny","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5f756312b8bc4c45b78b0dc74b5e7ca0","e1f51d05208d4246b19900bd30819e08","35623ce5c256498eba491801939f067b","f777c53ffe4046ddb093adbccfb51226","3285f18c39aa4c74991784d2eeb6c638","d63fdf839bd0471bb6e56f8cbe540f75","48f462d421a547c9ac67d3bd69b9e06c","c3df1108c7fc4560808ff9f8887e6ed8","3d46eeb74add41caa57616f0343b56df","2b1ebb4f2b8c41a1ba41608ebf9bb38e","f31dfaadc372462e8a64ca253144f83d","d5ba0a0acb914906bd0e3c780d4272fe","39bc3703ab4a495ca62c3c38b61499b3","260ccf92d3ec453daf96aac32a2e6051","c2b37233d0ae4d0f987769481bf4a294","1af45d64d0764a019e6da49e29b1d27a","401f77ddd97a4a63a15c9092b5731fc4","3f75ec189f8b49c7a0dac03d6a516060","a852bb77efbd44588d5b4f9ed713609e","eb62623535204039ad1d203b9ad7b326","15e1514a3b11453a8082b8627c227b72","071385b8a2e64909935d192079f1b884","f8d5efc2fac34d5fbb266fd0ea6a4442","e443d3ef54f64e56b7059c958c634337","8291bc6ac8f44f1a845ea536538a9fa7","304be1e7971f4a9b8ba7872cb0b5f955","29a641670ec74293a76c5f95dc6555fc","2dceabd539be499b8665cd9aa7fc1cc9","c5ff0e998f1e417d9f00ccf26606485a","e8e20cea22d94c9cb85700600ee51dfe","66e8a4d819ed4e48b7d542fb4c27798b","b8dcbb1c0b764bc9be1d9b3e47351574","b9edba3c0fb94d72a26acb0ce4acbf51","cd26dec55ba54cbea581e2bc6bb9896e","25b1c6c7718e430c82a8e3c2fcdf4b9e","0d4d4750cc094e51ba3fb644af0d6e21","16177cf50da0456bb57fede520c63001","08e46ea34b464e03a309596a322394cc","a81daa308691458aa827296793220425","76405b65e6df492aaf0d945877360f00","9090f0d48c8e43b8aaef555992aca8f1","8e45e361900e4377a2e1841c6f75ec30","f0ffebfca1ce4d50a1979a92672af1a5","658af1224823430a95d5d9746bbe0b97","24751bd60fad4b3782ad5efc6215c555","817a047c3bce4434ad795af4c7a587cc","6c8408d88b2f4c198ecc7c2fff0aeb98","570d47fc94e44395b6503a7b49ba0266","9c78124e584c4893ad74ba76e84283ad","9918feba471e4d4cb7703c90087a95c6","ca7d3f75152a4351839ae8d7346ac080","6db65fcee9404327b07683fca5815a82","94bb5704a7524786b53afdf82110b4a2","0ff8cfcdd9764bbb87fc713774893565","157da8d746704a6eb7689ab8ab2c05f3","12cb82c88a3946128269436929769660","eb1c4839c49f463fb321e3c722af68e2","e204832fb8cb488b8a92bdabd055b7a5","377a291d13ab4d1fae9315e2d2e1381f","b8e35b2526d34b9fac71d2534f7bfdc4","ae0cc1292d9c4157bc4e356df5fe3931","b54182c1618e48a59af278d3fbedda05","8b82445c11664de29ae3f26144df7046","403030e15d9c4aa694d565c1bd7bf852","b557efa384a845e997ba54c5abe56272","eff26e26da9c4b24af9a266bb5fdd8c8","c2b0099e5324495893453c16cec39104","abbaa4da1f5d4a3897daaf40b5c1a14b","3124e3f2dc7e4ad5a41e9cce23f8baf0","1e7346dd582c445a98557b83d9b75740","1ac1231dbfb5451ebfc513709b042304","d973ad80e28d4c758430706d6493881d","36848b89df704c8ba2b193c101dfceb0","2e61ad6e2b8b4f8893ea16fa96401386","b9ad54c1127841c9b2071cb4a0b4d1bc","7dd806b3ea674ab4846e4876d6c099bc","fbe5798aa67b40f88210fdb4e8762262","a284a67098574d22a5ce1e2457757f7d","10412e630c2f4da18ee909f1307ab73a","1a59f0cf374a4fe7a42082c89a5ce8e1","1c70cac4bb964e9ca0c4ee48ed47c988","9b6872f797be43c6be38112a8d4006d6","2f6629d74c10413097450b908296b558","44c4b61f8e814eb287232fdeba2768e1","680cdf8f2c2a4595a0268ef9477bc36b","a993ad5a2565427aa2d97793761017d1","7d3f786aec014ffc99646dc2138bbdf0","b197d043725348e78c90dcfb18a4796e","941b59b26d99412ead555431db0469c1","8f0c80e24053402b9e2435145b2fb9c9","b55484d482ed4529bebf28b47516b893","3256afcaffcf449fb09a51bd2ceb37aa","94d5d8e942d14998803ec44e42e86588","60a7a36d92064040b3c82e6b85e0e6b0","d41246faf43449d1b2dddb714e51f7a1","3e3895df37bd4e63908a43b81a059513","5ccc7e11a15e467591fb42ad945a015e","526cd30fe20e4bafa08329c16b5ed160","4a167a7227854381806c883a542b01f9","5f2e6b643cf9447f96f4617cab77d51f","2894046bbc214e9a88b8b4c2763f0fbe","4d5f49d6179249d3bc3f7ec187a113bc","f07517c5583141b7b203534996e47073","7afd4dae04cf44649190cf7f1bf00eaa","4666d5d76e40486a96416440837a04e6","260cd738f92a4e2da00f81c836d73839","c5fd60296ecb429589335bdaedbdd8d5","1743d331df974f1a8891c4edf13d4c91","b5d0518c98704e60bdf7059e2c00be00","8dd66f44bb8c4ecfb383eb657afaec97","ef497effc689438a8a81b2fa510bdedf","0343fe4023774746a9964f519de16c62","1c27a7047fcc497cbc1be7d43ce1c22c","2464f9fbc4d644d19054754d875dc9de","72baebaa4c674ee2b021c7cb89410209","b31410143cb746919e9206d7a4234bff","fc7e9559d432451a8fe13d26ecace871","49ebd576bcb34b3bbc0b483047615cf5","c50288771b5d477d8443662c280fabac","3d8bd1265146434a96add1d73d4f3b3e","02d5a192d51d42e29a09d6e8c4e6b5f9","5c2c4d2e64e447dca185591b1fa7d000","51dd188dae8e49b7aa62a90814871f00","a5ba00f6f6a04c2fadb6c8a0f8d2aa42","85c67506e3f24cdfa15311a6c5e5dca7","a192d24d130f48b192ec3c4795ec20d9","a05307bcd0864b42aadc129a41a7c223","e8638c99b34047d5a6da131f19992623","02eb542bd71b49deb0c3c037962d3525","78e44d3897254bed9d880bc352db9ef0","4d4b53edfb7b402cb7466203eb99099e","2853220ffced40d6893576a91b657b1f"]},"executionInfo":{"status":"ok","timestamp":1650558788335,"user_tz":-540,"elapsed":30129039,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"}},"outputId":"cd0b2ff8-893f-4312-88d6-e3660417b3fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["========== fold: 0 training ==========\n","get pseudo plain from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n","get pseudo labels from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_0.npy\n","get pseudo labels from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_0.npy\n","get pseudo labels from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp083/pseudo_labels_0.npy\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/612602 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f756312b8bc4c45b78b0dc74b5e7ca0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/612602 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5ba0a0acb914906bd0e3c780d4272fe"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["(612602, 8)\n","(100000, 9)\n","(110725, 11)\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\n","Epoch: [1][0/36908] Elapsed 0m 1s (remain 650m 7s) Loss: 0.0995(0.0995) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/36908] Elapsed 0m 20s (remain 126m 16s) Loss: 0.0686(0.0924) Grad: 74713.8359  LR: 0.000001  \n","Epoch: [1][200/36908] Elapsed 0m 40s (remain 122m 53s) Loss: 0.0280(0.0696) Grad: 21232.4844  LR: 0.000001  \n","Epoch: [1][300/36908] Elapsed 0m 59s (remain 121m 30s) Loss: 0.0184(0.0515) Grad: 6661.7559  LR: 0.000002  \n","Epoch: [1][400/36908] Elapsed 1m 19s (remain 120m 42s) Loss: 0.0110(0.0416) Grad: 4888.3027  LR: 0.000002  \n","Epoch: [1][500/36908] Elapsed 1m 39s (remain 119m 58s) Loss: 0.0024(0.0359) Grad: 4135.3188  LR: 0.000003  \n","Epoch: [1][600/36908] Elapsed 1m 58s (remain 119m 32s) Loss: 0.0201(0.0318) Grad: 7904.0547  LR: 0.000003  \n","Epoch: [1][700/36908] Elapsed 2m 18s (remain 119m 6s) Loss: 0.0051(0.0288) Grad: 4200.0278  LR: 0.000004  \n","Epoch: [1][800/36908] Elapsed 2m 37s (remain 118m 41s) Loss: 0.0101(0.0268) Grad: 4861.5029  LR: 0.000004  \n","Epoch: [1][900/36908] Elapsed 2m 57s (remain 118m 15s) Loss: 0.0162(0.0249) Grad: 6169.4214  LR: 0.000005  \n","Epoch: [1][1000/36908] Elapsed 3m 17s (remain 117m 51s) Loss: 0.0039(0.0233) Grad: 2605.8665  LR: 0.000005  \n","Epoch: [1][1100/36908] Elapsed 3m 36s (remain 117m 30s) Loss: 0.0062(0.0217) Grad: 6258.7422  LR: 0.000006  \n","Epoch: [1][1200/36908] Elapsed 3m 56s (remain 117m 13s) Loss: 0.0044(0.0204) Grad: 13194.6309  LR: 0.000007  \n","Epoch: [1][1300/36908] Elapsed 4m 16s (remain 116m 54s) Loss: 0.0035(0.0192) Grad: 7872.2964  LR: 0.000007  \n","Epoch: [1][1400/36908] Elapsed 4m 35s (remain 116m 33s) Loss: 0.0034(0.0180) Grad: 10854.6094  LR: 0.000008  \n","Epoch: [1][1500/36908] Elapsed 4m 55s (remain 116m 21s) Loss: 0.0007(0.0171) Grad: 1397.8944  LR: 0.000008  \n","Epoch: [1][1600/36908] Elapsed 5m 15s (remain 115m 53s) Loss: 0.0009(0.0162) Grad: 3687.1182  LR: 0.000009  \n","Epoch: [1][1700/36908] Elapsed 5m 34s (remain 115m 25s) Loss: 0.0027(0.0154) Grad: 4279.6851  LR: 0.000009  \n","Epoch: [1][1800/36908] Elapsed 5m 53s (remain 114m 56s) Loss: 0.0001(0.0148) Grad: 399.7263  LR: 0.000010  \n","Epoch: [1][1900/36908] Elapsed 6m 13s (remain 114m 29s) Loss: 0.0005(0.0141) Grad: 1758.4276  LR: 0.000010  \n","Epoch: [1][2000/36908] Elapsed 6m 32s (remain 114m 3s) Loss: 0.0016(0.0135) Grad: 2375.5964  LR: 0.000011  \n","Epoch: [1][2100/36908] Elapsed 6m 51s (remain 113m 37s) Loss: 0.0001(0.0130) Grad: 703.0740  LR: 0.000011  \n","Epoch: [1][2200/36908] Elapsed 7m 10s (remain 113m 12s) Loss: 0.0031(0.0125) Grad: 7697.4189  LR: 0.000012  \n","Epoch: [1][2300/36908] Elapsed 7m 30s (remain 112m 50s) Loss: 0.0015(0.0121) Grad: 3494.6909  LR: 0.000012  \n","Epoch: [1][2400/36908] Elapsed 7m 49s (remain 112m 27s) Loss: 0.0014(0.0117) Grad: 2429.7966  LR: 0.000013  \n","Epoch: [1][2500/36908] Elapsed 8m 8s (remain 112m 5s) Loss: 0.0005(0.0113) Grad: 1465.4788  LR: 0.000014  \n","Epoch: [1][2600/36908] Elapsed 8m 28s (remain 111m 43s) Loss: 0.0003(0.0109) Grad: 1395.1801  LR: 0.000014  \n","Epoch: [1][2700/36908] Elapsed 8m 47s (remain 111m 21s) Loss: 0.0001(0.0106) Grad: 387.5774  LR: 0.000015  \n","Epoch: [1][2800/36908] Elapsed 9m 7s (remain 111m 1s) Loss: 0.0001(0.0103) Grad: 423.8675  LR: 0.000015  \n","Epoch: [1][2900/36908] Elapsed 9m 26s (remain 110m 38s) Loss: 0.0003(0.0100) Grad: 867.8011  LR: 0.000016  \n","Epoch: [1][3000/36908] Elapsed 9m 45s (remain 110m 16s) Loss: 0.0033(0.0098) Grad: 3907.4031  LR: 0.000016  \n","Epoch: [1][3100/36908] Elapsed 10m 4s (remain 109m 55s) Loss: 0.0004(0.0095) Grad: 1133.7897  LR: 0.000017  \n","Epoch: [1][3200/36908] Elapsed 10m 24s (remain 109m 35s) Loss: 0.0004(0.0093) Grad: 1444.9879  LR: 0.000017  \n","Epoch: [1][3300/36908] Elapsed 10m 43s (remain 109m 15s) Loss: 0.0034(0.0090) Grad: 3844.0881  LR: 0.000018  \n","Epoch: [1][3400/36908] Elapsed 11m 3s (remain 108m 55s) Loss: 0.0001(0.0088) Grad: 1392.6910  LR: 0.000018  \n","Epoch: [1][3500/36908] Elapsed 11m 22s (remain 108m 34s) Loss: 0.0023(0.0086) Grad: 2571.4705  LR: 0.000019  \n","Epoch: [1][3600/36908] Elapsed 11m 41s (remain 108m 12s) Loss: 0.0054(0.0084) Grad: 9510.9355  LR: 0.000020  \n","Epoch: [1][3700/36908] Elapsed 12m 1s (remain 107m 50s) Loss: 0.0008(0.0083) Grad: 1453.3976  LR: 0.000020  \n","Epoch: [1][3800/36908] Elapsed 12m 20s (remain 107m 29s) Loss: 0.0001(0.0081) Grad: 455.7661  LR: 0.000020  \n","Epoch: [1][3900/36908] Elapsed 12m 39s (remain 107m 7s) Loss: 0.0001(0.0079) Grad: 657.5601  LR: 0.000020  \n","Epoch: [1][4000/36908] Elapsed 12m 58s (remain 106m 46s) Loss: 0.0008(0.0078) Grad: 2873.4426  LR: 0.000020  \n","Epoch: [1][4100/36908] Elapsed 13m 18s (remain 106m 26s) Loss: 0.0001(0.0076) Grad: 1023.1611  LR: 0.000020  \n","Epoch: [1][4200/36908] Elapsed 13m 37s (remain 106m 5s) Loss: 0.0002(0.0075) Grad: 1624.8545  LR: 0.000020  \n","Epoch: [1][4300/36908] Elapsed 13m 56s (remain 105m 45s) Loss: 0.0010(0.0073) Grad: 6307.2002  LR: 0.000020  \n","Epoch: [1][4400/36908] Elapsed 14m 16s (remain 105m 24s) Loss: 0.0001(0.0072) Grad: 257.8450  LR: 0.000020  \n","Epoch: [1][4500/36908] Elapsed 14m 35s (remain 105m 4s) Loss: 0.0001(0.0071) Grad: 666.3967  LR: 0.000020  \n","Epoch: [1][4600/36908] Elapsed 14m 55s (remain 104m 44s) Loss: 0.0001(0.0070) Grad: 1180.6906  LR: 0.000019  \n","Epoch: [1][4700/36908] Elapsed 15m 14s (remain 104m 24s) Loss: 0.0007(0.0068) Grad: 5093.8125  LR: 0.000019  \n","Epoch: [1][4800/36908] Elapsed 15m 33s (remain 104m 4s) Loss: 0.0001(0.0067) Grad: 730.6360  LR: 0.000019  \n","Epoch: [1][4900/36908] Elapsed 15m 53s (remain 103m 44s) Loss: 0.0001(0.0066) Grad: 1638.7960  LR: 0.000019  \n","Epoch: [1][5000/36908] Elapsed 16m 12s (remain 103m 24s) Loss: 0.0032(0.0065) Grad: 18633.5879  LR: 0.000019  \n","Epoch: [1][5100/36908] Elapsed 16m 31s (remain 103m 3s) Loss: 0.0000(0.0064) Grad: 180.5032  LR: 0.000019  \n","Epoch: [1][5200/36908] Elapsed 16m 50s (remain 102m 42s) Loss: 0.0005(0.0063) Grad: 3660.0269  LR: 0.000019  \n","Epoch: [1][5300/36908] Elapsed 17m 10s (remain 102m 22s) Loss: 0.0028(0.0063) Grad: 8534.6904  LR: 0.000019  \n","Epoch: [1][5400/36908] Elapsed 17m 29s (remain 102m 2s) Loss: 0.0000(0.0062) Grad: 139.7159  LR: 0.000019  \n","Epoch: [1][5500/36908] Elapsed 17m 48s (remain 101m 43s) Loss: 0.0005(0.0061) Grad: 2022.2288  LR: 0.000019  \n","Epoch: [1][5600/36908] Elapsed 18m 8s (remain 101m 22s) Loss: 0.0003(0.0060) Grad: 1466.7488  LR: 0.000019  \n","Epoch: [1][5700/36908] Elapsed 18m 27s (remain 101m 3s) Loss: 0.0000(0.0059) Grad: 303.3793  LR: 0.000019  \n","Epoch: [1][5800/36908] Elapsed 18m 47s (remain 100m 44s) Loss: 0.0011(0.0058) Grad: 9710.5566  LR: 0.000019  \n","Epoch: [1][5900/36908] Elapsed 19m 6s (remain 100m 24s) Loss: 0.0092(0.0058) Grad: 7044.6797  LR: 0.000019  \n","Epoch: [1][6000/36908] Elapsed 19m 25s (remain 100m 4s) Loss: 0.0000(0.0057) Grad: 237.8451  LR: 0.000019  \n","Epoch: [1][6100/36908] Elapsed 19m 45s (remain 99m 45s) Loss: 0.0005(0.0056) Grad: 2488.8032  LR: 0.000019  \n","Epoch: [1][6200/36908] Elapsed 20m 4s (remain 99m 25s) Loss: 0.0000(0.0055) Grad: 19.0270  LR: 0.000018  \n","Epoch: [1][6300/36908] Elapsed 20m 24s (remain 99m 6s) Loss: 0.0031(0.0055) Grad: 11053.9824  LR: 0.000018  \n","Epoch: [1][6400/36908] Elapsed 20m 43s (remain 98m 46s) Loss: 0.0001(0.0054) Grad: 1033.0554  LR: 0.000018  \n","Epoch: [1][6500/36908] Elapsed 21m 2s (remain 98m 26s) Loss: 0.0031(0.0053) Grad: 5206.9561  LR: 0.000018  \n","Epoch: [1][6600/36908] Elapsed 21m 22s (remain 98m 7s) Loss: 0.0012(0.0053) Grad: 6701.1187  LR: 0.000018  \n","Epoch: [1][6700/36908] Elapsed 21m 41s (remain 97m 48s) Loss: 0.0028(0.0052) Grad: 26078.0332  LR: 0.000018  \n","Epoch: [1][6800/36908] Elapsed 22m 1s (remain 97m 29s) Loss: 0.0017(0.0052) Grad: 4783.4653  LR: 0.000018  \n","Epoch: [1][6900/36908] Elapsed 22m 20s (remain 97m 9s) Loss: 0.0074(0.0051) Grad: 12053.2764  LR: 0.000018  \n","Epoch: [1][7000/36908] Elapsed 22m 40s (remain 96m 49s) Loss: 0.0008(0.0051) Grad: 4639.8408  LR: 0.000018  \n","Epoch: [1][7100/36908] Elapsed 22m 59s (remain 96m 30s) Loss: 0.0004(0.0050) Grad: 5464.3589  LR: 0.000018  \n","Epoch: [1][7200/36908] Elapsed 23m 18s (remain 96m 10s) Loss: 0.0000(0.0050) Grad: 213.1163  LR: 0.000018  \n","Epoch: [1][7300/36908] Elapsed 23m 38s (remain 95m 50s) Loss: 0.0001(0.0049) Grad: 2155.8042  LR: 0.000018  \n","Epoch: [1][7400/36908] Elapsed 23m 57s (remain 95m 31s) Loss: 0.0006(0.0048) Grad: 6651.1025  LR: 0.000018  \n","Epoch: [1][7500/36908] Elapsed 24m 16s (remain 95m 11s) Loss: 0.0001(0.0048) Grad: 702.7706  LR: 0.000018  \n","Epoch: [1][7600/36908] Elapsed 24m 36s (remain 94m 52s) Loss: 0.0000(0.0047) Grad: 210.3405  LR: 0.000018  \n","Epoch: [1][7700/36908] Elapsed 24m 55s (remain 94m 32s) Loss: 0.0064(0.0047) Grad: 10567.4990  LR: 0.000018  \n","Epoch: [1][7800/36908] Elapsed 25m 15s (remain 94m 12s) Loss: 0.0000(0.0047) Grad: 58.5220  LR: 0.000018  \n","Epoch: [1][7900/36908] Elapsed 25m 34s (remain 93m 53s) Loss: 0.0001(0.0046) Grad: 387.1839  LR: 0.000017  \n","Epoch: [1][8000/36908] Elapsed 25m 53s (remain 93m 33s) Loss: 0.0000(0.0046) Grad: 506.0955  LR: 0.000017  \n","Epoch: [1][8100/36908] Elapsed 26m 13s (remain 93m 14s) Loss: 0.0000(0.0045) Grad: 683.3814  LR: 0.000017  \n","Epoch: [1][8200/36908] Elapsed 26m 32s (remain 92m 54s) Loss: 0.0005(0.0045) Grad: 3963.3398  LR: 0.000017  \n","Epoch: [1][8300/36908] Elapsed 26m 51s (remain 92m 35s) Loss: 0.0000(0.0044) Grad: 234.2251  LR: 0.000017  \n","Epoch: [1][8400/36908] Elapsed 27m 11s (remain 92m 16s) Loss: 0.0000(0.0044) Grad: 16.9680  LR: 0.000017  \n","Epoch: [1][8500/36908] Elapsed 27m 31s (remain 91m 57s) Loss: 0.0004(0.0044) Grad: 5901.8599  LR: 0.000017  \n","Epoch: [1][8600/36908] Elapsed 27m 50s (remain 91m 37s) Loss: 0.0000(0.0043) Grad: 6.7121  LR: 0.000017  \n","Epoch: [1][8700/36908] Elapsed 28m 9s (remain 91m 18s) Loss: 0.0003(0.0043) Grad: 5528.0947  LR: 0.000017  \n","Epoch: [1][8800/36908] Elapsed 28m 29s (remain 90m 58s) Loss: 0.0008(0.0043) Grad: 10907.6748  LR: 0.000017  \n","Epoch: [1][8900/36908] Elapsed 28m 48s (remain 90m 38s) Loss: 0.0000(0.0042) Grad: 69.8429  LR: 0.000017  \n","Epoch: [1][9000/36908] Elapsed 29m 7s (remain 90m 19s) Loss: 0.0013(0.0042) Grad: 6884.9878  LR: 0.000017  \n","Epoch: [1][9100/36908] Elapsed 29m 27s (remain 89m 59s) Loss: 0.0000(0.0042) Grad: 230.1687  LR: 0.000017  \n","Epoch: [1][9200/36908] Elapsed 29m 46s (remain 89m 39s) Loss: 0.0000(0.0041) Grad: 5.3204  LR: 0.000017  \n","Epoch: [1][9300/36908] Elapsed 30m 5s (remain 89m 20s) Loss: 0.0112(0.0041) Grad: 62143.5586  LR: 0.000017  \n","Epoch: [1][9400/36908] Elapsed 30m 25s (remain 89m 0s) Loss: 0.0000(0.0041) Grad: 91.0716  LR: 0.000017  \n","Epoch: [1][9500/36908] Elapsed 30m 44s (remain 88m 41s) Loss: 0.0000(0.0040) Grad: 71.0599  LR: 0.000017  \n","Epoch: [1][9600/36908] Elapsed 31m 4s (remain 88m 21s) Loss: 0.0000(0.0040) Grad: 10.1256  LR: 0.000016  \n","Epoch: [1][9700/36908] Elapsed 31m 23s (remain 88m 2s) Loss: 0.0000(0.0040) Grad: 44.9623  LR: 0.000016  \n","Epoch: [1][9800/36908] Elapsed 31m 42s (remain 87m 43s) Loss: 0.0006(0.0039) Grad: 7362.9004  LR: 0.000016  \n","Epoch: [1][9900/36908] Elapsed 32m 2s (remain 87m 23s) Loss: 0.0084(0.0039) Grad: 45674.7695  LR: 0.000016  \n","Epoch: [1][10000/36908] Elapsed 32m 21s (remain 87m 4s) Loss: 0.0000(0.0039) Grad: 1119.2101  LR: 0.000016  \n","Epoch: [1][10100/36908] Elapsed 32m 41s (remain 86m 44s) Loss: 0.0000(0.0039) Grad: 390.9904  LR: 0.000016  \n","Epoch: [1][10200/36908] Elapsed 33m 0s (remain 86m 25s) Loss: 0.0001(0.0038) Grad: 1087.6517  LR: 0.000016  \n","Epoch: [1][10300/36908] Elapsed 33m 19s (remain 86m 5s) Loss: 0.0005(0.0038) Grad: 4602.9639  LR: 0.000016  \n","Epoch: [1][10400/36908] Elapsed 33m 39s (remain 85m 46s) Loss: 0.0008(0.0038) Grad: 20824.4824  LR: 0.000016  \n","Epoch: [1][10500/36908] Elapsed 33m 58s (remain 85m 26s) Loss: 0.0009(0.0037) Grad: 10499.7646  LR: 0.000016  \n","Epoch: [1][10600/36908] Elapsed 34m 18s (remain 85m 7s) Loss: 0.0004(0.0037) Grad: 7102.7500  LR: 0.000016  \n","Epoch: [1][10700/36908] Elapsed 34m 37s (remain 84m 47s) Loss: 0.0007(0.0037) Grad: 7418.9937  LR: 0.000016  \n","Epoch: [1][10800/36908] Elapsed 34m 56s (remain 84m 27s) Loss: 0.0000(0.0037) Grad: 428.3734  LR: 0.000016  \n","Epoch: [1][10900/36908] Elapsed 35m 16s (remain 84m 8s) Loss: 0.0012(0.0036) Grad: 26391.5000  LR: 0.000016  \n","Epoch: [1][11000/36908] Elapsed 35m 35s (remain 83m 49s) Loss: 0.0000(0.0036) Grad: 451.4309  LR: 0.000016  \n","Epoch: [1][11100/36908] Elapsed 35m 54s (remain 83m 29s) Loss: 0.0061(0.0036) Grad: 11352.9336  LR: 0.000016  \n","Epoch: [1][11200/36908] Elapsed 36m 14s (remain 83m 10s) Loss: 0.0000(0.0036) Grad: 14.2210  LR: 0.000015  \n","Epoch: [1][11300/36908] Elapsed 36m 33s (remain 82m 51s) Loss: 0.0000(0.0035) Grad: 19.9189  LR: 0.000015  \n","Epoch: [1][11400/36908] Elapsed 36m 53s (remain 82m 31s) Loss: 0.0045(0.0035) Grad: 32305.6504  LR: 0.000015  \n","Epoch: [1][11500/36908] Elapsed 37m 12s (remain 82m 12s) Loss: 0.0004(0.0035) Grad: 4461.6621  LR: 0.000015  \n","Epoch: [1][11600/36908] Elapsed 37m 32s (remain 81m 53s) Loss: 0.0008(0.0035) Grad: 8891.2148  LR: 0.000015  \n","Epoch: [1][11700/36908] Elapsed 37m 51s (remain 81m 34s) Loss: 0.0049(0.0035) Grad: 27108.9766  LR: 0.000015  \n","Epoch: [1][11800/36908] Elapsed 38m 11s (remain 81m 14s) Loss: 0.0004(0.0034) Grad: 5479.2837  LR: 0.000015  \n","Epoch: [1][11900/36908] Elapsed 38m 30s (remain 80m 55s) Loss: 0.0002(0.0034) Grad: 4494.9512  LR: 0.000015  \n","Epoch: [1][12000/36908] Elapsed 38m 50s (remain 80m 35s) Loss: 0.0000(0.0034) Grad: 5.3877  LR: 0.000015  \n","Epoch: [1][12100/36908] Elapsed 39m 9s (remain 80m 16s) Loss: 0.0004(0.0034) Grad: 16327.9307  LR: 0.000015  \n","Epoch: [1][12200/36908] Elapsed 39m 29s (remain 79m 57s) Loss: 0.0000(0.0034) Grad: 395.7552  LR: 0.000015  \n","Epoch: [1][12300/36908] Elapsed 39m 48s (remain 79m 37s) Loss: 0.0002(0.0033) Grad: 8298.3779  LR: 0.000015  \n","Epoch: [1][12400/36908] Elapsed 40m 7s (remain 79m 18s) Loss: 0.0000(0.0033) Grad: 125.3145  LR: 0.000015  \n","Epoch: [1][12500/36908] Elapsed 40m 26s (remain 78m 58s) Loss: 0.0000(0.0033) Grad: 125.9587  LR: 0.000015  \n","Epoch: [1][12600/36908] Elapsed 40m 46s (remain 78m 38s) Loss: 0.0000(0.0033) Grad: 85.6015  LR: 0.000015  \n","Epoch: [1][12700/36908] Elapsed 41m 5s (remain 78m 19s) Loss: 0.0001(0.0033) Grad: 2126.8689  LR: 0.000015  \n","Epoch: [1][12800/36908] Elapsed 41m 25s (remain 78m 0s) Loss: 0.0038(0.0033) Grad: 69161.4062  LR: 0.000015  \n","Epoch: [1][12900/36908] Elapsed 41m 44s (remain 77m 41s) Loss: 0.0000(0.0032) Grad: 1550.2572  LR: 0.000014  \n","Epoch: [1][13000/36908] Elapsed 42m 4s (remain 77m 21s) Loss: 0.0000(0.0032) Grad: 1038.1530  LR: 0.000014  \n","Epoch: [1][13100/36908] Elapsed 42m 23s (remain 77m 2s) Loss: 0.0085(0.0032) Grad: 109238.6250  LR: 0.000014  \n","Epoch: [1][13200/36908] Elapsed 42m 42s (remain 76m 42s) Loss: 0.0000(0.0032) Grad: 23.4771  LR: 0.000014  \n","Epoch: [1][13300/36908] Elapsed 43m 2s (remain 76m 23s) Loss: 0.0000(0.0032) Grad: 1867.5474  LR: 0.000014  \n","Epoch: [1][13400/36908] Elapsed 43m 21s (remain 76m 3s) Loss: 0.0004(0.0031) Grad: 12931.2373  LR: 0.000014  \n","Epoch: [1][13500/36908] Elapsed 43m 41s (remain 75m 44s) Loss: 0.0000(0.0031) Grad: 24.6270  LR: 0.000014  \n","Epoch: [1][13600/36908] Elapsed 44m 0s (remain 75m 24s) Loss: 0.0007(0.0031) Grad: 30966.8926  LR: 0.000014  \n","Epoch: [1][13700/36908] Elapsed 44m 20s (remain 75m 5s) Loss: 0.0000(0.0031) Grad: 2.9812  LR: 0.000014  \n","Epoch: [1][13800/36908] Elapsed 44m 39s (remain 74m 46s) Loss: 0.0000(0.0031) Grad: 190.8645  LR: 0.000014  \n","Epoch: [1][13900/36908] Elapsed 44m 58s (remain 74m 26s) Loss: 0.0000(0.0031) Grad: 20.0580  LR: 0.000014  \n","Epoch: [1][14000/36908] Elapsed 45m 18s (remain 74m 7s) Loss: 0.0001(0.0031) Grad: 4248.2979  LR: 0.000014  \n","Epoch: [1][14100/36908] Elapsed 45m 37s (remain 73m 47s) Loss: 0.0000(0.0030) Grad: 11.5719  LR: 0.000014  \n","Epoch: [1][14200/36908] Elapsed 45m 56s (remain 73m 27s) Loss: 0.0001(0.0030) Grad: 18240.8457  LR: 0.000014  \n","Epoch: [1][14300/36908] Elapsed 46m 16s (remain 73m 8s) Loss: 0.0000(0.0030) Grad: 12.1180  LR: 0.000014  \n","Epoch: [1][14400/36908] Elapsed 46m 35s (remain 72m 48s) Loss: 0.0000(0.0030) Grad: 24.4551  LR: 0.000014  \n","Epoch: [1][14500/36908] Elapsed 46m 54s (remain 72m 29s) Loss: 0.0004(0.0030) Grad: 18379.6680  LR: 0.000013  \n","Epoch: [1][14600/36908] Elapsed 47m 14s (remain 72m 9s) Loss: 0.0002(0.0030) Grad: 12136.3359  LR: 0.000013  \n","Epoch: [1][14700/36908] Elapsed 47m 33s (remain 71m 50s) Loss: 0.0000(0.0030) Grad: 22.4473  LR: 0.000013  \n","Epoch: [1][14800/36908] Elapsed 47m 52s (remain 71m 30s) Loss: 0.0000(0.0029) Grad: 421.4912  LR: 0.000013  \n","Epoch: [1][14900/36908] Elapsed 48m 12s (remain 71m 11s) Loss: 0.0001(0.0029) Grad: 5649.7085  LR: 0.000013  \n","Epoch: [1][15000/36908] Elapsed 48m 31s (remain 70m 52s) Loss: 0.0000(0.0029) Grad: 1819.2820  LR: 0.000013  \n","Epoch: [1][15100/36908] Elapsed 48m 51s (remain 70m 33s) Loss: 0.0000(0.0029) Grad: 43.1322  LR: 0.000013  \n","Epoch: [1][15200/36908] Elapsed 49m 10s (remain 70m 13s) Loss: 0.0001(0.0029) Grad: 2539.1248  LR: 0.000013  \n","Epoch: [1][15300/36908] Elapsed 49m 30s (remain 69m 54s) Loss: 0.0000(0.0029) Grad: 853.5508  LR: 0.000013  \n","Epoch: [1][15400/36908] Elapsed 49m 49s (remain 69m 34s) Loss: 0.0000(0.0029) Grad: 143.0606  LR: 0.000013  \n","Epoch: [1][15500/36908] Elapsed 50m 9s (remain 69m 15s) Loss: 0.0000(0.0029) Grad: 895.3866  LR: 0.000013  \n","Epoch: [1][15600/36908] Elapsed 50m 28s (remain 68m 55s) Loss: 0.0009(0.0028) Grad: 32273.0508  LR: 0.000013  \n","Epoch: [1][15700/36908] Elapsed 50m 47s (remain 68m 36s) Loss: 0.0001(0.0028) Grad: 4331.1074  LR: 0.000013  \n","Epoch: [1][15800/36908] Elapsed 51m 7s (remain 68m 16s) Loss: 0.0000(0.0028) Grad: 332.4009  LR: 0.000013  \n","Epoch: [1][15900/36908] Elapsed 51m 26s (remain 67m 57s) Loss: 0.0012(0.0028) Grad: 103857.0312  LR: 0.000013  \n","Epoch: [1][16000/36908] Elapsed 51m 45s (remain 67m 37s) Loss: 0.0000(0.0028) Grad: 100.4291  LR: 0.000013  \n","Epoch: [1][16100/36908] Elapsed 52m 4s (remain 67m 18s) Loss: 0.0000(0.0028) Grad: 529.8436  LR: 0.000013  \n","Epoch: [1][16200/36908] Elapsed 52m 24s (remain 66m 58s) Loss: 0.0000(0.0028) Grad: 18.5724  LR: 0.000012  \n","Epoch: [1][16300/36908] Elapsed 52m 43s (remain 66m 39s) Loss: 0.0013(0.0028) Grad: 220389.9062  LR: 0.000012  \n","Epoch: [1][16400/36908] Elapsed 53m 2s (remain 66m 19s) Loss: 0.0000(0.0028) Grad: 175.4740  LR: 0.000012  \n","Epoch: [1][16500/36908] Elapsed 53m 22s (remain 66m 0s) Loss: 0.0006(0.0027) Grad: 54885.6562  LR: 0.000012  \n","Epoch: [1][16600/36908] Elapsed 53m 41s (remain 65m 40s) Loss: 0.0000(0.0027) Grad: 1377.7905  LR: 0.000012  \n","Epoch: [1][16700/36908] Elapsed 54m 0s (remain 65m 21s) Loss: 0.0025(0.0027) Grad: 125355.9297  LR: 0.000012  \n","Epoch: [1][16800/36908] Elapsed 54m 20s (remain 65m 1s) Loss: 0.0000(0.0027) Grad: 7.4084  LR: 0.000012  \n","Epoch: [1][16900/36908] Elapsed 54m 39s (remain 64m 42s) Loss: 0.0000(0.0027) Grad: 4.7360  LR: 0.000012  \n","Epoch: [1][17000/36908] Elapsed 54m 59s (remain 64m 22s) Loss: 0.0000(0.0027) Grad: 1932.6356  LR: 0.000012  \n","Epoch: [1][17100/36908] Elapsed 55m 18s (remain 64m 3s) Loss: 0.0016(0.0027) Grad: 51118.6172  LR: 0.000012  \n","Epoch: [1][17200/36908] Elapsed 55m 37s (remain 63m 43s) Loss: 0.0009(0.0027) Grad: 45842.5469  LR: 0.000012  \n","Epoch: [1][17300/36908] Elapsed 55m 57s (remain 63m 24s) Loss: 0.0014(0.0027) Grad: 17775.6191  LR: 0.000012  \n","Epoch: [1][17400/36908] Elapsed 56m 16s (remain 63m 5s) Loss: 0.0000(0.0027) Grad: 25.8038  LR: 0.000012  \n","Epoch: [1][17500/36908] Elapsed 56m 35s (remain 62m 45s) Loss: 0.0000(0.0026) Grad: 25.7560  LR: 0.000012  \n","Epoch: [1][17600/36908] Elapsed 56m 55s (remain 62m 26s) Loss: 0.0000(0.0026) Grad: 13.4574  LR: 0.000012  \n","Epoch: [1][17700/36908] Elapsed 57m 14s (remain 62m 6s) Loss: 0.0000(0.0026) Grad: 54.4515  LR: 0.000012  \n","Epoch: [1][17800/36908] Elapsed 57m 34s (remain 61m 47s) Loss: 0.0000(0.0026) Grad: 79.7164  LR: 0.000012  \n","Epoch: [1][17900/36908] Elapsed 57m 53s (remain 61m 28s) Loss: 0.0000(0.0026) Grad: 19.2547  LR: 0.000011  \n","Epoch: [1][18000/36908] Elapsed 58m 12s (remain 61m 8s) Loss: 0.0005(0.0026) Grad: 22977.2500  LR: 0.000011  \n","Epoch: [1][18100/36908] Elapsed 58m 32s (remain 60m 49s) Loss: 0.0000(0.0026) Grad: 333.6074  LR: 0.000011  \n","Epoch: [1][18200/36908] Elapsed 58m 51s (remain 60m 29s) Loss: 0.0000(0.0026) Grad: 454.1112  LR: 0.000011  \n","Epoch: [1][18300/36908] Elapsed 59m 11s (remain 60m 10s) Loss: 0.0000(0.0026) Grad: 594.9049  LR: 0.000011  \n","Epoch: [1][18400/36908] Elapsed 59m 30s (remain 59m 51s) Loss: 0.0026(0.0026) Grad: 20662.3633  LR: 0.000011  \n","Epoch: [1][18500/36908] Elapsed 59m 49s (remain 59m 31s) Loss: 0.0000(0.0025) Grad: 50.5215  LR: 0.000011  \n","Epoch: [1][18600/36908] Elapsed 60m 9s (remain 59m 12s) Loss: 0.0003(0.0025) Grad: 20596.0801  LR: 0.000011  \n","Epoch: [1][18700/36908] Elapsed 60m 28s (remain 58m 52s) Loss: 0.0012(0.0025) Grad: 27481.8672  LR: 0.000011  \n","Epoch: [1][18800/36908] Elapsed 60m 47s (remain 58m 33s) Loss: 0.0031(0.0025) Grad: 19301.2188  LR: 0.000011  \n","Epoch: [1][18900/36908] Elapsed 61m 7s (remain 58m 13s) Loss: 0.0000(0.0025) Grad: 258.7238  LR: 0.000011  \n","Epoch: [1][19000/36908] Elapsed 61m 26s (remain 57m 54s) Loss: 0.0024(0.0025) Grad: 55634.4648  LR: 0.000011  \n","Epoch: [1][19100/36908] Elapsed 61m 46s (remain 57m 35s) Loss: 0.0000(0.0025) Grad: 40.1035  LR: 0.000011  \n","Epoch: [1][19200/36908] Elapsed 62m 5s (remain 57m 15s) Loss: 0.0000(0.0025) Grad: 208.3566  LR: 0.000011  \n","Epoch: [1][19300/36908] Elapsed 62m 24s (remain 56m 56s) Loss: 0.0054(0.0025) Grad: 50325.3594  LR: 0.000011  \n","Epoch: [1][19400/36908] Elapsed 62m 44s (remain 56m 36s) Loss: 0.0000(0.0025) Grad: 38.8555  LR: 0.000011  \n","Epoch: [1][19500/36908] Elapsed 63m 3s (remain 56m 17s) Loss: 0.0000(0.0025) Grad: 355.0132  LR: 0.000010  \n","Epoch: [1][19600/36908] Elapsed 63m 22s (remain 55m 57s) Loss: 0.0001(0.0025) Grad: 4196.0063  LR: 0.000010  \n","Epoch: [1][19700/36908] Elapsed 63m 42s (remain 55m 38s) Loss: 0.0000(0.0024) Grad: 1237.6141  LR: 0.000010  \n","Epoch: [1][19800/36908] Elapsed 64m 1s (remain 55m 19s) Loss: 0.0000(0.0024) Grad: 1522.8940  LR: 0.000010  \n","Epoch: [1][19900/36908] Elapsed 64m 21s (remain 54m 59s) Loss: 0.0005(0.0024) Grad: 20687.3809  LR: 0.000010  \n","Epoch: [1][20000/36908] Elapsed 64m 40s (remain 54m 40s) Loss: 0.0000(0.0024) Grad: 85.4039  LR: 0.000010  \n","Epoch: [1][20100/36908] Elapsed 64m 59s (remain 54m 20s) Loss: 0.0000(0.0024) Grad: 20.0673  LR: 0.000010  \n","Epoch: [1][20200/36908] Elapsed 65m 19s (remain 54m 1s) Loss: 0.0000(0.0024) Grad: 67.1050  LR: 0.000010  \n","Epoch: [1][20300/36908] Elapsed 65m 38s (remain 53m 42s) Loss: 0.0000(0.0024) Grad: 11.9619  LR: 0.000010  \n","Epoch: [1][20400/36908] Elapsed 65m 58s (remain 53m 22s) Loss: 0.0087(0.0024) Grad: 81117.0938  LR: 0.000010  \n","Epoch: [1][20500/36908] Elapsed 66m 17s (remain 53m 3s) Loss: 0.0000(0.0024) Grad: 54.4644  LR: 0.000010  \n","Epoch: [1][20600/36908] Elapsed 66m 36s (remain 52m 43s) Loss: 0.0004(0.0024) Grad: 7758.1255  LR: 0.000010  \n","Epoch: [1][20700/36908] Elapsed 66m 56s (remain 52m 24s) Loss: 0.0000(0.0024) Grad: 18.3307  LR: 0.000010  \n","Epoch: [1][20800/36908] Elapsed 67m 15s (remain 52m 5s) Loss: 0.0000(0.0024) Grad: 1540.3220  LR: 0.000010  \n","Epoch: [1][20900/36908] Elapsed 67m 35s (remain 51m 45s) Loss: 0.0002(0.0024) Grad: 26170.7715  LR: 0.000010  \n","Epoch: [1][21000/36908] Elapsed 67m 54s (remain 51m 26s) Loss: 0.0000(0.0023) Grad: 1053.6119  LR: 0.000010  \n","Epoch: [1][21100/36908] Elapsed 68m 13s (remain 51m 6s) Loss: 0.0000(0.0023) Grad: 16.5164  LR: 0.000010  \n","Epoch: [1][21200/36908] Elapsed 68m 33s (remain 50m 47s) Loss: 0.0000(0.0023) Grad: 9.9941  LR: 0.000009  \n","Epoch: [1][21300/36908] Elapsed 68m 52s (remain 50m 28s) Loss: 0.0045(0.0023) Grad: 238924.7656  LR: 0.000009  \n","Epoch: [1][21400/36908] Elapsed 69m 12s (remain 50m 8s) Loss: 0.0000(0.0023) Grad: 65.9272  LR: 0.000009  \n","Epoch: [1][21500/36908] Elapsed 69m 31s (remain 49m 49s) Loss: 0.0005(0.0023) Grad: 47679.8555  LR: 0.000009  \n","Epoch: [1][21600/36908] Elapsed 69m 50s (remain 49m 29s) Loss: 0.0000(0.0023) Grad: 62.7340  LR: 0.000009  \n","Epoch: [1][21700/36908] Elapsed 70m 10s (remain 49m 10s) Loss: 0.0000(0.0023) Grad: 11.2411  LR: 0.000009  \n","Epoch: [1][21800/36908] Elapsed 70m 29s (remain 48m 50s) Loss: 0.0197(0.0023) Grad: 249531.2656  LR: 0.000009  \n","Epoch: [1][21900/36908] Elapsed 70m 49s (remain 48m 31s) Loss: 0.0000(0.0023) Grad: 7.1354  LR: 0.000009  \n","Epoch: [1][22000/36908] Elapsed 71m 8s (remain 48m 12s) Loss: 0.0000(0.0023) Grad: 13.3872  LR: 0.000009  \n","Epoch: [1][22100/36908] Elapsed 71m 27s (remain 47m 52s) Loss: 0.0134(0.0023) Grad: 174010.9219  LR: 0.000009  \n","Epoch: [1][22200/36908] Elapsed 71m 47s (remain 47m 33s) Loss: 0.0000(0.0023) Grad: 11.6474  LR: 0.000009  \n","Epoch: [1][22300/36908] Elapsed 72m 6s (remain 47m 13s) Loss: 0.0000(0.0023) Grad: 4143.2954  LR: 0.000009  \n","Epoch: [1][22400/36908] Elapsed 72m 25s (remain 46m 54s) Loss: 0.0005(0.0023) Grad: 62735.0742  LR: 0.000009  \n","Epoch: [1][22500/36908] Elapsed 72m 45s (remain 46m 34s) Loss: 0.0048(0.0023) Grad: 39680.9180  LR: 0.000009  \n","Epoch: [1][22600/36908] Elapsed 73m 4s (remain 46m 15s) Loss: 0.0000(0.0022) Grad: 8.3478  LR: 0.000009  \n","Epoch: [1][22700/36908] Elapsed 73m 23s (remain 45m 55s) Loss: 0.0003(0.0022) Grad: 37809.6367  LR: 0.000009  \n","Epoch: [1][22800/36908] Elapsed 73m 43s (remain 45m 36s) Loss: 0.0000(0.0022) Grad: 2313.3928  LR: 0.000008  \n","Epoch: [1][22900/36908] Elapsed 74m 2s (remain 45m 17s) Loss: 0.0000(0.0022) Grad: 12.9511  LR: 0.000008  \n","Epoch: [1][23000/36908] Elapsed 74m 21s (remain 44m 57s) Loss: 0.0001(0.0022) Grad: 4371.4561  LR: 0.000008  \n","Epoch: [1][23100/36908] Elapsed 74m 41s (remain 44m 38s) Loss: 0.0000(0.0022) Grad: 1501.4083  LR: 0.000008  \n","Epoch: [1][23200/36908] Elapsed 75m 0s (remain 44m 19s) Loss: 0.0010(0.0022) Grad: 188472.0938  LR: 0.000008  \n","Epoch: [1][23300/36908] Elapsed 75m 20s (remain 43m 59s) Loss: 0.0000(0.0022) Grad: 9.8730  LR: 0.000008  \n","Epoch: [1][23400/36908] Elapsed 75m 39s (remain 43m 40s) Loss: 0.0000(0.0022) Grad: 85.8996  LR: 0.000008  \n","Epoch: [1][23500/36908] Elapsed 75m 58s (remain 43m 20s) Loss: 0.0063(0.0022) Grad: 147028.1719  LR: 0.000008  \n","Epoch: [1][23600/36908] Elapsed 76m 18s (remain 43m 1s) Loss: 0.0000(0.0022) Grad: 4.0219  LR: 0.000008  \n","Epoch: [1][23700/36908] Elapsed 76m 37s (remain 42m 42s) Loss: 0.0000(0.0022) Grad: 17.3141  LR: 0.000008  \n","Epoch: [1][23800/36908] Elapsed 76m 57s (remain 42m 22s) Loss: 0.0000(0.0022) Grad: 4390.0312  LR: 0.000008  \n","Epoch: [1][23900/36908] Elapsed 77m 16s (remain 42m 3s) Loss: 0.0000(0.0022) Grad: 4.2524  LR: 0.000008  \n","Epoch: [1][24000/36908] Elapsed 77m 36s (remain 41m 44s) Loss: 0.0000(0.0022) Grad: 4.6015  LR: 0.000008  \n","Epoch: [1][24100/36908] Elapsed 77m 55s (remain 41m 24s) Loss: 0.0041(0.0022) Grad: 67570.6641  LR: 0.000008  \n","Epoch: [1][24200/36908] Elapsed 78m 15s (remain 41m 5s) Loss: 0.0000(0.0022) Grad: 223.8958  LR: 0.000008  \n","Epoch: [1][24300/36908] Elapsed 78m 35s (remain 40m 46s) Loss: 0.0015(0.0022) Grad: 41745.3164  LR: 0.000008  \n","Epoch: [1][24400/36908] Elapsed 78m 54s (remain 40m 26s) Loss: 0.0040(0.0021) Grad: 85938.3281  LR: 0.000008  \n","Epoch: [1][24500/36908] Elapsed 79m 13s (remain 40m 7s) Loss: 0.0000(0.0021) Grad: 1.3906  LR: 0.000007  \n","Epoch: [1][24600/36908] Elapsed 79m 33s (remain 39m 47s) Loss: 0.0000(0.0021) Grad: 71.6945  LR: 0.000007  \n","Epoch: [1][24700/36908] Elapsed 79m 52s (remain 39m 28s) Loss: 0.0000(0.0021) Grad: 417.7760  LR: 0.000007  \n","Epoch: [1][24800/36908] Elapsed 80m 11s (remain 39m 9s) Loss: 0.0000(0.0021) Grad: 312.4348  LR: 0.000007  \n","Epoch: [1][24900/36908] Elapsed 80m 31s (remain 38m 49s) Loss: 0.0012(0.0021) Grad: 30938.2305  LR: 0.000007  \n","Epoch: [1][25000/36908] Elapsed 80m 50s (remain 38m 30s) Loss: 0.0000(0.0021) Grad: 3.5921  LR: 0.000007  \n","Epoch: [1][25100/36908] Elapsed 81m 10s (remain 38m 11s) Loss: 0.0000(0.0021) Grad: 795.2546  LR: 0.000007  \n","Epoch: [1][25200/36908] Elapsed 81m 30s (remain 37m 51s) Loss: 0.0000(0.0021) Grad: 10.0598  LR: 0.000007  \n","Epoch: [1][25300/36908] Elapsed 81m 49s (remain 37m 32s) Loss: 0.0011(0.0021) Grad: 26042.9551  LR: 0.000007  \n","Epoch: [1][25400/36908] Elapsed 82m 9s (remain 37m 13s) Loss: 0.0000(0.0021) Grad: 27.6023  LR: 0.000007  \n","Epoch: [1][25500/36908] Elapsed 82m 29s (remain 36m 53s) Loss: 0.0000(0.0021) Grad: 18.0311  LR: 0.000007  \n","Epoch: [1][25600/36908] Elapsed 82m 48s (remain 36m 34s) Loss: 0.0000(0.0021) Grad: 23.9718  LR: 0.000007  \n","Epoch: [1][25700/36908] Elapsed 83m 8s (remain 36m 15s) Loss: 0.0000(0.0021) Grad: 20.2341  LR: 0.000007  \n","Epoch: [1][25800/36908] Elapsed 83m 27s (remain 35m 55s) Loss: 0.0072(0.0021) Grad: 31968.9277  LR: 0.000007  \n","Epoch: [1][25900/36908] Elapsed 83m 47s (remain 35m 36s) Loss: 0.0000(0.0021) Grad: 15.1717  LR: 0.000007  \n","Epoch: [1][26000/36908] Elapsed 84m 7s (remain 35m 17s) Loss: 0.0001(0.0021) Grad: 3311.1343  LR: 0.000007  \n","Epoch: [1][26100/36908] Elapsed 84m 27s (remain 34m 57s) Loss: 0.0000(0.0021) Grad: 10.6346  LR: 0.000007  \n","Epoch: [1][26200/36908] Elapsed 84m 46s (remain 34m 38s) Loss: 0.0013(0.0021) Grad: 31804.1621  LR: 0.000006  \n","Epoch: [1][26300/36908] Elapsed 85m 6s (remain 34m 19s) Loss: 0.0000(0.0021) Grad: 651.6796  LR: 0.000006  \n","Epoch: [1][26400/36908] Elapsed 85m 25s (remain 33m 59s) Loss: 0.0034(0.0021) Grad: 69995.3750  LR: 0.000006  \n","Epoch: [1][26500/36908] Elapsed 85m 45s (remain 33m 40s) Loss: 0.0000(0.0020) Grad: 31.9297  LR: 0.000006  \n","Epoch: [1][26600/36908] Elapsed 86m 5s (remain 33m 21s) Loss: 0.0000(0.0020) Grad: 6.1917  LR: 0.000006  \n","Epoch: [1][26700/36908] Elapsed 86m 24s (remain 33m 2s) Loss: 0.0315(0.0020) Grad: 140270.5312  LR: 0.000006  \n","Epoch: [1][26800/36908] Elapsed 86m 44s (remain 32m 42s) Loss: 0.0027(0.0020) Grad: 35371.4883  LR: 0.000006  \n","Epoch: [1][26900/36908] Elapsed 87m 4s (remain 32m 23s) Loss: 0.0000(0.0020) Grad: 2922.3967  LR: 0.000006  \n","Epoch: [1][27000/36908] Elapsed 87m 23s (remain 32m 3s) Loss: 0.0000(0.0020) Grad: 11.5926  LR: 0.000006  \n","Epoch: [1][27100/36908] Elapsed 87m 43s (remain 31m 44s) Loss: 0.0000(0.0020) Grad: 282.9923  LR: 0.000006  \n","Epoch: [1][27200/36908] Elapsed 88m 3s (remain 31m 25s) Loss: 0.0000(0.0020) Grad: 17.3306  LR: 0.000006  \n","Epoch: [1][27300/36908] Elapsed 88m 22s (remain 31m 6s) Loss: 0.0005(0.0020) Grad: 9277.7930  LR: 0.000006  \n","Epoch: [1][27400/36908] Elapsed 88m 42s (remain 30m 46s) Loss: 0.0024(0.0020) Grad: 17568.2695  LR: 0.000006  \n","Epoch: [1][27500/36908] Elapsed 89m 2s (remain 30m 27s) Loss: 0.0002(0.0020) Grad: 6098.9082  LR: 0.000006  \n","Epoch: [1][27600/36908] Elapsed 89m 21s (remain 30m 7s) Loss: 0.0077(0.0020) Grad: 86290.0703  LR: 0.000006  \n","Epoch: [1][27700/36908] Elapsed 89m 41s (remain 29m 48s) Loss: 0.0001(0.0020) Grad: 9423.3193  LR: 0.000006  \n","Epoch: [1][27800/36908] Elapsed 90m 0s (remain 29m 29s) Loss: 0.0000(0.0020) Grad: 20.0627  LR: 0.000005  \n","Epoch: [1][27900/36908] Elapsed 90m 20s (remain 29m 9s) Loss: 0.0000(0.0020) Grad: 110.8326  LR: 0.000005  \n","Epoch: [1][28000/36908] Elapsed 90m 40s (remain 28m 50s) Loss: 0.0000(0.0020) Grad: 9.2286  LR: 0.000005  \n","Epoch: [1][28100/36908] Elapsed 91m 0s (remain 28m 31s) Loss: 0.0000(0.0020) Grad: 17.4055  LR: 0.000005  \n","Epoch: [1][28200/36908] Elapsed 91m 19s (remain 28m 11s) Loss: 0.0007(0.0020) Grad: 16209.0420  LR: 0.000005  \n","Epoch: [1][28300/36908] Elapsed 91m 39s (remain 27m 52s) Loss: 0.0000(0.0020) Grad: 1827.1559  LR: 0.000005  \n","Epoch: [1][28400/36908] Elapsed 91m 59s (remain 27m 33s) Loss: 0.0001(0.0020) Grad: 13263.1240  LR: 0.000005  \n","Epoch: [1][28500/36908] Elapsed 92m 18s (remain 27m 13s) Loss: 0.0000(0.0020) Grad: 1398.1813  LR: 0.000005  \n","Epoch: [1][28600/36908] Elapsed 92m 38s (remain 26m 54s) Loss: 0.0030(0.0020) Grad: 98486.8203  LR: 0.000005  \n","Epoch: [1][28700/36908] Elapsed 92m 58s (remain 26m 35s) Loss: 0.0000(0.0020) Grad: 67.0142  LR: 0.000005  \n","Epoch: [1][28800/36908] Elapsed 93m 17s (remain 26m 15s) Loss: 0.0000(0.0020) Grad: 9.9127  LR: 0.000005  \n","Epoch: [1][28900/36908] Elapsed 93m 37s (remain 25m 56s) Loss: 0.0040(0.0020) Grad: 35149.1641  LR: 0.000005  \n","Epoch: [1][29000/36908] Elapsed 93m 57s (remain 25m 36s) Loss: 0.0000(0.0019) Grad: 3.9903  LR: 0.000005  \n","Epoch: [1][29100/36908] Elapsed 94m 16s (remain 25m 17s) Loss: 0.0009(0.0019) Grad: 15565.8682  LR: 0.000005  \n","Epoch: [1][29200/36908] Elapsed 94m 36s (remain 24m 58s) Loss: 0.0001(0.0019) Grad: 3475.5527  LR: 0.000005  \n","Epoch: [1][29300/36908] Elapsed 94m 56s (remain 24m 38s) Loss: 0.0000(0.0019) Grad: 218.1846  LR: 0.000005  \n","Epoch: [1][29400/36908] Elapsed 95m 15s (remain 24m 19s) Loss: 0.0000(0.0019) Grad: 4.8342  LR: 0.000005  \n","Epoch: [1][29500/36908] Elapsed 95m 35s (remain 24m 0s) Loss: 0.0010(0.0019) Grad: 26280.5938  LR: 0.000004  \n","Epoch: [1][29600/36908] Elapsed 95m 55s (remain 23m 40s) Loss: 0.0066(0.0019) Grad: 121305.4766  LR: 0.000004  \n","Epoch: [1][29700/36908] Elapsed 96m 15s (remain 23m 21s) Loss: 0.0000(0.0019) Grad: 84.3095  LR: 0.000004  \n","Epoch: [1][29800/36908] Elapsed 96m 35s (remain 23m 2s) Loss: 0.0000(0.0019) Grad: 15.0585  LR: 0.000004  \n","Epoch: [1][29900/36908] Elapsed 96m 54s (remain 22m 42s) Loss: 0.0004(0.0019) Grad: 13968.3154  LR: 0.000004  \n","Epoch: [1][30000/36908] Elapsed 97m 14s (remain 22m 23s) Loss: 0.0000(0.0019) Grad: 1354.3513  LR: 0.000004  \n","Epoch: [1][30100/36908] Elapsed 97m 34s (remain 22m 3s) Loss: 0.0030(0.0019) Grad: 41079.2969  LR: 0.000004  \n","Epoch: [1][30200/36908] Elapsed 97m 53s (remain 21m 44s) Loss: 0.0032(0.0019) Grad: 27753.7031  LR: 0.000004  \n","Epoch: [1][30300/36908] Elapsed 98m 13s (remain 21m 25s) Loss: 0.0000(0.0019) Grad: 13.2415  LR: 0.000004  \n","Epoch: [1][30400/36908] Elapsed 98m 33s (remain 21m 5s) Loss: 0.0000(0.0019) Grad: 174.1475  LR: 0.000004  \n","Epoch: [1][30500/36908] Elapsed 98m 52s (remain 20m 46s) Loss: 0.0000(0.0019) Grad: 7.2392  LR: 0.000004  \n","Epoch: [1][30600/36908] Elapsed 99m 12s (remain 20m 26s) Loss: 0.0000(0.0019) Grad: 143.5364  LR: 0.000004  \n","Epoch: [1][30700/36908] Elapsed 99m 31s (remain 20m 7s) Loss: 0.0000(0.0019) Grad: 32.9546  LR: 0.000004  \n","Epoch: [1][30800/36908] Elapsed 99m 51s (remain 19m 47s) Loss: 0.0000(0.0019) Grad: 643.8336  LR: 0.000004  \n","Epoch: [1][30900/36908] Elapsed 100m 11s (remain 19m 28s) Loss: 0.0000(0.0019) Grad: 3737.5361  LR: 0.000004  \n","Epoch: [1][31000/36908] Elapsed 100m 30s (remain 19m 9s) Loss: 0.0000(0.0019) Grad: 641.4296  LR: 0.000004  \n","Epoch: [1][31100/36908] Elapsed 100m 50s (remain 18m 49s) Loss: 0.0000(0.0019) Grad: 7.1393  LR: 0.000003  \n","Epoch: [1][31200/36908] Elapsed 101m 10s (remain 18m 30s) Loss: 0.0018(0.0019) Grad: 18346.8047  LR: 0.000003  \n","Epoch: [1][31300/36908] Elapsed 101m 29s (remain 18m 10s) Loss: 0.0000(0.0019) Grad: 50.0751  LR: 0.000003  \n","Epoch: [1][31400/36908] Elapsed 101m 49s (remain 17m 51s) Loss: 0.0000(0.0019) Grad: 13.7572  LR: 0.000003  \n","Epoch: [1][31500/36908] Elapsed 102m 9s (remain 17m 32s) Loss: 0.0001(0.0019) Grad: 1423.6853  LR: 0.000003  \n","Epoch: [1][31600/36908] Elapsed 102m 29s (remain 17m 12s) Loss: 0.0000(0.0019) Grad: 19.4689  LR: 0.000003  \n","Epoch: [1][31700/36908] Elapsed 102m 49s (remain 16m 53s) Loss: 0.0000(0.0019) Grad: 10.7780  LR: 0.000003  \n","Epoch: [1][31800/36908] Elapsed 103m 8s (remain 16m 33s) Loss: 0.0000(0.0019) Grad: 151.2523  LR: 0.000003  \n","Epoch: [1][31900/36908] Elapsed 103m 28s (remain 16m 14s) Loss: 0.0000(0.0018) Grad: 875.9650  LR: 0.000003  \n","Epoch: [1][32000/36908] Elapsed 103m 47s (remain 15m 54s) Loss: 0.0000(0.0018) Grad: 12.3437  LR: 0.000003  \n","Epoch: [1][32100/36908] Elapsed 104m 7s (remain 15m 35s) Loss: 0.0000(0.0018) Grad: 2.6468  LR: 0.000003  \n","Epoch: [1][32200/36908] Elapsed 104m 27s (remain 15m 16s) Loss: 0.0000(0.0018) Grad: 77.3357  LR: 0.000003  \n","Epoch: [1][32300/36908] Elapsed 104m 46s (remain 14m 56s) Loss: 0.0013(0.0018) Grad: 29312.2812  LR: 0.000003  \n","Epoch: [1][32400/36908] Elapsed 105m 6s (remain 14m 37s) Loss: 0.0017(0.0018) Grad: 29467.8203  LR: 0.000003  \n","Epoch: [1][32500/36908] Elapsed 105m 26s (remain 14m 17s) Loss: 0.0000(0.0018) Grad: 7.3277  LR: 0.000003  \n","Epoch: [1][32600/36908] Elapsed 105m 45s (remain 13m 58s) Loss: 0.0000(0.0018) Grad: 2.2131  LR: 0.000003  \n","Epoch: [1][32700/36908] Elapsed 106m 5s (remain 13m 38s) Loss: 0.0057(0.0018) Grad: 12886.0352  LR: 0.000003  \n","Epoch: [1][32800/36908] Elapsed 106m 24s (remain 13m 19s) Loss: 0.0000(0.0018) Grad: 4.1882  LR: 0.000002  \n","Epoch: [1][32900/36908] Elapsed 106m 44s (remain 12m 59s) Loss: 0.0000(0.0018) Grad: 39.2871  LR: 0.000002  \n","Epoch: [1][33000/36908] Elapsed 107m 4s (remain 12m 40s) Loss: 0.0009(0.0018) Grad: 6235.0483  LR: 0.000002  \n","Epoch: [1][33100/36908] Elapsed 107m 23s (remain 12m 21s) Loss: 0.0000(0.0018) Grad: 92.3280  LR: 0.000002  \n","Epoch: [1][33200/36908] Elapsed 107m 43s (remain 12m 1s) Loss: 0.0015(0.0018) Grad: 8856.0723  LR: 0.000002  \n","Epoch: [1][33300/36908] Elapsed 108m 3s (remain 11m 42s) Loss: 0.0002(0.0018) Grad: 4423.3926  LR: 0.000002  \n","Epoch: [1][33400/36908] Elapsed 108m 22s (remain 11m 22s) Loss: 0.0000(0.0018) Grad: 16.9131  LR: 0.000002  \n","Epoch: [1][33500/36908] Elapsed 108m 42s (remain 11m 3s) Loss: 0.0001(0.0018) Grad: 1827.0077  LR: 0.000002  \n","Epoch: [1][33600/36908] Elapsed 109m 2s (remain 10m 43s) Loss: 0.0003(0.0018) Grad: 2699.7144  LR: 0.000002  \n","Epoch: [1][33700/36908] Elapsed 109m 22s (remain 10m 24s) Loss: 0.0000(0.0018) Grad: 1131.2458  LR: 0.000002  \n","Epoch: [1][33800/36908] Elapsed 109m 41s (remain 10m 4s) Loss: 0.0000(0.0018) Grad: 1119.6794  LR: 0.000002  \n","Epoch: [1][33900/36908] Elapsed 110m 1s (remain 9m 45s) Loss: 0.0000(0.0018) Grad: 133.0364  LR: 0.000002  \n","Epoch: [1][34000/36908] Elapsed 110m 21s (remain 9m 26s) Loss: 0.0000(0.0018) Grad: 10.3241  LR: 0.000002  \n","Epoch: [1][34100/36908] Elapsed 110m 40s (remain 9m 6s) Loss: 0.0055(0.0018) Grad: 159436.0625  LR: 0.000002  \n","Epoch: [1][34200/36908] Elapsed 111m 0s (remain 8m 47s) Loss: 0.0000(0.0018) Grad: 16.8452  LR: 0.000002  \n","Epoch: [1][34300/36908] Elapsed 111m 20s (remain 8m 27s) Loss: 0.0000(0.0018) Grad: 43.4677  LR: 0.000002  \n","Epoch: [1][34400/36908] Elapsed 111m 40s (remain 8m 8s) Loss: 0.0000(0.0018) Grad: 4.9367  LR: 0.000002  \n","Epoch: [1][34500/36908] Elapsed 111m 59s (remain 7m 48s) Loss: 0.0000(0.0018) Grad: 83.6060  LR: 0.000001  \n","Epoch: [1][34600/36908] Elapsed 112m 19s (remain 7m 29s) Loss: 0.0000(0.0018) Grad: 32.2819  LR: 0.000001  \n","Epoch: [1][34700/36908] Elapsed 112m 38s (remain 7m 9s) Loss: 0.0000(0.0018) Grad: 337.9126  LR: 0.000001  \n","Epoch: [1][34800/36908] Elapsed 112m 58s (remain 6m 50s) Loss: 0.0008(0.0018) Grad: 9143.6787  LR: 0.000001  \n","Epoch: [1][34900/36908] Elapsed 113m 18s (remain 6m 30s) Loss: 0.0009(0.0018) Grad: 6090.7476  LR: 0.000001  \n","Epoch: [1][35000/36908] Elapsed 113m 38s (remain 6m 11s) Loss: 0.0000(0.0018) Grad: 45.8751  LR: 0.000001  \n","Epoch: [1][35100/36908] Elapsed 113m 58s (remain 5m 52s) Loss: 0.0000(0.0017) Grad: 132.9472  LR: 0.000001  \n","Epoch: [1][35200/36908] Elapsed 114m 17s (remain 5m 32s) Loss: 0.0000(0.0017) Grad: 5.5613  LR: 0.000001  \n","Epoch: [1][35300/36908] Elapsed 114m 37s (remain 5m 13s) Loss: 0.0008(0.0017) Grad: 31430.1191  LR: 0.000001  \n","Epoch: [1][35400/36908] Elapsed 114m 57s (remain 4m 53s) Loss: 0.0000(0.0017) Grad: 181.9846  LR: 0.000001  \n","Epoch: [1][35500/36908] Elapsed 115m 16s (remain 4m 34s) Loss: 0.0000(0.0017) Grad: 234.1971  LR: 0.000001  \n","Epoch: [1][35600/36908] Elapsed 115m 36s (remain 4m 14s) Loss: 0.0022(0.0017) Grad: 15791.8076  LR: 0.000001  \n","Epoch: [1][35700/36908] Elapsed 115m 56s (remain 3m 55s) Loss: 0.0044(0.0017) Grad: 27355.0391  LR: 0.000001  \n","Epoch: [1][35800/36908] Elapsed 116m 15s (remain 3m 35s) Loss: 0.0001(0.0017) Grad: 7724.0923  LR: 0.000001  \n","Epoch: [1][35900/36908] Elapsed 116m 35s (remain 3m 16s) Loss: 0.0010(0.0017) Grad: 17812.9863  LR: 0.000001  \n","Epoch: [1][36000/36908] Elapsed 116m 54s (remain 2m 56s) Loss: 0.0009(0.0017) Grad: 101989.1641  LR: 0.000001  \n","Epoch: [1][36100/36908] Elapsed 117m 14s (remain 2m 37s) Loss: 0.0000(0.0017) Grad: 679.1921  LR: 0.000000  \n","Epoch: [1][36200/36908] Elapsed 117m 34s (remain 2m 17s) Loss: 0.0000(0.0017) Grad: 22.2316  LR: 0.000000  \n","Epoch: [1][36300/36908] Elapsed 117m 53s (remain 1m 58s) Loss: 0.0012(0.0017) Grad: 16546.9922  LR: 0.000000  \n","Epoch: [1][36400/36908] Elapsed 118m 13s (remain 1m 38s) Loss: 0.0000(0.0017) Grad: 352.1711  LR: 0.000000  \n","Epoch: [1][36500/36908] Elapsed 118m 32s (remain 1m 19s) Loss: 0.0000(0.0017) Grad: 440.4125  LR: 0.000000  \n","Epoch: [1][36600/36908] Elapsed 118m 52s (remain 0m 59s) Loss: 0.0000(0.0017) Grad: 156.5706  LR: 0.000000  \n","Epoch: [1][36700/36908] Elapsed 119m 12s (remain 0m 40s) Loss: 0.0010(0.0017) Grad: 24177.1367  LR: 0.000000  \n","Epoch: [1][36800/36908] Elapsed 119m 31s (remain 0m 20s) Loss: 0.0000(0.0017) Grad: 96.2982  LR: 0.000000  \n","Epoch: [1][36900/36908] Elapsed 119m 51s (remain 0m 1s) Loss: 0.0000(0.0017) Grad: 24.6329  LR: 0.000000  \n","Epoch: [1][36907/36908] Elapsed 119m 52s (remain 0m 0s) Loss: 0.0000(0.0017) Grad: 1000.1906  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 7m 21s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 6s (remain 1m 7s) Loss: 0.0047(0.0025) \n","EVAL: [200/1192] Elapsed 0m 12s (remain 1m 0s) Loss: 0.0038(0.0028) \n","EVAL: [300/1192] Elapsed 0m 18s (remain 0m 53s) Loss: 0.0018(0.0031) \n","EVAL: [400/1192] Elapsed 0m 24s (remain 0m 47s) Loss: 0.0064(0.0033) \n","EVAL: [500/1192] Elapsed 0m 30s (remain 0m 41s) Loss: 0.0046(0.0031) \n","EVAL: [600/1192] Elapsed 0m 36s (remain 0m 35s) Loss: 0.0000(0.0033) \n","EVAL: [700/1192] Elapsed 0m 42s (remain 0m 29s) Loss: 0.0511(0.0039) \n","EVAL: [800/1192] Elapsed 0m 48s (remain 0m 23s) Loss: 0.0029(0.0041) \n","EVAL: [900/1192] Elapsed 0m 54s (remain 0m 17s) Loss: 0.0001(0.0042) \n","EVAL: [1000/1192] Elapsed 1m 0s (remain 0m 11s) Loss: 0.0000(0.0041) \n","EVAL: [1100/1192] Elapsed 1m 6s (remain 0m 5s) Loss: 0.0012(0.0039) \n","EVAL: [1191/1192] Elapsed 1m 12s (remain 0m 0s) Loss: 0.0000(0.0038) \n","Epoch 1 - avg_train_loss: 0.0017  avg_val_loss: 0.0038  time: 7272s\n","Epoch 1 - Score: 0.8838\n","Epoch 1 - Save Best Score: 0.8838 Model\n","========== fold: 1 training ==========\n","get pseudo plain from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n","get pseudo labels from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_1.npy\n","get pseudo labels from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_1.npy\n","get pseudo labels from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp083/pseudo_labels_1.npy\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/612602 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8d5efc2fac34d5fbb266fd0ea6a4442"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/612602 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd26dec55ba54cbea581e2bc6bb9896e"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["(612602, 8)\n","(100000, 9)\n","(110725, 11)\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\n","Epoch: [1][0/36908] Elapsed 0m 0s (remain 369m 20s) Loss: 0.1152(0.1152) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/36908] Elapsed 0m 20s (remain 121m 51s) Loss: 0.0775(0.1019) Grad: 77228.5391  LR: 0.000001  \n","Epoch: [1][200/36908] Elapsed 0m 39s (remain 120m 9s) Loss: 0.0330(0.0783) Grad: 28226.5449  LR: 0.000001  \n","Epoch: [1][300/36908] Elapsed 0m 58s (remain 119m 24s) Loss: 0.0090(0.0580) Grad: 4272.1528  LR: 0.000002  \n","Epoch: [1][400/36908] Elapsed 1m 18s (remain 119m 5s) Loss: 0.0177(0.0469) Grad: 7709.6890  LR: 0.000002  \n","Epoch: [1][500/36908] Elapsed 1m 37s (remain 118m 40s) Loss: 0.0167(0.0400) Grad: 5897.0513  LR: 0.000003  \n","Epoch: [1][600/36908] Elapsed 1m 57s (remain 118m 12s) Loss: 0.0114(0.0354) Grad: 4099.9263  LR: 0.000003  \n","Epoch: [1][700/36908] Elapsed 2m 16s (remain 117m 50s) Loss: 0.0255(0.0320) Grad: 12014.2061  LR: 0.000004  \n","Epoch: [1][800/36908] Elapsed 2m 36s (remain 117m 29s) Loss: 0.0072(0.0295) Grad: 3087.1743  LR: 0.000004  \n","Epoch: [1][900/36908] Elapsed 2m 55s (remain 117m 9s) Loss: 0.0237(0.0274) Grad: 10660.2510  LR: 0.000005  \n","Epoch: [1][1000/36908] Elapsed 3m 15s (remain 116m 48s) Loss: 0.0040(0.0257) Grad: 4306.1909  LR: 0.000005  \n","Epoch: [1][1100/36908] Elapsed 3m 34s (remain 116m 25s) Loss: 0.0101(0.0241) Grad: 6623.1284  LR: 0.000006  \n","Epoch: [1][1200/36908] Elapsed 3m 54s (remain 116m 7s) Loss: 0.0019(0.0225) Grad: 3006.8528  LR: 0.000007  \n","Epoch: [1][1300/36908] Elapsed 4m 13s (remain 115m 46s) Loss: 0.0098(0.0211) Grad: 32495.8281  LR: 0.000007  \n","Epoch: [1][1400/36908] Elapsed 4m 33s (remain 115m 26s) Loss: 0.0123(0.0199) Grad: 15834.9111  LR: 0.000008  \n","Epoch: [1][1500/36908] Elapsed 4m 52s (remain 115m 4s) Loss: 0.0011(0.0188) Grad: 1621.3639  LR: 0.000008  \n","Epoch: [1][1600/36908] Elapsed 5m 12s (remain 114m 45s) Loss: 0.0008(0.0178) Grad: 3005.8875  LR: 0.000009  \n","Epoch: [1][1700/36908] Elapsed 5m 31s (remain 114m 25s) Loss: 0.0030(0.0170) Grad: 6114.9668  LR: 0.000009  \n","Epoch: [1][1800/36908] Elapsed 5m 51s (remain 114m 6s) Loss: 0.0001(0.0162) Grad: 187.3572  LR: 0.000010  \n","Epoch: [1][1900/36908] Elapsed 6m 10s (remain 113m 46s) Loss: 0.0010(0.0155) Grad: 2587.2065  LR: 0.000010  \n","Epoch: [1][2000/36908] Elapsed 6m 30s (remain 113m 27s) Loss: 0.0001(0.0148) Grad: 344.8317  LR: 0.000011  \n","Epoch: [1][2100/36908] Elapsed 6m 49s (remain 113m 8s) Loss: 0.0000(0.0142) Grad: 56.6019  LR: 0.000011  \n","Epoch: [1][2200/36908] Elapsed 7m 9s (remain 112m 48s) Loss: 0.0000(0.0136) Grad: 74.0640  LR: 0.000012  \n","Epoch: [1][2300/36908] Elapsed 7m 28s (remain 112m 29s) Loss: 0.0007(0.0132) Grad: 3229.2795  LR: 0.000012  \n","Epoch: [1][2400/36908] Elapsed 7m 48s (remain 112m 9s) Loss: 0.0005(0.0127) Grad: 1567.5381  LR: 0.000013  \n","Epoch: [1][2500/36908] Elapsed 8m 7s (remain 111m 49s) Loss: 0.0020(0.0123) Grad: 6568.8921  LR: 0.000014  \n","Epoch: [1][2600/36908] Elapsed 8m 27s (remain 111m 30s) Loss: 0.0006(0.0119) Grad: 2203.9209  LR: 0.000014  \n","Epoch: [1][2700/36908] Elapsed 8m 46s (remain 111m 10s) Loss: 0.0005(0.0115) Grad: 1406.6700  LR: 0.000015  \n","Epoch: [1][2800/36908] Elapsed 9m 6s (remain 110m 51s) Loss: 0.0002(0.0112) Grad: 3139.4397  LR: 0.000015  \n","Epoch: [1][2900/36908] Elapsed 9m 25s (remain 110m 31s) Loss: 0.0007(0.0108) Grad: 2637.2043  LR: 0.000016  \n","Epoch: [1][3000/36908] Elapsed 9m 45s (remain 110m 11s) Loss: 0.0008(0.0105) Grad: 1837.8319  LR: 0.000016  \n","Epoch: [1][3100/36908] Elapsed 10m 4s (remain 109m 51s) Loss: 0.0017(0.0103) Grad: 5915.9102  LR: 0.000017  \n","Epoch: [1][3200/36908] Elapsed 10m 24s (remain 109m 31s) Loss: 0.0000(0.0100) Grad: 118.4651  LR: 0.000017  \n","Epoch: [1][3300/36908] Elapsed 10m 43s (remain 109m 12s) Loss: 0.0013(0.0098) Grad: 3675.2466  LR: 0.000018  \n","Epoch: [1][3400/36908] Elapsed 11m 3s (remain 108m 54s) Loss: 0.0010(0.0096) Grad: 1871.7242  LR: 0.000018  \n","Epoch: [1][3500/36908] Elapsed 11m 23s (remain 108m 37s) Loss: 0.0002(0.0093) Grad: 441.4587  LR: 0.000019  \n","Epoch: [1][3600/36908] Elapsed 11m 42s (remain 108m 17s) Loss: 0.0000(0.0091) Grad: 39.0757  LR: 0.000020  \n","Epoch: [1][3700/36908] Elapsed 12m 1s (remain 107m 57s) Loss: 0.0026(0.0089) Grad: 4450.1172  LR: 0.000020  \n","Epoch: [1][3800/36908] Elapsed 12m 21s (remain 107m 38s) Loss: 0.0013(0.0087) Grad: 8468.3184  LR: 0.000020  \n","Epoch: [1][3900/36908] Elapsed 12m 41s (remain 107m 19s) Loss: 0.0027(0.0086) Grad: 9109.7305  LR: 0.000020  \n","Epoch: [1][4000/36908] Elapsed 13m 0s (remain 107m 0s) Loss: 0.0113(0.0084) Grad: 7600.0791  LR: 0.000020  \n","Epoch: [1][4100/36908] Elapsed 13m 20s (remain 106m 41s) Loss: 0.0009(0.0082) Grad: 3643.1760  LR: 0.000020  \n","Epoch: [1][4200/36908] Elapsed 13m 39s (remain 106m 21s) Loss: 0.0041(0.0081) Grad: 12667.6328  LR: 0.000020  \n","Epoch: [1][4300/36908] Elapsed 13m 59s (remain 106m 1s) Loss: 0.0000(0.0079) Grad: 162.8156  LR: 0.000020  \n","Epoch: [1][4400/36908] Elapsed 14m 18s (remain 105m 42s) Loss: 0.0001(0.0078) Grad: 353.4439  LR: 0.000020  \n","Epoch: [1][4500/36908] Elapsed 14m 38s (remain 105m 22s) Loss: 0.0000(0.0076) Grad: 17.4632  LR: 0.000020  \n","Epoch: [1][4600/36908] Elapsed 14m 57s (remain 105m 2s) Loss: 0.0003(0.0075) Grad: 2194.3372  LR: 0.000019  \n","Epoch: [1][4700/36908] Elapsed 15m 17s (remain 104m 47s) Loss: 0.0000(0.0074) Grad: 28.5369  LR: 0.000019  \n","Epoch: [1][4800/36908] Elapsed 15m 37s (remain 104m 28s) Loss: 0.0000(0.0073) Grad: 96.1529  LR: 0.000019  \n","Epoch: [1][4900/36908] Elapsed 15m 56s (remain 104m 8s) Loss: 0.0006(0.0071) Grad: 2023.6670  LR: 0.000019  \n","Epoch: [1][5000/36908] Elapsed 16m 16s (remain 103m 49s) Loss: 0.0016(0.0070) Grad: 6612.7944  LR: 0.000019  \n","Epoch: [1][5100/36908] Elapsed 16m 35s (remain 103m 29s) Loss: 0.0000(0.0069) Grad: 239.0331  LR: 0.000019  \n","Epoch: [1][5200/36908] Elapsed 16m 55s (remain 103m 9s) Loss: 0.0005(0.0068) Grad: 4717.8882  LR: 0.000019  \n","Epoch: [1][5300/36908] Elapsed 17m 14s (remain 102m 50s) Loss: 0.0000(0.0067) Grad: 52.9894  LR: 0.000019  \n","Epoch: [1][5400/36908] Elapsed 17m 34s (remain 102m 31s) Loss: 0.0244(0.0066) Grad: 44501.8438  LR: 0.000019  \n","Epoch: [1][5500/36908] Elapsed 17m 54s (remain 102m 12s) Loss: 0.0017(0.0065) Grad: 12931.5078  LR: 0.000019  \n","Epoch: [1][5600/36908] Elapsed 18m 13s (remain 101m 52s) Loss: 0.0000(0.0064) Grad: 53.0827  LR: 0.000019  \n","Epoch: [1][5700/36908] Elapsed 18m 33s (remain 101m 33s) Loss: 0.0000(0.0063) Grad: 33.3173  LR: 0.000019  \n","Epoch: [1][5800/36908] Elapsed 18m 52s (remain 101m 14s) Loss: 0.0001(0.0063) Grad: 520.8764  LR: 0.000019  \n","Epoch: [1][5900/36908] Elapsed 19m 12s (remain 100m 55s) Loss: 0.0004(0.0062) Grad: 1006.9874  LR: 0.000019  \n","Epoch: [1][6000/36908] Elapsed 19m 31s (remain 100m 35s) Loss: 0.0012(0.0061) Grad: 10261.6562  LR: 0.000019  \n","Epoch: [1][6100/36908] Elapsed 19m 51s (remain 100m 16s) Loss: 0.0003(0.0060) Grad: 1878.3722  LR: 0.000019  \n","Epoch: [1][6200/36908] Elapsed 20m 11s (remain 99m 57s) Loss: 0.0009(0.0059) Grad: 5064.6943  LR: 0.000018  \n","Epoch: [1][6300/36908] Elapsed 20m 30s (remain 99m 37s) Loss: 0.0007(0.0059) Grad: 10553.3301  LR: 0.000018  \n","Epoch: [1][6400/36908] Elapsed 20m 50s (remain 99m 18s) Loss: 0.0013(0.0058) Grad: 27204.6562  LR: 0.000018  \n","Epoch: [1][6500/36908] Elapsed 21m 9s (remain 98m 59s) Loss: 0.0001(0.0057) Grad: 664.7542  LR: 0.000018  \n","Epoch: [1][6600/36908] Elapsed 21m 29s (remain 98m 39s) Loss: 0.0000(0.0057) Grad: 75.6544  LR: 0.000018  \n","Epoch: [1][6700/36908] Elapsed 21m 48s (remain 98m 20s) Loss: 0.0024(0.0056) Grad: 5064.0806  LR: 0.000018  \n","Epoch: [1][6800/36908] Elapsed 22m 8s (remain 98m 1s) Loss: 0.0003(0.0055) Grad: 1835.7146  LR: 0.000018  \n","Epoch: [1][6900/36908] Elapsed 22m 28s (remain 97m 43s) Loss: 0.0001(0.0055) Grad: 3112.0247  LR: 0.000018  \n","Epoch: [1][7000/36908] Elapsed 22m 48s (remain 97m 23s) Loss: 0.0001(0.0054) Grad: 1024.8754  LR: 0.000018  \n","Epoch: [1][7100/36908] Elapsed 23m 7s (remain 97m 4s) Loss: 0.0000(0.0054) Grad: 56.5196  LR: 0.000018  \n","Epoch: [1][7200/36908] Elapsed 23m 27s (remain 96m 44s) Loss: 0.0000(0.0053) Grad: 173.8793  LR: 0.000018  \n","Epoch: [1][7300/36908] Elapsed 23m 46s (remain 96m 25s) Loss: 0.0017(0.0053) Grad: 5712.2324  LR: 0.000018  \n","Epoch: [1][7400/36908] Elapsed 24m 6s (remain 96m 5s) Loss: 0.0001(0.0052) Grad: 1377.6959  LR: 0.000018  \n","Epoch: [1][7500/36908] Elapsed 24m 25s (remain 95m 46s) Loss: 0.0005(0.0052) Grad: 2534.8730  LR: 0.000018  \n","Epoch: [1][7600/36908] Elapsed 24m 45s (remain 95m 26s) Loss: 0.0001(0.0051) Grad: 304.7662  LR: 0.000018  \n","Epoch: [1][7700/36908] Elapsed 25m 4s (remain 95m 7s) Loss: 0.0009(0.0051) Grad: 14474.8027  LR: 0.000018  \n","Epoch: [1][7800/36908] Elapsed 25m 24s (remain 94m 47s) Loss: 0.0000(0.0050) Grad: 56.0322  LR: 0.000018  \n","Epoch: [1][7900/36908] Elapsed 25m 43s (remain 94m 28s) Loss: 0.0000(0.0049) Grad: 171.9056  LR: 0.000017  \n","Epoch: [1][8000/36908] Elapsed 26m 3s (remain 94m 8s) Loss: 0.0024(0.0049) Grad: 12010.1699  LR: 0.000017  \n","Epoch: [1][8100/36908] Elapsed 26m 23s (remain 93m 49s) Loss: 0.0000(0.0049) Grad: 12.1441  LR: 0.000017  \n","Epoch: [1][8200/36908] Elapsed 26m 42s (remain 93m 30s) Loss: 0.0010(0.0048) Grad: 14211.4971  LR: 0.000017  \n","Epoch: [1][8300/36908] Elapsed 27m 2s (remain 93m 11s) Loss: 0.0000(0.0048) Grad: 13.4026  LR: 0.000017  \n","Epoch: [1][8400/36908] Elapsed 27m 22s (remain 92m 52s) Loss: 0.0000(0.0047) Grad: 87.8823  LR: 0.000017  \n","Epoch: [1][8500/36908] Elapsed 27m 41s (remain 92m 32s) Loss: 0.0000(0.0047) Grad: 154.0089  LR: 0.000017  \n","Epoch: [1][8600/36908] Elapsed 28m 1s (remain 92m 13s) Loss: 0.0004(0.0046) Grad: 13112.2031  LR: 0.000017  \n","Epoch: [1][8700/36908] Elapsed 28m 20s (remain 91m 53s) Loss: 0.0000(0.0046) Grad: 68.3080  LR: 0.000017  \n","Epoch: [1][8800/36908] Elapsed 28m 40s (remain 91m 34s) Loss: 0.0001(0.0045) Grad: 1159.7654  LR: 0.000017  \n","Epoch: [1][8900/36908] Elapsed 29m 0s (remain 91m 15s) Loss: 0.0127(0.0045) Grad: 29324.0938  LR: 0.000017  \n","Epoch: [1][9000/36908] Elapsed 29m 19s (remain 90m 55s) Loss: 0.0000(0.0045) Grad: 28.8092  LR: 0.000017  \n","Epoch: [1][9100/36908] Elapsed 29m 39s (remain 90m 36s) Loss: 0.0000(0.0044) Grad: 792.5081  LR: 0.000017  \n","Epoch: [1][9200/36908] Elapsed 29m 58s (remain 90m 16s) Loss: 0.0025(0.0044) Grad: 17833.3789  LR: 0.000017  \n","Epoch: [1][9300/36908] Elapsed 30m 18s (remain 89m 56s) Loss: 0.0000(0.0044) Grad: 725.6816  LR: 0.000017  \n","Epoch: [1][9400/36908] Elapsed 30m 37s (remain 89m 37s) Loss: 0.0005(0.0043) Grad: 11898.6924  LR: 0.000017  \n","Epoch: [1][9500/36908] Elapsed 30m 57s (remain 89m 17s) Loss: 0.0024(0.0043) Grad: 17318.3945  LR: 0.000017  \n","Epoch: [1][9600/36908] Elapsed 31m 16s (remain 88m 57s) Loss: 0.0000(0.0043) Grad: 28.9394  LR: 0.000016  \n","Epoch: [1][9700/36908] Elapsed 31m 36s (remain 88m 37s) Loss: 0.0067(0.0042) Grad: 42047.8828  LR: 0.000016  \n","Epoch: [1][9800/36908] Elapsed 31m 55s (remain 88m 18s) Loss: 0.0000(0.0042) Grad: 104.2035  LR: 0.000016  \n","Epoch: [1][9900/36908] Elapsed 32m 15s (remain 87m 58s) Loss: 0.0000(0.0042) Grad: 8.2377  LR: 0.000016  \n","Epoch: [1][10000/36908] Elapsed 32m 34s (remain 87m 38s) Loss: 0.0008(0.0041) Grad: 14760.0586  LR: 0.000016  \n","Epoch: [1][10100/36908] Elapsed 32m 54s (remain 87m 19s) Loss: 0.0000(0.0041) Grad: 547.6217  LR: 0.000016  \n","Epoch: [1][10200/36908] Elapsed 33m 13s (remain 86m 59s) Loss: 0.0013(0.0041) Grad: 28093.1816  LR: 0.000016  \n","Epoch: [1][10300/36908] Elapsed 33m 33s (remain 86m 39s) Loss: 0.0000(0.0040) Grad: 1022.2213  LR: 0.000016  \n","Epoch: [1][10400/36908] Elapsed 33m 52s (remain 86m 20s) Loss: 0.0000(0.0040) Grad: 95.5602  LR: 0.000016  \n","Epoch: [1][10500/36908] Elapsed 34m 12s (remain 86m 0s) Loss: 0.0000(0.0040) Grad: 837.1996  LR: 0.000016  \n","Epoch: [1][10600/36908] Elapsed 34m 31s (remain 85m 41s) Loss: 0.0001(0.0040) Grad: 2035.2963  LR: 0.000016  \n","Epoch: [1][10700/36908] Elapsed 34m 51s (remain 85m 21s) Loss: 0.0001(0.0039) Grad: 3186.6506  LR: 0.000016  \n","Epoch: [1][10800/36908] Elapsed 35m 10s (remain 85m 2s) Loss: 0.0000(0.0039) Grad: 431.4225  LR: 0.000016  \n","Epoch: [1][10900/36908] Elapsed 35m 30s (remain 84m 42s) Loss: 0.0047(0.0039) Grad: 94698.5469  LR: 0.000016  \n","Epoch: [1][11000/36908] Elapsed 35m 49s (remain 84m 22s) Loss: 0.0005(0.0039) Grad: 8229.8750  LR: 0.000016  \n","Epoch: [1][11100/36908] Elapsed 36m 9s (remain 84m 3s) Loss: 0.0000(0.0038) Grad: 15.2832  LR: 0.000016  \n","Epoch: [1][11200/36908] Elapsed 36m 28s (remain 83m 43s) Loss: 0.0000(0.0038) Grad: 18.5315  LR: 0.000015  \n","Epoch: [1][11300/36908] Elapsed 36m 48s (remain 83m 24s) Loss: 0.0002(0.0038) Grad: 4342.9521  LR: 0.000015  \n","Epoch: [1][11400/36908] Elapsed 37m 7s (remain 83m 4s) Loss: 0.0003(0.0038) Grad: 3403.1895  LR: 0.000015  \n","Epoch: [1][11500/36908] Elapsed 37m 27s (remain 82m 45s) Loss: 0.0000(0.0037) Grad: 16.7948  LR: 0.000015  \n","Epoch: [1][11600/36908] Elapsed 37m 47s (remain 82m 26s) Loss: 0.0024(0.0037) Grad: 9005.9756  LR: 0.000015  \n","Epoch: [1][11700/36908] Elapsed 38m 6s (remain 82m 6s) Loss: 0.0004(0.0037) Grad: 12081.6934  LR: 0.000015  \n","Epoch: [1][11800/36908] Elapsed 38m 26s (remain 81m 47s) Loss: 0.0000(0.0037) Grad: 38.5136  LR: 0.000015  \n","Epoch: [1][11900/36908] Elapsed 38m 45s (remain 81m 27s) Loss: 0.0000(0.0036) Grad: 5.7277  LR: 0.000015  \n","Epoch: [1][12000/36908] Elapsed 39m 5s (remain 81m 7s) Loss: 0.0000(0.0036) Grad: 6.8954  LR: 0.000015  \n","Epoch: [1][12100/36908] Elapsed 39m 25s (remain 80m 48s) Loss: 0.0006(0.0036) Grad: 14749.2393  LR: 0.000015  \n","Epoch: [1][12200/36908] Elapsed 39m 44s (remain 80m 29s) Loss: 0.0085(0.0036) Grad: 173076.6406  LR: 0.000015  \n","Epoch: [1][12300/36908] Elapsed 40m 4s (remain 80m 9s) Loss: 0.0000(0.0036) Grad: 216.8624  LR: 0.000015  \n","Epoch: [1][12400/36908] Elapsed 40m 23s (remain 79m 49s) Loss: 0.0005(0.0035) Grad: 32572.9395  LR: 0.000015  \n","Epoch: [1][12500/36908] Elapsed 40m 43s (remain 79m 30s) Loss: 0.0000(0.0035) Grad: 610.9201  LR: 0.000015  \n","Epoch: [1][12600/36908] Elapsed 41m 2s (remain 79m 10s) Loss: 0.0004(0.0035) Grad: 13612.0537  LR: 0.000015  \n","Epoch: [1][12700/36908] Elapsed 41m 22s (remain 78m 51s) Loss: 0.0000(0.0035) Grad: 461.4963  LR: 0.000015  \n","Epoch: [1][12800/36908] Elapsed 41m 41s (remain 78m 31s) Loss: 0.0007(0.0035) Grad: 15097.7070  LR: 0.000015  \n","Epoch: [1][12900/36908] Elapsed 42m 1s (remain 78m 12s) Loss: 0.0000(0.0034) Grad: 13.7116  LR: 0.000014  \n","Epoch: [1][13000/36908] Elapsed 42m 20s (remain 77m 52s) Loss: 0.0000(0.0034) Grad: 8.8953  LR: 0.000014  \n","Epoch: [1][13100/36908] Elapsed 42m 40s (remain 77m 32s) Loss: 0.0002(0.0034) Grad: 7356.6123  LR: 0.000014  \n","Epoch: [1][13200/36908] Elapsed 43m 0s (remain 77m 13s) Loss: 0.0000(0.0034) Grad: 36.7276  LR: 0.000014  \n","Epoch: [1][13300/36908] Elapsed 43m 19s (remain 76m 54s) Loss: 0.0000(0.0034) Grad: 1694.8468  LR: 0.000014  \n","Epoch: [1][13400/36908] Elapsed 43m 39s (remain 76m 34s) Loss: 0.0004(0.0033) Grad: 13231.1787  LR: 0.000014  \n","Epoch: [1][13500/36908] Elapsed 43m 58s (remain 76m 14s) Loss: 0.0015(0.0033) Grad: 25014.5000  LR: 0.000014  \n","Epoch: [1][13600/36908] Elapsed 44m 18s (remain 75m 55s) Loss: 0.0000(0.0033) Grad: 117.7616  LR: 0.000014  \n","Epoch: [1][13700/36908] Elapsed 44m 37s (remain 75m 35s) Loss: 0.0042(0.0033) Grad: 89317.9609  LR: 0.000014  \n","Epoch: [1][13800/36908] Elapsed 44m 57s (remain 75m 16s) Loss: 0.0000(0.0033) Grad: 454.8788  LR: 0.000014  \n","Epoch: [1][13900/36908] Elapsed 45m 16s (remain 74m 56s) Loss: 0.0000(0.0033) Grad: 541.5179  LR: 0.000014  \n","Epoch: [1][14000/36908] Elapsed 45m 36s (remain 74m 37s) Loss: 0.0012(0.0032) Grad: 40986.8086  LR: 0.000014  \n","Epoch: [1][14100/36908] Elapsed 45m 56s (remain 74m 17s) Loss: 0.0000(0.0032) Grad: 1267.3625  LR: 0.000014  \n","Epoch: [1][14200/36908] Elapsed 46m 15s (remain 73m 58s) Loss: 0.0009(0.0032) Grad: 44942.8984  LR: 0.000014  \n","Epoch: [1][14300/36908] Elapsed 46m 35s (remain 73m 38s) Loss: 0.0000(0.0032) Grad: 35.0367  LR: 0.000014  \n","Epoch: [1][14400/36908] Elapsed 46m 54s (remain 73m 19s) Loss: 0.0000(0.0032) Grad: 340.7791  LR: 0.000014  \n","Epoch: [1][14500/36908] Elapsed 47m 14s (remain 72m 59s) Loss: 0.0005(0.0032) Grad: 11186.4355  LR: 0.000013  \n","Epoch: [1][14600/36908] Elapsed 47m 33s (remain 72m 40s) Loss: 0.0000(0.0032) Grad: 4.4991  LR: 0.000013  \n","Epoch: [1][14700/36908] Elapsed 47m 53s (remain 72m 20s) Loss: 0.0000(0.0031) Grad: 1907.0486  LR: 0.000013  \n","Epoch: [1][14800/36908] Elapsed 48m 12s (remain 72m 1s) Loss: 0.0000(0.0031) Grad: 32.9758  LR: 0.000013  \n","Epoch: [1][14900/36908] Elapsed 48m 32s (remain 71m 41s) Loss: 0.0001(0.0031) Grad: 2100.7278  LR: 0.000013  \n","Epoch: [1][15000/36908] Elapsed 48m 52s (remain 71m 22s) Loss: 0.0002(0.0031) Grad: 7708.7837  LR: 0.000013  \n","Epoch: [1][15100/36908] Elapsed 49m 11s (remain 71m 2s) Loss: 0.0000(0.0031) Grad: 2088.2473  LR: 0.000013  \n","Epoch: [1][15200/36908] Elapsed 49m 31s (remain 70m 42s) Loss: 0.0000(0.0031) Grad: 8.2408  LR: 0.000013  \n","Epoch: [1][15300/36908] Elapsed 49m 50s (remain 70m 23s) Loss: 0.0000(0.0030) Grad: 24.9531  LR: 0.000013  \n","Epoch: [1][15400/36908] Elapsed 50m 10s (remain 70m 3s) Loss: 0.0000(0.0030) Grad: 202.9391  LR: 0.000013  \n","Epoch: [1][15500/36908] Elapsed 50m 29s (remain 69m 44s) Loss: 0.0002(0.0030) Grad: 14974.6006  LR: 0.000013  \n","Epoch: [1][15600/36908] Elapsed 50m 49s (remain 69m 24s) Loss: 0.0000(0.0030) Grad: 4.8836  LR: 0.000013  \n","Epoch: [1][15700/36908] Elapsed 51m 8s (remain 69m 4s) Loss: 0.0000(0.0030) Grad: 3257.2700  LR: 0.000013  \n","Epoch: [1][15800/36908] Elapsed 51m 28s (remain 68m 45s) Loss: 0.0000(0.0030) Grad: 328.4509  LR: 0.000013  \n","Epoch: [1][15900/36908] Elapsed 51m 47s (remain 68m 25s) Loss: 0.0000(0.0030) Grad: 259.0974  LR: 0.000013  \n","Epoch: [1][16000/36908] Elapsed 52m 7s (remain 68m 6s) Loss: 0.0000(0.0030) Grad: 808.2850  LR: 0.000013  \n","Epoch: [1][16100/36908] Elapsed 52m 27s (remain 67m 46s) Loss: 0.0005(0.0029) Grad: 34234.0352  LR: 0.000013  \n","Epoch: [1][16200/36908] Elapsed 52m 46s (remain 67m 27s) Loss: 0.0000(0.0029) Grad: 42.8583  LR: 0.000012  \n","Epoch: [1][16300/36908] Elapsed 53m 6s (remain 67m 7s) Loss: 0.0000(0.0029) Grad: 5.2273  LR: 0.000012  \n","Epoch: [1][16400/36908] Elapsed 53m 25s (remain 66m 48s) Loss: 0.0001(0.0029) Grad: 11967.5947  LR: 0.000012  \n","Epoch: [1][16500/36908] Elapsed 53m 45s (remain 66m 28s) Loss: 0.0000(0.0029) Grad: 16.8384  LR: 0.000012  \n","Epoch: [1][16600/36908] Elapsed 54m 4s (remain 66m 9s) Loss: 0.0000(0.0029) Grad: 900.1813  LR: 0.000012  \n","Epoch: [1][16700/36908] Elapsed 54m 24s (remain 65m 49s) Loss: 0.0000(0.0029) Grad: 4.7665  LR: 0.000012  \n","Epoch: [1][16800/36908] Elapsed 54m 44s (remain 65m 30s) Loss: 0.0000(0.0029) Grad: 12.6001  LR: 0.000012  \n","Epoch: [1][16900/36908] Elapsed 55m 3s (remain 65m 11s) Loss: 0.0000(0.0029) Grad: 18.4681  LR: 0.000012  \n","Epoch: [1][17000/36908] Elapsed 55m 23s (remain 64m 51s) Loss: 0.0000(0.0028) Grad: 1147.1808  LR: 0.000012  \n","Epoch: [1][17100/36908] Elapsed 55m 42s (remain 64m 31s) Loss: 0.0000(0.0028) Grad: 6.6775  LR: 0.000012  \n","Epoch: [1][17200/36908] Elapsed 56m 2s (remain 64m 12s) Loss: 0.0001(0.0028) Grad: 5412.2832  LR: 0.000012  \n","Epoch: [1][17300/36908] Elapsed 56m 21s (remain 63m 52s) Loss: 0.0001(0.0028) Grad: 7063.1753  LR: 0.000012  \n","Epoch: [1][17400/36908] Elapsed 56m 41s (remain 63m 33s) Loss: 0.0000(0.0028) Grad: 23.9852  LR: 0.000012  \n","Epoch: [1][17500/36908] Elapsed 57m 1s (remain 63m 13s) Loss: 0.0004(0.0028) Grad: 7857.2246  LR: 0.000012  \n","Epoch: [1][17600/36908] Elapsed 57m 20s (remain 62m 54s) Loss: 0.0000(0.0028) Grad: 812.4590  LR: 0.000012  \n","Epoch: [1][17700/36908] Elapsed 57m 40s (remain 62m 34s) Loss: 0.0001(0.0028) Grad: 8320.8887  LR: 0.000012  \n","Epoch: [1][17800/36908] Elapsed 57m 59s (remain 62m 15s) Loss: 0.0045(0.0028) Grad: 62852.8906  LR: 0.000012  \n","Epoch: [1][17900/36908] Elapsed 58m 19s (remain 61m 55s) Loss: 0.0000(0.0027) Grad: 42.2114  LR: 0.000011  \n","Epoch: [1][18000/36908] Elapsed 58m 39s (remain 61m 36s) Loss: 0.0000(0.0027) Grad: 736.8815  LR: 0.000011  \n","Epoch: [1][18100/36908] Elapsed 58m 58s (remain 61m 16s) Loss: 0.0013(0.0027) Grad: 31246.5254  LR: 0.000011  \n","Epoch: [1][18200/36908] Elapsed 59m 18s (remain 60m 56s) Loss: 0.0014(0.0027) Grad: 57625.2930  LR: 0.000011  \n","Epoch: [1][18300/36908] Elapsed 59m 37s (remain 60m 37s) Loss: 0.0003(0.0027) Grad: 9830.2051  LR: 0.000011  \n","Epoch: [1][18400/36908] Elapsed 59m 57s (remain 60m 17s) Loss: 0.0009(0.0027) Grad: 45635.2266  LR: 0.000011  \n","Epoch: [1][18500/36908] Elapsed 60m 16s (remain 59m 58s) Loss: 0.0012(0.0027) Grad: 28542.9258  LR: 0.000011  \n","Epoch: [1][18600/36908] Elapsed 60m 36s (remain 59m 38s) Loss: 0.0006(0.0027) Grad: 16363.8740  LR: 0.000011  \n","Epoch: [1][18700/36908] Elapsed 60m 55s (remain 59m 18s) Loss: 0.0002(0.0027) Grad: 24632.4023  LR: 0.000011  \n","Epoch: [1][18800/36908] Elapsed 61m 15s (remain 58m 59s) Loss: 0.0000(0.0027) Grad: 5.2887  LR: 0.000011  \n","Epoch: [1][18900/36908] Elapsed 61m 34s (remain 58m 39s) Loss: 0.0027(0.0026) Grad: 48492.7422  LR: 0.000011  \n","Epoch: [1][19000/36908] Elapsed 61m 54s (remain 58m 20s) Loss: 0.0105(0.0026) Grad: 41764.1914  LR: 0.000011  \n","Epoch: [1][19100/36908] Elapsed 62m 13s (remain 58m 0s) Loss: 0.0000(0.0026) Grad: 3153.1228  LR: 0.000011  \n","Epoch: [1][19200/36908] Elapsed 62m 33s (remain 57m 41s) Loss: 0.0000(0.0026) Grad: 32.1255  LR: 0.000011  \n","Epoch: [1][19300/36908] Elapsed 62m 52s (remain 57m 21s) Loss: 0.0000(0.0026) Grad: 214.8030  LR: 0.000011  \n","Epoch: [1][19400/36908] Elapsed 63m 12s (remain 57m 2s) Loss: 0.0000(0.0026) Grad: 445.3331  LR: 0.000011  \n","Epoch: [1][19500/36908] Elapsed 63m 32s (remain 56m 42s) Loss: 0.0011(0.0026) Grad: 23831.6035  LR: 0.000010  \n","Epoch: [1][19600/36908] Elapsed 63m 51s (remain 56m 23s) Loss: 0.0000(0.0026) Grad: 482.6235  LR: 0.000010  \n","Epoch: [1][19700/36908] Elapsed 64m 11s (remain 56m 3s) Loss: 0.0000(0.0026) Grad: 14.8982  LR: 0.000010  \n","Epoch: [1][19800/36908] Elapsed 64m 30s (remain 55m 44s) Loss: 0.0000(0.0026) Grad: 530.0994  LR: 0.000010  \n","Epoch: [1][19900/36908] Elapsed 64m 50s (remain 55m 24s) Loss: 0.0005(0.0026) Grad: 21356.4590  LR: 0.000010  \n","Epoch: [1][20000/36908] Elapsed 65m 9s (remain 55m 5s) Loss: 0.0016(0.0026) Grad: 32055.3574  LR: 0.000010  \n","Epoch: [1][20100/36908] Elapsed 65m 29s (remain 54m 45s) Loss: 0.0000(0.0025) Grad: 493.1817  LR: 0.000010  \n","Epoch: [1][20200/36908] Elapsed 65m 48s (remain 54m 25s) Loss: 0.0014(0.0025) Grad: 26982.6250  LR: 0.000010  \n","Epoch: [1][20300/36908] Elapsed 66m 8s (remain 54m 6s) Loss: 0.0001(0.0025) Grad: 6793.8760  LR: 0.000010  \n","Epoch: [1][20400/36908] Elapsed 66m 28s (remain 53m 46s) Loss: 0.0000(0.0025) Grad: 23.2444  LR: 0.000010  \n","Epoch: [1][20500/36908] Elapsed 66m 47s (remain 53m 27s) Loss: 0.0017(0.0025) Grad: 30209.4180  LR: 0.000010  \n","Epoch: [1][20600/36908] Elapsed 67m 7s (remain 53m 7s) Loss: 0.0000(0.0025) Grad: 21.8839  LR: 0.000010  \n","Epoch: [1][20700/36908] Elapsed 67m 26s (remain 52m 48s) Loss: 0.0000(0.0025) Grad: 30.8926  LR: 0.000010  \n","Epoch: [1][20800/36908] Elapsed 67m 46s (remain 52m 28s) Loss: 0.0000(0.0025) Grad: 242.1679  LR: 0.000010  \n","Epoch: [1][20900/36908] Elapsed 68m 5s (remain 52m 9s) Loss: 0.0005(0.0025) Grad: 33547.0195  LR: 0.000010  \n","Epoch: [1][21000/36908] Elapsed 68m 25s (remain 51m 49s) Loss: 0.0000(0.0025) Grad: 601.8768  LR: 0.000010  \n","Epoch: [1][21100/36908] Elapsed 68m 45s (remain 51m 30s) Loss: 0.0039(0.0025) Grad: 161447.7656  LR: 0.000010  \n","Epoch: [1][21200/36908] Elapsed 69m 4s (remain 51m 10s) Loss: 0.0027(0.0024) Grad: 141815.1406  LR: 0.000009  \n","Epoch: [1][21300/36908] Elapsed 69m 24s (remain 50m 50s) Loss: 0.0000(0.0024) Grad: 12.6231  LR: 0.000009  \n","Epoch: [1][21400/36908] Elapsed 69m 43s (remain 50m 31s) Loss: 0.0007(0.0024) Grad: 77933.2031  LR: 0.000009  \n","Epoch: [1][21500/36908] Elapsed 70m 3s (remain 50m 11s) Loss: 0.0000(0.0024) Grad: 35.3153  LR: 0.000009  \n","Epoch: [1][21600/36908] Elapsed 70m 22s (remain 49m 52s) Loss: 0.0000(0.0024) Grad: 1501.5769  LR: 0.000009  \n","Epoch: [1][21700/36908] Elapsed 70m 42s (remain 49m 33s) Loss: 0.0000(0.0024) Grad: 26.1975  LR: 0.000009  \n","Epoch: [1][21800/36908] Elapsed 71m 2s (remain 49m 13s) Loss: 0.0000(0.0024) Grad: 18.9363  LR: 0.000009  \n","Epoch: [1][21900/36908] Elapsed 71m 21s (remain 48m 53s) Loss: 0.0000(0.0024) Grad: 394.9383  LR: 0.000009  \n","Epoch: [1][22000/36908] Elapsed 71m 41s (remain 48m 34s) Loss: 0.0000(0.0024) Grad: 8.1030  LR: 0.000009  \n","Epoch: [1][22100/36908] Elapsed 72m 0s (remain 48m 14s) Loss: 0.0011(0.0024) Grad: 35142.9805  LR: 0.000009  \n","Epoch: [1][22200/36908] Elapsed 72m 20s (remain 47m 55s) Loss: 0.0005(0.0024) Grad: 25107.2773  LR: 0.000009  \n","Epoch: [1][22300/36908] Elapsed 72m 40s (remain 47m 35s) Loss: 0.0000(0.0024) Grad: 139.5589  LR: 0.000009  \n","Epoch: [1][22400/36908] Elapsed 72m 59s (remain 47m 16s) Loss: 0.0000(0.0024) Grad: 12.7758  LR: 0.000009  \n","Epoch: [1][22500/36908] Elapsed 73m 19s (remain 46m 56s) Loss: 0.0000(0.0024) Grad: 3.8291  LR: 0.000009  \n","Epoch: [1][22600/36908] Elapsed 73m 39s (remain 46m 37s) Loss: 0.0027(0.0024) Grad: 158784.0625  LR: 0.000009  \n","Epoch: [1][22700/36908] Elapsed 73m 58s (remain 46m 17s) Loss: 0.0052(0.0023) Grad: 98380.0781  LR: 0.000009  \n","Epoch: [1][22800/36908] Elapsed 74m 18s (remain 45m 58s) Loss: 0.0010(0.0023) Grad: 21921.2891  LR: 0.000008  \n","Epoch: [1][22900/36908] Elapsed 74m 37s (remain 45m 38s) Loss: 0.0005(0.0023) Grad: 20038.0215  LR: 0.000008  \n","Epoch: [1][23000/36908] Elapsed 74m 57s (remain 45m 19s) Loss: 0.0001(0.0023) Grad: 3221.6831  LR: 0.000008  \n","Epoch: [1][23100/36908] Elapsed 75m 16s (remain 44m 59s) Loss: 0.0000(0.0023) Grad: 3.8050  LR: 0.000008  \n","Epoch: [1][23200/36908] Elapsed 75m 35s (remain 44m 39s) Loss: 0.0052(0.0023) Grad: 205165.6094  LR: 0.000008  \n","Epoch: [1][23300/36908] Elapsed 75m 55s (remain 44m 20s) Loss: 0.0010(0.0023) Grad: 42045.0820  LR: 0.000008  \n","Epoch: [1][23400/36908] Elapsed 76m 15s (remain 44m 0s) Loss: 0.0022(0.0023) Grad: 23823.1641  LR: 0.000008  \n","Epoch: [1][23500/36908] Elapsed 76m 34s (remain 43m 41s) Loss: 0.0000(0.0023) Grad: 600.3448  LR: 0.000008  \n","Epoch: [1][23600/36908] Elapsed 76m 54s (remain 43m 21s) Loss: 0.0000(0.0023) Grad: 20.5613  LR: 0.000008  \n","Epoch: [1][23700/36908] Elapsed 77m 13s (remain 43m 2s) Loss: 0.0000(0.0023) Grad: 343.1515  LR: 0.000008  \n","Epoch: [1][23800/36908] Elapsed 77m 33s (remain 42m 42s) Loss: 0.0050(0.0023) Grad: 45940.9688  LR: 0.000008  \n","Epoch: [1][23900/36908] Elapsed 77m 53s (remain 42m 23s) Loss: 0.0042(0.0023) Grad: 37515.9531  LR: 0.000008  \n","Epoch: [1][24000/36908] Elapsed 78m 12s (remain 42m 3s) Loss: 0.0020(0.0023) Grad: 15029.8477  LR: 0.000008  \n","Epoch: [1][24100/36908] Elapsed 78m 32s (remain 41m 43s) Loss: 0.0000(0.0023) Grad: 5.4027  LR: 0.000008  \n","Epoch: [1][24200/36908] Elapsed 78m 51s (remain 41m 24s) Loss: 0.0033(0.0023) Grad: 80412.5547  LR: 0.000008  \n","Epoch: [1][24300/36908] Elapsed 79m 11s (remain 41m 4s) Loss: 0.0013(0.0023) Grad: 35431.0117  LR: 0.000008  \n","Epoch: [1][24400/36908] Elapsed 79m 30s (remain 40m 45s) Loss: 0.0001(0.0022) Grad: 1936.1602  LR: 0.000008  \n","Epoch: [1][24500/36908] Elapsed 79m 50s (remain 40m 25s) Loss: 0.0004(0.0022) Grad: 13024.6426  LR: 0.000007  \n","Epoch: [1][24600/36908] Elapsed 80m 9s (remain 40m 6s) Loss: 0.0000(0.0022) Grad: 1728.7786  LR: 0.000007  \n","Epoch: [1][24700/36908] Elapsed 80m 29s (remain 39m 46s) Loss: 0.0000(0.0022) Grad: 106.8506  LR: 0.000007  \n","Epoch: [1][24800/36908] Elapsed 80m 48s (remain 39m 27s) Loss: 0.0000(0.0022) Grad: 20.4347  LR: 0.000007  \n","Epoch: [1][24900/36908] Elapsed 81m 8s (remain 39m 7s) Loss: 0.0000(0.0022) Grad: 33.3800  LR: 0.000007  \n","Epoch: [1][25000/36908] Elapsed 81m 27s (remain 38m 47s) Loss: 0.0000(0.0022) Grad: 2097.0933  LR: 0.000007  \n","Epoch: [1][25100/36908] Elapsed 81m 47s (remain 38m 28s) Loss: 0.0000(0.0022) Grad: 248.8761  LR: 0.000007  \n","Epoch: [1][25200/36908] Elapsed 82m 7s (remain 38m 8s) Loss: 0.0000(0.0022) Grad: 295.3813  LR: 0.000007  \n","Epoch: [1][25300/36908] Elapsed 82m 26s (remain 37m 49s) Loss: 0.0004(0.0022) Grad: 11964.2109  LR: 0.000007  \n","Epoch: [1][25400/36908] Elapsed 82m 46s (remain 37m 29s) Loss: 0.0000(0.0022) Grad: 6.2405  LR: 0.000007  \n","Epoch: [1][25500/36908] Elapsed 83m 5s (remain 37m 10s) Loss: 0.0000(0.0022) Grad: 79.5257  LR: 0.000007  \n","Epoch: [1][25600/36908] Elapsed 83m 25s (remain 36m 50s) Loss: 0.0000(0.0022) Grad: 36.9599  LR: 0.000007  \n","Epoch: [1][25700/36908] Elapsed 83m 44s (remain 36m 31s) Loss: 0.0000(0.0022) Grad: 507.3506  LR: 0.000007  \n","Epoch: [1][25800/36908] Elapsed 84m 4s (remain 36m 11s) Loss: 0.0000(0.0022) Grad: 6.5527  LR: 0.000007  \n","Epoch: [1][25900/36908] Elapsed 84m 24s (remain 35m 52s) Loss: 0.0000(0.0022) Grad: 2073.8748  LR: 0.000007  \n","Epoch: [1][26000/36908] Elapsed 84m 43s (remain 35m 32s) Loss: 0.0004(0.0022) Grad: 19582.4102  LR: 0.000007  \n","Epoch: [1][26100/36908] Elapsed 85m 3s (remain 35m 12s) Loss: 0.0000(0.0022) Grad: 173.5619  LR: 0.000007  \n","Epoch: [1][26200/36908] Elapsed 85m 22s (remain 34m 53s) Loss: 0.0000(0.0021) Grad: 150.3972  LR: 0.000006  \n","Epoch: [1][26300/36908] Elapsed 85m 42s (remain 34m 33s) Loss: 0.0003(0.0021) Grad: 10123.2207  LR: 0.000006  \n","Epoch: [1][26400/36908] Elapsed 86m 1s (remain 34m 14s) Loss: 0.0000(0.0021) Grad: 7.8613  LR: 0.000006  \n","Epoch: [1][26500/36908] Elapsed 86m 21s (remain 33m 54s) Loss: 0.0000(0.0021) Grad: 8.7410  LR: 0.000006  \n","Epoch: [1][26600/36908] Elapsed 86m 41s (remain 33m 35s) Loss: 0.0000(0.0021) Grad: 25.3333  LR: 0.000006  \n","Epoch: [1][26700/36908] Elapsed 87m 0s (remain 33m 15s) Loss: 0.0000(0.0021) Grad: 374.5987  LR: 0.000006  \n","Epoch: [1][26800/36908] Elapsed 87m 20s (remain 32m 56s) Loss: 0.0000(0.0021) Grad: 57.9820  LR: 0.000006  \n","Epoch: [1][26900/36908] Elapsed 87m 39s (remain 32m 36s) Loss: 0.0000(0.0021) Grad: 1259.5902  LR: 0.000006  \n","Epoch: [1][27000/36908] Elapsed 87m 59s (remain 32m 16s) Loss: 0.0008(0.0021) Grad: 15727.5469  LR: 0.000006  \n","Epoch: [1][27100/36908] Elapsed 88m 18s (remain 31m 57s) Loss: 0.0001(0.0021) Grad: 6491.7881  LR: 0.000006  \n","Epoch: [1][27200/36908] Elapsed 88m 38s (remain 31m 37s) Loss: 0.0050(0.0021) Grad: 39902.6992  LR: 0.000006  \n","Epoch: [1][27300/36908] Elapsed 88m 57s (remain 31m 18s) Loss: 0.0008(0.0021) Grad: 29699.1855  LR: 0.000006  \n","Epoch: [1][27400/36908] Elapsed 89m 17s (remain 30m 58s) Loss: 0.0000(0.0021) Grad: 9.7261  LR: 0.000006  \n","Epoch: [1][27500/36908] Elapsed 89m 37s (remain 30m 39s) Loss: 0.0000(0.0021) Grad: 15.2430  LR: 0.000006  \n","Epoch: [1][27600/36908] Elapsed 89m 56s (remain 30m 19s) Loss: 0.0001(0.0021) Grad: 2496.3098  LR: 0.000006  \n","Epoch: [1][27700/36908] Elapsed 90m 16s (remain 30m 0s) Loss: 0.0000(0.0021) Grad: 8.2110  LR: 0.000006  \n","Epoch: [1][27800/36908] Elapsed 90m 35s (remain 29m 40s) Loss: 0.0000(0.0021) Grad: 3084.3555  LR: 0.000005  \n","Epoch: [1][27900/36908] Elapsed 90m 55s (remain 29m 21s) Loss: 0.0000(0.0021) Grad: 177.1345  LR: 0.000005  \n","Epoch: [1][28000/36908] Elapsed 91m 14s (remain 29m 1s) Loss: 0.0000(0.0021) Grad: 6.7340  LR: 0.000005  \n","Epoch: [1][28100/36908] Elapsed 91m 34s (remain 28m 41s) Loss: 0.0002(0.0021) Grad: 6612.7871  LR: 0.000005  \n","Epoch: [1][28200/36908] Elapsed 91m 53s (remain 28m 22s) Loss: 0.0000(0.0021) Grad: 172.4910  LR: 0.000005  \n","Epoch: [1][28300/36908] Elapsed 92m 13s (remain 28m 2s) Loss: 0.0000(0.0021) Grad: 50.4562  LR: 0.000005  \n","Epoch: [1][28400/36908] Elapsed 92m 32s (remain 27m 43s) Loss: 0.0000(0.0021) Grad: 33.0662  LR: 0.000005  \n","Epoch: [1][28500/36908] Elapsed 92m 52s (remain 27m 23s) Loss: 0.0001(0.0020) Grad: 11330.8730  LR: 0.000005  \n","Epoch: [1][28600/36908] Elapsed 93m 11s (remain 27m 4s) Loss: 0.0003(0.0020) Grad: 14524.9062  LR: 0.000005  \n","Epoch: [1][28700/36908] Elapsed 93m 31s (remain 26m 44s) Loss: 0.0005(0.0020) Grad: 16769.0137  LR: 0.000005  \n","Epoch: [1][28800/36908] Elapsed 93m 51s (remain 26m 25s) Loss: 0.0000(0.0020) Grad: 1776.4185  LR: 0.000005  \n","Epoch: [1][28900/36908] Elapsed 94m 10s (remain 26m 5s) Loss: 0.0000(0.0020) Grad: 676.2803  LR: 0.000005  \n","Epoch: [1][29000/36908] Elapsed 94m 30s (remain 25m 45s) Loss: 0.0000(0.0020) Grad: 27.1235  LR: 0.000005  \n","Epoch: [1][29100/36908] Elapsed 94m 49s (remain 25m 26s) Loss: 0.0000(0.0020) Grad: 10.4009  LR: 0.000005  \n","Epoch: [1][29200/36908] Elapsed 95m 9s (remain 25m 6s) Loss: 0.0008(0.0020) Grad: 17837.4844  LR: 0.000005  \n","Epoch: [1][29300/36908] Elapsed 95m 28s (remain 24m 47s) Loss: 0.0000(0.0020) Grad: 6.0279  LR: 0.000005  \n","Epoch: [1][29400/36908] Elapsed 95m 48s (remain 24m 27s) Loss: 0.0000(0.0020) Grad: 998.0654  LR: 0.000005  \n","Epoch: [1][29500/36908] Elapsed 96m 7s (remain 24m 8s) Loss: 0.0000(0.0020) Grad: 422.8915  LR: 0.000004  \n","Epoch: [1][29600/36908] Elapsed 96m 27s (remain 23m 48s) Loss: 0.0000(0.0020) Grad: 9.1977  LR: 0.000004  \n","Epoch: [1][29700/36908] Elapsed 96m 46s (remain 23m 29s) Loss: 0.0000(0.0020) Grad: 238.2196  LR: 0.000004  \n","Epoch: [1][29800/36908] Elapsed 97m 6s (remain 23m 9s) Loss: 0.0000(0.0020) Grad: 241.9084  LR: 0.000004  \n","Epoch: [1][29900/36908] Elapsed 97m 26s (remain 22m 49s) Loss: 0.0000(0.0020) Grad: 12.3125  LR: 0.000004  \n","Epoch: [1][30000/36908] Elapsed 97m 45s (remain 22m 30s) Loss: 0.0000(0.0020) Grad: 21.9823  LR: 0.000004  \n","Epoch: [1][30100/36908] Elapsed 98m 5s (remain 22m 10s) Loss: 0.0001(0.0020) Grad: 2654.1223  LR: 0.000004  \n","Epoch: [1][30200/36908] Elapsed 98m 24s (remain 21m 51s) Loss: 0.0000(0.0020) Grad: 79.0403  LR: 0.000004  \n","Epoch: [1][30300/36908] Elapsed 98m 44s (remain 21m 31s) Loss: 0.0102(0.0020) Grad: 73505.0000  LR: 0.000004  \n","Epoch: [1][30400/36908] Elapsed 99m 3s (remain 21m 12s) Loss: 0.0050(0.0020) Grad: 154594.5469  LR: 0.000004  \n","Epoch: [1][30500/36908] Elapsed 99m 23s (remain 20m 52s) Loss: 0.0000(0.0020) Grad: 9.8665  LR: 0.000004  \n","Epoch: [1][30600/36908] Elapsed 99m 43s (remain 20m 33s) Loss: 0.0000(0.0020) Grad: 8.8032  LR: 0.000004  \n","Epoch: [1][30700/36908] Elapsed 100m 2s (remain 20m 13s) Loss: 0.0000(0.0020) Grad: 1003.3034  LR: 0.000004  \n","Epoch: [1][30800/36908] Elapsed 100m 22s (remain 19m 54s) Loss: 0.0000(0.0020) Grad: 14.3431  LR: 0.000004  \n","Epoch: [1][30900/36908] Elapsed 100m 41s (remain 19m 34s) Loss: 0.0000(0.0019) Grad: 12.8576  LR: 0.000004  \n","Epoch: [1][31000/36908] Elapsed 101m 1s (remain 19m 14s) Loss: 0.0006(0.0019) Grad: 20514.1270  LR: 0.000004  \n","Epoch: [1][31100/36908] Elapsed 101m 20s (remain 18m 55s) Loss: 0.0005(0.0019) Grad: 22322.5898  LR: 0.000003  \n","Epoch: [1][31200/36908] Elapsed 101m 40s (remain 18m 35s) Loss: 0.0000(0.0019) Grad: 17.2415  LR: 0.000003  \n","Epoch: [1][31300/36908] Elapsed 101m 59s (remain 18m 16s) Loss: 0.0000(0.0019) Grad: 1235.3553  LR: 0.000003  \n","Epoch: [1][31400/36908] Elapsed 102m 19s (remain 17m 56s) Loss: 0.0016(0.0019) Grad: 41270.5742  LR: 0.000003  \n","Epoch: [1][31500/36908] Elapsed 102m 39s (remain 17m 37s) Loss: 0.0000(0.0019) Grad: 173.5147  LR: 0.000003  \n","Epoch: [1][31600/36908] Elapsed 102m 58s (remain 17m 17s) Loss: 0.0000(0.0019) Grad: 13.6434  LR: 0.000003  \n","Epoch: [1][31700/36908] Elapsed 103m 18s (remain 16m 58s) Loss: 0.0000(0.0019) Grad: 1090.7516  LR: 0.000003  \n","Epoch: [1][31800/36908] Elapsed 103m 37s (remain 16m 38s) Loss: 0.0020(0.0019) Grad: 14174.5889  LR: 0.000003  \n","Epoch: [1][31900/36908] Elapsed 103m 57s (remain 16m 19s) Loss: 0.0001(0.0019) Grad: 7384.3945  LR: 0.000003  \n","Epoch: [1][32000/36908] Elapsed 104m 17s (remain 15m 59s) Loss: 0.0033(0.0019) Grad: 21968.0566  LR: 0.000003  \n","Epoch: [1][32100/36908] Elapsed 104m 36s (remain 15m 39s) Loss: 0.0000(0.0019) Grad: 4.7196  LR: 0.000003  \n","Epoch: [1][32200/36908] Elapsed 104m 56s (remain 15m 20s) Loss: 0.0023(0.0019) Grad: 62982.2070  LR: 0.000003  \n","Epoch: [1][32300/36908] Elapsed 105m 16s (remain 15m 0s) Loss: 0.0027(0.0019) Grad: 55707.8828  LR: 0.000003  \n","Epoch: [1][32400/36908] Elapsed 105m 35s (remain 14m 41s) Loss: 0.0000(0.0019) Grad: 11.0432  LR: 0.000003  \n","Epoch: [1][32500/36908] Elapsed 105m 55s (remain 14m 21s) Loss: 0.0004(0.0019) Grad: 14057.1943  LR: 0.000003  \n","Epoch: [1][32600/36908] Elapsed 106m 14s (remain 14m 2s) Loss: 0.0155(0.0019) Grad: 166705.1250  LR: 0.000003  \n","Epoch: [1][32700/36908] Elapsed 106m 34s (remain 13m 42s) Loss: 0.0000(0.0019) Grad: 4.4351  LR: 0.000003  \n","Epoch: [1][32800/36908] Elapsed 106m 53s (remain 13m 23s) Loss: 0.0001(0.0019) Grad: 2867.6821  LR: 0.000002  \n","Epoch: [1][32900/36908] Elapsed 107m 13s (remain 13m 3s) Loss: 0.0000(0.0019) Grad: 28.3457  LR: 0.000002  \n","Epoch: [1][33000/36908] Elapsed 107m 33s (remain 12m 43s) Loss: 0.0000(0.0019) Grad: 5.3842  LR: 0.000002  \n","Epoch: [1][33100/36908] Elapsed 107m 52s (remain 12m 24s) Loss: 0.0000(0.0019) Grad: 37.3375  LR: 0.000002  \n","Epoch: [1][33200/36908] Elapsed 108m 12s (remain 12m 4s) Loss: 0.0000(0.0019) Grad: 20.7036  LR: 0.000002  \n","Epoch: [1][33300/36908] Elapsed 108m 31s (remain 11m 45s) Loss: 0.0000(0.0019) Grad: 16.2619  LR: 0.000002  \n","Epoch: [1][33400/36908] Elapsed 108m 51s (remain 11m 25s) Loss: 0.0000(0.0019) Grad: 6.6627  LR: 0.000002  \n","Epoch: [1][33500/36908] Elapsed 109m 11s (remain 11m 6s) Loss: 0.0000(0.0019) Grad: 651.2967  LR: 0.000002  \n","Epoch: [1][33600/36908] Elapsed 109m 30s (remain 10m 46s) Loss: 0.0000(0.0019) Grad: 2022.9928  LR: 0.000002  \n","Epoch: [1][33700/36908] Elapsed 109m 50s (remain 10m 27s) Loss: 0.0004(0.0018) Grad: 9534.4365  LR: 0.000002  \n","Epoch: [1][33800/36908] Elapsed 110m 9s (remain 10m 7s) Loss: 0.0020(0.0018) Grad: 32853.8203  LR: 0.000002  \n","Epoch: [1][33900/36908] Elapsed 110m 29s (remain 9m 48s) Loss: 0.0000(0.0018) Grad: 11.4374  LR: 0.000002  \n","Epoch: [1][34000/36908] Elapsed 110m 48s (remain 9m 28s) Loss: 0.0140(0.0018) Grad: 85444.4609  LR: 0.000002  \n","Epoch: [1][34100/36908] Elapsed 111m 8s (remain 9m 8s) Loss: 0.0000(0.0018) Grad: 16.1304  LR: 0.000002  \n","Epoch: [1][34200/36908] Elapsed 111m 27s (remain 8m 49s) Loss: 0.0000(0.0018) Grad: 1466.0934  LR: 0.000002  \n","Epoch: [1][34300/36908] Elapsed 111m 47s (remain 8m 29s) Loss: 0.0000(0.0018) Grad: 14.6259  LR: 0.000002  \n","Epoch: [1][34400/36908] Elapsed 112m 6s (remain 8m 10s) Loss: 0.0001(0.0018) Grad: 7142.1978  LR: 0.000002  \n","Epoch: [1][34500/36908] Elapsed 112m 26s (remain 7m 50s) Loss: 0.0000(0.0018) Grad: 4.8018  LR: 0.000001  \n","Epoch: [1][34600/36908] Elapsed 112m 46s (remain 7m 31s) Loss: 0.0000(0.0018) Grad: 4.4256  LR: 0.000001  \n","Epoch: [1][34700/36908] Elapsed 113m 5s (remain 7m 11s) Loss: 0.0000(0.0018) Grad: 11.3008  LR: 0.000001  \n","Epoch: [1][34800/36908] Elapsed 113m 25s (remain 6m 52s) Loss: 0.0000(0.0018) Grad: 135.5452  LR: 0.000001  \n","Epoch: [1][34900/36908] Elapsed 113m 44s (remain 6m 32s) Loss: 0.0015(0.0018) Grad: 47901.9883  LR: 0.000001  \n","Epoch: [1][35000/36908] Elapsed 114m 4s (remain 6m 12s) Loss: 0.0000(0.0018) Grad: 103.1366  LR: 0.000001  \n","Epoch: [1][35100/36908] Elapsed 114m 24s (remain 5m 53s) Loss: 0.0000(0.0018) Grad: 224.9682  LR: 0.000001  \n","Epoch: [1][35200/36908] Elapsed 114m 43s (remain 5m 33s) Loss: 0.0000(0.0018) Grad: 11.3358  LR: 0.000001  \n","Epoch: [1][35300/36908] Elapsed 115m 3s (remain 5m 14s) Loss: 0.0000(0.0018) Grad: 25.3098  LR: 0.000001  \n","Epoch: [1][35400/36908] Elapsed 115m 22s (remain 4m 54s) Loss: 0.0000(0.0018) Grad: 66.8708  LR: 0.000001  \n","Epoch: [1][35500/36908] Elapsed 115m 42s (remain 4m 35s) Loss: 0.0015(0.0018) Grad: 30717.3516  LR: 0.000001  \n","Epoch: [1][35600/36908] Elapsed 116m 1s (remain 4m 15s) Loss: 0.0000(0.0018) Grad: 21.3820  LR: 0.000001  \n","Epoch: [1][35700/36908] Elapsed 116m 21s (remain 3m 56s) Loss: 0.0000(0.0018) Grad: 6.2021  LR: 0.000001  \n","Epoch: [1][35800/36908] Elapsed 116m 40s (remain 3m 36s) Loss: 0.0013(0.0018) Grad: 57048.1250  LR: 0.000001  \n","Epoch: [1][35900/36908] Elapsed 117m 0s (remain 3m 16s) Loss: 0.0000(0.0018) Grad: 318.1222  LR: 0.000001  \n","Epoch: [1][36000/36908] Elapsed 117m 20s (remain 2m 57s) Loss: 0.0000(0.0018) Grad: 112.6343  LR: 0.000001  \n","Epoch: [1][36100/36908] Elapsed 117m 39s (remain 2m 37s) Loss: 0.0000(0.0018) Grad: 8.0406  LR: 0.000000  \n","Epoch: [1][36200/36908] Elapsed 117m 59s (remain 2m 18s) Loss: 0.0000(0.0018) Grad: 9.2836  LR: 0.000000  \n","Epoch: [1][36300/36908] Elapsed 118m 18s (remain 1m 58s) Loss: 0.0000(0.0018) Grad: 57.2776  LR: 0.000000  \n","Epoch: [1][36400/36908] Elapsed 118m 38s (remain 1m 39s) Loss: 0.0011(0.0018) Grad: 37695.0312  LR: 0.000000  \n","Epoch: [1][36500/36908] Elapsed 118m 57s (remain 1m 19s) Loss: 0.0000(0.0018) Grad: 99.2395  LR: 0.000000  \n","Epoch: [1][36600/36908] Elapsed 119m 17s (remain 1m 0s) Loss: 0.0033(0.0018) Grad: 184223.0781  LR: 0.000000  \n","Epoch: [1][36700/36908] Elapsed 119m 36s (remain 0m 40s) Loss: 0.0000(0.0018) Grad: 12.3858  LR: 0.000000  \n","Epoch: [1][36800/36908] Elapsed 119m 56s (remain 0m 20s) Loss: 0.0013(0.0018) Grad: 77826.9297  LR: 0.000000  \n","Epoch: [1][36900/36908] Elapsed 120m 15s (remain 0m 1s) Loss: 0.0000(0.0018) Grad: 40.6922  LR: 0.000000  \n","Epoch: [1][36907/36908] Elapsed 120m 17s (remain 0m 0s) Loss: 0.0001(0.0018) Grad: 8933.3867  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 31s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 6s (remain 1m 8s) Loss: 0.0000(0.0031) \n","EVAL: [200/1192] Elapsed 0m 12s (remain 1m 0s) Loss: 0.0000(0.0033) \n","EVAL: [300/1192] Elapsed 0m 18s (remain 0m 53s) Loss: 0.0000(0.0049) \n","EVAL: [400/1192] Elapsed 0m 23s (remain 0m 47s) Loss: 0.0134(0.0053) \n","EVAL: [500/1192] Elapsed 0m 29s (remain 0m 41s) Loss: 0.0106(0.0050) \n","EVAL: [600/1192] Elapsed 0m 35s (remain 0m 35s) Loss: 0.0696(0.0049) \n","EVAL: [700/1192] Elapsed 0m 41s (remain 0m 29s) Loss: 0.0133(0.0057) \n","EVAL: [800/1192] Elapsed 0m 47s (remain 0m 23s) Loss: 0.0066(0.0055) \n","EVAL: [900/1192] Elapsed 0m 53s (remain 0m 17s) Loss: 0.0028(0.0054) \n","EVAL: [1000/1192] Elapsed 0m 59s (remain 0m 11s) Loss: 0.0000(0.0052) \n","EVAL: [1100/1192] Elapsed 1m 5s (remain 0m 5s) Loss: 0.0035(0.0050) \n","EVAL: [1191/1192] Elapsed 1m 10s (remain 0m 0s) Loss: 0.0056(0.0047) \n","Epoch 1 - avg_train_loss: 0.0018  avg_val_loss: 0.0047  time: 7292s\n","Epoch 1 - Score: 0.8820\n","Epoch 1 - Save Best Score: 0.8820 Model\n","========== fold: 2 training ==========\n","get pseudo plain from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n","get pseudo labels from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_2.npy\n","get pseudo labels from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_2.npy\n","get pseudo labels from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp083/pseudo_labels_2.npy\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/612602 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24751bd60fad4b3782ad5efc6215c555"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/612602 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12cb82c88a3946128269436929769660"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["(612602, 8)\n","(100000, 9)\n","(110725, 11)\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\n","Epoch: [1][0/36908] Elapsed 0m 0s (remain 435m 34s) Loss: 0.1149(0.1149) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/36908] Elapsed 0m 20s (remain 124m 37s) Loss: 0.0888(0.1053) Grad: 86086.5391  LR: 0.000001  \n","Epoch: [1][200/36908] Elapsed 0m 40s (remain 122m 16s) Loss: 0.0273(0.0793) Grad: 25135.1504  LR: 0.000001  \n","Epoch: [1][300/36908] Elapsed 0m 59s (remain 121m 31s) Loss: 0.0211(0.0586) Grad: 6697.1641  LR: 0.000002  \n","Epoch: [1][400/36908] Elapsed 1m 19s (remain 120m 51s) Loss: 0.0152(0.0467) Grad: 7354.3940  LR: 0.000002  \n","Epoch: [1][500/36908] Elapsed 1m 39s (remain 120m 22s) Loss: 0.0154(0.0400) Grad: 6111.4248  LR: 0.000003  \n","Epoch: [1][600/36908] Elapsed 1m 59s (remain 120m 3s) Loss: 0.0125(0.0353) Grad: 4596.1997  LR: 0.000003  \n","Epoch: [1][700/36908] Elapsed 2m 19s (remain 119m 45s) Loss: 0.0131(0.0318) Grad: 5290.3350  LR: 0.000004  \n","Epoch: [1][800/36908] Elapsed 2m 38s (remain 119m 24s) Loss: 0.0092(0.0292) Grad: 4084.3240  LR: 0.000004  \n","Epoch: [1][900/36908] Elapsed 2m 58s (remain 119m 10s) Loss: 0.0218(0.0272) Grad: 9067.7969  LR: 0.000005  \n","Epoch: [1][1000/36908] Elapsed 3m 18s (remain 118m 54s) Loss: 0.0024(0.0254) Grad: 2635.4880  LR: 0.000005  \n","Epoch: [1][1100/36908] Elapsed 3m 38s (remain 118m 30s) Loss: 0.0050(0.0238) Grad: 3669.6663  LR: 0.000006  \n","Epoch: [1][1200/36908] Elapsed 3m 58s (remain 118m 11s) Loss: 0.0024(0.0224) Grad: 3474.3752  LR: 0.000007  \n","Epoch: [1][1300/36908] Elapsed 4m 18s (remain 117m 49s) Loss: 0.0031(0.0211) Grad: 3145.1658  LR: 0.000007  \n","Epoch: [1][1400/36908] Elapsed 4m 38s (remain 117m 29s) Loss: 0.0023(0.0200) Grad: 4640.0239  LR: 0.000008  \n","Epoch: [1][1500/36908] Elapsed 4m 57s (remain 117m 6s) Loss: 0.0005(0.0189) Grad: 1347.2319  LR: 0.000008  \n","Epoch: [1][1600/36908] Elapsed 5m 17s (remain 116m 45s) Loss: 0.0025(0.0180) Grad: 7579.0806  LR: 0.000009  \n","Epoch: [1][1700/36908] Elapsed 5m 37s (remain 116m 23s) Loss: 0.0007(0.0171) Grad: 1670.8024  LR: 0.000009  \n","Epoch: [1][1800/36908] Elapsed 5m 57s (remain 116m 1s) Loss: 0.0040(0.0163) Grad: 6320.1450  LR: 0.000010  \n","Epoch: [1][1900/36908] Elapsed 6m 16s (remain 115m 41s) Loss: 0.0005(0.0156) Grad: 1062.6311  LR: 0.000010  \n","Epoch: [1][2000/36908] Elapsed 6m 36s (remain 115m 20s) Loss: 0.0002(0.0150) Grad: 1061.9672  LR: 0.000011  \n","Epoch: [1][2100/36908] Elapsed 6m 56s (remain 114m 58s) Loss: 0.0008(0.0144) Grad: 1684.2654  LR: 0.000011  \n","Epoch: [1][2200/36908] Elapsed 7m 16s (remain 114m 38s) Loss: 0.0007(0.0139) Grad: 2651.3601  LR: 0.000012  \n","Epoch: [1][2300/36908] Elapsed 7m 36s (remain 114m 19s) Loss: 0.0008(0.0134) Grad: 4978.5649  LR: 0.000012  \n","Epoch: [1][2400/36908] Elapsed 7m 55s (remain 113m 59s) Loss: 0.0018(0.0129) Grad: 3489.3145  LR: 0.000013  \n","Epoch: [1][2500/36908] Elapsed 8m 15s (remain 113m 38s) Loss: 0.0017(0.0125) Grad: 5239.4800  LR: 0.000014  \n","Epoch: [1][2600/36908] Elapsed 8m 35s (remain 113m 18s) Loss: 0.0009(0.0121) Grad: 2291.3030  LR: 0.000014  \n","Epoch: [1][2700/36908] Elapsed 8m 55s (remain 112m 58s) Loss: 0.0000(0.0117) Grad: 34.8982  LR: 0.000015  \n","Epoch: [1][2800/36908] Elapsed 9m 15s (remain 112m 38s) Loss: 0.0050(0.0114) Grad: 12708.4033  LR: 0.000015  \n","Epoch: [1][2900/36908] Elapsed 9m 34s (remain 112m 19s) Loss: 0.0006(0.0111) Grad: 1339.3647  LR: 0.000016  \n","Epoch: [1][3000/36908] Elapsed 9m 54s (remain 111m 59s) Loss: 0.0033(0.0108) Grad: 11345.5908  LR: 0.000016  \n","Epoch: [1][3100/36908] Elapsed 10m 14s (remain 111m 40s) Loss: 0.0024(0.0105) Grad: 4793.6362  LR: 0.000017  \n","Epoch: [1][3200/36908] Elapsed 10m 34s (remain 111m 20s) Loss: 0.0001(0.0102) Grad: 143.1533  LR: 0.000017  \n","Epoch: [1][3300/36908] Elapsed 10m 54s (remain 110m 59s) Loss: 0.0000(0.0100) Grad: 11.6976  LR: 0.000018  \n","Epoch: [1][3400/36908] Elapsed 11m 13s (remain 110m 39s) Loss: 0.0010(0.0097) Grad: 1697.0468  LR: 0.000018  \n","Epoch: [1][3500/36908] Elapsed 11m 33s (remain 110m 19s) Loss: 0.0026(0.0095) Grad: 4169.0225  LR: 0.000019  \n","Epoch: [1][3600/36908] Elapsed 11m 53s (remain 109m 58s) Loss: 0.0006(0.0093) Grad: 1349.9009  LR: 0.000020  \n","Epoch: [1][3700/36908] Elapsed 12m 13s (remain 109m 38s) Loss: 0.0017(0.0091) Grad: 13249.1289  LR: 0.000020  \n","Epoch: [1][3800/36908] Elapsed 12m 33s (remain 109m 19s) Loss: 0.0001(0.0089) Grad: 166.7243  LR: 0.000020  \n","Epoch: [1][3900/36908] Elapsed 12m 52s (remain 108m 59s) Loss: 0.0001(0.0087) Grad: 281.0607  LR: 0.000020  \n","Epoch: [1][4000/36908] Elapsed 13m 12s (remain 108m 40s) Loss: 0.0014(0.0085) Grad: 6316.8633  LR: 0.000020  \n","Epoch: [1][4100/36908] Elapsed 13m 32s (remain 108m 21s) Loss: 0.0000(0.0084) Grad: 153.6037  LR: 0.000020  \n","Epoch: [1][4200/36908] Elapsed 13m 52s (remain 108m 1s) Loss: 0.0002(0.0082) Grad: 1026.5657  LR: 0.000020  \n","Epoch: [1][4300/36908] Elapsed 14m 12s (remain 107m 41s) Loss: 0.0001(0.0081) Grad: 6901.1387  LR: 0.000020  \n","Epoch: [1][4400/36908] Elapsed 14m 32s (remain 107m 21s) Loss: 0.0000(0.0079) Grad: 71.8847  LR: 0.000020  \n","Epoch: [1][4500/36908] Elapsed 14m 51s (remain 107m 2s) Loss: 0.0009(0.0078) Grad: 3002.1716  LR: 0.000020  \n","Epoch: [1][4600/36908] Elapsed 15m 11s (remain 106m 42s) Loss: 0.0001(0.0076) Grad: 1303.5236  LR: 0.000019  \n","Epoch: [1][4700/36908] Elapsed 15m 31s (remain 106m 22s) Loss: 0.0045(0.0075) Grad: 31352.9727  LR: 0.000019  \n","Epoch: [1][4800/36908] Elapsed 15m 51s (remain 106m 3s) Loss: 0.0003(0.0074) Grad: 3129.1638  LR: 0.000019  \n","Epoch: [1][4900/36908] Elapsed 16m 11s (remain 105m 43s) Loss: 0.0022(0.0073) Grad: 11230.5430  LR: 0.000019  \n","Epoch: [1][5000/36908] Elapsed 16m 31s (remain 105m 23s) Loss: 0.0016(0.0072) Grad: 6662.0796  LR: 0.000019  \n","Epoch: [1][5100/36908] Elapsed 16m 51s (remain 105m 6s) Loss: 0.0001(0.0070) Grad: 429.1417  LR: 0.000019  \n","Epoch: [1][5200/36908] Elapsed 17m 10s (remain 104m 44s) Loss: 0.0022(0.0069) Grad: 9273.0400  LR: 0.000019  \n","Epoch: [1][5300/36908] Elapsed 17m 30s (remain 104m 23s) Loss: 0.0000(0.0068) Grad: 12.2315  LR: 0.000019  \n","Epoch: [1][5400/36908] Elapsed 17m 50s (remain 104m 2s) Loss: 0.0001(0.0067) Grad: 538.4225  LR: 0.000019  \n","Epoch: [1][5500/36908] Elapsed 18m 9s (remain 103m 41s) Loss: 0.0003(0.0067) Grad: 1775.0509  LR: 0.000019  \n","Epoch: [1][5600/36908] Elapsed 18m 29s (remain 103m 20s) Loss: 0.0041(0.0066) Grad: 24316.7109  LR: 0.000019  \n","Epoch: [1][5700/36908] Elapsed 18m 48s (remain 103m 0s) Loss: 0.0000(0.0065) Grad: 172.4443  LR: 0.000019  \n","Epoch: [1][5800/36908] Elapsed 19m 8s (remain 102m 38s) Loss: 0.0000(0.0064) Grad: 216.0685  LR: 0.000019  \n","Epoch: [1][5900/36908] Elapsed 19m 28s (remain 102m 17s) Loss: 0.0000(0.0063) Grad: 11.5520  LR: 0.000019  \n","Epoch: [1][6000/36908] Elapsed 19m 47s (remain 101m 56s) Loss: 0.0000(0.0062) Grad: 189.8735  LR: 0.000019  \n","Epoch: [1][6100/36908] Elapsed 20m 7s (remain 101m 35s) Loss: 0.0002(0.0061) Grad: 1274.2220  LR: 0.000019  \n","Epoch: [1][6200/36908] Elapsed 20m 26s (remain 101m 14s) Loss: 0.0030(0.0061) Grad: 13271.1904  LR: 0.000018  \n","Epoch: [1][6300/36908] Elapsed 20m 46s (remain 100m 53s) Loss: 0.0000(0.0060) Grad: 72.1800  LR: 0.000018  \n","Epoch: [1][6400/36908] Elapsed 21m 5s (remain 100m 32s) Loss: 0.0000(0.0059) Grad: 184.1494  LR: 0.000018  \n","Epoch: [1][6500/36908] Elapsed 21m 25s (remain 100m 11s) Loss: 0.0000(0.0058) Grad: 118.8004  LR: 0.000018  \n","Epoch: [1][6600/36908] Elapsed 21m 44s (remain 99m 50s) Loss: 0.0000(0.0058) Grad: 49.7421  LR: 0.000018  \n","Epoch: [1][6700/36908] Elapsed 22m 4s (remain 99m 30s) Loss: 0.0000(0.0057) Grad: 35.6426  LR: 0.000018  \n","Epoch: [1][6800/36908] Elapsed 22m 24s (remain 99m 10s) Loss: 0.0001(0.0056) Grad: 979.1158  LR: 0.000018  \n","Epoch: [1][6900/36908] Elapsed 22m 43s (remain 98m 50s) Loss: 0.0025(0.0056) Grad: 10271.1348  LR: 0.000018  \n","Epoch: [1][7000/36908] Elapsed 23m 3s (remain 98m 29s) Loss: 0.0020(0.0055) Grad: 7610.3350  LR: 0.000018  \n","Epoch: [1][7100/36908] Elapsed 23m 22s (remain 98m 8s) Loss: 0.0087(0.0054) Grad: 34368.5469  LR: 0.000018  \n","Epoch: [1][7200/36908] Elapsed 23m 42s (remain 97m 48s) Loss: 0.0027(0.0054) Grad: 6616.8838  LR: 0.000018  \n","Epoch: [1][7300/36908] Elapsed 24m 2s (remain 97m 28s) Loss: 0.0006(0.0053) Grad: 3892.3574  LR: 0.000018  \n","Epoch: [1][7400/36908] Elapsed 24m 21s (remain 97m 7s) Loss: 0.0003(0.0053) Grad: 7998.2090  LR: 0.000018  \n","Epoch: [1][7500/36908] Elapsed 24m 41s (remain 96m 47s) Loss: 0.0001(0.0052) Grad: 858.5888  LR: 0.000018  \n","Epoch: [1][7600/36908] Elapsed 25m 0s (remain 96m 26s) Loss: 0.0001(0.0052) Grad: 1124.5122  LR: 0.000018  \n","Epoch: [1][7700/36908] Elapsed 25m 20s (remain 96m 5s) Loss: 0.0001(0.0051) Grad: 596.6878  LR: 0.000018  \n","Epoch: [1][7800/36908] Elapsed 25m 39s (remain 95m 45s) Loss: 0.0010(0.0051) Grad: 3636.3889  LR: 0.000018  \n","Epoch: [1][7900/36908] Elapsed 25m 59s (remain 95m 24s) Loss: 0.0000(0.0050) Grad: 18.4556  LR: 0.000017  \n","Epoch: [1][8000/36908] Elapsed 26m 18s (remain 95m 4s) Loss: 0.0008(0.0050) Grad: 4587.4932  LR: 0.000017  \n","Epoch: [1][8100/36908] Elapsed 26m 38s (remain 94m 43s) Loss: 0.0002(0.0049) Grad: 2060.1689  LR: 0.000017  \n","Epoch: [1][8200/36908] Elapsed 26m 57s (remain 94m 23s) Loss: 0.0036(0.0049) Grad: 11429.3408  LR: 0.000017  \n","Epoch: [1][8300/36908] Elapsed 27m 17s (remain 94m 2s) Loss: 0.0000(0.0048) Grad: 33.4935  LR: 0.000017  \n","Epoch: [1][8400/36908] Elapsed 27m 36s (remain 93m 42s) Loss: 0.0041(0.0048) Grad: 21553.1934  LR: 0.000017  \n","Epoch: [1][8500/36908] Elapsed 27m 56s (remain 93m 22s) Loss: 0.0000(0.0047) Grad: 194.5930  LR: 0.000017  \n","Epoch: [1][8600/36908] Elapsed 28m 16s (remain 93m 2s) Loss: 0.0003(0.0047) Grad: 2859.5400  LR: 0.000017  \n","Epoch: [1][8700/36908] Elapsed 28m 35s (remain 92m 41s) Loss: 0.0002(0.0046) Grad: 1741.6183  LR: 0.000017  \n","Epoch: [1][8800/36908] Elapsed 28m 55s (remain 92m 21s) Loss: 0.0004(0.0046) Grad: 4594.3452  LR: 0.000017  \n","Epoch: [1][8900/36908] Elapsed 29m 14s (remain 92m 1s) Loss: 0.0000(0.0046) Grad: 43.5739  LR: 0.000017  \n","Epoch: [1][9000/36908] Elapsed 29m 34s (remain 91m 40s) Loss: 0.0005(0.0045) Grad: 7790.4546  LR: 0.000017  \n","Epoch: [1][9100/36908] Elapsed 29m 53s (remain 91m 20s) Loss: 0.0007(0.0045) Grad: 8477.4932  LR: 0.000017  \n","Epoch: [1][9200/36908] Elapsed 30m 13s (remain 91m 0s) Loss: 0.0001(0.0045) Grad: 2916.0403  LR: 0.000017  \n","Epoch: [1][9300/36908] Elapsed 30m 32s (remain 90m 40s) Loss: 0.0452(0.0044) Grad: 53341.5859  LR: 0.000017  \n","Epoch: [1][9400/36908] Elapsed 30m 52s (remain 90m 20s) Loss: 0.0000(0.0044) Grad: 379.2598  LR: 0.000017  \n","Epoch: [1][9500/36908] Elapsed 31m 12s (remain 90m 0s) Loss: 0.0002(0.0043) Grad: 5709.5220  LR: 0.000017  \n","Epoch: [1][9600/36908] Elapsed 31m 31s (remain 89m 39s) Loss: 0.0000(0.0043) Grad: 984.1862  LR: 0.000016  \n","Epoch: [1][9700/36908] Elapsed 31m 51s (remain 89m 19s) Loss: 0.0008(0.0043) Grad: 9624.4316  LR: 0.000016  \n","Epoch: [1][9800/36908] Elapsed 32m 10s (remain 88m 59s) Loss: 0.0000(0.0043) Grad: 370.4716  LR: 0.000016  \n","Epoch: [1][9900/36908] Elapsed 32m 30s (remain 88m 39s) Loss: 0.0000(0.0042) Grad: 77.3853  LR: 0.000016  \n","Epoch: [1][10000/36908] Elapsed 32m 49s (remain 88m 19s) Loss: 0.0060(0.0042) Grad: 38298.9414  LR: 0.000016  \n","Epoch: [1][10100/36908] Elapsed 33m 9s (remain 87m 58s) Loss: 0.0000(0.0042) Grad: 95.2323  LR: 0.000016  \n","Epoch: [1][10200/36908] Elapsed 33m 28s (remain 87m 38s) Loss: 0.0000(0.0041) Grad: 218.1097  LR: 0.000016  \n","Epoch: [1][10300/36908] Elapsed 33m 48s (remain 87m 18s) Loss: 0.0000(0.0041) Grad: 153.8249  LR: 0.000016  \n","Epoch: [1][10400/36908] Elapsed 34m 7s (remain 86m 58s) Loss: 0.0000(0.0041) Grad: 21.3998  LR: 0.000016  \n","Epoch: [1][10500/36908] Elapsed 34m 27s (remain 86m 38s) Loss: 0.0006(0.0040) Grad: 9664.1836  LR: 0.000016  \n","Epoch: [1][10600/36908] Elapsed 34m 46s (remain 86m 18s) Loss: 0.0035(0.0040) Grad: 32589.4141  LR: 0.000016  \n","Epoch: [1][10700/36908] Elapsed 35m 6s (remain 85m 58s) Loss: 0.0000(0.0040) Grad: 831.8138  LR: 0.000016  \n","Epoch: [1][10800/36908] Elapsed 35m 25s (remain 85m 37s) Loss: 0.0000(0.0040) Grad: 5.3030  LR: 0.000016  \n","Epoch: [1][10900/36908] Elapsed 35m 45s (remain 85m 18s) Loss: 0.0004(0.0039) Grad: 2911.6968  LR: 0.000016  \n","Epoch: [1][11000/36908] Elapsed 36m 4s (remain 84m 57s) Loss: 0.0001(0.0039) Grad: 1322.7181  LR: 0.000016  \n","Epoch: [1][11100/36908] Elapsed 36m 24s (remain 84m 37s) Loss: 0.0000(0.0039) Grad: 36.9708  LR: 0.000016  \n","Epoch: [1][11200/36908] Elapsed 36m 43s (remain 84m 18s) Loss: 0.0000(0.0039) Grad: 26.2323  LR: 0.000015  \n","Epoch: [1][11300/36908] Elapsed 37m 3s (remain 83m 58s) Loss: 0.0000(0.0038) Grad: 6.0895  LR: 0.000015  \n","Epoch: [1][11400/36908] Elapsed 37m 22s (remain 83m 37s) Loss: 0.0000(0.0038) Grad: 461.8259  LR: 0.000015  \n","Epoch: [1][11500/36908] Elapsed 37m 42s (remain 83m 18s) Loss: 0.0000(0.0038) Grad: 127.7144  LR: 0.000015  \n","Epoch: [1][11600/36908] Elapsed 38m 1s (remain 82m 58s) Loss: 0.0000(0.0038) Grad: 3.4729  LR: 0.000015  \n","Epoch: [1][11700/36908] Elapsed 38m 21s (remain 82m 38s) Loss: 0.0016(0.0037) Grad: 10824.9160  LR: 0.000015  \n","Epoch: [1][11800/36908] Elapsed 38m 41s (remain 82m 18s) Loss: 0.0000(0.0037) Grad: 966.2150  LR: 0.000015  \n","Epoch: [1][11900/36908] Elapsed 39m 0s (remain 81m 58s) Loss: 0.0000(0.0037) Grad: 949.6468  LR: 0.000015  \n","Epoch: [1][12000/36908] Elapsed 39m 20s (remain 81m 38s) Loss: 0.0017(0.0037) Grad: 52348.2578  LR: 0.000015  \n","Epoch: [1][12100/36908] Elapsed 39m 39s (remain 81m 18s) Loss: 0.0005(0.0036) Grad: 13159.6914  LR: 0.000015  \n","Epoch: [1][12200/36908] Elapsed 39m 59s (remain 80m 58s) Loss: 0.0000(0.0036) Grad: 121.1332  LR: 0.000015  \n","Epoch: [1][12300/36908] Elapsed 40m 18s (remain 80m 38s) Loss: 0.0001(0.0036) Grad: 3355.9968  LR: 0.000015  \n","Epoch: [1][12400/36908] Elapsed 40m 38s (remain 80m 18s) Loss: 0.0000(0.0036) Grad: 86.6979  LR: 0.000015  \n","Epoch: [1][12500/36908] Elapsed 40m 57s (remain 79m 58s) Loss: 0.0000(0.0036) Grad: 86.4247  LR: 0.000015  \n","Epoch: [1][12600/36908] Elapsed 41m 17s (remain 79m 38s) Loss: 0.0000(0.0035) Grad: 272.2954  LR: 0.000015  \n","Epoch: [1][12700/36908] Elapsed 41m 36s (remain 79m 19s) Loss: 0.0000(0.0035) Grad: 9.1729  LR: 0.000015  \n","Epoch: [1][12800/36908] Elapsed 41m 56s (remain 78m 59s) Loss: 0.0088(0.0035) Grad: 148553.2344  LR: 0.000015  \n","Epoch: [1][12900/36908] Elapsed 42m 15s (remain 78m 39s) Loss: 0.0000(0.0035) Grad: 5.6629  LR: 0.000014  \n","Epoch: [1][13000/36908] Elapsed 42m 35s (remain 78m 19s) Loss: 0.0000(0.0035) Grad: 1.2492  LR: 0.000014  \n","Epoch: [1][13100/36908] Elapsed 42m 55s (remain 77m 59s) Loss: 0.0001(0.0034) Grad: 2386.1204  LR: 0.000014  \n","Epoch: [1][13200/36908] Elapsed 43m 14s (remain 77m 39s) Loss: 0.0000(0.0034) Grad: 116.7094  LR: 0.000014  \n","Epoch: [1][13300/36908] Elapsed 43m 34s (remain 77m 19s) Loss: 0.0019(0.0034) Grad: 11889.7725  LR: 0.000014  \n","Epoch: [1][13400/36908] Elapsed 43m 54s (remain 77m 0s) Loss: 0.0001(0.0034) Grad: 1114.4508  LR: 0.000014  \n","Epoch: [1][13500/36908] Elapsed 44m 13s (remain 76m 40s) Loss: 0.0033(0.0034) Grad: 27760.6270  LR: 0.000014  \n","Epoch: [1][13600/36908] Elapsed 44m 33s (remain 76m 21s) Loss: 0.0000(0.0034) Grad: 249.3401  LR: 0.000014  \n","Epoch: [1][13700/36908] Elapsed 44m 53s (remain 76m 1s) Loss: 0.0000(0.0034) Grad: 319.5657  LR: 0.000014  \n","Epoch: [1][13800/36908] Elapsed 45m 12s (remain 75m 42s) Loss: 0.0001(0.0033) Grad: 1739.0234  LR: 0.000014  \n","Epoch: [1][13900/36908] Elapsed 45m 32s (remain 75m 22s) Loss: 0.0013(0.0033) Grad: 12211.4844  LR: 0.000014  \n","Epoch: [1][14000/36908] Elapsed 45m 51s (remain 75m 2s) Loss: 0.0020(0.0033) Grad: 12659.3271  LR: 0.000014  \n","Epoch: [1][14100/36908] Elapsed 46m 11s (remain 74m 42s) Loss: 0.0009(0.0033) Grad: 12806.7520  LR: 0.000014  \n","Epoch: [1][14200/36908] Elapsed 46m 31s (remain 74m 23s) Loss: 0.0000(0.0033) Grad: 770.1628  LR: 0.000014  \n","Epoch: [1][14300/36908] Elapsed 46m 50s (remain 74m 3s) Loss: 0.0000(0.0032) Grad: 157.8787  LR: 0.000014  \n","Epoch: [1][14400/36908] Elapsed 47m 10s (remain 73m 43s) Loss: 0.0027(0.0032) Grad: 19322.6621  LR: 0.000014  \n","Epoch: [1][14500/36908] Elapsed 47m 29s (remain 73m 23s) Loss: 0.0015(0.0032) Grad: 11066.9756  LR: 0.000013  \n","Epoch: [1][14600/36908] Elapsed 47m 49s (remain 73m 4s) Loss: 0.0004(0.0032) Grad: 5949.6152  LR: 0.000013  \n","Epoch: [1][14700/36908] Elapsed 48m 9s (remain 72m 44s) Loss: 0.0023(0.0032) Grad: 25784.3633  LR: 0.000013  \n","Epoch: [1][14800/36908] Elapsed 48m 28s (remain 72m 24s) Loss: 0.0039(0.0032) Grad: 27393.1582  LR: 0.000013  \n","Epoch: [1][14900/36908] Elapsed 48m 48s (remain 72m 4s) Loss: 0.0000(0.0032) Grad: 338.5508  LR: 0.000013  \n","Epoch: [1][15000/36908] Elapsed 49m 7s (remain 71m 44s) Loss: 0.0004(0.0031) Grad: 2503.5815  LR: 0.000013  \n","Epoch: [1][15100/36908] Elapsed 49m 26s (remain 71m 24s) Loss: 0.0000(0.0031) Grad: 5.6627  LR: 0.000013  \n","Epoch: [1][15200/36908] Elapsed 49m 46s (remain 71m 4s) Loss: 0.0000(0.0031) Grad: 22.4933  LR: 0.000013  \n","Epoch: [1][15300/36908] Elapsed 50m 6s (remain 70m 45s) Loss: 0.0019(0.0031) Grad: 20598.0215  LR: 0.000013  \n","Epoch: [1][15400/36908] Elapsed 50m 25s (remain 70m 25s) Loss: 0.0000(0.0031) Grad: 654.9554  LR: 0.000013  \n","Epoch: [1][15500/36908] Elapsed 50m 45s (remain 70m 5s) Loss: 0.0000(0.0031) Grad: 36.1040  LR: 0.000013  \n","Epoch: [1][15600/36908] Elapsed 51m 4s (remain 69m 45s) Loss: 0.0000(0.0031) Grad: 1951.1383  LR: 0.000013  \n","Epoch: [1][15700/36908] Elapsed 51m 24s (remain 69m 25s) Loss: 0.0000(0.0030) Grad: 56.3366  LR: 0.000013  \n","Epoch: [1][15800/36908] Elapsed 51m 43s (remain 69m 6s) Loss: 0.0000(0.0030) Grad: 72.8176  LR: 0.000013  \n","Epoch: [1][15900/36908] Elapsed 52m 3s (remain 68m 46s) Loss: 0.0001(0.0030) Grad: 2640.3142  LR: 0.000013  \n","Epoch: [1][16000/36908] Elapsed 52m 22s (remain 68m 26s) Loss: 0.0006(0.0030) Grad: 7859.5176  LR: 0.000013  \n","Epoch: [1][16100/36908] Elapsed 52m 42s (remain 68m 6s) Loss: 0.0000(0.0030) Grad: 62.6329  LR: 0.000013  \n","Epoch: [1][16200/36908] Elapsed 53m 1s (remain 67m 46s) Loss: 0.0000(0.0030) Grad: 286.6529  LR: 0.000012  \n","Epoch: [1][16300/36908] Elapsed 53m 21s (remain 67m 27s) Loss: 0.0000(0.0030) Grad: 5.5911  LR: 0.000012  \n","Epoch: [1][16400/36908] Elapsed 53m 41s (remain 67m 7s) Loss: 0.0030(0.0029) Grad: 62870.0898  LR: 0.000012  \n","Epoch: [1][16500/36908] Elapsed 54m 0s (remain 66m 47s) Loss: 0.0000(0.0029) Grad: 10.5909  LR: 0.000012  \n","Epoch: [1][16600/36908] Elapsed 54m 20s (remain 66m 28s) Loss: 0.0000(0.0029) Grad: 35.7438  LR: 0.000012  \n","Epoch: [1][16700/36908] Elapsed 54m 39s (remain 66m 8s) Loss: 0.0005(0.0029) Grad: 5519.3550  LR: 0.000012  \n","Epoch: [1][16800/36908] Elapsed 54m 59s (remain 65m 48s) Loss: 0.0007(0.0029) Grad: 2850.1025  LR: 0.000012  \n","Epoch: [1][16900/36908] Elapsed 55m 19s (remain 65m 29s) Loss: 0.0000(0.0029) Grad: 10.7968  LR: 0.000012  \n","Epoch: [1][17000/36908] Elapsed 55m 38s (remain 65m 9s) Loss: 0.0000(0.0029) Grad: 403.2676  LR: 0.000012  \n","Epoch: [1][17100/36908] Elapsed 55m 58s (remain 64m 49s) Loss: 0.0000(0.0029) Grad: 1209.4558  LR: 0.000012  \n","Epoch: [1][17200/36908] Elapsed 56m 17s (remain 64m 29s) Loss: 0.0001(0.0029) Grad: 4478.6128  LR: 0.000012  \n","Epoch: [1][17300/36908] Elapsed 56m 37s (remain 64m 10s) Loss: 0.0000(0.0028) Grad: 22.8721  LR: 0.000012  \n","Epoch: [1][17400/36908] Elapsed 56m 57s (remain 63m 50s) Loss: 0.0000(0.0028) Grad: 512.8680  LR: 0.000012  \n","Epoch: [1][17500/36908] Elapsed 57m 16s (remain 63m 30s) Loss: 0.0013(0.0028) Grad: 14605.0479  LR: 0.000012  \n","Epoch: [1][17600/36908] Elapsed 57m 36s (remain 63m 11s) Loss: 0.0000(0.0028) Grad: 20.7274  LR: 0.000012  \n","Epoch: [1][17700/36908] Elapsed 57m 55s (remain 62m 51s) Loss: 0.0005(0.0028) Grad: 18779.8633  LR: 0.000012  \n","Epoch: [1][17800/36908] Elapsed 58m 15s (remain 62m 31s) Loss: 0.0028(0.0028) Grad: 27690.3770  LR: 0.000012  \n","Epoch: [1][17900/36908] Elapsed 58m 34s (remain 62m 11s) Loss: 0.0000(0.0028) Grad: 7.6813  LR: 0.000011  \n","Epoch: [1][18000/36908] Elapsed 58m 54s (remain 61m 52s) Loss: 0.0000(0.0028) Grad: 3.3420  LR: 0.000011  \n","Epoch: [1][18100/36908] Elapsed 59m 13s (remain 61m 32s) Loss: 0.0000(0.0028) Grad: 246.7388  LR: 0.000011  \n","Epoch: [1][18200/36908] Elapsed 59m 33s (remain 61m 12s) Loss: 0.0000(0.0027) Grad: 3.6089  LR: 0.000011  \n","Epoch: [1][18300/36908] Elapsed 59m 52s (remain 60m 53s) Loss: 0.0000(0.0027) Grad: 115.5790  LR: 0.000011  \n","Epoch: [1][18400/36908] Elapsed 60m 12s (remain 60m 33s) Loss: 0.0001(0.0027) Grad: 3622.9163  LR: 0.000011  \n","Epoch: [1][18500/36908] Elapsed 60m 32s (remain 60m 13s) Loss: 0.0031(0.0027) Grad: 76240.1094  LR: 0.000011  \n","Epoch: [1][18600/36908] Elapsed 60m 51s (remain 59m 54s) Loss: 0.0031(0.0027) Grad: 39266.7773  LR: 0.000011  \n","Epoch: [1][18700/36908] Elapsed 61m 11s (remain 59m 34s) Loss: 0.0071(0.0027) Grad: 93334.8594  LR: 0.000011  \n","Epoch: [1][18800/36908] Elapsed 61m 31s (remain 59m 14s) Loss: 0.0000(0.0027) Grad: 60.2349  LR: 0.000011  \n","Epoch: [1][18900/36908] Elapsed 61m 50s (remain 58m 55s) Loss: 0.0000(0.0027) Grad: 1161.2306  LR: 0.000011  \n","Epoch: [1][19000/36908] Elapsed 62m 10s (remain 58m 35s) Loss: 0.0000(0.0027) Grad: 40.8924  LR: 0.000011  \n","Epoch: [1][19100/36908] Elapsed 62m 29s (remain 58m 15s) Loss: 0.0015(0.0027) Grad: 63557.0781  LR: 0.000011  \n","Epoch: [1][19200/36908] Elapsed 62m 49s (remain 57m 56s) Loss: 0.0000(0.0026) Grad: 17.4777  LR: 0.000011  \n","Epoch: [1][19300/36908] Elapsed 63m 9s (remain 57m 36s) Loss: 0.0000(0.0026) Grad: 4.8366  LR: 0.000011  \n","Epoch: [1][19400/36908] Elapsed 63m 28s (remain 57m 16s) Loss: 0.0000(0.0026) Grad: 9.4163  LR: 0.000011  \n","Epoch: [1][19500/36908] Elapsed 63m 48s (remain 56m 57s) Loss: 0.0000(0.0026) Grad: 97.9583  LR: 0.000010  \n","Epoch: [1][19600/36908] Elapsed 64m 7s (remain 56m 37s) Loss: 0.0056(0.0026) Grad: 127015.0703  LR: 0.000010  \n","Epoch: [1][19700/36908] Elapsed 64m 27s (remain 56m 17s) Loss: 0.0000(0.0026) Grad: 6.1201  LR: 0.000010  \n","Epoch: [1][19800/36908] Elapsed 64m 46s (remain 55m 58s) Loss: 0.0028(0.0026) Grad: 42224.7227  LR: 0.000010  \n","Epoch: [1][19900/36908] Elapsed 65m 6s (remain 55m 38s) Loss: 0.0000(0.0026) Grad: 38.8064  LR: 0.000010  \n","Epoch: [1][20000/36908] Elapsed 65m 25s (remain 55m 18s) Loss: 0.0001(0.0026) Grad: 3877.4487  LR: 0.000010  \n","Epoch: [1][20100/36908] Elapsed 65m 45s (remain 54m 58s) Loss: 0.0019(0.0026) Grad: 16459.5645  LR: 0.000010  \n","Epoch: [1][20200/36908] Elapsed 66m 5s (remain 54m 39s) Loss: 0.0000(0.0026) Grad: 1520.1654  LR: 0.000010  \n","Epoch: [1][20300/36908] Elapsed 66m 24s (remain 54m 19s) Loss: 0.0000(0.0026) Grad: 1178.7808  LR: 0.000010  \n","Epoch: [1][20400/36908] Elapsed 66m 44s (remain 53m 59s) Loss: 0.0000(0.0026) Grad: 6.0173  LR: 0.000010  \n","Epoch: [1][20500/36908] Elapsed 67m 3s (remain 53m 40s) Loss: 0.0008(0.0025) Grad: 24187.8867  LR: 0.000010  \n","Epoch: [1][20600/36908] Elapsed 67m 23s (remain 53m 20s) Loss: 0.0020(0.0025) Grad: 39655.4727  LR: 0.000010  \n","Epoch: [1][20700/36908] Elapsed 67m 42s (remain 53m 0s) Loss: 0.0003(0.0025) Grad: 6685.5083  LR: 0.000010  \n","Epoch: [1][20800/36908] Elapsed 68m 2s (remain 52m 40s) Loss: 0.0005(0.0025) Grad: 14834.3027  LR: 0.000010  \n","Epoch: [1][20900/36908] Elapsed 68m 21s (remain 52m 21s) Loss: 0.0000(0.0025) Grad: 1068.1781  LR: 0.000010  \n","Epoch: [1][21000/36908] Elapsed 68m 41s (remain 52m 1s) Loss: 0.0022(0.0025) Grad: 39913.5469  LR: 0.000010  \n","Epoch: [1][21100/36908] Elapsed 69m 0s (remain 51m 41s) Loss: 0.0012(0.0025) Grad: 46654.3164  LR: 0.000010  \n","Epoch: [1][21200/36908] Elapsed 69m 19s (remain 51m 21s) Loss: 0.0000(0.0025) Grad: 17.9505  LR: 0.000009  \n","Epoch: [1][21300/36908] Elapsed 69m 39s (remain 51m 2s) Loss: 0.0000(0.0025) Grad: 52.4762  LR: 0.000009  \n","Epoch: [1][21400/36908] Elapsed 69m 58s (remain 50m 42s) Loss: 0.0001(0.0025) Grad: 4546.9604  LR: 0.000009  \n","Epoch: [1][21500/36908] Elapsed 70m 18s (remain 50m 22s) Loss: 0.0071(0.0025) Grad: 179448.2188  LR: 0.000009  \n","Epoch: [1][21600/36908] Elapsed 70m 37s (remain 50m 2s) Loss: 0.0015(0.0025) Grad: 17797.4922  LR: 0.000009  \n","Epoch: [1][21700/36908] Elapsed 70m 57s (remain 49m 43s) Loss: 0.0004(0.0025) Grad: 16469.7305  LR: 0.000009  \n","Epoch: [1][21800/36908] Elapsed 71m 16s (remain 49m 23s) Loss: 0.0037(0.0025) Grad: 136655.0469  LR: 0.000009  \n","Epoch: [1][21900/36908] Elapsed 71m 36s (remain 49m 3s) Loss: 0.0002(0.0024) Grad: 10452.5498  LR: 0.000009  \n","Epoch: [1][22000/36908] Elapsed 71m 56s (remain 48m 44s) Loss: 0.0040(0.0024) Grad: 256290.7188  LR: 0.000009  \n","Epoch: [1][22100/36908] Elapsed 72m 15s (remain 48m 24s) Loss: 0.0028(0.0024) Grad: 52616.0430  LR: 0.000009  \n","Epoch: [1][22200/36908] Elapsed 72m 34s (remain 48m 4s) Loss: 0.0000(0.0024) Grad: 25.0777  LR: 0.000009  \n","Epoch: [1][22300/36908] Elapsed 72m 54s (remain 47m 45s) Loss: 0.0000(0.0024) Grad: 30.6412  LR: 0.000009  \n","Epoch: [1][22400/36908] Elapsed 73m 13s (remain 47m 25s) Loss: 0.0049(0.0024) Grad: 73514.3750  LR: 0.000009  \n","Epoch: [1][22500/36908] Elapsed 73m 33s (remain 47m 5s) Loss: 0.0000(0.0024) Grad: 8.1321  LR: 0.000009  \n","Epoch: [1][22600/36908] Elapsed 73m 52s (remain 46m 46s) Loss: 0.0000(0.0024) Grad: 21.1537  LR: 0.000009  \n","Epoch: [1][22700/36908] Elapsed 74m 12s (remain 46m 26s) Loss: 0.0000(0.0024) Grad: 117.5308  LR: 0.000009  \n","Epoch: [1][22800/36908] Elapsed 74m 32s (remain 46m 7s) Loss: 0.0006(0.0024) Grad: 35941.1133  LR: 0.000008  \n","Epoch: [1][22900/36908] Elapsed 74m 51s (remain 45m 47s) Loss: 0.0000(0.0024) Grad: 32.9574  LR: 0.000008  \n","Epoch: [1][23000/36908] Elapsed 75m 11s (remain 45m 27s) Loss: 0.0000(0.0024) Grad: 198.9984  LR: 0.000008  \n","Epoch: [1][23100/36908] Elapsed 75m 30s (remain 45m 8s) Loss: 0.0018(0.0024) Grad: 120334.8281  LR: 0.000008  \n","Epoch: [1][23200/36908] Elapsed 75m 50s (remain 44m 48s) Loss: 0.0092(0.0024) Grad: 67792.2578  LR: 0.000008  \n","Epoch: [1][23300/36908] Elapsed 76m 9s (remain 44m 28s) Loss: 0.0007(0.0023) Grad: 11366.9961  LR: 0.000008  \n","Epoch: [1][23400/36908] Elapsed 76m 29s (remain 44m 8s) Loss: 0.0007(0.0023) Grad: 33255.0820  LR: 0.000008  \n","Epoch: [1][23500/36908] Elapsed 76m 48s (remain 43m 49s) Loss: 0.0000(0.0023) Grad: 339.7587  LR: 0.000008  \n","Epoch: [1][23600/36908] Elapsed 77m 8s (remain 43m 29s) Loss: 0.0000(0.0023) Grad: 649.7646  LR: 0.000008  \n","Epoch: [1][23700/36908] Elapsed 77m 27s (remain 43m 9s) Loss: 0.0015(0.0023) Grad: 17273.9043  LR: 0.000008  \n","Epoch: [1][23800/36908] Elapsed 77m 47s (remain 42m 50s) Loss: 0.0000(0.0023) Grad: 264.6412  LR: 0.000008  \n","Epoch: [1][23900/36908] Elapsed 78m 6s (remain 42m 30s) Loss: 0.0000(0.0023) Grad: 38.4954  LR: 0.000008  \n","Epoch: [1][24000/36908] Elapsed 78m 26s (remain 42m 10s) Loss: 0.0000(0.0023) Grad: 11.5375  LR: 0.000008  \n","Epoch: [1][24100/36908] Elapsed 78m 45s (remain 41m 51s) Loss: 0.0000(0.0023) Grad: 11.9479  LR: 0.000008  \n","Epoch: [1][24200/36908] Elapsed 79m 5s (remain 41m 31s) Loss: 0.0026(0.0023) Grad: 16997.4668  LR: 0.000008  \n","Epoch: [1][24300/36908] Elapsed 79m 24s (remain 41m 11s) Loss: 0.0001(0.0023) Grad: 3077.9233  LR: 0.000008  \n","Epoch: [1][24400/36908] Elapsed 79m 44s (remain 40m 52s) Loss: 0.0000(0.0023) Grad: 37.3478  LR: 0.000008  \n","Epoch: [1][24500/36908] Elapsed 80m 3s (remain 40m 32s) Loss: 0.0000(0.0023) Grad: 448.7146  LR: 0.000007  \n","Epoch: [1][24600/36908] Elapsed 80m 23s (remain 40m 13s) Loss: 0.0000(0.0023) Grad: 19.7625  LR: 0.000007  \n","Epoch: [1][24700/36908] Elapsed 80m 43s (remain 39m 53s) Loss: 0.0000(0.0023) Grad: 14.2572  LR: 0.000007  \n","Epoch: [1][24800/36908] Elapsed 81m 2s (remain 39m 33s) Loss: 0.0000(0.0023) Grad: 21.7100  LR: 0.000007  \n","Epoch: [1][24900/36908] Elapsed 81m 22s (remain 39m 14s) Loss: 0.0000(0.0022) Grad: 21.9255  LR: 0.000007  \n","Epoch: [1][25000/36908] Elapsed 81m 41s (remain 38m 54s) Loss: 0.0000(0.0022) Grad: 65.8353  LR: 0.000007  \n","Epoch: [1][25100/36908] Elapsed 82m 1s (remain 38m 34s) Loss: 0.0009(0.0022) Grad: 25045.2637  LR: 0.000007  \n","Epoch: [1][25200/36908] Elapsed 82m 20s (remain 38m 15s) Loss: 0.0001(0.0022) Grad: 9429.3789  LR: 0.000007  \n","Epoch: [1][25300/36908] Elapsed 82m 40s (remain 37m 55s) Loss: 0.0000(0.0022) Grad: 124.9821  LR: 0.000007  \n","Epoch: [1][25400/36908] Elapsed 82m 59s (remain 37m 35s) Loss: 0.0000(0.0022) Grad: 20.8151  LR: 0.000007  \n","Epoch: [1][25500/36908] Elapsed 83m 19s (remain 37m 16s) Loss: 0.0001(0.0022) Grad: 7720.1191  LR: 0.000007  \n","Epoch: [1][25600/36908] Elapsed 83m 38s (remain 36m 56s) Loss: 0.0000(0.0022) Grad: 13.1763  LR: 0.000007  \n","Epoch: [1][25700/36908] Elapsed 83m 58s (remain 36m 37s) Loss: 0.0000(0.0022) Grad: 12.0901  LR: 0.000007  \n","Epoch: [1][25800/36908] Elapsed 84m 18s (remain 36m 17s) Loss: 0.0007(0.0022) Grad: 39763.8906  LR: 0.000007  \n","Epoch: [1][25900/36908] Elapsed 84m 37s (remain 35m 57s) Loss: 0.0000(0.0022) Grad: 31.3022  LR: 0.000007  \n","Epoch: [1][26000/36908] Elapsed 84m 57s (remain 35m 38s) Loss: 0.0000(0.0022) Grad: 26.8051  LR: 0.000007  \n","Epoch: [1][26100/36908] Elapsed 85m 16s (remain 35m 18s) Loss: 0.0000(0.0022) Grad: 23.0819  LR: 0.000007  \n","Epoch: [1][26200/36908] Elapsed 85m 36s (remain 34m 59s) Loss: 0.0000(0.0022) Grad: 1569.8201  LR: 0.000006  \n","Epoch: [1][26300/36908] Elapsed 85m 56s (remain 34m 39s) Loss: 0.0000(0.0022) Grad: 11.7587  LR: 0.000006  \n","Epoch: [1][26400/36908] Elapsed 86m 15s (remain 34m 19s) Loss: 0.0000(0.0022) Grad: 17.5865  LR: 0.000006  \n","Epoch: [1][26500/36908] Elapsed 86m 35s (remain 34m 0s) Loss: 0.0004(0.0022) Grad: 11286.4834  LR: 0.000006  \n","Epoch: [1][26600/36908] Elapsed 86m 54s (remain 33m 40s) Loss: 0.0001(0.0022) Grad: 3376.7864  LR: 0.000006  \n","Epoch: [1][26700/36908] Elapsed 87m 14s (remain 33m 20s) Loss: 0.0000(0.0021) Grad: 10.6889  LR: 0.000006  \n","Epoch: [1][26800/36908] Elapsed 87m 33s (remain 33m 1s) Loss: 0.0001(0.0021) Grad: 5197.2236  LR: 0.000006  \n","Epoch: [1][26900/36908] Elapsed 87m 53s (remain 32m 41s) Loss: 0.0096(0.0021) Grad: 19397.6465  LR: 0.000006  \n","Epoch: [1][27000/36908] Elapsed 88m 12s (remain 32m 22s) Loss: 0.0000(0.0021) Grad: 192.6252  LR: 0.000006  \n","Epoch: [1][27100/36908] Elapsed 88m 32s (remain 32m 2s) Loss: 0.0002(0.0021) Grad: 7581.4146  LR: 0.000006  \n","Epoch: [1][27200/36908] Elapsed 88m 52s (remain 31m 42s) Loss: 0.0000(0.0021) Grad: 358.6619  LR: 0.000006  \n","Epoch: [1][27300/36908] Elapsed 89m 11s (remain 31m 23s) Loss: 0.0000(0.0021) Grad: 258.4761  LR: 0.000006  \n","Epoch: [1][27400/36908] Elapsed 89m 31s (remain 31m 3s) Loss: 0.0001(0.0021) Grad: 2965.0969  LR: 0.000006  \n","Epoch: [1][27500/36908] Elapsed 89m 51s (remain 30m 44s) Loss: 0.0000(0.0021) Grad: 11.0972  LR: 0.000006  \n","Epoch: [1][27600/36908] Elapsed 90m 10s (remain 30m 24s) Loss: 0.0000(0.0021) Grad: 44.8988  LR: 0.000006  \n","Epoch: [1][27700/36908] Elapsed 90m 30s (remain 30m 4s) Loss: 0.0008(0.0021) Grad: 4787.3291  LR: 0.000006  \n","Epoch: [1][27800/36908] Elapsed 90m 49s (remain 29m 45s) Loss: 0.0009(0.0021) Grad: 22638.6309  LR: 0.000005  \n","Epoch: [1][27900/36908] Elapsed 91m 9s (remain 29m 25s) Loss: 0.0000(0.0021) Grad: 497.4417  LR: 0.000005  \n","Epoch: [1][28000/36908] Elapsed 91m 29s (remain 29m 6s) Loss: 0.0001(0.0021) Grad: 4291.8564  LR: 0.000005  \n","Epoch: [1][28100/36908] Elapsed 91m 48s (remain 28m 46s) Loss: 0.0000(0.0021) Grad: 212.9771  LR: 0.000005  \n","Epoch: [1][28200/36908] Elapsed 92m 8s (remain 28m 26s) Loss: 0.0000(0.0021) Grad: 15.3871  LR: 0.000005  \n","Epoch: [1][28300/36908] Elapsed 92m 27s (remain 28m 7s) Loss: 0.0000(0.0021) Grad: 6.1672  LR: 0.000005  \n","Epoch: [1][28400/36908] Elapsed 92m 47s (remain 27m 47s) Loss: 0.0000(0.0021) Grad: 21.8724  LR: 0.000005  \n","Epoch: [1][28500/36908] Elapsed 93m 7s (remain 27m 28s) Loss: 0.0002(0.0021) Grad: 3030.3958  LR: 0.000005  \n","Epoch: [1][28600/36908] Elapsed 93m 26s (remain 27m 8s) Loss: 0.0000(0.0021) Grad: 22.8595  LR: 0.000005  \n","Epoch: [1][28700/36908] Elapsed 93m 46s (remain 26m 48s) Loss: 0.0001(0.0021) Grad: 1561.1367  LR: 0.000005  \n","Epoch: [1][28800/36908] Elapsed 94m 5s (remain 26m 29s) Loss: 0.0000(0.0021) Grad: 8.3221  LR: 0.000005  \n","Epoch: [1][28900/36908] Elapsed 94m 25s (remain 26m 9s) Loss: 0.0403(0.0020) Grad: 94591.4688  LR: 0.000005  \n","Epoch: [1][29000/36908] Elapsed 94m 44s (remain 25m 49s) Loss: 0.0001(0.0020) Grad: 932.8566  LR: 0.000005  \n","Epoch: [1][29100/36908] Elapsed 95m 4s (remain 25m 30s) Loss: 0.0014(0.0020) Grad: 8600.9756  LR: 0.000005  \n","Epoch: [1][29200/36908] Elapsed 95m 23s (remain 25m 10s) Loss: 0.0004(0.0020) Grad: 6518.3164  LR: 0.000005  \n","Epoch: [1][29300/36908] Elapsed 95m 43s (remain 24m 51s) Loss: 0.0010(0.0020) Grad: 26127.8672  LR: 0.000005  \n","Epoch: [1][29400/36908] Elapsed 96m 2s (remain 24m 31s) Loss: 0.0033(0.0020) Grad: 11234.6738  LR: 0.000005  \n","Epoch: [1][29500/36908] Elapsed 96m 22s (remain 24m 11s) Loss: 0.0000(0.0020) Grad: 254.6279  LR: 0.000004  \n","Epoch: [1][29600/36908] Elapsed 96m 41s (remain 23m 52s) Loss: 0.0000(0.0020) Grad: 38.2594  LR: 0.000004  \n","Epoch: [1][29700/36908] Elapsed 97m 1s (remain 23m 32s) Loss: 0.0002(0.0020) Grad: 4936.8037  LR: 0.000004  \n","Epoch: [1][29800/36908] Elapsed 97m 20s (remain 23m 12s) Loss: 0.0001(0.0020) Grad: 2365.0952  LR: 0.000004  \n","Epoch: [1][29900/36908] Elapsed 97m 40s (remain 22m 53s) Loss: 0.0000(0.0020) Grad: 106.8596  LR: 0.000004  \n","Epoch: [1][30000/36908] Elapsed 98m 0s (remain 22m 33s) Loss: 0.0053(0.0020) Grad: 25941.6875  LR: 0.000004  \n","Epoch: [1][30100/36908] Elapsed 98m 19s (remain 22m 14s) Loss: 0.0000(0.0020) Grad: 29.2387  LR: 0.000004  \n","Epoch: [1][30200/36908] Elapsed 98m 39s (remain 21m 54s) Loss: 0.0000(0.0020) Grad: 747.4267  LR: 0.000004  \n","Epoch: [1][30300/36908] Elapsed 98m 58s (remain 21m 34s) Loss: 0.0009(0.0020) Grad: 23266.4648  LR: 0.000004  \n","Epoch: [1][30400/36908] Elapsed 99m 18s (remain 21m 15s) Loss: 0.0000(0.0020) Grad: 237.7401  LR: 0.000004  \n","Epoch: [1][30500/36908] Elapsed 99m 37s (remain 20m 55s) Loss: 0.0003(0.0020) Grad: 5889.0454  LR: 0.000004  \n","Epoch: [1][30600/36908] Elapsed 99m 57s (remain 20m 36s) Loss: 0.0000(0.0020) Grad: 55.1324  LR: 0.000004  \n","Epoch: [1][30700/36908] Elapsed 100m 16s (remain 20m 16s) Loss: 0.0008(0.0020) Grad: 10140.2432  LR: 0.000004  \n","Epoch: [1][30800/36908] Elapsed 100m 36s (remain 19m 56s) Loss: 0.0020(0.0020) Grad: 8569.1963  LR: 0.000004  \n","Epoch: [1][30900/36908] Elapsed 100m 55s (remain 19m 37s) Loss: 0.0012(0.0020) Grad: 18250.2227  LR: 0.000004  \n","Epoch: [1][31000/36908] Elapsed 101m 15s (remain 19m 17s) Loss: 0.0000(0.0020) Grad: 15.6230  LR: 0.000004  \n","Epoch: [1][31100/36908] Elapsed 101m 35s (remain 18m 58s) Loss: 0.0007(0.0020) Grad: 16738.4219  LR: 0.000003  \n","Epoch: [1][31200/36908] Elapsed 101m 54s (remain 18m 38s) Loss: 0.0000(0.0020) Grad: 3234.0554  LR: 0.000003  \n","Epoch: [1][31300/36908] Elapsed 102m 14s (remain 18m 18s) Loss: 0.0000(0.0020) Grad: 10.5860  LR: 0.000003  \n","Epoch: [1][31400/36908] Elapsed 102m 33s (remain 17m 59s) Loss: 0.0000(0.0020) Grad: 15.2547  LR: 0.000003  \n","Epoch: [1][31500/36908] Elapsed 102m 53s (remain 17m 39s) Loss: 0.0000(0.0019) Grad: 49.3678  LR: 0.000003  \n","Epoch: [1][31600/36908] Elapsed 103m 12s (remain 17m 19s) Loss: 0.0012(0.0019) Grad: 21729.1973  LR: 0.000003  \n","Epoch: [1][31700/36908] Elapsed 103m 32s (remain 17m 0s) Loss: 0.0000(0.0019) Grad: 69.2184  LR: 0.000003  \n","Epoch: [1][31800/36908] Elapsed 103m 51s (remain 16m 40s) Loss: 0.0000(0.0019) Grad: 13.5967  LR: 0.000003  \n","Epoch: [1][31900/36908] Elapsed 104m 11s (remain 16m 21s) Loss: 0.0005(0.0019) Grad: 20855.2129  LR: 0.000003  \n","Epoch: [1][32000/36908] Elapsed 104m 30s (remain 16m 1s) Loss: 0.0021(0.0019) Grad: 28527.0547  LR: 0.000003  \n","Epoch: [1][32100/36908] Elapsed 104m 50s (remain 15m 41s) Loss: 0.0001(0.0019) Grad: 4226.6011  LR: 0.000003  \n","Epoch: [1][32200/36908] Elapsed 105m 10s (remain 15m 22s) Loss: 0.0002(0.0019) Grad: 6705.2593  LR: 0.000003  \n","Epoch: [1][32300/36908] Elapsed 105m 29s (remain 15m 2s) Loss: 0.0007(0.0019) Grad: 13092.4424  LR: 0.000003  \n","Epoch: [1][32400/36908] Elapsed 105m 49s (remain 14m 43s) Loss: 0.0000(0.0019) Grad: 1255.0112  LR: 0.000003  \n","Epoch: [1][32500/36908] Elapsed 106m 8s (remain 14m 23s) Loss: 0.0000(0.0019) Grad: 33.5726  LR: 0.000003  \n","Epoch: [1][32600/36908] Elapsed 106m 28s (remain 14m 3s) Loss: 0.0034(0.0019) Grad: 77559.6484  LR: 0.000003  \n","Epoch: [1][32700/36908] Elapsed 106m 48s (remain 13m 44s) Loss: 0.0034(0.0019) Grad: 53909.2539  LR: 0.000003  \n","Epoch: [1][32800/36908] Elapsed 107m 7s (remain 13m 24s) Loss: 0.0000(0.0019) Grad: 6.5024  LR: 0.000002  \n","Epoch: [1][32900/36908] Elapsed 107m 27s (remain 13m 5s) Loss: 0.0000(0.0019) Grad: 3187.5903  LR: 0.000002  \n","Epoch: [1][33000/36908] Elapsed 107m 46s (remain 12m 45s) Loss: 0.0000(0.0019) Grad: 34.8076  LR: 0.000002  \n","Epoch: [1][33100/36908] Elapsed 108m 6s (remain 12m 26s) Loss: 0.0001(0.0019) Grad: 4590.7158  LR: 0.000002  \n","Epoch: [1][33200/36908] Elapsed 108m 26s (remain 12m 6s) Loss: 0.0016(0.0019) Grad: 48313.6250  LR: 0.000002  \n","Epoch: [1][33300/36908] Elapsed 108m 45s (remain 11m 46s) Loss: 0.0000(0.0019) Grad: 133.2959  LR: 0.000002  \n","Epoch: [1][33400/36908] Elapsed 109m 5s (remain 11m 27s) Loss: 0.0029(0.0019) Grad: 38720.3477  LR: 0.000002  \n","Epoch: [1][33500/36908] Elapsed 109m 24s (remain 11m 7s) Loss: 0.0000(0.0019) Grad: 113.6153  LR: 0.000002  \n","Epoch: [1][33600/36908] Elapsed 109m 44s (remain 10m 48s) Loss: 0.0011(0.0019) Grad: 33938.4609  LR: 0.000002  \n","Epoch: [1][33700/36908] Elapsed 110m 4s (remain 10m 28s) Loss: 0.0000(0.0019) Grad: 59.0576  LR: 0.000002  \n","Epoch: [1][33800/36908] Elapsed 110m 23s (remain 10m 8s) Loss: 0.0000(0.0019) Grad: 40.9133  LR: 0.000002  \n","Epoch: [1][33900/36908] Elapsed 110m 43s (remain 9m 49s) Loss: 0.0000(0.0019) Grad: 36.3308  LR: 0.000002  \n","Epoch: [1][34000/36908] Elapsed 111m 2s (remain 9m 29s) Loss: 0.0019(0.0019) Grad: 27748.5742  LR: 0.000002  \n","Epoch: [1][34100/36908] Elapsed 111m 22s (remain 9m 10s) Loss: 0.0000(0.0019) Grad: 85.8507  LR: 0.000002  \n","Epoch: [1][34200/36908] Elapsed 111m 42s (remain 8m 50s) Loss: 0.0000(0.0018) Grad: 93.7718  LR: 0.000002  \n","Epoch: [1][34300/36908] Elapsed 112m 1s (remain 8m 30s) Loss: 0.0000(0.0018) Grad: 44.2878  LR: 0.000002  \n","Epoch: [1][34400/36908] Elapsed 112m 21s (remain 8m 11s) Loss: 0.0000(0.0018) Grad: 4.1010  LR: 0.000002  \n","Epoch: [1][34500/36908] Elapsed 112m 40s (remain 7m 51s) Loss: 0.0000(0.0018) Grad: 139.8230  LR: 0.000001  \n","Epoch: [1][34600/36908] Elapsed 113m 0s (remain 7m 32s) Loss: 0.0000(0.0018) Grad: 3.4683  LR: 0.000001  \n","Epoch: [1][34700/36908] Elapsed 113m 20s (remain 7m 12s) Loss: 0.0011(0.0018) Grad: 21878.3555  LR: 0.000001  \n","Epoch: [1][34800/36908] Elapsed 113m 39s (remain 6m 52s) Loss: 0.0000(0.0018) Grad: 5.9557  LR: 0.000001  \n","Epoch: [1][34900/36908] Elapsed 113m 59s (remain 6m 33s) Loss: 0.0000(0.0018) Grad: 14.6095  LR: 0.000001  \n","Epoch: [1][35000/36908] Elapsed 114m 18s (remain 6m 13s) Loss: 0.0013(0.0018) Grad: 274024.8438  LR: 0.000001  \n","Epoch: [1][35100/36908] Elapsed 114m 38s (remain 5m 54s) Loss: 0.0144(0.0018) Grad: inf  LR: 0.000001  \n","Epoch: [1][35200/36908] Elapsed 114m 58s (remain 5m 34s) Loss: 0.0125(0.0018) Grad: 126088.6562  LR: 0.000001  \n","Epoch: [1][35300/36908] Elapsed 115m 18s (remain 5m 14s) Loss: 0.0014(0.0018) Grad: 34046.1523  LR: 0.000001  \n","Epoch: [1][35400/36908] Elapsed 115m 37s (remain 4m 55s) Loss: 0.0000(0.0018) Grad: 79.2358  LR: 0.000001  \n","Epoch: [1][35500/36908] Elapsed 115m 57s (remain 4m 35s) Loss: 0.0001(0.0018) Grad: 3570.4590  LR: 0.000001  \n","Epoch: [1][35600/36908] Elapsed 116m 16s (remain 4m 16s) Loss: 0.0000(0.0018) Grad: 189.2559  LR: 0.000001  \n","Epoch: [1][35700/36908] Elapsed 116m 36s (remain 3m 56s) Loss: 0.0007(0.0018) Grad: 17359.0840  LR: 0.000001  \n","Epoch: [1][35800/36908] Elapsed 116m 55s (remain 3m 36s) Loss: 0.0025(0.0018) Grad: 14412.4619  LR: 0.000001  \n","Epoch: [1][35900/36908] Elapsed 117m 15s (remain 3m 17s) Loss: 0.0007(0.0018) Grad: 34365.8047  LR: 0.000001  \n","Epoch: [1][36000/36908] Elapsed 117m 34s (remain 2m 57s) Loss: 0.0025(0.0018) Grad: 72556.1172  LR: 0.000001  \n","Epoch: [1][36100/36908] Elapsed 117m 54s (remain 2m 38s) Loss: 0.0000(0.0018) Grad: 12.9335  LR: 0.000000  \n","Epoch: [1][36200/36908] Elapsed 118m 13s (remain 2m 18s) Loss: 0.0015(0.0018) Grad: 17653.9941  LR: 0.000000  \n","Epoch: [1][36300/36908] Elapsed 118m 33s (remain 1m 58s) Loss: 0.0001(0.0018) Grad: 2343.0059  LR: 0.000000  \n","Epoch: [1][36400/36908] Elapsed 118m 53s (remain 1m 39s) Loss: 0.0013(0.0018) Grad: 36809.5977  LR: 0.000000  \n","Epoch: [1][36500/36908] Elapsed 119m 12s (remain 1m 19s) Loss: 0.0000(0.0018) Grad: 234.0820  LR: 0.000000  \n","Epoch: [1][36600/36908] Elapsed 119m 32s (remain 1m 0s) Loss: 0.0000(0.0018) Grad: 1448.9628  LR: 0.000000  \n","Epoch: [1][36700/36908] Elapsed 119m 51s (remain 0m 40s) Loss: 0.0000(0.0018) Grad: 182.3494  LR: 0.000000  \n","Epoch: [1][36800/36908] Elapsed 120m 11s (remain 0m 20s) Loss: 0.0000(0.0018) Grad: 54.3624  LR: 0.000000  \n","Epoch: [1][36900/36908] Elapsed 120m 30s (remain 0m 1s) Loss: 0.0000(0.0018) Grad: 8.2092  LR: 0.000000  \n","Epoch: [1][36907/36908] Elapsed 120m 32s (remain 0m 0s) Loss: 0.0005(0.0018) Grad: 19190.0000  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 10m 45s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 6s (remain 1m 9s) Loss: 0.0208(0.0043) \n","EVAL: [200/1192] Elapsed 0m 12s (remain 1m 0s) Loss: 0.0071(0.0039) \n","EVAL: [300/1192] Elapsed 0m 18s (remain 0m 53s) Loss: 0.0086(0.0036) \n","EVAL: [400/1192] Elapsed 0m 23s (remain 0m 47s) Loss: 0.0000(0.0038) \n","EVAL: [500/1192] Elapsed 0m 29s (remain 0m 41s) Loss: 0.0000(0.0035) \n","EVAL: [600/1192] Elapsed 0m 35s (remain 0m 35s) Loss: 0.0024(0.0035) \n","EVAL: [700/1192] Elapsed 0m 41s (remain 0m 29s) Loss: 0.0041(0.0040) \n","EVAL: [800/1192] Elapsed 0m 47s (remain 0m 23s) Loss: 0.0000(0.0039) \n","EVAL: [900/1192] Elapsed 0m 53s (remain 0m 17s) Loss: 0.0059(0.0041) \n","EVAL: [1000/1192] Elapsed 0m 59s (remain 0m 11s) Loss: 0.0000(0.0040) \n","EVAL: [1100/1192] Elapsed 1m 5s (remain 0m 5s) Loss: 0.0189(0.0038) \n","EVAL: [1191/1192] Elapsed 1m 10s (remain 0m 0s) Loss: 0.0000(0.0037) \n","Epoch 1 - avg_train_loss: 0.0018  avg_val_loss: 0.0037  time: 7308s\n","Epoch 1 - Score: 0.8893\n","Epoch 1 - Save Best Score: 0.8893 Model\n","========== fold: 3 training ==========\n","get pseudo plain from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n","get pseudo labels from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_3.npy\n","get pseudo labels from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_3.npy\n","get pseudo labels from ./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp083/pseudo_labels_3.npy\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/612602 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2b0099e5324495893453c16cec39104"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/612602 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a284a67098574d22a5ce1e2457757f7d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["(612602, 8)\n","(100000, 9)\n","(110725, 11)\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\n","Epoch: [1][0/36908] Elapsed 0m 0s (remain 473m 15s) Loss: 0.0622(0.0622) Grad: 134841.5000  LR: 0.000000  \n","Epoch: [1][100/36908] Elapsed 0m 20s (remain 126m 20s) Loss: 0.0417(0.0556) Grad: 95267.2109  LR: 0.000001  \n","Epoch: [1][200/36908] Elapsed 0m 40s (remain 122m 34s) Loss: 0.0139(0.0449) Grad: 34693.6289  LR: 0.000001  \n","Epoch: [1][300/36908] Elapsed 0m 59s (remain 121m 10s) Loss: 0.0062(0.0355) Grad: 7757.9370  LR: 0.000002  \n","Epoch: [1][400/36908] Elapsed 1m 19s (remain 120m 23s) Loss: 0.0214(0.0300) Grad: 19125.1328  LR: 0.000002  \n","Epoch: [1][500/36908] Elapsed 1m 38s (remain 119m 42s) Loss: 0.0062(0.0265) Grad: 7724.3525  LR: 0.000003  \n","Epoch: [1][600/36908] Elapsed 1m 58s (remain 119m 3s) Loss: 0.0119(0.0242) Grad: 9479.1504  LR: 0.000003  \n","Epoch: [1][700/36908] Elapsed 2m 17s (remain 118m 39s) Loss: 0.0097(0.0224) Grad: 7418.0488  LR: 0.000004  \n","Epoch: [1][800/36908] Elapsed 2m 37s (remain 118m 15s) Loss: 0.0161(0.0210) Grad: 12766.8350  LR: 0.000004  \n","Epoch: [1][900/36908] Elapsed 2m 56s (remain 117m 50s) Loss: 0.0261(0.0200) Grad: 19214.0996  LR: 0.000005  \n","Epoch: [1][1000/36908] Elapsed 3m 16s (remain 117m 24s) Loss: 0.0090(0.0191) Grad: 7870.9790  LR: 0.000005  \n","Epoch: [1][1100/36908] Elapsed 3m 35s (remain 117m 2s) Loss: 0.0067(0.0182) Grad: 6710.5894  LR: 0.000006  \n","Epoch: [1][1200/36908] Elapsed 3m 55s (remain 116m 41s) Loss: 0.0185(0.0175) Grad: 15766.2285  LR: 0.000007  \n","Epoch: [1][1300/36908] Elapsed 4m 15s (remain 116m 20s) Loss: 0.0018(0.0169) Grad: 3676.9436  LR: 0.000007  \n","Epoch: [1][1400/36908] Elapsed 4m 34s (remain 115m 59s) Loss: 0.0009(0.0162) Grad: 3786.4248  LR: 0.000008  \n","Epoch: [1][1500/36908] Elapsed 4m 54s (remain 115m 36s) Loss: 0.0009(0.0154) Grad: 2667.9182  LR: 0.000008  \n","Epoch: [1][1600/36908] Elapsed 5m 13s (remain 115m 18s) Loss: 0.0012(0.0148) Grad: 4815.8979  LR: 0.000009  \n","Epoch: [1][1700/36908] Elapsed 5m 33s (remain 115m 0s) Loss: 0.0005(0.0141) Grad: 2712.3079  LR: 0.000009  \n","Epoch: [1][1800/36908] Elapsed 5m 52s (remain 114m 37s) Loss: 0.0002(0.0135) Grad: 2290.6755  LR: 0.000010  \n","Epoch: [1][1900/36908] Elapsed 6m 12s (remain 114m 16s) Loss: 0.0001(0.0130) Grad: 558.0251  LR: 0.000010  \n","Epoch: [1][2000/36908] Elapsed 6m 31s (remain 113m 56s) Loss: 0.0028(0.0125) Grad: 9222.2266  LR: 0.000011  \n","Epoch: [1][2100/36908] Elapsed 6m 51s (remain 113m 35s) Loss: 0.0007(0.0121) Grad: 2784.8022  LR: 0.000011  \n","Epoch: [1][2200/36908] Elapsed 7m 10s (remain 113m 15s) Loss: 0.0001(0.0116) Grad: 429.8077  LR: 0.000012  \n","Epoch: [1][2300/36908] Elapsed 7m 30s (remain 112m 55s) Loss: 0.0000(0.0112) Grad: 215.2786  LR: 0.000012  \n","Epoch: [1][2400/36908] Elapsed 7m 50s (remain 112m 35s) Loss: 0.0081(0.0109) Grad: 16304.6279  LR: 0.000013  \n","Epoch: [1][2500/36908] Elapsed 8m 9s (remain 112m 14s) Loss: 0.0013(0.0106) Grad: 5485.1948  LR: 0.000014  \n","Epoch: [1][2600/36908] Elapsed 8m 29s (remain 111m 54s) Loss: 0.0002(0.0103) Grad: 813.8797  LR: 0.000014  \n","Epoch: [1][2700/36908] Elapsed 8m 48s (remain 111m 33s) Loss: 0.0091(0.0100) Grad: 28403.7383  LR: 0.000015  \n","Epoch: [1][2800/36908] Elapsed 9m 7s (remain 111m 11s) Loss: 0.0010(0.0097) Grad: 6972.8506  LR: 0.000015  \n","Epoch: [1][2900/36908] Elapsed 9m 27s (remain 110m 51s) Loss: 0.0018(0.0095) Grad: 6586.5317  LR: 0.000016  \n","Epoch: [1][3000/36908] Elapsed 9m 46s (remain 110m 31s) Loss: 0.0021(0.0092) Grad: 6816.6221  LR: 0.000016  \n","Epoch: [1][3100/36908] Elapsed 10m 6s (remain 110m 11s) Loss: 0.0043(0.0090) Grad: 25178.4922  LR: 0.000017  \n","Epoch: [1][3200/36908] Elapsed 10m 25s (remain 109m 51s) Loss: 0.0004(0.0088) Grad: 1833.1965  LR: 0.000017  \n","Epoch: [1][3300/36908] Elapsed 10m 45s (remain 109m 31s) Loss: 0.0005(0.0086) Grad: 6695.6729  LR: 0.000018  \n","Epoch: [1][3400/36908] Elapsed 11m 5s (remain 109m 12s) Loss: 0.0079(0.0084) Grad: 38433.0469  LR: 0.000018  \n","Epoch: [1][3500/36908] Elapsed 11m 24s (remain 108m 52s) Loss: 0.0001(0.0082) Grad: 801.1147  LR: 0.000019  \n","Epoch: [1][3600/36908] Elapsed 11m 44s (remain 108m 32s) Loss: 0.0023(0.0080) Grad: 17499.8730  LR: 0.000020  \n","Epoch: [1][3700/36908] Elapsed 12m 3s (remain 108m 13s) Loss: 0.0117(0.0079) Grad: 57932.5625  LR: 0.000020  \n","Epoch: [1][3800/36908] Elapsed 12m 23s (remain 107m 54s) Loss: 0.0000(0.0077) Grad: 217.2717  LR: 0.000020  \n","Epoch: [1][3900/36908] Elapsed 12m 42s (remain 107m 34s) Loss: 0.0000(0.0076) Grad: 83.4727  LR: 0.000020  \n","Epoch: [1][4000/36908] Elapsed 13m 2s (remain 107m 13s) Loss: 0.0000(0.0074) Grad: 338.3198  LR: 0.000020  \n","Epoch: [1][4100/36908] Elapsed 13m 21s (remain 106m 54s) Loss: 0.0000(0.0073) Grad: 291.0420  LR: 0.000020  \n","Epoch: [1][4200/36908] Elapsed 13m 41s (remain 106m 34s) Loss: 0.0056(0.0072) Grad: 22375.1406  LR: 0.000020  \n","Epoch: [1][4300/36908] Elapsed 14m 1s (remain 106m 15s) Loss: 0.0002(0.0070) Grad: 7524.8154  LR: 0.000020  \n","Epoch: [1][4400/36908] Elapsed 14m 20s (remain 105m 56s) Loss: 0.0005(0.0069) Grad: 4889.8281  LR: 0.000020  \n","Epoch: [1][4500/36908] Elapsed 14m 40s (remain 105m 38s) Loss: 0.0000(0.0068) Grad: 306.2709  LR: 0.000020  \n","Epoch: [1][4600/36908] Elapsed 14m 59s (remain 105m 19s) Loss: 0.0005(0.0067) Grad: 4162.1953  LR: 0.000019  \n","Epoch: [1][4700/36908] Elapsed 15m 19s (remain 105m 0s) Loss: 0.0026(0.0066) Grad: 33783.1172  LR: 0.000019  \n","Epoch: [1][4800/36908] Elapsed 15m 39s (remain 104m 40s) Loss: 0.0015(0.0065) Grad: 21503.5684  LR: 0.000019  \n","Epoch: [1][4900/36908] Elapsed 15m 58s (remain 104m 20s) Loss: 0.0001(0.0064) Grad: 1029.0674  LR: 0.000019  \n","Epoch: [1][5000/36908] Elapsed 16m 18s (remain 104m 5s) Loss: 0.0000(0.0063) Grad: 611.0923  LR: 0.000019  \n","Epoch: [1][5100/36908] Elapsed 16m 38s (remain 103m 44s) Loss: 0.0019(0.0062) Grad: 11106.9531  LR: 0.000019  \n","Epoch: [1][5200/36908] Elapsed 16m 57s (remain 103m 25s) Loss: 0.0018(0.0061) Grad: 19719.4727  LR: 0.000019  \n","Epoch: [1][5300/36908] Elapsed 17m 17s (remain 103m 5s) Loss: 0.0019(0.0060) Grad: 20021.4648  LR: 0.000019  \n","Epoch: [1][5400/36908] Elapsed 17m 36s (remain 102m 45s) Loss: 0.0029(0.0059) Grad: 16338.7197  LR: 0.000019  \n","Epoch: [1][5500/36908] Elapsed 17m 56s (remain 102m 25s) Loss: 0.0000(0.0059) Grad: 10.0464  LR: 0.000019  \n","Epoch: [1][5600/36908] Elapsed 18m 15s (remain 102m 5s) Loss: 0.0000(0.0058) Grad: 432.7431  LR: 0.000019  \n","Epoch: [1][5700/36908] Elapsed 18m 35s (remain 101m 44s) Loss: 0.0006(0.0057) Grad: 13789.1104  LR: 0.000019  \n","Epoch: [1][5800/36908] Elapsed 18m 54s (remain 101m 24s) Loss: 0.0027(0.0056) Grad: 25844.2656  LR: 0.000019  \n","Epoch: [1][5900/36908] Elapsed 19m 14s (remain 101m 6s) Loss: 0.0009(0.0056) Grad: 29043.1582  LR: 0.000019  \n","Epoch: [1][6000/36908] Elapsed 19m 34s (remain 100m 47s) Loss: 0.0002(0.0055) Grad: 5559.8872  LR: 0.000019  \n","Epoch: [1][6100/36908] Elapsed 19m 53s (remain 100m 27s) Loss: 0.0000(0.0054) Grad: 15.5519  LR: 0.000019  \n","Epoch: [1][6200/36908] Elapsed 20m 13s (remain 100m 8s) Loss: 0.0005(0.0053) Grad: 9265.2236  LR: 0.000018  \n","Epoch: [1][6300/36908] Elapsed 20m 33s (remain 99m 49s) Loss: 0.0000(0.0053) Grad: 190.3430  LR: 0.000018  \n","Epoch: [1][6400/36908] Elapsed 20m 52s (remain 99m 29s) Loss: 0.0001(0.0052) Grad: 741.4440  LR: 0.000018  \n","Epoch: [1][6500/36908] Elapsed 21m 12s (remain 99m 10s) Loss: 0.0017(0.0052) Grad: 14014.8906  LR: 0.000018  \n","Epoch: [1][6600/36908] Elapsed 21m 31s (remain 98m 50s) Loss: 0.0000(0.0051) Grad: 76.1273  LR: 0.000018  \n","Epoch: [1][6700/36908] Elapsed 21m 51s (remain 98m 30s) Loss: 0.0039(0.0050) Grad: 9990.8652  LR: 0.000018  \n","Epoch: [1][6800/36908] Elapsed 22m 10s (remain 98m 10s) Loss: 0.0042(0.0050) Grad: 22769.3398  LR: 0.000018  \n","Epoch: [1][6900/36908] Elapsed 22m 30s (remain 97m 51s) Loss: 0.0006(0.0049) Grad: 8571.8613  LR: 0.000018  \n","Epoch: [1][7000/36908] Elapsed 22m 49s (remain 97m 31s) Loss: 0.0001(0.0049) Grad: 2133.4082  LR: 0.000018  \n","Epoch: [1][7100/36908] Elapsed 23m 9s (remain 97m 11s) Loss: 0.0000(0.0048) Grad: 582.7380  LR: 0.000018  \n","Epoch: [1][7200/36908] Elapsed 23m 28s (remain 96m 51s) Loss: 0.0004(0.0048) Grad: 7548.9585  LR: 0.000018  \n","Epoch: [1][7300/36908] Elapsed 23m 48s (remain 96m 32s) Loss: 0.0059(0.0047) Grad: 51361.0938  LR: 0.000018  \n","Epoch: [1][7400/36908] Elapsed 24m 7s (remain 96m 12s) Loss: 0.0008(0.0047) Grad: 7718.5146  LR: 0.000018  \n","Epoch: [1][7500/36908] Elapsed 24m 27s (remain 95m 52s) Loss: 0.0001(0.0047) Grad: 2086.4504  LR: 0.000018  \n","Epoch: [1][7600/36908] Elapsed 24m 46s (remain 95m 32s) Loss: 0.0000(0.0046) Grad: 12.1152  LR: 0.000018  \n","Epoch: [1][7700/36908] Elapsed 25m 6s (remain 95m 12s) Loss: 0.0002(0.0046) Grad: 5593.9795  LR: 0.000018  \n","Epoch: [1][7800/36908] Elapsed 25m 25s (remain 94m 53s) Loss: 0.0000(0.0045) Grad: 6.9468  LR: 0.000018  \n","Epoch: [1][7900/36908] Elapsed 25m 45s (remain 94m 33s) Loss: 0.0011(0.0045) Grad: 11657.4932  LR: 0.000017  \n","Epoch: [1][8000/36908] Elapsed 26m 5s (remain 94m 14s) Loss: 0.0000(0.0044) Grad: 153.5095  LR: 0.000017  \n","Epoch: [1][8100/36908] Elapsed 26m 24s (remain 93m 55s) Loss: 0.0011(0.0044) Grad: 17614.6094  LR: 0.000017  \n","Epoch: [1][8200/36908] Elapsed 26m 44s (remain 93m 35s) Loss: 0.0004(0.0044) Grad: 8695.3203  LR: 0.000017  \n","Epoch: [1][8300/36908] Elapsed 27m 3s (remain 93m 15s) Loss: 0.0002(0.0043) Grad: 27733.5625  LR: 0.000017  \n","Epoch: [1][8400/36908] Elapsed 27m 23s (remain 92m 56s) Loss: 0.0000(0.0043) Grad: 44.5505  LR: 0.000017  \n","Epoch: [1][8500/36908] Elapsed 27m 42s (remain 92m 36s) Loss: 0.0000(0.0043) Grad: 189.1480  LR: 0.000017  \n","Epoch: [1][8600/36908] Elapsed 28m 2s (remain 92m 17s) Loss: 0.0004(0.0042) Grad: 12648.1641  LR: 0.000017  \n","Epoch: [1][8700/36908] Elapsed 28m 22s (remain 91m 57s) Loss: 0.0000(0.0042) Grad: 114.5710  LR: 0.000017  \n","Epoch: [1][8800/36908] Elapsed 28m 41s (remain 91m 38s) Loss: 0.0001(0.0041) Grad: 2497.5901  LR: 0.000017  \n","Epoch: [1][8900/36908] Elapsed 29m 1s (remain 91m 19s) Loss: 0.0000(0.0041) Grad: 36.5872  LR: 0.000017  \n","Epoch: [1][9000/36908] Elapsed 29m 20s (remain 90m 59s) Loss: 0.0000(0.0041) Grad: 21.2263  LR: 0.000017  \n","Epoch: [1][9100/36908] Elapsed 29m 40s (remain 90m 39s) Loss: 0.0002(0.0041) Grad: 10875.3281  LR: 0.000017  \n","Epoch: [1][9200/36908] Elapsed 29m 59s (remain 90m 19s) Loss: 0.0000(0.0040) Grad: 1674.0327  LR: 0.000017  \n","Epoch: [1][9300/36908] Elapsed 30m 19s (remain 89m 59s) Loss: 0.0000(0.0040) Grad: 15.1113  LR: 0.000017  \n","Epoch: [1][9400/36908] Elapsed 30m 38s (remain 89m 40s) Loss: 0.0007(0.0040) Grad: 25458.0625  LR: 0.000017  \n","Epoch: [1][9500/36908] Elapsed 30m 58s (remain 89m 20s) Loss: 0.0003(0.0039) Grad: 14481.9219  LR: 0.000017  \n","Epoch: [1][9600/36908] Elapsed 31m 17s (remain 89m 0s) Loss: 0.0002(0.0039) Grad: 6288.4434  LR: 0.000016  \n","Epoch: [1][9700/36908] Elapsed 31m 37s (remain 88m 41s) Loss: 0.0000(0.0039) Grad: 148.0633  LR: 0.000016  \n","Epoch: [1][9800/36908] Elapsed 31m 57s (remain 88m 22s) Loss: 0.0003(0.0038) Grad: 11052.3945  LR: 0.000016  \n","Epoch: [1][9900/36908] Elapsed 32m 16s (remain 88m 2s) Loss: 0.0010(0.0038) Grad: 25915.3965  LR: 0.000016  \n","Epoch: [1][10000/36908] Elapsed 32m 36s (remain 87m 42s) Loss: 0.0000(0.0038) Grad: 82.8990  LR: 0.000016  \n","Epoch: [1][10100/36908] Elapsed 32m 55s (remain 87m 23s) Loss: 0.0001(0.0038) Grad: 3472.6965  LR: 0.000016  \n","Epoch: [1][10200/36908] Elapsed 33m 15s (remain 87m 3s) Loss: 0.0000(0.0037) Grad: 795.5358  LR: 0.000016  \n","Epoch: [1][10300/36908] Elapsed 33m 34s (remain 86m 44s) Loss: 0.0000(0.0037) Grad: 1068.7081  LR: 0.000016  \n","Epoch: [1][10400/36908] Elapsed 33m 54s (remain 86m 24s) Loss: 0.0001(0.0037) Grad: 1709.5599  LR: 0.000016  \n","Epoch: [1][10500/36908] Elapsed 34m 13s (remain 86m 5s) Loss: 0.0000(0.0037) Grad: 204.8423  LR: 0.000016  \n","Epoch: [1][10600/36908] Elapsed 34m 33s (remain 85m 45s) Loss: 0.0000(0.0037) Grad: 277.0463  LR: 0.000016  \n","Epoch: [1][10700/36908] Elapsed 34m 52s (remain 85m 25s) Loss: 0.0020(0.0036) Grad: 9999.1299  LR: 0.000016  \n","Epoch: [1][10800/36908] Elapsed 35m 12s (remain 85m 6s) Loss: 0.0015(0.0036) Grad: 17332.8594  LR: 0.000016  \n","Epoch: [1][10900/36908] Elapsed 35m 32s (remain 84m 46s) Loss: 0.0017(0.0036) Grad: 20626.9980  LR: 0.000016  \n","Epoch: [1][11000/36908] Elapsed 35m 51s (remain 84m 27s) Loss: 0.0000(0.0036) Grad: 476.8424  LR: 0.000016  \n","Epoch: [1][11100/36908] Elapsed 36m 11s (remain 84m 7s) Loss: 0.0000(0.0035) Grad: 161.3149  LR: 0.000016  \n","Epoch: [1][11200/36908] Elapsed 36m 30s (remain 83m 48s) Loss: 0.0035(0.0035) Grad: 66334.1797  LR: 0.000015  \n","Epoch: [1][11300/36908] Elapsed 36m 50s (remain 83m 29s) Loss: 0.0049(0.0035) Grad: 32166.0723  LR: 0.000015  \n","Epoch: [1][11400/36908] Elapsed 37m 10s (remain 83m 9s) Loss: 0.0000(0.0035) Grad: 251.1158  LR: 0.000015  \n","Epoch: [1][11500/36908] Elapsed 37m 29s (remain 82m 49s) Loss: 0.0000(0.0034) Grad: 770.1276  LR: 0.000015  \n","Epoch: [1][11600/36908] Elapsed 37m 49s (remain 82m 30s) Loss: 0.0003(0.0034) Grad: 4298.2627  LR: 0.000015  \n","Epoch: [1][11700/36908] Elapsed 38m 8s (remain 82m 10s) Loss: 0.0000(0.0034) Grad: 51.0234  LR: 0.000015  \n","Epoch: [1][11800/36908] Elapsed 38m 28s (remain 81m 51s) Loss: 0.0000(0.0034) Grad: 26.6614  LR: 0.000015  \n","Epoch: [1][11900/36908] Elapsed 38m 48s (remain 81m 32s) Loss: 0.0001(0.0034) Grad: 2318.0854  LR: 0.000015  \n","Epoch: [1][12000/36908] Elapsed 39m 7s (remain 81m 12s) Loss: 0.0001(0.0034) Grad: 2167.3357  LR: 0.000015  \n","Epoch: [1][12100/36908] Elapsed 39m 27s (remain 80m 53s) Loss: 0.0000(0.0033) Grad: 28.8157  LR: 0.000015  \n","Epoch: [1][12200/36908] Elapsed 39m 47s (remain 80m 33s) Loss: 0.0000(0.0033) Grad: 244.4137  LR: 0.000015  \n","Epoch: [1][12300/36908] Elapsed 40m 6s (remain 80m 14s) Loss: 0.0000(0.0033) Grad: 54.5953  LR: 0.000015  \n","Epoch: [1][12400/36908] Elapsed 40m 25s (remain 79m 54s) Loss: 0.0000(0.0033) Grad: 7.2086  LR: 0.000015  \n","Epoch: [1][12500/36908] Elapsed 40m 45s (remain 79m 34s) Loss: 0.0001(0.0033) Grad: 1223.2869  LR: 0.000015  \n","Epoch: [1][12600/36908] Elapsed 41m 4s (remain 79m 14s) Loss: 0.0000(0.0032) Grad: 188.7005  LR: 0.000015  \n","Epoch: [1][12700/36908] Elapsed 41m 24s (remain 78m 55s) Loss: 0.0001(0.0032) Grad: 2412.2693  LR: 0.000015  \n","Epoch: [1][12800/36908] Elapsed 41m 43s (remain 78m 35s) Loss: 0.0000(0.0032) Grad: 22.5515  LR: 0.000015  \n","Epoch: [1][12900/36908] Elapsed 42m 3s (remain 78m 15s) Loss: 0.0002(0.0032) Grad: 3374.8506  LR: 0.000014  \n","Epoch: [1][13000/36908] Elapsed 42m 22s (remain 77m 55s) Loss: 0.0006(0.0032) Grad: 12329.2598  LR: 0.000014  \n","Epoch: [1][13100/36908] Elapsed 42m 42s (remain 77m 36s) Loss: 0.0000(0.0032) Grad: 12.9832  LR: 0.000014  \n","Epoch: [1][13200/36908] Elapsed 43m 1s (remain 77m 16s) Loss: 0.0043(0.0032) Grad: 33873.0430  LR: 0.000014  \n","Epoch: [1][13300/36908] Elapsed 43m 21s (remain 76m 56s) Loss: 0.0000(0.0031) Grad: 81.5561  LR: 0.000014  \n","Epoch: [1][13400/36908] Elapsed 43m 40s (remain 76m 37s) Loss: 0.0003(0.0031) Grad: 4682.5361  LR: 0.000014  \n","Epoch: [1][13500/36908] Elapsed 44m 0s (remain 76m 17s) Loss: 0.0051(0.0031) Grad: 39260.5625  LR: 0.000014  \n","Epoch: [1][13600/36908] Elapsed 44m 19s (remain 75m 57s) Loss: 0.0000(0.0031) Grad: 518.5916  LR: 0.000014  \n","Epoch: [1][13700/36908] Elapsed 44m 39s (remain 75m 38s) Loss: 0.0000(0.0031) Grad: 25.5123  LR: 0.000014  \n","Epoch: [1][13800/36908] Elapsed 44m 58s (remain 75m 18s) Loss: 0.0000(0.0031) Grad: 51.1395  LR: 0.000014  \n","Epoch: [1][13900/36908] Elapsed 45m 18s (remain 74m 58s) Loss: 0.0002(0.0030) Grad: 4022.5884  LR: 0.000014  \n","Epoch: [1][14000/36908] Elapsed 45m 37s (remain 74m 39s) Loss: 0.0001(0.0030) Grad: 1557.9851  LR: 0.000014  \n","Epoch: [1][14100/36908] Elapsed 45m 57s (remain 74m 19s) Loss: 0.0021(0.0030) Grad: 13187.9814  LR: 0.000014  \n","Epoch: [1][14200/36908] Elapsed 46m 16s (remain 74m 0s) Loss: 0.0016(0.0030) Grad: 22597.3750  LR: 0.000014  \n","Epoch: [1][14300/36908] Elapsed 46m 36s (remain 73m 40s) Loss: 0.0011(0.0030) Grad: 10198.3330  LR: 0.000014  \n","Epoch: [1][14400/36908] Elapsed 46m 55s (remain 73m 20s) Loss: 0.0032(0.0030) Grad: 73059.8438  LR: 0.000014  \n","Epoch: [1][14500/36908] Elapsed 47m 15s (remain 73m 1s) Loss: 0.0000(0.0030) Grad: 67.7827  LR: 0.000013  \n","Epoch: [1][14600/36908] Elapsed 47m 34s (remain 72m 41s) Loss: 0.0000(0.0029) Grad: 39.6605  LR: 0.000013  \n","Epoch: [1][14700/36908] Elapsed 47m 54s (remain 72m 22s) Loss: 0.0003(0.0029) Grad: 20437.4180  LR: 0.000013  \n","Epoch: [1][14800/36908] Elapsed 48m 14s (remain 72m 2s) Loss: 0.0000(0.0029) Grad: 183.8891  LR: 0.000013  \n","Epoch: [1][14900/36908] Elapsed 48m 33s (remain 71m 42s) Loss: 0.0012(0.0029) Grad: 22224.1367  LR: 0.000013  \n","Epoch: [1][15000/36908] Elapsed 48m 53s (remain 71m 23s) Loss: 0.0000(0.0029) Grad: 9.6467  LR: 0.000013  \n","Epoch: [1][15100/36908] Elapsed 49m 12s (remain 71m 3s) Loss: 0.0041(0.0029) Grad: 119115.5547  LR: 0.000013  \n","Epoch: [1][15200/36908] Elapsed 49m 32s (remain 70m 44s) Loss: 0.0012(0.0029) Grad: 27094.6289  LR: 0.000013  \n","Epoch: [1][15300/36908] Elapsed 49m 51s (remain 70m 24s) Loss: 0.0000(0.0029) Grad: 51.0702  LR: 0.000013  \n","Epoch: [1][15400/36908] Elapsed 50m 11s (remain 70m 5s) Loss: 0.0000(0.0028) Grad: 5.3046  LR: 0.000013  \n","Epoch: [1][15500/36908] Elapsed 50m 30s (remain 69m 45s) Loss: 0.0000(0.0028) Grad: 23.0876  LR: 0.000013  \n","Epoch: [1][15600/36908] Elapsed 50m 50s (remain 69m 25s) Loss: 0.0030(0.0028) Grad: 29759.2266  LR: 0.000013  \n","Epoch: [1][15700/36908] Elapsed 51m 9s (remain 69m 6s) Loss: 0.0003(0.0028) Grad: 36554.9219  LR: 0.000013  \n","Epoch: [1][15800/36908] Elapsed 51m 29s (remain 68m 46s) Loss: 0.0000(0.0028) Grad: 302.3443  LR: 0.000013  \n","Epoch: [1][15900/36908] Elapsed 51m 48s (remain 68m 27s) Loss: 0.0038(0.0028) Grad: 72385.0391  LR: 0.000013  \n","Epoch: [1][16000/36908] Elapsed 52m 8s (remain 68m 7s) Loss: 0.0000(0.0028) Grad: 70.6284  LR: 0.000013  \n","Epoch: [1][16100/36908] Elapsed 52m 28s (remain 67m 48s) Loss: 0.0003(0.0028) Grad: 11968.2021  LR: 0.000013  \n","Epoch: [1][16200/36908] Elapsed 52m 47s (remain 67m 28s) Loss: 0.0037(0.0028) Grad: 18611.6367  LR: 0.000012  \n","Epoch: [1][16300/36908] Elapsed 53m 7s (remain 67m 9s) Loss: 0.0035(0.0027) Grad: 59650.7344  LR: 0.000012  \n","Epoch: [1][16400/36908] Elapsed 53m 26s (remain 66m 49s) Loss: 0.0026(0.0027) Grad: 35738.0547  LR: 0.000012  \n","Epoch: [1][16500/36908] Elapsed 53m 46s (remain 66m 29s) Loss: 0.0003(0.0027) Grad: 8268.0000  LR: 0.000012  \n","Epoch: [1][16600/36908] Elapsed 54m 5s (remain 66m 10s) Loss: 0.0028(0.0027) Grad: 176100.7344  LR: 0.000012  \n","Epoch: [1][16700/36908] Elapsed 54m 25s (remain 65m 50s) Loss: 0.0000(0.0027) Grad: 353.7840  LR: 0.000012  \n","Epoch: [1][16800/36908] Elapsed 54m 45s (remain 65m 31s) Loss: 0.0001(0.0027) Grad: 2463.9563  LR: 0.000012  \n","Epoch: [1][16900/36908] Elapsed 55m 4s (remain 65m 11s) Loss: 0.0008(0.0027) Grad: 32587.9492  LR: 0.000012  \n","Epoch: [1][17000/36908] Elapsed 55m 24s (remain 64m 52s) Loss: 0.0069(0.0027) Grad: 55224.3203  LR: 0.000012  \n","Epoch: [1][17100/36908] Elapsed 55m 43s (remain 64m 32s) Loss: 0.0000(0.0027) Grad: 93.4427  LR: 0.000012  \n","Epoch: [1][17200/36908] Elapsed 56m 3s (remain 64m 13s) Loss: 0.0000(0.0027) Grad: 43.3425  LR: 0.000012  \n","Epoch: [1][17300/36908] Elapsed 56m 22s (remain 63m 53s) Loss: 0.0014(0.0026) Grad: 33522.4102  LR: 0.000012  \n","Epoch: [1][17400/36908] Elapsed 56m 42s (remain 63m 34s) Loss: 0.0009(0.0026) Grad: 293228.3750  LR: 0.000012  \n","Epoch: [1][17500/36908] Elapsed 57m 2s (remain 63m 14s) Loss: 0.0102(0.0026) Grad: 118460.2578  LR: 0.000012  \n","Epoch: [1][17600/36908] Elapsed 57m 21s (remain 62m 55s) Loss: 0.0002(0.0026) Grad: 6966.3169  LR: 0.000012  \n","Epoch: [1][17700/36908] Elapsed 57m 41s (remain 62m 35s) Loss: 0.0005(0.0026) Grad: 38864.3359  LR: 0.000012  \n","Epoch: [1][17800/36908] Elapsed 58m 0s (remain 62m 15s) Loss: 0.0004(0.0026) Grad: 17047.4727  LR: 0.000012  \n","Epoch: [1][17900/36908] Elapsed 58m 20s (remain 61m 56s) Loss: 0.0008(0.0026) Grad: 34542.6602  LR: 0.000011  \n","Epoch: [1][18000/36908] Elapsed 58m 39s (remain 61m 36s) Loss: 0.0000(0.0026) Grad: 378.5597  LR: 0.000011  \n","Epoch: [1][18100/36908] Elapsed 58m 59s (remain 61m 17s) Loss: 0.0000(0.0026) Grad: 325.2455  LR: 0.000011  \n","Epoch: [1][18200/36908] Elapsed 59m 18s (remain 60m 57s) Loss: 0.0000(0.0026) Grad: 148.6907  LR: 0.000011  \n","Epoch: [1][18300/36908] Elapsed 59m 38s (remain 60m 38s) Loss: 0.0303(0.0026) Grad: 125812.4141  LR: 0.000011  \n","Epoch: [1][18400/36908] Elapsed 59m 57s (remain 60m 18s) Loss: 0.0000(0.0026) Grad: 759.0542  LR: 0.000011  \n","Epoch: [1][18500/36908] Elapsed 60m 17s (remain 59m 58s) Loss: 0.0000(0.0026) Grad: 223.4112  LR: 0.000011  \n","Epoch: [1][18600/36908] Elapsed 60m 36s (remain 59m 39s) Loss: 0.0004(0.0025) Grad: 23623.4707  LR: 0.000011  \n","Epoch: [1][18700/36908] Elapsed 60m 56s (remain 59m 19s) Loss: 0.0029(0.0025) Grad: 44430.6602  LR: 0.000011  \n","Epoch: [1][18800/36908] Elapsed 61m 15s (remain 58m 59s) Loss: 0.0032(0.0025) Grad: 69928.3984  LR: 0.000011  \n","Epoch: [1][18900/36908] Elapsed 61m 35s (remain 58m 40s) Loss: 0.0001(0.0025) Grad: 4048.4065  LR: 0.000011  \n","Epoch: [1][19000/36908] Elapsed 61m 54s (remain 58m 20s) Loss: 0.0002(0.0025) Grad: 4695.8472  LR: 0.000011  \n","Epoch: [1][19100/36908] Elapsed 62m 14s (remain 58m 1s) Loss: 0.0000(0.0025) Grad: 6.3538  LR: 0.000011  \n","Epoch: [1][19200/36908] Elapsed 62m 33s (remain 57m 41s) Loss: 0.0000(0.0025) Grad: 2584.7629  LR: 0.000011  \n","Epoch: [1][19300/36908] Elapsed 62m 53s (remain 57m 22s) Loss: 0.0000(0.0025) Grad: 7.1362  LR: 0.000011  \n","Epoch: [1][19400/36908] Elapsed 63m 12s (remain 57m 2s) Loss: 0.0000(0.0025) Grad: 30.0223  LR: 0.000011  \n","Epoch: [1][19500/36908] Elapsed 63m 32s (remain 56m 42s) Loss: 0.0063(0.0025) Grad: 26145.0938  LR: 0.000010  \n","Epoch: [1][19600/36908] Elapsed 63m 51s (remain 56m 23s) Loss: 0.0003(0.0025) Grad: 6846.7783  LR: 0.000010  \n","Epoch: [1][19700/36908] Elapsed 64m 11s (remain 56m 3s) Loss: 0.0000(0.0025) Grad: 780.6679  LR: 0.000010  \n","Epoch: [1][19800/36908] Elapsed 64m 30s (remain 55m 44s) Loss: 0.0000(0.0025) Grad: 6.7019  LR: 0.000010  \n","Epoch: [1][19900/36908] Elapsed 64m 50s (remain 55m 24s) Loss: 0.0000(0.0024) Grad: 24.8036  LR: 0.000010  \n","Epoch: [1][20000/36908] Elapsed 65m 10s (remain 55m 5s) Loss: 0.0008(0.0024) Grad: 20563.8105  LR: 0.000010  \n","Epoch: [1][20100/36908] Elapsed 65m 29s (remain 54m 45s) Loss: 0.0000(0.0024) Grad: 290.9127  LR: 0.000010  \n","Epoch: [1][20200/36908] Elapsed 65m 49s (remain 54m 26s) Loss: 0.0013(0.0024) Grad: 20208.1348  LR: 0.000010  \n","Epoch: [1][20300/36908] Elapsed 66m 8s (remain 54m 6s) Loss: 0.0000(0.0024) Grad: 13.1397  LR: 0.000010  \n","Epoch: [1][20400/36908] Elapsed 66m 28s (remain 53m 46s) Loss: 0.0001(0.0024) Grad: 2251.0793  LR: 0.000010  \n","Epoch: [1][20500/36908] Elapsed 66m 47s (remain 53m 27s) Loss: 0.0000(0.0024) Grad: 227.5037  LR: 0.000010  \n","Epoch: [1][20600/36908] Elapsed 67m 7s (remain 53m 7s) Loss: 0.0000(0.0024) Grad: 1841.7560  LR: 0.000010  \n","Epoch: [1][20700/36908] Elapsed 67m 26s (remain 52m 48s) Loss: 0.0062(0.0024) Grad: 53371.9453  LR: 0.000010  \n","Epoch: [1][20800/36908] Elapsed 67m 46s (remain 52m 28s) Loss: 0.0008(0.0024) Grad: 38506.0234  LR: 0.000010  \n","Epoch: [1][20900/36908] Elapsed 68m 6s (remain 52m 9s) Loss: 0.0000(0.0024) Grad: 18.6270  LR: 0.000010  \n","Epoch: [1][21000/36908] Elapsed 68m 25s (remain 51m 49s) Loss: 0.0002(0.0024) Grad: 5419.9722  LR: 0.000010  \n","Epoch: [1][21100/36908] Elapsed 68m 45s (remain 51m 30s) Loss: 0.0004(0.0024) Grad: 15624.5078  LR: 0.000010  \n","Epoch: [1][21200/36908] Elapsed 69m 4s (remain 51m 10s) Loss: 0.0036(0.0024) Grad: 186848.2969  LR: 0.000009  \n","Epoch: [1][21300/36908] Elapsed 69m 24s (remain 50m 51s) Loss: 0.0004(0.0023) Grad: 25542.1465  LR: 0.000009  \n","Epoch: [1][21400/36908] Elapsed 69m 43s (remain 50m 31s) Loss: 0.0000(0.0023) Grad: 333.0480  LR: 0.000009  \n","Epoch: [1][21500/36908] Elapsed 70m 3s (remain 50m 12s) Loss: 0.0022(0.0023) Grad: 20047.5879  LR: 0.000009  \n","Epoch: [1][21600/36908] Elapsed 70m 23s (remain 49m 52s) Loss: 0.0000(0.0023) Grad: 41.3109  LR: 0.000009  \n","Epoch: [1][21700/36908] Elapsed 70m 42s (remain 49m 32s) Loss: 0.0000(0.0023) Grad: 13.7088  LR: 0.000009  \n","Epoch: [1][21800/36908] Elapsed 71m 2s (remain 49m 13s) Loss: 0.0000(0.0023) Grad: 226.0236  LR: 0.000009  \n","Epoch: [1][21900/36908] Elapsed 71m 21s (remain 48m 53s) Loss: 0.0000(0.0023) Grad: 7.9187  LR: 0.000009  \n","Epoch: [1][22000/36908] Elapsed 71m 41s (remain 48m 34s) Loss: 0.0000(0.0023) Grad: 4.8547  LR: 0.000009  \n","Epoch: [1][22100/36908] Elapsed 72m 0s (remain 48m 14s) Loss: 0.0000(0.0023) Grad: 835.0979  LR: 0.000009  \n","Epoch: [1][22200/36908] Elapsed 72m 20s (remain 47m 55s) Loss: 0.0000(0.0023) Grad: 37.0436  LR: 0.000009  \n","Epoch: [1][22300/36908] Elapsed 72m 39s (remain 47m 35s) Loss: 0.0007(0.0023) Grad: 21119.2422  LR: 0.000009  \n","Epoch: [1][22400/36908] Elapsed 72m 59s (remain 47m 16s) Loss: 0.0000(0.0023) Grad: 13.3493  LR: 0.000009  \n","Epoch: [1][22500/36908] Elapsed 73m 18s (remain 46m 56s) Loss: 0.0000(0.0023) Grad: 58.6181  LR: 0.000009  \n","Epoch: [1][22600/36908] Elapsed 73m 38s (remain 46m 36s) Loss: 0.0000(0.0023) Grad: 128.5965  LR: 0.000009  \n","Epoch: [1][22700/36908] Elapsed 73m 58s (remain 46m 17s) Loss: 0.0004(0.0023) Grad: 37759.8086  LR: 0.000009  \n","Epoch: [1][22800/36908] Elapsed 74m 17s (remain 45m 57s) Loss: 0.0001(0.0023) Grad: 18456.7148  LR: 0.000008  \n","Epoch: [1][22900/36908] Elapsed 74m 37s (remain 45m 38s) Loss: 0.0001(0.0023) Grad: 18017.8164  LR: 0.000008  \n","Epoch: [1][23000/36908] Elapsed 74m 56s (remain 45m 18s) Loss: 0.0031(0.0022) Grad: 21845.3164  LR: 0.000008  \n","Epoch: [1][23100/36908] Elapsed 75m 16s (remain 44m 59s) Loss: 0.0000(0.0022) Grad: 12.1058  LR: 0.000008  \n","Epoch: [1][23200/36908] Elapsed 75m 35s (remain 44m 39s) Loss: 0.0001(0.0022) Grad: 7908.4780  LR: 0.000008  \n","Epoch: [1][23300/36908] Elapsed 75m 55s (remain 44m 19s) Loss: 0.0006(0.0022) Grad: 16840.6406  LR: 0.000008  \n","Epoch: [1][23400/36908] Elapsed 76m 14s (remain 44m 0s) Loss: 0.0021(0.0022) Grad: 129970.6719  LR: 0.000008  \n","Epoch: [1][23500/36908] Elapsed 76m 34s (remain 43m 40s) Loss: 0.0000(0.0022) Grad: 134.5525  LR: 0.000008  \n","Epoch: [1][23600/36908] Elapsed 76m 53s (remain 43m 21s) Loss: 0.0000(0.0022) Grad: 18.1243  LR: 0.000008  \n","Epoch: [1][23700/36908] Elapsed 77m 13s (remain 43m 1s) Loss: 0.0000(0.0022) Grad: 6.3330  LR: 0.000008  \n","Epoch: [1][23800/36908] Elapsed 77m 32s (remain 42m 42s) Loss: 0.0030(0.0022) Grad: 44699.4336  LR: 0.000008  \n","Epoch: [1][23900/36908] Elapsed 77m 52s (remain 42m 22s) Loss: 0.0001(0.0022) Grad: 1814.5997  LR: 0.000008  \n","Epoch: [1][24000/36908] Elapsed 78m 11s (remain 42m 2s) Loss: 0.0000(0.0022) Grad: 31.3826  LR: 0.000008  \n","Epoch: [1][24100/36908] Elapsed 78m 30s (remain 41m 43s) Loss: 0.0000(0.0022) Grad: 1465.3575  LR: 0.000008  \n","Epoch: [1][24200/36908] Elapsed 78m 50s (remain 41m 23s) Loss: 0.0041(0.0022) Grad: 52276.9375  LR: 0.000008  \n","Epoch: [1][24300/36908] Elapsed 79m 9s (remain 41m 4s) Loss: 0.0002(0.0022) Grad: 11862.3955  LR: 0.000008  \n","Epoch: [1][24400/36908] Elapsed 79m 29s (remain 40m 44s) Loss: 0.0000(0.0022) Grad: 34.0080  LR: 0.000008  \n","Epoch: [1][24500/36908] Elapsed 79m 48s (remain 40m 24s) Loss: 0.0004(0.0022) Grad: 16289.9258  LR: 0.000007  \n","Epoch: [1][24600/36908] Elapsed 80m 8s (remain 40m 5s) Loss: 0.0000(0.0022) Grad: 475.2108  LR: 0.000007  \n","Epoch: [1][24700/36908] Elapsed 80m 27s (remain 39m 45s) Loss: 0.0007(0.0022) Grad: 23775.2676  LR: 0.000007  \n","Epoch: [1][24800/36908] Elapsed 80m 47s (remain 39m 26s) Loss: 0.0000(0.0022) Grad: 17.6944  LR: 0.000007  \n","Epoch: [1][24900/36908] Elapsed 81m 6s (remain 39m 6s) Loss: 0.0000(0.0021) Grad: 251.0452  LR: 0.000007  \n","Epoch: [1][25000/36908] Elapsed 81m 26s (remain 38m 47s) Loss: 0.0000(0.0021) Grad: 10.7635  LR: 0.000007  \n","Epoch: [1][25100/36908] Elapsed 81m 45s (remain 38m 27s) Loss: 0.0000(0.0021) Grad: 419.1955  LR: 0.000007  \n","Epoch: [1][25200/36908] Elapsed 82m 5s (remain 38m 7s) Loss: 0.0000(0.0021) Grad: 15.0808  LR: 0.000007  \n","Epoch: [1][25300/36908] Elapsed 82m 24s (remain 37m 48s) Loss: 0.0000(0.0021) Grad: 36.2580  LR: 0.000007  \n","Epoch: [1][25400/36908] Elapsed 82m 44s (remain 37m 28s) Loss: 0.0039(0.0021) Grad: 68373.5078  LR: 0.000007  \n","Epoch: [1][25500/36908] Elapsed 83m 3s (remain 37m 9s) Loss: 0.0000(0.0021) Grad: 6.7169  LR: 0.000007  \n","Epoch: [1][25600/36908] Elapsed 83m 23s (remain 36m 49s) Loss: 0.0051(0.0021) Grad: 17075.9590  LR: 0.000007  \n","Epoch: [1][25700/36908] Elapsed 83m 42s (remain 36m 30s) Loss: 0.0000(0.0021) Grad: 12.8363  LR: 0.000007  \n","Epoch: [1][25800/36908] Elapsed 84m 2s (remain 36m 10s) Loss: 0.0000(0.0021) Grad: 62.7832  LR: 0.000007  \n","Epoch: [1][25900/36908] Elapsed 84m 21s (remain 35m 51s) Loss: 0.0004(0.0021) Grad: 23649.9531  LR: 0.000007  \n","Epoch: [1][26000/36908] Elapsed 84m 41s (remain 35m 31s) Loss: 0.0003(0.0021) Grad: 19202.7148  LR: 0.000007  \n","Epoch: [1][26100/36908] Elapsed 85m 0s (remain 35m 11s) Loss: 0.0002(0.0021) Grad: 9687.4932  LR: 0.000007  \n","Epoch: [1][26200/36908] Elapsed 85m 20s (remain 34m 52s) Loss: 0.0012(0.0021) Grad: 31030.2891  LR: 0.000006  \n","Epoch: [1][26300/36908] Elapsed 85m 39s (remain 34m 32s) Loss: 0.0000(0.0021) Grad: 32.7769  LR: 0.000006  \n","Epoch: [1][26400/36908] Elapsed 85m 59s (remain 34m 13s) Loss: 0.0013(0.0021) Grad: 24334.5000  LR: 0.000006  \n","Epoch: [1][26500/36908] Elapsed 86m 18s (remain 33m 53s) Loss: 0.0001(0.0021) Grad: 76022.7031  LR: 0.000006  \n","Epoch: [1][26600/36908] Elapsed 86m 38s (remain 33m 34s) Loss: 0.0000(0.0021) Grad: 4.9628  LR: 0.000006  \n","Epoch: [1][26700/36908] Elapsed 86m 57s (remain 33m 14s) Loss: 0.0000(0.0021) Grad: 2040.2010  LR: 0.000006  \n","Epoch: [1][26800/36908] Elapsed 87m 17s (remain 32m 55s) Loss: 0.0005(0.0021) Grad: 27623.0762  LR: 0.000006  \n","Epoch: [1][26900/36908] Elapsed 87m 36s (remain 32m 35s) Loss: 0.0040(0.0020) Grad: 84882.6484  LR: 0.000006  \n","Epoch: [1][27000/36908] Elapsed 87m 56s (remain 32m 15s) Loss: 0.0001(0.0020) Grad: 18547.8809  LR: 0.000006  \n","Epoch: [1][27100/36908] Elapsed 88m 16s (remain 31m 56s) Loss: 0.0000(0.0020) Grad: 631.0853  LR: 0.000006  \n","Epoch: [1][27200/36908] Elapsed 88m 35s (remain 31m 36s) Loss: 0.0000(0.0020) Grad: 97.5410  LR: 0.000006  \n","Epoch: [1][27300/36908] Elapsed 88m 55s (remain 31m 17s) Loss: 0.0000(0.0020) Grad: 219.1414  LR: 0.000006  \n","Epoch: [1][27400/36908] Elapsed 89m 14s (remain 30m 57s) Loss: 0.0000(0.0020) Grad: 6.0555  LR: 0.000006  \n","Epoch: [1][27500/36908] Elapsed 89m 34s (remain 30m 38s) Loss: 0.0000(0.0020) Grad: 244.0007  LR: 0.000006  \n","Epoch: [1][27600/36908] Elapsed 89m 53s (remain 30m 18s) Loss: 0.0013(0.0020) Grad: 39872.7891  LR: 0.000006  \n","Epoch: [1][27700/36908] Elapsed 90m 13s (remain 29m 59s) Loss: 0.0000(0.0020) Grad: 2639.2905  LR: 0.000006  \n","Epoch: [1][27800/36908] Elapsed 90m 32s (remain 29m 39s) Loss: 0.0000(0.0020) Grad: 11.7951  LR: 0.000005  \n","Epoch: [1][27900/36908] Elapsed 90m 52s (remain 29m 20s) Loss: 0.0050(0.0020) Grad: 104017.2188  LR: 0.000005  \n","Epoch: [1][28000/36908] Elapsed 91m 12s (remain 29m 0s) Loss: 0.0011(0.0020) Grad: 36153.3711  LR: 0.000005  \n","Epoch: [1][28100/36908] Elapsed 91m 31s (remain 28m 41s) Loss: 0.0000(0.0020) Grad: 46.2750  LR: 0.000005  \n","Epoch: [1][28200/36908] Elapsed 91m 51s (remain 28m 21s) Loss: 0.0020(0.0020) Grad: 39553.7227  LR: 0.000005  \n","Epoch: [1][28300/36908] Elapsed 92m 11s (remain 28m 2s) Loss: 0.0000(0.0020) Grad: 33.4351  LR: 0.000005  \n","Epoch: [1][28400/36908] Elapsed 92m 30s (remain 27m 42s) Loss: 0.0002(0.0020) Grad: 10008.5352  LR: 0.000005  \n","Epoch: [1][28500/36908] Elapsed 92m 50s (remain 27m 23s) Loss: 0.0001(0.0020) Grad: 6152.4131  LR: 0.000005  \n","Epoch: [1][28600/36908] Elapsed 93m 9s (remain 27m 3s) Loss: 0.0001(0.0020) Grad: 3080.4456  LR: 0.000005  \n","Epoch: [1][28700/36908] Elapsed 93m 29s (remain 26m 43s) Loss: 0.0000(0.0020) Grad: 40.3817  LR: 0.000005  \n","Epoch: [1][28800/36908] Elapsed 93m 48s (remain 26m 24s) Loss: 0.0002(0.0020) Grad: 8694.2803  LR: 0.000005  \n","Epoch: [1][28900/36908] Elapsed 94m 8s (remain 26m 4s) Loss: 0.0001(0.0020) Grad: 2269.2258  LR: 0.000005  \n","Epoch: [1][29000/36908] Elapsed 94m 27s (remain 25m 45s) Loss: 0.0000(0.0020) Grad: 42.8393  LR: 0.000005  \n","Epoch: [1][29100/36908] Elapsed 94m 47s (remain 25m 25s) Loss: 0.0000(0.0020) Grad: 177.1065  LR: 0.000005  \n","Epoch: [1][29200/36908] Elapsed 95m 6s (remain 25m 6s) Loss: 0.0000(0.0020) Grad: 41.3173  LR: 0.000005  \n","Epoch: [1][29300/36908] Elapsed 95m 26s (remain 24m 46s) Loss: 0.0009(0.0020) Grad: 18272.1953  LR: 0.000005  \n","Epoch: [1][29400/36908] Elapsed 95m 45s (remain 24m 27s) Loss: 0.0000(0.0020) Grad: 10.6704  LR: 0.000005  \n","Epoch: [1][29500/36908] Elapsed 96m 5s (remain 24m 7s) Loss: 0.0084(0.0020) Grad: 87099.7969  LR: 0.000004  \n","Epoch: [1][29600/36908] Elapsed 96m 24s (remain 23m 47s) Loss: 0.0007(0.0019) Grad: 59887.5977  LR: 0.000004  \n","Epoch: [1][29700/36908] Elapsed 96m 44s (remain 23m 28s) Loss: 0.0023(0.0019) Grad: 229288.5000  LR: 0.000004  \n","Epoch: [1][29800/36908] Elapsed 97m 3s (remain 23m 8s) Loss: 0.0000(0.0019) Grad: 41.7292  LR: 0.000004  \n","Epoch: [1][29900/36908] Elapsed 97m 23s (remain 22m 49s) Loss: 0.0000(0.0019) Grad: 19.4428  LR: 0.000004  \n","Epoch: [1][30000/36908] Elapsed 97m 42s (remain 22m 29s) Loss: 0.0042(0.0019) Grad: 91158.6250  LR: 0.000004  \n","Epoch: [1][30100/36908] Elapsed 98m 2s (remain 22m 10s) Loss: 0.0001(0.0019) Grad: 3109.9985  LR: 0.000004  \n","Epoch: [1][30200/36908] Elapsed 98m 21s (remain 21m 50s) Loss: 0.0000(0.0019) Grad: 14.0580  LR: 0.000004  \n","Epoch: [1][30300/36908] Elapsed 98m 41s (remain 21m 31s) Loss: 0.0009(0.0019) Grad: 17978.3965  LR: 0.000004  \n","Epoch: [1][30400/36908] Elapsed 99m 0s (remain 21m 11s) Loss: 0.0005(0.0019) Grad: 20596.4277  LR: 0.000004  \n","Epoch: [1][30500/36908] Elapsed 99m 20s (remain 20m 52s) Loss: 0.0000(0.0019) Grad: 5.7825  LR: 0.000004  \n","Epoch: [1][30600/36908] Elapsed 99m 39s (remain 20m 32s) Loss: 0.0000(0.0019) Grad: 105.0918  LR: 0.000004  \n","Epoch: [1][30700/36908] Elapsed 99m 59s (remain 20m 12s) Loss: 0.0000(0.0019) Grad: 5.8838  LR: 0.000004  \n","Epoch: [1][30800/36908] Elapsed 100m 18s (remain 19m 53s) Loss: 0.0005(0.0019) Grad: 20530.0977  LR: 0.000004  \n","Epoch: [1][30900/36908] Elapsed 100m 38s (remain 19m 33s) Loss: 0.0000(0.0019) Grad: 7.2752  LR: 0.000004  \n","Epoch: [1][31000/36908] Elapsed 100m 57s (remain 19m 14s) Loss: 0.0000(0.0019) Grad: 5.4888  LR: 0.000004  \n","Epoch: [1][31100/36908] Elapsed 101m 17s (remain 18m 54s) Loss: 0.0021(0.0019) Grad: 21247.4355  LR: 0.000003  \n","Epoch: [1][31200/36908] Elapsed 101m 36s (remain 18m 35s) Loss: 0.0011(0.0019) Grad: 98908.2656  LR: 0.000003  \n","Epoch: [1][31300/36908] Elapsed 101m 56s (remain 18m 15s) Loss: 0.0021(0.0019) Grad: 133681.3125  LR: 0.000003  \n","Epoch: [1][31400/36908] Elapsed 102m 15s (remain 17m 56s) Loss: 0.0015(0.0019) Grad: 30137.9414  LR: 0.000003  \n","Epoch: [1][31500/36908] Elapsed 102m 35s (remain 17m 36s) Loss: 0.0005(0.0019) Grad: 10951.1191  LR: 0.000003  \n","Epoch: [1][31600/36908] Elapsed 102m 54s (remain 17m 16s) Loss: 0.0000(0.0019) Grad: 20.0603  LR: 0.000003  \n","Epoch: [1][31700/36908] Elapsed 103m 14s (remain 16m 57s) Loss: 0.0000(0.0019) Grad: 36.7805  LR: 0.000003  \n","Epoch: [1][31800/36908] Elapsed 103m 33s (remain 16m 37s) Loss: 0.0229(0.0019) Grad: 117883.0625  LR: 0.000003  \n","Epoch: [1][31900/36908] Elapsed 103m 53s (remain 16m 18s) Loss: 0.0000(0.0019) Grad: 19.9686  LR: 0.000003  \n","Epoch: [1][32000/36908] Elapsed 104m 12s (remain 15m 58s) Loss: 0.0000(0.0019) Grad: 2402.4861  LR: 0.000003  \n","Epoch: [1][32100/36908] Elapsed 104m 32s (remain 15m 39s) Loss: 0.0017(0.0019) Grad: 63377.3633  LR: 0.000003  \n","Epoch: [1][32200/36908] Elapsed 104m 51s (remain 15m 19s) Loss: 0.0002(0.0019) Grad: 7796.9717  LR: 0.000003  \n","Epoch: [1][32300/36908] Elapsed 105m 11s (remain 15m 0s) Loss: 0.0000(0.0019) Grad: 10.3022  LR: 0.000003  \n","Epoch: [1][32400/36908] Elapsed 105m 30s (remain 14m 40s) Loss: 0.0013(0.0018) Grad: 37306.4375  LR: 0.000003  \n","Epoch: [1][32500/36908] Elapsed 105m 50s (remain 14m 21s) Loss: 0.0000(0.0018) Grad: 10.7889  LR: 0.000003  \n","Epoch: [1][32600/36908] Elapsed 106m 9s (remain 14m 1s) Loss: 0.0022(0.0018) Grad: 99945.4453  LR: 0.000003  \n","Epoch: [1][32700/36908] Elapsed 106m 29s (remain 13m 41s) Loss: 0.0005(0.0018) Grad: 33052.1211  LR: 0.000003  \n","Epoch: [1][32800/36908] Elapsed 106m 48s (remain 13m 22s) Loss: 0.0022(0.0018) Grad: 31504.0059  LR: 0.000002  \n","Epoch: [1][32900/36908] Elapsed 107m 8s (remain 13m 2s) Loss: 0.0000(0.0018) Grad: 1005.1922  LR: 0.000002  \n","Epoch: [1][33000/36908] Elapsed 107m 27s (remain 12m 43s) Loss: 0.0000(0.0018) Grad: 126.8685  LR: 0.000002  \n","Epoch: [1][33100/36908] Elapsed 107m 47s (remain 12m 23s) Loss: 0.0000(0.0018) Grad: 492.9945  LR: 0.000002  \n","Epoch: [1][33200/36908] Elapsed 108m 6s (remain 12m 4s) Loss: 0.0000(0.0018) Grad: 22.1426  LR: 0.000002  \n","Epoch: [1][33300/36908] Elapsed 108m 26s (remain 11m 44s) Loss: 0.0000(0.0018) Grad: 23.5984  LR: 0.000002  \n","Epoch: [1][33400/36908] Elapsed 108m 46s (remain 11m 25s) Loss: 0.0001(0.0018) Grad: 4714.9854  LR: 0.000002  \n","Epoch: [1][33500/36908] Elapsed 109m 5s (remain 11m 5s) Loss: 0.0024(0.0018) Grad: 58451.5508  LR: 0.000002  \n","Epoch: [1][33600/36908] Elapsed 109m 25s (remain 10m 46s) Loss: 0.0000(0.0018) Grad: 760.3032  LR: 0.000002  \n","Epoch: [1][33700/36908] Elapsed 109m 44s (remain 10m 26s) Loss: 0.0077(0.0018) Grad: 72624.2344  LR: 0.000002  \n","Epoch: [1][33800/36908] Elapsed 110m 4s (remain 10m 7s) Loss: 0.0015(0.0018) Grad: 33708.0938  LR: 0.000002  \n","Epoch: [1][33900/36908] Elapsed 110m 23s (remain 9m 47s) Loss: 0.0000(0.0018) Grad: 580.9781  LR: 0.000002  \n","Epoch: [1][34000/36908] Elapsed 110m 43s (remain 9m 27s) Loss: 0.0013(0.0018) Grad: 22339.6973  LR: 0.000002  \n","Epoch: [1][34100/36908] Elapsed 111m 2s (remain 9m 8s) Loss: 0.0000(0.0018) Grad: 4.6260  LR: 0.000002  \n","Epoch: [1][34200/36908] Elapsed 111m 22s (remain 8m 48s) Loss: 0.0000(0.0018) Grad: 360.3189  LR: 0.000002  \n","Epoch: [1][34300/36908] Elapsed 111m 41s (remain 8m 29s) Loss: 0.0077(0.0018) Grad: 40925.0195  LR: 0.000002  \n","Epoch: [1][34400/36908] Elapsed 112m 0s (remain 8m 9s) Loss: 0.0000(0.0018) Grad: 6.1064  LR: 0.000002  \n","Epoch: [1][34500/36908] Elapsed 112m 20s (remain 7m 50s) Loss: 0.0000(0.0018) Grad: 216.2755  LR: 0.000001  \n","Epoch: [1][34600/36908] Elapsed 112m 39s (remain 7m 30s) Loss: 0.0112(0.0018) Grad: 68107.4766  LR: 0.000001  \n","Epoch: [1][34700/36908] Elapsed 112m 59s (remain 7m 11s) Loss: 0.0028(0.0018) Grad: 21571.5820  LR: 0.000001  \n","Epoch: [1][34800/36908] Elapsed 113m 18s (remain 6m 51s) Loss: 0.0000(0.0018) Grad: 1469.4586  LR: 0.000001  \n","Epoch: [1][34900/36908] Elapsed 113m 38s (remain 6m 32s) Loss: 0.0020(0.0018) Grad: 96757.6719  LR: 0.000001  \n","Epoch: [1][35000/36908] Elapsed 113m 57s (remain 6m 12s) Loss: 0.0000(0.0018) Grad: 19.3266  LR: 0.000001  \n","Epoch: [1][35100/36908] Elapsed 114m 17s (remain 5m 53s) Loss: 0.0000(0.0018) Grad: 45.4816  LR: 0.000001  \n","Epoch: [1][35200/36908] Elapsed 114m 36s (remain 5m 33s) Loss: 0.0001(0.0018) Grad: 3325.2219  LR: 0.000001  \n","Epoch: [1][35300/36908] Elapsed 114m 56s (remain 5m 13s) Loss: 0.0008(0.0018) Grad: 102215.4453  LR: 0.000001  \n","Epoch: [1][35400/36908] Elapsed 115m 15s (remain 4m 54s) Loss: 0.0001(0.0018) Grad: 19117.5977  LR: 0.000001  \n","Epoch: [1][35500/36908] Elapsed 115m 35s (remain 4m 34s) Loss: 0.0000(0.0018) Grad: 12.6245  LR: 0.000001  \n","Epoch: [1][35600/36908] Elapsed 115m 54s (remain 4m 15s) Loss: 0.0001(0.0018) Grad: 2336.9285  LR: 0.000001  \n","Epoch: [1][35700/36908] Elapsed 116m 14s (remain 3m 55s) Loss: 0.0000(0.0018) Grad: 11.3412  LR: 0.000001  \n","Epoch: [1][35800/36908] Elapsed 116m 33s (remain 3m 36s) Loss: 0.0000(0.0018) Grad: 219.8550  LR: 0.000001  \n","Epoch: [1][35900/36908] Elapsed 116m 53s (remain 3m 16s) Loss: 0.0092(0.0018) Grad: 147585.7188  LR: 0.000001  \n","Epoch: [1][36000/36908] Elapsed 117m 12s (remain 2m 57s) Loss: 0.0000(0.0017) Grad: 10.5454  LR: 0.000001  \n","Epoch: [1][36100/36908] Elapsed 117m 32s (remain 2m 37s) Loss: 0.0000(0.0017) Grad: 459.9565  LR: 0.000000  \n","Epoch: [1][36200/36908] Elapsed 117m 51s (remain 2m 18s) Loss: 0.0000(0.0017) Grad: 1230.9314  LR: 0.000000  \n","Epoch: [1][36300/36908] Elapsed 118m 11s (remain 1m 58s) Loss: 0.0012(0.0017) Grad: 31870.5977  LR: 0.000000  \n","Epoch: [1][36400/36908] Elapsed 118m 30s (remain 1m 39s) Loss: 0.0000(0.0017) Grad: 10.1860  LR: 0.000000  \n","Epoch: [1][36500/36908] Elapsed 118m 50s (remain 1m 19s) Loss: 0.0000(0.0017) Grad: 19.9049  LR: 0.000000  \n","Epoch: [1][36600/36908] Elapsed 119m 9s (remain 0m 59s) Loss: 0.0000(0.0017) Grad: 2812.7769  LR: 0.000000  \n","Epoch: [1][36700/36908] Elapsed 119m 29s (remain 0m 40s) Loss: 0.0000(0.0017) Grad: 8.1732  LR: 0.000000  \n","Epoch: [1][36800/36908] Elapsed 119m 48s (remain 0m 20s) Loss: 0.0041(0.0017) Grad: 24967.4492  LR: 0.000000  \n","Epoch: [1][36900/36908] Elapsed 120m 8s (remain 0m 1s) Loss: 0.0001(0.0017) Grad: 9132.8838  LR: 0.000000  \n","Epoch: [1][36907/36908] Elapsed 120m 9s (remain 0m 0s) Loss: 0.0002(0.0017) Grad: 7785.8340  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 11m 37s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 6s (remain 1m 9s) Loss: 0.0185(0.0035) \n","EVAL: [200/1192] Elapsed 0m 12s (remain 1m 0s) Loss: 0.0021(0.0032) \n","EVAL: [300/1192] Elapsed 0m 18s (remain 0m 53s) Loss: 0.0045(0.0035) \n","EVAL: [400/1192] Elapsed 0m 24s (remain 0m 47s) Loss: 0.0000(0.0035) \n","EVAL: [500/1192] Elapsed 0m 29s (remain 0m 41s) Loss: 0.0143(0.0033) \n","EVAL: [600/1192] Elapsed 0m 35s (remain 0m 35s) Loss: 0.0100(0.0035) \n","EVAL: [700/1192] Elapsed 0m 41s (remain 0m 29s) Loss: 0.0020(0.0039) \n","EVAL: [800/1192] Elapsed 0m 47s (remain 0m 23s) Loss: 0.0112(0.0039) \n","EVAL: [900/1192] Elapsed 0m 53s (remain 0m 17s) Loss: 0.0010(0.0041) \n","EVAL: [1000/1192] Elapsed 0m 59s (remain 0m 11s) Loss: 0.0000(0.0039) \n","EVAL: [1100/1192] Elapsed 1m 5s (remain 0m 5s) Loss: 0.0063(0.0038) \n","EVAL: [1191/1192] Elapsed 1m 10s (remain 0m 0s) Loss: 0.0000(0.0037) \n","Epoch 1 - avg_train_loss: 0.0017  avg_val_loss: 0.0037  time: 7285s\n","Epoch 1 - Score: 0.8903\n","Epoch 1 - Save Best Score: 0.8903 Model\n","Best thres: 0.5, Score: 0.8863\n","Best thres: 0.5125, Score: 0.8865\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"941b59b26d99412ead555431db0469c1"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f2e6b643cf9447f96f4617cab77d51f"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef497effc689438a8a81b2fa510bdedf"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c2c4d2e64e447dca185591b1fa7d000"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n"]}],"source":["if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"name":"nbme-exp090.ipynb","provenance":[{"file_id":"1VyA5CJlM6l099SW-rHcp0GctebaNOB_0","timestamp":1650512873344},{"file_id":"1RX_ZvZAkBkJKpYjOf4JkbUw7fDIvFDvK","timestamp":1649243946413},{"file_id":"10yG4L3_nzpdL2CDwqxa9r-KWq6jYkWfl","timestamp":1649164439720}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"papermill":{"default_parameters":{},"duration":641.572862,"end_time":"2022-02-27T11:39:50.972497","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-02-27T11:29:09.399635","version":"2.3.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b6614359d578492bb5573280c915b6c4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ea430aaa256e49eb96079a74f3b4fecc","IPY_MODEL_2e7aea141d4d4b4f9df6b6ad9f02575b","IPY_MODEL_fbe6adb0e2a0473d87e516e475d6e22b"],"layout":"IPY_MODEL_e2e730a7bb444d27888c7c8b939dc2b3"}},"ea430aaa256e49eb96079a74f3b4fecc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b7cb5ed67b446ada62593df7cf85732","placeholder":"​","style":"IPY_MODEL_36506a8f48034a44be169658e7fd9264","value":"100%"}},"2e7aea141d4d4b4f9df6b6ad9f02575b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_622069a10c4b4dc8a9cf9d36ae5717c3","max":42146,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ebcb1784deef421d904e97b7bffd1eae","value":42146}},"fbe6adb0e2a0473d87e516e475d6e22b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d974ecce62d8481d87203c42adc9886e","placeholder":"​","style":"IPY_MODEL_1f8405cdfbed4347b4cabfd0242100f1","value":" 42146/42146 [00:22&lt;00:00, 1970.09it/s]"}},"e2e730a7bb444d27888c7c8b939dc2b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b7cb5ed67b446ada62593df7cf85732":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36506a8f48034a44be169658e7fd9264":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"622069a10c4b4dc8a9cf9d36ae5717c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebcb1784deef421d904e97b7bffd1eae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d974ecce62d8481d87203c42adc9886e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f8405cdfbed4347b4cabfd0242100f1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9a5be675403049e4b723d5d2d5f0f780":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_842e793e673546f1958be885102651be","IPY_MODEL_7382650b39064fca99c40f773ab7a15f","IPY_MODEL_1525f1d419c64db0ac150665cd091508"],"layout":"IPY_MODEL_e64200af7c9e40af9a72b0c6979c1330"}},"842e793e673546f1958be885102651be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f16f832089c84e2e930ee34e7f3f7fbf","placeholder":"​","style":"IPY_MODEL_ced8a9be7a6f4f37ba1496615717eedd","value":"100%"}},"7382650b39064fca99c40f773ab7a15f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f26a45f0081c41e899ddd3e45a4ce55c","max":143,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6527aa2cb9a243e4ba4a9d833326d094","value":143}},"1525f1d419c64db0ac150665cd091508":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a275df2f1504430f963586fbcad1ddd0","placeholder":"​","style":"IPY_MODEL_e54d05fa48774c029f49472a3f9f0be2","value":" 143/143 [00:00&lt;00:00, 3419.30it/s]"}},"e64200af7c9e40af9a72b0c6979c1330":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f16f832089c84e2e930ee34e7f3f7fbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ced8a9be7a6f4f37ba1496615717eedd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f26a45f0081c41e899ddd3e45a4ce55c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6527aa2cb9a243e4ba4a9d833326d094":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a275df2f1504430f963586fbcad1ddd0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e54d05fa48774c029f49472a3f9f0be2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f756312b8bc4c45b78b0dc74b5e7ca0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e1f51d05208d4246b19900bd30819e08","IPY_MODEL_35623ce5c256498eba491801939f067b","IPY_MODEL_f777c53ffe4046ddb093adbccfb51226"],"layout":"IPY_MODEL_3285f18c39aa4c74991784d2eeb6c638"}},"e1f51d05208d4246b19900bd30819e08":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d63fdf839bd0471bb6e56f8cbe540f75","placeholder":"​","style":"IPY_MODEL_48f462d421a547c9ac67d3bd69b9e06c","value":"100%"}},"35623ce5c256498eba491801939f067b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3df1108c7fc4560808ff9f8887e6ed8","max":612602,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3d46eeb74add41caa57616f0343b56df","value":612602}},"f777c53ffe4046ddb093adbccfb51226":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b1ebb4f2b8c41a1ba41608ebf9bb38e","placeholder":"​","style":"IPY_MODEL_f31dfaadc372462e8a64ca253144f83d","value":" 612602/612602 [00:00&lt;00:00, 978992.68it/s]"}},"3285f18c39aa4c74991784d2eeb6c638":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d63fdf839bd0471bb6e56f8cbe540f75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48f462d421a547c9ac67d3bd69b9e06c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3df1108c7fc4560808ff9f8887e6ed8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d46eeb74add41caa57616f0343b56df":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2b1ebb4f2b8c41a1ba41608ebf9bb38e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f31dfaadc372462e8a64ca253144f83d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d5ba0a0acb914906bd0e3c780d4272fe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_39bc3703ab4a495ca62c3c38b61499b3","IPY_MODEL_260ccf92d3ec453daf96aac32a2e6051","IPY_MODEL_c2b37233d0ae4d0f987769481bf4a294"],"layout":"IPY_MODEL_1af45d64d0764a019e6da49e29b1d27a"}},"39bc3703ab4a495ca62c3c38b61499b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_401f77ddd97a4a63a15c9092b5731fc4","placeholder":"​","style":"IPY_MODEL_3f75ec189f8b49c7a0dac03d6a516060","value":"100%"}},"260ccf92d3ec453daf96aac32a2e6051":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a852bb77efbd44588d5b4f9ed713609e","max":612602,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eb62623535204039ad1d203b9ad7b326","value":612602}},"c2b37233d0ae4d0f987769481bf4a294":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_15e1514a3b11453a8082b8627c227b72","placeholder":"​","style":"IPY_MODEL_071385b8a2e64909935d192079f1b884","value":" 612602/612602 [00:30&lt;00:00, 24346.37it/s]"}},"1af45d64d0764a019e6da49e29b1d27a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"401f77ddd97a4a63a15c9092b5731fc4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f75ec189f8b49c7a0dac03d6a516060":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a852bb77efbd44588d5b4f9ed713609e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb62623535204039ad1d203b9ad7b326":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"15e1514a3b11453a8082b8627c227b72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"071385b8a2e64909935d192079f1b884":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f8d5efc2fac34d5fbb266fd0ea6a4442":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e443d3ef54f64e56b7059c958c634337","IPY_MODEL_8291bc6ac8f44f1a845ea536538a9fa7","IPY_MODEL_304be1e7971f4a9b8ba7872cb0b5f955"],"layout":"IPY_MODEL_29a641670ec74293a76c5f95dc6555fc"}},"e443d3ef54f64e56b7059c958c634337":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2dceabd539be499b8665cd9aa7fc1cc9","placeholder":"​","style":"IPY_MODEL_c5ff0e998f1e417d9f00ccf26606485a","value":"100%"}},"8291bc6ac8f44f1a845ea536538a9fa7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8e20cea22d94c9cb85700600ee51dfe","max":612602,"min":0,"orientation":"horizontal","style":"IPY_MODEL_66e8a4d819ed4e48b7d542fb4c27798b","value":612602}},"304be1e7971f4a9b8ba7872cb0b5f955":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8dcbb1c0b764bc9be1d9b3e47351574","placeholder":"​","style":"IPY_MODEL_b9edba3c0fb94d72a26acb0ce4acbf51","value":" 612602/612602 [00:00&lt;00:00, 938660.80it/s]"}},"29a641670ec74293a76c5f95dc6555fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2dceabd539be499b8665cd9aa7fc1cc9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5ff0e998f1e417d9f00ccf26606485a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8e20cea22d94c9cb85700600ee51dfe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66e8a4d819ed4e48b7d542fb4c27798b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b8dcbb1c0b764bc9be1d9b3e47351574":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9edba3c0fb94d72a26acb0ce4acbf51":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd26dec55ba54cbea581e2bc6bb9896e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_25b1c6c7718e430c82a8e3c2fcdf4b9e","IPY_MODEL_0d4d4750cc094e51ba3fb644af0d6e21","IPY_MODEL_16177cf50da0456bb57fede520c63001"],"layout":"IPY_MODEL_08e46ea34b464e03a309596a322394cc"}},"25b1c6c7718e430c82a8e3c2fcdf4b9e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a81daa308691458aa827296793220425","placeholder":"​","style":"IPY_MODEL_76405b65e6df492aaf0d945877360f00","value":"100%"}},"0d4d4750cc094e51ba3fb644af0d6e21":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9090f0d48c8e43b8aaef555992aca8f1","max":612602,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8e45e361900e4377a2e1841c6f75ec30","value":612602}},"16177cf50da0456bb57fede520c63001":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f0ffebfca1ce4d50a1979a92672af1a5","placeholder":"​","style":"IPY_MODEL_658af1224823430a95d5d9746bbe0b97","value":" 612602/612602 [00:30&lt;00:00, 23971.90it/s]"}},"08e46ea34b464e03a309596a322394cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a81daa308691458aa827296793220425":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76405b65e6df492aaf0d945877360f00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9090f0d48c8e43b8aaef555992aca8f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e45e361900e4377a2e1841c6f75ec30":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f0ffebfca1ce4d50a1979a92672af1a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"658af1224823430a95d5d9746bbe0b97":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24751bd60fad4b3782ad5efc6215c555":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_817a047c3bce4434ad795af4c7a587cc","IPY_MODEL_6c8408d88b2f4c198ecc7c2fff0aeb98","IPY_MODEL_570d47fc94e44395b6503a7b49ba0266"],"layout":"IPY_MODEL_9c78124e584c4893ad74ba76e84283ad"}},"817a047c3bce4434ad795af4c7a587cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9918feba471e4d4cb7703c90087a95c6","placeholder":"​","style":"IPY_MODEL_ca7d3f75152a4351839ae8d7346ac080","value":"100%"}},"6c8408d88b2f4c198ecc7c2fff0aeb98":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6db65fcee9404327b07683fca5815a82","max":612602,"min":0,"orientation":"horizontal","style":"IPY_MODEL_94bb5704a7524786b53afdf82110b4a2","value":612602}},"570d47fc94e44395b6503a7b49ba0266":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ff8cfcdd9764bbb87fc713774893565","placeholder":"​","style":"IPY_MODEL_157da8d746704a6eb7689ab8ab2c05f3","value":" 612602/612602 [00:00&lt;00:00, 923497.73it/s]"}},"9c78124e584c4893ad74ba76e84283ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9918feba471e4d4cb7703c90087a95c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca7d3f75152a4351839ae8d7346ac080":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6db65fcee9404327b07683fca5815a82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94bb5704a7524786b53afdf82110b4a2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0ff8cfcdd9764bbb87fc713774893565":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"157da8d746704a6eb7689ab8ab2c05f3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"12cb82c88a3946128269436929769660":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eb1c4839c49f463fb321e3c722af68e2","IPY_MODEL_e204832fb8cb488b8a92bdabd055b7a5","IPY_MODEL_377a291d13ab4d1fae9315e2d2e1381f"],"layout":"IPY_MODEL_b8e35b2526d34b9fac71d2534f7bfdc4"}},"eb1c4839c49f463fb321e3c722af68e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae0cc1292d9c4157bc4e356df5fe3931","placeholder":"​","style":"IPY_MODEL_b54182c1618e48a59af278d3fbedda05","value":"100%"}},"e204832fb8cb488b8a92bdabd055b7a5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b82445c11664de29ae3f26144df7046","max":612602,"min":0,"orientation":"horizontal","style":"IPY_MODEL_403030e15d9c4aa694d565c1bd7bf852","value":612602}},"377a291d13ab4d1fae9315e2d2e1381f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b557efa384a845e997ba54c5abe56272","placeholder":"​","style":"IPY_MODEL_eff26e26da9c4b24af9a266bb5fdd8c8","value":" 612602/612602 [00:30&lt;00:00, 23114.20it/s]"}},"b8e35b2526d34b9fac71d2534f7bfdc4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae0cc1292d9c4157bc4e356df5fe3931":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b54182c1618e48a59af278d3fbedda05":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8b82445c11664de29ae3f26144df7046":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"403030e15d9c4aa694d565c1bd7bf852":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b557efa384a845e997ba54c5abe56272":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eff26e26da9c4b24af9a266bb5fdd8c8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c2b0099e5324495893453c16cec39104":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_abbaa4da1f5d4a3897daaf40b5c1a14b","IPY_MODEL_3124e3f2dc7e4ad5a41e9cce23f8baf0","IPY_MODEL_1e7346dd582c445a98557b83d9b75740"],"layout":"IPY_MODEL_1ac1231dbfb5451ebfc513709b042304"}},"abbaa4da1f5d4a3897daaf40b5c1a14b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d973ad80e28d4c758430706d6493881d","placeholder":"​","style":"IPY_MODEL_36848b89df704c8ba2b193c101dfceb0","value":"100%"}},"3124e3f2dc7e4ad5a41e9cce23f8baf0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e61ad6e2b8b4f8893ea16fa96401386","max":612602,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b9ad54c1127841c9b2071cb4a0b4d1bc","value":612602}},"1e7346dd582c445a98557b83d9b75740":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7dd806b3ea674ab4846e4876d6c099bc","placeholder":"​","style":"IPY_MODEL_fbe5798aa67b40f88210fdb4e8762262","value":" 612602/612602 [00:00&lt;00:00, 945189.86it/s]"}},"1ac1231dbfb5451ebfc513709b042304":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d973ad80e28d4c758430706d6493881d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36848b89df704c8ba2b193c101dfceb0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e61ad6e2b8b4f8893ea16fa96401386":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9ad54c1127841c9b2071cb4a0b4d1bc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7dd806b3ea674ab4846e4876d6c099bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbe5798aa67b40f88210fdb4e8762262":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a284a67098574d22a5ce1e2457757f7d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_10412e630c2f4da18ee909f1307ab73a","IPY_MODEL_1a59f0cf374a4fe7a42082c89a5ce8e1","IPY_MODEL_1c70cac4bb964e9ca0c4ee48ed47c988"],"layout":"IPY_MODEL_9b6872f797be43c6be38112a8d4006d6"}},"10412e630c2f4da18ee909f1307ab73a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f6629d74c10413097450b908296b558","placeholder":"​","style":"IPY_MODEL_44c4b61f8e814eb287232fdeba2768e1","value":"100%"}},"1a59f0cf374a4fe7a42082c89a5ce8e1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_680cdf8f2c2a4595a0268ef9477bc36b","max":612602,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a993ad5a2565427aa2d97793761017d1","value":612602}},"1c70cac4bb964e9ca0c4ee48ed47c988":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d3f786aec014ffc99646dc2138bbdf0","placeholder":"​","style":"IPY_MODEL_b197d043725348e78c90dcfb18a4796e","value":" 612602/612602 [00:30&lt;00:00, 23194.85it/s]"}},"9b6872f797be43c6be38112a8d4006d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f6629d74c10413097450b908296b558":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44c4b61f8e814eb287232fdeba2768e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"680cdf8f2c2a4595a0268ef9477bc36b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a993ad5a2565427aa2d97793761017d1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7d3f786aec014ffc99646dc2138bbdf0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b197d043725348e78c90dcfb18a4796e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"941b59b26d99412ead555431db0469c1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8f0c80e24053402b9e2435145b2fb9c9","IPY_MODEL_b55484d482ed4529bebf28b47516b893","IPY_MODEL_3256afcaffcf449fb09a51bd2ceb37aa"],"layout":"IPY_MODEL_94d5d8e942d14998803ec44e42e86588"}},"8f0c80e24053402b9e2435145b2fb9c9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_60a7a36d92064040b3c82e6b85e0e6b0","placeholder":"​","style":"IPY_MODEL_d41246faf43449d1b2dddb714e51f7a1","value":"100%"}},"b55484d482ed4529bebf28b47516b893":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e3895df37bd4e63908a43b81a059513","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5ccc7e11a15e467591fb42ad945a015e","value":2}},"3256afcaffcf449fb09a51bd2ceb37aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_526cd30fe20e4bafa08329c16b5ed160","placeholder":"​","style":"IPY_MODEL_4a167a7227854381806c883a542b01f9","value":" 2/2 [00:01&lt;00:00,  1.13s/it]"}},"94d5d8e942d14998803ec44e42e86588":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60a7a36d92064040b3c82e6b85e0e6b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d41246faf43449d1b2dddb714e51f7a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3e3895df37bd4e63908a43b81a059513":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ccc7e11a15e467591fb42ad945a015e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"526cd30fe20e4bafa08329c16b5ed160":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a167a7227854381806c883a542b01f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f2e6b643cf9447f96f4617cab77d51f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2894046bbc214e9a88b8b4c2763f0fbe","IPY_MODEL_4d5f49d6179249d3bc3f7ec187a113bc","IPY_MODEL_f07517c5583141b7b203534996e47073"],"layout":"IPY_MODEL_7afd4dae04cf44649190cf7f1bf00eaa"}},"2894046bbc214e9a88b8b4c2763f0fbe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4666d5d76e40486a96416440837a04e6","placeholder":"​","style":"IPY_MODEL_260cd738f92a4e2da00f81c836d73839","value":"100%"}},"4d5f49d6179249d3bc3f7ec187a113bc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5fd60296ecb429589335bdaedbdd8d5","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1743d331df974f1a8891c4edf13d4c91","value":2}},"f07517c5583141b7b203534996e47073":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5d0518c98704e60bdf7059e2c00be00","placeholder":"​","style":"IPY_MODEL_8dd66f44bb8c4ecfb383eb657afaec97","value":" 2/2 [00:01&lt;00:00,  1.10s/it]"}},"7afd4dae04cf44649190cf7f1bf00eaa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4666d5d76e40486a96416440837a04e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"260cd738f92a4e2da00f81c836d73839":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c5fd60296ecb429589335bdaedbdd8d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1743d331df974f1a8891c4edf13d4c91":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b5d0518c98704e60bdf7059e2c00be00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8dd66f44bb8c4ecfb383eb657afaec97":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef497effc689438a8a81b2fa510bdedf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0343fe4023774746a9964f519de16c62","IPY_MODEL_1c27a7047fcc497cbc1be7d43ce1c22c","IPY_MODEL_2464f9fbc4d644d19054754d875dc9de"],"layout":"IPY_MODEL_72baebaa4c674ee2b021c7cb89410209"}},"0343fe4023774746a9964f519de16c62":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b31410143cb746919e9206d7a4234bff","placeholder":"​","style":"IPY_MODEL_fc7e9559d432451a8fe13d26ecace871","value":"100%"}},"1c27a7047fcc497cbc1be7d43ce1c22c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_49ebd576bcb34b3bbc0b483047615cf5","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c50288771b5d477d8443662c280fabac","value":2}},"2464f9fbc4d644d19054754d875dc9de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d8bd1265146434a96add1d73d4f3b3e","placeholder":"​","style":"IPY_MODEL_02d5a192d51d42e29a09d6e8c4e6b5f9","value":" 2/2 [00:01&lt;00:00,  1.10s/it]"}},"72baebaa4c674ee2b021c7cb89410209":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b31410143cb746919e9206d7a4234bff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc7e9559d432451a8fe13d26ecace871":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"49ebd576bcb34b3bbc0b483047615cf5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c50288771b5d477d8443662c280fabac":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3d8bd1265146434a96add1d73d4f3b3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02d5a192d51d42e29a09d6e8c4e6b5f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c2c4d2e64e447dca185591b1fa7d000":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_51dd188dae8e49b7aa62a90814871f00","IPY_MODEL_a5ba00f6f6a04c2fadb6c8a0f8d2aa42","IPY_MODEL_85c67506e3f24cdfa15311a6c5e5dca7"],"layout":"IPY_MODEL_a192d24d130f48b192ec3c4795ec20d9"}},"51dd188dae8e49b7aa62a90814871f00":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a05307bcd0864b42aadc129a41a7c223","placeholder":"​","style":"IPY_MODEL_e8638c99b34047d5a6da131f19992623","value":"100%"}},"a5ba00f6f6a04c2fadb6c8a0f8d2aa42":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_02eb542bd71b49deb0c3c037962d3525","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_78e44d3897254bed9d880bc352db9ef0","value":2}},"85c67506e3f24cdfa15311a6c5e5dca7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d4b53edfb7b402cb7466203eb99099e","placeholder":"​","style":"IPY_MODEL_2853220ffced40d6893576a91b657b1f","value":" 2/2 [00:01&lt;00:00,  1.10s/it]"}},"a192d24d130f48b192ec3c4795ec20d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a05307bcd0864b42aadc129a41a7c223":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8638c99b34047d5a6da131f19992623":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"02eb542bd71b49deb0c3c037962d3525":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78e44d3897254bed9d880bc352db9ef0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4d4b53edfb7b402cb7466203eb99099e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2853220ffced40d6893576a91b657b1f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}