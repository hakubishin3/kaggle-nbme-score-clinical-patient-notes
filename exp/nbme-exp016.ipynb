{"cells":[{"cell_type":"markdown","metadata":{"id":"aa1f8e80"},"source":["## References"],"id":"aa1f8e80"},{"cell_type":"markdown","metadata":{"id":"c0138fac"},"source":["- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"],"id":"c0138fac"},{"cell_type":"markdown","metadata":{"id":"cf1dfda9"},"source":["## Configurations"],"id":"cf1dfda9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a7a78d25"},"outputs":[],"source":["EXP_NAME = \"nbme-exp016\"\n","ENV = \"colab\"\n","DEBUG_MODE = False\n","SUBMISSION_MODE = False"],"id":"a7a78d25"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ecc4e4d"},"outputs":[],"source":["class CFG:\n","    env=ENV\n","    exp_name=EXP_NAME\n","    debug=DEBUG_MODE\n","    submission=SUBMISSION_MODE\n","    apex=True\n","    input_dir=None\n","    output_dir=None\n","    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n","    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n","    competition_name=\"nbme-score-clinical-patient-notes\"\n","    id_col=\"id\"\n","    target_col=\"location\"\n","    pretrained_model_name=\"microsoft/deberta-base\"\n","    tokenizer=None\n","    max_len=None\n","    output_dim=1\n","    dropout=0.2\n","    num_workers=4\n","    batch_size=8\n","    lr=2e-5\n","    betas=(0.9, 0.98)\n","    weight_decay=0.1\n","    num_warmup_steps_rate=0.1\n","    batch_scheduler=True\n","    epochs=5\n","    n_fold=5\n","    train_fold=[0, 1, 2, 3, 4]\n","    seed=71\n","    gradient_accumulation_steps=1\n","    max_grad_norm=1000\n","    print_freq=100\n","    train=True\n","    inference=True"],"id":"4ecc4e4d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"3894c88b"},"outputs":[],"source":["if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.train_fold = [0, 1]\n","\n","if CFG.submission:\n","    CFG.train = False\n","    CFG.inference = True"],"id":"3894c88b"},{"cell_type":"markdown","metadata":{"id":"31768c85"},"source":["## Directory Settings"],"id":"31768c85"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4744,"status":"ok","timestamp":1646284912879,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"00e7d967","outputId":"9d7cbcfc-07c5-464c-a3e4-a778c4683944"},"outputs":[{"name":"stdout","output_type":"stream","text":["colab\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"]}],"source":["import sys\n","from pathlib import Path\n","\n","\n","print(CFG.env)\n","if CFG.env == \"colab\":\n","    # colab環境\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","    # install packages\n","    !pip install transformers\n","\n","elif CFG.env == \"local\":\n","    # ローカルサーバ\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","\n","elif CFG.env == \"kaggle\":\n","    # kaggle環境\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./\")"],"id":"00e7d967"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d726b7d9"},"outputs":[],"source":["import gc\n","import os\n","import ast\n","import time\n","import math\n","import random\n","import itertools\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","from scipy.optimize import minimize\n","from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as T\n","from torchvision.io import read_image\n","from torch.utils.data import DataLoader, Dataset\n","\n","from transformers import AutoModelForMaskedLM\n","from transformers import BartModel,BertModel,BertTokenizer\n","from transformers import DebertaModel,DebertaTokenizer\n","from transformers import RobertaModel,RobertaTokenizer\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"id":"d726b7d9"},{"cell_type":"markdown","metadata":{"id":"b6d82f71"},"source":["## Utilities"],"id":"b6d82f71"},{"cell_type":"code","execution_count":null,"metadata":{"id":"95abbe2c"},"outputs":[],"source":["def micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on binary arrays.\n","\n","    Args:\n","        preds (list of lists of ints): Predictions.\n","        truths (list of lists of ints): Ground truths.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    # Micro : aggregating over all instances\n","    preds = np.concatenate(preds)\n","    truths = np.concatenate(truths)\n","    return f1_score(truths, preds)\n","\n","\n","def spans_to_binary(spans, length=None):\n","    \"\"\"\n","    Converts spans to a binary array indicating whether each character is in the span.\n","\n","    Args:\n","        spans (list of lists of two ints): Spans.\n","\n","    Returns:\n","        np array [length]: Binarized spans.\n","    \"\"\"\n","    length = np.max(spans) if length is None else length\n","    binary = np.zeros(length)\n","    for start, end in spans:\n","        binary[start:end] = 1\n","    return binary\n","\n","\n","def span_micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on spans.\n","\n","    Args:\n","        preds (list of lists of two ints): Prediction spans.\n","        truths (list of lists of two ints): Ground truth spans.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    bin_preds = []\n","    bin_truths = []\n","    for pred, truth in zip(preds, truths):\n","        if not len(pred) and not len(truth):\n","            continue\n","        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n","        bin_preds.append(spans_to_binary(pred, length))\n","        bin_truths.append(spans_to_binary(truth, length))\n","    return micro_f1(bin_preds, bin_truths)\n","\n","\n","def get_score(y_true, y_pred):\n","    score = span_micro_f1(y_true, y_pred)\n","    return score"],"id":"95abbe2c"},{"cell_type":"code","execution_count":null,"metadata":{"id":"832ee36d"},"outputs":[],"source":["def create_labels_for_scoring(df):\n","    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n","    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n","    for i in range(len(df)):\n","        lst = df.loc[i, \"location\"]\n","        if lst:\n","            new_lst = \";\".join(lst)\n","            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n","\n","    # create labels\n","    truths = []\n","    for location_list in df[\"location_for_create_labels\"].values:\n","        truth = []\n","        if len(location_list) > 0:\n","            location = location_list[0]\n","            for loc in [s.split() for s in location.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                truth.append([start, end])\n","        truths.append(truth)\n","\n","    return truths\n","\n","\n","def get_char_probs(texts, token_probs, tokenizer):\n","    res = [np.zeros(len(t)) for t in texts]\n","    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n","        encoded = tokenizer(\n","            text=text,\n","            max_length=CFG.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n","            start, end = offset_mapping\n","            res[i][start:end] = pred\n","    return res\n","\n","\n","def get_predicted_location_str(char_probs, th=0.5):\n","    results = []\n","    for char_prob in char_probs:\n","        result = np.where(char_prob >= th)[0] + 1\n","        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n","        result = [f\"{min(r)} {max(r)}\" for r in result]\n","        result = \";\".join(result)\n","        results.append(result)\n","    return results\n","\n","\n","def get_predictions(results):\n","    predictions = []\n","    for result in results:\n","        prediction = []\n","        if result != \"\":\n","            for loc in [s.split() for s in result.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                prediction.append([start, end])\n","        predictions.append(prediction)\n","    return predictions\n","\n","\n","def scoring(df, th=0.5):\n","    labels = create_labels_for_scoring(df)\n","\n","    token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n","    char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n","    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n","    preds = get_predictions(predicted_location_str)\n","\n","    score = get_score(labels, preds)\n","    return score\n","\n","\n","def get_best_thres(oof_df):\n","    def f1_opt(x):\n","        return -1 * scoring(oof_df, th=x)\n","\n","    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n","    return best_thres"],"id":"832ee36d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"918828a7"},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return \"%dm %ds\" % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"],"id":"918828a7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d02a78e1"},"outputs":[],"source":["seed_everything()"],"id":"d02a78e1"},{"cell_type":"markdown","metadata":{"id":"47266f39"},"source":["## Data Loading"],"id":"47266f39"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":517,"status":"ok","timestamp":1646284917081,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"20fed6da","outputId":"16135bdf-9a76-49ee-eeca-24764428d6cb"},"outputs":[{"data":{"text/plain":["((14300, 6), (143, 3), (42146, 3), (5, 4))"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["train = pd.read_csv(CFG.input_dir / \"train.csv\")\n","features = pd.read_csv(CFG.input_dir / \"features.csv\")\n","patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n","test = pd.read_csv(CFG.input_dir / \"test.csv\")\n","\n","train.shape, features.shape, patient_notes.shape, test.shape"],"id":"20fed6da"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e67d0132"},"outputs":[],"source":["if CFG.debug:\n","    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n","    print(train.shape)"],"id":"e67d0132"},{"cell_type":"markdown","metadata":{"id":"47bca11a"},"source":["## Preprocessing"],"id":"47bca11a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d9c8e9ba"},"outputs":[],"source":["def preprocess_features(features):\n","    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n","    return features\n","\n","\n","features = preprocess_features(features)"],"id":"d9c8e9ba"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1646284917082,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"7ef41e18","outputId":"716ed285-cf21-4d03-aaa6-f779ecc5eb72"},"outputs":[{"data":{"text/plain":["((14300, 8), (5, 6))"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","\n","train.shape, test.shape"],"id":"7ef41e18"},{"cell_type":"code","execution_count":null,"metadata":{"id":"8233df16"},"outputs":[],"source":["train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n","train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"],"id":"8233df16"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1646284917083,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"e9143e61","outputId":"c427e93b-072a-46a6-b0b5-25f3b771bd98"},"outputs":[{"data":{"text/plain":["0    4399\n","1    8181\n","2    1296\n","3     287\n","4      99\n","5      27\n","6       9\n","7       1\n","8       1\n","Name: annotation_length, dtype: int64"]},"metadata":{},"output_type":"display_data"}],"source":["train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n","display(train['annotation_length'].value_counts().sort_index())"],"id":"e9143e61"},{"cell_type":"markdown","metadata":{"id":"6bdc7949"},"source":["## CV split"],"id":"6bdc7949"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c4acf61d"},"outputs":[],"source":["def get_groupkfold(df, group_name):\n","    groups = df[group_name].unique()\n","\n","    kf = KFold(\n","        n_splits=CFG.n_fold,\n","        shuffle=True,\n","        random_state=CFG.seed,\n","    )\n","    folds_ids = []\n","    for i_fold, (_, val_group_idx) in enumerate(kf.split(groups)):\n","        val_group = groups[val_group_idx]\n","        is_val = df[group_name].isin(val_group)\n","        val_idx = df[is_val].index\n","        df.loc[val_idx, \"fold\"] = int(i_fold)\n","\n","    df[\"fold\"] = df[\"fold\"].astype(int)\n","    return df"],"id":"c4acf61d"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1646284917083,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"2ca0c08e","outputId":"3aa64f27-a28e-4e6c-c806-e19b6a25d265"},"outputs":[{"data":{"text/plain":["fold\n","0    2902\n","1    2894\n","2    2813\n","3    2791\n","4    2900\n","dtype: int64"]},"metadata":{},"output_type":"display_data"}],"source":["train = get_groupkfold(train, \"pn_num\")\n","display(train.groupby(\"fold\").size())"],"id":"2ca0c08e"},{"cell_type":"markdown","metadata":{"id":"a8560070"},"source":["## Setup tokenizer"],"id":"a8560070"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c316b13f"},"outputs":[],"source":["if CFG.submission:\n","    tokenizer = AutoTokenizer.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n","else:\n","    tokenizer = AutoTokenizer.from_pretrained(CFG.pretrained_model_name)\n","    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n","\n","CFG.tokenizer = tokenizer"],"id":"c316b13f"},{"cell_type":"markdown","metadata":{"id":"e689a7fc"},"source":["## Create dataset"],"id":"e689a7fc"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["d32fe5c8d7484fe487949b77035864e4","ec70c4a5e8e84f568e35e23e215f0c72","e516b811f7cb40c9a389d65b87f09eb6","ed26fa6977b24204a7fe3898b4c5100f","592dd30a52f5443ba41c181d2aa6186c","7e8283e0d1f94166906e2152e1e052ab","bc5c1f5e1fa3449480ee30b3bd4c32b3","dd2e880359ab43dca23c9f421a5a3525","c5fe7a1649094faeaeab1ab4b6b05b45","a773a22c91464f5e918c0baa9308845a","7337f75f00224b688d339f93ca08e6b5"]},"executionInfo":{"elapsed":22585,"status":"ok","timestamp":1646284941529,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"df31758e","outputId":"6ef5b1dd-2d8f-4e94-e40e-a26a2d7bed9b"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d32fe5c8d7484fe487949b77035864e4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/42146 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["max length: 433\n"]}],"source":["pn_history_lengths = []\n","tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    pn_history_lengths.append(length)\n","\n","print(\"max length:\", np.max(pn_history_lengths))"],"id":"df31758e"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["1220250f7e17452fb0a7e24052b71197","5735fc4858054cb2aefccba5ffa6bd14","ffb3bd07a8b7450a8d980816356e54db","8933a0baf3eb4dfa9b43b73c2ecf499b","482b5c91256442e9a3acc4dcdb0150cf","fe5f3260ff9040468b6c913595318387","c1514f61f2af4d07aae115959d038ba3","e0d32bbd3c2e4fcf924e2963f80f70ce","41ca547022f44ea5a1fdc052194944b3","f412a965b5744fc1b93ec083a65e739b","d7ce65dbd4e64e05bc719593b32d8492"]},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1646284941529,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"3caff24a","outputId":"34abaf57-453a-4c7b-eb90-060b1aa5adca"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1220250f7e17452fb0a7e24052b71197","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/143 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["max length: 30\n"]}],"source":["feature_text_lengths = []\n","tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    feature_text_lengths.append(length)\n","\n","print(\"max length:\", np.max(feature_text_lengths))"],"id":"3caff24a"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1646284941530,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"756d83ff","outputId":"aa0e1919-a6fc-470a-f21a-6485a46b0294"},"outputs":[{"name":"stdout","output_type":"stream","text":["max length: 466\n"]}],"source":["CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n","\n","print(\"max length:\", CFG.max_len)"],"id":"756d83ff"},{"cell_type":"code","execution_count":null,"metadata":{"id":"054b899a"},"outputs":[],"source":["class TrainingDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","        self.feature_nums = self.df[\"feature_num\"].values\n","        self.annotation_lengths = self.df[\"annotation_length\"].values\n","        self.locations = self.df[\"location\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def _create_label(self, pn_history, annotation_length, location_list):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        offset_mapping = encoded[\"offset_mapping\"]\n","        ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n","        label = np.zeros(len(offset_mapping))\n","        label[ignore_idxes] = -1\n","\n","        if annotation_length > 0:\n","            for location in location_list:\n","                for loc in [s.split() for s in location.split(\";\")]:\n","                    start, end = int(loc[0]), int(loc[1])\n","                    start_idx = -1\n","                    end_idx = -1\n","                    for idx in range(len(offset_mapping)):\n","                        if (start_idx == -1) & (start < offset_mapping[idx][0]):\n","                            start_idx = idx - 1\n","                        if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n","                            end_idx = idx + 1\n","                    if start_idx == -1:\n","                        start_idx = end_idx\n","                    if (start_idx != -1) & (end_idx != -1):\n","                        label[start_idx:end_idx] = 1\n","\n","        return torch.tensor(label, dtype=torch.float)\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        feature_num = self.feature_nums[idx]\n","        label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n","        return input_, feature_num, label"],"id":"054b899a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1d58367c"},"outputs":[],"source":["class TestDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","        self.feature_nums = self.df[\"feature_num\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        feature_num = self.feature_nums[idx]\n","        return input_, feature_num"],"id":"1d58367c"},{"cell_type":"markdown","metadata":{"id":"8c57abef"},"source":["## Model"],"id":"8c57abef"},{"cell_type":"code","execution_count":null,"metadata":{"id":"54f92d89"},"outputs":[],"source":["class CustomModel(nn.Module):\n","    def __init__(self, cfg, model_config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        if model_config_path is None:\n","            self.model_config = AutoConfig.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                output_hidden_states=True,\n","            )\n","        else:\n","            self.model_config = torch.load(model_config_path)\n","\n","        if pretrained:\n","            self.backbone = AutoModel.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                config=self.model_config,\n","            )\n","            print(f\"Load weight from pretrained\")\n","        else:\n","            #self.backbone = AutoModel.from_config(self.model_config)\n","            itpt = AutoModelForMaskedLM.from_config(self.model_config)\n","            path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp009/checkpoint-129000/pytorch_model.bin\")\n","            state_dict = torch.load(path)\n","            itpt.load_state_dict(state_dict)\n","            self.backbone = itpt.deberta\n","            print(f\"Load weight from {path}\")\n","\n","        self.fc = nn.Sequential(\n","            nn.Dropout(self.cfg.dropout),\n","            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n","        )\n","\n","    def forward(self, inputs):\n","        h = self.backbone(**inputs)[\"last_hidden_state\"]\n","        output = self.fc(h)\n","        return output"],"id":"54f92d89"},{"cell_type":"markdown","metadata":{"id":"91401041"},"source":["## Training"],"id":"91401041"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5400,"status":"ok","timestamp":1646284947304,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"kJWX45WJZcKo","outputId":"e9e5c56d-b7be-426c-dba7-5093e6bd6ecb"},"outputs":[{"data":{"text/plain":["{0: 1.0745106861642295,\n"," 1: 1.5256192975878016,\n"," 2: 0.8536032995875515,\n"," 3: 1.1817472815898014,\n"," 4: 0.5590601174853144,\n"," 5: 0.7864016997875266,\n"," 6: 0.8185726784151981,\n"," 7: 0.47255593050868644,\n"," 8: 0.3860517435320585,\n"," 9: 1.7393775778027745,\n"," 10: 1.1631596050493687,\n"," 11: 0.4189376327959005,\n"," 12: 0.14798650168728908,\n"," 100: 0.3424421947256593,\n"," 101: 0.5633495813023371,\n"," 102: 2.0718110236220473,\n"," 103: 1.454843144606924,\n"," 104: 0.40320959880014995,\n"," 105: 0.9758530183727034,\n"," 106: 1.7322284714410698,\n"," 107: 1.375488063992001,\n"," 108: 0.809278840144982,\n"," 109: 0.8407349081364829,\n"," 110: 1.1674490688663917,\n"," 111: 1.4519835020622422,\n"," 112: 0.20160479940007497,\n"," 200: 3.021212348456443,\n"," 201: 0.3953455818022747,\n"," 202: 0.16728908886389202,\n"," 203: 0.9808573928258967,\n"," 204: 0.9007874015748032,\n"," 205: 2.5186301712285966,\n"," 206: 0.7842569678790151,\n"," 207: 0.4117885264341957,\n"," 208: 0.23663542057242845,\n"," 209: 0.01143857017872766,\n"," 210: 0.8686164229471317,\n"," 211: 0.6777352830896138,\n"," 212: 4.178652668416448,\n"," 213: 1.291843519560055,\n"," 214: 0.8671866016747906,\n"," 215: 0.2823897012873391,\n"," 216: 0.32385451818522687,\n"," 300: 1.578522684664417,\n"," 301: 1.387641544806899,\n"," 302: 1.0023047119110111,\n"," 303: 0.604814398200225,\n"," 304: 0.6176827896512935,\n"," 305: 0.7377877765279339,\n"," 306: 1.3676240469941259,\n"," 307: 0.9887214098237721,\n"," 308: 0.1951706036745407,\n"," 309: 1.0330458692663416,\n"," 310: 0.6898887639045119,\n"," 311: 0.4489638795150606,\n"," 312: 0.5240294963129609,\n"," 313: 2.137582802149731,\n"," 314: 0.40749906261717284,\n"," 315: 0.4439595050618672,\n"," 400: 3.218527684039495,\n"," 401: 1.7150706161729783,\n"," 402: 2.0274865641794775,\n"," 403: 0.539042619672541,\n"," 404: 1.6492988376452944,\n"," 405: 0.7535158105236844,\n"," 406: 1.5635095613048369,\n"," 407: 0.2151881014873141,\n"," 408: 1.091668541432321,\n"," 409: 0.4518235220597425,\n"," 500: 1.3468916385451817,\n"," 501: 0.30383702037245347,\n"," 502: 0.3259992500937383,\n"," 503: 1.5320534933133358,\n"," 504: 1.8373203349581302,\n"," 505: 3.178492688413948,\n"," 506: 0.4461042369703787,\n"," 507: 0.9472565929258842,\n"," 508: 0.9837170353705788,\n"," 509: 2.730243719535058,\n"," 510: 1.1066816647919009,\n"," 511: 0.639130108736408,\n"," 512: 1.1638745156855395,\n"," 513: 0.9401074865641795,\n"," 514: 1.1317035370578676,\n"," 515: 0.3917710286214223,\n"," 516: 2.055368078990126,\n"," 517: 0.37103862017247846,\n"," 600: 0.6505686789151356,\n"," 601: 0.17586801649793776,\n"," 602: 0.437525309336333,\n"," 603: 1.4476940382452195,\n"," 604: 1.959570053743282,\n"," 605: 1.6435795525559305,\n"," 606: 0.9293838270216224,\n"," 607: 1.023037120359955,\n"," 608: 0.536182977127859,\n"," 609: 0.4696962879640045,\n"," 610: 1.4190976127984,\n"," 611: 0.5154505686789151,\n"," 700: 0.19803024621922258,\n"," 701: 0.9515460567429072,\n"," 702: 4.671941007374078,\n"," 703: 0.719915010623672,\n"," 704: 0.9880064991876015,\n"," 705: 0.23020122484689415,\n"," 706: 1.5613648293963256,\n"," 707: 0.45110861142357206,\n"," 708: 0.7470816147981502,\n"," 800: 0.9551206099237596,\n"," 801: 1.9452718410198724,\n"," 802: 0.250933633295838,\n"," 803: 1.8408948881389826,\n"," 804: 0.2902537182852144,\n"," 805: 0.44324459442569675,\n"," 806: 1.5227596550431197,\n"," 807: 0.0571928508936383,\n"," 808: 0.9765679290088739,\n"," 809: 0.15370578677665292,\n"," 810: 1.2174928133983252,\n"," 811: 1.0158880139982502,\n"," 812: 0.8600374953130858,\n"," 813: 1.6678865141857269,\n"," 814: 0.920089988751406,\n"," 815: 0.8192875890513686,\n"," 816: 1.1481464816897886,\n"," 817: 3.10628671416073,\n"," 900: 2.2197975253093363,\n"," 901: 0.4496787901512311,\n"," 902: 0.6991826021747282,\n"," 903: 0.1944556930383702,\n"," 904: 0.8700462442194726,\n"," 905: 0.35316585426821645,\n"," 906: 0.588371453568304,\n"," 907: 0.07506561679790026,\n"," 908: 0.28596425446819146,\n"," 909: 0.6033845769278839,\n"," 910: 0.4024946881639795,\n"," 911: 0.04432445944256968,\n"," 912: 1.6357155355580553,\n"," 913: 0.21804774403199598,\n"," 914: 0.9880064991876015,\n"," 915: 0.6305511811023622,\n"," 916: 0.5654943132108486}"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["labels = create_labels_for_scoring(train)\n","res = []\n","for i in labels:\n","    tot = 0\n","    for j in i:\n","        tot += j[1]-j[0]\n","    res.append(tot)\n","train[\"num_of_chars\"] = res\n","weights = dict((train.groupby('feature_num')['num_of_chars'].sum()) / (train.groupby('feature_num')['num_of_chars'].sum()).sum() * 143)\n","weights"],"id":"kJWX45WJZcKo"},{"cell_type":"code","execution_count":null,"metadata":{"id":"eda8175d"},"outputs":[],"source":["def train_fn(\n","    train_dataloader,\n","    model,\n","    criterion,\n","    optimizer,\n","    epoch,\n","    scheduler,\n","    device,\n","):\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, feature_nums, labels) in enumerate(train_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","\n","        feature_nums = torch.tensor([[num] * CFG.max_len for num in feature_nums]).view(-1, 1).to(device)\n","        feature_nums = torch.masked_select(feature_nums, labels.view(-1, 1) != -1)\n","        weight = []\n","        for case_num in feature_nums:\n","            weight.append(weights[case_num.item()])\n","        weight = torch.tensor(weight).to(device)\n","        loss = loss * weight\n","\n","        loss = loss.mean()\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","\n","        if CFG.batch_scheduler:\n","            scheduler.step()\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n","            print(\n","                \"Epoch: [{0}][{1}/{2}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                \"Grad: {grad_norm:.4f}  \"\n","                \"LR: {lr:.6f}  \"\n","                .format(\n","                    epoch+1,\n","                    step,\n","                    len(train_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n","                    loss=losses,\n","                     grad_norm=grad_norm,\n","                     lr=scheduler.get_lr()[0],\n","                )\n","            )\n","    return losses.avg"],"id":"eda8175d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c44b63a7"},"outputs":[],"source":["def valid_fn(\n","    val_dataloader,\n","    model,\n","    criterion,\n","    device,\n","):\n","    model.eval()\n","    preds = []\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, feature_nums, labels) in enumerate(val_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.no_grad():\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","\n","        feature_nums = torch.tensor([[num] * CFG.max_len for num in feature_nums]).view(-1, 1).to(device)\n","        feature_nums = torch.masked_select(feature_nums, labels.view(-1, 1) != -1)\n","        weight = []\n","        for case_num in feature_nums:\n","            weight.append(weights[case_num.item()])\n","        weight = torch.tensor(weight).to(device)\n","        loss = loss * weight\n","\n","        loss = loss.mean()\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(output.sigmoid().squeeze().detach().cpu().numpy())\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n","            print(\n","                \"EVAL: [{0}/{1}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                .format(\n","                    step, len(val_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n","                    loss=losses,\n","                )\n","            )\n","    preds = np.concatenate(preds)\n","    return losses.avg, preds"],"id":"c44b63a7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4219ac38"},"outputs":[],"source":["def inference_fn(test_dataloader, model, device):\n","    model.eval()\n","    model.to(device)\n","    preds = []\n","    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n","    for (inputs, case_nums) in tk0:\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        with torch.no_grad():\n","            output = model(inputs)\n","        preds.append(output.sigmoid().squeeze().detach().cpu().numpy())\n","    preds = np.concatenate(preds)\n","    return preds"],"id":"4219ac38"},{"cell_type":"code","execution_count":null,"metadata":{"id":"014a76b7"},"outputs":[],"source":["def train_loop(df, i_fold, device):\n","    print(f\"========== fold: {i_fold} training ==========\")\n","    train_idx = df[df[\"fold\"] != i_fold].index\n","    val_idx = df[df[\"fold\"] == i_fold].index\n","\n","    train_folds = df.loc[train_idx].reset_index(drop=True)\n","    val_folds = df.loc[val_idx].reset_index(drop=True)\n","\n","    train_dataset = TrainingDataset(CFG, train_folds)\n","    val_dataset = TrainingDataset(CFG, val_folds)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=True,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=True,\n","    )\n","    val_dataloader = DataLoader(\n","        val_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=False,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=False,\n","    )\n","\n","    # model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","    model = CustomModel(CFG, model_config_path=None, pretrained=False)   # itptを使うため\n","    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n","    model.to(device)\n","\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\"params\": [p for n, p in param_optimizer if not any(\n","            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n","        {\"params\": [p for n, p in param_optimizer if any(\n","            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n","    ]\n","    optimizer = AdamW(\n","        optimizer_grouped_parameters,\n","        lr=CFG.lr,\n","        betas=CFG.betas,\n","        weight_decay=CFG.weight_decay,\n","    )\n","    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n","    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        num_training_steps=num_train_optimization_steps,\n","    )\n","\n","    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    best_score = -1 * np.inf\n","\n","    for epoch in range(CFG.epochs):\n","        start_time = time.time()\n","        avg_loss = train_fn(\n","            train_dataloader,\n","            model,\n","            criterion,\n","            optimizer,\n","            epoch,\n","            scheduler,\n","            device,\n","        )\n","        avg_val_loss, val_preds = valid_fn(\n","            val_dataloader,\n","            model,\n","            criterion,\n","            device,\n","        )\n","\n","        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n","            scheduler.step()\n","\n","        # scoring\n","        val_folds[[str(i) for i in range(CFG.max_len)]] = val_preds\n","        score = scoring(val_folds, th=0.5)\n","\n","        elapsed = time.time() - start_time\n","\n","        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n","        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n","        if score > best_score:\n","            best_score = score\n","            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n","            torch.save({\n","                \"model\": model.state_dict(),\n","                \"predictions\": val_preds,\n","                },\n","                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","            )\n","\n","    predictions = torch.load(\n","        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","        map_location=torch.device(\"cpu\"),\n","    )[\"predictions\"]\n","    val_folds[[str(i) for i in range(CFG.max_len)]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return val_folds"],"id":"014a76b7"},{"cell_type":"markdown","metadata":{"id":"c38fb834"},"source":["## Main"],"id":"c38fb834"},{"cell_type":"code","execution_count":null,"metadata":{"id":"62d677cd"},"outputs":[],"source":["def main():\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    if CFG.train:\n","        oof_df = pd.DataFrame()\n","        for i_fold in range(CFG.n_fold):\n","            if i_fold in CFG.train_fold:\n","                _oof_df = train_loop(train, i_fold, device)\n","                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n","        #oof_df.to_csv(CFG.output_dir / \"oof_df.csv\", index=False)\n","        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    if CFG.submission:\n","        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n","    else:\n","        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    score = scoring(oof_df, th=0.5)\n","    print(f\"Best thres: 0.5, Score: {score:.4f}\")\n","    best_thres = get_best_thres(oof_df)\n","    score = scoring(oof_df, th=best_thres)\n","    print(f\"Best thres: {best_thres}, Score: {score:.4f}\")\n","\n","    if CFG.inference:\n","        test_dataset = TestDataset(CFG, test)\n","        test_dataloader = DataLoader(\n","            test_dataset,\n","            batch_size=CFG.batch_size,\n","            shuffle=False,\n","            num_workers=CFG.num_workers,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","        predictions = []\n","        for i_fold in CFG.train_fold:\n","            if CFG.submission:\n","                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n","                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n","            else:\n","                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n","\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            model.load_state_dict(state[\"model\"])\n","            test_token_probs = inference_fn(test_dataloader, model, device)\n","            test[[f\"fold{i_fold}_{i}\" for i in range(CFG.max_len)]] = test_token_probs\n","            test_char_probs = get_char_probs(test[\"pn_history\"].values, test_token_probs, CFG.tokenizer)\n","            predictions.append(test_char_probs)\n","\n","            del state, test_token_probs, model; gc.collect()\n","            torch.cuda.empty_cache()\n","\n","        predictions = np.mean(predictions, axis=0)\n","        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n","        test[CFG.target_col] = predicted_location_str\n","        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n","        test[[CFG.id_col, CFG.target_col]].to_csv(\n","            CFG.output_dir / \"submission.csv\", index=False\n","        )"],"id":"62d677cd"},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["24e4e49430f9475f9876b54d4e0df12c","211394f17c0c43b88143e2f88df1d774","79ea80b1b5f2486db1d6dca536471640","1d00cab8a56f4032af2503b61ee53907","9eb4543804384a0eadb8efdcb1aa72d2","76331fdef4a5448fafb9924f5ec252f9","2a146eb9aa04449c89a803768e6983da","953bc48260194236bc3d11b6e862b78c","12194b5f66734c5eb4afe84cb2952a6a","e762fb8a31b94215bcea55338d9c1208","0c7cc0a7920046bfbd748c92460a000e","eed95f53642440c8ac70d9a83066f0bb","c60b7df23ab14cdf9c74fef304b69916","b176c093b4d34b0693420212c6f780b5","8cd4e57e52984d3d9b94066ddbb3bee5","c78061c3cb87427c9b1b07e9bb2ec254","739029261f934eac8b25b25f939e6dfe","ff8a0486edcf4afdb1a4b179548edc77","7b5c4f803c58401384a391db12256999","903a4a7239264e8f9d2bd4e19d7b01d0","f7e4d51d453a40b0afe8d0e84c349ad2","6f1099e3abaa488fb760cd6093e67968","9437fe5e7f3d4dc4b4190f44aab12c9d","4ac5e0964dc8422fa8fe29c14887fc5a","9873854b29ab4aaaafc2a7bee3d4872f","01ee0aa9afbe416c904bc3286db9f7a5","c48a0bedb32a40c789306becd1cecd7f","aa498a8b82894905b41ae28f5762ea71","2cc6a4f403d14b9ebb77737c2c839d1e","906e5fd4fc3b434196758499eaf4d7b5","2e393ea6c5a74efe8684b3958633fb1a","090863dc8cec41719988077dd9aa346b","7dd57bbe841447cfbc2e2441744d30dc","e4e33393c97c42aaaae9f01618be7658","393cefdaa6904dd08461b32904d84099","857b78b3ff0041199b8674d383c98f46","b4931dc8e6f449409280c6171abd586d","e57234d0c68248ebab5f47ab2e5855bf","fa696636404a4768b57b3093e6a65759","fc9cdd0a5c1b42d5b0b776f3bf5d1c30","de29d2a723bf4d6e82f3a0dc7a34f336","50a23b5d2d444035a00f4dcd74b8cd63","91ee99383a1e474ebb11b6d617088dae","ec4b8387fb4a424bbceda2b1a6d66fa4","1be6f852e4b3450083b6efc3e0d897fb","82527d63da5c485c91b4da7f7cf70a76","c6a9dcb2003945ad8528c43df25f242f","24613ad69dfb4be89b11c8f5028b68a2","2fd56cd5e6b24ed4bef9a0369cac5d59","f8abab83d66e4d5db8244c4a71e5b557","ffe1a8fec73c47b89474316ec63b8dbc","171f0f3f2d8c4984bd0eb1f8d2ed9e7e","04400fd7c6384c8a86792e5c77e2efe3","d235dff0210c411ca45a3131af63a64a","c9578d1a5bfb4af6ab99e75ab0d641a4","6dbd3c2cbb614ebabcbebb1fc40678fc","cb386adc5ff14514941eecf366374baa","f6c9c774405d4a4a8e2f5a5b3b4a664f","a4cbdf34297b4895a97e80ae8519927f","09ebdd8f02404afea1b926edbeb42227","cf340943ad40464bb67a78ddf87e9ee3","147b933570604b3697cc7e312e5ee960","1234e9d75b974a279743cd703667660c","bda4d9eec6894bc2899dddb18501bbb3","be7eb139c082474da641d5f88bec0e67","470714f90d7549069e5c6c29d9a595e5"]},"id":"1d4fcf7c","executionInfo":{"status":"ok","timestamp":1646293757305,"user_tz":-540,"elapsed":171328,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}},"outputId":"0f5f24d9-e888-4151-a721-308c66a4b597"},"outputs":[{"output_type":"stream","name":"stdout","text":["========== fold: 0 training ==========\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp009/checkpoint-129000/pytorch_model.bin\n","Epoch: [1][0/1424] Elapsed 0m 0s (remain 14m 51s) Loss: 0.8744(0.8744) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/1424] Elapsed 0m 20s (remain 4m 34s) Loss: 0.1334(0.5169) Grad: 8747.5742  LR: 0.000003  \n","Epoch: [1][200/1424] Elapsed 0m 41s (remain 4m 10s) Loss: 0.1580(0.3158) Grad: 2609.8228  LR: 0.000006  \n","Epoch: [1][300/1424] Elapsed 1m 1s (remain 3m 49s) Loss: 0.1156(0.2442) Grad: 2074.2615  LR: 0.000008  \n","Epoch: [1][400/1424] Elapsed 1m 21s (remain 3m 28s) Loss: 0.0290(0.2036) Grad: 3558.5747  LR: 0.000011  \n","Epoch: [1][500/1424] Elapsed 1m 42s (remain 3m 7s) Loss: 0.1854(0.1723) Grad: 9050.5010  LR: 0.000014  \n","Epoch: [1][600/1424] Elapsed 2m 2s (remain 2m 47s) Loss: 0.0138(0.1499) Grad: 2651.4189  LR: 0.000017  \n","Epoch: [1][700/1424] Elapsed 2m 22s (remain 2m 26s) Loss: 0.0170(0.1330) Grad: 2560.0286  LR: 0.000020  \n","Epoch: [1][800/1424] Elapsed 2m 42s (remain 2m 6s) Loss: 0.0066(0.1204) Grad: 792.8763  LR: 0.000020  \n","Epoch: [1][900/1424] Elapsed 3m 2s (remain 1m 46s) Loss: 0.0277(0.1108) Grad: 1680.0532  LR: 0.000019  \n","Epoch: [1][1000/1424] Elapsed 3m 23s (remain 1m 25s) Loss: 0.0391(0.1030) Grad: 2256.3162  LR: 0.000019  \n","Epoch: [1][1100/1424] Elapsed 3m 43s (remain 1m 5s) Loss: 0.0064(0.0962) Grad: 2231.5430  LR: 0.000019  \n","Epoch: [1][1200/1424] Elapsed 4m 3s (remain 0m 45s) Loss: 0.0024(0.0902) Grad: 907.0742  LR: 0.000018  \n","Epoch: [1][1300/1424] Elapsed 4m 23s (remain 0m 24s) Loss: 0.0109(0.0852) Grad: 1348.2723  LR: 0.000018  \n","Epoch: [1][1400/1424] Elapsed 4m 44s (remain 0m 4s) Loss: 0.0045(0.0807) Grad: 567.5796  LR: 0.000018  \n","Epoch: [1][1423/1424] Elapsed 4m 48s (remain 0m 0s) Loss: 0.0974(0.0799) Grad: 14396.1621  LR: 0.000018  \n","EVAL: [0/363] Elapsed 0m 0s (remain 2m 3s) Loss: 0.0199(0.0199) \n","EVAL: [100/363] Elapsed 0m 7s (remain 0m 18s) Loss: 0.0210(0.0241) \n","EVAL: [200/363] Elapsed 0m 14s (remain 0m 11s) Loss: 0.0158(0.0246) \n","EVAL: [300/363] Elapsed 0m 20s (remain 0m 4s) Loss: 0.0052(0.0267) \n","EVAL: [362/363] Elapsed 0m 24s (remain 0m 0s) Loss: 0.0053(0.0233) \n","Epoch 1 - avg_train_loss: 0.0799  avg_val_loss: 0.0233  time: 321s\n","Epoch 1 - Score: 0.8327\n","Epoch 1 - Save Best Score: 0.8327 Model\n","Epoch: [2][0/1424] Elapsed 0m 0s (remain 10m 45s) Loss: 0.0013(0.0013) Grad: 3526.1086  LR: 0.000018  \n","Epoch: [2][100/1424] Elapsed 0m 20s (remain 4m 35s) Loss: 0.0236(0.0167) Grad: 101112.3984  LR: 0.000017  \n","Epoch: [2][200/1424] Elapsed 0m 41s (remain 4m 10s) Loss: 0.0232(0.0181) Grad: 29685.1738  LR: 0.000017  \n","Epoch: [2][300/1424] Elapsed 1m 1s (remain 3m 48s) Loss: 0.0053(0.0175) Grad: 24495.5449  LR: 0.000017  \n","Epoch: [2][400/1424] Elapsed 1m 21s (remain 3m 28s) Loss: 0.0107(0.0181) Grad: 18870.0059  LR: 0.000017  \n","Epoch: [2][500/1424] Elapsed 1m 41s (remain 3m 7s) Loss: 0.0044(0.0188) Grad: 7821.0264  LR: 0.000016  \n","Epoch: [2][600/1424] Elapsed 2m 2s (remain 2m 47s) Loss: 0.0012(0.0187) Grad: 4214.2100  LR: 0.000016  \n","Epoch: [2][700/1424] Elapsed 2m 22s (remain 2m 26s) Loss: 0.0166(0.0188) Grad: 34410.5156  LR: 0.000016  \n","Epoch: [2][800/1424] Elapsed 2m 42s (remain 2m 6s) Loss: 0.0277(0.0186) Grad: 38005.4180  LR: 0.000015  \n","Epoch: [2][900/1424] Elapsed 3m 2s (remain 1m 46s) Loss: 0.0157(0.0185) Grad: 28892.0859  LR: 0.000015  \n","Epoch: [2][1000/1424] Elapsed 3m 22s (remain 1m 25s) Loss: 0.0129(0.0180) Grad: 22317.3379  LR: 0.000015  \n","Epoch: [2][1100/1424] Elapsed 3m 43s (remain 1m 5s) Loss: 0.0124(0.0181) Grad: 11907.3623  LR: 0.000014  \n","Epoch: [2][1200/1424] Elapsed 4m 3s (remain 0m 45s) Loss: 0.0968(0.0183) Grad: 335519.7188  LR: 0.000014  \n","Epoch: [2][1300/1424] Elapsed 4m 23s (remain 0m 24s) Loss: 0.0008(0.0184) Grad: 5920.4917  LR: 0.000014  \n","Epoch: [2][1400/1424] Elapsed 4m 43s (remain 0m 4s) Loss: 0.0032(0.0186) Grad: 10466.2803  LR: 0.000013  \n","Epoch: [2][1423/1424] Elapsed 4m 48s (remain 0m 0s) Loss: 0.0222(0.0187) Grad: 37147.3984  LR: 0.000013  \n","EVAL: [0/363] Elapsed 0m 0s (remain 1m 54s) Loss: 0.0046(0.0046) \n","EVAL: [100/363] Elapsed 0m 7s (remain 0m 18s) Loss: 0.0243(0.0200) \n","EVAL: [200/363] Elapsed 0m 14s (remain 0m 11s) Loss: 0.0398(0.0231) \n","EVAL: [300/363] Elapsed 0m 20s (remain 0m 4s) Loss: 0.0047(0.0228) \n","EVAL: [362/363] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0082(0.0201) \n","Epoch 2 - avg_train_loss: 0.0187  avg_val_loss: 0.0201  time: 318s\n","Epoch 2 - Score: 0.8655\n","Epoch 2 - Save Best Score: 0.8655 Model\n","Epoch: [3][0/1424] Elapsed 0m 0s (remain 11m 5s) Loss: 0.0076(0.0076) Grad: 12234.3242  LR: 0.000013  \n","Epoch: [3][100/1424] Elapsed 0m 20s (remain 4m 31s) Loss: 0.0148(0.0116) Grad: 24570.7500  LR: 0.000013  \n","Epoch: [3][200/1424] Elapsed 0m 41s (remain 4m 10s) Loss: 0.0094(0.0145) Grad: 18480.0430  LR: 0.000013  \n","Epoch: [3][300/1424] Elapsed 1m 1s (remain 3m 49s) Loss: 0.0161(0.0160) Grad: 32491.2031  LR: 0.000012  \n","Epoch: [3][400/1424] Elapsed 1m 21s (remain 3m 28s) Loss: 0.0034(0.0154) Grad: 10324.4854  LR: 0.000012  \n","Epoch: [3][500/1424] Elapsed 1m 41s (remain 3m 7s) Loss: 0.0057(0.0159) Grad: 11998.9824  LR: 0.000012  \n","Epoch: [3][600/1424] Elapsed 2m 2s (remain 2m 47s) Loss: 0.0065(0.0159) Grad: 4071.5706  LR: 0.000011  \n","Epoch: [3][700/1424] Elapsed 2m 22s (remain 2m 26s) Loss: 0.0024(0.0156) Grad: 6225.6348  LR: 0.000011  \n","Epoch: [3][800/1424] Elapsed 2m 42s (remain 2m 6s) Loss: 0.0111(0.0153) Grad: 51586.0312  LR: 0.000011  \n","Epoch: [3][900/1424] Elapsed 3m 2s (remain 1m 46s) Loss: 0.0076(0.0154) Grad: 21734.8594  LR: 0.000011  \n","Epoch: [3][1000/1424] Elapsed 3m 23s (remain 1m 25s) Loss: 0.0020(0.0152) Grad: 6065.5298  LR: 0.000010  \n","Epoch: [3][1100/1424] Elapsed 3m 43s (remain 1m 5s) Loss: 0.0523(0.0149) Grad: 47482.4258  LR: 0.000010  \n","Epoch: [3][1200/1424] Elapsed 4m 3s (remain 0m 45s) Loss: 0.0027(0.0149) Grad: 10818.5537  LR: 0.000010  \n","Epoch: [3][1300/1424] Elapsed 4m 23s (remain 0m 24s) Loss: 0.1163(0.0149) Grad: 170435.6875  LR: 0.000009  \n","Epoch: [3][1400/1424] Elapsed 4m 43s (remain 0m 4s) Loss: 0.0034(0.0148) Grad: 6277.9482  LR: 0.000009  \n","Epoch: [3][1423/1424] Elapsed 4m 48s (remain 0m 0s) Loss: 0.0024(0.0148) Grad: 7337.5327  LR: 0.000009  \n","EVAL: [0/363] Elapsed 0m 0s (remain 1m 48s) Loss: 0.0082(0.0082) \n","EVAL: [100/363] Elapsed 0m 7s (remain 0m 18s) Loss: 0.0301(0.0215) \n","EVAL: [200/363] Elapsed 0m 14s (remain 0m 11s) Loss: 0.0144(0.0226) \n","EVAL: [300/363] Elapsed 0m 20s (remain 0m 4s) Loss: 0.0035(0.0232) \n","EVAL: [362/363] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0062(0.0206) \n","Epoch 3 - avg_train_loss: 0.0148  avg_val_loss: 0.0206  time: 318s\n","Epoch 3 - Score: 0.8745\n","Epoch 3 - Save Best Score: 0.8745 Model\n","Epoch: [4][0/1424] Elapsed 0m 0s (remain 11m 22s) Loss: 0.0007(0.0007) Grad: 2365.4214  LR: 0.000009  \n","Epoch: [4][100/1424] Elapsed 0m 20s (remain 4m 32s) Loss: 0.0142(0.0136) Grad: 28821.6270  LR: 0.000009  \n","Epoch: [4][200/1424] Elapsed 0m 41s (remain 4m 9s) Loss: 0.0001(0.0121) Grad: 310.6270  LR: 0.000008  \n","Epoch: [4][300/1424] Elapsed 1m 1s (remain 3m 49s) Loss: 0.0018(0.0124) Grad: 8197.5537  LR: 0.000008  \n","Epoch: [4][400/1424] Elapsed 1m 21s (remain 3m 28s) Loss: 0.0040(0.0128) Grad: 8527.7822  LR: 0.000008  \n","Epoch: [4][500/1424] Elapsed 1m 41s (remain 3m 7s) Loss: 0.0171(0.0125) Grad: 48663.4102  LR: 0.000007  \n","Epoch: [4][600/1424] Elapsed 2m 1s (remain 2m 46s) Loss: 0.0004(0.0125) Grad: 1434.1090  LR: 0.000007  \n","Epoch: [4][700/1424] Elapsed 2m 22s (remain 2m 26s) Loss: 0.0628(0.0129) Grad: 67285.9609  LR: 0.000007  \n","Epoch: [4][800/1424] Elapsed 2m 42s (remain 2m 6s) Loss: 0.0079(0.0126) Grad: 40855.1250  LR: 0.000006  \n","Epoch: [4][900/1424] Elapsed 3m 2s (remain 1m 45s) Loss: 0.1432(0.0129) Grad: 206108.4219  LR: 0.000006  \n","Epoch: [4][1000/1424] Elapsed 3m 22s (remain 1m 25s) Loss: 0.0060(0.0126) Grad: 11795.3711  LR: 0.000006  \n","Epoch: [4][1100/1424] Elapsed 3m 42s (remain 1m 5s) Loss: 0.0021(0.0124) Grad: 5749.5264  LR: 0.000005  \n","Epoch: [4][1200/1424] Elapsed 4m 3s (remain 0m 45s) Loss: 0.0158(0.0122) Grad: 61547.0273  LR: 0.000005  \n","Epoch: [4][1300/1424] Elapsed 4m 23s (remain 0m 24s) Loss: 0.0009(0.0124) Grad: 2282.5042  LR: 0.000005  \n","Epoch: [4][1400/1424] Elapsed 4m 43s (remain 0m 4s) Loss: 0.0001(0.0123) Grad: 374.2346  LR: 0.000005  \n","Epoch: [4][1423/1424] Elapsed 4m 48s (remain 0m 0s) Loss: 0.0012(0.0123) Grad: 2629.1467  LR: 0.000004  \n","EVAL: [0/363] Elapsed 0m 0s (remain 1m 47s) Loss: 0.0043(0.0043) \n","EVAL: [100/363] Elapsed 0m 7s (remain 0m 18s) Loss: 0.0281(0.0241) \n","EVAL: [200/363] Elapsed 0m 14s (remain 0m 11s) Loss: 0.0326(0.0242) \n","EVAL: [300/363] Elapsed 0m 20s (remain 0m 4s) Loss: 0.0017(0.0241) \n","EVAL: [362/363] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0092(0.0215) \n","Epoch 4 - avg_train_loss: 0.0123  avg_val_loss: 0.0215  time: 320s\n","Epoch 4 - Score: 0.8748\n","Epoch 4 - Save Best Score: 0.8748 Model\n","Epoch: [5][0/1424] Elapsed 0m 0s (remain 11m 5s) Loss: 0.0083(0.0083) Grad: 12089.0342  LR: 0.000004  \n","Epoch: [5][100/1424] Elapsed 0m 20s (remain 4m 33s) Loss: 0.0007(0.0078) Grad: 1910.7518  LR: 0.000004  \n","Epoch: [5][200/1424] Elapsed 0m 41s (remain 4m 10s) Loss: 0.0033(0.0080) Grad: 7850.3359  LR: 0.000004  \n","Epoch: [5][300/1424] Elapsed 1m 1s (remain 3m 49s) Loss: 0.0049(0.0093) Grad: 25315.9004  LR: 0.000004  \n","Epoch: [5][400/1424] Elapsed 1m 21s (remain 3m 28s) Loss: 0.0177(0.0094) Grad: 39258.8984  LR: 0.000003  \n","Epoch: [5][500/1424] Elapsed 1m 42s (remain 3m 8s) Loss: 0.0002(0.0097) Grad: 2279.0955  LR: 0.000003  \n","Epoch: [5][600/1424] Elapsed 2m 2s (remain 2m 47s) Loss: 0.0047(0.0098) Grad: 10156.1680  LR: 0.000003  \n","Epoch: [5][700/1424] Elapsed 2m 22s (remain 2m 27s) Loss: 0.0021(0.0101) Grad: 10735.9287  LR: 0.000002  \n","Epoch: [5][800/1424] Elapsed 2m 42s (remain 2m 6s) Loss: 0.0003(0.0096) Grad: 1464.1404  LR: 0.000002  \n","Epoch: [5][900/1424] Elapsed 3m 3s (remain 1m 46s) Loss: 0.0079(0.0094) Grad: 21766.7969  LR: 0.000002  \n","Epoch: [5][1000/1424] Elapsed 3m 23s (remain 1m 25s) Loss: 0.0236(0.0098) Grad: 45814.2305  LR: 0.000001  \n","Epoch: [5][1100/1424] Elapsed 3m 43s (remain 1m 5s) Loss: 0.0009(0.0101) Grad: 9148.3809  LR: 0.000001  \n","Epoch: [5][1200/1424] Elapsed 4m 3s (remain 0m 45s) Loss: 0.0243(0.0101) Grad: 81369.5469  LR: 0.000001  \n","Epoch: [5][1300/1424] Elapsed 4m 23s (remain 0m 24s) Loss: 0.0000(0.0101) Grad: 143.7289  LR: 0.000000  \n","Epoch: [5][1400/1424] Elapsed 4m 44s (remain 0m 4s) Loss: 0.0126(0.0100) Grad: 19348.5176  LR: 0.000000  \n","Epoch: [5][1423/1424] Elapsed 4m 48s (remain 0m 0s) Loss: 0.0023(0.0100) Grad: 18278.0957  LR: 0.000000  \n","EVAL: [0/363] Elapsed 0m 0s (remain 1m 45s) Loss: 0.0051(0.0051) \n","EVAL: [100/363] Elapsed 0m 7s (remain 0m 18s) Loss: 0.0390(0.0253) \n","EVAL: [200/363] Elapsed 0m 13s (remain 0m 11s) Loss: 0.0456(0.0257) \n","EVAL: [300/363] Elapsed 0m 20s (remain 0m 4s) Loss: 0.0042(0.0257) \n","EVAL: [362/363] Elapsed 0m 24s (remain 0m 0s) Loss: 0.0061(0.0229) \n","Epoch 5 - avg_train_loss: 0.0100  avg_val_loss: 0.0229  time: 321s\n","Epoch 5 - Score: 0.8764\n","Epoch 5 - Save Best Score: 0.8764 Model\n","========== fold: 1 training ==========\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp009/checkpoint-129000/pytorch_model.bin\n","Epoch: [1][0/1425] Elapsed 0m 0s (remain 10m 45s) Loss: 0.5915(0.5915) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/1425] Elapsed 0m 20s (remain 4m 30s) Loss: 0.1764(0.4790) Grad: 9931.7070  LR: 0.000003  \n","Epoch: [1][200/1425] Elapsed 0m 40s (remain 4m 8s) Loss: 0.0593(0.2968) Grad: 640.2121  LR: 0.000006  \n","Epoch: [1][300/1425] Elapsed 1m 0s (remain 3m 47s) Loss: 0.0864(0.2289) Grad: 1498.2172  LR: 0.000008  \n","Epoch: [1][400/1425] Elapsed 1m 21s (remain 3m 27s) Loss: 0.0326(0.1900) Grad: 1951.8503  LR: 0.000011  \n","Epoch: [1][500/1425] Elapsed 1m 41s (remain 3m 7s) Loss: 0.0558(0.1626) Grad: 7728.4536  LR: 0.000014  \n","Epoch: [1][600/1425] Elapsed 2m 1s (remain 2m 46s) Loss: 0.0142(0.1419) Grad: 1571.2675  LR: 0.000017  \n","Epoch: [1][700/1425] Elapsed 2m 21s (remain 2m 26s) Loss: 0.0128(0.1263) Grad: 1226.9473  LR: 0.000020  \n","Epoch: [1][800/1425] Elapsed 2m 41s (remain 2m 6s) Loss: 0.0690(0.1152) Grad: 9159.7939  LR: 0.000020  \n","Epoch: [1][900/1425] Elapsed 3m 2s (remain 1m 45s) Loss: 0.0688(0.1057) Grad: 8170.9600  LR: 0.000019  \n","Epoch: [1][1000/1425] Elapsed 3m 22s (remain 1m 25s) Loss: 0.0421(0.0980) Grad: 3063.7634  LR: 0.000019  \n","Epoch: [1][1100/1425] Elapsed 3m 42s (remain 1m 5s) Loss: 0.0238(0.0916) Grad: 3101.3293  LR: 0.000019  \n","Epoch: [1][1200/1425] Elapsed 4m 2s (remain 0m 45s) Loss: 0.0018(0.0863) Grad: 201.7398  LR: 0.000018  \n","Epoch: [1][1300/1425] Elapsed 4m 22s (remain 0m 25s) Loss: 0.0122(0.0815) Grad: 1615.9780  LR: 0.000018  \n","Epoch: [1][1400/1425] Elapsed 4m 43s (remain 0m 4s) Loss: 0.0010(0.0776) Grad: 121.5727  LR: 0.000018  \n","Epoch: [1][1424/1425] Elapsed 4m 48s (remain 0m 0s) Loss: 0.0333(0.0766) Grad: 5413.3110  LR: 0.000018  \n","EVAL: [0/362] Elapsed 0m 0s (remain 1m 52s) Loss: 0.0058(0.0058) \n","EVAL: [100/362] Elapsed 0m 7s (remain 0m 18s) Loss: 0.0085(0.0227) \n","EVAL: [200/362] Elapsed 0m 14s (remain 0m 11s) Loss: 0.0236(0.0218) \n","EVAL: [300/362] Elapsed 0m 21s (remain 0m 4s) Loss: 0.0542(0.0230) \n","EVAL: [361/362] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0044(0.0214) \n","Epoch 1 - avg_train_loss: 0.0766  avg_val_loss: 0.0214  time: 317s\n","Epoch 1 - Score: 0.8327\n","Epoch 1 - Save Best Score: 0.8327 Model\n","Epoch: [2][0/1425] Elapsed 0m 0s (remain 11m 16s) Loss: 0.0033(0.0033) Grad: 8604.1709  LR: 0.000018  \n","Epoch: [2][100/1425] Elapsed 0m 20s (remain 4m 34s) Loss: 0.0054(0.0220) Grad: 30908.0020  LR: 0.000017  \n","Epoch: [2][200/1425] Elapsed 0m 41s (remain 4m 10s) Loss: 0.0115(0.0193) Grad: 23043.3652  LR: 0.000017  \n","Epoch: [2][300/1425] Elapsed 1m 1s (remain 3m 49s) Loss: 0.0099(0.0173) Grad: 13925.1982  LR: 0.000017  \n","Epoch: [2][400/1425] Elapsed 1m 21s (remain 3m 28s) Loss: 0.0135(0.0175) Grad: 34780.3008  LR: 0.000017  \n","Epoch: [2][500/1425] Elapsed 1m 41s (remain 3m 7s) Loss: 0.0333(0.0181) Grad: 54625.2500  LR: 0.000016  \n","Epoch: [2][600/1425] Elapsed 2m 2s (remain 2m 47s) Loss: 0.1140(0.0186) Grad: 141276.0938  LR: 0.000016  \n","Epoch: [2][700/1425] Elapsed 2m 22s (remain 2m 27s) Loss: 0.0337(0.0184) Grad: 53255.5469  LR: 0.000016  \n","Epoch: [2][800/1425] Elapsed 2m 42s (remain 2m 6s) Loss: 0.0830(0.0184) Grad: 148651.4688  LR: 0.000015  \n","Epoch: [2][900/1425] Elapsed 3m 2s (remain 1m 46s) Loss: 0.0627(0.0184) Grad: 96629.8828  LR: 0.000015  \n","Epoch: [2][1000/1425] Elapsed 3m 23s (remain 1m 26s) Loss: 0.0002(0.0184) Grad: 875.4207  LR: 0.000015  \n","Epoch: [2][1100/1425] Elapsed 3m 43s (remain 1m 5s) Loss: 0.0177(0.0185) Grad: 22563.7344  LR: 0.000014  \n","Epoch: [2][1200/1425] Elapsed 4m 3s (remain 0m 45s) Loss: 0.0034(0.0187) Grad: 7187.4746  LR: 0.000014  \n","Epoch: [2][1300/1425] Elapsed 4m 23s (remain 0m 25s) Loss: 0.1221(0.0187) Grad: 263717.9062  LR: 0.000014  \n","Epoch: [2][1400/1425] Elapsed 4m 44s (remain 0m 4s) Loss: 0.0066(0.0188) Grad: 12418.2559  LR: 0.000013  \n","Epoch: [2][1424/1425] Elapsed 4m 48s (remain 0m 0s) Loss: 0.0105(0.0187) Grad: 16406.6113  LR: 0.000013  \n","EVAL: [0/362] Elapsed 0m 0s (remain 1m 48s) Loss: 0.0015(0.0015) \n","EVAL: [100/362] Elapsed 0m 7s (remain 0m 19s) Loss: 0.0064(0.0188) \n","EVAL: [200/362] Elapsed 0m 14s (remain 0m 11s) Loss: 0.0194(0.0177) \n","EVAL: [300/362] Elapsed 0m 21s (remain 0m 4s) Loss: 0.0048(0.0191) \n","EVAL: [361/362] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0017(0.0172) \n","Epoch 2 - avg_train_loss: 0.0187  avg_val_loss: 0.0172  time: 318s\n","Epoch 2 - Score: 0.8653\n","Epoch 2 - Save Best Score: 0.8653 Model\n","Epoch: [3][0/1425] Elapsed 0m 0s (remain 11m 24s) Loss: 0.0023(0.0023) Grad: 5463.9448  LR: 0.000013  \n","Epoch: [3][100/1425] Elapsed 0m 20s (remain 4m 31s) Loss: 0.0025(0.0151) Grad: 4792.2617  LR: 0.000013  \n","Epoch: [3][200/1425] Elapsed 0m 41s (remain 4m 10s) Loss: 0.0078(0.0139) Grad: 54988.9141  LR: 0.000013  \n","Epoch: [3][300/1425] Elapsed 1m 1s (remain 3m 48s) Loss: 0.0004(0.0155) Grad: 2951.5217  LR: 0.000012  \n","Epoch: [3][400/1425] Elapsed 1m 21s (remain 3m 28s) Loss: 0.0669(0.0163) Grad: 133959.4531  LR: 0.000012  \n","Epoch: [3][500/1425] Elapsed 1m 41s (remain 3m 7s) Loss: 0.0043(0.0164) Grad: 13739.8945  LR: 0.000012  \n","Epoch: [3][600/1425] Elapsed 2m 1s (remain 2m 47s) Loss: 0.0047(0.0160) Grad: 6577.8867  LR: 0.000011  \n","Epoch: [3][700/1425] Elapsed 2m 22s (remain 2m 26s) Loss: 0.0053(0.0158) Grad: 8897.9482  LR: 0.000011  \n","Epoch: [3][800/1425] Elapsed 2m 42s (remain 2m 6s) Loss: 0.0210(0.0157) Grad: 49873.0664  LR: 0.000011  \n","Epoch: [3][900/1425] Elapsed 3m 2s (remain 1m 46s) Loss: 0.0094(0.0154) Grad: 20668.8926  LR: 0.000011  \n","Epoch: [3][1000/1425] Elapsed 3m 22s (remain 1m 25s) Loss: 0.0015(0.0155) Grad: 7263.8306  LR: 0.000010  \n","Epoch: [3][1100/1425] Elapsed 3m 42s (remain 1m 5s) Loss: 0.0119(0.0154) Grad: 29492.3008  LR: 0.000010  \n","Epoch: [3][1200/1425] Elapsed 4m 3s (remain 0m 45s) Loss: 0.0084(0.0153) Grad: 19509.1367  LR: 0.000010  \n","Epoch: [3][1300/1425] Elapsed 4m 23s (remain 0m 25s) Loss: 0.0164(0.0151) Grad: 20364.9414  LR: 0.000009  \n","Epoch: [3][1400/1425] Elapsed 4m 43s (remain 0m 4s) Loss: 0.0109(0.0148) Grad: 99530.1562  LR: 0.000009  \n","Epoch: [3][1424/1425] Elapsed 4m 48s (remain 0m 0s) Loss: 0.0028(0.0147) Grad: 8866.5293  LR: 0.000009  \n","EVAL: [0/362] Elapsed 0m 0s (remain 1m 50s) Loss: 0.0007(0.0007) \n","EVAL: [100/362] Elapsed 0m 7s (remain 0m 18s) Loss: 0.0031(0.0220) \n","EVAL: [200/362] Elapsed 0m 14s (remain 0m 11s) Loss: 0.0079(0.0197) \n","EVAL: [300/362] Elapsed 0m 21s (remain 0m 4s) Loss: 0.0026(0.0214) \n","EVAL: [361/362] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0018(0.0192) \n","Epoch 3 - avg_train_loss: 0.0147  avg_val_loss: 0.0192  time: 318s\n","Epoch 3 - Score: 0.8697\n","Epoch 3 - Save Best Score: 0.8697 Model\n","Epoch: [4][0/1425] Elapsed 0m 0s (remain 11m 40s) Loss: 0.0042(0.0042) Grad: 8996.7139  LR: 0.000009  \n","Epoch: [4][100/1425] Elapsed 0m 20s (remain 4m 33s) Loss: 0.0037(0.0105) Grad: 9605.1582  LR: 0.000009  \n","Epoch: [4][200/1425] Elapsed 0m 41s (remain 4m 9s) Loss: 0.0121(0.0106) Grad: 51797.0000  LR: 0.000008  \n","Epoch: [4][300/1425] Elapsed 1m 1s (remain 3m 49s) Loss: 0.1090(0.0109) Grad: 177565.1562  LR: 0.000008  \n","Epoch: [4][400/1425] Elapsed 1m 21s (remain 3m 28s) Loss: 0.0448(0.0121) Grad: 105289.4453  LR: 0.000008  \n","Epoch: [4][500/1425] Elapsed 1m 42s (remain 3m 8s) Loss: 0.0089(0.0116) Grad: 23368.2598  LR: 0.000007  \n","Epoch: [4][600/1425] Elapsed 2m 2s (remain 2m 47s) Loss: 0.0009(0.0115) Grad: 7438.8516  LR: 0.000007  \n","Epoch: [4][700/1425] Elapsed 2m 22s (remain 2m 27s) Loss: 0.0232(0.0117) Grad: 52000.3789  LR: 0.000007  \n","Epoch: [4][800/1425] Elapsed 2m 42s (remain 2m 6s) Loss: 0.0006(0.0122) Grad: 2357.4485  LR: 0.000006  \n","Epoch: [4][900/1425] Elapsed 3m 2s (remain 1m 46s) Loss: 0.0038(0.0119) Grad: 18218.6992  LR: 0.000006  \n","Epoch: [4][1000/1425] Elapsed 3m 23s (remain 1m 25s) Loss: 0.0001(0.0122) Grad: 738.5035  LR: 0.000006  \n","Epoch: [4][1100/1425] Elapsed 3m 43s (remain 1m 5s) Loss: 0.0068(0.0122) Grad: 5814.6836  LR: 0.000005  \n","Epoch: [4][1200/1425] Elapsed 4m 3s (remain 0m 45s) Loss: 0.0120(0.0119) Grad: 6999.1040  LR: 0.000005  \n","Epoch: [4][1300/1425] Elapsed 4m 23s (remain 0m 25s) Loss: 0.0032(0.0120) Grad: 15012.1152  LR: 0.000005  \n","Epoch: [4][1400/1425] Elapsed 4m 43s (remain 0m 4s) Loss: 0.0021(0.0119) Grad: 18489.5957  LR: 0.000005  \n","Epoch: [4][1424/1425] Elapsed 4m 48s (remain 0m 0s) Loss: 0.0043(0.0118) Grad: 15291.8867  LR: 0.000004  \n","EVAL: [0/362] Elapsed 0m 0s (remain 1m 54s) Loss: 0.0012(0.0012) \n","EVAL: [100/362] Elapsed 0m 7s (remain 0m 18s) Loss: 0.0028(0.0228) \n","EVAL: [200/362] Elapsed 0m 14s (remain 0m 11s) Loss: 0.0023(0.0206) \n","EVAL: [300/362] Elapsed 0m 21s (remain 0m 4s) Loss: 0.0012(0.0227) \n","EVAL: [361/362] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0009(0.0205) \n","Epoch 4 - avg_train_loss: 0.0118  avg_val_loss: 0.0205  time: 318s\n","Epoch 4 - Score: 0.8746\n","Epoch 4 - Save Best Score: 0.8746 Model\n","Epoch: [5][0/1425] Elapsed 0m 0s (remain 11m 14s) Loss: 0.0119(0.0119) Grad: 37835.6797  LR: 0.000004  \n","Epoch: [5][100/1425] Elapsed 0m 20s (remain 4m 30s) Loss: 0.0047(0.0106) Grad: 16603.2031  LR: 0.000004  \n","Epoch: [5][200/1425] Elapsed 0m 40s (remain 4m 8s) Loss: 0.0067(0.0122) Grad: 14234.4072  LR: 0.000004  \n","Epoch: [5][300/1425] Elapsed 1m 1s (remain 3m 48s) Loss: 0.0025(0.0111) Grad: 7765.5483  LR: 0.000004  \n","Epoch: [5][400/1425] Elapsed 1m 21s (remain 3m 27s) Loss: 0.0121(0.0106) Grad: 48690.0234  LR: 0.000003  \n","Epoch: [5][500/1425] Elapsed 1m 41s (remain 3m 7s) Loss: 0.0360(0.0109) Grad: 74566.6875  LR: 0.000003  \n","Epoch: [5][600/1425] Elapsed 2m 2s (remain 2m 47s) Loss: 0.0094(0.0108) Grad: 48915.3359  LR: 0.000003  \n","Epoch: [5][700/1425] Elapsed 2m 22s (remain 2m 26s) Loss: 0.0005(0.0108) Grad: 1893.1614  LR: 0.000002  \n","Epoch: [5][800/1425] Elapsed 2m 42s (remain 2m 6s) Loss: 0.0027(0.0107) Grad: 9539.2539  LR: 0.000002  \n","Epoch: [5][900/1425] Elapsed 3m 2s (remain 1m 46s) Loss: 0.0030(0.0103) Grad: 20591.0566  LR: 0.000002  \n","Epoch: [5][1000/1425] Elapsed 3m 22s (remain 1m 25s) Loss: 0.0046(0.0102) Grad: 23363.5723  LR: 0.000001  \n","Epoch: [5][1100/1425] Elapsed 3m 43s (remain 1m 5s) Loss: 0.0097(0.0103) Grad: 27050.1738  LR: 0.000001  \n","Epoch: [5][1200/1425] Elapsed 4m 3s (remain 0m 45s) Loss: 0.0021(0.0103) Grad: 8640.8486  LR: 0.000001  \n","Epoch: [5][1300/1425] Elapsed 4m 23s (remain 0m 25s) Loss: 0.0384(0.0104) Grad: 78397.7578  LR: 0.000000  \n","Epoch: [5][1400/1425] Elapsed 4m 43s (remain 0m 4s) Loss: 0.0026(0.0103) Grad: 6263.2993  LR: 0.000000  \n","Epoch: [5][1424/1425] Elapsed 4m 48s (remain 0m 0s) Loss: 0.0051(0.0103) Grad: 11992.1865  LR: 0.000000  \n","EVAL: [0/362] Elapsed 0m 0s (remain 1m 48s) Loss: 0.0013(0.0013) \n","EVAL: [100/362] Elapsed 0m 7s (remain 0m 18s) Loss: 0.0021(0.0231) \n","EVAL: [200/362] Elapsed 0m 14s (remain 0m 11s) Loss: 0.0020(0.0210) \n","EVAL: [300/362] Elapsed 0m 21s (remain 0m 4s) Loss: 0.0009(0.0233) \n","EVAL: [361/362] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0009(0.0210) \n","Epoch 5 - avg_train_loss: 0.0103  avg_val_loss: 0.0210  time: 318s\n","Epoch 5 - Score: 0.8772\n","Epoch 5 - Save Best Score: 0.8772 Model\n","========== fold: 2 training ==========\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp009/checkpoint-129000/pytorch_model.bin\n","Epoch: [1][0/1435] Elapsed 0m 0s (remain 10m 59s) Loss: 0.6190(0.6190) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/1435] Elapsed 0m 20s (remain 4m 33s) Loss: 0.1155(0.6501) Grad: 8777.9795  LR: 0.000003  \n","Epoch: [1][200/1435] Elapsed 0m 40s (remain 4m 10s) Loss: 0.1141(0.3812) Grad: 1646.8087  LR: 0.000006  \n","Epoch: [1][300/1435] Elapsed 1m 0s (remain 3m 49s) Loss: 0.0443(0.2891) Grad: 752.7922  LR: 0.000008  \n","Epoch: [1][400/1435] Elapsed 1m 21s (remain 3m 29s) Loss: 0.0398(0.2369) Grad: 2076.5127  LR: 0.000011  \n","Epoch: [1][500/1435] Elapsed 1m 41s (remain 3m 9s) Loss: 0.0564(0.2008) Grad: 5748.1821  LR: 0.000014  \n","Epoch: [1][600/1435] Elapsed 2m 1s (remain 2m 49s) Loss: 0.0384(0.1745) Grad: 3333.8274  LR: 0.000017  \n","Epoch: [1][700/1435] Elapsed 2m 22s (remain 2m 28s) Loss: 0.0384(0.1547) Grad: 3691.9514  LR: 0.000020  \n","Epoch: [1][800/1435] Elapsed 2m 42s (remain 2m 8s) Loss: 0.0083(0.1388) Grad: 526.6398  LR: 0.000020  \n","Epoch: [1][900/1435] Elapsed 3m 2s (remain 1m 48s) Loss: 0.0105(0.1263) Grad: 1218.0763  LR: 0.000019  \n","Epoch: [1][1000/1435] Elapsed 3m 22s (remain 1m 27s) Loss: 0.0085(0.1165) Grad: 9924.3428  LR: 0.000019  \n","Epoch: [1][1100/1435] Elapsed 3m 42s (remain 1m 7s) Loss: 0.0265(0.1083) Grad: 2227.4338  LR: 0.000019  \n","Epoch: [1][1200/1435] Elapsed 4m 3s (remain 0m 47s) Loss: 0.0310(0.1015) Grad: 2018.1541  LR: 0.000019  \n","Epoch: [1][1300/1435] Elapsed 4m 23s (remain 0m 27s) Loss: 0.0522(0.0955) Grad: 4682.9702  LR: 0.000018  \n","Epoch: [1][1400/1435] Elapsed 4m 43s (remain 0m 6s) Loss: 0.0044(0.0903) Grad: 475.0706  LR: 0.000018  \n","Epoch: [1][1434/1435] Elapsed 4m 50s (remain 0m 0s) Loss: 0.0694(0.0888) Grad: 3498.5239  LR: 0.000018  \n","EVAL: [0/352] Elapsed 0m 0s (remain 3m 5s) Loss: 0.0126(0.0126) \n","EVAL: [100/352] Elapsed 0m 7s (remain 0m 18s) Loss: 0.0753(0.0190) \n","EVAL: [200/352] Elapsed 0m 14s (remain 0m 10s) Loss: 0.0325(0.0218) \n","EVAL: [300/352] Elapsed 0m 21s (remain 0m 3s) Loss: 0.0099(0.0267) \n","EVAL: [351/352] Elapsed 0m 24s (remain 0m 0s) Loss: 0.0033(0.0247) \n","Epoch 1 - avg_train_loss: 0.0888  avg_val_loss: 0.0247  time: 321s\n","Epoch 1 - Score: 0.8147\n","Epoch 1 - Save Best Score: 0.8147 Model\n","Epoch: [2][0/1435] Elapsed 0m 0s (remain 11m 12s) Loss: 0.0068(0.0068) Grad: 15330.6406  LR: 0.000018  \n","Epoch: [2][100/1435] Elapsed 0m 21s (remain 4m 37s) Loss: 0.0765(0.0204) Grad: 86996.5469  LR: 0.000017  \n","Epoch: [2][200/1435] Elapsed 0m 41s (remain 4m 12s) Loss: 0.0042(0.0183) Grad: 14769.4102  LR: 0.000017  \n","Epoch: [2][300/1435] Elapsed 1m 1s (remain 3m 51s) Loss: 0.0009(0.0177) Grad: 6041.6006  LR: 0.000017  \n","Epoch: [2][400/1435] Elapsed 1m 21s (remain 3m 30s) Loss: 0.0064(0.0183) Grad: 10317.7480  LR: 0.000017  \n","Epoch: [2][500/1435] Elapsed 1m 41s (remain 3m 9s) Loss: 0.0101(0.0175) Grad: 38610.4922  LR: 0.000016  \n","Epoch: [2][600/1435] Elapsed 2m 2s (remain 2m 49s) Loss: 0.0033(0.0169) Grad: 17687.5566  LR: 0.000016  \n","Epoch: [2][700/1435] Elapsed 2m 22s (remain 2m 29s) Loss: 0.0104(0.0173) Grad: 10883.4736  LR: 0.000016  \n","Epoch: [2][800/1435] Elapsed 2m 42s (remain 2m 8s) Loss: 0.0530(0.0174) Grad: 73096.3906  LR: 0.000015  \n","Epoch: [2][900/1435] Elapsed 3m 2s (remain 1m 48s) Loss: 0.0006(0.0170) Grad: 2579.4163  LR: 0.000015  \n","Epoch: [2][1000/1435] Elapsed 3m 23s (remain 1m 28s) Loss: 0.0064(0.0176) Grad: 11762.1455  LR: 0.000015  \n","Epoch: [2][1100/1435] Elapsed 3m 43s (remain 1m 7s) Loss: 0.0319(0.0176) Grad: 28657.4121  LR: 0.000014  \n","Epoch: [2][1200/1435] Elapsed 4m 3s (remain 0m 47s) Loss: 0.0070(0.0179) Grad: 13722.8838  LR: 0.000014  \n","Epoch: [2][1300/1435] Elapsed 4m 23s (remain 0m 27s) Loss: 0.0019(0.0178) Grad: 5082.2534  LR: 0.000014  \n","Epoch: [2][1400/1435] Elapsed 4m 43s (remain 0m 6s) Loss: 0.0212(0.0182) Grad: 46595.8477  LR: 0.000013  \n","Epoch: [2][1434/1435] Elapsed 4m 50s (remain 0m 0s) Loss: 0.0214(0.0181) Grad: 31576.4766  LR: 0.000013  \n","EVAL: [0/352] Elapsed 0m 0s (remain 1m 48s) Loss: 0.0103(0.0103) \n","EVAL: [100/352] Elapsed 0m 7s (remain 0m 18s) Loss: 0.0677(0.0166) \n","EVAL: [200/352] Elapsed 0m 14s (remain 0m 10s) Loss: 0.0468(0.0187) \n","EVAL: [300/352] Elapsed 0m 21s (remain 0m 3s) Loss: 0.0023(0.0241) \n","EVAL: [351/352] Elapsed 0m 24s (remain 0m 0s) Loss: 0.0010(0.0223) \n","Epoch 2 - avg_train_loss: 0.0181  avg_val_loss: 0.0223  time: 321s\n","Epoch 2 - Score: 0.8553\n","Epoch 2 - Save Best Score: 0.8553 Model\n","Epoch: [3][0/1435] Elapsed 0m 0s (remain 11m 8s) Loss: 0.0145(0.0145) Grad: 44310.7500  LR: 0.000013  \n","Epoch: [3][100/1435] Elapsed 0m 20s (remain 4m 32s) Loss: 0.0186(0.0100) Grad: 48399.0938  LR: 0.000013  \n","Epoch: [3][200/1435] Elapsed 0m 41s (remain 4m 11s) Loss: 0.0211(0.0110) Grad: 36061.4336  LR: 0.000013  \n","Epoch: [3][300/1435] Elapsed 1m 1s (remain 3m 50s) Loss: 0.0171(0.0118) Grad: 38181.3398  LR: 0.000012  \n","Epoch: [3][400/1435] Elapsed 1m 21s (remain 3m 29s) Loss: 0.0212(0.0133) Grad: 129284.1250  LR: 0.000012  \n","Epoch: [3][500/1435] Elapsed 1m 41s (remain 3m 9s) Loss: 0.0026(0.0133) Grad: 3952.9998  LR: 0.000012  \n","Epoch: [3][600/1435] Elapsed 2m 1s (remain 2m 48s) Loss: 0.0014(0.0144) Grad: 4325.4771  LR: 0.000011  \n","Epoch: [3][700/1435] Elapsed 2m 21s (remain 2m 28s) Loss: 0.0048(0.0147) Grad: 8641.6855  LR: 0.000011  \n","Epoch: [3][800/1435] Elapsed 2m 41s (remain 2m 8s) Loss: 0.0027(0.0144) Grad: 22065.1211  LR: 0.000011  \n","Epoch: [3][900/1435] Elapsed 3m 2s (remain 1m 47s) Loss: 0.0101(0.0143) Grad: 13276.2432  LR: 0.000011  \n","Epoch: [3][1000/1435] Elapsed 3m 22s (remain 1m 27s) Loss: 0.0193(0.0141) Grad: 31053.9160  LR: 0.000010  \n","Epoch: [3][1100/1435] Elapsed 3m 42s (remain 1m 7s) Loss: 0.0129(0.0140) Grad: 38021.4883  LR: 0.000010  \n","Epoch: [3][1200/1435] Elapsed 4m 2s (remain 0m 47s) Loss: 0.0166(0.0141) Grad: 18870.5977  LR: 0.000010  \n","Epoch: [3][1300/1435] Elapsed 4m 22s (remain 0m 27s) Loss: 0.0003(0.0142) Grad: 1058.5605  LR: 0.000009  \n","Epoch: [3][1400/1435] Elapsed 4m 42s (remain 0m 6s) Loss: 0.0107(0.0145) Grad: 15444.3584  LR: 0.000009  \n","Epoch: [3][1434/1435] Elapsed 4m 49s (remain 0m 0s) Loss: 0.0141(0.0145) Grad: 32816.0000  LR: 0.000009  \n","EVAL: [0/352] Elapsed 0m 0s (remain 1m 52s) Loss: 0.0022(0.0022) \n","EVAL: [100/352] Elapsed 0m 7s (remain 0m 17s) Loss: 0.0417(0.0158) \n","EVAL: [200/352] Elapsed 0m 14s (remain 0m 10s) Loss: 0.0505(0.0185) \n","EVAL: [300/352] Elapsed 0m 21s (remain 0m 3s) Loss: 0.0036(0.0233) \n","EVAL: [351/352] Elapsed 0m 24s (remain 0m 0s) Loss: 0.0008(0.0215) \n","Epoch 3 - avg_train_loss: 0.0145  avg_val_loss: 0.0215  time: 320s\n","Epoch 3 - Score: 0.8624\n","Epoch 3 - Save Best Score: 0.8624 Model\n","Epoch: [4][0/1435] Elapsed 0m 0s (remain 11m 6s) Loss: 0.0027(0.0027) Grad: 3626.7461  LR: 0.000009  \n","Epoch: [4][100/1435] Elapsed 0m 20s (remain 4m 33s) Loss: 0.0059(0.0071) Grad: 26362.3320  LR: 0.000009  \n","Epoch: [4][200/1435] Elapsed 0m 40s (remain 4m 10s) Loss: 0.0338(0.0091) Grad: 55159.3711  LR: 0.000008  \n","Epoch: [4][300/1435] Elapsed 1m 0s (remain 3m 49s) Loss: 0.0013(0.0104) Grad: 5229.9224  LR: 0.000008  \n","Epoch: [4][400/1435] Elapsed 1m 21s (remain 3m 29s) Loss: 0.0445(0.0114) Grad: 109216.1094  LR: 0.000008  \n","Epoch: [4][500/1435] Elapsed 1m 41s (remain 3m 9s) Loss: 0.0040(0.0108) Grad: 9099.9238  LR: 0.000007  \n","Epoch: [4][600/1435] Elapsed 2m 1s (remain 2m 48s) Loss: 0.0011(0.0110) Grad: 8182.6582  LR: 0.000007  \n","Epoch: [4][700/1435] Elapsed 2m 21s (remain 2m 28s) Loss: 0.0016(0.0107) Grad: 12355.8438  LR: 0.000007  \n","Epoch: [4][800/1435] Elapsed 2m 41s (remain 2m 8s) Loss: 0.0041(0.0111) Grad: 12741.6650  LR: 0.000006  \n","Epoch: [4][900/1435] Elapsed 3m 2s (remain 1m 47s) Loss: 0.0001(0.0111) Grad: 377.9981  LR: 0.000006  \n","Epoch: [4][1000/1435] Elapsed 3m 22s (remain 1m 27s) Loss: 0.0003(0.0112) Grad: 612.3845  LR: 0.000006  \n","Epoch: [4][1100/1435] Elapsed 3m 42s (remain 1m 7s) Loss: 0.0017(0.0117) Grad: 8209.5107  LR: 0.000005  \n","Epoch: [4][1200/1435] Elapsed 4m 2s (remain 0m 47s) Loss: 0.0146(0.0118) Grad: 37973.4570  LR: 0.000005  \n","Epoch: [4][1300/1435] Elapsed 4m 22s (remain 0m 27s) Loss: 0.0034(0.0118) Grad: 6989.1606  LR: 0.000005  \n","Epoch: [4][1400/1435] Elapsed 4m 43s (remain 0m 6s) Loss: 0.0029(0.0118) Grad: 6817.5464  LR: 0.000005  \n","Epoch: [4][1434/1435] Elapsed 4m 50s (remain 0m 0s) Loss: 0.0001(0.0117) Grad: 649.7552  LR: 0.000004  \n","EVAL: [0/352] Elapsed 0m 0s (remain 1m 51s) Loss: 0.0055(0.0055) \n","EVAL: [100/352] Elapsed 0m 7s (remain 0m 17s) Loss: 0.0413(0.0178) \n","EVAL: [200/352] Elapsed 0m 14s (remain 0m 10s) Loss: 0.0571(0.0212) \n","EVAL: [300/352] Elapsed 0m 20s (remain 0m 3s) Loss: 0.0015(0.0281) \n","EVAL: [351/352] Elapsed 0m 24s (remain 0m 0s) Loss: 0.0007(0.0258) \n","Epoch 4 - avg_train_loss: 0.0117  avg_val_loss: 0.0258  time: 320s\n","Epoch 4 - Score: 0.8638\n","Epoch 4 - Save Best Score: 0.8638 Model\n","Epoch: [5][0/1435] Elapsed 0m 0s (remain 16m 18s) Loss: 0.0525(0.0525) Grad: 77373.2734  LR: 0.000004  \n","Epoch: [5][100/1435] Elapsed 0m 20s (remain 4m 35s) Loss: 0.0117(0.0079) Grad: 21759.4141  LR: 0.000004  \n","Epoch: [5][200/1435] Elapsed 0m 41s (remain 4m 11s) Loss: 0.0009(0.0073) Grad: 5221.8247  LR: 0.000004  \n","Epoch: [5][300/1435] Elapsed 1m 1s (remain 3m 50s) Loss: 0.0096(0.0079) Grad: 6986.0869  LR: 0.000004  \n","Epoch: [5][400/1435] Elapsed 1m 21s (remain 3m 29s) Loss: 0.0054(0.0089) Grad: 22177.1543  LR: 0.000003  \n","Epoch: [5][500/1435] Elapsed 1m 41s (remain 3m 9s) Loss: 0.0078(0.0090) Grad: 28086.5195  LR: 0.000003  \n","Epoch: [5][600/1435] Elapsed 2m 1s (remain 2m 48s) Loss: 0.0001(0.0093) Grad: 491.2927  LR: 0.000003  \n","Epoch: [5][700/1435] Elapsed 2m 21s (remain 2m 28s) Loss: 0.0006(0.0097) Grad: 4008.2891  LR: 0.000002  \n","Epoch: [5][800/1435] Elapsed 2m 42s (remain 2m 8s) Loss: 0.0132(0.0101) Grad: 53059.5977  LR: 0.000002  \n","Epoch: [5][900/1435] Elapsed 3m 2s (remain 1m 47s) Loss: 0.0098(0.0097) Grad: 27721.7363  LR: 0.000002  \n","Epoch: [5][1000/1435] Elapsed 3m 22s (remain 1m 27s) Loss: 0.0028(0.0097) Grad: 19647.6797  LR: 0.000001  \n","Epoch: [5][1100/1435] Elapsed 3m 42s (remain 1m 7s) Loss: 0.1162(0.0099) Grad: 179164.0469  LR: 0.000001  \n","Epoch: [5][1200/1435] Elapsed 4m 2s (remain 0m 47s) Loss: 0.0354(0.0099) Grad: 16305.7236  LR: 0.000001  \n","Epoch: [5][1300/1435] Elapsed 4m 22s (remain 0m 27s) Loss: 0.0052(0.0100) Grad: 17860.2559  LR: 0.000000  \n","Epoch: [5][1400/1435] Elapsed 4m 43s (remain 0m 6s) Loss: 0.0063(0.0100) Grad: 24219.3184  LR: 0.000000  \n","Epoch: [5][1434/1435] Elapsed 4m 50s (remain 0m 0s) Loss: 0.0013(0.0101) Grad: 2584.1431  LR: 0.000000  \n","EVAL: [0/352] Elapsed 0m 0s (remain 1m 52s) Loss: 0.0035(0.0035) \n","EVAL: [100/352] Elapsed 0m 7s (remain 0m 17s) Loss: 0.0613(0.0193) \n","EVAL: [200/352] Elapsed 0m 13s (remain 0m 10s) Loss: 0.0700(0.0224) \n","EVAL: [300/352] Elapsed 0m 20s (remain 0m 3s) Loss: 0.0015(0.0288) \n","EVAL: [351/352] Elapsed 0m 24s (remain 0m 0s) Loss: 0.0008(0.0263) \n","Epoch 5 - avg_train_loss: 0.0101  avg_val_loss: 0.0263  time: 320s\n","Epoch 5 - Score: 0.8639\n","Epoch 5 - Save Best Score: 0.8639 Model\n","========== fold: 3 training ==========\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp009/checkpoint-129000/pytorch_model.bin\n","Epoch: [1][0/1438] Elapsed 0m 0s (remain 10m 42s) Loss: 0.9619(0.9619) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/1438] Elapsed 0m 20s (remain 4m 33s) Loss: 0.1300(0.4713) Grad: 10137.1016  LR: 0.000003  \n","Epoch: [1][200/1438] Elapsed 0m 40s (remain 4m 11s) Loss: 0.1913(0.2898) Grad: 4060.0325  LR: 0.000006  \n","Epoch: [1][300/1438] Elapsed 1m 1s (remain 3m 50s) Loss: 0.0577(0.2266) Grad: 992.8676  LR: 0.000008  \n","Epoch: [1][400/1438] Elapsed 1m 21s (remain 3m 30s) Loss: 0.0103(0.1907) Grad: 838.2972  LR: 0.000011  \n","Epoch: [1][500/1438] Elapsed 1m 41s (remain 3m 10s) Loss: 0.0268(0.1634) Grad: 3955.9529  LR: 0.000014  \n","Epoch: [1][600/1438] Elapsed 2m 1s (remain 2m 49s) Loss: 0.0143(0.1429) Grad: 2220.2629  LR: 0.000017  \n","Epoch: [1][700/1438] Elapsed 2m 21s (remain 2m 29s) Loss: 0.0583(0.1282) Grad: 4751.8779  LR: 0.000019  \n","Epoch: [1][800/1438] Elapsed 2m 42s (remain 2m 8s) Loss: 0.0125(0.1163) Grad: 1932.5938  LR: 0.000020  \n","Epoch: [1][900/1438] Elapsed 3m 2s (remain 1m 48s) Loss: 0.0144(0.1069) Grad: 3545.4744  LR: 0.000019  \n","Epoch: [1][1000/1438] Elapsed 3m 22s (remain 1m 28s) Loss: 0.0279(0.0991) Grad: 3267.0710  LR: 0.000019  \n","Epoch: [1][1100/1438] Elapsed 3m 42s (remain 1m 8s) Loss: 0.0178(0.0922) Grad: 3142.6453  LR: 0.000019  \n","Epoch: [1][1200/1438] Elapsed 4m 2s (remain 0m 47s) Loss: 0.0176(0.0865) Grad: 1141.2469  LR: 0.000019  \n","Epoch: [1][1300/1438] Elapsed 4m 23s (remain 0m 27s) Loss: 0.0704(0.0817) Grad: 3121.1746  LR: 0.000018  \n","Epoch: [1][1400/1438] Elapsed 4m 43s (remain 0m 7s) Loss: 0.0147(0.0775) Grad: 1177.0579  LR: 0.000018  \n","Epoch: [1][1437/1438] Elapsed 4m 50s (remain 0m 0s) Loss: 0.0117(0.0760) Grad: 1177.8524  LR: 0.000018  \n","EVAL: [0/349] Elapsed 0m 0s (remain 1m 43s) Loss: 0.0052(0.0052) \n","EVAL: [100/349] Elapsed 0m 7s (remain 0m 17s) Loss: 0.0412(0.0185) \n","EVAL: [200/349] Elapsed 0m 14s (remain 0m 10s) Loss: 0.0135(0.0213) \n","EVAL: [300/349] Elapsed 0m 20s (remain 0m 3s) Loss: 0.0119(0.0239) \n","EVAL: [348/349] Elapsed 0m 24s (remain 0m 0s) Loss: 0.0020(0.0220) \n","Epoch 1 - avg_train_loss: 0.0760  avg_val_loss: 0.0220  time: 319s\n","Epoch 1 - Score: 0.8386\n","Epoch 1 - Save Best Score: 0.8386 Model\n","Epoch: [2][0/1438] Elapsed 0m 0s (remain 11m 17s) Loss: 0.0099(0.0099) Grad: 15319.6143  LR: 0.000018  \n","Epoch: [2][100/1438] Elapsed 0m 20s (remain 4m 34s) Loss: 0.0027(0.0197) Grad: 5410.7998  LR: 0.000017  \n","Epoch: [2][200/1438] Elapsed 0m 41s (remain 4m 12s) Loss: 0.0158(0.0207) Grad: 32657.4453  LR: 0.000017  \n","Epoch: [2][300/1438] Elapsed 1m 1s (remain 3m 51s) Loss: 0.0041(0.0206) Grad: 11394.5693  LR: 0.000017  \n","Epoch: [2][400/1438] Elapsed 1m 21s (remain 3m 30s) Loss: 0.0034(0.0203) Grad: 12493.8477  LR: 0.000017  \n","Epoch: [2][500/1438] Elapsed 1m 41s (remain 3m 9s) Loss: 0.0001(0.0195) Grad: 731.7792  LR: 0.000016  \n","Epoch: [2][600/1438] Elapsed 2m 1s (remain 2m 49s) Loss: 0.0088(0.0189) Grad: 18058.8008  LR: 0.000016  \n","Epoch: [2][700/1438] Elapsed 2m 22s (remain 2m 29s) Loss: 0.0064(0.0192) Grad: 9825.6250  LR: 0.000016  \n","Epoch: [2][800/1438] Elapsed 2m 42s (remain 2m 8s) Loss: 0.0261(0.0197) Grad: 25257.0098  LR: 0.000015  \n","Epoch: [2][900/1438] Elapsed 3m 2s (remain 1m 48s) Loss: 0.0357(0.0192) Grad: 57019.8984  LR: 0.000015  \n","Epoch: [2][1000/1438] Elapsed 3m 22s (remain 1m 28s) Loss: 0.0145(0.0193) Grad: 48891.3281  LR: 0.000015  \n","Epoch: [2][1100/1438] Elapsed 3m 42s (remain 1m 8s) Loss: 0.0057(0.0191) Grad: 6394.1289  LR: 0.000014  \n","Epoch: [2][1200/1438] Elapsed 4m 3s (remain 0m 47s) Loss: 0.0279(0.0190) Grad: 61834.9062  LR: 0.000014  \n","Epoch: [2][1300/1438] Elapsed 4m 23s (remain 0m 27s) Loss: 0.0105(0.0187) Grad: 9461.8008  LR: 0.000014  \n","Epoch: [2][1400/1438] Elapsed 4m 43s (remain 0m 7s) Loss: 0.0049(0.0186) Grad: 8907.9277  LR: 0.000013  \n","Epoch: [2][1437/1438] Elapsed 4m 50s (remain 0m 0s) Loss: 0.0025(0.0188) Grad: 8783.2354  LR: 0.000013  \n","EVAL: [0/349] Elapsed 0m 0s (remain 1m 47s) Loss: 0.0084(0.0084) \n","EVAL: [100/349] Elapsed 0m 7s (remain 0m 17s) Loss: 0.0518(0.0188) \n","EVAL: [200/349] Elapsed 0m 14s (remain 0m 10s) Loss: 0.0216(0.0206) \n","EVAL: [300/349] Elapsed 0m 21s (remain 0m 3s) Loss: 0.0070(0.0206) \n","EVAL: [348/349] Elapsed 0m 24s (remain 0m 0s) Loss: 0.0003(0.0192) \n","Epoch 2 - avg_train_loss: 0.0188  avg_val_loss: 0.0192  time: 319s\n","Epoch 2 - Score: 0.8694\n","Epoch 2 - Save Best Score: 0.8694 Model\n","Epoch: [3][0/1438] Elapsed 0m 0s (remain 11m 28s) Loss: 0.0339(0.0339) Grad: 38156.7539  LR: 0.000013  \n","Epoch: [3][100/1438] Elapsed 0m 20s (remain 4m 33s) Loss: 0.0023(0.0173) Grad: 3912.5281  LR: 0.000013  \n","Epoch: [3][200/1438] Elapsed 0m 40s (remain 4m 11s) Loss: 0.0025(0.0145) Grad: 10371.7764  LR: 0.000013  \n","Epoch: [3][300/1438] Elapsed 1m 1s (remain 3m 51s) Loss: 0.0018(0.0142) Grad: 15093.8018  LR: 0.000012  \n","Epoch: [3][400/1438] Elapsed 1m 21s (remain 3m 30s) Loss: 0.0061(0.0156) Grad: 10626.3916  LR: 0.000012  \n","Epoch: [3][500/1438] Elapsed 1m 41s (remain 3m 9s) Loss: 0.0147(0.0159) Grad: 39259.5312  LR: 0.000012  \n","Epoch: [3][600/1438] Elapsed 2m 1s (remain 2m 49s) Loss: 0.0505(0.0160) Grad: 95028.3516  LR: 0.000011  \n","Epoch: [3][700/1438] Elapsed 2m 21s (remain 2m 29s) Loss: 0.0117(0.0158) Grad: 13488.9561  LR: 0.000011  \n","Epoch: [3][800/1438] Elapsed 2m 42s (remain 2m 9s) Loss: 0.0006(0.0162) Grad: 2630.6431  LR: 0.000011  \n","Epoch: [3][900/1438] Elapsed 3m 2s (remain 1m 48s) Loss: 0.0115(0.0163) Grad: 38691.9453  LR: 0.000011  \n","Epoch: [3][1000/1438] Elapsed 3m 22s (remain 1m 28s) Loss: 0.0067(0.0158) Grad: 13249.0391  LR: 0.000010  \n","Epoch: [3][1100/1438] Elapsed 3m 42s (remain 1m 8s) Loss: 0.0009(0.0158) Grad: 6517.2061  LR: 0.000010  \n","Epoch: [3][1200/1438] Elapsed 4m 2s (remain 0m 47s) Loss: 0.0220(0.0157) Grad: 33624.1602  LR: 0.000010  \n","Epoch: [3][1300/1438] Elapsed 4m 23s (remain 0m 27s) Loss: 0.0165(0.0152) Grad: 35786.0508  LR: 0.000009  \n","Epoch: [3][1400/1438] Elapsed 4m 43s (remain 0m 7s) Loss: 0.0009(0.0151) Grad: 3654.5708  LR: 0.000009  \n","Epoch: [3][1437/1438] Elapsed 4m 51s (remain 0m 0s) Loss: 0.0771(0.0152) Grad: 57126.8828  LR: 0.000009  \n","EVAL: [0/349] Elapsed 0m 0s (remain 1m 45s) Loss: 0.0072(0.0072) \n","EVAL: [100/349] Elapsed 0m 7s (remain 0m 17s) Loss: 0.0322(0.0202) \n","EVAL: [200/349] Elapsed 0m 14s (remain 0m 10s) Loss: 0.0509(0.0217) \n","EVAL: [300/349] Elapsed 0m 21s (remain 0m 3s) Loss: 0.0064(0.0220) \n","EVAL: [348/349] Elapsed 0m 24s (remain 0m 0s) Loss: 0.0003(0.0204) \n","Epoch 3 - avg_train_loss: 0.0152  avg_val_loss: 0.0204  time: 320s\n","Epoch 3 - Score: 0.8697\n","Epoch 3 - Save Best Score: 0.8697 Model\n","Epoch: [4][0/1438] Elapsed 0m 0s (remain 11m 24s) Loss: 0.0021(0.0021) Grad: 6854.3120  LR: 0.000009  \n","Epoch: [4][100/1438] Elapsed 0m 20s (remain 4m 36s) Loss: 0.0096(0.0104) Grad: 27123.6035  LR: 0.000009  \n","Epoch: [4][200/1438] Elapsed 0m 41s (remain 4m 12s) Loss: 0.0024(0.0125) Grad: 12261.9736  LR: 0.000008  \n","Epoch: [4][300/1438] Elapsed 1m 1s (remain 3m 51s) Loss: 0.0015(0.0120) Grad: 4391.2319  LR: 0.000008  \n","Epoch: [4][400/1438] Elapsed 1m 21s (remain 3m 30s) Loss: 0.0028(0.0118) Grad: 21445.0781  LR: 0.000008  \n","Epoch: [4][500/1438] Elapsed 1m 41s (remain 3m 10s) Loss: 0.0059(0.0116) Grad: 12400.4688  LR: 0.000007  \n","Epoch: [4][600/1438] Elapsed 2m 1s (remain 2m 49s) Loss: 0.0149(0.0116) Grad: 53152.4961  LR: 0.000007  \n","Epoch: [4][700/1438] Elapsed 2m 22s (remain 2m 29s) Loss: 0.0010(0.0116) Grad: 3912.7068  LR: 0.000007  \n","Epoch: [4][800/1438] Elapsed 2m 42s (remain 2m 9s) Loss: 0.0010(0.0116) Grad: 10635.7305  LR: 0.000006  \n","Epoch: [4][900/1438] Elapsed 3m 2s (remain 1m 48s) Loss: 0.0028(0.0117) Grad: 7003.3438  LR: 0.000006  \n","Epoch: [4][1000/1438] Elapsed 3m 22s (remain 1m 28s) Loss: 0.0018(0.0118) Grad: 3206.0103  LR: 0.000006  \n","Epoch: [4][1100/1438] Elapsed 3m 43s (remain 1m 8s) Loss: 0.0165(0.0120) Grad: 28999.7383  LR: 0.000005  \n","Epoch: [4][1200/1438] Elapsed 4m 3s (remain 0m 48s) Loss: 0.0023(0.0121) Grad: 6416.4048  LR: 0.000005  \n","Epoch: [4][1300/1438] Elapsed 4m 23s (remain 0m 27s) Loss: 0.0102(0.0121) Grad: 38172.0820  LR: 0.000005  \n","Epoch: [4][1400/1438] Elapsed 4m 43s (remain 0m 7s) Loss: 0.0018(0.0123) Grad: 14080.9170  LR: 0.000005  \n","Epoch: [4][1437/1438] Elapsed 4m 51s (remain 0m 0s) Loss: 0.0168(0.0124) Grad: 17896.3066  LR: 0.000004  \n","EVAL: [0/349] Elapsed 0m 0s (remain 1m 46s) Loss: 0.0086(0.0086) \n","EVAL: [100/349] Elapsed 0m 7s (remain 0m 18s) Loss: 0.0276(0.0212) \n","EVAL: [200/349] Elapsed 0m 14s (remain 0m 10s) Loss: 0.0592(0.0226) \n","EVAL: [300/349] Elapsed 0m 21s (remain 0m 3s) Loss: 0.0059(0.0224) \n","EVAL: [348/349] Elapsed 0m 24s (remain 0m 0s) Loss: 0.0001(0.0207) \n","Epoch 4 - avg_train_loss: 0.0124  avg_val_loss: 0.0207  time: 320s\n","Epoch 4 - Score: 0.8775\n","Epoch 4 - Save Best Score: 0.8775 Model\n","Epoch: [5][0/1438] Elapsed 0m 0s (remain 11m 51s) Loss: 0.0011(0.0011) Grad: 7230.6519  LR: 0.000004  \n","Epoch: [5][100/1438] Elapsed 0m 20s (remain 4m 34s) Loss: 0.0044(0.0125) Grad: 16490.6641  LR: 0.000004  \n","Epoch: [5][200/1438] Elapsed 0m 41s (remain 4m 12s) Loss: 0.0034(0.0123) Grad: 18920.9629  LR: 0.000004  \n","Epoch: [5][300/1438] Elapsed 1m 1s (remain 3m 52s) Loss: 0.0030(0.0111) Grad: 13154.9326  LR: 0.000004  \n","Epoch: [5][400/1438] Elapsed 1m 21s (remain 3m 31s) Loss: 0.0311(0.0108) Grad: 59083.2148  LR: 0.000003  \n","Epoch: [5][500/1438] Elapsed 1m 42s (remain 3m 10s) Loss: 0.0002(0.0103) Grad: 1283.6790  LR: 0.000003  \n","Epoch: [5][600/1438] Elapsed 2m 2s (remain 2m 50s) Loss: 0.0197(0.0103) Grad: 22577.5742  LR: 0.000003  \n","Epoch: [5][700/1438] Elapsed 2m 22s (remain 2m 29s) Loss: 0.0049(0.0101) Grad: 22490.7676  LR: 0.000002  \n","Epoch: [5][800/1438] Elapsed 2m 42s (remain 2m 9s) Loss: 0.0046(0.0105) Grad: 12740.9600  LR: 0.000002  \n","Epoch: [5][900/1438] Elapsed 3m 3s (remain 1m 49s) Loss: 0.0004(0.0105) Grad: 2729.2253  LR: 0.000002  \n","Epoch: [5][1000/1438] Elapsed 3m 23s (remain 1m 28s) Loss: 0.0009(0.0106) Grad: 6917.1514  LR: 0.000001  \n","Epoch: [5][1100/1438] Elapsed 3m 43s (remain 1m 8s) Loss: 0.0010(0.0103) Grad: 4618.6909  LR: 0.000001  \n","Epoch: [5][1200/1438] Elapsed 4m 3s (remain 0m 48s) Loss: 0.0033(0.0102) Grad: 9721.5088  LR: 0.000001  \n","Epoch: [5][1300/1438] Elapsed 4m 24s (remain 0m 27s) Loss: 0.0069(0.0103) Grad: 26298.3496  LR: 0.000000  \n","Epoch: [5][1400/1438] Elapsed 4m 44s (remain 0m 7s) Loss: 0.0204(0.0103) Grad: 43566.9805  LR: 0.000000  \n","Epoch: [5][1437/1438] Elapsed 4m 51s (remain 0m 0s) Loss: 0.0024(0.0102) Grad: 7979.5146  LR: 0.000000  \n","EVAL: [0/349] Elapsed 0m 0s (remain 1m 43s) Loss: 0.0105(0.0105) \n","EVAL: [100/349] Elapsed 0m 7s (remain 0m 17s) Loss: 0.0247(0.0218) \n","EVAL: [200/349] Elapsed 0m 14s (remain 0m 10s) Loss: 0.0657(0.0235) \n","EVAL: [300/349] Elapsed 0m 21s (remain 0m 3s) Loss: 0.0057(0.0235) \n","EVAL: [348/349] Elapsed 0m 24s (remain 0m 0s) Loss: 0.0000(0.0217) \n","Epoch 5 - avg_train_loss: 0.0102  avg_val_loss: 0.0217  time: 320s\n","Epoch 5 - Score: 0.8803\n","Epoch 5 - Save Best Score: 0.8803 Model\n","========== fold: 4 training ==========\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp009/checkpoint-129000/pytorch_model.bin\n","Epoch: [1][0/1425] Elapsed 0m 0s (remain 10m 43s) Loss: 1.4513(1.4513) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/1425] Elapsed 0m 20s (remain 4m 31s) Loss: 0.3307(0.6403) Grad: 17741.4102  LR: 0.000003  \n","Epoch: [1][200/1425] Elapsed 0m 40s (remain 4m 9s) Loss: 0.0512(0.3761) Grad: 994.1177  LR: 0.000006  \n","Epoch: [1][300/1425] Elapsed 1m 1s (remain 3m 48s) Loss: 0.0415(0.2836) Grad: 769.7883  LR: 0.000008  \n","Epoch: [1][400/1425] Elapsed 1m 21s (remain 3m 28s) Loss: 0.0804(0.2319) Grad: 4995.3960  LR: 0.000011  \n","Epoch: [1][500/1425] Elapsed 1m 41s (remain 3m 8s) Loss: 0.0321(0.1949) Grad: 2511.5825  LR: 0.000014  \n","Epoch: [1][600/1425] Elapsed 2m 2s (remain 2m 47s) Loss: 0.0245(0.1686) Grad: 2782.7869  LR: 0.000017  \n","Epoch: [1][700/1425] Elapsed 2m 22s (remain 2m 27s) Loss: 0.0167(0.1491) Grad: 2426.5400  LR: 0.000020  \n","Epoch: [1][800/1425] Elapsed 2m 42s (remain 2m 6s) Loss: 0.0183(0.1347) Grad: 1094.1477  LR: 0.000020  \n","Epoch: [1][900/1425] Elapsed 3m 2s (remain 1m 46s) Loss: 0.0133(0.1235) Grad: 1367.6321  LR: 0.000019  \n","Epoch: [1][1000/1425] Elapsed 3m 23s (remain 1m 26s) Loss: 0.0151(0.1140) Grad: 1131.2157  LR: 0.000019  \n","Epoch: [1][1100/1425] Elapsed 3m 43s (remain 1m 5s) Loss: 0.0108(0.1061) Grad: 2936.4512  LR: 0.000019  \n","Epoch: [1][1200/1425] Elapsed 4m 3s (remain 0m 45s) Loss: 0.0374(0.0995) Grad: 1953.3267  LR: 0.000018  \n","Epoch: [1][1300/1425] Elapsed 4m 24s (remain 0m 25s) Loss: 0.0263(0.0939) Grad: 2068.3821  LR: 0.000018  \n","Epoch: [1][1400/1425] Elapsed 4m 44s (remain 0m 4s) Loss: 0.0198(0.0889) Grad: 1187.1853  LR: 0.000018  \n","Epoch: [1][1424/1425] Elapsed 4m 49s (remain 0m 0s) Loss: 0.0047(0.0877) Grad: 387.6383  LR: 0.000018  \n","EVAL: [0/363] Elapsed 0m 0s (remain 1m 45s) Loss: 0.0132(0.0132) \n","EVAL: [100/363] Elapsed 0m 7s (remain 0m 19s) Loss: 0.0229(0.0236) \n","EVAL: [200/363] Elapsed 0m 14s (remain 0m 11s) Loss: 0.2898(0.0267) \n","EVAL: [300/363] Elapsed 0m 21s (remain 0m 4s) Loss: 0.0253(0.0275) \n","EVAL: [362/363] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0012(0.0242) \n","Epoch 1 - avg_train_loss: 0.0877  avg_val_loss: 0.0242  time: 319s\n","Epoch 1 - Score: 0.8260\n","Epoch 1 - Save Best Score: 0.8260 Model\n","Epoch: [2][0/1425] Elapsed 0m 0s (remain 11m 26s) Loss: 0.0127(0.0127) Grad: 24387.3086  LR: 0.000018  \n","Epoch: [2][100/1425] Elapsed 0m 20s (remain 4m 32s) Loss: 0.0119(0.0236) Grad: 27956.4688  LR: 0.000017  \n","Epoch: [2][200/1425] Elapsed 0m 41s (remain 4m 11s) Loss: 0.0046(0.0220) Grad: 13399.2891  LR: 0.000017  \n","Epoch: [2][300/1425] Elapsed 1m 1s (remain 3m 49s) Loss: 0.0027(0.0202) Grad: 6550.0444  LR: 0.000017  \n","Epoch: [2][400/1425] Elapsed 1m 21s (remain 3m 28s) Loss: 0.0193(0.0192) Grad: 15017.8193  LR: 0.000017  \n","Epoch: [2][500/1425] Elapsed 1m 42s (remain 3m 8s) Loss: 0.0017(0.0191) Grad: 5945.1128  LR: 0.000016  \n","Epoch: [2][600/1425] Elapsed 2m 2s (remain 2m 47s) Loss: 0.0038(0.0189) Grad: 9226.3730  LR: 0.000016  \n","Epoch: [2][700/1425] Elapsed 2m 22s (remain 2m 27s) Loss: 0.0072(0.0190) Grad: 17531.5234  LR: 0.000016  \n","Epoch: [2][800/1425] Elapsed 2m 42s (remain 2m 6s) Loss: 0.0016(0.0190) Grad: 5125.8374  LR: 0.000015  \n","Epoch: [2][900/1425] Elapsed 3m 3s (remain 1m 46s) Loss: 0.0057(0.0190) Grad: 16744.4199  LR: 0.000015  \n","Epoch: [2][1000/1425] Elapsed 3m 23s (remain 1m 26s) Loss: 0.1242(0.0189) Grad: 183756.8281  LR: 0.000015  \n","Epoch: [2][1100/1425] Elapsed 3m 43s (remain 1m 5s) Loss: 0.0063(0.0188) Grad: 29164.8203  LR: 0.000014  \n","Epoch: [2][1200/1425] Elapsed 4m 4s (remain 0m 45s) Loss: 0.0344(0.0190) Grad: 132092.9688  LR: 0.000014  \n","Epoch: [2][1300/1425] Elapsed 4m 24s (remain 0m 25s) Loss: 0.0527(0.0190) Grad: 59413.5703  LR: 0.000014  \n","Epoch: [2][1400/1425] Elapsed 4m 44s (remain 0m 4s) Loss: 0.0087(0.0188) Grad: 18442.9453  LR: 0.000013  \n","Epoch: [2][1424/1425] Elapsed 4m 49s (remain 0m 0s) Loss: 0.0006(0.0187) Grad: 2879.2131  LR: 0.000013  \n","EVAL: [0/363] Elapsed 0m 0s (remain 1m 47s) Loss: 0.0120(0.0120) \n","EVAL: [100/363] Elapsed 0m 7s (remain 0m 18s) Loss: 0.0303(0.0248) \n","EVAL: [200/363] Elapsed 0m 14s (remain 0m 11s) Loss: 0.3017(0.0259) \n","EVAL: [300/363] Elapsed 0m 21s (remain 0m 4s) Loss: 0.0083(0.0251) \n","EVAL: [362/363] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0016(0.0218) \n","Epoch 2 - avg_train_loss: 0.0187  avg_val_loss: 0.0218  time: 319s\n","Epoch 2 - Score: 0.8703\n","Epoch 2 - Save Best Score: 0.8703 Model\n","Epoch: [3][0/1425] Elapsed 0m 0s (remain 11m 25s) Loss: 0.0068(0.0068) Grad: 12722.2354  LR: 0.000013  \n","Epoch: [3][100/1425] Elapsed 0m 20s (remain 4m 33s) Loss: 0.0156(0.0138) Grad: 45284.1992  LR: 0.000013  \n","Epoch: [3][200/1425] Elapsed 0m 41s (remain 4m 10s) Loss: 0.0026(0.0134) Grad: 11002.6865  LR: 0.000013  \n","Epoch: [3][300/1425] Elapsed 1m 1s (remain 3m 49s) Loss: 0.0104(0.0143) Grad: 19226.5488  LR: 0.000012  \n","Epoch: [3][400/1425] Elapsed 1m 21s (remain 3m 29s) Loss: 0.0602(0.0142) Grad: 175973.3438  LR: 0.000012  \n","Epoch: [3][500/1425] Elapsed 1m 42s (remain 3m 8s) Loss: 0.0146(0.0137) Grad: 22344.0332  LR: 0.000012  \n","Epoch: [3][600/1425] Elapsed 2m 2s (remain 2m 48s) Loss: 0.0115(0.0138) Grad: 20138.3320  LR: 0.000011  \n","Epoch: [3][700/1425] Elapsed 2m 22s (remain 2m 27s) Loss: 0.0044(0.0139) Grad: 14059.9014  LR: 0.000011  \n","Epoch: [3][800/1425] Elapsed 2m 43s (remain 2m 7s) Loss: 0.0203(0.0139) Grad: 54221.4375  LR: 0.000011  \n","Epoch: [3][900/1425] Elapsed 3m 3s (remain 1m 46s) Loss: 0.0199(0.0139) Grad: 35201.2109  LR: 0.000011  \n","Epoch: [3][1000/1425] Elapsed 3m 23s (remain 1m 26s) Loss: 0.0125(0.0140) Grad: 33892.4727  LR: 0.000010  \n","Epoch: [3][1100/1425] Elapsed 3m 43s (remain 1m 5s) Loss: 0.0011(0.0142) Grad: 4095.5762  LR: 0.000010  \n","Epoch: [3][1200/1425] Elapsed 4m 4s (remain 0m 45s) Loss: 0.0129(0.0144) Grad: 16871.3926  LR: 0.000010  \n","Epoch: [3][1300/1425] Elapsed 4m 24s (remain 0m 25s) Loss: 0.0002(0.0145) Grad: 1177.4177  LR: 0.000009  \n","Epoch: [3][1400/1425] Elapsed 4m 44s (remain 0m 4s) Loss: 0.0003(0.0146) Grad: 1640.3002  LR: 0.000009  \n","Epoch: [3][1424/1425] Elapsed 4m 49s (remain 0m 0s) Loss: 0.0021(0.0147) Grad: 7528.0635  LR: 0.000009  \n","EVAL: [0/363] Elapsed 0m 0s (remain 1m 46s) Loss: 0.0074(0.0074) \n","EVAL: [100/363] Elapsed 0m 7s (remain 0m 18s) Loss: 0.0266(0.0226) \n","EVAL: [200/363] Elapsed 0m 14s (remain 0m 11s) Loss: 0.3248(0.0259) \n","EVAL: [300/363] Elapsed 0m 21s (remain 0m 4s) Loss: 0.0095(0.0253) \n","EVAL: [362/363] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0016(0.0220) \n","Epoch 3 - avg_train_loss: 0.0147  avg_val_loss: 0.0220  time: 319s\n","Epoch 3 - Score: 0.8681\n","Epoch: [4][0/1425] Elapsed 0m 0s (remain 10m 57s) Loss: 0.0008(0.0008) Grad: 2191.3333  LR: 0.000009  \n","Epoch: [4][100/1425] Elapsed 0m 20s (remain 4m 34s) Loss: 0.0034(0.0097) Grad: 10035.1123  LR: 0.000009  \n","Epoch: [4][200/1425] Elapsed 0m 41s (remain 4m 11s) Loss: 0.0071(0.0102) Grad: 15796.1650  LR: 0.000008  \n","Epoch: [4][300/1425] Elapsed 1m 1s (remain 3m 49s) Loss: 0.0103(0.0113) Grad: 32374.6875  LR: 0.000008  \n","Epoch: [4][400/1425] Elapsed 1m 21s (remain 3m 28s) Loss: 0.0080(0.0111) Grad: 23949.1855  LR: 0.000008  \n","Epoch: [4][500/1425] Elapsed 1m 41s (remain 3m 8s) Loss: 0.0486(0.0116) Grad: 177153.0938  LR: 0.000007  \n","Epoch: [4][600/1425] Elapsed 2m 2s (remain 2m 47s) Loss: 0.0032(0.0116) Grad: 22970.1113  LR: 0.000007  \n","Epoch: [4][700/1425] Elapsed 2m 22s (remain 2m 27s) Loss: 0.0101(0.0113) Grad: 20505.9727  LR: 0.000007  \n","Epoch: [4][800/1425] Elapsed 2m 42s (remain 2m 6s) Loss: 0.0059(0.0117) Grad: 9711.7744  LR: 0.000006  \n","Epoch: [4][900/1425] Elapsed 3m 3s (remain 1m 46s) Loss: 0.0083(0.0115) Grad: 30694.2773  LR: 0.000006  \n","Epoch: [4][1000/1425] Elapsed 3m 23s (remain 1m 26s) Loss: 0.0005(0.0117) Grad: 4028.8440  LR: 0.000006  \n","Epoch: [4][1100/1425] Elapsed 3m 43s (remain 1m 5s) Loss: 0.0011(0.0116) Grad: 11160.9033  LR: 0.000005  \n","Epoch: [4][1200/1425] Elapsed 4m 4s (remain 0m 45s) Loss: 0.0011(0.0116) Grad: 4738.6436  LR: 0.000005  \n","Epoch: [4][1300/1425] Elapsed 4m 24s (remain 0m 25s) Loss: 0.0086(0.0115) Grad: 26819.7109  LR: 0.000005  \n","Epoch: [4][1400/1425] Elapsed 4m 44s (remain 0m 4s) Loss: 0.0212(0.0116) Grad: 120779.7422  LR: 0.000005  \n","Epoch: [4][1424/1425] Elapsed 4m 49s (remain 0m 0s) Loss: 0.0827(0.0117) Grad: 211060.3281  LR: 0.000004  \n","EVAL: [0/363] Elapsed 0m 0s (remain 1m 48s) Loss: 0.0193(0.0193) \n","EVAL: [100/363] Elapsed 0m 7s (remain 0m 18s) Loss: 0.0211(0.0251) \n","EVAL: [200/363] Elapsed 0m 14s (remain 0m 11s) Loss: 0.2003(0.0261) \n","EVAL: [300/363] Elapsed 0m 21s (remain 0m 4s) Loss: 0.0119(0.0259) \n","EVAL: [362/363] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0017(0.0226) \n","Epoch 4 - avg_train_loss: 0.0117  avg_val_loss: 0.0226  time: 319s\n","Epoch 4 - Score: 0.8756\n","Epoch 4 - Save Best Score: 0.8756 Model\n","Epoch: [5][0/1425] Elapsed 0m 0s (remain 11m 56s) Loss: 0.0011(0.0011) Grad: 8229.7695  LR: 0.000004  \n","Epoch: [5][100/1425] Elapsed 0m 20s (remain 4m 33s) Loss: 0.0097(0.0100) Grad: 30457.3906  LR: 0.000004  \n","Epoch: [5][200/1425] Elapsed 0m 41s (remain 4m 9s) Loss: 0.0023(0.0094) Grad: 18641.5605  LR: 0.000004  \n","Epoch: [5][300/1425] Elapsed 1m 1s (remain 3m 49s) Loss: 0.0031(0.0101) Grad: 7889.4629  LR: 0.000004  \n","Epoch: [5][400/1425] Elapsed 1m 21s (remain 3m 28s) Loss: 0.0023(0.0097) Grad: 8637.6631  LR: 0.000003  \n","Epoch: [5][500/1425] Elapsed 1m 42s (remain 3m 8s) Loss: 0.0014(0.0096) Grad: 11149.1602  LR: 0.000003  \n","Epoch: [5][600/1425] Elapsed 2m 2s (remain 2m 47s) Loss: 0.0000(0.0095) Grad: 424.3537  LR: 0.000003  \n","Epoch: [5][700/1425] Elapsed 2m 22s (remain 2m 27s) Loss: 0.0024(0.0098) Grad: 10683.6465  LR: 0.000002  \n","Epoch: [5][800/1425] Elapsed 2m 43s (remain 2m 7s) Loss: 0.0031(0.0097) Grad: 9474.2979  LR: 0.000002  \n","Epoch: [5][900/1425] Elapsed 3m 3s (remain 1m 46s) Loss: 0.0014(0.0095) Grad: 4741.5649  LR: 0.000002  \n","Epoch: [5][1000/1425] Elapsed 3m 23s (remain 1m 26s) Loss: 0.0369(0.0094) Grad: 76064.9141  LR: 0.000001  \n","Epoch: [5][1100/1425] Elapsed 3m 43s (remain 1m 5s) Loss: 0.0116(0.0094) Grad: 38074.1562  LR: 0.000001  \n","Epoch: [5][1200/1425] Elapsed 4m 4s (remain 0m 45s) Loss: 0.0004(0.0093) Grad: 5221.4731  LR: 0.000001  \n","Epoch: [5][1300/1425] Elapsed 4m 24s (remain 0m 25s) Loss: 0.0048(0.0093) Grad: 14089.9609  LR: 0.000000  \n","Epoch: [5][1400/1425] Elapsed 4m 44s (remain 0m 4s) Loss: 0.0013(0.0097) Grad: 2955.6577  LR: 0.000000  \n","Epoch: [5][1424/1425] Elapsed 4m 49s (remain 0m 0s) Loss: 0.0012(0.0097) Grad: 4227.5269  LR: 0.000000  \n","EVAL: [0/363] Elapsed 0m 0s (remain 1m 51s) Loss: 0.0268(0.0268) \n","EVAL: [100/363] Elapsed 0m 7s (remain 0m 18s) Loss: 0.0210(0.0275) \n","EVAL: [200/363] Elapsed 0m 14s (remain 0m 11s) Loss: 0.3007(0.0293) \n","EVAL: [300/363] Elapsed 0m 21s (remain 0m 4s) Loss: 0.0145(0.0292) \n","EVAL: [362/363] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0018(0.0254) \n","Epoch 5 - avg_train_loss: 0.0097  avg_val_loss: 0.0254  time: 319s\n","Epoch 5 - Score: 0.8769\n","Epoch 5 - Save Best Score: 0.8769 Model\n","Best thres: 0.5, Score: 0.8749\n","Best thres: 0.54296875, Score: 0.8752\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"24e4e49430f9475f9876b54d4e0df12c","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/533M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eed95f53642440c8ac70d9a83066f0bb","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception ignored in: <function _ConnectionBase.__del__ at 0x7f27fc3f00e0>\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 132, in __del__\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9437fe5e7f3d4dc4b4190f44aab12c9d","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","\n","Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e4e33393c97c42aaaae9f01618be7658","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1be6f852e4b3450083b6efc3e0d897fb","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6dbd3c2cbb614ebabcbebb1fc40678fc","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n"]}],"source":["if __name__ == \"__main__\":\n","    main()"],"id":"1d4fcf7c"}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"name":"nbme-exp016.ipynb","provenance":[{"file_id":"1Ki_klcxaZrsVzr5gSBbmm5PArM4mfZeQ","timestamp":1646284249430},{"file_id":"1v3I41Ql3KDNAvGIfYb7iRVxyrfNb1VXn","timestamp":1646219594965}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":641.572862,"end_time":"2022-02-27T11:39:50.972497","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-02-27T11:29:09.399635","version":"2.3.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1220250f7e17452fb0a7e24052b71197":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5735fc4858054cb2aefccba5ffa6bd14","IPY_MODEL_ffb3bd07a8b7450a8d980816356e54db","IPY_MODEL_8933a0baf3eb4dfa9b43b73c2ecf499b"],"layout":"IPY_MODEL_482b5c91256442e9a3acc4dcdb0150cf"}},"41ca547022f44ea5a1fdc052194944b3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"482b5c91256442e9a3acc4dcdb0150cf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5735fc4858054cb2aefccba5ffa6bd14":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe5f3260ff9040468b6c913595318387","placeholder":"​","style":"IPY_MODEL_c1514f61f2af4d07aae115959d038ba3","value":"100%"}},"592dd30a52f5443ba41c181d2aa6186c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7337f75f00224b688d339f93ca08e6b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e8283e0d1f94166906e2152e1e052ab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8933a0baf3eb4dfa9b43b73c2ecf499b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f412a965b5744fc1b93ec083a65e739b","placeholder":"​","style":"IPY_MODEL_d7ce65dbd4e64e05bc719593b32d8492","value":" 143/143 [00:00&lt;00:00, 3088.56it/s]"}},"a773a22c91464f5e918c0baa9308845a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc5c1f5e1fa3449480ee30b3bd4c32b3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1514f61f2af4d07aae115959d038ba3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c5fe7a1649094faeaeab1ab4b6b05b45":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d32fe5c8d7484fe487949b77035864e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ec70c4a5e8e84f568e35e23e215f0c72","IPY_MODEL_e516b811f7cb40c9a389d65b87f09eb6","IPY_MODEL_ed26fa6977b24204a7fe3898b4c5100f"],"layout":"IPY_MODEL_592dd30a52f5443ba41c181d2aa6186c"}},"d7ce65dbd4e64e05bc719593b32d8492":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dd2e880359ab43dca23c9f421a5a3525":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0d32bbd3c2e4fcf924e2963f80f70ce":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e516b811f7cb40c9a389d65b87f09eb6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd2e880359ab43dca23c9f421a5a3525","max":42146,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c5fe7a1649094faeaeab1ab4b6b05b45","value":42146}},"ec70c4a5e8e84f568e35e23e215f0c72":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e8283e0d1f94166906e2152e1e052ab","placeholder":"​","style":"IPY_MODEL_bc5c1f5e1fa3449480ee30b3bd4c32b3","value":"100%"}},"ed26fa6977b24204a7fe3898b4c5100f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a773a22c91464f5e918c0baa9308845a","placeholder":"​","style":"IPY_MODEL_7337f75f00224b688d339f93ca08e6b5","value":" 42146/42146 [00:22&lt;00:00, 2009.05it/s]"}},"f412a965b5744fc1b93ec083a65e739b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe5f3260ff9040468b6c913595318387":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffb3bd07a8b7450a8d980816356e54db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0d32bbd3c2e4fcf924e2963f80f70ce","max":143,"min":0,"orientation":"horizontal","style":"IPY_MODEL_41ca547022f44ea5a1fdc052194944b3","value":143}},"24e4e49430f9475f9876b54d4e0df12c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_211394f17c0c43b88143e2f88df1d774","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_79ea80b1b5f2486db1d6dca536471640","IPY_MODEL_1d00cab8a56f4032af2503b61ee53907","IPY_MODEL_9eb4543804384a0eadb8efdcb1aa72d2"]}},"211394f17c0c43b88143e2f88df1d774":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"79ea80b1b5f2486db1d6dca536471640":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_76331fdef4a5448fafb9924f5ec252f9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2a146eb9aa04449c89a803768e6983da"}},"1d00cab8a56f4032af2503b61ee53907":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_953bc48260194236bc3d11b6e862b78c","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":558614189,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":558614189,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_12194b5f66734c5eb4afe84cb2952a6a"}},"9eb4543804384a0eadb8efdcb1aa72d2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e762fb8a31b94215bcea55338d9c1208","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 533M/533M [00:09&lt;00:00, 59.1MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0c7cc0a7920046bfbd748c92460a000e"}},"76331fdef4a5448fafb9924f5ec252f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2a146eb9aa04449c89a803768e6983da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"953bc48260194236bc3d11b6e862b78c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"12194b5f66734c5eb4afe84cb2952a6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e762fb8a31b94215bcea55338d9c1208":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0c7cc0a7920046bfbd748c92460a000e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"eed95f53642440c8ac70d9a83066f0bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c60b7df23ab14cdf9c74fef304b69916","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b176c093b4d34b0693420212c6f780b5","IPY_MODEL_8cd4e57e52984d3d9b94066ddbb3bee5","IPY_MODEL_c78061c3cb87427c9b1b07e9bb2ec254"]}},"c60b7df23ab14cdf9c74fef304b69916":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b176c093b4d34b0693420212c6f780b5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_739029261f934eac8b25b25f939e6dfe","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ff8a0486edcf4afdb1a4b179548edc77"}},"8cd4e57e52984d3d9b94066ddbb3bee5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7b5c4f803c58401384a391db12256999","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_903a4a7239264e8f9d2bd4e19d7b01d0"}},"c78061c3cb87427c9b1b07e9bb2ec254":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f7e4d51d453a40b0afe8d0e84c349ad2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:00&lt;00:00,  1.94it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6f1099e3abaa488fb760cd6093e67968"}},"739029261f934eac8b25b25f939e6dfe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ff8a0486edcf4afdb1a4b179548edc77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7b5c4f803c58401384a391db12256999":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"903a4a7239264e8f9d2bd4e19d7b01d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f7e4d51d453a40b0afe8d0e84c349ad2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6f1099e3abaa488fb760cd6093e67968":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9437fe5e7f3d4dc4b4190f44aab12c9d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4ac5e0964dc8422fa8fe29c14887fc5a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9873854b29ab4aaaafc2a7bee3d4872f","IPY_MODEL_01ee0aa9afbe416c904bc3286db9f7a5","IPY_MODEL_c48a0bedb32a40c789306becd1cecd7f"]}},"4ac5e0964dc8422fa8fe29c14887fc5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9873854b29ab4aaaafc2a7bee3d4872f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_aa498a8b82894905b41ae28f5762ea71","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2cc6a4f403d14b9ebb77737c2c839d1e"}},"01ee0aa9afbe416c904bc3286db9f7a5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_906e5fd4fc3b434196758499eaf4d7b5","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2e393ea6c5a74efe8684b3958633fb1a"}},"c48a0bedb32a40c789306becd1cecd7f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_090863dc8cec41719988077dd9aa346b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:00&lt;00:00,  1.96it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7dd57bbe841447cfbc2e2441744d30dc"}},"aa498a8b82894905b41ae28f5762ea71":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2cc6a4f403d14b9ebb77737c2c839d1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"906e5fd4fc3b434196758499eaf4d7b5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2e393ea6c5a74efe8684b3958633fb1a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"090863dc8cec41719988077dd9aa346b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7dd57bbe841447cfbc2e2441744d30dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e4e33393c97c42aaaae9f01618be7658":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_393cefdaa6904dd08461b32904d84099","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_857b78b3ff0041199b8674d383c98f46","IPY_MODEL_b4931dc8e6f449409280c6171abd586d","IPY_MODEL_e57234d0c68248ebab5f47ab2e5855bf"]}},"393cefdaa6904dd08461b32904d84099":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"857b78b3ff0041199b8674d383c98f46":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_fa696636404a4768b57b3093e6a65759","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fc9cdd0a5c1b42d5b0b776f3bf5d1c30"}},"b4931dc8e6f449409280c6171abd586d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_de29d2a723bf4d6e82f3a0dc7a34f336","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_50a23b5d2d444035a00f4dcd74b8cd63"}},"e57234d0c68248ebab5f47ab2e5855bf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_91ee99383a1e474ebb11b6d617088dae","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:00&lt;00:00,  1.82it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ec4b8387fb4a424bbceda2b1a6d66fa4"}},"fa696636404a4768b57b3093e6a65759":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fc9cdd0a5c1b42d5b0b776f3bf5d1c30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"de29d2a723bf4d6e82f3a0dc7a34f336":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"50a23b5d2d444035a00f4dcd74b8cd63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"91ee99383a1e474ebb11b6d617088dae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ec4b8387fb4a424bbceda2b1a6d66fa4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1be6f852e4b3450083b6efc3e0d897fb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_82527d63da5c485c91b4da7f7cf70a76","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c6a9dcb2003945ad8528c43df25f242f","IPY_MODEL_24613ad69dfb4be89b11c8f5028b68a2","IPY_MODEL_2fd56cd5e6b24ed4bef9a0369cac5d59"]}},"82527d63da5c485c91b4da7f7cf70a76":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c6a9dcb2003945ad8528c43df25f242f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f8abab83d66e4d5db8244c4a71e5b557","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ffe1a8fec73c47b89474316ec63b8dbc"}},"24613ad69dfb4be89b11c8f5028b68a2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_171f0f3f2d8c4984bd0eb1f8d2ed9e7e","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_04400fd7c6384c8a86792e5c77e2efe3"}},"2fd56cd5e6b24ed4bef9a0369cac5d59":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d235dff0210c411ca45a3131af63a64a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:00&lt;00:00,  1.72it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c9578d1a5bfb4af6ab99e75ab0d641a4"}},"f8abab83d66e4d5db8244c4a71e5b557":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ffe1a8fec73c47b89474316ec63b8dbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"171f0f3f2d8c4984bd0eb1f8d2ed9e7e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"04400fd7c6384c8a86792e5c77e2efe3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d235dff0210c411ca45a3131af63a64a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c9578d1a5bfb4af6ab99e75ab0d641a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6dbd3c2cbb614ebabcbebb1fc40678fc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cb386adc5ff14514941eecf366374baa","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f6c9c774405d4a4a8e2f5a5b3b4a664f","IPY_MODEL_a4cbdf34297b4895a97e80ae8519927f","IPY_MODEL_09ebdd8f02404afea1b926edbeb42227"]}},"cb386adc5ff14514941eecf366374baa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f6c9c774405d4a4a8e2f5a5b3b4a664f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cf340943ad40464bb67a78ddf87e9ee3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_147b933570604b3697cc7e312e5ee960"}},"a4cbdf34297b4895a97e80ae8519927f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1234e9d75b974a279743cd703667660c","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bda4d9eec6894bc2899dddb18501bbb3"}},"09ebdd8f02404afea1b926edbeb42227":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_be7eb139c082474da641d5f88bec0e67","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:00&lt;00:00,  1.63it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_470714f90d7549069e5c6c29d9a595e5"}},"cf340943ad40464bb67a78ddf87e9ee3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"147b933570604b3697cc7e312e5ee960":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1234e9d75b974a279743cd703667660c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"bda4d9eec6894bc2899dddb18501bbb3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"be7eb139c082474da641d5f88bec0e67":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"470714f90d7549069e5c6c29d9a595e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":5}