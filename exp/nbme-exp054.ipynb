{"cells":[{"cell_type":"markdown","id":"colored-security","metadata":{"id":"colored-security"},"source":["## References"]},{"cell_type":"markdown","id":"educational-operator","metadata":{"id":"educational-operator"},"source":["- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"]},{"cell_type":"markdown","id":"incorrect-greek","metadata":{"id":"incorrect-greek"},"source":["## Configurations"]},{"cell_type":"code","execution_count":1,"id":"alive-granny","metadata":{"id":"alive-granny","executionInfo":{"status":"ok","timestamp":1647787613621,"user_tz":-540,"elapsed":8,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["EXP_NAME = \"nbme-exp054\"\n","ENV = \"colab\"\n","DEBUG_MODE = False\n","SUBMISSION_MODE = False"]},{"cell_type":"code","execution_count":2,"id":"heavy-prophet","metadata":{"id":"heavy-prophet","executionInfo":{"status":"ok","timestamp":1647787613622,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class CFG:\n","    env=ENV\n","    exp_name=EXP_NAME\n","    debug=DEBUG_MODE\n","    submission=SUBMISSION_MODE\n","    apex=True\n","    input_dir=None\n","    output_dir=None\n","    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n","    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n","    competition_name=\"nbme-score-clinical-patient-notes\"\n","    id_col=\"id\"\n","    target_col=\"location\"\n","    pretrained_model_name=\"microsoft/deberta-large\"\n","    tokenizer=None\n","    max_len=None\n","    output_dim=1\n","    dropout=0.2\n","    num_workers=4\n","    batch_size=3\n","    lr=2e-5\n","    betas=(0.9, 0.98)\n","    weight_decay=0.1\n","    num_warmup_steps_rate=0.1\n","    batch_scheduler=True\n","    epochs=5\n","    n_fold=4\n","    train_fold=[0, 1, 2, 3]\n","    seed=71\n","    gradient_accumulation_steps=2\n","    max_grad_norm=1000\n","    print_freq=100\n","    train=True\n","    inference=True"]},{"cell_type":"code","execution_count":3,"id":"vocational-coating","metadata":{"id":"vocational-coating","executionInfo":{"status":"ok","timestamp":1647787613622,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.train_fold = [0, 1]\n","\n","if CFG.submission:\n","    CFG.train = False\n","    CFG.inference = True"]},{"cell_type":"markdown","id":"private-moderator","metadata":{"id":"private-moderator"},"source":["## Directory Settings"]},{"cell_type":"code","execution_count":4,"id":"married-tokyo","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"married-tokyo","outputId":"3162b751-7288-4180-c141-44b3259b524e","executionInfo":{"status":"ok","timestamp":1647787646472,"user_tz":-540,"elapsed":32856,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["colab\n","Mounted at /content/drive\n","Collecting transformers==4.16.2\n","  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 14.5 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 49.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.63.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2.23.0)\n","Collecting tokenizers!=0.11.3,>=0.10.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 54.0 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.11.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (1.21.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (3.6.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 66.7 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 7.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.16.2) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.16.2) (3.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (1.1.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.16.2\n"]}],"source":["import sys\n","from pathlib import Path\n","\n","\n","print(CFG.env)\n","if CFG.env == \"colab\":\n","    # colab環境\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","    # install packages\n","    !pip install transformers==4.16.2\n","\n","elif CFG.env == \"local\":\n","    # ローカルサーバ\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","\n","elif CFG.env == \"kaggle\":\n","    # kaggle環境\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./\")"]},{"cell_type":"code","execution_count":5,"id":"blank-pierre","metadata":{"id":"blank-pierre","executionInfo":{"status":"ok","timestamp":1647787658223,"user_tz":-540,"elapsed":11759,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["import gc\n","import os\n","import ast\n","import time\n","import math\n","import random\n","import itertools\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","from scipy.optimize import minimize\n","from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as T\n","from torchvision.io import read_image\n","from torch.utils.data import DataLoader, Dataset\n","\n","from transformers import AutoModelForMaskedLM\n","from transformers import BartModel,BertModel,BertTokenizer\n","from transformers import DebertaModel,DebertaTokenizer\n","from transformers import RobertaModel,RobertaTokenizer\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","id":"sound-still","metadata":{"id":"sound-still"},"source":["## Utilities"]},{"cell_type":"code","execution_count":6,"id":"surprised-commercial","metadata":{"id":"surprised-commercial","executionInfo":{"status":"ok","timestamp":1647787658224,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on binary arrays.\n","\n","    Args:\n","        preds (list of lists of ints): Predictions.\n","        truths (list of lists of ints): Ground truths.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    # Micro : aggregating over all instances\n","    preds = np.concatenate(preds)\n","    truths = np.concatenate(truths)\n","    return f1_score(truths, preds)\n","\n","\n","def spans_to_binary(spans, length=None):\n","    \"\"\"\n","    Converts spans to a binary array indicating whether each character is in the span.\n","\n","    Args:\n","        spans (list of lists of two ints): Spans.\n","\n","    Returns:\n","        np array [length]: Binarized spans.\n","    \"\"\"\n","    length = np.max(spans) if length is None else length\n","    binary = np.zeros(length)\n","    for start, end in spans:\n","        binary[start:end] = 1\n","    return binary\n","\n","\n","def span_micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on spans.\n","\n","    Args:\n","        preds (list of lists of two ints): Prediction spans.\n","        truths (list of lists of two ints): Ground truth spans.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    bin_preds = []\n","    bin_truths = []\n","    for pred, truth in zip(preds, truths):\n","        if not len(pred) and not len(truth):\n","            continue\n","        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n","        bin_preds.append(spans_to_binary(pred, length))\n","        bin_truths.append(spans_to_binary(truth, length))\n","    return micro_f1(bin_preds, bin_truths)\n","\n","\n","def get_score(y_true, y_pred):\n","    score = span_micro_f1(y_true, y_pred)\n","    return score"]},{"cell_type":"code","execution_count":7,"id":"interstate-accident","metadata":{"id":"interstate-accident","executionInfo":{"status":"ok","timestamp":1647787658224,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def create_labels_for_scoring(df):\n","    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n","    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n","    for i in range(len(df)):\n","        lst = df.loc[i, \"location\"]\n","        if lst:\n","            new_lst = \";\".join(lst)\n","            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n","\n","    # create labels\n","    truths = []\n","    for location_list in df[\"location_for_create_labels\"].values:\n","        truth = []\n","        if len(location_list) > 0:\n","            location = location_list[0]\n","            for loc in [s.split() for s in location.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                truth.append([start, end])\n","        truths.append(truth)\n","\n","    return truths\n","\n","\n","def get_char_probs(texts, token_probs, tokenizer):\n","    res = [np.zeros(len(t)) for t in texts]\n","    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n","        encoded = tokenizer(\n","            text=text,\n","            max_length=CFG.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n","            start, end = offset_mapping\n","            res[i][start:end] = pred\n","    return res\n","\n","\n","def get_predicted_location_str(char_probs, th=0.5):\n","    results = []\n","    for char_prob in char_probs:\n","        result = np.where(char_prob >= th)[0] + 1\n","        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n","        result = [f\"{min(r)} {max(r)}\" for r in result]\n","        result = \";\".join(result)\n","        results.append(result)\n","    return results\n","\n","\n","def get_predictions(results):\n","    predictions = []\n","    for result in results:\n","        prediction = []\n","        if result != \"\":\n","            for loc in [s.split() for s in result.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                prediction.append([start, end])\n","        predictions.append(prediction)\n","    return predictions\n","\n","\n","def scoring(df, th=0.5):\n","    labels = create_labels_for_scoring(df)\n","\n","    token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n","    char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n","    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n","    preds = get_predictions(predicted_location_str)\n","\n","    score = get_score(labels, preds)\n","    return score\n","\n","\n","def get_best_thres(oof_df):\n","    def f1_opt(x):\n","        return -1 * scoring(oof_df, th=x)\n","\n","    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n","    return best_thres"]},{"cell_type":"code","execution_count":8,"id":"coated-pioneer","metadata":{"id":"coated-pioneer","executionInfo":{"status":"ok","timestamp":1647787658224,"user_tz":-540,"elapsed":5,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return \"%dm %ds\" % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":9,"id":"nervous-delaware","metadata":{"id":"nervous-delaware","executionInfo":{"status":"ok","timestamp":1647787658225,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["seed_everything()"]},{"cell_type":"markdown","id":"functioning-destruction","metadata":{"id":"functioning-destruction"},"source":["## Data Loading"]},{"cell_type":"code","execution_count":10,"id":"global-monte","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"global-monte","outputId":"69810f6f-09d4-48a2-a6b3-927fb04525dc","executionInfo":{"status":"ok","timestamp":1647787660684,"user_tz":-540,"elapsed":2464,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14300, 6), (143, 3), (42146, 3), (5, 4))"]},"metadata":{},"execution_count":10}],"source":["train = pd.read_csv(CFG.input_dir / \"train.csv\")\n","features = pd.read_csv(CFG.input_dir / \"features.csv\")\n","patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n","test = pd.read_csv(CFG.input_dir / \"test.csv\")\n","\n","train.shape, features.shape, patient_notes.shape, test.shape"]},{"cell_type":"code","execution_count":11,"id":"independent-airfare","metadata":{"id":"independent-airfare","executionInfo":{"status":"ok","timestamp":1647787660685,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["if CFG.debug:\n","    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n","    print(train.shape)"]},{"cell_type":"markdown","id":"silent-locator","metadata":{"id":"silent-locator"},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":12,"id":"unusual-fifty","metadata":{"id":"unusual-fifty","executionInfo":{"status":"ok","timestamp":1647787660685,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def preprocess_features(features):\n","    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n","    return features\n","\n","\n","features = preprocess_features(features)"]},{"cell_type":"code","execution_count":13,"id":"decreased-mustang","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"decreased-mustang","outputId":"6886c083-4b86-4c36-ed54-436a6a5e5104","executionInfo":{"status":"ok","timestamp":1647787660685,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14300, 8), (5, 6))"]},"metadata":{},"execution_count":13}],"source":["train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","\n","train.shape, test.shape"]},{"cell_type":"code","execution_count":14,"id":"boolean-trade","metadata":{"id":"boolean-trade","executionInfo":{"status":"ok","timestamp":1647787661018,"user_tz":-540,"elapsed":337,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n","train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"]},{"cell_type":"code","execution_count":15,"id":"accomplished-dakota","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"id":"accomplished-dakota","outputId":"72650165-0712-4704-b34a-90ee0b2f6499","executionInfo":{"status":"ok","timestamp":1647787661019,"user_tz":-540,"elapsed":10,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["0    4399\n","1    8181\n","2    1296\n","3     287\n","4      99\n","5      27\n","6       9\n","7       1\n","8       1\n","Name: annotation_length, dtype: int64"]},"metadata":{}}],"source":["train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n","display(train['annotation_length'].value_counts().sort_index())"]},{"cell_type":"markdown","id":"funded-elizabeth","metadata":{"id":"funded-elizabeth"},"source":["## CV split"]},{"cell_type":"code","execution_count":16,"id":"unexpected-columbia","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":124},"id":"unexpected-columbia","outputId":"eeeb7936-4bbf-47e8-fdde-b8b8207f8b77","executionInfo":{"status":"ok","timestamp":1647787661019,"user_tz":-540,"elapsed":8,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["fold\n","0    3575\n","1    3575\n","2    3575\n","3    3575\n","dtype: int64"]},"metadata":{}}],"source":["Fold = GroupKFold(n_splits=CFG.n_fold)\n","groups = train['pn_num'].values\n","for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n","    train.loc[val_index, 'fold'] = int(n)\n","train['fold'] = train['fold'].astype(int)\n","display(train.groupby('fold').size())"]},{"cell_type":"markdown","id":"critical-archive","metadata":{"id":"critical-archive"},"source":["## Setup tokenizer"]},{"cell_type":"code","execution_count":17,"id":"broken-generator","metadata":{"id":"broken-generator","colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["b12ba7f0320f402589087f3698ed74bb","ec9f2c786cea4a29975a41662b687afd","01f6a644de0a47399fdff9dbc025e68b","93631ec5087643d2abd864b2c0275c92","0cea3148262640c488a7695d78d5beca","2d5ae9e8c5e5428388212d03a57d1108","4def68ab3ab04258b94f40e6b599d8ba","5e42d3dcc97c413fa80ac98cf49c2f11","4e029bc4a0a54667bdaafabfa4f8c426","77dadf187f044d94800d5a2b3d3670df","3f53132ca5a547a99762632fc1cc137e","aabb446961f24dcfb7a53fb3e05dd7e8","f9df580b39e34c419b39a02bd0c9783d","a3788cf9109342e3a9d56a1365bd11ef","f2e9ab33d526472e91eb9e76b4afd08c","feb88886718b43838deacb442a0bdb04","c0ed774b1a9b40efb001981c4a4acaf9","67b5662cf52f4cbf90456df5e46cda3d","b0a75d0ddb25404fa7f976e80222fe15","d5096089fca9407da3ec14297dd84d58","30a507c7deb443eca095902438ca407e","ee69b63640dd4291b14ce80fb1a4a7a8","9d449fbbc07548e1ba097d72fdcad8fe","2074ace01f6240cc8919d86d75682671","742a829aa54541d99e1b0b912ed1bddd","9c0f2abafeaa4975b91e79c20f9f1f27","f603e95248304672a134e47edab6dfee","a98ae10e462b40cb9eacce2444bc8368","6aa69365b619406e83d78f8547c5ce16","e01ecabeae19488e81966a969e31be52","54f9a4f989354459a23924c307418fbf","6b009a79c9d64575b8ec82daf2e2970b","bc676cfc2f4c494abcb5563482381f98","d086a79d02694e9db586f0f2173f667e","f68d9350b6e04a81a960d7d602bfeefc","2ed0c2c853614596999d6650d1494b17","584600300cfc411782d7ecb17b293823","198bb94984b543ae8c3828e759f46ea4","867af0ee00934371b20e9d670dba9d79","789181104cc248ac9ce263afdba41bdc","81ca4495c56148d3bfadf15973586917","885cf71fb18f44269814bda1638b1729","ade1985b963b40068db95a54c287d610","2c5bb10812fc46f9a2f907c0f9a72d5e"]},"executionInfo":{"status":"ok","timestamp":1647787668059,"user_tz":-540,"elapsed":7047,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}},"outputId":"2d90e93c-c95a-4980-fb92-67f01ff0bcfe"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b12ba7f0320f402589087f3698ed74bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/475 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aabb446961f24dcfb7a53fb3e05dd7e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d449fbbc07548e1ba097d72fdcad8fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d086a79d02694e9db586f0f2173f667e"}},"metadata":{}}],"source":["if CFG.submission:\n","    tokenizer = AutoTokenizer.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n","else:\n","    tokenizer = AutoTokenizer.from_pretrained(CFG.pretrained_model_name)\n","    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n","\n","CFG.tokenizer = tokenizer"]},{"cell_type":"markdown","id":"compatible-lincoln","metadata":{"id":"compatible-lincoln"},"source":["## Create dataset"]},{"cell_type":"code","execution_count":18,"id":"fluid-nancy","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["77a61d36dae94ec08352e99853a92706","fea02d90b7b84e7c846fcc2a5750f113","96e92848c4644c5fb49f63c890193257","bfb5de6707624235a220584d4d39b0e9","cc5cfcc00c754042be3e76f37aa70a9c","8f82baca32864d5b8730ef98323cbe5e","1c6c3360d5014173a04566e2831d0978","3e8ee5dacb824dca83690730359df2fd","089d3e7ef7274e6a9dc47806aa4b271b","539e82596c204e208c665edb93dcd601","8da14d3e1d844e40b5c984299d1aad95"]},"id":"fluid-nancy","outputId":"aeafc5c0-0074-41ce-84c9-51221e335cd8","executionInfo":{"status":"ok","timestamp":1647787700319,"user_tz":-540,"elapsed":32274,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/42146 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77a61d36dae94ec08352e99853a92706"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 433\n"]}],"source":["pn_history_lengths = []\n","tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    pn_history_lengths.append(length)\n","\n","print(\"max length:\", np.max(pn_history_lengths))"]},{"cell_type":"code","execution_count":19,"id":"posted-humidity","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["290d7ee54b4d476aa5b6094306e5280d","a8b47ec5238548da84b4866423b8616b","26894cd923324b1091f49e5e850ce8d3","7a747d0f950047cc890106591a197fb6","74cb65d2a5b74853aa3119bb508e2879","d74662b69c874d3db3489ea569158311","4f30fcd408784a5db98d3da1bf8da822","14f940c1aa434194b6bf2deea8ca56f9","9a29b9785382438d8917cb97a8eaf9ea","9c69e1c5ce6846f08e328b076ba39d52","1db64f9777664ef1a07da193b87f7b53"]},"id":"posted-humidity","outputId":"75f9c971-7ccc-4d95-dd4a-6c86293342de","executionInfo":{"status":"ok","timestamp":1647787700320,"user_tz":-540,"elapsed":17,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/143 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"290d7ee54b4d476aa5b6094306e5280d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 30\n"]}],"source":["feature_text_lengths = []\n","tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    feature_text_lengths.append(length)\n","\n","print(\"max length:\", np.max(feature_text_lengths))"]},{"cell_type":"code","execution_count":20,"id":"resistant-amount","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resistant-amount","outputId":"c5380733-e334-4526-93c0-2cbf75d28e25","executionInfo":{"status":"ok","timestamp":1647787700320,"user_tz":-540,"elapsed":11,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["max length: 466\n"]}],"source":["CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n","\n","print(\"max length:\", CFG.max_len)"]},{"cell_type":"code","execution_count":21,"id":"august-equity","metadata":{"id":"august-equity","executionInfo":{"status":"ok","timestamp":1647787700321,"user_tz":-540,"elapsed":8,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class TrainingDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","        self.annotation_lengths = self.df[\"annotation_length\"].values\n","        self.locations = self.df[\"location\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def _create_label(self, pn_history, annotation_length, location_list):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        offset_mapping = encoded[\"offset_mapping\"]\n","        ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n","        label = np.zeros(len(offset_mapping))\n","        label[ignore_idxes] = -1\n","\n","        if annotation_length > 0:\n","            for location in location_list:\n","                for loc in [s.split() for s in location.split(\";\")]:\n","                    start, end = int(loc[0]), int(loc[1])\n","                    start_idx = -1\n","                    end_idx = -1\n","                    for idx in range(len(offset_mapping)):\n","                        if (start_idx == -1) & (start < offset_mapping[idx][0]):\n","                            start_idx = idx - 1\n","                        if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n","                            end_idx = idx + 1\n","                    if start_idx == -1:\n","                        start_idx = end_idx\n","                    if (start_idx != -1) & (end_idx != -1):\n","                        label[start_idx:end_idx] = 1\n","\n","        return torch.tensor(label, dtype=torch.float)\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n","        return input_, label"]},{"cell_type":"code","execution_count":22,"id":"weird-interaction","metadata":{"id":"weird-interaction","executionInfo":{"status":"ok","timestamp":1647787700321,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class TestDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        return input_"]},{"cell_type":"markdown","id":"upper-mobility","metadata":{"id":"upper-mobility"},"source":["## Model"]},{"cell_type":"code","execution_count":23,"id":"spanish-destruction","metadata":{"id":"spanish-destruction","executionInfo":{"status":"ok","timestamp":1647787700322,"user_tz":-540,"elapsed":8,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class CustomModel(nn.Module):\n","    def __init__(self, cfg, model_config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        if model_config_path is None:\n","            self.model_config = AutoConfig.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                output_hidden_states=True,\n","            )\n","        else:\n","            self.model_config = torch.load(model_config_path)\n","\n","        if pretrained:\n","            self.backbone = AutoModel.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                config=self.model_config,\n","            )\n","            print(f\"Load weight from pretrained\")\n","        else:\n","            #self.backbone = AutoModel.from_config(self.model_config)\n","            itpt = AutoModelForMaskedLM.from_config(self.model_config)\n","            path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp010/checkpoint-130170/pytorch_model.bin\")\n","            # path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n","            state_dict = torch.load(path)\n","            itpt.load_state_dict(state_dict)\n","            self.backbone = itpt.deberta\n","            print(f\"Load weight from {path}\")\n","\n","        self.fc = nn.Sequential(\n","            nn.Dropout(self.cfg.dropout),\n","            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n","        )\n","\n","    def forward(self, inputs):\n","        h = self.backbone(**inputs)[\"last_hidden_state\"]\n","        output = self.fc(h)\n","        return output"]},{"cell_type":"markdown","id":"chronic-bullet","metadata":{"id":"chronic-bullet"},"source":["## Training"]},{"cell_type":"code","execution_count":24,"id":"biological-hunger","metadata":{"id":"biological-hunger","executionInfo":{"status":"ok","timestamp":1647787700890,"user_tz":-540,"elapsed":10,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def train_fn(\n","    train_dataloader,\n","    model,\n","    criterion,\n","    optimizer,\n","    epoch,\n","    scheduler,\n","    device,\n","):\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(train_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","        loss = loss.mean()\n","\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","\n","        if CFG.batch_scheduler:\n","            scheduler.step()\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n","            print(\n","                \"Epoch: [{0}][{1}/{2}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                \"Grad: {grad_norm:.4f}  \"\n","                \"LR: {lr:.6f}  \"\n","                .format(\n","                    epoch+1,\n","                    step,\n","                    len(train_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n","                    loss=losses,\n","                     grad_norm=grad_norm,\n","                     lr=scheduler.get_lr()[0],\n","                )\n","            )\n","    return losses.avg"]},{"cell_type":"code","execution_count":25,"id":"satisfied-sterling","metadata":{"id":"satisfied-sterling","executionInfo":{"status":"ok","timestamp":1647787700890,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def valid_fn(\n","    val_dataloader,\n","    model,\n","    criterion,\n","    device,\n","):\n","    model.eval()\n","    preds = []\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(val_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.no_grad():\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","        loss = loss.mean()\n","\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n","            print(\n","                \"EVAL: [{0}/{1}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                .format(\n","                    step, len(val_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n","                    loss=losses,\n","                )\n","            )\n","    preds = np.concatenate(preds)\n","    return losses.avg, preds"]},{"cell_type":"code","execution_count":26,"id":"incorporate-viking","metadata":{"id":"incorporate-viking","executionInfo":{"status":"ok","timestamp":1647787700891,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def inference_fn(test_dataloader, model, device):\n","    model.eval()\n","    model.to(device)\n","    preds = []\n","    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n","    for inputs in tk0:\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        with torch.no_grad():\n","            output = model(inputs)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","    preds = np.concatenate(preds)\n","    return preds"]},{"cell_type":"code","execution_count":27,"id":"dental-sunset","metadata":{"id":"dental-sunset","executionInfo":{"status":"ok","timestamp":1647787700891,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def train_loop(df, i_fold, device):\n","    print(f\"========== fold: {i_fold} training ==========\")\n","    train_idx = df[df[\"fold\"] != i_fold].index\n","    val_idx = df[df[\"fold\"] == i_fold].index\n","\n","    train_folds = df.loc[train_idx].reset_index(drop=True)\n","    val_folds = df.loc[val_idx].reset_index(drop=True)\n","\n","    train_dataset = TrainingDataset(CFG, train_folds)\n","    val_dataset = TrainingDataset(CFG, val_folds)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=True,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=True,\n","    )\n","    val_dataloader = DataLoader(\n","        val_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=False,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=False,\n","    )\n","\n","    # model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","    model = CustomModel(CFG, model_config_path=None, pretrained=False)   # itptを使うため\n","    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n","    model.to(device)\n","\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\"params\": [p for n, p in param_optimizer if not any(\n","            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n","        {\"params\": [p for n, p in param_optimizer if any(\n","            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n","    ]\n","    optimizer = AdamW(\n","        optimizer_grouped_parameters,\n","        lr=CFG.lr,\n","        betas=CFG.betas,\n","        weight_decay=CFG.weight_decay,\n","    )\n","    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n","    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        num_training_steps=num_train_optimization_steps,\n","    )\n","\n","    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    best_score = -1 * np.inf\n","\n","    for epoch in range(CFG.epochs):\n","        start_time = time.time()\n","        avg_loss = train_fn(\n","            train_dataloader,\n","            model,\n","            criterion,\n","            optimizer,\n","            epoch,\n","            scheduler,\n","            device,\n","        )\n","        avg_val_loss, val_preds = valid_fn(\n","            val_dataloader,\n","            model,\n","            criterion,\n","            device,\n","        )\n","\n","        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n","            scheduler.step()\n","\n","        # scoring\n","        val_folds[[str(i) for i in range(CFG.max_len)]] = val_preds\n","        score = scoring(val_folds, th=0.5)\n","\n","        elapsed = time.time() - start_time\n","\n","        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n","        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n","        if score > best_score:\n","            best_score = score\n","            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n","            torch.save({\n","                \"model\": model.state_dict(),\n","                \"predictions\": val_preds,\n","                },\n","                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","            )\n","\n","    predictions = torch.load(\n","        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","        map_location=torch.device(\"cpu\"),\n","    )[\"predictions\"]\n","    val_folds[[str(i) for i in range(CFG.max_len)]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return val_folds"]},{"cell_type":"markdown","id":"brazilian-graphics","metadata":{"id":"brazilian-graphics"},"source":["## Main"]},{"cell_type":"code","execution_count":28,"id":"connected-protein","metadata":{"id":"connected-protein","executionInfo":{"status":"ok","timestamp":1647787700892,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def main():\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    if CFG.train:\n","        oof_df = pd.DataFrame()\n","        for i_fold in range(CFG.n_fold):\n","            if i_fold in CFG.train_fold:\n","                _oof_df = train_loop(train, i_fold, device)\n","                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n","        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    if CFG.submission:\n","        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n","    else:\n","        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    score = scoring(oof_df, th=0.5)\n","    print(f\"Best thres: 0.5, Score: {score:.4f}\")\n","    best_thres = get_best_thres(oof_df)\n","    score = scoring(oof_df, th=best_thres)\n","    print(f\"Best thres: {best_thres}, Score: {score:.4f}\")\n","\n","    if CFG.inference:\n","        test_dataset = TestDataset(CFG, test)\n","        test_dataloader = DataLoader(\n","            test_dataset,\n","            batch_size=CFG.batch_size,\n","            shuffle=False,\n","            num_workers=CFG.num_workers,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","        predictions = []\n","        for i_fold in CFG.train_fold:\n","            if CFG.submission:\n","                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n","                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n","            else:\n","                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n","\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            model.load_state_dict(state[\"model\"])\n","            test_token_probs = inference_fn(test_dataloader, model, device)\n","            test[[f\"fold{i_fold}_{i}\" for i in range(CFG.max_len)]] = test_token_probs\n","            test_char_probs = get_char_probs(test[\"pn_history\"].values, test_token_probs, CFG.tokenizer)\n","            predictions.append(test_char_probs)\n","\n","            del state, test_token_probs, model; gc.collect()\n","            torch.cuda.empty_cache()\n","\n","        predictions = np.mean(predictions, axis=0)\n","        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n","        test[CFG.target_col] = predicted_location_str\n","        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n","        test[[CFG.id_col, CFG.target_col]].to_csv(\n","            CFG.output_dir / \"submission.csv\", index=False\n","        )"]},{"cell_type":"code","execution_count":null,"id":"serious-bunny","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["10be394e641b473a9c1423517bfc9b19","aae3787dfa324de7baebb37c98f367bf","feb0688b35eb4a989c6d9bb72dc66bc7","509e544a0d9d4dce8bc58e9dab282f4e","6e730352ee594fda8755a625fc782c8c","bb6f18e982ec4f988ac8a88239ba609f","22b92759aba34490b60d0d4de7c8c7fa","d5ecaecd0f944c039e60035e7521b50a","91ca493447e1469f9fe35bb9fd21a530","ccd4be6d23aa4fccb4771956de2108cf","59928e8e5bd44b6ebc4b6d5ef0729c5c","4cba8a7329ba479d96980def21c41186","8a5c850862d1420ab1d80bfc68ec4bdd","8670d3fc64ab4c069e33f99ea6990e7b","7e1efe3304db4a5e9403dfdc5f67add5","06fbc68b560741359603eab69cc8c7e0","78b7ec27a08f43329990d37cd967b1fe","a4b8c3f217fc42d9be4e18a945aec0d1","5f4f74ef501f48df8279de45f9cc5f1c","1e715aabca0f4e6aac6f0711a56fb51e","5a434a45d5104c3c82faae3f42a8e974","3cfba912367b4a04879183d13959ecf3","c2370367dc5544bbb288ecd686e33552","863bd50f4afd42ee8375d5a1347b2db7","7cd91870cf394d159a778259d00a0bc5","7227cdbf6ff54101b95c4db8d71f4a32","747072492c9d4686aa615ae44925525b","73b670e64eb8491a8bda8b4c0aa06369","aa426cbcbd1e43ed8d03efeb8afe80c8","b85d1006e33f43b6b121b57d1f040666","3189f8c422894f839999d4019d41ee24","d2f4cbf1f89945598039101ff521fc5a","5aeabef5e99143738c9af0b7ce10fe59","a8a6576de5954a45b61e26cc7cffeef7","018aa8372888466f859f3fb70acfb109","7784b925fb874ebda549afdebc2649bd","ba8b9d7da21848029615bf8e410eac88","ebdaa6ef664641c4b35ac81eb256018e","8a52522b516a4d229040635cd5af97cb","0f9a3d6de4214501bae5a31969b89154","d4033f6ccc0e4640b55d52e7301f3b84","30841d0d333d4fa0b62be47035c0c9ad","38e4dc6435b24a4b927259f0c5a5d1c1","c8080c2268454ca5aeadb618d74e7599"]},"id":"serious-bunny","outputId":"69dc8f73-7ade-400e-8651-25cf0a02537f"},"outputs":[{"output_type":"stream","name":"stdout","text":["========== fold: 0 training ==========\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n","Epoch: [1][0/3575] Elapsed 0m 0s (remain 55m 24s) Loss: 0.3091(0.3091) Grad: 186104.0625  LR: 0.000000  \n","Epoch: [1][100/3575] Elapsed 0m 28s (remain 16m 26s) Loss: 0.2687(0.3001) Grad: 83508.0234  LR: 0.000001  \n","Epoch: [1][200/3575] Elapsed 0m 55s (remain 15m 38s) Loss: 0.1273(0.2505) Grad: 46025.1289  LR: 0.000002  \n","Epoch: [1][300/3575] Elapsed 1m 22s (remain 14m 59s) Loss: 0.0488(0.1937) Grad: inf  LR: 0.000003  \n","Epoch: [1][400/3575] Elapsed 1m 49s (remain 14m 27s) Loss: 0.0258(0.1552) Grad: 2631.4773  LR: 0.000004  \n","Epoch: [1][500/3575] Elapsed 2m 16s (remain 13m 58s) Loss: 0.0400(0.1315) Grad: 2910.5046  LR: 0.000006  \n","Epoch: [1][600/3575] Elapsed 2m 43s (remain 13m 28s) Loss: 0.0097(0.1151) Grad: 2167.4880  LR: 0.000007  \n","Epoch: [1][700/3575] Elapsed 3m 10s (remain 13m 0s) Loss: 0.0031(0.1021) Grad: 1011.3499  LR: 0.000008  \n","Epoch: [1][800/3575] Elapsed 3m 37s (remain 12m 31s) Loss: 0.0241(0.0915) Grad: 20512.9609  LR: 0.000009  \n","Epoch: [1][900/3575] Elapsed 4m 3s (remain 12m 4s) Loss: 0.0037(0.0833) Grad: 2051.4260  LR: 0.000010  \n","Epoch: [1][1000/3575] Elapsed 4m 30s (remain 11m 36s) Loss: 0.0198(0.0761) Grad: 9230.2510  LR: 0.000011  \n","Epoch: [1][1100/3575] Elapsed 4m 57s (remain 11m 8s) Loss: 0.0015(0.0704) Grad: 1817.8834  LR: 0.000012  \n","Epoch: [1][1200/3575] Elapsed 5m 24s (remain 10m 41s) Loss: 0.0015(0.0654) Grad: 2507.7087  LR: 0.000013  \n","Epoch: [1][1300/3575] Elapsed 5m 51s (remain 10m 14s) Loss: 0.0064(0.0613) Grad: 4553.4985  LR: 0.000015  \n","Epoch: [1][1400/3575] Elapsed 6m 18s (remain 9m 46s) Loss: 0.0294(0.0577) Grad: 16559.2480  LR: 0.000016  \n","Epoch: [1][1500/3575] Elapsed 6m 44s (remain 9m 19s) Loss: 0.0127(0.0546) Grad: 6854.9253  LR: 0.000017  \n","Epoch: [1][1600/3575] Elapsed 7m 11s (remain 8m 52s) Loss: 0.0015(0.0518) Grad: 6501.9678  LR: 0.000018  \n","Epoch: [1][1700/3575] Elapsed 7m 38s (remain 8m 25s) Loss: 0.0022(0.0492) Grad: 1753.6016  LR: 0.000019  \n","Epoch: [1][1800/3575] Elapsed 8m 5s (remain 7m 58s) Loss: 0.0283(0.0471) Grad: 6062.7490  LR: 0.000020  \n","Epoch: [1][1900/3575] Elapsed 8m 32s (remain 7m 31s) Loss: 0.0087(0.0451) Grad: 6367.8330  LR: 0.000020  \n","Epoch: [1][2000/3575] Elapsed 8m 59s (remain 7m 4s) Loss: 0.0049(0.0432) Grad: 9177.9297  LR: 0.000020  \n","Epoch: [1][2100/3575] Elapsed 9m 26s (remain 6m 37s) Loss: 0.0036(0.0415) Grad: 2871.4482  LR: 0.000020  \n","Epoch: [1][2200/3575] Elapsed 9m 52s (remain 6m 10s) Loss: 0.0004(0.0400) Grad: 399.6803  LR: 0.000019  \n","Epoch: [1][2300/3575] Elapsed 10m 19s (remain 5m 43s) Loss: 0.0075(0.0387) Grad: 3001.5154  LR: 0.000019  \n","Epoch: [1][2400/3575] Elapsed 10m 46s (remain 5m 16s) Loss: 0.0011(0.0374) Grad: 1241.9459  LR: 0.000019  \n","Epoch: [1][2500/3575] Elapsed 11m 13s (remain 4m 49s) Loss: 0.0004(0.0364) Grad: 246.7233  LR: 0.000019  \n","Epoch: [1][2600/3575] Elapsed 11m 40s (remain 4m 22s) Loss: 0.0006(0.0353) Grad: 1296.7101  LR: 0.000019  \n","Epoch: [1][2700/3575] Elapsed 12m 7s (remain 3m 55s) Loss: 0.0077(0.0343) Grad: 3477.7075  LR: 0.000019  \n","Epoch: [1][2800/3575] Elapsed 12m 33s (remain 3m 28s) Loss: 0.0021(0.0333) Grad: 1368.8033  LR: 0.000019  \n","Epoch: [1][2900/3575] Elapsed 13m 0s (remain 3m 1s) Loss: 0.0069(0.0325) Grad: 5148.9331  LR: 0.000019  \n","Epoch: [1][3000/3575] Elapsed 13m 27s (remain 2m 34s) Loss: 0.0014(0.0316) Grad: 985.1162  LR: 0.000018  \n","Epoch: [1][3100/3575] Elapsed 13m 54s (remain 2m 7s) Loss: 0.0004(0.0308) Grad: 408.5387  LR: 0.000018  \n","Epoch: [1][3200/3575] Elapsed 14m 21s (remain 1m 40s) Loss: 0.0459(0.0301) Grad: 12744.2080  LR: 0.000018  \n","Epoch: [1][3300/3575] Elapsed 14m 48s (remain 1m 13s) Loss: 0.0067(0.0294) Grad: 1467.4297  LR: 0.000018  \n","Epoch: [1][3400/3575] Elapsed 15m 14s (remain 0m 46s) Loss: 0.0142(0.0288) Grad: 15781.9404  LR: 0.000018  \n","Epoch: [1][3500/3575] Elapsed 15m 41s (remain 0m 19s) Loss: 0.0004(0.0281) Grad: 300.3145  LR: 0.000018  \n","Epoch: [1][3574/3575] Elapsed 16m 1s (remain 0m 0s) Loss: 0.0253(0.0277) Grad: 16423.8613  LR: 0.000018  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 7m 31s) Loss: 0.0011(0.0011) \n","EVAL: [100/1192] Elapsed 0m 14s (remain 2m 38s) Loss: 0.0095(0.0053) \n","EVAL: [200/1192] Elapsed 0m 28s (remain 2m 22s) Loss: 0.0247(0.0061) \n","EVAL: [300/1192] Elapsed 0m 43s (remain 2m 8s) Loss: 0.0072(0.0072) \n","EVAL: [400/1192] Elapsed 0m 57s (remain 1m 53s) Loss: 0.0027(0.0073) \n","EVAL: [500/1192] Elapsed 1m 12s (remain 1m 39s) Loss: 0.0115(0.0067) \n","EVAL: [600/1192] Elapsed 1m 26s (remain 1m 24s) Loss: 0.0131(0.0070) \n","EVAL: [700/1192] Elapsed 1m 40s (remain 1m 10s) Loss: 0.0788(0.0082) \n","EVAL: [800/1192] Elapsed 1m 55s (remain 0m 56s) Loss: 0.0038(0.0082) \n","EVAL: [900/1192] Elapsed 2m 9s (remain 0m 41s) Loss: 0.0042(0.0082) \n","EVAL: [1000/1192] Elapsed 2m 23s (remain 0m 27s) Loss: 0.0008(0.0080) \n","EVAL: [1100/1192] Elapsed 2m 38s (remain 0m 13s) Loss: 0.0017(0.0076) \n","EVAL: [1191/1192] Elapsed 2m 51s (remain 0m 0s) Loss: 0.0003(0.0074) \n","Epoch 1 - avg_train_loss: 0.0277  avg_val_loss: 0.0074  time: 1139s\n","Epoch 1 - Score: 0.8476\n","Epoch 1 - Save Best Score: 0.8476 Model\n","Epoch: [2][0/3575] Elapsed 0m 0s (remain 34m 30s) Loss: 0.0020(0.0020) Grad: 6460.4561  LR: 0.000018  \n","Epoch: [2][100/3575] Elapsed 0m 28s (remain 16m 14s) Loss: 0.0118(0.0055) Grad: 12458.9414  LR: 0.000018  \n","Epoch: [2][200/3575] Elapsed 0m 55s (remain 15m 37s) Loss: 0.0002(0.0052) Grad: 3379.4941  LR: 0.000018  \n","Epoch: [2][300/3575] Elapsed 1m 22s (remain 14m 59s) Loss: 0.0058(0.0054) Grad: 10612.8867  LR: 0.000017  \n","Epoch: [2][400/3575] Elapsed 1m 49s (remain 14m 27s) Loss: 0.0318(0.0052) Grad: 15163.7480  LR: 0.000017  \n","Epoch: [2][500/3575] Elapsed 2m 16s (remain 13m 56s) Loss: 0.0029(0.0055) Grad: 36489.1016  LR: 0.000017  \n","Epoch: [2][600/3575] Elapsed 2m 43s (remain 13m 27s) Loss: 0.0134(0.0058) Grad: 21649.0020  LR: 0.000017  \n","Epoch: [2][700/3575] Elapsed 3m 10s (remain 12m 59s) Loss: 0.0048(0.0059) Grad: 13907.8018  LR: 0.000017  \n","Epoch: [2][800/3575] Elapsed 3m 37s (remain 12m 32s) Loss: 0.0060(0.0057) Grad: 8075.6870  LR: 0.000017  \n","Epoch: [2][900/3575] Elapsed 4m 3s (remain 12m 4s) Loss: 0.0046(0.0056) Grad: 16137.8623  LR: 0.000017  \n","Epoch: [2][1000/3575] Elapsed 4m 30s (remain 11m 36s) Loss: 0.0000(0.0055) Grad: 133.1086  LR: 0.000017  \n","Epoch: [2][1100/3575] Elapsed 4m 57s (remain 11m 8s) Loss: 0.0184(0.0055) Grad: 13272.7754  LR: 0.000016  \n","Epoch: [2][1200/3575] Elapsed 5m 24s (remain 10m 41s) Loss: 0.0032(0.0056) Grad: 4599.6396  LR: 0.000016  \n","Epoch: [2][1300/3575] Elapsed 5m 51s (remain 10m 13s) Loss: 0.0008(0.0058) Grad: 1785.1437  LR: 0.000016  \n","Epoch: [2][1400/3575] Elapsed 6m 17s (remain 9m 46s) Loss: 0.0001(0.0057) Grad: 208.5615  LR: 0.000016  \n","Epoch: [2][1500/3575] Elapsed 6m 44s (remain 9m 19s) Loss: 0.0018(0.0057) Grad: 6203.9424  LR: 0.000016  \n","Epoch: [2][1600/3575] Elapsed 7m 11s (remain 8m 52s) Loss: 0.0007(0.0057) Grad: 3511.3223  LR: 0.000016  \n","Epoch: [2][1700/3575] Elapsed 7m 38s (remain 8m 25s) Loss: 0.0139(0.0057) Grad: 18598.5176  LR: 0.000016  \n","Epoch: [2][1800/3575] Elapsed 8m 5s (remain 7m 58s) Loss: 0.0000(0.0057) Grad: 123.1479  LR: 0.000016  \n","Epoch: [2][1900/3575] Elapsed 8m 32s (remain 7m 30s) Loss: 0.0017(0.0056) Grad: 4236.9849  LR: 0.000015  \n","Epoch: [2][2000/3575] Elapsed 8m 58s (remain 7m 3s) Loss: 0.0067(0.0056) Grad: 12651.0801  LR: 0.000015  \n","Epoch: [2][2100/3575] Elapsed 9m 25s (remain 6m 36s) Loss: 0.0207(0.0056) Grad: 5497.5278  LR: 0.000015  \n","Epoch: [2][2200/3575] Elapsed 9m 52s (remain 6m 9s) Loss: 0.0082(0.0056) Grad: 6653.1650  LR: 0.000015  \n","Epoch: [2][2300/3575] Elapsed 10m 19s (remain 5m 42s) Loss: 0.0016(0.0056) Grad: 2881.2212  LR: 0.000015  \n","Epoch: [2][2400/3575] Elapsed 10m 46s (remain 5m 16s) Loss: 0.0058(0.0055) Grad: 12170.9688  LR: 0.000015  \n","Epoch: [2][2500/3575] Elapsed 11m 13s (remain 4m 49s) Loss: 0.0131(0.0055) Grad: 7995.3081  LR: 0.000015  \n","Epoch: [2][2600/3575] Elapsed 11m 39s (remain 4m 22s) Loss: 0.0001(0.0056) Grad: 6274.8604  LR: 0.000015  \n","Epoch: [2][2700/3575] Elapsed 12m 6s (remain 3m 55s) Loss: 0.0106(0.0055) Grad: 21938.1309  LR: 0.000014  \n","Epoch: [2][2800/3575] Elapsed 12m 33s (remain 3m 28s) Loss: 0.0001(0.0056) Grad: 226.9378  LR: 0.000014  \n","Epoch: [2][2900/3575] Elapsed 13m 0s (remain 3m 1s) Loss: 0.0058(0.0056) Grad: 6828.1963  LR: 0.000014  \n","Epoch: [2][3000/3575] Elapsed 13m 27s (remain 2m 34s) Loss: 0.0002(0.0055) Grad: 293.1747  LR: 0.000014  \n","Epoch: [2][3100/3575] Elapsed 13m 53s (remain 2m 7s) Loss: 0.0008(0.0055) Grad: 2101.4116  LR: 0.000014  \n","Epoch: [2][3200/3575] Elapsed 14m 20s (remain 1m 40s) Loss: 0.0001(0.0055) Grad: 70.6004  LR: 0.000014  \n","Epoch: [2][3300/3575] Elapsed 14m 47s (remain 1m 13s) Loss: 0.0029(0.0055) Grad: 3693.5366  LR: 0.000014  \n","Epoch: [2][3400/3575] Elapsed 15m 14s (remain 0m 46s) Loss: 0.0000(0.0055) Grad: 34.0282  LR: 0.000014  \n","Epoch: [2][3500/3575] Elapsed 15m 41s (remain 0m 19s) Loss: 0.0005(0.0056) Grad: 439.5085  LR: 0.000013  \n","Epoch: [2][3574/3575] Elapsed 16m 1s (remain 0m 0s) Loss: 0.0029(0.0056) Grad: 2766.0549  LR: 0.000013  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 7m 59s) Loss: 0.0001(0.0001) \n","EVAL: [100/1192] Elapsed 0m 14s (remain 2m 38s) Loss: 0.0077(0.0035) \n","EVAL: [200/1192] Elapsed 0m 29s (remain 2m 23s) Loss: 0.0101(0.0044) \n","EVAL: [300/1192] Elapsed 0m 43s (remain 2m 8s) Loss: 0.0071(0.0050) \n","EVAL: [400/1192] Elapsed 0m 57s (remain 1m 53s) Loss: 0.0033(0.0052) \n","EVAL: [500/1192] Elapsed 1m 12s (remain 1m 39s) Loss: 0.0114(0.0048) \n","EVAL: [600/1192] Elapsed 1m 26s (remain 1m 25s) Loss: 0.0085(0.0051) \n","EVAL: [700/1192] Elapsed 1m 40s (remain 1m 10s) Loss: 0.0653(0.0064) \n","EVAL: [800/1192] Elapsed 1m 55s (remain 0m 56s) Loss: 0.0017(0.0066) \n","EVAL: [900/1192] Elapsed 2m 9s (remain 0m 41s) Loss: 0.0091(0.0065) \n","EVAL: [1000/1192] Elapsed 2m 23s (remain 0m 27s) Loss: 0.0000(0.0064) \n","EVAL: [1100/1192] Elapsed 2m 38s (remain 0m 13s) Loss: 0.0033(0.0061) \n","EVAL: [1191/1192] Elapsed 2m 51s (remain 0m 0s) Loss: 0.0000(0.0059) \n","Epoch 2 - avg_train_loss: 0.0056  avg_val_loss: 0.0059  time: 1137s\n","Epoch 2 - Score: 0.8707\n","Epoch 2 - Save Best Score: 0.8707 Model\n","Epoch: [3][0/3575] Elapsed 0m 0s (remain 32m 14s) Loss: 0.0004(0.0004) Grad: 1490.4221  LR: 0.000013  \n","Epoch: [3][100/3575] Elapsed 0m 28s (remain 16m 22s) Loss: 0.0000(0.0038) Grad: 172.2761  LR: 0.000013  \n","Epoch: [3][200/3575] Elapsed 0m 55s (remain 15m 38s) Loss: 0.0052(0.0042) Grad: 8174.0938  LR: 0.000013  \n","Epoch: [3][300/3575] Elapsed 1m 22s (remain 15m 0s) Loss: 0.0024(0.0039) Grad: 5388.3413  LR: 0.000013  \n","Epoch: [3][400/3575] Elapsed 1m 49s (remain 14m 27s) Loss: 0.0060(0.0039) Grad: 14839.9443  LR: 0.000013  \n","Epoch: [3][500/3575] Elapsed 2m 16s (remain 13m 56s) Loss: 0.0001(0.0040) Grad: 178.6199  LR: 0.000013  \n","Epoch: [3][600/3575] Elapsed 2m 43s (remain 13m 27s) Loss: 0.0715(0.0040) Grad: 102266.4297  LR: 0.000013  \n","Epoch: [3][700/3575] Elapsed 3m 10s (remain 12m 59s) Loss: 0.0091(0.0042) Grad: 34976.5273  LR: 0.000012  \n","Epoch: [3][800/3575] Elapsed 3m 36s (remain 12m 30s) Loss: 0.0002(0.0042) Grad: 2078.6255  LR: 0.000012  \n","Epoch: [3][900/3575] Elapsed 4m 3s (remain 12m 3s) Loss: 0.0000(0.0042) Grad: 51.5844  LR: 0.000012  \n","Epoch: [3][1000/3575] Elapsed 4m 30s (remain 11m 35s) Loss: 0.0001(0.0041) Grad: 578.8734  LR: 0.000012  \n","Epoch: [3][1100/3575] Elapsed 4m 57s (remain 11m 7s) Loss: 0.0001(0.0040) Grad: 476.2025  LR: 0.000012  \n","Epoch: [3][1200/3575] Elapsed 5m 24s (remain 10m 40s) Loss: 0.0067(0.0041) Grad: 10907.0967  LR: 0.000012  \n","Epoch: [3][1300/3575] Elapsed 5m 50s (remain 10m 13s) Loss: 0.0000(0.0041) Grad: 7.7260  LR: 0.000012  \n","Epoch: [3][1400/3575] Elapsed 6m 17s (remain 9m 46s) Loss: 0.0068(0.0041) Grad: 20025.5762  LR: 0.000012  \n","Epoch: [3][1500/3575] Elapsed 6m 44s (remain 9m 19s) Loss: 0.0000(0.0042) Grad: 13.5934  LR: 0.000011  \n","Epoch: [3][1600/3575] Elapsed 7m 11s (remain 8m 51s) Loss: 0.0001(0.0041) Grad: 692.2795  LR: 0.000011  \n","Epoch: [3][1700/3575] Elapsed 7m 38s (remain 8m 24s) Loss: 0.0005(0.0041) Grad: 2826.7839  LR: 0.000011  \n","Epoch: [3][1800/3575] Elapsed 8m 5s (remain 7m 57s) Loss: 0.0066(0.0040) Grad: 30891.6230  LR: 0.000011  \n","Epoch: [3][1900/3575] Elapsed 8m 31s (remain 7m 30s) Loss: 0.0417(0.0041) Grad: 18898.5273  LR: 0.000011  \n","Epoch: [3][2000/3575] Elapsed 8m 58s (remain 7m 3s) Loss: 0.0072(0.0041) Grad: 31938.8262  LR: 0.000011  \n","Epoch: [3][2100/3575] Elapsed 9m 25s (remain 6m 36s) Loss: 0.0232(0.0041) Grad: 38430.3828  LR: 0.000011  \n","Epoch: [3][2200/3575] Elapsed 9m 52s (remain 6m 9s) Loss: 0.0081(0.0041) Grad: 21605.5957  LR: 0.000011  \n","Epoch: [3][2300/3575] Elapsed 10m 18s (remain 5m 42s) Loss: 0.0047(0.0041) Grad: 4416.7554  LR: 0.000010  \n","Epoch: [3][2400/3575] Elapsed 10m 45s (remain 5m 15s) Loss: 0.0020(0.0041) Grad: 4224.9385  LR: 0.000010  \n","Epoch: [3][2500/3575] Elapsed 11m 12s (remain 4m 48s) Loss: 0.0000(0.0041) Grad: 6.6286  LR: 0.000010  \n","Epoch: [3][2600/3575] Elapsed 11m 39s (remain 4m 21s) Loss: 0.0001(0.0041) Grad: 361.7117  LR: 0.000010  \n","Epoch: [3][2700/3575] Elapsed 12m 6s (remain 3m 55s) Loss: 0.0051(0.0040) Grad: 5175.5586  LR: 0.000010  \n","Epoch: [3][2800/3575] Elapsed 12m 33s (remain 3m 28s) Loss: 0.0047(0.0041) Grad: 9595.6436  LR: 0.000010  \n","Epoch: [3][2900/3575] Elapsed 12m 59s (remain 3m 1s) Loss: 0.0021(0.0041) Grad: 6083.4272  LR: 0.000010  \n","Epoch: [3][3000/3575] Elapsed 13m 26s (remain 2m 34s) Loss: 0.0001(0.0041) Grad: 77.9856  LR: 0.000010  \n","Epoch: [3][3100/3575] Elapsed 13m 53s (remain 2m 7s) Loss: 0.0005(0.0041) Grad: 1620.9718  LR: 0.000009  \n","Epoch: [3][3200/3575] Elapsed 14m 20s (remain 1m 40s) Loss: 0.0029(0.0041) Grad: 4129.1587  LR: 0.000009  \n","Epoch: [3][3300/3575] Elapsed 14m 47s (remain 1m 13s) Loss: 0.0000(0.0041) Grad: 11.4316  LR: 0.000009  \n","Epoch: [3][3400/3575] Elapsed 15m 13s (remain 0m 46s) Loss: 0.0001(0.0041) Grad: 207.2934  LR: 0.000009  \n","Epoch: [3][3500/3575] Elapsed 15m 40s (remain 0m 19s) Loss: 0.0000(0.0041) Grad: 81.8275  LR: 0.000009  \n","Epoch: [3][3574/3575] Elapsed 16m 0s (remain 0m 0s) Loss: 0.0019(0.0041) Grad: 3544.4717  LR: 0.000009  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 4s) Loss: 0.0002(0.0002) \n","EVAL: [100/1192] Elapsed 0m 14s (remain 2m 38s) Loss: 0.0054(0.0045) \n","EVAL: [200/1192] Elapsed 0m 29s (remain 2m 23s) Loss: 0.0065(0.0052) \n","EVAL: [300/1192] Elapsed 0m 43s (remain 2m 8s) Loss: 0.0082(0.0057) \n","EVAL: [400/1192] Elapsed 0m 57s (remain 1m 53s) Loss: 0.0022(0.0058) \n","EVAL: [500/1192] Elapsed 1m 12s (remain 1m 39s) Loss: 0.0085(0.0054) \n","EVAL: [600/1192] Elapsed 1m 26s (remain 1m 25s) Loss: 0.0055(0.0056) \n","EVAL: [700/1192] Elapsed 1m 40s (remain 1m 10s) Loss: 0.0596(0.0068) \n","EVAL: [800/1192] Elapsed 1m 55s (remain 0m 56s) Loss: 0.0010(0.0070) \n","EVAL: [900/1192] Elapsed 2m 9s (remain 0m 41s) Loss: 0.0006(0.0070) \n","EVAL: [1000/1192] Elapsed 2m 23s (remain 0m 27s) Loss: 0.0000(0.0069) \n","EVAL: [1100/1192] Elapsed 2m 38s (remain 0m 13s) Loss: 0.0010(0.0066) \n","EVAL: [1191/1192] Elapsed 2m 51s (remain 0m 0s) Loss: 0.0000(0.0065) \n","Epoch 3 - avg_train_loss: 0.0041  avg_val_loss: 0.0065  time: 1138s\n","Epoch 3 - Score: 0.8761\n","Epoch 3 - Save Best Score: 0.8761 Model\n","Epoch: [4][0/3575] Elapsed 0m 0s (remain 31m 53s) Loss: 0.0005(0.0005) Grad: 3177.6938  LR: 0.000009  \n","Epoch: [4][100/3575] Elapsed 0m 28s (remain 16m 15s) Loss: 0.0061(0.0040) Grad: 17656.7539  LR: 0.000009  \n","Epoch: [4][200/3575] Elapsed 0m 55s (remain 15m 39s) Loss: 0.0000(0.0042) Grad: 213.0178  LR: 0.000009  \n","Epoch: [4][300/3575] Elapsed 1m 22s (remain 15m 0s) Loss: 0.0000(0.0039) Grad: 15.9118  LR: 0.000009  \n","Epoch: [4][400/3575] Elapsed 1m 49s (remain 14m 27s) Loss: 0.0287(0.0039) Grad: 75540.3125  LR: 0.000008  \n","Epoch: [4][500/3575] Elapsed 2m 16s (remain 13m 57s) Loss: 0.0069(0.0037) Grad: 42495.0742  LR: 0.000008  \n","Epoch: [4][600/3575] Elapsed 2m 43s (remain 13m 27s) Loss: 0.0000(0.0038) Grad: 224.0397  LR: 0.000008  \n","Epoch: [4][700/3575] Elapsed 3m 9s (remain 12m 58s) Loss: 0.0020(0.0036) Grad: 4415.2539  LR: 0.000008  \n","Epoch: [4][800/3575] Elapsed 3m 36s (remain 12m 30s) Loss: 0.0000(0.0036) Grad: 45.6617  LR: 0.000008  \n","Epoch: [4][900/3575] Elapsed 4m 3s (remain 12m 2s) Loss: 0.0000(0.0034) Grad: 46.1266  LR: 0.000008  \n","Epoch: [4][1000/3575] Elapsed 4m 30s (remain 11m 35s) Loss: 0.0000(0.0035) Grad: 54.7943  LR: 0.000008  \n","Epoch: [4][1100/3575] Elapsed 4m 57s (remain 11m 7s) Loss: 0.0000(0.0034) Grad: 14.2349  LR: 0.000008  \n","Epoch: [4][1200/3575] Elapsed 5m 23s (remain 10m 40s) Loss: 0.0000(0.0033) Grad: 40.1066  LR: 0.000007  \n","Epoch: [4][1300/3575] Elapsed 5m 50s (remain 10m 12s) Loss: 0.0008(0.0034) Grad: 3768.2710  LR: 0.000007  \n","Epoch: [4][1400/3575] Elapsed 6m 17s (remain 9m 45s) Loss: 0.0019(0.0033) Grad: 18722.5430  LR: 0.000007  \n","Epoch: [4][1500/3575] Elapsed 6m 44s (remain 9m 18s) Loss: 0.0001(0.0033) Grad: 377.4009  LR: 0.000007  \n","Epoch: [4][1600/3575] Elapsed 7m 11s (remain 8m 51s) Loss: 0.0001(0.0033) Grad: 664.5869  LR: 0.000007  \n","Epoch: [4][1700/3575] Elapsed 7m 38s (remain 8m 24s) Loss: 0.0000(0.0034) Grad: 9.1823  LR: 0.000007  \n","Epoch: [4][1800/3575] Elapsed 8m 4s (remain 7m 57s) Loss: 0.0000(0.0033) Grad: 173.9503  LR: 0.000007  \n","Epoch: [4][1900/3575] Elapsed 8m 31s (remain 7m 30s) Loss: 0.0009(0.0033) Grad: 8367.2178  LR: 0.000007  \n","Epoch: [4][2000/3575] Elapsed 8m 58s (remain 7m 3s) Loss: 0.0033(0.0033) Grad: 20556.8301  LR: 0.000006  \n","Epoch: [4][2100/3575] Elapsed 9m 25s (remain 6m 36s) Loss: 0.0029(0.0033) Grad: 9087.1885  LR: 0.000006  \n","Epoch: [4][2200/3575] Elapsed 9m 52s (remain 6m 9s) Loss: 0.0063(0.0033) Grad: 16239.9736  LR: 0.000006  \n","Epoch: [4][2300/3575] Elapsed 10m 18s (remain 5m 42s) Loss: 0.0142(0.0034) Grad: 14063.4678  LR: 0.000006  \n","Epoch: [4][2400/3575] Elapsed 10m 45s (remain 5m 15s) Loss: 0.0000(0.0034) Grad: 10.3829  LR: 0.000006  \n","Epoch: [4][2500/3575] Elapsed 11m 12s (remain 4m 48s) Loss: 0.0011(0.0034) Grad: 13326.5400  LR: 0.000006  \n","Epoch: [4][2600/3575] Elapsed 11m 39s (remain 4m 21s) Loss: 0.0000(0.0034) Grad: 28.0931  LR: 0.000006  \n","Epoch: [4][2700/3575] Elapsed 12m 6s (remain 3m 54s) Loss: 0.0172(0.0033) Grad: 26160.7168  LR: 0.000006  \n","Epoch: [4][2800/3575] Elapsed 12m 32s (remain 3m 28s) Loss: 0.0053(0.0033) Grad: 5650.1235  LR: 0.000005  \n","Epoch: [4][2900/3575] Elapsed 12m 59s (remain 3m 1s) Loss: 0.0052(0.0034) Grad: 45856.5508  LR: 0.000005  \n","Epoch: [4][3000/3575] Elapsed 13m 26s (remain 2m 34s) Loss: 0.0000(0.0033) Grad: 30.0656  LR: 0.000005  \n","Epoch: [4][3100/3575] Elapsed 13m 53s (remain 2m 7s) Loss: 0.0028(0.0034) Grad: 6757.1357  LR: 0.000005  \n","Epoch: [4][3200/3575] Elapsed 14m 20s (remain 1m 40s) Loss: 0.0000(0.0033) Grad: 12.0999  LR: 0.000005  \n","Epoch: [4][3300/3575] Elapsed 14m 46s (remain 1m 13s) Loss: 0.0000(0.0033) Grad: 11.4659  LR: 0.000005  \n","Epoch: [4][3400/3575] Elapsed 15m 13s (remain 0m 46s) Loss: 0.0009(0.0033) Grad: 9970.0283  LR: 0.000005  \n","Epoch: [4][3500/3575] Elapsed 15m 40s (remain 0m 19s) Loss: 0.0000(0.0033) Grad: 92.2109  LR: 0.000005  \n","Epoch: [4][3574/3575] Elapsed 16m 0s (remain 0m 0s) Loss: 0.0000(0.0032) Grad: 19.8234  LR: 0.000004  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 7m 40s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 14s (remain 2m 38s) Loss: 0.0147(0.0059) \n","EVAL: [200/1192] Elapsed 0m 29s (remain 2m 23s) Loss: 0.0056(0.0067) \n","EVAL: [300/1192] Elapsed 0m 43s (remain 2m 8s) Loss: 0.0043(0.0072) \n","EVAL: [400/1192] Elapsed 0m 57s (remain 1m 53s) Loss: 0.0052(0.0072) \n","EVAL: [500/1192] Elapsed 1m 12s (remain 1m 39s) Loss: 0.0109(0.0066) \n","EVAL: [600/1192] Elapsed 1m 26s (remain 1m 25s) Loss: 0.0105(0.0069) \n","EVAL: [700/1192] Elapsed 1m 40s (remain 1m 10s) Loss: 0.0877(0.0085) \n","EVAL: [800/1192] Elapsed 1m 55s (remain 0m 56s) Loss: 0.0011(0.0087) \n","EVAL: [900/1192] Elapsed 2m 9s (remain 0m 41s) Loss: 0.0010(0.0087) \n","EVAL: [1000/1192] Elapsed 2m 23s (remain 0m 27s) Loss: 0.0000(0.0086) \n","EVAL: [1100/1192] Elapsed 2m 38s (remain 0m 13s) Loss: 0.0005(0.0082) \n","EVAL: [1191/1192] Elapsed 2m 51s (remain 0m 0s) Loss: 0.0000(0.0080) \n","Epoch 4 - avg_train_loss: 0.0032  avg_val_loss: 0.0080  time: 1138s\n","Epoch 4 - Score: 0.8793\n","Epoch 4 - Save Best Score: 0.8793 Model\n","Epoch: [5][0/3575] Elapsed 0m 0s (remain 31m 11s) Loss: 0.0000(0.0000) Grad: 51.2928  LR: 0.000004  \n","Epoch: [5][100/3575] Elapsed 0m 28s (remain 16m 6s) Loss: 0.0000(0.0020) Grad: 40.2571  LR: 0.000004  \n","Epoch: [5][200/3575] Elapsed 0m 55s (remain 15m 36s) Loss: 0.0000(0.0021) Grad: 7.4828  LR: 0.000004  \n","Epoch: [5][300/3575] Elapsed 1m 22s (remain 14m 59s) Loss: 0.0000(0.0024) Grad: 3.8040  LR: 0.000004  \n","Epoch: [5][400/3575] Elapsed 1m 49s (remain 14m 26s) Loss: 0.0000(0.0021) Grad: 203.4216  LR: 0.000004  \n","Epoch: [5][500/3575] Elapsed 2m 16s (remain 13m 56s) Loss: 0.0000(0.0021) Grad: 43.7355  LR: 0.000004  \n","Epoch: [5][600/3575] Elapsed 2m 43s (remain 13m 28s) Loss: 0.0059(0.0022) Grad: 27776.8027  LR: 0.000004  \n","Epoch: [5][700/3575] Elapsed 3m 10s (remain 12m 59s) Loss: 0.0000(0.0022) Grad: 2.7673  LR: 0.000004  \n","Epoch: [5][800/3575] Elapsed 3m 37s (remain 12m 31s) Loss: 0.0012(0.0023) Grad: 9003.3887  LR: 0.000003  \n","Epoch: [5][900/3575] Elapsed 4m 3s (remain 12m 4s) Loss: 0.0000(0.0025) Grad: 7.7849  LR: 0.000003  \n","Epoch: [5][1000/3575] Elapsed 4m 30s (remain 11m 36s) Loss: 0.0868(0.0026) Grad: 75929.8281  LR: 0.000003  \n","Epoch: [5][1100/3575] Elapsed 4m 57s (remain 11m 8s) Loss: 0.0063(0.0026) Grad: 27362.5215  LR: 0.000003  \n","Epoch: [5][1200/3575] Elapsed 5m 24s (remain 10m 41s) Loss: 0.0000(0.0026) Grad: 3.0044  LR: 0.000003  \n","Epoch: [5][1300/3575] Elapsed 5m 51s (remain 10m 13s) Loss: 0.0035(0.0026) Grad: 6889.6904  LR: 0.000003  \n","Epoch: [5][1400/3575] Elapsed 6m 18s (remain 9m 46s) Loss: 0.0002(0.0027) Grad: 807.0216  LR: 0.000003  \n","Epoch: [5][1500/3575] Elapsed 6m 44s (remain 9m 19s) Loss: 0.0001(0.0026) Grad: 598.9153  LR: 0.000003  \n","Epoch: [5][1600/3575] Elapsed 7m 11s (remain 8m 52s) Loss: 0.0000(0.0026) Grad: 67.8665  LR: 0.000002  \n","Epoch: [5][1700/3575] Elapsed 7m 38s (remain 8m 25s) Loss: 0.0000(0.0026) Grad: 6.7647  LR: 0.000002  \n","Epoch: [5][1800/3575] Elapsed 8m 5s (remain 7m 58s) Loss: 0.0014(0.0026) Grad: 8946.1016  LR: 0.000002  \n","Epoch: [5][1900/3575] Elapsed 8m 32s (remain 7m 30s) Loss: 0.0000(0.0026) Grad: 8.4381  LR: 0.000002  \n","Epoch: [5][2000/3575] Elapsed 8m 59s (remain 7m 4s) Loss: 0.0001(0.0025) Grad: 597.6441  LR: 0.000002  \n","Epoch: [5][2100/3575] Elapsed 9m 25s (remain 6m 37s) Loss: 0.0039(0.0025) Grad: 27801.9355  LR: 0.000002  \n","Epoch: [5][2200/3575] Elapsed 9m 52s (remain 6m 9s) Loss: 0.0031(0.0025) Grad: 9802.2217  LR: 0.000002  \n","Epoch: [5][2300/3575] Elapsed 10m 19s (remain 5m 42s) Loss: 0.0000(0.0026) Grad: 30.5695  LR: 0.000002  \n","Epoch: [5][2400/3575] Elapsed 10m 46s (remain 5m 16s) Loss: 0.0000(0.0026) Grad: 41.2711  LR: 0.000001  \n","Epoch: [5][2500/3575] Elapsed 11m 13s (remain 4m 49s) Loss: 0.0000(0.0026) Grad: 18.7395  LR: 0.000001  \n","Epoch: [5][2600/3575] Elapsed 11m 39s (remain 4m 22s) Loss: 0.0000(0.0026) Grad: 18.9116  LR: 0.000001  \n","Epoch: [5][2700/3575] Elapsed 12m 6s (remain 3m 55s) Loss: 0.0001(0.0026) Grad: 546.3521  LR: 0.000001  \n","Epoch: [5][2800/3575] Elapsed 12m 33s (remain 3m 28s) Loss: 0.0002(0.0026) Grad: 3537.7014  LR: 0.000001  \n","Epoch: [5][2900/3575] Elapsed 13m 0s (remain 3m 1s) Loss: 0.0045(0.0026) Grad: 21963.4238  LR: 0.000001  \n","Epoch: [5][3000/3575] Elapsed 13m 27s (remain 2m 34s) Loss: 0.0000(0.0026) Grad: 9.8450  LR: 0.000001  \n","Epoch: [5][3100/3575] Elapsed 13m 53s (remain 2m 7s) Loss: 0.0042(0.0025) Grad: 8812.0811  LR: 0.000001  \n","Epoch: [5][3200/3575] Elapsed 14m 20s (remain 1m 40s) Loss: 0.0026(0.0026) Grad: 3351.6160  LR: 0.000000  \n","Epoch: [5][3300/3575] Elapsed 14m 47s (remain 1m 13s) Loss: 0.0000(0.0026) Grad: 32.7306  LR: 0.000000  \n","Epoch: [5][3400/3575] Elapsed 15m 14s (remain 0m 46s) Loss: 0.0001(0.0026) Grad: 891.9313  LR: 0.000000  \n","Epoch: [5][3500/3575] Elapsed 15m 41s (remain 0m 19s) Loss: 0.0000(0.0026) Grad: 994.7393  LR: 0.000000  \n","Epoch: [5][3574/3575] Elapsed 16m 1s (remain 0m 0s) Loss: 0.0032(0.0025) Grad: 11810.6582  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 7m 28s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 14s (remain 2m 38s) Loss: 0.0134(0.0061) \n","EVAL: [200/1192] Elapsed 0m 29s (remain 2m 23s) Loss: 0.0044(0.0071) \n","EVAL: [300/1192] Elapsed 0m 43s (remain 2m 8s) Loss: 0.0043(0.0076) \n","EVAL: [400/1192] Elapsed 0m 57s (remain 1m 53s) Loss: 0.0060(0.0076) \n","EVAL: [500/1192] Elapsed 1m 12s (remain 1m 39s) Loss: 0.0127(0.0070) \n","EVAL: [600/1192] Elapsed 1m 26s (remain 1m 25s) Loss: 0.0130(0.0073) \n","EVAL: [700/1192] Elapsed 1m 40s (remain 1m 10s) Loss: 0.0897(0.0089) \n","EVAL: [800/1192] Elapsed 1m 55s (remain 0m 56s) Loss: 0.0019(0.0091) \n","EVAL: [900/1192] Elapsed 2m 9s (remain 0m 41s) Loss: 0.0018(0.0091) \n","EVAL: [1000/1192] Elapsed 2m 23s (remain 0m 27s) Loss: 0.0000(0.0089) \n","EVAL: [1100/1192] Elapsed 2m 38s (remain 0m 13s) Loss: 0.0001(0.0085) \n","EVAL: [1191/1192] Elapsed 2m 51s (remain 0m 0s) Loss: 0.0000(0.0083) \n","Epoch 5 - avg_train_loss: 0.0025  avg_val_loss: 0.0083  time: 1137s\n","Epoch 5 - Score: 0.8796\n","Epoch 5 - Save Best Score: 0.8796 Model\n","========== fold: 1 training ==========\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n","Epoch: [1][0/3575] Elapsed 0m 0s (remain 40m 19s) Loss: 0.3628(0.3628) Grad: 196703.6250  LR: 0.000000  \n","Epoch: [1][100/3575] Elapsed 0m 29s (remain 16m 40s) Loss: 0.2908(0.3533) Grad: 22454.2480  LR: 0.000001  \n","Epoch: [1][200/3575] Elapsed 0m 55s (remain 15m 39s) Loss: 0.1149(0.2738) Grad: 8062.5728  LR: 0.000002  \n","Epoch: [1][300/3575] Elapsed 1m 22s (remain 15m 1s) Loss: 0.0232(0.2001) Grad: 1006.9271  LR: 0.000003  \n","Epoch: [1][400/3575] Elapsed 1m 49s (remain 14m 28s) Loss: 0.0234(0.1590) Grad: 1305.9164  LR: 0.000004  \n","Epoch: [1][500/3575] Elapsed 2m 16s (remain 13m 57s) Loss: 0.0164(0.1336) Grad: 769.4846  LR: 0.000006  \n","Epoch: [1][600/3575] Elapsed 2m 43s (remain 13m 28s) Loss: 0.0263(0.1168) Grad: 1658.4456  LR: 0.000007  \n","Epoch: [1][700/3575] Elapsed 3m 10s (remain 12m 59s) Loss: 0.0058(0.1035) Grad: 881.3987  LR: 0.000008  \n","Epoch: [1][800/3575] Elapsed 3m 37s (remain 12m 31s) Loss: 0.0955(0.0929) Grad: 6011.4551  LR: 0.000009  \n","Epoch: [1][900/3575] Elapsed 4m 3s (remain 12m 3s) Loss: 0.0298(0.0844) Grad: 3240.9238  LR: 0.000010  \n","Epoch: [1][1000/3575] Elapsed 4m 30s (remain 11m 36s) Loss: 0.0030(0.0770) Grad: 704.5534  LR: 0.000011  \n","Epoch: [1][1100/3575] Elapsed 4m 57s (remain 11m 8s) Loss: 0.0028(0.0712) Grad: 593.3497  LR: 0.000012  \n","Epoch: [1][1200/3575] Elapsed 5m 24s (remain 10m 41s) Loss: 0.0089(0.0661) Grad: 887.5356  LR: 0.000013  \n","Epoch: [1][1300/3575] Elapsed 5m 51s (remain 10m 14s) Loss: 0.0002(0.0618) Grad: 52.5216  LR: 0.000015  \n","Epoch: [1][1400/3575] Elapsed 6m 18s (remain 9m 46s) Loss: 0.0023(0.0581) Grad: 843.5448  LR: 0.000016  \n","Epoch: [1][1500/3575] Elapsed 6m 45s (remain 9m 19s) Loss: 0.0004(0.0549) Grad: 99.5298  LR: 0.000017  \n","Epoch: [1][1600/3575] Elapsed 7m 11s (remain 8m 52s) Loss: 0.0025(0.0521) Grad: 268.3929  LR: 0.000018  \n","Epoch: [1][1700/3575] Elapsed 7m 38s (remain 8m 25s) Loss: 0.0016(0.0495) Grad: 240.6173  LR: 0.000019  \n","Epoch: [1][1800/3575] Elapsed 8m 5s (remain 7m 58s) Loss: 0.0151(0.0472) Grad: 915.6708  LR: 0.000020  \n","Epoch: [1][1900/3575] Elapsed 8m 32s (remain 7m 31s) Loss: 0.0013(0.0451) Grad: 394.0183  LR: 0.000020  \n","Epoch: [1][2000/3575] Elapsed 8m 59s (remain 7m 4s) Loss: 0.0012(0.0432) Grad: 453.6173  LR: 0.000020  \n","Epoch: [1][2100/3575] Elapsed 9m 26s (remain 6m 37s) Loss: 0.0019(0.0415) Grad: 320.0151  LR: 0.000020  \n","Epoch: [1][2200/3575] Elapsed 9m 52s (remain 6m 10s) Loss: 0.0006(0.0400) Grad: 110.7285  LR: 0.000019  \n","Epoch: [1][2300/3575] Elapsed 10m 19s (remain 5m 43s) Loss: 0.0026(0.0386) Grad: 343.2859  LR: 0.000019  \n","Epoch: [1][2400/3575] Elapsed 10m 46s (remain 5m 16s) Loss: 0.0016(0.0372) Grad: 541.7881  LR: 0.000019  \n","Epoch: [1][2500/3575] Elapsed 11m 13s (remain 4m 49s) Loss: 0.0118(0.0361) Grad: 1660.9438  LR: 0.000019  \n","Epoch: [1][2600/3575] Elapsed 11m 40s (remain 4m 22s) Loss: 0.0000(0.0349) Grad: 9.3623  LR: 0.000019  \n","Epoch: [1][2700/3575] Elapsed 12m 7s (remain 3m 55s) Loss: 0.0101(0.0339) Grad: 1040.4015  LR: 0.000019  \n","Epoch: [1][2800/3575] Elapsed 12m 34s (remain 3m 28s) Loss: 0.0058(0.0329) Grad: 672.9091  LR: 0.000019  \n","Epoch: [1][2900/3575] Elapsed 13m 0s (remain 3m 1s) Loss: 0.0162(0.0319) Grad: 2565.7810  LR: 0.000019  \n","Epoch: [1][3000/3575] Elapsed 13m 27s (remain 2m 34s) Loss: 0.0013(0.0311) Grad: 161.3148  LR: 0.000018  \n","Epoch: [1][3100/3575] Elapsed 13m 54s (remain 2m 7s) Loss: 0.0000(0.0303) Grad: 12.6420  LR: 0.000018  \n","Epoch: [1][3200/3575] Elapsed 14m 21s (remain 1m 40s) Loss: 0.0056(0.0296) Grad: 474.7356  LR: 0.000018  \n","Epoch: [1][3300/3575] Elapsed 14m 48s (remain 1m 13s) Loss: 0.0016(0.0289) Grad: 153.7524  LR: 0.000018  \n","Epoch: [1][3400/3575] Elapsed 15m 15s (remain 0m 46s) Loss: 0.0025(0.0282) Grad: 489.4986  LR: 0.000018  \n","Epoch: [1][3500/3575] Elapsed 15m 41s (remain 0m 19s) Loss: 0.0000(0.0276) Grad: 30.0258  LR: 0.000018  \n","Epoch: [1][3574/3575] Elapsed 16m 1s (remain 0m 0s) Loss: 0.0001(0.0271) Grad: 30.0781  LR: 0.000018  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 2s) Loss: 0.0001(0.0001) \n","EVAL: [100/1192] Elapsed 0m 14s (remain 2m 38s) Loss: 0.0001(0.0041) \n","EVAL: [200/1192] Elapsed 0m 28s (remain 2m 22s) Loss: 0.0007(0.0052) \n","EVAL: [300/1192] Elapsed 0m 43s (remain 2m 8s) Loss: 0.0014(0.0084) \n","EVAL: [400/1192] Elapsed 0m 57s (remain 1m 53s) Loss: 0.0141(0.0089) \n","EVAL: [500/1192] Elapsed 1m 11s (remain 1m 39s) Loss: 0.0086(0.0086) \n","EVAL: [600/1192] Elapsed 1m 26s (remain 1m 24s) Loss: 0.1492(0.0090) \n","EVAL: [700/1192] Elapsed 1m 40s (remain 1m 10s) Loss: 0.0145(0.0100) \n","EVAL: [800/1192] Elapsed 1m 55s (remain 0m 56s) Loss: 0.0121(0.0095) \n","EVAL: [900/1192] Elapsed 2m 9s (remain 0m 41s) Loss: 0.0015(0.0095) \n","EVAL: [1000/1192] Elapsed 2m 23s (remain 0m 27s) Loss: 0.0001(0.0091) \n","EVAL: [1100/1192] Elapsed 2m 38s (remain 0m 13s) Loss: 0.0045(0.0086) \n","EVAL: [1191/1192] Elapsed 2m 51s (remain 0m 0s) Loss: 0.0074(0.0081) \n","Epoch 1 - avg_train_loss: 0.0271  avg_val_loss: 0.0081  time: 1138s\n","Epoch 1 - Score: 0.8573\n","Epoch 1 - Save Best Score: 0.8573 Model\n","Epoch: [2][0/3575] Elapsed 0m 0s (remain 36m 3s) Loss: 0.0218(0.0218) Grad: 40778.6797  LR: 0.000018  \n","Epoch: [2][100/3575] Elapsed 0m 28s (remain 16m 26s) Loss: 0.0131(0.0069) Grad: 12378.6504  LR: 0.000018  \n","Epoch: [2][200/3575] Elapsed 0m 56s (remain 15m 40s) Loss: 0.0165(0.0062) Grad: 10879.8076  LR: 0.000018  \n","Epoch: [2][300/3575] Elapsed 1m 23s (remain 15m 3s) Loss: 0.0001(0.0058) Grad: 375.9961  LR: 0.000017  \n","Epoch: [2][400/3575] Elapsed 1m 49s (remain 14m 29s) Loss: 0.0015(0.0058) Grad: 8291.4629  LR: 0.000017  \n","Epoch: [2][500/3575] Elapsed 2m 16s (remain 13m 58s) Loss: 0.0101(0.0058) Grad: 61644.9453  LR: 0.000017  \n","Epoch: [2][600/3575] Elapsed 2m 43s (remain 13m 28s) Loss: 0.0002(0.0056) Grad: 816.6984  LR: 0.000017  \n","Epoch: [2][700/3575] Elapsed 3m 10s (remain 12m 59s) Loss: 0.0000(0.0054) Grad: 54.9663  LR: 0.000017  \n","Epoch: [2][800/3575] Elapsed 3m 36s (remain 12m 31s) Loss: 0.0065(0.0053) Grad: 9841.1123  LR: 0.000017  \n","Epoch: [2][900/3575] Elapsed 4m 3s (remain 12m 3s) Loss: 0.0036(0.0055) Grad: 15788.3955  LR: 0.000017  \n","Epoch: [2][1000/3575] Elapsed 4m 30s (remain 11m 35s) Loss: 0.0076(0.0054) Grad: 21137.4414  LR: 0.000017  \n","Epoch: [2][1100/3575] Elapsed 4m 57s (remain 11m 8s) Loss: 0.0097(0.0053) Grad: 9757.9033  LR: 0.000016  \n","Epoch: [2][1200/3575] Elapsed 5m 24s (remain 10m 40s) Loss: 0.0000(0.0052) Grad: 57.2762  LR: 0.000016  \n","Epoch: [2][1300/3575] Elapsed 5m 50s (remain 10m 13s) Loss: 0.0000(0.0053) Grad: 168.7834  LR: 0.000016  \n","Epoch: [2][1400/3575] Elapsed 6m 17s (remain 9m 46s) Loss: 0.0000(0.0053) Grad: 41.1848  LR: 0.000016  \n","Epoch: [2][1500/3575] Elapsed 6m 44s (remain 9m 18s) Loss: 0.0001(0.0053) Grad: 459.7180  LR: 0.000016  \n","Epoch: [2][1600/3575] Elapsed 7m 11s (remain 8m 51s) Loss: 0.0000(0.0053) Grad: 91.8203  LR: 0.000016  \n","Epoch: [2][1700/3575] Elapsed 7m 38s (remain 8m 24s) Loss: 0.0062(0.0053) Grad: 9667.9941  LR: 0.000016  \n","Epoch: [2][1800/3575] Elapsed 8m 5s (remain 7m 57s) Loss: 0.0000(0.0052) Grad: 22.4029  LR: 0.000016  \n","Epoch: [2][1900/3575] Elapsed 8m 31s (remain 7m 30s) Loss: 0.0013(0.0052) Grad: 8695.1299  LR: 0.000015  \n","Epoch: [2][2000/3575] Elapsed 8m 58s (remain 7m 3s) Loss: 0.0346(0.0052) Grad: 48324.7969  LR: 0.000015  \n","Epoch: [2][2100/3575] Elapsed 9m 25s (remain 6m 36s) Loss: 0.0000(0.0053) Grad: 83.0215  LR: 0.000015  \n","Epoch: [2][2200/3575] Elapsed 9m 52s (remain 6m 9s) Loss: 0.0038(0.0052) Grad: 17776.6816  LR: 0.000015  \n","Epoch: [2][2300/3575] Elapsed 10m 19s (remain 5m 42s) Loss: 0.0000(0.0052) Grad: 28.9912  LR: 0.000015  \n","Epoch: [2][2400/3575] Elapsed 10m 45s (remain 5m 15s) Loss: 0.0074(0.0052) Grad: 17616.9492  LR: 0.000015  \n","Epoch: [2][2500/3575] Elapsed 11m 12s (remain 4m 48s) Loss: 0.0002(0.0052) Grad: 926.5997  LR: 0.000015  \n","Epoch: [2][2600/3575] Elapsed 11m 39s (remain 4m 21s) Loss: 0.0122(0.0051) Grad: 28705.4297  LR: 0.000015  \n","Epoch: [2][2700/3575] Elapsed 12m 6s (remain 3m 55s) Loss: 0.0007(0.0051) Grad: 6996.8091  LR: 0.000014  \n","Epoch: [2][2800/3575] Elapsed 12m 33s (remain 3m 28s) Loss: 0.0001(0.0051) Grad: 166.9204  LR: 0.000014  \n","Epoch: [2][2900/3575] Elapsed 12m 59s (remain 3m 1s) Loss: 0.0080(0.0050) Grad: 66693.1016  LR: 0.000014  \n","Epoch: [2][3000/3575] Elapsed 13m 26s (remain 2m 34s) Loss: 0.0014(0.0051) Grad: 3811.1987  LR: 0.000014  \n","Epoch: [2][3100/3575] Elapsed 13m 53s (remain 2m 7s) Loss: 0.0006(0.0050) Grad: 2819.1545  LR: 0.000014  \n","Epoch: [2][3200/3575] Elapsed 14m 20s (remain 1m 40s) Loss: 0.0003(0.0050) Grad: 2072.7737  LR: 0.000014  \n","Epoch: [2][3300/3575] Elapsed 14m 47s (remain 1m 13s) Loss: 0.0002(0.0050) Grad: 532.4473  LR: 0.000014  \n","Epoch: [2][3400/3575] Elapsed 15m 14s (remain 0m 46s) Loss: 0.0064(0.0050) Grad: 21404.1680  LR: 0.000014  \n","Epoch: [2][3500/3575] Elapsed 15m 40s (remain 0m 19s) Loss: 0.0256(0.0050) Grad: 108490.1641  LR: 0.000013  \n","Epoch: [2][3574/3575] Elapsed 16m 0s (remain 0m 0s) Loss: 0.0000(0.0050) Grad: 104.0165  LR: 0.000013  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 7m 29s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 14s (remain 2m 38s) Loss: 0.0001(0.0047) \n","EVAL: [200/1192] Elapsed 0m 28s (remain 2m 22s) Loss: 0.0004(0.0050) \n","EVAL: [300/1192] Elapsed 0m 43s (remain 2m 8s) Loss: 0.0004(0.0077) \n","EVAL: [400/1192] Elapsed 0m 57s (remain 1m 53s) Loss: 0.0170(0.0080) \n","EVAL: [500/1192] Elapsed 1m 11s (remain 1m 39s) Loss: 0.0147(0.0076) \n","EVAL: [600/1192] Elapsed 1m 26s (remain 1m 24s) Loss: 0.1065(0.0077) \n","EVAL: [700/1192] Elapsed 1m 40s (remain 1m 10s) Loss: 0.0086(0.0085) \n","EVAL: [800/1192] Elapsed 1m 54s (remain 0m 56s) Loss: 0.0218(0.0081) \n","EVAL: [900/1192] Elapsed 2m 9s (remain 0m 41s) Loss: 0.0066(0.0079) \n","EVAL: [1000/1192] Elapsed 2m 23s (remain 0m 27s) Loss: 0.0001(0.0075) \n","EVAL: [1100/1192] Elapsed 2m 38s (remain 0m 13s) Loss: 0.0047(0.0071) \n","EVAL: [1191/1192] Elapsed 2m 50s (remain 0m 0s) Loss: 0.0072(0.0067) \n","Epoch 2 - avg_train_loss: 0.0050  avg_val_loss: 0.0067  time: 1137s\n","Epoch 2 - Score: 0.8754\n","Epoch 2 - Save Best Score: 0.8754 Model\n","Epoch: [3][0/3575] Elapsed 0m 0s (remain 33m 56s) Loss: 0.0000(0.0000) Grad: 112.3566  LR: 0.000013  \n","Epoch: [3][100/3575] Elapsed 0m 28s (remain 16m 24s) Loss: 0.0140(0.0024) Grad: 20348.7695  LR: 0.000013  \n","Epoch: [3][200/3575] Elapsed 0m 56s (remain 15m 40s) Loss: 0.0000(0.0027) Grad: 67.9133  LR: 0.000013  \n","Epoch: [3][300/3575] Elapsed 1m 22s (remain 15m 1s) Loss: 0.0010(0.0036) Grad: 2088.1819  LR: 0.000013  \n","Epoch: [3][400/3575] Elapsed 1m 49s (remain 14m 27s) Loss: 0.0000(0.0038) Grad: 198.8258  LR: 0.000013  \n","Epoch: [3][500/3575] Elapsed 2m 16s (remain 13m 57s) Loss: 0.0032(0.0038) Grad: 9003.5264  LR: 0.000013  \n","Epoch: [3][600/3575] Elapsed 2m 43s (remain 13m 27s) Loss: 0.0000(0.0039) Grad: 56.9340  LR: 0.000013  \n","Epoch: [3][700/3575] Elapsed 3m 10s (remain 13m 0s) Loss: 0.0004(0.0040) Grad: 2245.2742  LR: 0.000012  \n","Epoch: [3][800/3575] Elapsed 3m 37s (remain 12m 32s) Loss: 0.0003(0.0041) Grad: 1398.1481  LR: 0.000012  \n","Epoch: [3][900/3575] Elapsed 4m 4s (remain 12m 4s) Loss: 0.0000(0.0039) Grad: 17.8643  LR: 0.000012  \n","Epoch: [3][1000/3575] Elapsed 4m 30s (remain 11m 36s) Loss: 0.0000(0.0040) Grad: 58.4582  LR: 0.000012  \n","Epoch: [3][1100/3575] Elapsed 4m 57s (remain 11m 8s) Loss: 0.0018(0.0039) Grad: 7882.0767  LR: 0.000012  \n","Epoch: [3][1200/3575] Elapsed 5m 24s (remain 10m 41s) Loss: 0.0014(0.0039) Grad: 4190.4287  LR: 0.000012  \n","Epoch: [3][1300/3575] Elapsed 5m 51s (remain 10m 13s) Loss: 0.0000(0.0039) Grad: 91.1113  LR: 0.000012  \n","Epoch: [3][1400/3575] Elapsed 6m 18s (remain 9m 46s) Loss: 0.0006(0.0040) Grad: 2345.9548  LR: 0.000012  \n","Epoch: [3][1500/3575] Elapsed 6m 44s (remain 9m 19s) Loss: 0.0142(0.0040) Grad: 39466.2031  LR: 0.000011  \n","Epoch: [3][1600/3575] Elapsed 7m 11s (remain 8m 52s) Loss: 0.0003(0.0039) Grad: 2285.1399  LR: 0.000011  \n","Epoch: [3][1700/3575] Elapsed 7m 38s (remain 8m 25s) Loss: 0.0000(0.0039) Grad: 67.9472  LR: 0.000011  \n","Epoch: [3][1800/3575] Elapsed 8m 5s (remain 7m 57s) Loss: 0.0002(0.0039) Grad: 2513.6270  LR: 0.000011  \n","Epoch: [3][1900/3575] Elapsed 8m 32s (remain 7m 30s) Loss: 0.0000(0.0039) Grad: 333.7368  LR: 0.000011  \n","Epoch: [3][2000/3575] Elapsed 8m 58s (remain 7m 3s) Loss: 0.0000(0.0039) Grad: 12.6949  LR: 0.000011  \n","Epoch: [3][2100/3575] Elapsed 9m 25s (remain 6m 36s) Loss: 0.0107(0.0038) Grad: 16534.5664  LR: 0.000011  \n","Epoch: [3][2200/3575] Elapsed 9m 52s (remain 6m 9s) Loss: 0.0000(0.0039) Grad: 49.0088  LR: 0.000011  \n","Epoch: [3][2300/3575] Elapsed 10m 19s (remain 5m 42s) Loss: 0.0083(0.0039) Grad: 98126.9688  LR: 0.000010  \n","Epoch: [3][2400/3575] Elapsed 10m 46s (remain 5m 15s) Loss: 0.0007(0.0040) Grad: 2601.6506  LR: 0.000010  \n","Epoch: [3][2500/3575] Elapsed 11m 12s (remain 4m 48s) Loss: 0.0030(0.0039) Grad: 8723.1465  LR: 0.000010  \n","Epoch: [3][2600/3575] Elapsed 11m 39s (remain 4m 22s) Loss: 0.0033(0.0039) Grad: 39027.6133  LR: 0.000010  \n","Epoch: [3][2700/3575] Elapsed 12m 6s (remain 3m 55s) Loss: 0.0190(0.0039) Grad: 22693.1406  LR: 0.000010  \n","Epoch: [3][2800/3575] Elapsed 12m 33s (remain 3m 28s) Loss: 0.0022(0.0039) Grad: 19201.6543  LR: 0.000010  \n","Epoch: [3][2900/3575] Elapsed 13m 0s (remain 3m 1s) Loss: 0.0079(0.0039) Grad: 23119.9609  LR: 0.000010  \n","Epoch: [3][3000/3575] Elapsed 13m 26s (remain 2m 34s) Loss: 0.0000(0.0039) Grad: 146.9611  LR: 0.000010  \n","Epoch: [3][3100/3575] Elapsed 13m 53s (remain 2m 7s) Loss: 0.0112(0.0039) Grad: 17627.1934  LR: 0.000009  \n","Epoch: [3][3200/3575] Elapsed 14m 20s (remain 1m 40s) Loss: 0.0016(0.0039) Grad: 8405.1299  LR: 0.000009  \n","Epoch: [3][3300/3575] Elapsed 14m 47s (remain 1m 13s) Loss: 0.0000(0.0039) Grad: 8.6388  LR: 0.000009  \n","Epoch: [3][3400/3575] Elapsed 15m 14s (remain 0m 46s) Loss: 0.0001(0.0039) Grad: 2138.5598  LR: 0.000009  \n","Epoch: [3][3500/3575] Elapsed 15m 40s (remain 0m 19s) Loss: 0.0003(0.0039) Grad: 1317.7197  LR: 0.000009  \n","Epoch: [3][3574/3575] Elapsed 16m 0s (remain 0m 0s) Loss: 0.0068(0.0039) Grad: 15312.6465  LR: 0.000009  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 4s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 14s (remain 2m 38s) Loss: 0.0001(0.0049) \n","EVAL: [200/1192] Elapsed 0m 29s (remain 2m 23s) Loss: 0.0006(0.0053) \n","EVAL: [300/1192] Elapsed 0m 43s (remain 2m 8s) Loss: 0.0004(0.0079) \n","EVAL: [400/1192] Elapsed 0m 57s (remain 1m 53s) Loss: 0.0160(0.0079) \n","EVAL: [500/1192] Elapsed 1m 12s (remain 1m 39s) Loss: 0.0165(0.0075) \n","EVAL: [600/1192] Elapsed 1m 26s (remain 1m 24s) Loss: 0.1105(0.0075) \n","EVAL: [700/1192] Elapsed 1m 40s (remain 1m 10s) Loss: 0.0142(0.0083) \n","EVAL: [800/1192] Elapsed 1m 55s (remain 0m 56s) Loss: 0.0132(0.0080) \n","EVAL: [900/1192] Elapsed 2m 9s (remain 0m 41s) Loss: 0.0035(0.0079) \n","EVAL: [1000/1192] Elapsed 2m 23s (remain 0m 27s) Loss: 0.0000(0.0076) \n","EVAL: [1100/1192] Elapsed 2m 38s (remain 0m 13s) Loss: 0.0064(0.0071) \n","EVAL: [1191/1192] Elapsed 2m 51s (remain 0m 0s) Loss: 0.0080(0.0067) \n","Epoch 3 - avg_train_loss: 0.0039  avg_val_loss: 0.0067  time: 1137s\n","Epoch 3 - Score: 0.8832\n","Epoch 3 - Save Best Score: 0.8832 Model\n","Epoch: [4][0/3575] Elapsed 0m 0s (remain 35m 23s) Loss: 0.0002(0.0002) Grad: 1151.1285  LR: 0.000009  \n","Epoch: [4][100/3575] Elapsed 0m 28s (remain 16m 12s) Loss: 0.0000(0.0033) Grad: 18.3368  LR: 0.000009  \n","Epoch: [4][200/3575] Elapsed 0m 55s (remain 15m 32s) Loss: 0.0031(0.0035) Grad: 9805.2451  LR: 0.000009  \n","Epoch: [4][300/3575] Elapsed 1m 22s (remain 14m 55s) Loss: 0.0038(0.0037) Grad: 21363.4160  LR: 0.000009  \n","Epoch: [4][400/3575] Elapsed 1m 49s (remain 14m 24s) Loss: 0.0000(0.0035) Grad: 22.3941  LR: 0.000008  \n","Epoch: [4][500/3575] Elapsed 2m 15s (remain 13m 54s) Loss: 0.0118(0.0034) Grad: 10923.8770  LR: 0.000008  \n","Epoch: [4][600/3575] Elapsed 2m 42s (remain 13m 25s) Loss: 0.0000(0.0031) Grad: 10.4795  LR: 0.000008  \n","Epoch: [4][700/3575] Elapsed 3m 9s (remain 12m 56s) Loss: 0.0000(0.0032) Grad: 248.2583  LR: 0.000008  \n","Epoch: [4][800/3575] Elapsed 3m 36s (remain 12m 29s) Loss: 0.0000(0.0031) Grad: 105.4259  LR: 0.000008  \n","Epoch: [4][900/3575] Elapsed 4m 3s (remain 12m 1s) Loss: 0.0000(0.0030) Grad: 53.8275  LR: 0.000008  \n","Epoch: [4][1000/3575] Elapsed 4m 29s (remain 11m 33s) Loss: 0.0009(0.0030) Grad: 14873.2002  LR: 0.000008  \n","Epoch: [4][1100/3575] Elapsed 4m 56s (remain 11m 7s) Loss: 0.0000(0.0029) Grad: 284.9894  LR: 0.000008  \n","Epoch: [4][1200/3575] Elapsed 5m 23s (remain 10m 39s) Loss: 0.0219(0.0030) Grad: 24248.8320  LR: 0.000007  \n","Epoch: [4][1300/3575] Elapsed 5m 50s (remain 10m 12s) Loss: 0.0000(0.0030) Grad: 29.4067  LR: 0.000007  \n","Epoch: [4][1400/3575] Elapsed 6m 17s (remain 9m 45s) Loss: 0.0001(0.0030) Grad: 3908.6931  LR: 0.000007  \n","Epoch: [4][1500/3575] Elapsed 6m 44s (remain 9m 18s) Loss: 0.0001(0.0031) Grad: 732.3813  LR: 0.000007  \n","Epoch: [4][1600/3575] Elapsed 7m 10s (remain 8m 51s) Loss: 0.0009(0.0031) Grad: 4948.1821  LR: 0.000007  \n","Epoch: [4][1700/3575] Elapsed 7m 37s (remain 8m 24s) Loss: 0.0026(0.0030) Grad: 9100.4053  LR: 0.000007  \n","Epoch: [4][1800/3575] Elapsed 8m 4s (remain 7m 57s) Loss: 0.0000(0.0031) Grad: 52.0047  LR: 0.000007  \n","Epoch: [4][1900/3575] Elapsed 8m 31s (remain 7m 30s) Loss: 0.0000(0.0030) Grad: 3.0022  LR: 0.000007  \n","Epoch: [4][2000/3575] Elapsed 8m 58s (remain 7m 3s) Loss: 0.0136(0.0030) Grad: 42334.4922  LR: 0.000006  \n","Epoch: [4][2100/3575] Elapsed 9m 25s (remain 6m 36s) Loss: 0.0034(0.0030) Grad: 18656.8652  LR: 0.000006  \n","Epoch: [4][2200/3575] Elapsed 9m 51s (remain 6m 9s) Loss: 0.0011(0.0030) Grad: 15345.9814  LR: 0.000006  \n","Epoch: [4][2300/3575] Elapsed 10m 18s (remain 5m 42s) Loss: 0.0005(0.0030) Grad: 4043.0417  LR: 0.000006  \n","Epoch: [4][2400/3575] Elapsed 10m 45s (remain 5m 15s) Loss: 0.0000(0.0030) Grad: 107.4575  LR: 0.000006  \n","Epoch: [4][2500/3575] Elapsed 11m 12s (remain 4m 48s) Loss: 0.0000(0.0030) Grad: 47.5926  LR: 0.000006  \n","Epoch: [4][2600/3575] Elapsed 11m 39s (remain 4m 21s) Loss: 0.0001(0.0030) Grad: 1061.6107  LR: 0.000006  \n","Epoch: [4][2700/3575] Elapsed 12m 6s (remain 3m 54s) Loss: 0.0207(0.0030) Grad: 24448.1484  LR: 0.000006  \n","Epoch: [4][2800/3575] Elapsed 12m 32s (remain 3m 28s) Loss: 0.0003(0.0030) Grad: 1613.3850  LR: 0.000005  \n","Epoch: [4][2900/3575] Elapsed 12m 59s (remain 3m 1s) Loss: 0.0000(0.0031) Grad: 4.0780  LR: 0.000005  \n","Epoch: [4][3000/3575] Elapsed 13m 26s (remain 2m 34s) Loss: 0.0000(0.0031) Grad: 142.4924  LR: 0.000005  \n","Epoch: [4][3100/3575] Elapsed 13m 53s (remain 2m 7s) Loss: 0.0021(0.0031) Grad: 12589.7529  LR: 0.000005  \n","Epoch: [4][3200/3575] Elapsed 14m 20s (remain 1m 40s) Loss: 0.0085(0.0031) Grad: 34897.6719  LR: 0.000005  \n","Epoch: [4][3300/3575] Elapsed 14m 46s (remain 1m 13s) Loss: 0.0090(0.0030) Grad: 26037.8340  LR: 0.000005  \n","Epoch: [4][3400/3575] Elapsed 15m 13s (remain 0m 46s) Loss: 0.0025(0.0030) Grad: 11816.0693  LR: 0.000005  \n","Epoch: [4][3500/3575] Elapsed 15m 40s (remain 0m 19s) Loss: 0.0552(0.0030) Grad: 55196.1211  LR: 0.000005  \n","Epoch: [4][3574/3575] Elapsed 16m 0s (remain 0m 0s) Loss: 0.0002(0.0030) Grad: 4355.8447  LR: 0.000004  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 3s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 14s (remain 2m 38s) Loss: 0.0000(0.0061) \n","EVAL: [200/1192] Elapsed 0m 29s (remain 2m 22s) Loss: 0.0001(0.0069) \n","EVAL: [300/1192] Elapsed 0m 43s (remain 2m 8s) Loss: 0.0001(0.0101) \n","EVAL: [400/1192] Elapsed 0m 57s (remain 1m 53s) Loss: 0.0220(0.0104) \n","EVAL: [500/1192] Elapsed 1m 12s (remain 1m 39s) Loss: 0.0184(0.0097) \n","EVAL: [600/1192] Elapsed 1m 26s (remain 1m 24s) Loss: 0.1210(0.0097) \n","EVAL: [700/1192] Elapsed 1m 40s (remain 1m 10s) Loss: 0.0235(0.0110) \n","EVAL: [800/1192] Elapsed 1m 55s (remain 0m 56s) Loss: 0.0141(0.0105) \n","EVAL: [900/1192] Elapsed 2m 9s (remain 0m 41s) Loss: 0.0036(0.0102) \n","EVAL: [1000/1192] Elapsed 2m 23s (remain 0m 27s) Loss: 0.0000(0.0098) \n","EVAL: [1100/1192] Elapsed 2m 38s (remain 0m 13s) Loss: 0.0071(0.0093) \n","EVAL: [1191/1192] Elapsed 2m 51s (remain 0m 0s) Loss: 0.0097(0.0088) \n","Epoch 4 - avg_train_loss: 0.0030  avg_val_loss: 0.0088  time: 1137s\n","Epoch 4 - Score: 0.8809\n","Epoch: [5][0/3575] Elapsed 0m 0s (remain 31m 16s) Loss: 0.0000(0.0000) Grad: 1000.2751  LR: 0.000004  \n","Epoch: [5][100/3575] Elapsed 0m 27s (remain 15m 48s) Loss: 0.0000(0.0027) Grad: 5.7828  LR: 0.000004  \n","Epoch: [5][200/3575] Elapsed 0m 54s (remain 15m 13s) Loss: 0.0000(0.0025) Grad: 225.8409  LR: 0.000004  \n","Epoch: [5][300/3575] Elapsed 1m 21s (remain 14m 43s) Loss: 0.0018(0.0023) Grad: 23605.8535  LR: 0.000004  \n","Epoch: [5][400/3575] Elapsed 1m 48s (remain 14m 15s) Loss: 0.0000(0.0024) Grad: 164.3576  LR: 0.000004  \n","Epoch: [5][500/3575] Elapsed 2m 15s (remain 13m 48s) Loss: 0.0173(0.0026) Grad: 38336.4023  LR: 0.000004  \n","Epoch: [5][600/3575] Elapsed 2m 42s (remain 13m 21s) Loss: 0.0007(0.0025) Grad: 22549.6035  LR: 0.000004  \n","Epoch: [5][700/3575] Elapsed 3m 8s (remain 12m 54s) Loss: 0.0000(0.0026) Grad: 13.2523  LR: 0.000004  \n","Epoch: [5][800/3575] Elapsed 3m 35s (remain 12m 27s) Loss: 0.0116(0.0026) Grad: 61461.3867  LR: 0.000003  \n","Epoch: [5][900/3575] Elapsed 4m 2s (remain 11m 59s) Loss: 0.0009(0.0026) Grad: 6810.7026  LR: 0.000003  \n","Epoch: [5][1000/3575] Elapsed 4m 29s (remain 11m 32s) Loss: 0.0713(0.0028) Grad: 57865.6406  LR: 0.000003  \n","Epoch: [5][1100/3575] Elapsed 4m 56s (remain 11m 5s) Loss: 0.0004(0.0027) Grad: 8874.6133  LR: 0.000003  \n","Epoch: [5][1200/3575] Elapsed 5m 23s (remain 10m 38s) Loss: 0.0001(0.0027) Grad: 3814.0234  LR: 0.000003  \n","Epoch: [5][1300/3575] Elapsed 5m 49s (remain 10m 11s) Loss: 0.0000(0.0027) Grad: 4.7602  LR: 0.000003  \n","Epoch: [5][1400/3575] Elapsed 6m 16s (remain 9m 44s) Loss: 0.0007(0.0027) Grad: 6454.7695  LR: 0.000003  \n","Epoch: [5][1500/3575] Elapsed 6m 43s (remain 9m 17s) Loss: 0.0030(0.0026) Grad: 5532.8354  LR: 0.000003  \n","Epoch: [5][1600/3575] Elapsed 7m 10s (remain 8m 50s) Loss: 0.0000(0.0026) Grad: 15.5326  LR: 0.000002  \n","Epoch: [5][1700/3575] Elapsed 7m 37s (remain 8m 23s) Loss: 0.0000(0.0026) Grad: 27.0814  LR: 0.000002  \n","Epoch: [5][1800/3575] Elapsed 8m 4s (remain 7m 56s) Loss: 0.0003(0.0026) Grad: 2378.5632  LR: 0.000002  \n","Epoch: [5][1900/3575] Elapsed 8m 30s (remain 7m 29s) Loss: 0.0000(0.0025) Grad: 55.8835  LR: 0.000002  \n","Epoch: [5][2000/3575] Elapsed 8m 57s (remain 7m 3s) Loss: 0.0004(0.0025) Grad: 10838.9756  LR: 0.000002  \n","Epoch: [5][2100/3575] Elapsed 9m 24s (remain 6m 36s) Loss: 0.0001(0.0025) Grad: 1388.5562  LR: 0.000002  \n","Epoch: [5][2200/3575] Elapsed 9m 51s (remain 6m 9s) Loss: 0.0000(0.0025) Grad: 39.7518  LR: 0.000002  \n","Epoch: [5][2300/3575] Elapsed 10m 18s (remain 5m 42s) Loss: 0.0518(0.0025) Grad: 19109.2051  LR: 0.000002  \n","Epoch: [5][2400/3575] Elapsed 10m 45s (remain 5m 15s) Loss: 0.0034(0.0025) Grad: 15809.8496  LR: 0.000001  \n","Epoch: [5][2500/3575] Elapsed 11m 11s (remain 4m 48s) Loss: 0.0013(0.0024) Grad: 10977.7803  LR: 0.000001  \n","Epoch: [5][2600/3575] Elapsed 11m 38s (remain 4m 21s) Loss: 0.0016(0.0024) Grad: 9740.3867  LR: 0.000001  \n","Epoch: [5][2700/3575] Elapsed 12m 5s (remain 3m 54s) Loss: 0.0000(0.0025) Grad: 24.8304  LR: 0.000001  \n","Epoch: [5][2800/3575] Elapsed 12m 32s (remain 3m 27s) Loss: 0.0000(0.0025) Grad: 28.5045  LR: 0.000001  \n","Epoch: [5][2900/3575] Elapsed 12m 59s (remain 3m 1s) Loss: 0.0001(0.0025) Grad: 695.7945  LR: 0.000001  \n","Epoch: [5][3000/3575] Elapsed 13m 26s (remain 2m 34s) Loss: 0.0012(0.0024) Grad: 105231.5234  LR: 0.000001  \n","Epoch: [5][3100/3575] Elapsed 13m 53s (remain 2m 7s) Loss: 0.0000(0.0024) Grad: 21.8080  LR: 0.000001  \n","Epoch: [5][3200/3575] Elapsed 14m 19s (remain 1m 40s) Loss: 0.0000(0.0024) Grad: 36.7496  LR: 0.000000  \n","Epoch: [5][3300/3575] Elapsed 14m 46s (remain 1m 13s) Loss: 0.0000(0.0024) Grad: 68.8226  LR: 0.000000  \n","Epoch: [5][3400/3575] Elapsed 15m 13s (remain 0m 46s) Loss: 0.0030(0.0024) Grad: 22399.2051  LR: 0.000000  \n","Epoch: [5][3500/3575] Elapsed 15m 40s (remain 0m 19s) Loss: 0.0000(0.0024) Grad: 395.6182  LR: 0.000000  \n","Epoch: [5][3574/3575] Elapsed 16m 0s (remain 0m 0s) Loss: 0.0000(0.0024) Grad: 123.0289  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 7m 49s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 14s (remain 2m 38s) Loss: 0.0000(0.0064) \n","EVAL: [200/1192] Elapsed 0m 29s (remain 2m 23s) Loss: 0.0001(0.0069) \n","EVAL: [300/1192] Elapsed 0m 43s (remain 2m 8s) Loss: 0.0001(0.0102) \n","EVAL: [400/1192] Elapsed 0m 57s (remain 1m 53s) Loss: 0.0233(0.0104) \n","EVAL: [500/1192] Elapsed 1m 12s (remain 1m 39s) Loss: 0.0200(0.0097) \n","EVAL: [600/1192] Elapsed 1m 26s (remain 1m 24s) Loss: 0.1186(0.0097) \n","EVAL: [700/1192] Elapsed 1m 40s (remain 1m 10s) Loss: 0.0194(0.0108) \n","EVAL: [800/1192] Elapsed 1m 55s (remain 0m 56s) Loss: 0.0139(0.0104) \n","EVAL: [900/1192] Elapsed 2m 9s (remain 0m 41s) Loss: 0.0025(0.0102) \n","EVAL: [1000/1192] Elapsed 2m 23s (remain 0m 27s) Loss: 0.0000(0.0098) \n","EVAL: [1100/1192] Elapsed 2m 38s (remain 0m 13s) Loss: 0.0069(0.0093) \n","EVAL: [1191/1192] Elapsed 2m 51s (remain 0m 0s) Loss: 0.0096(0.0088) \n","Epoch 5 - avg_train_loss: 0.0024  avg_val_loss: 0.0088  time: 1136s\n","Epoch 5 - Score: 0.8836\n","Epoch 5 - Save Best Score: 0.8836 Model\n","========== fold: 2 training ==========\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n","Epoch: [1][0/3575] Elapsed 0m 0s (remain 41m 2s) Loss: 0.2903(0.2903) Grad: 179879.7812  LR: 0.000000  \n","Epoch: [1][100/3575] Elapsed 0m 28s (remain 16m 35s) Loss: 0.2582(0.2897) Grad: 81746.2969  LR: 0.000001  \n","Epoch: [1][200/3575] Elapsed 0m 55s (remain 15m 37s) Loss: 0.1443(0.2438) Grad: 43142.1758  LR: 0.000002  \n","Epoch: [1][300/3575] Elapsed 1m 22s (remain 15m 0s) Loss: 0.0423(0.1913) Grad: 9051.3574  LR: 0.000003  \n","Epoch: [1][400/3575] Elapsed 1m 49s (remain 14m 28s) Loss: 0.0818(0.1524) Grad: 11887.3672  LR: 0.000004  \n","Epoch: [1][500/3575] Elapsed 2m 16s (remain 13m 58s) Loss: 0.0503(0.1295) Grad: 7600.1504  LR: 0.000006  \n","Epoch: [1][600/3575] Elapsed 2m 43s (remain 13m 28s) Loss: 0.0194(0.1137) Grad: 4443.4839  LR: 0.000007  \n","Epoch: [1][700/3575] Elapsed 3m 10s (remain 13m 0s) Loss: 0.0073(0.1024) Grad: 4984.9014  LR: 0.000008  \n","Epoch: [1][800/3575] Elapsed 3m 37s (remain 12m 32s) Loss: 0.0177(0.0930) Grad: 8884.5850  LR: 0.000009  \n","Epoch: [1][900/3575] Elapsed 4m 4s (remain 12m 4s) Loss: 0.0111(0.0847) Grad: 6345.1577  LR: 0.000010  \n","Epoch: [1][1000/3575] Elapsed 4m 31s (remain 11m 36s) Loss: 0.0233(0.0780) Grad: 7264.9570  LR: 0.000011  \n","Epoch: [1][1100/3575] Elapsed 4m 57s (remain 11m 9s) Loss: 0.0110(0.0721) Grad: 12115.1133  LR: 0.000012  \n","Epoch: [1][1200/3575] Elapsed 5m 24s (remain 10m 42s) Loss: 0.0035(0.0671) Grad: 7225.3789  LR: 0.000013  \n","Epoch: [1][1300/3575] Elapsed 5m 51s (remain 10m 15s) Loss: 0.0026(0.0630) Grad: 2723.0903  LR: 0.000015  \n","Epoch: [1][1400/3575] Elapsed 6m 18s (remain 9m 47s) Loss: 0.0099(0.0593) Grad: 17174.8516  LR: 0.000016  \n","Epoch: [1][1500/3575] Elapsed 6m 45s (remain 9m 20s) Loss: 0.0026(0.0561) Grad: 2762.3799  LR: 0.000017  \n","Epoch: [1][1600/3575] Elapsed 7m 12s (remain 8m 53s) Loss: 0.0105(0.0534) Grad: 9995.1025  LR: 0.000018  \n","Epoch: [1][1700/3575] Elapsed 7m 39s (remain 8m 26s) Loss: 0.0002(0.0508) Grad: 3244.2092  LR: 0.000019  \n","Epoch: [1][1800/3575] Elapsed 8m 6s (remain 7m 58s) Loss: 0.0222(0.0487) Grad: 30452.3184  LR: 0.000020  \n","Epoch: [1][1900/3575] Elapsed 8m 33s (remain 7m 31s) Loss: 0.0087(0.0466) Grad: 9883.0127  LR: 0.000020  \n","Epoch: [1][2000/3575] Elapsed 8m 59s (remain 7m 4s) Loss: 0.0102(0.0447) Grad: 5459.5068  LR: 0.000020  \n","Epoch: [1][2100/3575] Elapsed 9m 26s (remain 6m 37s) Loss: 0.0067(0.0431) Grad: 23598.8965  LR: 0.000020  \n","Epoch: [1][2200/3575] Elapsed 9m 53s (remain 6m 10s) Loss: 0.0120(0.0417) Grad: 14591.4395  LR: 0.000019  \n","Epoch: [1][2300/3575] Elapsed 10m 20s (remain 5m 43s) Loss: 0.0041(0.0402) Grad: 29018.9277  LR: 0.000019  \n","Epoch: [1][2400/3575] Elapsed 10m 47s (remain 5m 16s) Loss: 0.0035(0.0390) Grad: 5214.0234  LR: 0.000019  \n","Epoch: [1][2500/3575] Elapsed 11m 14s (remain 4m 49s) Loss: 0.0066(0.0377) Grad: 8649.2529  LR: 0.000019  \n","Epoch: [1][2600/3575] Elapsed 11m 41s (remain 4m 22s) Loss: 0.0008(0.0365) Grad: 1965.7693  LR: 0.000019  \n","Epoch: [1][2700/3575] Elapsed 12m 8s (remain 3m 55s) Loss: 0.0446(0.0356) Grad: 98834.4141  LR: 0.000019  \n","Epoch: [1][2800/3575] Elapsed 12m 34s (remain 3m 28s) Loss: 0.0016(0.0346) Grad: 1839.6475  LR: 0.000019  \n","Epoch: [1][2900/3575] Elapsed 13m 1s (remain 3m 1s) Loss: 0.0043(0.0337) Grad: 5522.0254  LR: 0.000019  \n","Epoch: [1][3000/3575] Elapsed 13m 28s (remain 2m 34s) Loss: 0.0036(0.0328) Grad: 6870.4741  LR: 0.000018  \n","Epoch: [1][3100/3575] Elapsed 13m 55s (remain 2m 7s) Loss: 0.0181(0.0320) Grad: 13728.0186  LR: 0.000018  \n","Epoch: [1][3200/3575] Elapsed 14m 22s (remain 1m 40s) Loss: 0.0000(0.0312) Grad: 43.4974  LR: 0.000018  \n","Epoch: [1][3300/3575] Elapsed 14m 49s (remain 1m 13s) Loss: 0.0011(0.0305) Grad: 1173.1458  LR: 0.000018  \n","Epoch: [1][3400/3575] Elapsed 15m 15s (remain 0m 46s) Loss: 0.0177(0.0299) Grad: 16345.9951  LR: 0.000018  \n","Epoch: [1][3500/3575] Elapsed 15m 42s (remain 0m 19s) Loss: 0.0021(0.0292) Grad: 2315.2625  LR: 0.000018  \n","Epoch: [1][3574/3575] Elapsed 16m 2s (remain 0m 0s) Loss: 0.0070(0.0288) Grad: 5593.9575  LR: 0.000018  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 49s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 14s (remain 2m 39s) Loss: 0.0119(0.0065) \n","EVAL: [200/1192] Elapsed 0m 29s (remain 2m 23s) Loss: 0.0067(0.0063) \n","EVAL: [300/1192] Elapsed 0m 43s (remain 2m 8s) Loss: 0.0201(0.0060) \n","EVAL: [400/1192] Elapsed 0m 57s (remain 1m 54s) Loss: 0.0029(0.0067) \n","EVAL: [500/1192] Elapsed 1m 12s (remain 1m 39s) Loss: 0.0116(0.0064) \n","EVAL: [600/1192] Elapsed 1m 26s (remain 1m 25s) Loss: 0.0059(0.0069) \n","EVAL: [700/1192] Elapsed 1m 40s (remain 1m 10s) Loss: 0.0053(0.0080) \n","EVAL: [800/1192] Elapsed 1m 55s (remain 0m 56s) Loss: 0.0000(0.0080) \n","EVAL: [900/1192] Elapsed 2m 9s (remain 0m 41s) Loss: 0.0077(0.0084) \n","EVAL: [1000/1192] Elapsed 2m 23s (remain 0m 27s) Loss: 0.0070(0.0084) \n","EVAL: [1100/1192] Elapsed 2m 38s (remain 0m 13s) Loss: 0.0448(0.0080) \n","EVAL: [1191/1192] Elapsed 2m 51s (remain 0m 0s) Loss: 0.0001(0.0076) \n","Epoch 1 - avg_train_loss: 0.0288  avg_val_loss: 0.0076  time: 1140s\n","Epoch 1 - Score: 0.8506\n","Epoch 1 - Save Best Score: 0.8506 Model\n","Epoch: [2][0/3575] Elapsed 0m 0s (remain 36m 24s) Loss: 0.0001(0.0001) Grad: 1306.0039  LR: 0.000018  \n","Epoch: [2][100/3575] Elapsed 0m 28s (remain 16m 12s) Loss: 0.0099(0.0065) Grad: 12789.0830  LR: 0.000018  \n","Epoch: [2][200/3575] Elapsed 0m 55s (remain 15m 37s) Loss: 0.0027(0.0062) Grad: 4035.7153  LR: 0.000018  \n","Epoch: [2][300/3575] Elapsed 1m 22s (remain 15m 1s) Loss: 0.0178(0.0059) Grad: 41614.8281  LR: 0.000017  \n","Epoch: [2][400/3575] Elapsed 1m 49s (remain 14m 28s) Loss: 0.0001(0.0062) Grad: 1304.7491  LR: 0.000017  \n","Epoch: [2][500/3575] Elapsed 2m 16s (remain 13m 57s) Loss: 0.0003(0.0061) Grad: 3065.3440  LR: 0.000017  \n","Epoch: [2][600/3575] Elapsed 2m 43s (remain 13m 28s) Loss: 0.0000(0.0061) Grad: 220.8598  LR: 0.000017  \n","Epoch: [2][700/3575] Elapsed 3m 10s (remain 12m 59s) Loss: 0.0002(0.0060) Grad: 648.2978  LR: 0.000017  \n","Epoch: [2][800/3575] Elapsed 3m 37s (remain 12m 31s) Loss: 0.0368(0.0061) Grad: 110628.3672  LR: 0.000017  \n","Epoch: [2][900/3575] Elapsed 4m 3s (remain 12m 3s) Loss: 0.0001(0.0062) Grad: 914.5062  LR: 0.000017  \n","Epoch: [2][1000/3575] Elapsed 4m 30s (remain 11m 36s) Loss: 0.0005(0.0062) Grad: 1760.4319  LR: 0.000017  \n","Epoch: [2][1100/3575] Elapsed 4m 57s (remain 11m 8s) Loss: 0.0033(0.0063) Grad: 26081.7383  LR: 0.000016  \n","Epoch: [2][1200/3575] Elapsed 5m 24s (remain 10m 41s) Loss: 0.0063(0.0064) Grad: 15317.4023  LR: 0.000016  \n","Epoch: [2][1300/3575] Elapsed 5m 51s (remain 10m 14s) Loss: 0.0001(0.0063) Grad: 583.3552  LR: 0.000016  \n","Epoch: [2][1400/3575] Elapsed 6m 18s (remain 9m 47s) Loss: 0.0156(0.0063) Grad: 47564.9297  LR: 0.000016  \n","Epoch: [2][1500/3575] Elapsed 6m 45s (remain 9m 20s) Loss: 0.0009(0.0062) Grad: 10414.2822  LR: 0.000016  \n","Epoch: [2][1600/3575] Elapsed 7m 12s (remain 8m 53s) Loss: 0.0004(0.0063) Grad: 5663.9375  LR: 0.000016  \n","Epoch: [2][1700/3575] Elapsed 7m 39s (remain 8m 26s) Loss: 0.0035(0.0062) Grad: 20496.8887  LR: 0.000016  \n","Epoch: [2][1800/3575] Elapsed 8m 6s (remain 7m 59s) Loss: 0.0000(0.0062) Grad: 70.2941  LR: 0.000016  \n","Epoch: [2][1900/3575] Elapsed 8m 33s (remain 7m 32s) Loss: 0.0002(0.0064) Grad: 1254.9423  LR: 0.000015  \n","Epoch: [2][2000/3575] Elapsed 9m 0s (remain 7m 5s) Loss: 0.0001(0.0063) Grad: 251.0006  LR: 0.000015  \n","Epoch: [2][2100/3575] Elapsed 9m 27s (remain 6m 38s) Loss: 0.0091(0.0062) Grad: 21598.7539  LR: 0.000015  \n","Epoch: [2][2200/3575] Elapsed 9m 54s (remain 6m 11s) Loss: 0.0007(0.0062) Grad: 1450.5165  LR: 0.000015  \n","Epoch: [2][2300/3575] Elapsed 10m 21s (remain 5m 44s) Loss: 0.0015(0.0063) Grad: 1507.3569  LR: 0.000015  \n","Epoch: [2][2400/3575] Elapsed 10m 48s (remain 5m 17s) Loss: 0.0007(0.0063) Grad: 6855.5322  LR: 0.000015  \n","Epoch: [2][2500/3575] Elapsed 11m 15s (remain 4m 50s) Loss: 0.0160(0.0062) Grad: 20860.5996  LR: 0.000015  \n","Epoch: [2][2600/3575] Elapsed 11m 42s (remain 4m 23s) Loss: 0.0007(0.0063) Grad: 1531.5874  LR: 0.000015  \n","Epoch: [2][2700/3575] Elapsed 12m 9s (remain 3m 56s) Loss: 0.0618(0.0063) Grad: 22760.7949  LR: 0.000014  \n","Epoch: [2][2800/3575] Elapsed 12m 36s (remain 3m 29s) Loss: 0.0009(0.0063) Grad: 2372.0747  LR: 0.000014  \n","Epoch: [2][2900/3575] Elapsed 13m 3s (remain 3m 1s) Loss: 0.0134(0.0062) Grad: 20723.1387  LR: 0.000014  \n","Epoch: [2][3000/3575] Elapsed 13m 30s (remain 2m 34s) Loss: 0.0012(0.0062) Grad: 1407.0726  LR: 0.000014  \n","Epoch: [2][3100/3575] Elapsed 13m 57s (remain 2m 7s) Loss: 0.0052(0.0062) Grad: 4440.0186  LR: 0.000014  \n","Epoch: [2][3200/3575] Elapsed 14m 24s (remain 1m 40s) Loss: 0.0006(0.0061) Grad: 617.7770  LR: 0.000014  \n","Epoch: [2][3300/3575] Elapsed 14m 51s (remain 1m 13s) Loss: 0.0004(0.0061) Grad: 885.1745  LR: 0.000014  \n","Epoch: [2][3400/3575] Elapsed 15m 18s (remain 0m 46s) Loss: 0.0133(0.0061) Grad: 12507.7842  LR: 0.000014  \n","Epoch: [2][3500/3575] Elapsed 15m 45s (remain 0m 19s) Loss: 0.0039(0.0061) Grad: 4119.2085  LR: 0.000013  \n","Epoch: [2][3574/3575] Elapsed 16m 5s (remain 0m 0s) Loss: 0.0001(0.0061) Grad: 119.6155  LR: 0.000013  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 22s) Loss: 0.0001(0.0001) \n","EVAL: [100/1192] Elapsed 0m 14s (remain 2m 39s) Loss: 0.0240(0.0075) \n","EVAL: [200/1192] Elapsed 0m 29s (remain 2m 23s) Loss: 0.0068(0.0066) \n","EVAL: [300/1192] Elapsed 0m 43s (remain 2m 9s) Loss: 0.0201(0.0064) \n","EVAL: [400/1192] Elapsed 0m 58s (remain 1m 54s) Loss: 0.0002(0.0068) \n","EVAL: [500/1192] Elapsed 1m 12s (remain 1m 39s) Loss: 0.0034(0.0063) \n","EVAL: [600/1192] Elapsed 1m 26s (remain 1m 25s) Loss: 0.0068(0.0064) \n","EVAL: [700/1192] Elapsed 1m 41s (remain 1m 10s) Loss: 0.0040(0.0070) \n","EVAL: [800/1192] Elapsed 1m 55s (remain 0m 56s) Loss: 0.0000(0.0070) \n","EVAL: [900/1192] Elapsed 2m 10s (remain 0m 41s) Loss: 0.0024(0.0070) \n","EVAL: [1000/1192] Elapsed 2m 24s (remain 0m 27s) Loss: 0.0046(0.0068) \n","EVAL: [1100/1192] Elapsed 2m 38s (remain 0m 13s) Loss: 0.0315(0.0064) \n","EVAL: [1191/1192] Elapsed 2m 51s (remain 0m 0s) Loss: 0.0001(0.0062) \n","Epoch 2 - avg_train_loss: 0.0061  avg_val_loss: 0.0062  time: 1142s\n","Epoch 2 - Score: 0.8721\n","Epoch 2 - Save Best Score: 0.8721 Model\n","Epoch: [3][0/3575] Elapsed 0m 0s (remain 33m 17s) Loss: 0.0004(0.0004) Grad: 1154.2483  LR: 0.000013  \n","Epoch: [3][100/3575] Elapsed 0m 28s (remain 16m 34s) Loss: 0.0103(0.0044) Grad: 73906.8906  LR: 0.000013  \n","Epoch: [3][200/3575] Elapsed 0m 56s (remain 15m 47s) Loss: 0.0176(0.0036) Grad: 35805.0312  LR: 0.000013  \n","Epoch: [3][300/3575] Elapsed 1m 23s (remain 15m 6s) Loss: 0.0017(0.0041) Grad: 14148.1387  LR: 0.000013  \n","Epoch: [3][400/3575] Elapsed 1m 50s (remain 14m 32s) Loss: 0.0002(0.0043) Grad: 754.3936  LR: 0.000013  \n","Epoch: [3][500/3575] Elapsed 2m 17s (remain 14m 1s) Loss: 0.0109(0.0047) Grad: 9991.9785  LR: 0.000013  \n","Epoch: [3][600/3575] Elapsed 2m 44s (remain 13m 32s) Loss: 0.0005(0.0047) Grad: 2977.4299  LR: 0.000013  \n","Epoch: [3][700/3575] Elapsed 3m 11s (remain 13m 4s) Loss: 0.0108(0.0047) Grad: 8140.2876  LR: 0.000012  \n","Epoch: [3][800/3575] Elapsed 3m 38s (remain 12m 35s) Loss: 0.0000(0.0046) Grad: 52.2556  LR: 0.000012  \n","Epoch: [3][900/3575] Elapsed 4m 5s (remain 12m 7s) Loss: 0.0005(0.0047) Grad: 2760.4897  LR: 0.000012  \n","Epoch: [3][1000/3575] Elapsed 4m 32s (remain 11m 39s) Loss: 0.0000(0.0046) Grad: 48.3646  LR: 0.000012  \n","Epoch: [3][1100/3575] Elapsed 4m 58s (remain 11m 11s) Loss: 0.0010(0.0046) Grad: 1809.1674  LR: 0.000012  \n","Epoch: [3][1200/3575] Elapsed 5m 25s (remain 10m 44s) Loss: 0.0001(0.0045) Grad: 542.2881  LR: 0.000012  \n","Epoch: [3][1300/3575] Elapsed 5m 52s (remain 10m 16s) Loss: 0.0010(0.0045) Grad: 2796.8176  LR: 0.000012  \n","Epoch: [3][1400/3575] Elapsed 6m 19s (remain 9m 49s) Loss: 0.0012(0.0045) Grad: 3473.5681  LR: 0.000012  \n","Epoch: [3][1500/3575] Elapsed 6m 46s (remain 9m 22s) Loss: 0.0017(0.0044) Grad: 2487.9297  LR: 0.000011  \n","Epoch: [3][1600/3575] Elapsed 7m 13s (remain 8m 54s) Loss: 0.0017(0.0045) Grad: 5243.6479  LR: 0.000011  \n","Epoch: [3][1700/3575] Elapsed 7m 40s (remain 8m 27s) Loss: 0.0169(0.0044) Grad: 29884.1406  LR: 0.000011  \n","Epoch: [3][1800/3575] Elapsed 8m 7s (remain 8m 0s) Loss: 0.0001(0.0044) Grad: 147.0398  LR: 0.000011  \n","Epoch: [3][1900/3575] Elapsed 8m 34s (remain 7m 32s) Loss: 0.0101(0.0044) Grad: 4714.1562  LR: 0.000011  \n","Epoch: [3][2000/3575] Elapsed 9m 1s (remain 7m 5s) Loss: 0.0001(0.0044) Grad: 433.8027  LR: 0.000011  \n","Epoch: [3][2100/3575] Elapsed 9m 28s (remain 6m 38s) Loss: 0.0161(0.0044) Grad: 6810.3115  LR: 0.000011  \n","Epoch: [3][2200/3575] Elapsed 9m 55s (remain 6m 11s) Loss: 0.0114(0.0044) Grad: 19587.6738  LR: 0.000011  \n","Epoch: [3][2300/3575] Elapsed 10m 22s (remain 5m 44s) Loss: 0.0002(0.0044) Grad: 1868.8417  LR: 0.000010  \n","Epoch: [3][2400/3575] Elapsed 10m 49s (remain 5m 17s) Loss: 0.0003(0.0044) Grad: 496.5417  LR: 0.000010  \n","Epoch: [3][2500/3575] Elapsed 11m 16s (remain 4m 50s) Loss: 0.0039(0.0044) Grad: 9700.5693  LR: 0.000010  \n","Epoch: [3][2600/3575] Elapsed 11m 43s (remain 4m 23s) Loss: 0.0003(0.0044) Grad: 1363.1821  LR: 0.000010  \n","Epoch: [3][2700/3575] Elapsed 12m 10s (remain 3m 56s) Loss: 0.0002(0.0045) Grad: 344.7240  LR: 0.000010  \n","Epoch: [3][2800/3575] Elapsed 12m 36s (remain 3m 29s) Loss: 0.0094(0.0045) Grad: 4617.4272  LR: 0.000010  \n","Epoch: [3][2900/3575] Elapsed 13m 3s (remain 3m 2s) Loss: 0.0047(0.0045) Grad: 3550.8164  LR: 0.000010  \n","Epoch: [3][3000/3575] Elapsed 13m 30s (remain 2m 35s) Loss: 0.0000(0.0045) Grad: 6.5272  LR: 0.000010  \n","Epoch: [3][3100/3575] Elapsed 13m 57s (remain 2m 8s) Loss: 0.0056(0.0045) Grad: 3814.7170  LR: 0.000009  \n","Epoch: [3][3200/3575] Elapsed 14m 24s (remain 1m 41s) Loss: 0.0322(0.0045) Grad: 16518.7383  LR: 0.000009  \n","Epoch: [3][3300/3575] Elapsed 14m 51s (remain 1m 14s) Loss: 0.0001(0.0045) Grad: 133.5614  LR: 0.000009  \n","Epoch: [3][3400/3575] Elapsed 15m 18s (remain 0m 47s) Loss: 0.0002(0.0045) Grad: 1886.2723  LR: 0.000009  \n","Epoch: [3][3500/3575] Elapsed 15m 45s (remain 0m 19s) Loss: 0.0006(0.0045) Grad: 1499.0874  LR: 0.000009  \n","Epoch: [3][3574/3575] Elapsed 16m 5s (remain 0m 0s) Loss: 0.0023(0.0045) Grad: 3186.3521  LR: 0.000009  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 16s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 14s (remain 2m 39s) Loss: 0.0269(0.0066) \n","EVAL: [200/1192] Elapsed 0m 29s (remain 2m 23s) Loss: 0.0063(0.0059) \n","EVAL: [300/1192] Elapsed 0m 43s (remain 2m 8s) Loss: 0.0154(0.0057) \n","EVAL: [400/1192] Elapsed 0m 57s (remain 1m 54s) Loss: 0.0003(0.0060) \n","EVAL: [500/1192] Elapsed 1m 12s (remain 1m 39s) Loss: 0.0061(0.0055) \n","EVAL: [600/1192] Elapsed 1m 26s (remain 1m 25s) Loss: 0.0027(0.0057) \n","EVAL: [700/1192] Elapsed 1m 41s (remain 1m 10s) Loss: 0.0052(0.0063) \n","EVAL: [800/1192] Elapsed 1m 55s (remain 0m 56s) Loss: 0.0000(0.0063) \n","EVAL: [900/1192] Elapsed 2m 10s (remain 0m 41s) Loss: 0.0084(0.0064) \n","EVAL: [1000/1192] Elapsed 2m 24s (remain 0m 27s) Loss: 0.0003(0.0063) \n","EVAL: [1100/1192] Elapsed 2m 38s (remain 0m 13s) Loss: 0.0341(0.0059) \n","EVAL: [1191/1192] Elapsed 2m 51s (remain 0m 0s) Loss: 0.0001(0.0057) \n","Epoch 3 - avg_train_loss: 0.0045  avg_val_loss: 0.0057  time: 1143s\n","Epoch 3 - Score: 0.8826\n","Epoch 3 - Save Best Score: 0.8826 Model\n","Epoch: [4][0/3575] Elapsed 0m 0s (remain 33m 16s) Loss: 0.0013(0.0013) Grad: 6373.6025  LR: 0.000009  \n","Epoch: [4][100/3575] Elapsed 0m 28s (remain 16m 35s) Loss: 0.0000(0.0027) Grad: 68.5050  LR: 0.000009  \n","Epoch: [4][200/3575] Elapsed 0m 56s (remain 15m 50s) Loss: 0.0032(0.0028) Grad: 9953.3340  LR: 0.000009  \n","Epoch: [4][300/3575] Elapsed 1m 23s (remain 15m 8s) Loss: 0.0001(0.0029) Grad: 411.3742  LR: 0.000009  \n","Epoch: [4][400/3575] Elapsed 1m 50s (remain 14m 34s) Loss: 0.0197(0.0028) Grad: 40532.0234  LR: 0.000008  \n","Epoch: [4][500/3575] Elapsed 2m 17s (remain 14m 3s) Loss: 0.0060(0.0031) Grad: 59551.8359  LR: 0.000008  \n","Epoch: [4][600/3575] Elapsed 2m 44s (remain 13m 33s) Loss: 0.0004(0.0029) Grad: 2068.2903  LR: 0.000008  \n","Epoch: [4][700/3575] Elapsed 3m 11s (remain 13m 4s) Loss: 0.0054(0.0030) Grad: 9297.9219  LR: 0.000008  \n","Epoch: [4][800/3575] Elapsed 3m 38s (remain 12m 35s) Loss: 0.0035(0.0031) Grad: 6387.9512  LR: 0.000008  \n","Epoch: [4][900/3575] Elapsed 4m 5s (remain 12m 7s) Loss: 0.0000(0.0031) Grad: 7.1424  LR: 0.000008  \n","Epoch: [4][1000/3575] Elapsed 4m 32s (remain 11m 39s) Loss: 0.0000(0.0031) Grad: 95.0671  LR: 0.000008  \n","Epoch: [4][1100/3575] Elapsed 4m 59s (remain 11m 11s) Loss: 0.0001(0.0032) Grad: 1824.0508  LR: 0.000008  \n","Epoch: [4][1200/3575] Elapsed 5m 26s (remain 10m 44s) Loss: 0.0112(0.0032) Grad: 34593.5703  LR: 0.000007  \n","Epoch: [4][1300/3575] Elapsed 5m 53s (remain 10m 17s) Loss: 0.0001(0.0033) Grad: 490.7735  LR: 0.000007  \n","Epoch: [4][1400/3575] Elapsed 6m 20s (remain 9m 49s) Loss: 0.0008(0.0033) Grad: 5747.9146  LR: 0.000007  \n","Epoch: [4][1500/3575] Elapsed 6m 46s (remain 9m 22s) Loss: 0.0037(0.0033) Grad: 26496.4102  LR: 0.000007  \n","Epoch: [4][1600/3575] Elapsed 7m 13s (remain 8m 55s) Loss: 0.0001(0.0032) Grad: 431.0027  LR: 0.000007  \n","Epoch: [4][1700/3575] Elapsed 7m 40s (remain 8m 27s) Loss: 0.0000(0.0034) Grad: 4.1613  LR: 0.000007  \n","Epoch: [4][1800/3575] Elapsed 8m 7s (remain 8m 0s) Loss: 0.0259(0.0033) Grad: 37253.2227  LR: 0.000007  \n","Epoch: [4][1900/3575] Elapsed 8m 34s (remain 7m 33s) Loss: 0.0034(0.0034) Grad: 12128.8730  LR: 0.000007  \n","Epoch: [4][2000/3575] Elapsed 9m 1s (remain 7m 6s) Loss: 0.0058(0.0034) Grad: 4769.5771  LR: 0.000006  \n","Epoch: [4][2100/3575] Elapsed 9m 28s (remain 6m 38s) Loss: 0.0000(0.0035) Grad: 105.8935  LR: 0.000006  \n","Epoch: [4][2200/3575] Elapsed 9m 55s (remain 6m 11s) Loss: 0.0012(0.0035) Grad: 4436.4243  LR: 0.000006  \n","Epoch: [4][2300/3575] Elapsed 10m 22s (remain 5m 44s) Loss: 0.0047(0.0035) Grad: 26008.6094  LR: 0.000006  \n","Epoch: [4][2400/3575] Elapsed 10m 49s (remain 5m 17s) Loss: 0.0000(0.0035) Grad: 227.2506  LR: 0.000006  \n","Epoch: [4][2500/3575] Elapsed 11m 16s (remain 4m 50s) Loss: 0.0001(0.0035) Grad: 520.4277  LR: 0.000006  \n","Epoch: [4][2600/3575] Elapsed 11m 43s (remain 4m 23s) Loss: 0.0000(0.0035) Grad: 3.8508  LR: 0.000006  \n","Epoch: [4][2700/3575] Elapsed 12m 10s (remain 3m 56s) Loss: 0.0001(0.0035) Grad: 1192.9390  LR: 0.000006  \n","Epoch: [4][2800/3575] Elapsed 12m 37s (remain 3m 29s) Loss: 0.0227(0.0035) Grad: 42889.1602  LR: 0.000005  \n","Epoch: [4][2900/3575] Elapsed 13m 4s (remain 3m 2s) Loss: 0.0039(0.0035) Grad: 5910.5664  LR: 0.000005  \n","Epoch: [4][3000/3575] Elapsed 13m 31s (remain 2m 35s) Loss: 0.0012(0.0035) Grad: 7793.1489  LR: 0.000005  \n","Epoch: [4][3100/3575] Elapsed 13m 57s (remain 2m 8s) Loss: 0.0002(0.0036) Grad: 2448.0652  LR: 0.000005  \n","Epoch: [4][3200/3575] Elapsed 14m 24s (remain 1m 41s) Loss: 0.0001(0.0035) Grad: 387.9955  LR: 0.000005  \n","Epoch: [4][3300/3575] Elapsed 14m 51s (remain 1m 14s) Loss: 0.0001(0.0035) Grad: 1071.7854  LR: 0.000005  \n","Epoch: [4][3400/3575] Elapsed 15m 18s (remain 0m 46s) Loss: 0.0049(0.0035) Grad: 21437.4180  LR: 0.000005  \n","Epoch: [4][3500/3575] Elapsed 15m 45s (remain 0m 19s) Loss: 0.0056(0.0035) Grad: 34608.6367  LR: 0.000005  \n","Epoch: [4][3574/3575] Elapsed 16m 5s (remain 0m 0s) Loss: 0.0126(0.0035) Grad: 109078.9219  LR: 0.000004  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 7m 42s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 14s (remain 2m 39s) Loss: 0.0385(0.0084) \n","EVAL: [200/1192] Elapsed 0m 29s (remain 2m 23s) Loss: 0.0118(0.0077) \n","EVAL: [300/1192] Elapsed 0m 43s (remain 2m 8s) Loss: 0.0187(0.0073) \n","EVAL: [400/1192] Elapsed 0m 57s (remain 1m 53s) Loss: 0.0001(0.0076) \n","EVAL: [500/1192] Elapsed 1m 12s (remain 1m 39s) Loss: 0.0000(0.0070) \n","EVAL: [600/1192] Elapsed 1m 26s (remain 1m 25s) Loss: 0.0060(0.0070) \n","EVAL: [700/1192] Elapsed 1m 40s (remain 1m 10s) Loss: 0.0048(0.0078) \n","EVAL: [800/1192] Elapsed 1m 55s (remain 0m 56s) Loss: 0.0000(0.0080) \n","EVAL: [900/1192] Elapsed 2m 9s (remain 0m 41s) Loss: 0.0116(0.0081) \n","EVAL: [1000/1192] Elapsed 2m 23s (remain 0m 27s) Loss: 0.0001(0.0079) \n","EVAL: [1100/1192] Elapsed 2m 38s (remain 0m 13s) Loss: 0.0528(0.0075) \n","EVAL: [1191/1192] Elapsed 2m 51s (remain 0m 0s) Loss: 0.0000(0.0073) \n","Epoch 4 - avg_train_loss: 0.0035  avg_val_loss: 0.0073  time: 1141s\n","Epoch 4 - Score: 0.8840\n","Epoch 4 - Save Best Score: 0.8840 Model\n","Epoch: [5][0/3575] Elapsed 0m 0s (remain 34m 40s) Loss: 0.0096(0.0096) Grad: 19867.7871  LR: 0.000004  \n","Epoch: [5][100/3575] Elapsed 0m 29s (remain 16m 45s) Loss: 0.0023(0.0027) Grad: 10520.9160  LR: 0.000004  \n","Epoch: [5][200/3575] Elapsed 0m 56s (remain 15m 55s) Loss: 0.0020(0.0031) Grad: 4179.1353  LR: 0.000004  \n","Epoch: [5][300/3575] Elapsed 1m 23s (remain 15m 10s) Loss: 0.0095(0.0028) Grad: 58174.4102  LR: 0.000004  \n","Epoch: [5][400/3575] Elapsed 1m 50s (remain 14m 35s) Loss: 0.0000(0.0027) Grad: 203.1424  LR: 0.000004  \n","Epoch: [5][500/3575] Elapsed 2m 17s (remain 14m 2s) Loss: 0.0000(0.0029) Grad: 1.8809  LR: 0.000004  \n","Epoch: [5][600/3575] Elapsed 2m 44s (remain 13m 33s) Loss: 0.0009(0.0028) Grad: 6549.6709  LR: 0.000004  \n","Epoch: [5][700/3575] Elapsed 3m 11s (remain 13m 3s) Loss: 0.0001(0.0029) Grad: 272.4809  LR: 0.000004  \n","Epoch: [5][800/3575] Elapsed 3m 38s (remain 12m 35s) Loss: 0.0000(0.0028) Grad: 4.7317  LR: 0.000003  \n","Epoch: [5][900/3575] Elapsed 4m 4s (remain 12m 6s) Loss: 0.0013(0.0027) Grad: 7464.1094  LR: 0.000003  \n","Epoch: [5][1000/3575] Elapsed 4m 31s (remain 11m 38s) Loss: 0.0000(0.0029) Grad: 2.0877  LR: 0.000003  \n","Epoch: [5][1100/3575] Elapsed 4m 58s (remain 11m 10s) Loss: 0.0000(0.0029) Grad: 29.1919  LR: 0.000003  \n","Epoch: [5][1200/3575] Elapsed 5m 25s (remain 10m 43s) Loss: 0.0000(0.0029) Grad: 42.4545  LR: 0.000003  \n","Epoch: [5][1300/3575] Elapsed 5m 52s (remain 10m 15s) Loss: 0.0037(0.0030) Grad: 18306.3379  LR: 0.000003  \n","Epoch: [5][1400/3575] Elapsed 6m 19s (remain 9m 48s) Loss: 0.0021(0.0029) Grad: 53928.3516  LR: 0.000003  \n","Epoch: [5][1500/3575] Elapsed 6m 45s (remain 9m 20s) Loss: 0.0000(0.0029) Grad: 2.7998  LR: 0.000003  \n","Epoch: [5][1600/3575] Elapsed 7m 12s (remain 8m 53s) Loss: 0.0000(0.0029) Grad: 1.7664  LR: 0.000002  \n","Epoch: [5][1700/3575] Elapsed 7m 39s (remain 8m 26s) Loss: 0.0000(0.0029) Grad: 29.0007  LR: 0.000002  \n","Epoch: [5][1800/3575] Elapsed 8m 6s (remain 7m 59s) Loss: 0.0095(0.0029) Grad: 63700.1484  LR: 0.000002  \n","Epoch: [5][1900/3575] Elapsed 8m 33s (remain 7m 32s) Loss: 0.0000(0.0028) Grad: 97.4156  LR: 0.000002  \n","Epoch: [5][2000/3575] Elapsed 9m 0s (remain 7m 4s) Loss: 0.0002(0.0028) Grad: 1911.0490  LR: 0.000002  \n","Epoch: [5][2100/3575] Elapsed 9m 27s (remain 6m 37s) Loss: 0.0053(0.0028) Grad: 30471.6152  LR: 0.000002  \n","Epoch: [5][2200/3575] Elapsed 9m 53s (remain 6m 10s) Loss: 0.0004(0.0028) Grad: 3690.1233  LR: 0.000002  \n","Epoch: [5][2300/3575] Elapsed 10m 20s (remain 5m 43s) Loss: 0.0000(0.0028) Grad: 108.7936  LR: 0.000002  \n","Epoch: [5][2400/3575] Elapsed 10m 47s (remain 5m 16s) Loss: 0.0000(0.0028) Grad: 9.2945  LR: 0.000001  \n","Epoch: [5][2500/3575] Elapsed 11m 14s (remain 4m 49s) Loss: 0.0000(0.0028) Grad: 73.7036  LR: 0.000001  \n","Epoch: [5][2600/3575] Elapsed 11m 41s (remain 4m 22s) Loss: 0.0000(0.0028) Grad: 40.5673  LR: 0.000001  \n","Epoch: [5][2700/3575] Elapsed 12m 8s (remain 3m 55s) Loss: 0.0001(0.0028) Grad: 1348.4127  LR: 0.000001  \n","Epoch: [5][2800/3575] Elapsed 12m 34s (remain 3m 28s) Loss: 0.0045(0.0028) Grad: 12643.1885  LR: 0.000001  \n","Epoch: [5][2900/3575] Elapsed 13m 1s (remain 3m 1s) Loss: 0.0000(0.0028) Grad: 9.3543  LR: 0.000001  \n","Epoch: [5][3000/3575] Elapsed 13m 28s (remain 2m 34s) Loss: 0.0000(0.0028) Grad: 9.1570  LR: 0.000001  \n","Epoch: [5][3100/3575] Elapsed 13m 55s (remain 2m 7s) Loss: 0.0005(0.0028) Grad: 11531.1494  LR: 0.000001  \n","Epoch: [5][3200/3575] Elapsed 14m 22s (remain 1m 40s) Loss: 0.0000(0.0028) Grad: 9.2180  LR: 0.000000  \n","Epoch: [5][3300/3575] Elapsed 14m 49s (remain 1m 13s) Loss: 0.0021(0.0028) Grad: 9808.9600  LR: 0.000000  \n","Epoch: [5][3400/3575] Elapsed 15m 15s (remain 0m 46s) Loss: 0.0042(0.0029) Grad: 23367.3066  LR: 0.000000  \n","Epoch: [5][3500/3575] Elapsed 15m 42s (remain 0m 19s) Loss: 0.0000(0.0028) Grad: 4.4610  LR: 0.000000  \n","Epoch: [5][3574/3575] Elapsed 16m 2s (remain 0m 0s) Loss: 0.0000(0.0028) Grad: 6.1990  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 10s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 14s (remain 2m 39s) Loss: 0.0382(0.0092) \n","EVAL: [200/1192] Elapsed 0m 29s (remain 2m 23s) Loss: 0.0086(0.0080) \n","EVAL: [300/1192] Elapsed 0m 43s (remain 2m 8s) Loss: 0.0116(0.0078) \n","EVAL: [400/1192] Elapsed 0m 57s (remain 1m 53s) Loss: 0.0001(0.0081) \n","EVAL: [500/1192] Elapsed 1m 12s (remain 1m 39s) Loss: 0.0000(0.0074) \n","EVAL: [600/1192] Elapsed 1m 26s (remain 1m 25s) Loss: 0.0098(0.0075) \n","EVAL: [700/1192] Elapsed 1m 40s (remain 1m 10s) Loss: 0.0052(0.0083) \n","EVAL: [800/1192] Elapsed 1m 55s (remain 0m 56s) Loss: 0.0000(0.0085) \n","EVAL: [900/1192] Elapsed 2m 9s (remain 0m 41s) Loss: 0.0124(0.0086) \n","EVAL: [1000/1192] Elapsed 2m 23s (remain 0m 27s) Loss: 0.0001(0.0084) \n","EVAL: [1100/1192] Elapsed 2m 38s (remain 0m 13s) Loss: 0.0551(0.0080) \n","EVAL: [1191/1192] Elapsed 2m 51s (remain 0m 0s) Loss: 0.0000(0.0078) \n","Epoch 5 - avg_train_loss: 0.0028  avg_val_loss: 0.0078  time: 1139s\n","Epoch 5 - Score: 0.8871\n","Epoch 5 - Save Best Score: 0.8871 Model\n","========== fold: 3 training ==========\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n","Epoch: [1][0/3575] Elapsed 0m 0s (remain 43m 4s) Loss: 0.4372(0.4372) Grad: 240411.2812  LR: 0.000000  \n","Epoch: [1][100/3575] Elapsed 0m 29s (remain 16m 40s) Loss: 0.3449(0.4162) Grad: 24960.4102  LR: 0.000001  \n","Epoch: [1][200/3575] Elapsed 0m 55s (remain 15m 39s) Loss: 0.1372(0.3281) Grad: 11165.5635  LR: 0.000002  \n","Epoch: [1][300/3575] Elapsed 1m 22s (remain 15m 0s) Loss: 0.0263(0.2397) Grad: 959.0134  LR: 0.000003  \n","Epoch: [1][400/3575] Elapsed 1m 49s (remain 14m 28s) Loss: 0.0205(0.1895) Grad: 1088.8104  LR: 0.000004  \n","Epoch: [1][500/3575] Elapsed 2m 16s (remain 13m 57s) Loss: 0.0574(0.1589) Grad: 2462.3083  LR: 0.000006  \n","Epoch: [1][600/3575] Elapsed 2m 43s (remain 13m 28s) Loss: 0.0123(0.1378) Grad: 804.1732  LR: 0.000007  \n","Epoch: [1][700/3575] Elapsed 3m 10s (remain 12m 59s) Loss: 0.0037(0.1212) Grad: 395.7418  LR: 0.000008  \n","Epoch: [1][800/3575] Elapsed 3m 37s (remain 12m 31s) Loss: 0.0060(0.1081) Grad: 4147.4712  LR: 0.000009  \n","Epoch: [1][900/3575] Elapsed 4m 3s (remain 12m 3s) Loss: 0.0024(0.0975) Grad: 955.8153  LR: 0.000010  \n","Epoch: [1][1000/3575] Elapsed 4m 30s (remain 11m 36s) Loss: 0.0010(0.0889) Grad: 758.3273  LR: 0.000011  \n","Epoch: [1][1100/3575] Elapsed 4m 57s (remain 11m 8s) Loss: 0.0090(0.0822) Grad: 3217.0569  LR: 0.000012  \n","Epoch: [1][1200/3575] Elapsed 5m 24s (remain 10m 41s) Loss: 0.0017(0.0763) Grad: 974.5384  LR: 0.000013  \n","Epoch: [1][1300/3575] Elapsed 5m 51s (remain 10m 14s) Loss: 0.0019(0.0713) Grad: 830.9260  LR: 0.000015  \n","Epoch: [1][1400/3575] Elapsed 6m 18s (remain 9m 47s) Loss: 0.0027(0.0668) Grad: 1601.4198  LR: 0.000016  \n","Epoch: [1][1500/3575] Elapsed 6m 45s (remain 9m 19s) Loss: 0.0431(0.0630) Grad: 19741.2422  LR: 0.000017  \n","Epoch: [1][1600/3575] Elapsed 7m 12s (remain 8m 52s) Loss: 0.0619(0.0598) Grad: 7080.0410  LR: 0.000018  \n","Epoch: [1][1700/3575] Elapsed 7m 38s (remain 8m 25s) Loss: 0.0483(0.0568) Grad: 11957.5791  LR: 0.000019  \n","Epoch: [1][1800/3575] Elapsed 8m 5s (remain 7m 58s) Loss: 0.0170(0.0542) Grad: 6036.7134  LR: 0.000020  \n","Epoch: [1][1900/3575] Elapsed 8m 32s (remain 7m 31s) Loss: 0.0022(0.0518) Grad: 978.5292  LR: 0.000020  \n","Epoch: [1][2000/3575] Elapsed 8m 59s (remain 7m 4s) Loss: 0.0016(0.0496) Grad: 1503.0281  LR: 0.000020  \n","Epoch: [1][2100/3575] Elapsed 9m 26s (remain 6m 37s) Loss: 0.0001(0.0476) Grad: 84.6899  LR: 0.000020  \n","Epoch: [1][2200/3575] Elapsed 9m 53s (remain 6m 10s) Loss: 0.0001(0.0459) Grad: 225.5431  LR: 0.000019  \n","Epoch: [1][2300/3575] Elapsed 10m 19s (remain 5m 43s) Loss: 0.0049(0.0442) Grad: 2031.1565  LR: 0.000019  \n","Epoch: [1][2400/3575] Elapsed 10m 46s (remain 5m 16s) Loss: 0.0026(0.0427) Grad: 675.2778  LR: 0.000019  \n","Epoch: [1][2500/3575] Elapsed 11m 13s (remain 4m 49s) Loss: 0.0014(0.0412) Grad: 870.1519  LR: 0.000019  \n","Epoch: [1][2600/3575] Elapsed 11m 40s (remain 4m 22s) Loss: 0.0091(0.0400) Grad: 1028.7410  LR: 0.000019  \n","Epoch: [1][2700/3575] Elapsed 12m 7s (remain 3m 55s) Loss: 0.0169(0.0388) Grad: 5727.8564  LR: 0.000019  \n","Epoch: [1][2800/3575] Elapsed 12m 34s (remain 3m 28s) Loss: 0.0138(0.0377) Grad: 1489.4648  LR: 0.000019  \n","Epoch: [1][2900/3575] Elapsed 13m 1s (remain 3m 1s) Loss: 0.0021(0.0367) Grad: 1269.7317  LR: 0.000019  \n","Epoch: [1][3000/3575] Elapsed 13m 28s (remain 2m 34s) Loss: 0.0244(0.0357) Grad: 4594.5928  LR: 0.000018  \n","Epoch: [1][3100/3575] Elapsed 13m 54s (remain 2m 7s) Loss: 0.0048(0.0348) Grad: 2736.7275  LR: 0.000018  \n","Epoch: [1][3200/3575] Elapsed 14m 21s (remain 1m 40s) Loss: 0.0000(0.0339) Grad: 12.6549  LR: 0.000018  \n","Epoch: [1][3300/3575] Elapsed 14m 48s (remain 1m 13s) Loss: 0.0018(0.0331) Grad: 3667.1589  LR: 0.000018  \n","Epoch: [1][3400/3575] Elapsed 15m 15s (remain 0m 46s) Loss: 0.0032(0.0323) Grad: 2096.2876  LR: 0.000018  \n","Epoch: [1][3500/3575] Elapsed 15m 42s (remain 0m 19s) Loss: 0.0013(0.0316) Grad: 547.9977  LR: 0.000018  \n","Epoch: [1][3574/3575] Elapsed 16m 2s (remain 0m 0s) Loss: 0.0031(0.0311) Grad: 751.6175  LR: 0.000018  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 9m 1s) Loss: 0.0014(0.0014) \n","EVAL: [100/1192] Elapsed 0m 14s (remain 2m 39s) Loss: 0.0246(0.0046) \n","EVAL: [200/1192] Elapsed 0m 29s (remain 2m 23s) Loss: 0.0081(0.0047) \n","EVAL: [300/1192] Elapsed 0m 43s (remain 2m 8s) Loss: 0.0048(0.0052) \n","EVAL: [400/1192] Elapsed 0m 57s (remain 1m 54s) Loss: 0.0001(0.0049) \n","EVAL: [500/1192] Elapsed 1m 12s (remain 1m 39s) Loss: 0.0337(0.0047) \n","EVAL: [600/1192] Elapsed 1m 26s (remain 1m 25s) Loss: 0.0054(0.0050) \n","EVAL: [700/1192] Elapsed 1m 40s (remain 1m 10s) Loss: 0.0047(0.0055) \n","EVAL: [800/1192] Elapsed 1m 55s (remain 0m 56s) Loss: 0.0132(0.0055) \n","EVAL: [900/1192] Elapsed 2m 9s (remain 0m 41s) Loss: 0.0034(0.0061) \n","EVAL: [1000/1192] Elapsed 2m 23s (remain 0m 27s) Loss: 0.0001(0.0060) \n","EVAL: [1100/1192] Elapsed 2m 38s (remain 0m 13s) Loss: 0.0070(0.0058) \n","EVAL: [1191/1192] Elapsed 2m 51s (remain 0m 0s) Loss: 0.0004(0.0056) \n","Epoch 1 - avg_train_loss: 0.0311  avg_val_loss: 0.0056  time: 1139s\n","Epoch 1 - Score: 0.8593\n","Epoch 1 - Save Best Score: 0.8593 Model\n","Epoch: [2][0/3575] Elapsed 0m 0s (remain 38m 25s) Loss: 0.0105(0.0105) Grad: 14981.8467  LR: 0.000018  \n","Epoch: [2][100/3575] Elapsed 0m 28s (remain 16m 30s) Loss: 0.0241(0.0076) Grad: 54124.9102  LR: 0.000018  \n","Epoch: [2][200/3575] Elapsed 0m 56s (remain 15m 44s) Loss: 0.0008(0.0067) Grad: 5986.9326  LR: 0.000018  \n","Epoch: [2][300/3575] Elapsed 1m 23s (remain 15m 6s) Loss: 0.0241(0.0065) Grad: 146729.5156  LR: 0.000017  \n","Epoch: [2][400/3575] Elapsed 1m 50s (remain 14m 32s) Loss: 0.0194(0.0062) Grad: 17203.8145  LR: 0.000017  \n","Epoch: [2][500/3575] Elapsed 2m 17s (remain 14m 1s) Loss: 0.0008(0.0060) Grad: 1849.8508  LR: 0.000017  \n","Epoch: [2][600/3575] Elapsed 2m 43s (remain 13m 31s) Loss: 0.0057(0.0058) Grad: 5076.1592  LR: 0.000017  \n","Epoch: [2][700/3575] Elapsed 3m 10s (remain 13m 2s) Loss: 0.0059(0.0056) Grad: 6599.5024  LR: 0.000017  \n","Epoch: [2][800/3575] Elapsed 3m 37s (remain 12m 33s) Loss: 0.0106(0.0057) Grad: 11778.2236  LR: 0.000017  \n","Epoch: [2][900/3575] Elapsed 4m 4s (remain 12m 5s) Loss: 0.0000(0.0057) Grad: 75.4011  LR: 0.000017  \n","Epoch: [2][1000/3575] Elapsed 4m 31s (remain 11m 38s) Loss: 0.0013(0.0056) Grad: 1432.2869  LR: 0.000017  \n","Epoch: [2][1100/3575] Elapsed 4m 58s (remain 11m 10s) Loss: 0.0001(0.0056) Grad: 122.3992  LR: 0.000016  \n","Epoch: [2][1200/3575] Elapsed 5m 25s (remain 10m 43s) Loss: 0.0094(0.0055) Grad: 6164.9492  LR: 0.000016  \n","Epoch: [2][1300/3575] Elapsed 5m 52s (remain 10m 15s) Loss: 0.0020(0.0055) Grad: 2826.2805  LR: 0.000016  \n","Epoch: [2][1400/3575] Elapsed 6m 19s (remain 9m 48s) Loss: 0.0003(0.0055) Grad: 527.9438  LR: 0.000016  \n","Epoch: [2][1500/3575] Elapsed 6m 46s (remain 9m 20s) Loss: 0.0010(0.0054) Grad: 7300.4512  LR: 0.000016  \n","Epoch: [2][1600/3575] Elapsed 7m 12s (remain 8m 53s) Loss: 0.0223(0.0054) Grad: 13586.7803  LR: 0.000016  \n","Epoch: [2][1700/3575] Elapsed 7m 39s (remain 8m 26s) Loss: 0.0014(0.0054) Grad: 2370.8079  LR: 0.000016  \n","Epoch: [2][1800/3575] Elapsed 8m 6s (remain 7m 59s) Loss: 0.0001(0.0054) Grad: 520.8427  LR: 0.000016  \n","Epoch: [2][1900/3575] Elapsed 8m 33s (remain 7m 32s) Loss: 0.0000(0.0053) Grad: 32.5677  LR: 0.000015  \n","Epoch: [2][2000/3575] Elapsed 9m 0s (remain 7m 5s) Loss: 0.0084(0.0054) Grad: 4006.5217  LR: 0.000015  \n","Epoch: [2][2100/3575] Elapsed 9m 27s (remain 6m 38s) Loss: 0.0001(0.0053) Grad: 562.8063  LR: 0.000015  \n","Epoch: [2][2200/3575] Elapsed 9m 54s (remain 6m 10s) Loss: 0.0006(0.0053) Grad: 1088.3799  LR: 0.000015  \n","Epoch: [2][2300/3575] Elapsed 10m 21s (remain 5m 43s) Loss: 0.0024(0.0053) Grad: 7856.0425  LR: 0.000015  \n","Epoch: [2][2400/3575] Elapsed 10m 47s (remain 5m 16s) Loss: 0.0000(0.0053) Grad: 27.3642  LR: 0.000015  \n","Epoch: [2][2500/3575] Elapsed 11m 14s (remain 4m 49s) Loss: 0.0540(0.0054) Grad: 35982.9297  LR: 0.000015  \n","Epoch: [2][2600/3575] Elapsed 11m 41s (remain 4m 22s) Loss: 0.0007(0.0053) Grad: 1762.3916  LR: 0.000015  \n","Epoch: [2][2700/3575] Elapsed 12m 8s (remain 3m 55s) Loss: 0.0033(0.0054) Grad: 4452.9424  LR: 0.000014  \n","Epoch: [2][2800/3575] Elapsed 12m 35s (remain 3m 28s) Loss: 0.0324(0.0055) Grad: 38153.5273  LR: 0.000014  \n","Epoch: [2][2900/3575] Elapsed 13m 2s (remain 3m 1s) Loss: 0.0580(0.0055) Grad: 24676.8379  LR: 0.000014  \n","Epoch: [2][3000/3575] Elapsed 13m 29s (remain 2m 34s) Loss: 0.0059(0.0055) Grad: 4697.6333  LR: 0.000014  \n","Epoch: [2][3100/3575] Elapsed 13m 56s (remain 2m 7s) Loss: 0.0040(0.0055) Grad: 5477.1479  LR: 0.000014  \n","Epoch: [2][3200/3575] Elapsed 14m 23s (remain 1m 40s) Loss: 0.0052(0.0055) Grad: 5210.2793  LR: 0.000014  \n","Epoch: [2][3300/3575] Elapsed 14m 49s (remain 1m 13s) Loss: 0.0014(0.0054) Grad: 14255.1113  LR: 0.000014  \n","Epoch: [2][3400/3575] Elapsed 15m 16s (remain 0m 46s) Loss: 0.0000(0.0054) Grad: 40.9702  LR: 0.000014  \n","Epoch: [2][3500/3575] Elapsed 15m 43s (remain 0m 19s) Loss: 0.0028(0.0054) Grad: 6392.0757  LR: 0.000013  \n","Epoch: [2][3574/3575] Elapsed 16m 3s (remain 0m 0s) Loss: 0.0006(0.0054) Grad: 1704.9314  LR: 0.000013  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 25s) Loss: 0.0005(0.0005) \n","EVAL: [100/1192] Elapsed 0m 14s (remain 2m 39s) Loss: 0.0260(0.0041) \n","EVAL: [200/1192] Elapsed 0m 29s (remain 2m 23s) Loss: 0.0071(0.0042) \n","EVAL: [300/1192] Elapsed 0m 43s (remain 2m 8s) Loss: 0.0037(0.0049) \n","EVAL: [400/1192] Elapsed 0m 57s (remain 1m 53s) Loss: 0.0000(0.0046) \n","EVAL: [500/1192] Elapsed 1m 12s (remain 1m 39s) Loss: 0.0381(0.0044) \n","EVAL: [600/1192] Elapsed 1m 26s (remain 1m 25s) Loss: 0.0043(0.0047) \n","EVAL: [700/1192] Elapsed 1m 40s (remain 1m 10s) Loss: 0.0057(0.0053) \n","EVAL: [800/1192] Elapsed 1m 55s (remain 0m 56s) Loss: 0.0073(0.0054) \n","EVAL: [900/1192] Elapsed 2m 9s (remain 0m 41s) Loss: 0.0052(0.0055) \n","EVAL: [1000/1192] Elapsed 2m 23s (remain 0m 27s) Loss: 0.0013(0.0053) \n","EVAL: [1100/1192] Elapsed 2m 38s (remain 0m 13s) Loss: 0.0148(0.0052) \n","EVAL: [1191/1192] Elapsed 2m 51s (remain 0m 0s) Loss: 0.0000(0.0051) \n","Epoch 2 - avg_train_loss: 0.0054  avg_val_loss: 0.0051  time: 1140s\n","Epoch 2 - Score: 0.8800\n","Epoch 2 - Save Best Score: 0.8800 Model\n","Epoch: [3][0/3575] Elapsed 0m 0s (remain 34m 10s) Loss: 0.0008(0.0008) Grad: 3571.9253  LR: 0.000013  \n","Epoch: [3][100/3575] Elapsed 0m 28s (remain 16m 37s) Loss: 0.0005(0.0038) Grad: 2495.3110  LR: 0.000013  \n","Epoch: [3][200/3575] Elapsed 0m 56s (remain 15m 46s) Loss: 0.0007(0.0033) Grad: 6113.1875  LR: 0.000013  \n","Epoch: [3][300/3575] Elapsed 1m 23s (remain 15m 5s) Loss: 0.0274(0.0037) Grad: 23393.4668  LR: 0.000013  \n","Epoch: [3][400/3575] Elapsed 1m 50s (remain 14m 31s) Loss: 0.0058(0.0037) Grad: 31638.3398  LR: 0.000013  \n","Epoch: [3][500/3575] Elapsed 2m 16s (remain 14m 0s) Loss: 0.0033(0.0035) Grad: 28642.1348  LR: 0.000013  \n","Epoch: [3][600/3575] Elapsed 2m 43s (remain 13m 30s) Loss: 0.0000(0.0037) Grad: 15.1205  LR: 0.000013  \n","Epoch: [3][700/3575] Elapsed 3m 10s (remain 13m 1s) Loss: 0.0001(0.0037) Grad: 396.7585  LR: 0.000012  \n","Epoch: [3][800/3575] Elapsed 3m 37s (remain 12m 33s) Loss: 0.0016(0.0036) Grad: 7458.2939  LR: 0.000012  \n","Epoch: [3][900/3575] Elapsed 4m 4s (remain 12m 5s) Loss: 0.0009(0.0038) Grad: 5225.1260  LR: 0.000012  \n","Epoch: [3][1000/3575] Elapsed 4m 31s (remain 11m 37s) Loss: 0.0016(0.0037) Grad: 7464.7852  LR: 0.000012  \n","Epoch: [3][1100/3575] Elapsed 4m 58s (remain 11m 10s) Loss: 0.0187(0.0037) Grad: 27110.4102  LR: 0.000012  \n","Epoch: [3][1200/3575] Elapsed 5m 25s (remain 10m 42s) Loss: 0.0032(0.0037) Grad: 13019.3535  LR: 0.000012  \n","Epoch: [3][1300/3575] Elapsed 5m 51s (remain 10m 15s) Loss: 0.0031(0.0038) Grad: 5690.4043  LR: 0.000012  \n","Epoch: [3][1400/3575] Elapsed 6m 18s (remain 9m 47s) Loss: 0.0000(0.0037) Grad: 14.7593  LR: 0.000012  \n","Epoch: [3][1500/3575] Elapsed 6m 45s (remain 9m 20s) Loss: 0.0018(0.0036) Grad: 8367.1377  LR: 0.000011  \n","Epoch: [3][1600/3575] Elapsed 7m 12s (remain 8m 53s) Loss: 0.0016(0.0037) Grad: 5866.7227  LR: 0.000011  \n","Epoch: [3][1700/3575] Elapsed 7m 39s (remain 8m 26s) Loss: 0.0115(0.0037) Grad: 72238.7109  LR: 0.000011  \n","Epoch: [3][1800/3575] Elapsed 8m 6s (remain 7m 58s) Loss: 0.0001(0.0037) Grad: 332.6363  LR: 0.000011  \n","Epoch: [3][1900/3575] Elapsed 8m 33s (remain 7m 31s) Loss: 0.0106(0.0038) Grad: 44740.8047  LR: 0.000011  \n","Epoch: [3][2000/3575] Elapsed 9m 0s (remain 7m 4s) Loss: 0.0009(0.0038) Grad: 2522.0481  LR: 0.000011  \n","Epoch: [3][2100/3575] Elapsed 9m 26s (remain 6m 37s) Loss: 0.0000(0.0038) Grad: 23.0875  LR: 0.000011  \n","Epoch: [3][2200/3575] Elapsed 9m 53s (remain 6m 10s) Loss: 0.0009(0.0038) Grad: 4784.2832  LR: 0.000011  \n","Epoch: [3][2300/3575] Elapsed 10m 20s (remain 5m 43s) Loss: 0.0010(0.0038) Grad: 8480.1318  LR: 0.000010  \n","Epoch: [3][2400/3575] Elapsed 10m 47s (remain 5m 16s) Loss: 0.0004(0.0038) Grad: 3828.5881  LR: 0.000010  \n","Epoch: [3][2500/3575] Elapsed 11m 14s (remain 4m 49s) Loss: 0.0005(0.0038) Grad: 3083.2651  LR: 0.000010  \n","Epoch: [3][2600/3575] Elapsed 11m 41s (remain 4m 22s) Loss: 0.0017(0.0038) Grad: 13523.3506  LR: 0.000010  \n","Epoch: [3][2700/3575] Elapsed 12m 8s (remain 3m 55s) Loss: 0.0000(0.0038) Grad: 42.4033  LR: 0.000010  \n","Epoch: [3][2800/3575] Elapsed 12m 34s (remain 3m 28s) Loss: 0.0000(0.0038) Grad: 37.3031  LR: 0.000010  \n","Epoch: [3][2900/3575] Elapsed 13m 1s (remain 3m 1s) Loss: 0.0003(0.0038) Grad: 2439.9075  LR: 0.000010  \n","Epoch: [3][3000/3575] Elapsed 13m 28s (remain 2m 34s) Loss: 0.0000(0.0039) Grad: 117.8761  LR: 0.000010  \n","Epoch: [3][3100/3575] Elapsed 13m 55s (remain 2m 7s) Loss: 0.0010(0.0039) Grad: 2510.3289  LR: 0.000009  \n","Epoch: [3][3200/3575] Elapsed 14m 22s (remain 1m 40s) Loss: 0.0014(0.0039) Grad: 6435.7344  LR: 0.000009  \n","Epoch: [3][3300/3575] Elapsed 14m 49s (remain 1m 13s) Loss: 0.0037(0.0039) Grad: 20857.0684  LR: 0.000009  \n","Epoch: [3][3400/3575] Elapsed 15m 16s (remain 0m 46s) Loss: 0.0000(0.0039) Grad: 204.1748  LR: 0.000009  \n","Epoch: [3][3500/3575] Elapsed 15m 42s (remain 0m 19s) Loss: 0.0122(0.0039) Grad: 73764.5859  LR: 0.000009  \n","Epoch: [3][3574/3575] Elapsed 16m 2s (remain 0m 0s) Loss: 0.0028(0.0038) Grad: 94977.9219  LR: 0.000009  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 7m 46s) Loss: 0.0001(0.0001) \n","EVAL: [100/1192] Elapsed 0m 14s (remain 2m 38s) Loss: 0.0420(0.0063) \n","EVAL: [200/1192] Elapsed 0m 29s (remain 2m 23s) Loss: 0.0153(0.0062) \n","EVAL: [300/1192] Elapsed 0m 43s (remain 2m 8s) Loss: 0.0051(0.0070) \n","EVAL: [400/1192] Elapsed 0m 57s (remain 1m 53s) Loss: 0.0000(0.0065) \n","EVAL: [500/1192] Elapsed 1m 12s (remain 1m 39s) Loss: 0.0504(0.0062) \n","EVAL: [600/1192] Elapsed 1m 26s (remain 1m 24s) Loss: 0.0086(0.0065) \n","EVAL: [700/1192] Elapsed 1m 40s (remain 1m 10s) Loss: 0.0038(0.0070) \n","EVAL: [800/1192] Elapsed 1m 55s (remain 0m 56s) Loss: 0.0136(0.0072) \n","EVAL: [900/1192] Elapsed 2m 9s (remain 0m 41s) Loss: 0.0050(0.0075) \n","EVAL: [1000/1192] Elapsed 2m 23s (remain 0m 27s) Loss: 0.0000(0.0072) \n","EVAL: [1100/1192] Elapsed 2m 38s (remain 0m 13s) Loss: 0.0199(0.0070) \n","EVAL: [1191/1192] Elapsed 2m 51s (remain 0m 0s) Loss: 0.0000(0.0068) \n","Epoch 3 - avg_train_loss: 0.0038  avg_val_loss: 0.0068  time: 1139s\n","Epoch 3 - Score: 0.8873\n","Epoch 3 - Save Best Score: 0.8873 Model\n","Epoch: [4][0/3575] Elapsed 0m 0s (remain 34m 19s) Loss: 0.0000(0.0000) Grad: 998.3007  LR: 0.000009  \n","Epoch: [4][100/3575] Elapsed 0m 28s (remain 16m 27s) Loss: 0.0016(0.0031) Grad: 14202.4033  LR: 0.000009  \n","Epoch: [4][200/3575] Elapsed 0m 56s (remain 15m 43s) Loss: 0.0000(0.0025) Grad: 6.1892  LR: 0.000009  \n","Epoch: [4][300/3575] Elapsed 1m 23s (remain 15m 3s) Loss: 0.0000(0.0027) Grad: 306.4542  LR: 0.000009  \n","Epoch: [4][400/3575] Elapsed 1m 49s (remain 14m 30s) Loss: 0.0000(0.0025) Grad: 55.9448  LR: 0.000008  \n","Epoch: [4][500/3575] Elapsed 2m 16s (remain 13m 59s) Loss: 0.0000(0.0026) Grad: 15.4666  LR: 0.000008  \n","Epoch: [4][600/3575] Elapsed 2m 43s (remain 13m 29s) Loss: 0.0017(0.0029) Grad: 9300.2236  LR: 0.000008  \n","Epoch: [4][700/3575] Elapsed 3m 10s (remain 13m 0s) Loss: 0.0031(0.0029) Grad: 22389.2031  LR: 0.000008  \n","Epoch: [4][800/3575] Elapsed 3m 37s (remain 12m 32s) Loss: 0.0034(0.0030) Grad: 22913.8691  LR: 0.000008  \n","Epoch: [4][900/3575] Elapsed 4m 4s (remain 12m 4s) Loss: 0.0001(0.0032) Grad: 612.7320  LR: 0.000008  \n","Epoch: [4][1000/3575] Elapsed 4m 30s (remain 11m 36s) Loss: 0.0000(0.0031) Grad: 301.2667  LR: 0.000008  \n","Epoch: [4][1100/3575] Elapsed 4m 57s (remain 11m 8s) Loss: 0.0001(0.0030) Grad: 369.3985  LR: 0.000008  \n","Epoch: [4][1200/3575] Elapsed 5m 24s (remain 10m 41s) Loss: 0.0004(0.0030) Grad: 2594.1030  LR: 0.000007  \n","Epoch: [4][1300/3575] Elapsed 5m 51s (remain 10m 14s) Loss: 0.0095(0.0031) Grad: 38889.0039  LR: 0.000007  \n","Epoch: [4][1400/3575] Elapsed 6m 18s (remain 9m 47s) Loss: 0.0106(0.0030) Grad: 53703.0469  LR: 0.000007  \n","Epoch: [4][1500/3575] Elapsed 6m 45s (remain 9m 19s) Loss: 0.0001(0.0029) Grad: 659.6356  LR: 0.000007  \n","Epoch: [4][1600/3575] Elapsed 7m 11s (remain 8m 52s) Loss: 0.0000(0.0030) Grad: 5.5115  LR: 0.000007  \n","Epoch: [4][1700/3575] Elapsed 7m 38s (remain 8m 25s) Loss: 0.0000(0.0030) Grad: 21.6104  LR: 0.000007  \n","Epoch: [4][1800/3575] Elapsed 8m 5s (remain 7m 58s) Loss: 0.0002(0.0031) Grad: 1311.4550  LR: 0.000007  \n","Epoch: [4][1900/3575] Elapsed 8m 32s (remain 7m 31s) Loss: 0.0000(0.0030) Grad: 42.1222  LR: 0.000007  \n","Epoch: [4][2000/3575] Elapsed 8m 59s (remain 7m 4s) Loss: 0.0000(0.0031) Grad: 5.3392  LR: 0.000006  \n","Epoch: [4][2100/3575] Elapsed 9m 25s (remain 6m 37s) Loss: 0.0000(0.0031) Grad: 92.4643  LR: 0.000006  \n","Epoch: [4][2200/3575] Elapsed 9m 52s (remain 6m 10s) Loss: 0.0000(0.0031) Grad: 31.7668  LR: 0.000006  \n","Epoch: [4][2300/3575] Elapsed 10m 19s (remain 5m 43s) Loss: 0.0000(0.0031) Grad: 163.8836  LR: 0.000006  \n","Epoch: [4][2400/3575] Elapsed 10m 46s (remain 5m 16s) Loss: 0.0002(0.0031) Grad: 1464.2754  LR: 0.000006  \n","Epoch: [4][2500/3575] Elapsed 11m 13s (remain 4m 49s) Loss: 0.0000(0.0030) Grad: 11.4197  LR: 0.000006  \n","Epoch: [4][2600/3575] Elapsed 11m 39s (remain 4m 22s) Loss: 0.0030(0.0030) Grad: 9038.7646  LR: 0.000006  \n","Epoch: [4][2700/3575] Elapsed 12m 6s (remain 3m 55s) Loss: 0.0002(0.0031) Grad: 948.1605  LR: 0.000006  \n","Epoch: [4][2800/3575] Elapsed 12m 33s (remain 3m 28s) Loss: 0.0000(0.0031) Grad: 18.1674  LR: 0.000005  \n","Epoch: [4][2900/3575] Elapsed 13m 0s (remain 3m 1s) Loss: 0.0000(0.0031) Grad: 10.1721  LR: 0.000005  \n","Epoch: [4][3000/3575] Elapsed 13m 27s (remain 2m 34s) Loss: 0.0001(0.0031) Grad: 913.8804  LR: 0.000005  \n","Epoch: [4][3100/3575] Elapsed 13m 54s (remain 2m 7s) Loss: 0.0099(0.0031) Grad: 9158.2959  LR: 0.000005  \n","Epoch: [4][3200/3575] Elapsed 14m 20s (remain 1m 40s) Loss: 0.0000(0.0031) Grad: 328.0528  LR: 0.000005  \n","Epoch: [4][3300/3575] Elapsed 14m 47s (remain 1m 13s) Loss: 0.0000(0.0031) Grad: 103.2546  LR: 0.000005  \n","Epoch: [4][3400/3575] Elapsed 15m 14s (remain 0m 46s) Loss: 0.0000(0.0031) Grad: 21.1354  LR: 0.000005  \n","Epoch: [4][3500/3575] Elapsed 15m 41s (remain 0m 19s) Loss: 0.0001(0.0031) Grad: 711.9890  LR: 0.000005  \n","Epoch: [4][3574/3575] Elapsed 16m 1s (remain 0m 0s) Loss: 0.0026(0.0031) Grad: 7223.1230  LR: 0.000004  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 8m 25s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 14s (remain 2m 38s) Loss: 0.0375(0.0065) \n","EVAL: [200/1192] Elapsed 0m 29s (remain 2m 23s) Loss: 0.0155(0.0063) \n","EVAL: [300/1192] Elapsed 0m 43s (remain 2m 8s) Loss: 0.0085(0.0066) \n","EVAL: [400/1192] Elapsed 0m 57s (remain 1m 53s) Loss: 0.0000(0.0063) \n","EVAL: [500/1192] Elapsed 1m 12s (remain 1m 39s) Loss: 0.0482(0.0061) \n","EVAL: [600/1192] Elapsed 1m 26s (remain 1m 25s) Loss: 0.0073(0.0064) \n","EVAL: [700/1192] Elapsed 1m 40s (remain 1m 10s) Loss: 0.0032(0.0071) \n","EVAL: [800/1192] Elapsed 1m 55s (remain 0m 56s) Loss: 0.0149(0.0071) \n","EVAL: [900/1192] Elapsed 2m 9s (remain 0m 41s) Loss: 0.0079(0.0073) \n","EVAL: [1000/1192] Elapsed 2m 23s (remain 0m 27s) Loss: 0.0000(0.0071) \n","EVAL: [1100/1192] Elapsed 2m 38s (remain 0m 13s) Loss: 0.0162(0.0069) \n","EVAL: [1191/1192] Elapsed 2m 51s (remain 0m 0s) Loss: 0.0000(0.0067) \n","Epoch 4 - avg_train_loss: 0.0031  avg_val_loss: 0.0067  time: 1138s\n","Epoch 4 - Score: 0.8899\n","Epoch 4 - Save Best Score: 0.8899 Model\n","Epoch: [5][0/3575] Elapsed 0m 0s (remain 36m 29s) Loss: 0.0000(0.0000) Grad: 1077.4918  LR: 0.000004  \n","Epoch: [5][100/3575] Elapsed 0m 28s (remain 16m 26s) Loss: 0.0025(0.0014) Grad: 16821.1270  LR: 0.000004  \n","Epoch: [5][200/3575] Elapsed 0m 56s (remain 15m 42s) Loss: 0.0000(0.0025) Grad: 488.9469  LR: 0.000004  \n","Epoch: [5][300/3575] Elapsed 1m 22s (remain 15m 2s) Loss: 0.0000(0.0023) Grad: 8.2177  LR: 0.000004  \n","Epoch: [5][400/3575] Elapsed 1m 50s (remain 14m 30s) Loss: 0.0002(0.0025) Grad: 7840.3135  LR: 0.000004  \n","Epoch: [5][500/3575] Elapsed 2m 16s (remain 13m 59s) Loss: 0.0000(0.0026) Grad: 10.0377  LR: 0.000004  \n","Epoch: [5][600/3575] Elapsed 2m 43s (remain 13m 29s) Loss: 0.0010(0.0025) Grad: 8397.5010  LR: 0.000004  \n","Epoch: [5][700/3575] Elapsed 3m 10s (remain 13m 0s) Loss: 0.0041(0.0025) Grad: 21436.3867  LR: 0.000004  \n","Epoch: [5][800/3575] Elapsed 3m 37s (remain 12m 32s) Loss: 0.0000(0.0024) Grad: 6.9325  LR: 0.000003  \n","Epoch: [5][900/3575] Elapsed 4m 3s (remain 12m 3s) Loss: 0.0000(0.0024) Grad: 8.8416  LR: 0.000003  \n","Epoch: [5][1000/3575] Elapsed 4m 30s (remain 11m 36s) Loss: 0.0028(0.0024) Grad: 47236.3945  LR: 0.000003  \n","Epoch: [5][1100/3575] Elapsed 4m 57s (remain 11m 8s) Loss: 0.0000(0.0023) Grad: 123.1532  LR: 0.000003  \n","Epoch: [5][1200/3575] Elapsed 5m 24s (remain 10m 41s) Loss: 0.0001(0.0022) Grad: 4592.7959  LR: 0.000003  \n","Epoch: [5][1300/3575] Elapsed 5m 51s (remain 10m 13s) Loss: 0.0043(0.0023) Grad: 48673.9336  LR: 0.000003  \n","Epoch: [5][1400/3575] Elapsed 6m 17s (remain 9m 46s) Loss: 0.0002(0.0022) Grad: 1098.5944  LR: 0.000003  \n","Epoch: [5][1500/3575] Elapsed 6m 44s (remain 9m 19s) Loss: 0.0000(0.0022) Grad: 69.9303  LR: 0.000003  \n","Epoch: [5][1600/3575] Elapsed 7m 11s (remain 8m 52s) Loss: 0.0052(0.0024) Grad: 10375.4756  LR: 0.000002  \n","Epoch: [5][1700/3575] Elapsed 7m 38s (remain 8m 25s) Loss: 0.0000(0.0025) Grad: 444.2443  LR: 0.000002  \n","Epoch: [5][1800/3575] Elapsed 8m 5s (remain 7m 58s) Loss: 0.0175(0.0024) Grad: 14134.7695  LR: 0.000002  \n","Epoch: [5][1900/3575] Elapsed 8m 32s (remain 7m 30s) Loss: 0.0000(0.0024) Grad: 226.8500  LR: 0.000002  \n","Epoch: [5][2000/3575] Elapsed 8m 58s (remain 7m 3s) Loss: 0.0001(0.0024) Grad: 904.9087  LR: 0.000002  \n","Epoch: [5][2100/3575] Elapsed 9m 25s (remain 6m 36s) Loss: 0.0000(0.0025) Grad: 15.2783  LR: 0.000002  \n","Epoch: [5][2200/3575] Elapsed 9m 52s (remain 6m 9s) Loss: 0.0000(0.0025) Grad: 25.2196  LR: 0.000002  \n","Epoch: [5][2300/3575] Elapsed 10m 19s (remain 5m 42s) Loss: 0.0000(0.0025) Grad: 263.4862  LR: 0.000002  \n","Epoch: [5][2400/3575] Elapsed 10m 46s (remain 5m 15s) Loss: 0.0000(0.0025) Grad: 357.2113  LR: 0.000001  \n","Epoch: [5][2500/3575] Elapsed 11m 12s (remain 4m 48s) Loss: 0.0015(0.0025) Grad: 18077.3750  LR: 0.000001  \n","Epoch: [5][2600/3575] Elapsed 11m 39s (remain 4m 22s) Loss: 0.0000(0.0025) Grad: 144.4555  LR: 0.000001  \n","Epoch: [5][2700/3575] Elapsed 12m 6s (remain 3m 55s) Loss: 0.0000(0.0025) Grad: 11.0639  LR: 0.000001  \n","Epoch: [5][2800/3575] Elapsed 12m 33s (remain 3m 28s) Loss: 0.0001(0.0024) Grad: 652.7668  LR: 0.000001  \n","Epoch: [5][2900/3575] Elapsed 13m 0s (remain 3m 1s) Loss: 0.0001(0.0024) Grad: 1344.5067  LR: 0.000001  \n","Epoch: [5][3000/3575] Elapsed 13m 27s (remain 2m 34s) Loss: 0.0000(0.0025) Grad: 3.7833  LR: 0.000001  \n","Epoch: [5][3100/3575] Elapsed 13m 53s (remain 2m 7s) Loss: 0.0004(0.0024) Grad: 1589.9557  LR: 0.000001  \n","Epoch: [5][3200/3575] Elapsed 14m 20s (remain 1m 40s) Loss: 0.0000(0.0024) Grad: 4.2177  LR: 0.000000  \n","Epoch: [5][3300/3575] Elapsed 14m 47s (remain 1m 13s) Loss: 0.0000(0.0024) Grad: 26.4369  LR: 0.000000  \n","Epoch: [5][3400/3575] Elapsed 15m 14s (remain 0m 46s) Loss: 0.0000(0.0025) Grad: 24.5690  LR: 0.000000  \n","Epoch: [5][3500/3575] Elapsed 15m 41s (remain 0m 19s) Loss: 0.0016(0.0025) Grad: 4737.7607  LR: 0.000000  \n","Epoch: [5][3574/3575] Elapsed 16m 0s (remain 0m 0s) Loss: 0.0093(0.0024) Grad: 18704.8809  LR: 0.000000  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 7m 49s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 14s (remain 2m 38s) Loss: 0.0458(0.0078) \n","EVAL: [200/1192] Elapsed 0m 29s (remain 2m 23s) Loss: 0.0107(0.0072) \n","EVAL: [300/1192] Elapsed 0m 43s (remain 2m 8s) Loss: 0.0098(0.0077) \n","EVAL: [400/1192] Elapsed 0m 57s (remain 1m 53s) Loss: 0.0000(0.0072) \n","EVAL: [500/1192] Elapsed 1m 12s (remain 1m 39s) Loss: 0.0529(0.0069) \n","EVAL: [600/1192] Elapsed 1m 26s (remain 1m 24s) Loss: 0.0075(0.0073) \n","EVAL: [700/1192] Elapsed 1m 40s (remain 1m 10s) Loss: 0.0037(0.0079) \n","EVAL: [800/1192] Elapsed 1m 55s (remain 0m 56s) Loss: 0.0204(0.0080) \n","EVAL: [900/1192] Elapsed 2m 9s (remain 0m 41s) Loss: 0.0090(0.0082) \n","EVAL: [1000/1192] Elapsed 2m 23s (remain 0m 27s) Loss: 0.0000(0.0080) \n","EVAL: [1100/1192] Elapsed 2m 38s (remain 0m 13s) Loss: 0.0126(0.0078) \n","EVAL: [1191/1192] Elapsed 2m 51s (remain 0m 0s) Loss: 0.0000(0.0076) \n","Epoch 5 - avg_train_loss: 0.0024  avg_val_loss: 0.0076  time: 1137s\n","Epoch 5 - Score: 0.8914\n","Epoch 5 - Save Best Score: 0.8914 Model\n","Best thres: 0.5, Score: 0.8854\n","Best thres: 0.50078125, Score: 0.8855\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10be394e641b473a9c1423517bfc9b19"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cba8a7329ba479d96980def21c41186"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception ignored in: <function _ConnectionBase.__del__ at 0x7f98759ffa70>\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 132, in __del__\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2370367dc5544bbb288ecd686e33552"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","\n","Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8a6576de5954a45b61e26cc7cffeef7"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]}],"source":["if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"name":"nbme-exp054.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"papermill":{"default_parameters":{},"duration":641.572862,"end_time":"2022-02-27T11:39:50.972497","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-02-27T11:29:09.399635","version":"2.3.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b12ba7f0320f402589087f3698ed74bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ec9f2c786cea4a29975a41662b687afd","IPY_MODEL_01f6a644de0a47399fdff9dbc025e68b","IPY_MODEL_93631ec5087643d2abd864b2c0275c92"],"layout":"IPY_MODEL_0cea3148262640c488a7695d78d5beca"}},"ec9f2c786cea4a29975a41662b687afd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d5ae9e8c5e5428388212d03a57d1108","placeholder":"​","style":"IPY_MODEL_4def68ab3ab04258b94f40e6b599d8ba","value":"Downloading: 100%"}},"01f6a644de0a47399fdff9dbc025e68b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e42d3dcc97c413fa80ac98cf49c2f11","max":52,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4e029bc4a0a54667bdaafabfa4f8c426","value":52}},"93631ec5087643d2abd864b2c0275c92":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_77dadf187f044d94800d5a2b3d3670df","placeholder":"​","style":"IPY_MODEL_3f53132ca5a547a99762632fc1cc137e","value":" 52.0/52.0 [00:00&lt;00:00, 1.47kB/s]"}},"0cea3148262640c488a7695d78d5beca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d5ae9e8c5e5428388212d03a57d1108":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4def68ab3ab04258b94f40e6b599d8ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e42d3dcc97c413fa80ac98cf49c2f11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e029bc4a0a54667bdaafabfa4f8c426":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"77dadf187f044d94800d5a2b3d3670df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f53132ca5a547a99762632fc1cc137e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aabb446961f24dcfb7a53fb3e05dd7e8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f9df580b39e34c419b39a02bd0c9783d","IPY_MODEL_a3788cf9109342e3a9d56a1365bd11ef","IPY_MODEL_f2e9ab33d526472e91eb9e76b4afd08c"],"layout":"IPY_MODEL_feb88886718b43838deacb442a0bdb04"}},"f9df580b39e34c419b39a02bd0c9783d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0ed774b1a9b40efb001981c4a4acaf9","placeholder":"​","style":"IPY_MODEL_67b5662cf52f4cbf90456df5e46cda3d","value":"Downloading: 100%"}},"a3788cf9109342e3a9d56a1365bd11ef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0a75d0ddb25404fa7f976e80222fe15","max":475,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d5096089fca9407da3ec14297dd84d58","value":475}},"f2e9ab33d526472e91eb9e76b4afd08c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30a507c7deb443eca095902438ca407e","placeholder":"​","style":"IPY_MODEL_ee69b63640dd4291b14ce80fb1a4a7a8","value":" 475/475 [00:00&lt;00:00, 15.1kB/s]"}},"feb88886718b43838deacb442a0bdb04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0ed774b1a9b40efb001981c4a4acaf9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67b5662cf52f4cbf90456df5e46cda3d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0a75d0ddb25404fa7f976e80222fe15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5096089fca9407da3ec14297dd84d58":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"30a507c7deb443eca095902438ca407e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee69b63640dd4291b14ce80fb1a4a7a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d449fbbc07548e1ba097d72fdcad8fe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2074ace01f6240cc8919d86d75682671","IPY_MODEL_742a829aa54541d99e1b0b912ed1bddd","IPY_MODEL_9c0f2abafeaa4975b91e79c20f9f1f27"],"layout":"IPY_MODEL_f603e95248304672a134e47edab6dfee"}},"2074ace01f6240cc8919d86d75682671":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a98ae10e462b40cb9eacce2444bc8368","placeholder":"​","style":"IPY_MODEL_6aa69365b619406e83d78f8547c5ce16","value":"Downloading: 100%"}},"742a829aa54541d99e1b0b912ed1bddd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e01ecabeae19488e81966a969e31be52","max":898825,"min":0,"orientation":"horizontal","style":"IPY_MODEL_54f9a4f989354459a23924c307418fbf","value":898825}},"9c0f2abafeaa4975b91e79c20f9f1f27":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b009a79c9d64575b8ec82daf2e2970b","placeholder":"​","style":"IPY_MODEL_bc676cfc2f4c494abcb5563482381f98","value":" 878k/878k [00:00&lt;00:00, 1.98MB/s]"}},"f603e95248304672a134e47edab6dfee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a98ae10e462b40cb9eacce2444bc8368":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6aa69365b619406e83d78f8547c5ce16":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e01ecabeae19488e81966a969e31be52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54f9a4f989354459a23924c307418fbf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6b009a79c9d64575b8ec82daf2e2970b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc676cfc2f4c494abcb5563482381f98":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d086a79d02694e9db586f0f2173f667e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f68d9350b6e04a81a960d7d602bfeefc","IPY_MODEL_2ed0c2c853614596999d6650d1494b17","IPY_MODEL_584600300cfc411782d7ecb17b293823"],"layout":"IPY_MODEL_198bb94984b543ae8c3828e759f46ea4"}},"f68d9350b6e04a81a960d7d602bfeefc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_867af0ee00934371b20e9d670dba9d79","placeholder":"​","style":"IPY_MODEL_789181104cc248ac9ce263afdba41bdc","value":"Downloading: 100%"}},"2ed0c2c853614596999d6650d1494b17":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_81ca4495c56148d3bfadf15973586917","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_885cf71fb18f44269814bda1638b1729","value":456318}},"584600300cfc411782d7ecb17b293823":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ade1985b963b40068db95a54c287d610","placeholder":"​","style":"IPY_MODEL_2c5bb10812fc46f9a2f907c0f9a72d5e","value":" 446k/446k [00:00&lt;00:00, 503kB/s]"}},"198bb94984b543ae8c3828e759f46ea4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"867af0ee00934371b20e9d670dba9d79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"789181104cc248ac9ce263afdba41bdc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"81ca4495c56148d3bfadf15973586917":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"885cf71fb18f44269814bda1638b1729":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ade1985b963b40068db95a54c287d610":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c5bb10812fc46f9a2f907c0f9a72d5e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"77a61d36dae94ec08352e99853a92706":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fea02d90b7b84e7c846fcc2a5750f113","IPY_MODEL_96e92848c4644c5fb49f63c890193257","IPY_MODEL_bfb5de6707624235a220584d4d39b0e9"],"layout":"IPY_MODEL_cc5cfcc00c754042be3e76f37aa70a9c"}},"fea02d90b7b84e7c846fcc2a5750f113":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f82baca32864d5b8730ef98323cbe5e","placeholder":"​","style":"IPY_MODEL_1c6c3360d5014173a04566e2831d0978","value":"100%"}},"96e92848c4644c5fb49f63c890193257":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e8ee5dacb824dca83690730359df2fd","max":42146,"min":0,"orientation":"horizontal","style":"IPY_MODEL_089d3e7ef7274e6a9dc47806aa4b271b","value":42146}},"bfb5de6707624235a220584d4d39b0e9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_539e82596c204e208c665edb93dcd601","placeholder":"​","style":"IPY_MODEL_8da14d3e1d844e40b5c984299d1aad95","value":" 42146/42146 [00:32&lt;00:00, 1956.00it/s]"}},"cc5cfcc00c754042be3e76f37aa70a9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f82baca32864d5b8730ef98323cbe5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c6c3360d5014173a04566e2831d0978":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3e8ee5dacb824dca83690730359df2fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"089d3e7ef7274e6a9dc47806aa4b271b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"539e82596c204e208c665edb93dcd601":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8da14d3e1d844e40b5c984299d1aad95":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"290d7ee54b4d476aa5b6094306e5280d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a8b47ec5238548da84b4866423b8616b","IPY_MODEL_26894cd923324b1091f49e5e850ce8d3","IPY_MODEL_7a747d0f950047cc890106591a197fb6"],"layout":"IPY_MODEL_74cb65d2a5b74853aa3119bb508e2879"}},"a8b47ec5238548da84b4866423b8616b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d74662b69c874d3db3489ea569158311","placeholder":"​","style":"IPY_MODEL_4f30fcd408784a5db98d3da1bf8da822","value":"100%"}},"26894cd923324b1091f49e5e850ce8d3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_14f940c1aa434194b6bf2deea8ca56f9","max":143,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9a29b9785382438d8917cb97a8eaf9ea","value":143}},"7a747d0f950047cc890106591a197fb6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c69e1c5ce6846f08e328b076ba39d52","placeholder":"​","style":"IPY_MODEL_1db64f9777664ef1a07da193b87f7b53","value":" 143/143 [00:00&lt;00:00, 2283.35it/s]"}},"74cb65d2a5b74853aa3119bb508e2879":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d74662b69c874d3db3489ea569158311":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f30fcd408784a5db98d3da1bf8da822":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"14f940c1aa434194b6bf2deea8ca56f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a29b9785382438d8917cb97a8eaf9ea":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9c69e1c5ce6846f08e328b076ba39d52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1db64f9777664ef1a07da193b87f7b53":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10be394e641b473a9c1423517bfc9b19":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aae3787dfa324de7baebb37c98f367bf","IPY_MODEL_feb0688b35eb4a989c6d9bb72dc66bc7","IPY_MODEL_509e544a0d9d4dce8bc58e9dab282f4e"],"layout":"IPY_MODEL_6e730352ee594fda8755a625fc782c8c"}},"aae3787dfa324de7baebb37c98f367bf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb6f18e982ec4f988ac8a88239ba609f","placeholder":"​","style":"IPY_MODEL_22b92759aba34490b60d0d4de7c8c7fa","value":"Downloading: 100%"}},"feb0688b35eb4a989c6d9bb72dc66bc7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5ecaecd0f944c039e60035e7521b50a","max":1627284589,"min":0,"orientation":"horizontal","style":"IPY_MODEL_91ca493447e1469f9fe35bb9fd21a530","value":1627284589}},"509e544a0d9d4dce8bc58e9dab282f4e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ccd4be6d23aa4fccb4771956de2108cf","placeholder":"​","style":"IPY_MODEL_59928e8e5bd44b6ebc4b6d5ef0729c5c","value":" 1.52G/1.52G [00:31&lt;00:00, 53.7MB/s]"}},"6e730352ee594fda8755a625fc782c8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb6f18e982ec4f988ac8a88239ba609f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22b92759aba34490b60d0d4de7c8c7fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d5ecaecd0f944c039e60035e7521b50a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91ca493447e1469f9fe35bb9fd21a530":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ccd4be6d23aa4fccb4771956de2108cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59928e8e5bd44b6ebc4b6d5ef0729c5c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4cba8a7329ba479d96980def21c41186":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8a5c850862d1420ab1d80bfc68ec4bdd","IPY_MODEL_8670d3fc64ab4c069e33f99ea6990e7b","IPY_MODEL_7e1efe3304db4a5e9403dfdc5f67add5"],"layout":"IPY_MODEL_06fbc68b560741359603eab69cc8c7e0"}},"8a5c850862d1420ab1d80bfc68ec4bdd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_78b7ec27a08f43329990d37cd967b1fe","placeholder":"​","style":"IPY_MODEL_a4b8c3f217fc42d9be4e18a945aec0d1","value":"100%"}},"8670d3fc64ab4c069e33f99ea6990e7b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f4f74ef501f48df8279de45f9cc5f1c","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1e715aabca0f4e6aac6f0711a56fb51e","value":2}},"7e1efe3304db4a5e9403dfdc5f67add5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a434a45d5104c3c82faae3f42a8e974","placeholder":"​","style":"IPY_MODEL_3cfba912367b4a04879183d13959ecf3","value":" 2/2 [00:01&lt;00:00,  1.60it/s]"}},"06fbc68b560741359603eab69cc8c7e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78b7ec27a08f43329990d37cd967b1fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4b8c3f217fc42d9be4e18a945aec0d1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f4f74ef501f48df8279de45f9cc5f1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e715aabca0f4e6aac6f0711a56fb51e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5a434a45d5104c3c82faae3f42a8e974":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3cfba912367b4a04879183d13959ecf3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c2370367dc5544bbb288ecd686e33552":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_863bd50f4afd42ee8375d5a1347b2db7","IPY_MODEL_7cd91870cf394d159a778259d00a0bc5","IPY_MODEL_7227cdbf6ff54101b95c4db8d71f4a32"],"layout":"IPY_MODEL_747072492c9d4686aa615ae44925525b"}},"863bd50f4afd42ee8375d5a1347b2db7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73b670e64eb8491a8bda8b4c0aa06369","placeholder":"​","style":"IPY_MODEL_aa426cbcbd1e43ed8d03efeb8afe80c8","value":"100%"}},"7cd91870cf394d159a778259d00a0bc5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b85d1006e33f43b6b121b57d1f040666","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3189f8c422894f839999d4019d41ee24","value":2}},"7227cdbf6ff54101b95c4db8d71f4a32":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2f4cbf1f89945598039101ff521fc5a","placeholder":"​","style":"IPY_MODEL_5aeabef5e99143738c9af0b7ce10fe59","value":" 2/2 [00:01&lt;00:00,  1.79it/s]"}},"747072492c9d4686aa615ae44925525b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73b670e64eb8491a8bda8b4c0aa06369":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa426cbcbd1e43ed8d03efeb8afe80c8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b85d1006e33f43b6b121b57d1f040666":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3189f8c422894f839999d4019d41ee24":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d2f4cbf1f89945598039101ff521fc5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5aeabef5e99143738c9af0b7ce10fe59":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a8a6576de5954a45b61e26cc7cffeef7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_018aa8372888466f859f3fb70acfb109","IPY_MODEL_7784b925fb874ebda549afdebc2649bd","IPY_MODEL_ba8b9d7da21848029615bf8e410eac88"],"layout":"IPY_MODEL_ebdaa6ef664641c4b35ac81eb256018e"}},"018aa8372888466f859f3fb70acfb109":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a52522b516a4d229040635cd5af97cb","placeholder":"​","style":"IPY_MODEL_0f9a3d6de4214501bae5a31969b89154","value":"100%"}},"7784b925fb874ebda549afdebc2649bd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4033f6ccc0e4640b55d52e7301f3b84","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_30841d0d333d4fa0b62be47035c0c9ad","value":2}},"ba8b9d7da21848029615bf8e410eac88":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_38e4dc6435b24a4b927259f0c5a5d1c1","placeholder":"​","style":"IPY_MODEL_c8080c2268454ca5aeadb618d74e7599","value":" 2/2 [00:02&lt;00:00,  1.43it/s]"}},"ebdaa6ef664641c4b35ac81eb256018e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a52522b516a4d229040635cd5af97cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f9a3d6de4214501bae5a31969b89154":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d4033f6ccc0e4640b55d52e7301f3b84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30841d0d333d4fa0b62be47035c0c9ad":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"38e4dc6435b24a4b927259f0c5a5d1c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8080c2268454ca5aeadb618d74e7599":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}