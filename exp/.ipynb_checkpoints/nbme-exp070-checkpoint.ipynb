{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mathematical-haven",
   "metadata": {
    "id": "colored-security"
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-oriental",
   "metadata": {
    "id": "educational-operator"
   },
   "source": [
    "- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-administration",
   "metadata": {
    "id": "incorrect-greek"
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "guided-haven",
   "metadata": {
    "id": "alive-granny"
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"nbme-exp070\"\n",
    "ENV = \"local\"\n",
    "DEBUG_MODE = False\n",
    "SUBMISSION_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vertical-authentication",
   "metadata": {
    "id": "heavy-prophet"
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    env=ENV\n",
    "    exp_name=EXP_NAME\n",
    "    debug=DEBUG_MODE\n",
    "    submission=SUBMISSION_MODE\n",
    "    apex=True\n",
    "    input_dir=None\n",
    "    output_dir=None\n",
    "    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n",
    "    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n",
    "    competition_name=\"nbme-score-clinical-patient-notes\"\n",
    "    id_col=\"id\"\n",
    "    target_col=\"location\"\n",
    "    pretrained_model_name=\"microsoft/deberta-large\"\n",
    "    tokenizer=None\n",
    "    max_len=None\n",
    "    max_char_len=None\n",
    "    pseudo_plain_path='../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl'\n",
    "    n_pseudo_labels=100000\n",
    "    output_dim=1\n",
    "    dropout=0.2\n",
    "    num_workers=4\n",
    "    batch_size=3\n",
    "    lr=2e-5\n",
    "    betas=(0.9, 0.98)\n",
    "    weight_decay=0.1\n",
    "    num_warmup_steps_rate=0.1\n",
    "    batch_scheduler=True\n",
    "    epochs=1\n",
    "    n_fold=4\n",
    "    train_fold=[0, 1, 2, 3]  # [0, 1, 2, 3]\n",
    "    seed=71\n",
    "    gradient_accumulation_steps=2\n",
    "    max_grad_norm=1000\n",
    "    print_freq=100\n",
    "    train=True\n",
    "    inference=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "closed-governor",
   "metadata": {
    "id": "vocational-coating"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.train_fold = [0, 1]\n",
    "\n",
    "if CFG.submission:\n",
    "    CFG.train = False\n",
    "    CFG.inference = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-chapel",
   "metadata": {
    "id": "private-moderator"
   },
   "source": [
    "## Directory Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sapphire-relation",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "married-tokyo",
    "outputId": "9c0fba66-759b-4354-898f-1afb47256d96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "print(CFG.env)\n",
    "if CFG.env == \"colab\":\n",
    "    # colab環境\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "    # install packages\n",
    "    !pip install transformers==4.16.2\n",
    "\n",
    "elif CFG.env == \"local\":\n",
    "    # ローカルサーバ\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "\n",
    "elif CFG.env == \"kaggle\":\n",
    "    # kaggle環境\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mineral-syria",
   "metadata": {
    "id": "blank-pierre"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import BartModel,BertModel,BertTokenizer\n",
    "from transformers import DebertaModel,DebertaTokenizer\n",
    "from transformers import RobertaModel,RobertaTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-suicide",
   "metadata": {
    "id": "sound-still"
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "manual-courtesy",
   "metadata": {
    "id": "surprised-commercial"
   },
   "outputs": [],
   "source": [
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mental-joining",
   "metadata": {
    "id": "interstate-accident"
   },
   "outputs": [],
   "source": [
    "def create_labels_for_scoring(df):\n",
    "    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n",
    "    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, \"location\"]\n",
    "        if lst:\n",
    "            new_lst = \";\".join(lst)\n",
    "            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n",
    "\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df[\"location_for_create_labels\"].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, token_probs, tokenizer):\n",
    "    res = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n",
    "        encoded = tokenizer(\n",
    "            text=text,\n",
    "            max_length=CFG.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n",
    "            start, end = offset_mapping\n",
    "            res[i][start:end] = pred\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_predicted_location_str(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        # result = np.where(char_prob >= th)[0] + 1\n",
    "        result = np.where(char_prob >= th)[0]\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        # result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = [f\"{min(r)} {max(r) + 1}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def scoring(df, th=0.5, use_token_prob=True):\n",
    "    labels = create_labels_for_scoring(df)\n",
    "\n",
    "    if use_token_prob:\n",
    "        token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n",
    "        char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n",
    "    else:\n",
    "        char_probs = df[[str(i) for i in range(CFG.max_char_len)]].values\n",
    "        char_probs = [char_probs[i] for i in range(len(char_probs))]\n",
    "\n",
    "    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n",
    "    preds = get_predictions(predicted_location_str)\n",
    "\n",
    "    score = get_score(labels, preds)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_best_thres(oof_df):\n",
    "    def f1_opt(x):\n",
    "        return -1 * scoring(oof_df, th=x)\n",
    "\n",
    "    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n",
    "    return best_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "annual-support",
   "metadata": {
    "id": "coated-pioneer"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cathedral-therapist",
   "metadata": {
    "id": "nervous-delaware"
   },
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-optimum",
   "metadata": {
    "id": "functioning-destruction"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "medium-dependence",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "global-monte",
    "outputId": "77675ef9-9cb4-44a4-ebeb-ae9e74d7ebd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 6), (143, 3), (42146, 3), (5, 4))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(CFG.input_dir / \"train.csv\")\n",
    "features = pd.read_csv(CFG.input_dir / \"features.csv\")\n",
    "patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n",
    "test = pd.read_csv(CFG.input_dir / \"test.csv\")\n",
    "\n",
    "train.shape, features.shape, patient_notes.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cooked-street",
   "metadata": {
    "id": "independent-airfare"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n",
    "    print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-vocabulary",
   "metadata": {
    "id": "silent-locator"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "innovative-begin",
   "metadata": {
    "id": "unusual-fifty"
   },
   "outputs": [],
   "source": [
    "def preprocess_features(features):\n",
    "    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "\n",
    "\n",
    "features = preprocess_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "supposed-miami",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "decreased-mustang",
    "outputId": "61c7d744-fde7-4d3c-e0c2-8393e24159b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 8), (5, 6))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "international-matrix",
   "metadata": {
    "id": "boolean-trade"
   },
   "outputs": [],
   "source": [
    "train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n",
    "train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "other-amount",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "accomplished-dakota",
    "outputId": "0c1b9ec8-3c61-4a44-bafe-fb9ea649d668"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4399\n",
       "1    8181\n",
       "2    1296\n",
       "3     287\n",
       "4      99\n",
       "5      27\n",
       "6       9\n",
       "7       1\n",
       "8       1\n",
       "Name: annotation_length, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n",
    "display(train['annotation_length'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-investing",
   "metadata": {
    "id": "funded-elizabeth"
   },
   "source": [
    "## CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "going-sucking",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "unexpected-columbia",
    "outputId": "e4b2bd12-a470-45e9-89f8-18f01bfb8836"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    3575\n",
       "1    3575\n",
       "2    3575\n",
       "3    3575\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Fold = GroupKFold(n_splits=CFG.n_fold)\n",
    "groups = train['pn_num'].values\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-adventure",
   "metadata": {
    "id": "critical-archive"
   },
   "source": [
    "## Setup tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "english-bible",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "9826121100004ec49f1cd7ed26023d9f",
      "cfc527e5b7e84c45af32ba7e49275790",
      "33bbf6ff0a9847fc9900a16ea8247120",
      "1c64feffa14d4a56b3bf4c25f6d05c51",
      "b42a1221467e43648126dc9432be0b28",
      "1c619fc144a44ae8ba6d091c236a20f6",
      "f3dcb10079874c84a85896aae581b1dc",
      "61d290b99d78422494bd99ba7f4ce0b4",
      "511fe7d1f0db48f5848e3539ddb29fff",
      "1fc34ac85840429e8d5d62785817dd03",
      "c7dbd264bb804aabbb9a3a3922cdcc00",
      "a2fc47910e5a4b158eddf7faa455d683",
      "808441188c1a4b5788ae84fa3edf27db",
      "638cf831bc0443658b4b455200f9b990",
      "c5ad324c87914e26ad665efcc418f354",
      "8254bfa8a6fa47dba57db5ba29dccaeb",
      "3d505cf1fb134928953da8d79d68665a",
      "81714296902f4d26a32aa05da8890693",
      "8f0a30d22d004fd6866556696346fb74",
      "5504bc5bee8c4c189614fd398d1b7fef",
      "5a47b531ee814b1eba201f0aa79212db",
      "7c1442db3949418184bfb68266910d91",
      "f61172e130474199999fe10c8470b1e8",
      "530069c0165e4b3d8e077e9c6a4a1d21",
      "323239c2e16449c088afd6eb5eeaa73f",
      "263650ffbf3145048f331827b49ef11b",
      "7ebb3f8b10154ef4badb2d6856140d18",
      "2c0468cceac144b890d30b510202deba",
      "b4c22ff3f7064b5c82b501058fa367a6",
      "b5f8f50bdaf843f1a938f6129848cd83",
      "0f0864ef340b49aaa9b4596e032163a7",
      "886eee1754fd42e185a7548aa44871f1",
      "ec232b9c334c4987b35fde837fac77da",
      "0c748174ece64ca6992b434a2d92e1e4",
      "827b5e8189d242c9b62bb8d6a084de72",
      "0f910e441d334b10bc666e6ef47de941",
      "09da6c904a3344ad9d7d0f17c646c8b1",
      "d6ba1f9a002c49268799fc4a17f793ee",
      "dc036c5ffc0a4a0fb2d0f504cf4264f3",
      "80be6bc8b1c8454187599fe6f05cdcba",
      "760cf6427cdc4b05affb72378d92a0f3",
      "9a660863781b487392504ab3302a5f93",
      "66daca2dd30d44efbcd88adcbb1c2725",
      "02633c7de1ea4b7ca2fd899ae0c6d209"
     ]
    },
    "id": "broken-generator",
    "outputId": "9ed5f1df-2a6b-4b0d-bbfb-dd49474e17ea"
   },
   "outputs": [],
   "source": [
    "if CFG.submission:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG.pretrained_model_name)\n",
    "    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n",
    "\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-range",
   "metadata": {
    "id": "compatible-lincoln"
   },
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "exempt-default",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "8e1aca2f17c6476a855cbe11a1d661a9",
      "aa1104924af9458eb0d5443fd617cf29",
      "b94dd392dc874911b1deeb39f77b342f",
      "be8e2f5fc8eb4029a4aece3d1776054d",
      "032d2a594c45472e9681d2d7557ab93d",
      "cd01049905654fe6bee6c4c602b89ed9",
      "66a3685d05e1493c987e6cb1d9ed00a1",
      "13133c559d1b4a14ba19c157c169b532",
      "7e01c08e3af148f580fcdc6ef6ebf5b4",
      "e464d1ba21a1489183fda5c01bbc0cca",
      "5bb3e7fc97c64d59a20a7ebb29a39800"
     ]
    },
    "id": "fluid-nancy",
    "outputId": "9b844ea4-2568-4dea-dd39-867dcb402cfe"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674b1f19cb334f558aa8818255cf8e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 433\n"
     ]
    }
   ],
   "source": [
    "pn_history_lengths = []\n",
    "tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    pn_history_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(pn_history_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ongoing-management",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "6639cae9d2844ea3b7d9a777b43b2181",
      "6e5be1d244794198afee6c8fefd3a191",
      "2df59efc32cf4642a4cfff7db709db5a",
      "72d201f9470a4d04b1e89f7d0018c0d7",
      "cebcd629be2340bfa4ca1780d70dd364",
      "da9bce59ac4845cda340d840b79be311",
      "323befd254f741a7b041d124d4073817",
      "eb191fc8e771447ab389bbb57fe22d2f",
      "ced0845b868342c29d4e2d830a48f203",
      "13873604cb644df0b4b5e8e9ea57d645",
      "f129267b3ae6422e8d345c28b73f5516"
     ]
    },
    "id": "posted-humidity",
    "outputId": "c23cdcb5-4093-47d7-d95c-a35eaf72ea9d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24581658c7a549c09a001cb2d8c933f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 30\n"
     ]
    }
   ],
   "source": [
    "feature_text_lengths = []\n",
    "tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    feature_text_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(feature_text_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "third-snowboard",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "resistant-amount",
    "outputId": "8bc2659a-d25a-40a1-f85d-37d57b1f3fc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 466\n"
     ]
    }
   ],
   "source": [
    "CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n",
    "\n",
    "print(\"max length:\", CFG.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "forward-mailman",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "4c276b71af2f4301b4048e4f8dad36d3",
      "563519b534874ec8a145d292258b2dad",
      "535a8156a6da43928fd0d7a33c624540",
      "ef3dd8c9fb4a46bfa6c30489d2d75b02",
      "5d985ee5c7d84bf9a135972d46830ad0",
      "5bc25dd928614ba999cefcde5efa9197",
      "5ccb74cb082845d7ade9962f741a2910",
      "8fa83fd7255f43ff9126aa633ed1b663",
      "9febd109d3a24bef886c8d24808347ba",
      "811a7f7f9bc34108b113d084a7d488f4",
      "ecfdf18519e34e2d9a0b112b0815cba5"
     ]
    },
    "id": "be6XpsR0aIWS",
    "outputId": "1af87d7f-23bc-4035-d4c2-08d1f778e070"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2e2f4efa2f40dbaa2604c58b53cc7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 950\n"
     ]
    }
   ],
   "source": [
    "pn_history_lengths = []\n",
    "tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n",
    "for text in tk0:\n",
    "    length = len(text)\n",
    "    pn_history_lengths.append(length)\n",
    "\n",
    "CFG.max_char_len = max(pn_history_lengths)\n",
    "\n",
    "print(\"max length:\", CFG.max_char_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "textile-gross",
   "metadata": {
    "id": "fIzpppqiaMRn"
   },
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, cfg, df, pseudo_label=None):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.max_char_len = self.cfg.max_char_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "        self.annotation_lengths = self.df[\"annotation_length\"].values\n",
    "        self.locations = self.df[\"location\"].values\n",
    "        if \"pseudo_idx\" in df.columns:\n",
    "            self.pseudo_idx = self.df[\"pseudo_idx\"].values\n",
    "            self.pseudo_label = pseudo_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def _create_mapping_from_token_to_char(self, pn_history):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        mapping_from_token_to_char = np.zeros(self.max_char_len)\n",
    "        offset_mapping = encoded[\"offset_mapping\"]\n",
    "        for i, offset in enumerate(offset_mapping):\n",
    "            start_idx, end_idx = offset\n",
    "            mapping_from_token_to_char[start_idx:end_idx] = i\n",
    "        return torch.tensor(mapping_from_token_to_char, dtype=torch.long)\n",
    "\n",
    "    def _create_label(self, pn_history, annotation_length, location_list):\n",
    "        label = np.zeros(self.max_char_len)\n",
    "        label[len(pn_history):] = -1\n",
    "        if annotation_length > 0:\n",
    "            for location in location_list:\n",
    "                for loc in [s.split() for s in location.split(\";\")]:\n",
    "                    start, end = int(loc[0]), int(loc[1])\n",
    "                    label[start:end] = 1\n",
    "        return torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        if not np.isnan(self.annotation_lengths[idx]):\n",
    "            label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n",
    "        else:\n",
    "            p_idx = int(self.pseudo_idx[idx])\n",
    "            label = torch.tensor(self.pseudo_label[p_idx], dtype=torch.float)\n",
    "        mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n",
    "        return input_, label, mapping_from_token_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "julian-observer",
   "metadata": {
    "id": "weird-interaction"
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.max_char_len = self.cfg.max_char_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def _create_mapping_from_token_to_char(self, pn_history):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        mapping_from_token_to_char = np.zeros(self.max_char_len)\n",
    "        offset_mapping = encoded[\"offset_mapping\"]\n",
    "        for i, offset in enumerate(offset_mapping):\n",
    "            start_idx, end_idx = offset\n",
    "            mapping_from_token_to_char[start_idx:end_idx] = i\n",
    "        return torch.tensor(mapping_from_token_to_char, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n",
    "        return input_, mapping_from_token_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-sauce",
   "metadata": {
    "id": "upper-mobility"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "handmade-kansas",
   "metadata": {
    "id": "spanish-destruction"
   },
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, model_config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if model_config_path is None:\n",
    "            self.model_config = AutoConfig.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "        else:\n",
    "            self.model_config = torch.load(model_config_path)\n",
    "\n",
    "        if pretrained:\n",
    "            self.backbone = AutoModel.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                config=self.model_config,\n",
    "            )\n",
    "            print(f\"Load weight from pretrained\")\n",
    "        else:\n",
    "            #self.backbone = AutoModel.from_config(self.model_config)\n",
    "            itpt = AutoModelForMaskedLM.from_config(self.model_config)\n",
    "            # path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp010/checkpoint-130170/pytorch_model.bin\")\n",
    "            path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n",
    "            state_dict = torch.load(path)\n",
    "            itpt.load_state_dict(state_dict)\n",
    "            self.backbone = itpt.deberta\n",
    "            print(f\"Load weight from {path}\")\n",
    "\n",
    "        self.lstm = nn.GRU(\n",
    "            input_size=self.model_config.hidden_size,\n",
    "            bidirectional=True,\n",
    "            hidden_size=self.model_config.hidden_size // 2,\n",
    "            num_layers=4,\n",
    "            dropout=self.cfg.dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(self.cfg.dropout),\n",
    "            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, mappings_from_token_to_char):\n",
    "        h = self.backbone(**inputs)[\"last_hidden_state\"]  # [batch, seq_len, d_model]\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.unsqueeze(2).expand(-1, -1, self.model_config.hidden_size)\n",
    "        h = torch.gather(h, 1, mappings_from_token_to_char)    # [batch, seq_len, d_model]\n",
    "        h, _ = self.lstm(h)\n",
    "        output = self.fc(h)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-anchor",
   "metadata": {
    "id": "chronic-bullet"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "studied-journal",
   "metadata": {
    "id": "biological-hunger"
   },
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    train_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    epoch,\n",
    "    scheduler,\n",
    "    device,\n",
    "):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels, mappings_from_token_to_char) in enumerate(train_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device) \n",
    "        batch_size = labels.size(0)\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            output = model(inputs, mappings_from_token_to_char)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if CFG.batch_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n",
    "            print(\n",
    "                \"Epoch: [{0}][{1}/{2}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                \"Grad: {grad_norm:.4f}  \"\n",
    "                \"LR: {lr:.6f}  \"\n",
    "                .format(\n",
    "                    epoch+1,\n",
    "                    step,\n",
    "                    len(train_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n",
    "                    loss=losses,\n",
    "                     grad_norm=grad_norm,\n",
    "                     lr=scheduler.get_lr()[0],\n",
    "                )\n",
    "            )\n",
    "    del output, loss, inputs, labels, mappings_from_token_to_char, scaler, grad_norm; gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "serial-fireplace",
   "metadata": {
    "id": "satisfied-sterling"
   },
   "outputs": [],
   "source": [
    "def valid_fn(\n",
    "    val_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    device,\n",
    "):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels, mappings_from_token_to_char) in enumerate(val_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device) \n",
    "        batch_size = labels.size(0)\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            output = model(inputs, mappings_from_token_to_char)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "        loss = loss.mean()\n",
    "    \n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n",
    "            print(\n",
    "                \"EVAL: [{0}/{1}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                .format(\n",
    "                    step, len(val_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n",
    "                    loss=losses,\n",
    "                )\n",
    "            )\n",
    "    preds = np.concatenate(preds)\n",
    "    return losses.avg, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "willing-sculpture",
   "metadata": {
    "id": "incorporate-viking"
   },
   "outputs": [],
   "source": [
    "def inference_fn(test_dataloader, model, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preds = []\n",
    "    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n",
    "    for (inputs, mappings_from_token_to_char) in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs, mappings_from_token_to_char)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "neutral-princess",
   "metadata": {
    "id": "dental-sunset"
   },
   "outputs": [],
   "source": [
    "def train_loop(df, i_fold, device):\n",
    "    print(f\"========== fold: {i_fold} training ==========\")\n",
    "    train_idx = df[df[\"fold\"] != i_fold].index\n",
    "    val_idx = df[df[\"fold\"] == i_fold].index\n",
    "\n",
    "    train_folds = df.loc[train_idx].reset_index(drop=True)\n",
    "    val_folds = df.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    if CFG.pseudo_plain_path is not None:\n",
    "        pseudo_plain = pd.read_pickle(CFG.pseudo_plain_path)\n",
    "        print(f\"get pseudo plain from {CFG.pseudo_plain_path}\")\n",
    "        pseudo_label_list = []\n",
    "        for exp_name in [\"nbme-exp060\", \"nbme-exp067\"]:\n",
    "            pseudo_label_path = f'../output/nbme-score-clinical-patient-notes/{exp_name}/pseudo_labels_{i_fold}.npy'\n",
    "            pseudo_label = np.load(pseudo_label_path)\n",
    "            print(f\"get pseudo labels from {pseudo_label_path}\")\n",
    "            pseudo_label_list.append(pseudo_label)\n",
    "    \n",
    "        pseudo_label = 0.5 * pseudo_label_list[0] + 0.5 * pseudo_label_list[1]\n",
    "        print(pseudo_plain.shape, pseudo_label.shape)\n",
    "\n",
    "        pseudo_plain[\"pseudo_idx\"] = np.arange(len(pseudo_plain))\n",
    "        pseudo_plain = pseudo_plain.sample(n=CFG.n_pseudo_labels, random_state=i_fold)\n",
    "        print(pseudo_plain.shape)\n",
    "        train_folds = pd.concat([train_folds, pseudo_plain], axis=0, ignore_index=True)\n",
    "        print(train_folds.shape)\n",
    "\n",
    "    train_dataset = TrainingDataset(CFG, train_folds, pseudo_label)\n",
    "    val_dataset = TrainingDataset(CFG, val_folds)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "    model = CustomModel(CFG, model_config_path=None, pretrained=False)   # itptを使うため\n",
    "    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n",
    "    model.to(device)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in param_optimizer if not any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n",
    "        {\"params\": [p for n, p in param_optimizer if any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=CFG.lr,\n",
    "        betas=CFG.betas,\n",
    "        weight_decay=CFG.weight_decay,\n",
    "    )\n",
    "    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n",
    "    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_train_optimization_steps,\n",
    "    )\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    best_score = -1 * np.inf\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "        avg_loss = train_fn(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            scheduler,\n",
    "            device,\n",
    "        )\n",
    "        avg_val_loss, val_preds = valid_fn(\n",
    "            val_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "\n",
    "        # scoring\n",
    "        val_folds[[str(i) for i in range(CFG.max_char_len)]] = val_preds\n",
    "        score = scoring(val_folds, th=0.5, use_token_prob=False)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n",
    "        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"predictions\": val_preds,\n",
    "                },\n",
    "                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "            )\n",
    "\n",
    "    predictions = torch.load(\n",
    "        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )[\"predictions\"]\n",
    "    val_folds[[str(i) for i in range(CFG.max_char_len)]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return val_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-schedule",
   "metadata": {
    "id": "brazilian-graphics"
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "nasty-albania",
   "metadata": {
    "id": "connected-protein"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for i_fold in range(CFG.n_fold):\n",
    "            if i_fold in CFG.train_fold:\n",
    "                _oof_df = train_loop(train, i_fold, device)\n",
    "                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n",
    "        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    if CFG.submission:\n",
    "        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n",
    "    else:\n",
    "        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    best_thres = 0.5\n",
    "    best_score = 0.\n",
    "    for th in np.arange(0.45, 0.55, 0.01):\n",
    "        th = np.round(th, 2)\n",
    "        score = scoring(oof_df, th=th, use_token_prob=False)\n",
    "        if best_score < score:\n",
    "            best_thres = th\n",
    "            best_score = score\n",
    "    print(f\"best_thres: {best_thres}  score: {best_score:.5f}\")\n",
    "\n",
    "    if CFG.inference:\n",
    "        test_dataset = TestDataset(CFG, test)\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=CFG.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=CFG.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        predictions = []\n",
    "        for i_fold in CFG.train_fold:\n",
    "            if CFG.submission:\n",
    "                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n",
    "                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n",
    "            else:\n",
    "                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n",
    "\n",
    "            state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "            model.load_state_dict(state[\"model\"])\n",
    "            print(f\"load weights from {path}\")\n",
    "            test_char_probs = inference_fn(test_dataloader, model, device)\n",
    "            predictions.append(test_char_probs)\n",
    "\n",
    "            del state, test_char_probs, model; gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        predictions = np.mean(predictions, axis=0)\n",
    "        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n",
    "        test[CFG.target_col] = predicted_location_str\n",
    "        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n",
    "        test[[CFG.id_col, CFG.target_col]].to_csv(\n",
    "            CFG.output_dir / \"submission.csv\", index=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bronze-genesis",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "64961f62c5f94c3991e2b9f09c7c4782",
      "8cc41478e6584c039ec440aa3c314e6d",
      "5a5fdc30f42646058b9162b0e0974e3d",
      "c74b2e0635904f98920a922b26b6541d"
     ]
    },
    "id": "serious-bunny",
    "outputId": "be850b3e-328a-47cf-b797-2d51d289e13e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "get pseudo plain from ../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_0.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_0.npy\n",
      "(612602, 6) (612602, 950)\n",
      "(100000, 7)\n",
      "(110725, 11)\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/36908] Elapsed 0m 1s (remain 813m 35s) Loss: 0.3536(0.3536) Grad: 96998.8359  LR: 0.000000  \n",
      "Epoch: [1][100/36908] Elapsed 1m 23s (remain 508m 47s) Loss: 0.3347(0.3453) Grad: 24031.7012  LR: 0.000001  \n",
      "Epoch: [1][200/36908] Elapsed 2m 47s (remain 510m 8s) Loss: 0.2502(0.3202) Grad: 19186.7637  LR: 0.000001  \n",
      "Epoch: [1][300/36908] Elapsed 4m 9s (remain 506m 22s) Loss: 0.1446(0.2792) Grad: 14845.5469  LR: 0.000002  \n",
      "Epoch: [1][400/36908] Elapsed 5m 32s (remain 505m 2s) Loss: 0.0439(0.2307) Grad: 1639.1991  LR: 0.000002  \n",
      "Epoch: [1][500/36908] Elapsed 6m 55s (remain 502m 39s) Loss: 0.0262(0.1926) Grad: 346.9176  LR: 0.000003  \n",
      "Epoch: [1][600/36908] Elapsed 8m 15s (remain 499m 14s) Loss: 0.0381(0.1670) Grad: 346.5618  LR: 0.000003  \n",
      "Epoch: [1][700/36908] Elapsed 9m 37s (remain 497m 7s) Loss: 0.0077(0.1482) Grad: 991.2931  LR: 0.000004  \n",
      "Epoch: [1][800/36908] Elapsed 11m 0s (remain 496m 10s) Loss: 0.0083(0.1330) Grad: 1790.8889  LR: 0.000004  \n",
      "Epoch: [1][900/36908] Elapsed 12m 22s (remain 494m 22s) Loss: 0.0192(0.1205) Grad: 1527.4070  LR: 0.000005  \n",
      "Epoch: [1][1000/36908] Elapsed 13m 43s (remain 492m 35s) Loss: 0.0064(0.1103) Grad: 1178.1116  LR: 0.000005  \n",
      "Epoch: [1][1100/36908] Elapsed 15m 6s (remain 491m 30s) Loss: 0.0069(0.1015) Grad: 973.9995  LR: 0.000006  \n",
      "Epoch: [1][1200/36908] Elapsed 16m 28s (remain 489m 37s) Loss: 0.0374(0.0942) Grad: 3040.8025  LR: 0.000007  \n",
      "Epoch: [1][1300/36908] Elapsed 17m 50s (remain 488m 17s) Loss: 0.0085(0.0881) Grad: 1226.5150  LR: 0.000007  \n",
      "Epoch: [1][1400/36908] Elapsed 19m 13s (remain 487m 18s) Loss: 0.0059(0.0826) Grad: 811.4305  LR: 0.000008  \n",
      "Epoch: [1][1500/36908] Elapsed 20m 35s (remain 485m 42s) Loss: 0.0083(0.0779) Grad: 686.2379  LR: 0.000008  \n",
      "Epoch: [1][1600/36908] Elapsed 21m 57s (remain 484m 14s) Loss: 0.0009(0.0737) Grad: 110.3476  LR: 0.000009  \n",
      "Epoch: [1][1700/36908] Elapsed 23m 20s (remain 483m 11s) Loss: 0.0045(0.0700) Grad: 1296.2026  LR: 0.000009  \n",
      "Epoch: [1][1800/36908] Elapsed 24m 42s (remain 481m 46s) Loss: 0.0008(0.0666) Grad: 320.6961  LR: 0.000010  \n",
      "Epoch: [1][1900/36908] Elapsed 26m 5s (remain 480m 30s) Loss: 0.0071(0.0635) Grad: 2040.4840  LR: 0.000010  \n",
      "Epoch: [1][2000/36908] Elapsed 27m 29s (remain 479m 28s) Loss: 0.0012(0.0607) Grad: 363.8486  LR: 0.000011  \n",
      "Epoch: [1][2100/36908] Elapsed 28m 52s (remain 478m 25s) Loss: 0.0054(0.0582) Grad: 540.0491  LR: 0.000011  \n",
      "Epoch: [1][2200/36908] Elapsed 30m 15s (remain 477m 10s) Loss: 0.0010(0.0559) Grad: 111.1785  LR: 0.000012  \n",
      "Epoch: [1][2300/36908] Elapsed 31m 37s (remain 475m 32s) Loss: 0.0050(0.0538) Grad: 986.6773  LR: 0.000012  \n",
      "Epoch: [1][2400/36908] Elapsed 32m 59s (remain 474m 6s) Loss: 0.0009(0.0519) Grad: 254.5302  LR: 0.000013  \n",
      "Epoch: [1][2500/36908] Elapsed 34m 20s (remain 472m 25s) Loss: 0.0064(0.0501) Grad: 381.7219  LR: 0.000014  \n",
      "Epoch: [1][2600/36908] Elapsed 35m 42s (remain 471m 3s) Loss: 0.0155(0.0484) Grad: 3041.7568  LR: 0.000014  \n",
      "Epoch: [1][2700/36908] Elapsed 37m 4s (remain 469m 31s) Loss: 0.0022(0.0468) Grad: 574.9824  LR: 0.000015  \n",
      "Epoch: [1][2800/36908] Elapsed 38m 25s (remain 467m 58s) Loss: 0.0011(0.0453) Grad: 192.5266  LR: 0.000015  \n",
      "Epoch: [1][2900/36908] Elapsed 39m 47s (remain 466m 28s) Loss: 0.0006(0.0440) Grad: 39.8896  LR: 0.000016  \n",
      "Epoch: [1][3000/36908] Elapsed 41m 8s (remain 464m 54s) Loss: 0.0287(0.0428) Grad: 4167.1211  LR: 0.000016  \n",
      "Epoch: [1][3100/36908] Elapsed 42m 30s (remain 463m 27s) Loss: 0.0142(0.0416) Grad: 418.1515  LR: 0.000017  \n",
      "Epoch: [1][3200/36908] Elapsed 43m 52s (remain 461m 55s) Loss: 0.0023(0.0405) Grad: 56.2110  LR: 0.000017  \n",
      "Epoch: [1][3300/36908] Elapsed 45m 13s (remain 460m 20s) Loss: 0.0076(0.0394) Grad: 634.7490  LR: 0.000018  \n",
      "Epoch: [1][3400/36908] Elapsed 46m 33s (remain 458m 46s) Loss: 0.0245(0.0384) Grad: 1665.4587  LR: 0.000018  \n",
      "Epoch: [1][3500/36908] Elapsed 47m 55s (remain 457m 14s) Loss: 0.0012(0.0375) Grad: 143.9482  LR: 0.000019  \n",
      "Epoch: [1][3600/36908] Elapsed 49m 16s (remain 455m 49s) Loss: 0.0118(0.0367) Grad: 409.7822  LR: 0.000020  \n",
      "Epoch: [1][3700/36908] Elapsed 50m 38s (remain 454m 22s) Loss: 0.0071(0.0358) Grad: 776.3312  LR: 0.000020  \n",
      "Epoch: [1][3800/36908] Elapsed 52m 0s (remain 452m 57s) Loss: 0.0026(0.0350) Grad: 171.1998  LR: 0.000020  \n",
      "Epoch: [1][3900/36908] Elapsed 53m 23s (remain 451m 42s) Loss: 0.0028(0.0343) Grad: 273.7928  LR: 0.000020  \n",
      "Epoch: [1][4000/36908] Elapsed 54m 46s (remain 450m 27s) Loss: 0.0012(0.0335) Grad: 62.9296  LR: 0.000020  \n",
      "Epoch: [1][4100/36908] Elapsed 56m 7s (remain 448m 59s) Loss: 0.0027(0.0328) Grad: 386.0851  LR: 0.000020  \n",
      "Epoch: [1][4200/36908] Elapsed 57m 28s (remain 447m 28s) Loss: 0.0085(0.0322) Grad: 1807.3699  LR: 0.000020  \n",
      "Epoch: [1][4300/36908] Elapsed 58m 49s (remain 445m 59s) Loss: 0.0011(0.0315) Grad: 215.0029  LR: 0.000020  \n",
      "Epoch: [1][4400/36908] Elapsed 60m 12s (remain 444m 42s) Loss: 0.0045(0.0309) Grad: 573.3690  LR: 0.000020  \n",
      "Epoch: [1][4500/36908] Elapsed 61m 34s (remain 443m 22s) Loss: 0.0007(0.0303) Grad: 11.8473  LR: 0.000020  \n",
      "Epoch: [1][4600/36908] Elapsed 62m 56s (remain 441m 59s) Loss: 0.0120(0.0298) Grad: 1838.7423  LR: 0.000019  \n",
      "Epoch: [1][4700/36908] Elapsed 64m 18s (remain 440m 33s) Loss: 0.0136(0.0292) Grad: 1516.8629  LR: 0.000019  \n",
      "Epoch: [1][4800/36908] Elapsed 65m 40s (remain 439m 15s) Loss: 0.0036(0.0287) Grad: 298.5661  LR: 0.000019  \n",
      "Epoch: [1][4900/36908] Elapsed 67m 3s (remain 437m 54s) Loss: 0.0030(0.0283) Grad: 953.8503  LR: 0.000019  \n",
      "Epoch: [1][5000/36908] Elapsed 68m 25s (remain 436m 33s) Loss: 0.0001(0.0278) Grad: 19.9951  LR: 0.000019  \n",
      "Epoch: [1][5100/36908] Elapsed 69m 47s (remain 435m 9s) Loss: 0.0001(0.0273) Grad: 7.9044  LR: 0.000019  \n",
      "Epoch: [1][5200/36908] Elapsed 71m 10s (remain 433m 52s) Loss: 0.0100(0.0269) Grad: 3256.2354  LR: 0.000019  \n",
      "Epoch: [1][5300/36908] Elapsed 72m 32s (remain 432m 32s) Loss: 0.0117(0.0265) Grad: 2998.7170  LR: 0.000019  \n",
      "Epoch: [1][5400/36908] Elapsed 73m 55s (remain 431m 11s) Loss: 0.0001(0.0260) Grad: 7.1395  LR: 0.000019  \n",
      "Epoch: [1][5500/36908] Elapsed 75m 18s (remain 430m 0s) Loss: 0.0044(0.0256) Grad: 1039.3828  LR: 0.000019  \n",
      "Epoch: [1][5600/36908] Elapsed 76m 41s (remain 428m 40s) Loss: 0.0086(0.0252) Grad: 1431.3345  LR: 0.000019  \n",
      "Epoch: [1][5700/36908] Elapsed 78m 4s (remain 427m 24s) Loss: 0.0036(0.0249) Grad: 3297.5933  LR: 0.000019  \n",
      "Epoch: [1][5800/36908] Elapsed 79m 27s (remain 426m 5s) Loss: 0.0014(0.0245) Grad: 499.8009  LR: 0.000019  \n",
      "Epoch: [1][5900/36908] Elapsed 80m 49s (remain 424m 43s) Loss: 0.0009(0.0242) Grad: 562.1192  LR: 0.000019  \n",
      "Epoch: [1][6000/36908] Elapsed 82m 12s (remain 423m 22s) Loss: 0.0058(0.0238) Grad: 810.7001  LR: 0.000019  \n",
      "Epoch: [1][6100/36908] Elapsed 83m 35s (remain 422m 6s) Loss: 0.0033(0.0235) Grad: 243.4130  LR: 0.000019  \n",
      "Epoch: [1][6200/36908] Elapsed 84m 58s (remain 420m 48s) Loss: 0.0027(0.0232) Grad: 1209.8168  LR: 0.000018  \n",
      "Epoch: [1][6300/36908] Elapsed 86m 20s (remain 419m 26s) Loss: 0.0064(0.0229) Grad: 383.4165  LR: 0.000018  \n",
      "Epoch: [1][6400/36908] Elapsed 87m 42s (remain 418m 1s) Loss: 0.0013(0.0226) Grad: 468.1013  LR: 0.000018  \n",
      "Epoch: [1][6500/36908] Elapsed 89m 5s (remain 416m 43s) Loss: 0.0007(0.0223) Grad: 129.3680  LR: 0.000018  \n",
      "Epoch: [1][6600/36908] Elapsed 90m 29s (remain 415m 26s) Loss: 0.0017(0.0220) Grad: 498.6539  LR: 0.000018  \n",
      "Epoch: [1][6700/36908] Elapsed 91m 52s (remain 414m 7s) Loss: 0.0000(0.0217) Grad: 1.8598  LR: 0.000018  \n",
      "Epoch: [1][6800/36908] Elapsed 93m 15s (remain 412m 49s) Loss: 0.0001(0.0215) Grad: 102.1912  LR: 0.000018  \n",
      "Epoch: [1][6900/36908] Elapsed 94m 38s (remain 411m 32s) Loss: 0.0001(0.0212) Grad: 9.1696  LR: 0.000018  \n",
      "Epoch: [1][7000/36908] Elapsed 96m 0s (remain 410m 9s) Loss: 0.0004(0.0209) Grad: 143.7607  LR: 0.000018  \n",
      "Epoch: [1][7100/36908] Elapsed 97m 23s (remain 408m 49s) Loss: 0.0001(0.0207) Grad: 4.3050  LR: 0.000018  \n",
      "Epoch: [1][7200/36908] Elapsed 98m 47s (remain 407m 33s) Loss: 0.0038(0.0205) Grad: 911.2849  LR: 0.000018  \n",
      "Epoch: [1][7300/36908] Elapsed 100m 10s (remain 406m 13s) Loss: 0.0003(0.0202) Grad: 21.0605  LR: 0.000018  \n",
      "Epoch: [1][7400/36908] Elapsed 101m 33s (remain 404m 55s) Loss: 0.0018(0.0200) Grad: 545.7760  LR: 0.000018  \n",
      "Epoch: [1][7500/36908] Elapsed 102m 55s (remain 403m 31s) Loss: 0.0099(0.0198) Grad: 637.4899  LR: 0.000018  \n",
      "Epoch: [1][7600/36908] Elapsed 104m 17s (remain 402m 7s) Loss: 0.0000(0.0196) Grad: 19.3600  LR: 0.000018  \n",
      "Epoch: [1][7700/36908] Elapsed 105m 40s (remain 400m 47s) Loss: 0.0024(0.0194) Grad: 770.4390  LR: 0.000018  \n",
      "Epoch: [1][7800/36908] Elapsed 107m 2s (remain 399m 22s) Loss: 0.0001(0.0192) Grad: 17.9004  LR: 0.000018  \n",
      "Epoch: [1][7900/36908] Elapsed 108m 24s (remain 397m 58s) Loss: 0.0003(0.0190) Grad: 10.3991  LR: 0.000017  \n",
      "Epoch: [1][8000/36908] Elapsed 109m 47s (remain 396m 39s) Loss: 0.0017(0.0188) Grad: 331.0536  LR: 0.000017  \n",
      "Epoch: [1][8100/36908] Elapsed 111m 9s (remain 395m 17s) Loss: 0.0234(0.0186) Grad: 3518.7844  LR: 0.000017  \n",
      "Epoch: [1][8200/36908] Elapsed 112m 31s (remain 393m 54s) Loss: 0.0001(0.0184) Grad: 20.7264  LR: 0.000017  \n",
      "Epoch: [1][8300/36908] Elapsed 113m 54s (remain 392m 33s) Loss: 0.0033(0.0182) Grad: 909.9654  LR: 0.000017  \n",
      "Epoch: [1][8400/36908] Elapsed 115m 17s (remain 391m 12s) Loss: 0.0010(0.0181) Grad: 324.8549  LR: 0.000017  \n",
      "Epoch: [1][8500/36908] Elapsed 116m 39s (remain 389m 49s) Loss: 0.0007(0.0179) Grad: 581.4806  LR: 0.000017  \n",
      "Epoch: [1][8600/36908] Elapsed 118m 1s (remain 388m 26s) Loss: 0.0001(0.0177) Grad: 104.3348  LR: 0.000017  \n",
      "Epoch: [1][8700/36908] Elapsed 119m 23s (remain 387m 3s) Loss: 0.0026(0.0176) Grad: 880.0557  LR: 0.000017  \n",
      "Epoch: [1][8800/36908] Elapsed 120m 45s (remain 385m 38s) Loss: 0.0156(0.0174) Grad: 2862.6016  LR: 0.000017  \n",
      "Epoch: [1][8900/36908] Elapsed 122m 7s (remain 384m 14s) Loss: 0.0297(0.0172) Grad: 6458.3594  LR: 0.000017  \n",
      "Epoch: [1][9000/36908] Elapsed 123m 30s (remain 382m 54s) Loss: 0.0025(0.0171) Grad: 1264.1859  LR: 0.000017  \n",
      "Epoch: [1][9100/36908] Elapsed 124m 52s (remain 381m 32s) Loss: 0.0214(0.0169) Grad: 4031.0115  LR: 0.000017  \n",
      "Epoch: [1][9200/36908] Elapsed 126m 14s (remain 380m 8s) Loss: 0.0001(0.0168) Grad: 90.9778  LR: 0.000017  \n",
      "Epoch: [1][9300/36908] Elapsed 127m 35s (remain 378m 43s) Loss: 0.0016(0.0167) Grad: 1136.2394  LR: 0.000017  \n",
      "Epoch: [1][9400/36908] Elapsed 128m 57s (remain 377m 20s) Loss: 0.0004(0.0165) Grad: 27.1871  LR: 0.000017  \n",
      "Epoch: [1][9500/36908] Elapsed 130m 20s (remain 375m 58s) Loss: 0.0003(0.0164) Grad: 487.2823  LR: 0.000017  \n",
      "Epoch: [1][9600/36908] Elapsed 131m 41s (remain 374m 33s) Loss: 0.0011(0.0162) Grad: 76.9463  LR: 0.000016  \n",
      "Epoch: [1][9700/36908] Elapsed 133m 2s (remain 373m 7s) Loss: 0.0065(0.0161) Grad: 2261.5198  LR: 0.000016  \n",
      "Epoch: [1][9800/36908] Elapsed 134m 25s (remain 371m 45s) Loss: 0.0002(0.0160) Grad: 174.5163  LR: 0.000016  \n",
      "Epoch: [1][9900/36908] Elapsed 135m 46s (remain 370m 22s) Loss: 0.0052(0.0159) Grad: 360.5612  LR: 0.000016  \n",
      "Epoch: [1][10000/36908] Elapsed 137m 9s (remain 368m 59s) Loss: 0.0017(0.0157) Grad: 1317.7762  LR: 0.000016  \n",
      "Epoch: [1][10100/36908] Elapsed 138m 32s (remain 367m 41s) Loss: 0.0061(0.0156) Grad: 745.8370  LR: 0.000016  \n",
      "Epoch: [1][10200/36908] Elapsed 139m 55s (remain 366m 20s) Loss: 0.0004(0.0155) Grad: 314.2765  LR: 0.000016  \n",
      "Epoch: [1][10300/36908] Elapsed 141m 17s (remain 364m 56s) Loss: 0.0002(0.0154) Grad: 83.5693  LR: 0.000016  \n",
      "Epoch: [1][10400/36908] Elapsed 142m 40s (remain 363m 36s) Loss: 0.0043(0.0153) Grad: 1599.6538  LR: 0.000016  \n",
      "Epoch: [1][10500/36908] Elapsed 144m 3s (remain 362m 15s) Loss: 0.0033(0.0151) Grad: 845.6562  LR: 0.000016  \n",
      "Epoch: [1][10600/36908] Elapsed 145m 27s (remain 360m 57s) Loss: 0.0089(0.0150) Grad: 1921.2485  LR: 0.000016  \n",
      "Epoch: [1][10700/36908] Elapsed 146m 50s (remain 359m 36s) Loss: 0.0020(0.0149) Grad: 763.0964  LR: 0.000016  \n",
      "Epoch: [1][10800/36908] Elapsed 148m 12s (remain 358m 12s) Loss: 0.0007(0.0148) Grad: 116.0683  LR: 0.000016  \n",
      "Epoch: [1][10900/36908] Elapsed 149m 35s (remain 356m 53s) Loss: 0.0018(0.0147) Grad: 494.7229  LR: 0.000016  \n",
      "Epoch: [1][11000/36908] Elapsed 150m 58s (remain 355m 31s) Loss: 0.0001(0.0146) Grad: 12.3487  LR: 0.000016  \n",
      "Epoch: [1][11100/36908] Elapsed 152m 20s (remain 354m 9s) Loss: 0.0091(0.0145) Grad: 2852.8469  LR: 0.000016  \n",
      "Epoch: [1][11200/36908] Elapsed 153m 43s (remain 352m 48s) Loss: 0.0002(0.0144) Grad: 95.7459  LR: 0.000015  \n",
      "Epoch: [1][11300/36908] Elapsed 155m 5s (remain 351m 25s) Loss: 0.0010(0.0143) Grad: 129.7598  LR: 0.000015  \n",
      "Epoch: [1][11400/36908] Elapsed 156m 27s (remain 350m 1s) Loss: 0.0046(0.0142) Grad: 2902.7273  LR: 0.000015  \n",
      "Epoch: [1][11500/36908] Elapsed 157m 49s (remain 348m 38s) Loss: 0.0008(0.0141) Grad: 161.6794  LR: 0.000015  \n",
      "Epoch: [1][11600/36908] Elapsed 159m 12s (remain 347m 17s) Loss: 0.0131(0.0140) Grad: 7081.7344  LR: 0.000015  \n",
      "Epoch: [1][11700/36908] Elapsed 160m 35s (remain 345m 56s) Loss: 0.0003(0.0139) Grad: 591.0294  LR: 0.000015  \n",
      "Epoch: [1][11800/36908] Elapsed 161m 57s (remain 344m 34s) Loss: 0.0037(0.0138) Grad: 1922.1365  LR: 0.000015  \n",
      "Epoch: [1][11900/36908] Elapsed 163m 19s (remain 343m 11s) Loss: 0.0009(0.0137) Grad: 440.5054  LR: 0.000015  \n",
      "Epoch: [1][12000/36908] Elapsed 164m 42s (remain 341m 50s) Loss: 0.0029(0.0136) Grad: 1745.7330  LR: 0.000015  \n",
      "Epoch: [1][12100/36908] Elapsed 166m 5s (remain 340m 30s) Loss: 0.0036(0.0136) Grad: 591.9299  LR: 0.000015  \n",
      "Epoch: [1][12200/36908] Elapsed 167m 29s (remain 339m 9s) Loss: 0.0063(0.0135) Grad: 2528.4561  LR: 0.000015  \n",
      "Epoch: [1][12300/36908] Elapsed 168m 52s (remain 337m 49s) Loss: 0.0001(0.0134) Grad: 60.6342  LR: 0.000015  \n",
      "Epoch: [1][12400/36908] Elapsed 170m 14s (remain 336m 26s) Loss: 0.0041(0.0133) Grad: 98.3342  LR: 0.000015  \n",
      "Epoch: [1][12500/36908] Elapsed 171m 36s (remain 335m 2s) Loss: 0.0029(0.0132) Grad: 544.1688  LR: 0.000015  \n",
      "Epoch: [1][12600/36908] Elapsed 172m 58s (remain 333m 39s) Loss: 0.0007(0.0131) Grad: 507.9161  LR: 0.000015  \n",
      "Epoch: [1][12700/36908] Elapsed 174m 20s (remain 332m 16s) Loss: 0.0002(0.0131) Grad: 20.0568  LR: 0.000015  \n",
      "Epoch: [1][12800/36908] Elapsed 175m 42s (remain 330m 54s) Loss: 0.0011(0.0130) Grad: 67.5048  LR: 0.000015  \n",
      "Epoch: [1][12900/36908] Elapsed 177m 4s (remain 329m 31s) Loss: 0.0023(0.0129) Grad: 1854.4800  LR: 0.000014  \n",
      "Epoch: [1][13000/36908] Elapsed 178m 26s (remain 328m 8s) Loss: 0.0014(0.0128) Grad: 991.8057  LR: 0.000014  \n",
      "Epoch: [1][13100/36908] Elapsed 179m 49s (remain 326m 46s) Loss: 0.0002(0.0128) Grad: 72.9432  LR: 0.000014  \n",
      "Epoch: [1][13200/36908] Elapsed 181m 12s (remain 325m 25s) Loss: 0.0002(0.0127) Grad: 365.2624  LR: 0.000014  \n",
      "Epoch: [1][13300/36908] Elapsed 182m 34s (remain 324m 3s) Loss: 0.0010(0.0126) Grad: 902.2277  LR: 0.000014  \n",
      "Epoch: [1][13400/36908] Elapsed 183m 57s (remain 322m 41s) Loss: 0.0025(0.0125) Grad: 1483.1228  LR: 0.000014  \n",
      "Epoch: [1][13500/36908] Elapsed 185m 20s (remain 321m 19s) Loss: 0.0002(0.0125) Grad: 41.2925  LR: 0.000014  \n",
      "Epoch: [1][13600/36908] Elapsed 186m 44s (remain 320m 0s) Loss: 0.0012(0.0124) Grad: 1282.0538  LR: 0.000014  \n",
      "Epoch: [1][13700/36908] Elapsed 188m 8s (remain 318m 40s) Loss: 0.0005(0.0123) Grad: 601.8408  LR: 0.000014  \n",
      "Epoch: [1][13800/36908] Elapsed 189m 31s (remain 317m 18s) Loss: 0.0003(0.0123) Grad: 55.1309  LR: 0.000014  \n",
      "Epoch: [1][13900/36908] Elapsed 190m 53s (remain 315m 56s) Loss: 0.0028(0.0122) Grad: 1219.9680  LR: 0.000014  \n",
      "Epoch: [1][14000/36908] Elapsed 192m 16s (remain 314m 35s) Loss: 0.0049(0.0121) Grad: 4908.0762  LR: 0.000014  \n",
      "Epoch: [1][14100/36908] Elapsed 193m 40s (remain 313m 14s) Loss: 0.0013(0.0121) Grad: 1492.2905  LR: 0.000014  \n",
      "Epoch: [1][14200/36908] Elapsed 195m 2s (remain 311m 52s) Loss: 0.0053(0.0120) Grad: 9418.2568  LR: 0.000014  \n",
      "Epoch: [1][14300/36908] Elapsed 196m 25s (remain 310m 30s) Loss: 0.0002(0.0119) Grad: 211.6555  LR: 0.000014  \n",
      "Epoch: [1][14400/36908] Elapsed 197m 48s (remain 309m 9s) Loss: 0.0001(0.0119) Grad: 19.9898  LR: 0.000014  \n",
      "Epoch: [1][14500/36908] Elapsed 199m 12s (remain 307m 49s) Loss: 0.0025(0.0118) Grad: 3132.0854  LR: 0.000013  \n",
      "Epoch: [1][14600/36908] Elapsed 200m 35s (remain 306m 27s) Loss: 0.0001(0.0117) Grad: 11.9694  LR: 0.000013  \n",
      "Epoch: [1][14700/36908] Elapsed 201m 58s (remain 305m 6s) Loss: 0.0002(0.0117) Grad: 88.6793  LR: 0.000013  \n",
      "Epoch: [1][14800/36908] Elapsed 203m 20s (remain 303m 43s) Loss: 0.0026(0.0116) Grad: 812.0225  LR: 0.000013  \n",
      "Epoch: [1][14900/36908] Elapsed 204m 43s (remain 302m 20s) Loss: 0.0003(0.0116) Grad: 37.0453  LR: 0.000013  \n",
      "Epoch: [1][15000/36908] Elapsed 206m 6s (remain 300m 59s) Loss: 0.0037(0.0115) Grad: 13454.1152  LR: 0.000013  \n",
      "Epoch: [1][15100/36908] Elapsed 207m 28s (remain 299m 35s) Loss: 0.0005(0.0114) Grad: 535.6286  LR: 0.000013  \n",
      "Epoch: [1][15200/36908] Elapsed 208m 50s (remain 298m 13s) Loss: 0.0032(0.0114) Grad: 1328.3289  LR: 0.000013  \n",
      "Epoch: [1][15300/36908] Elapsed 210m 12s (remain 296m 50s) Loss: 0.0002(0.0114) Grad: 157.2983  LR: 0.000013  \n",
      "Epoch: [1][15400/36908] Elapsed 211m 36s (remain 295m 30s) Loss: 0.0031(0.0113) Grad: 1295.8334  LR: 0.000013  \n",
      "Epoch: [1][15500/36908] Elapsed 212m 59s (remain 294m 8s) Loss: 0.0001(0.0112) Grad: 7.8034  LR: 0.000013  \n",
      "Epoch: [1][15600/36908] Elapsed 214m 21s (remain 292m 45s) Loss: 0.0001(0.0112) Grad: 22.7066  LR: 0.000013  \n",
      "Epoch: [1][15700/36908] Elapsed 215m 44s (remain 291m 24s) Loss: 0.0028(0.0111) Grad: 1622.9625  LR: 0.000013  \n",
      "Epoch: [1][15800/36908] Elapsed 217m 8s (remain 290m 3s) Loss: 0.0004(0.0111) Grad: 276.4208  LR: 0.000013  \n",
      "Epoch: [1][15900/36908] Elapsed 218m 30s (remain 288m 39s) Loss: 0.0003(0.0110) Grad: 148.3682  LR: 0.000013  \n",
      "Epoch: [1][16000/36908] Elapsed 219m 51s (remain 287m 15s) Loss: 0.0058(0.0110) Grad: 730.7637  LR: 0.000013  \n",
      "Epoch: [1][16100/36908] Elapsed 221m 12s (remain 285m 52s) Loss: 0.0001(0.0109) Grad: 25.4640  LR: 0.000013  \n",
      "Epoch: [1][16200/36908] Elapsed 222m 35s (remain 284m 30s) Loss: 0.0048(0.0109) Grad: 5939.7812  LR: 0.000012  \n",
      "Epoch: [1][16300/36908] Elapsed 223m 57s (remain 283m 7s) Loss: 0.0083(0.0108) Grad: 2362.3501  LR: 0.000012  \n",
      "Epoch: [1][16400/36908] Elapsed 225m 19s (remain 281m 44s) Loss: 0.0005(0.0108) Grad: 577.5505  LR: 0.000012  \n",
      "Epoch: [1][16500/36908] Elapsed 226m 42s (remain 280m 22s) Loss: 0.0060(0.0107) Grad: 5546.2710  LR: 0.000012  \n",
      "Epoch: [1][16600/36908] Elapsed 228m 6s (remain 279m 1s) Loss: 0.0018(0.0107) Grad: 169.9619  LR: 0.000012  \n",
      "Epoch: [1][16700/36908] Elapsed 229m 29s (remain 277m 39s) Loss: 0.0000(0.0106) Grad: 6.0810  LR: 0.000012  \n",
      "Epoch: [1][16800/36908] Elapsed 230m 51s (remain 276m 17s) Loss: 0.0033(0.0106) Grad: 959.9182  LR: 0.000012  \n",
      "Epoch: [1][16900/36908] Elapsed 232m 15s (remain 274m 56s) Loss: 0.0082(0.0106) Grad: 8796.1494  LR: 0.000012  \n",
      "Epoch: [1][17000/36908] Elapsed 233m 38s (remain 273m 34s) Loss: 0.0000(0.0105) Grad: 27.9923  LR: 0.000012  \n",
      "Epoch: [1][17100/36908] Elapsed 235m 0s (remain 272m 12s) Loss: 0.0000(0.0105) Grad: 9.1235  LR: 0.000012  \n",
      "Epoch: [1][17200/36908] Elapsed 236m 23s (remain 270m 49s) Loss: 0.0002(0.0104) Grad: 697.2517  LR: 0.000012  \n",
      "Epoch: [1][17300/36908] Elapsed 237m 46s (remain 269m 27s) Loss: 0.0001(0.0104) Grad: 79.5680  LR: 0.000012  \n",
      "Epoch: [1][17400/36908] Elapsed 239m 9s (remain 268m 5s) Loss: 0.0037(0.0103) Grad: 4004.3877  LR: 0.000012  \n",
      "Epoch: [1][17500/36908] Elapsed 240m 31s (remain 266m 43s) Loss: 0.0003(0.0103) Grad: 734.0192  LR: 0.000012  \n",
      "Epoch: [1][17600/36908] Elapsed 241m 53s (remain 265m 20s) Loss: 0.0105(0.0102) Grad: 16374.1709  LR: 0.000012  \n",
      "Epoch: [1][17700/36908] Elapsed 243m 15s (remain 263m 57s) Loss: 0.0010(0.0102) Grad: 458.0548  LR: 0.000012  \n",
      "Epoch: [1][17800/36908] Elapsed 244m 38s (remain 262m 35s) Loss: 0.0001(0.0102) Grad: 21.8236  LR: 0.000012  \n",
      "Epoch: [1][17900/36908] Elapsed 246m 1s (remain 261m 13s) Loss: 0.0000(0.0101) Grad: 16.7753  LR: 0.000011  \n",
      "Epoch: [1][18000/36908] Elapsed 247m 24s (remain 259m 51s) Loss: 0.0044(0.0101) Grad: 1345.1287  LR: 0.000011  \n",
      "Epoch: [1][18100/36908] Elapsed 248m 46s (remain 258m 28s) Loss: 0.0024(0.0100) Grad: 3412.7400  LR: 0.000011  \n",
      "Epoch: [1][18200/36908] Elapsed 250m 8s (remain 257m 6s) Loss: 0.0013(0.0100) Grad: 2098.2209  LR: 0.000011  \n",
      "Epoch: [1][18300/36908] Elapsed 251m 31s (remain 255m 43s) Loss: 0.0075(0.0100) Grad: 20576.0566  LR: 0.000011  \n",
      "Epoch: [1][18400/36908] Elapsed 252m 54s (remain 254m 22s) Loss: 0.0004(0.0099) Grad: 1317.0460  LR: 0.000011  \n",
      "Epoch: [1][18500/36908] Elapsed 254m 16s (remain 252m 59s) Loss: 0.0006(0.0099) Grad: 957.9153  LR: 0.000011  \n",
      "Epoch: [1][18600/36908] Elapsed 255m 39s (remain 251m 37s) Loss: 0.0002(0.0098) Grad: 116.2098  LR: 0.000011  \n",
      "Epoch: [1][18700/36908] Elapsed 257m 1s (remain 250m 14s) Loss: 0.0030(0.0098) Grad: 2036.9263  LR: 0.000011  \n",
      "Epoch: [1][18800/36908] Elapsed 258m 25s (remain 248m 52s) Loss: 0.0031(0.0098) Grad: 454.1808  LR: 0.000011  \n",
      "Epoch: [1][18900/36908] Elapsed 259m 47s (remain 247m 30s) Loss: 0.0010(0.0097) Grad: 1457.8617  LR: 0.000011  \n",
      "Epoch: [1][19000/36908] Elapsed 261m 10s (remain 246m 8s) Loss: 0.0002(0.0097) Grad: 14.1940  LR: 0.000011  \n",
      "Epoch: [1][19100/36908] Elapsed 262m 33s (remain 244m 46s) Loss: 0.0001(0.0097) Grad: 174.0920  LR: 0.000011  \n",
      "Epoch: [1][19200/36908] Elapsed 263m 56s (remain 243m 24s) Loss: 0.0002(0.0096) Grad: 699.6703  LR: 0.000011  \n",
      "Epoch: [1][19300/36908] Elapsed 265m 19s (remain 242m 2s) Loss: 0.0004(0.0096) Grad: 1577.4622  LR: 0.000011  \n",
      "Epoch: [1][19400/36908] Elapsed 266m 43s (remain 240m 41s) Loss: 0.0040(0.0095) Grad: 13164.0205  LR: 0.000011  \n",
      "Epoch: [1][19500/36908] Elapsed 268m 6s (remain 239m 19s) Loss: 0.0009(0.0095) Grad: 1284.4841  LR: 0.000010  \n",
      "Epoch: [1][19600/36908] Elapsed 269m 29s (remain 237m 57s) Loss: 0.0009(0.0095) Grad: 1485.6044  LR: 0.000010  \n",
      "Epoch: [1][19700/36908] Elapsed 270m 52s (remain 236m 34s) Loss: 0.0052(0.0094) Grad: 6098.2280  LR: 0.000010  \n",
      "Epoch: [1][19800/36908] Elapsed 272m 14s (remain 235m 12s) Loss: 0.0001(0.0094) Grad: 554.0724  LR: 0.000010  \n",
      "Epoch: [1][19900/36908] Elapsed 273m 37s (remain 233m 50s) Loss: 0.0002(0.0094) Grad: 784.2055  LR: 0.000010  \n",
      "Epoch: [1][20000/36908] Elapsed 275m 0s (remain 232m 27s) Loss: 0.0007(0.0093) Grad: 138.4765  LR: 0.000010  \n",
      "Epoch: [1][20100/36908] Elapsed 276m 23s (remain 231m 5s) Loss: 0.0006(0.0093) Grad: 1328.8246  LR: 0.000010  \n",
      "Epoch: [1][20200/36908] Elapsed 277m 45s (remain 229m 43s) Loss: 0.0000(0.0093) Grad: 13.8279  LR: 0.000010  \n",
      "Epoch: [1][20300/36908] Elapsed 279m 8s (remain 228m 21s) Loss: 0.0002(0.0092) Grad: 263.9818  LR: 0.000010  \n",
      "Epoch: [1][20400/36908] Elapsed 280m 31s (remain 226m 58s) Loss: 0.0080(0.0092) Grad: 7967.2495  LR: 0.000010  \n",
      "Epoch: [1][20500/36908] Elapsed 281m 53s (remain 225m 36s) Loss: 0.0042(0.0092) Grad: 4651.4043  LR: 0.000010  \n",
      "Epoch: [1][20600/36908] Elapsed 283m 17s (remain 224m 14s) Loss: 0.0000(0.0091) Grad: 9.1331  LR: 0.000010  \n",
      "Epoch: [1][20700/36908] Elapsed 284m 39s (remain 222m 51s) Loss: 0.0003(0.0091) Grad: 89.2074  LR: 0.000010  \n",
      "Epoch: [1][20800/36908] Elapsed 286m 2s (remain 221m 29s) Loss: 0.0031(0.0091) Grad: 251.6339  LR: 0.000010  \n",
      "Epoch: [1][20900/36908] Elapsed 287m 25s (remain 220m 7s) Loss: 0.0057(0.0090) Grad: 17959.6309  LR: 0.000010  \n",
      "Epoch: [1][21000/36908] Elapsed 288m 47s (remain 218m 44s) Loss: 0.0001(0.0090) Grad: 107.8567  LR: 0.000010  \n",
      "Epoch: [1][21100/36908] Elapsed 290m 12s (remain 217m 23s) Loss: 0.0011(0.0090) Grad: 1250.3971  LR: 0.000010  \n",
      "Epoch: [1][21200/36908] Elapsed 291m 35s (remain 216m 1s) Loss: 0.0006(0.0090) Grad: 4135.4614  LR: 0.000009  \n",
      "Epoch: [1][21300/36908] Elapsed 292m 57s (remain 214m 39s) Loss: 0.0060(0.0089) Grad: 18338.6270  LR: 0.000009  \n",
      "Epoch: [1][21400/36908] Elapsed 294m 19s (remain 213m 16s) Loss: 0.0001(0.0089) Grad: 383.6812  LR: 0.000009  \n",
      "Epoch: [1][21500/36908] Elapsed 295m 42s (remain 211m 53s) Loss: 0.0001(0.0089) Grad: 352.9026  LR: 0.000009  \n",
      "Epoch: [1][21600/36908] Elapsed 297m 5s (remain 210m 31s) Loss: 0.0196(0.0088) Grad: 52017.1875  LR: 0.000009  \n",
      "Epoch: [1][21700/36908] Elapsed 298m 28s (remain 209m 9s) Loss: 0.0011(0.0088) Grad: 1876.2051  LR: 0.000009  \n",
      "Epoch: [1][21800/36908] Elapsed 299m 50s (remain 207m 46s) Loss: 0.0009(0.0088) Grad: 534.5164  LR: 0.000009  \n",
      "Epoch: [1][21900/36908] Elapsed 301m 13s (remain 206m 24s) Loss: 0.0051(0.0088) Grad: 81098.5469  LR: 0.000009  \n",
      "Epoch: [1][22000/36908] Elapsed 302m 36s (remain 205m 2s) Loss: 0.0180(0.0087) Grad: 70920.0391  LR: 0.000009  \n",
      "Epoch: [1][22100/36908] Elapsed 303m 58s (remain 203m 38s) Loss: 0.0002(0.0087) Grad: 73.4691  LR: 0.000009  \n",
      "Epoch: [1][22200/36908] Elapsed 305m 20s (remain 202m 16s) Loss: 0.0191(0.0087) Grad: 69931.2969  LR: 0.000009  \n",
      "Epoch: [1][22300/36908] Elapsed 306m 43s (remain 200m 53s) Loss: 0.0000(0.0086) Grad: 10.5091  LR: 0.000009  \n",
      "Epoch: [1][22400/36908] Elapsed 308m 5s (remain 199m 31s) Loss: 0.0025(0.0086) Grad: 8048.4395  LR: 0.000009  \n",
      "Epoch: [1][22500/36908] Elapsed 309m 28s (remain 198m 8s) Loss: 0.0087(0.0086) Grad: 11931.6787  LR: 0.000009  \n",
      "Epoch: [1][22600/36908] Elapsed 310m 52s (remain 196m 47s) Loss: 0.0001(0.0086) Grad: 12.7406  LR: 0.000009  \n",
      "Epoch: [1][22700/36908] Elapsed 312m 16s (remain 195m 25s) Loss: 0.0002(0.0085) Grad: 107.7104  LR: 0.000009  \n",
      "Epoch: [1][22800/36908] Elapsed 313m 40s (remain 194m 4s) Loss: 0.0032(0.0085) Grad: 11945.1572  LR: 0.000008  \n",
      "Epoch: [1][22900/36908] Elapsed 315m 3s (remain 192m 41s) Loss: 0.0001(0.0085) Grad: 87.4740  LR: 0.000008  \n",
      "Epoch: [1][23000/36908] Elapsed 316m 26s (remain 191m 19s) Loss: 0.0016(0.0085) Grad: 4432.9468  LR: 0.000008  \n",
      "Epoch: [1][23100/36908] Elapsed 317m 49s (remain 189m 57s) Loss: 0.0009(0.0084) Grad: 6222.6689  LR: 0.000008  \n",
      "Epoch: [1][23200/36908] Elapsed 319m 12s (remain 188m 34s) Loss: 0.0074(0.0084) Grad: 9898.8945  LR: 0.000008  \n",
      "Epoch: [1][23300/36908] Elapsed 320m 36s (remain 187m 13s) Loss: 0.0018(0.0084) Grad: 3641.6116  LR: 0.000008  \n",
      "Epoch: [1][23400/36908] Elapsed 321m 59s (remain 185m 51s) Loss: 0.0004(0.0084) Grad: 549.4545  LR: 0.000008  \n",
      "Epoch: [1][23500/36908] Elapsed 323m 22s (remain 184m 28s) Loss: 0.0001(0.0083) Grad: 187.5072  LR: 0.000008  \n",
      "Epoch: [1][23600/36908] Elapsed 324m 45s (remain 183m 6s) Loss: 0.0028(0.0083) Grad: 4716.6304  LR: 0.000008  \n",
      "Epoch: [1][23700/36908] Elapsed 326m 9s (remain 181m 44s) Loss: 0.0077(0.0083) Grad: 3371.4189  LR: 0.000008  \n",
      "Epoch: [1][23800/36908] Elapsed 327m 33s (remain 180m 22s) Loss: 0.0045(0.0083) Grad: 2545.9417  LR: 0.000008  \n",
      "Epoch: [1][23900/36908] Elapsed 328m 56s (remain 179m 0s) Loss: 0.0149(0.0082) Grad: 19386.2930  LR: 0.000008  \n",
      "Epoch: [1][24000/36908] Elapsed 330m 19s (remain 177m 38s) Loss: 0.0050(0.0082) Grad: 10014.9980  LR: 0.000008  \n",
      "Epoch: [1][24100/36908] Elapsed 331m 41s (remain 176m 15s) Loss: 0.0021(0.0082) Grad: 26467.7969  LR: 0.000008  \n",
      "Epoch: [1][24200/36908] Elapsed 333m 4s (remain 174m 52s) Loss: 0.0002(0.0082) Grad: 226.5771  LR: 0.000008  \n",
      "Epoch: [1][24300/36908] Elapsed 334m 26s (remain 173m 30s) Loss: 0.0022(0.0082) Grad: 9301.7227  LR: 0.000008  \n",
      "Epoch: [1][24400/36908] Elapsed 335m 49s (remain 172m 8s) Loss: 0.0005(0.0081) Grad: 374.3161  LR: 0.000008  \n",
      "Epoch: [1][24500/36908] Elapsed 337m 13s (remain 170m 45s) Loss: 0.0042(0.0081) Grad: 8225.3193  LR: 0.000007  \n",
      "Epoch: [1][24600/36908] Elapsed 338m 35s (remain 169m 23s) Loss: 0.0005(0.0081) Grad: 6496.3428  LR: 0.000007  \n",
      "Epoch: [1][24700/36908] Elapsed 339m 58s (remain 168m 0s) Loss: 0.0047(0.0081) Grad: 4433.1729  LR: 0.000007  \n",
      "Epoch: [1][24800/36908] Elapsed 341m 23s (remain 166m 39s) Loss: 0.0001(0.0080) Grad: 142.6468  LR: 0.000007  \n",
      "Epoch: [1][24900/36908] Elapsed 342m 46s (remain 165m 16s) Loss: 0.0038(0.0080) Grad: 9160.3594  LR: 0.000007  \n",
      "Epoch: [1][25000/36908] Elapsed 344m 9s (remain 163m 54s) Loss: 0.0021(0.0080) Grad: 6987.2256  LR: 0.000007  \n",
      "Epoch: [1][25100/36908] Elapsed 345m 34s (remain 162m 33s) Loss: 0.0023(0.0080) Grad: 6947.5015  LR: 0.000007  \n",
      "Epoch: [1][25200/36908] Elapsed 346m 58s (remain 161m 11s) Loss: 0.0002(0.0080) Grad: 1365.2164  LR: 0.000007  \n",
      "Epoch: [1][25300/36908] Elapsed 348m 21s (remain 159m 48s) Loss: 0.0084(0.0079) Grad: 29467.2969  LR: 0.000007  \n",
      "Epoch: [1][25400/36908] Elapsed 349m 44s (remain 158m 26s) Loss: 0.0000(0.0079) Grad: 42.3307  LR: 0.000007  \n",
      "Epoch: [1][25500/36908] Elapsed 351m 6s (remain 157m 3s) Loss: 0.0023(0.0079) Grad: 7930.0522  LR: 0.000007  \n",
      "Epoch: [1][25600/36908] Elapsed 352m 28s (remain 155m 40s) Loss: 0.0001(0.0079) Grad: 119.0895  LR: 0.000007  \n",
      "Epoch: [1][25700/36908] Elapsed 353m 51s (remain 154m 18s) Loss: 0.0075(0.0079) Grad: 28673.9004  LR: 0.000007  \n",
      "Epoch: [1][25800/36908] Elapsed 355m 15s (remain 152m 55s) Loss: 0.0000(0.0078) Grad: 16.9693  LR: 0.000007  \n",
      "Epoch: [1][25900/36908] Elapsed 356m 37s (remain 151m 33s) Loss: 0.0018(0.0078) Grad: 6310.4619  LR: 0.000007  \n",
      "Epoch: [1][26000/36908] Elapsed 358m 0s (remain 150m 10s) Loss: 0.0030(0.0078) Grad: 5727.7949  LR: 0.000007  \n",
      "Epoch: [1][26100/36908] Elapsed 359m 22s (remain 148m 47s) Loss: 0.0150(0.0078) Grad: 178483.7656  LR: 0.000007  \n",
      "Epoch: [1][26200/36908] Elapsed 360m 44s (remain 147m 24s) Loss: 0.0001(0.0077) Grad: 1523.4778  LR: 0.000006  \n",
      "Epoch: [1][26300/36908] Elapsed 362m 7s (remain 146m 2s) Loss: 0.0003(0.0077) Grad: 173.6683  LR: 0.000006  \n",
      "Epoch: [1][26400/36908] Elapsed 363m 31s (remain 144m 40s) Loss: 0.0034(0.0077) Grad: 21716.1602  LR: 0.000006  \n",
      "Epoch: [1][26500/36908] Elapsed 364m 55s (remain 143m 18s) Loss: 0.0035(0.0077) Grad: 65602.2188  LR: 0.000006  \n",
      "Epoch: [1][26600/36908] Elapsed 366m 18s (remain 141m 55s) Loss: 0.0002(0.0077) Grad: 635.9477  LR: 0.000006  \n",
      "Epoch: [1][26700/36908] Elapsed 367m 39s (remain 140m 32s) Loss: 0.0068(0.0077) Grad: 37458.8672  LR: 0.000006  \n",
      "Epoch: [1][26800/36908] Elapsed 369m 3s (remain 139m 10s) Loss: 0.0076(0.0076) Grad: 20019.9922  LR: 0.000006  \n",
      "Epoch: [1][26900/36908] Elapsed 370m 27s (remain 137m 48s) Loss: 0.0002(0.0076) Grad: 1795.1963  LR: 0.000006  \n",
      "Epoch: [1][27000/36908] Elapsed 371m 50s (remain 136m 25s) Loss: 0.0013(0.0076) Grad: 15104.6748  LR: 0.000006  \n",
      "Epoch: [1][27100/36908] Elapsed 373m 11s (remain 135m 2s) Loss: 0.0012(0.0076) Grad: 5189.5703  LR: 0.000006  \n",
      "Epoch: [1][27200/36908] Elapsed 374m 33s (remain 133m 40s) Loss: 0.0001(0.0076) Grad: 545.1781  LR: 0.000006  \n",
      "Epoch: [1][27300/36908] Elapsed 375m 55s (remain 132m 17s) Loss: 0.0013(0.0075) Grad: 1369.4594  LR: 0.000006  \n",
      "Epoch: [1][27400/36908] Elapsed 377m 18s (remain 130m 54s) Loss: 0.0001(0.0075) Grad: 2947.2554  LR: 0.000006  \n",
      "Epoch: [1][27500/36908] Elapsed 378m 41s (remain 129m 32s) Loss: 0.0031(0.0075) Grad: 6980.8628  LR: 0.000006  \n",
      "Epoch: [1][27600/36908] Elapsed 380m 3s (remain 128m 9s) Loss: 0.0001(0.0075) Grad: 225.8270  LR: 0.000006  \n",
      "Epoch: [1][27700/36908] Elapsed 381m 26s (remain 126m 46s) Loss: 0.0059(0.0075) Grad: 30836.0098  LR: 0.000006  \n",
      "Epoch: [1][27800/36908] Elapsed 382m 50s (remain 125m 24s) Loss: 0.0057(0.0075) Grad: 36133.7109  LR: 0.000005  \n",
      "Epoch: [1][27900/36908] Elapsed 384m 12s (remain 124m 1s) Loss: 0.0032(0.0074) Grad: 3334.3984  LR: 0.000005  \n",
      "Epoch: [1][28000/36908] Elapsed 385m 35s (remain 122m 39s) Loss: 0.0014(0.0074) Grad: 112.5347  LR: 0.000005  \n",
      "Epoch: [1][28100/36908] Elapsed 386m 59s (remain 121m 16s) Loss: 0.0001(0.0074) Grad: 47.9152  LR: 0.000005  \n",
      "Epoch: [1][28200/36908] Elapsed 388m 22s (remain 119m 54s) Loss: 0.0027(0.0074) Grad: 12493.1162  LR: 0.000005  \n",
      "Epoch: [1][28300/36908] Elapsed 389m 44s (remain 118m 31s) Loss: 0.0000(0.0074) Grad: 27.3195  LR: 0.000005  \n",
      "Epoch: [1][28400/36908] Elapsed 391m 7s (remain 117m 9s) Loss: 0.0016(0.0074) Grad: 4484.1045  LR: 0.000005  \n",
      "Epoch: [1][28500/36908] Elapsed 392m 31s (remain 115m 46s) Loss: 0.0006(0.0073) Grad: 2907.6968  LR: 0.000005  \n",
      "Epoch: [1][28600/36908] Elapsed 393m 54s (remain 114m 24s) Loss: 0.0101(0.0073) Grad: 18287.9531  LR: 0.000005  \n",
      "Epoch: [1][28700/36908] Elapsed 395m 18s (remain 113m 2s) Loss: 0.0019(0.0073) Grad: 1123.2932  LR: 0.000005  \n",
      "Epoch: [1][28800/36908] Elapsed 396m 42s (remain 111m 39s) Loss: 0.0009(0.0073) Grad: 3501.6292  LR: 0.000005  \n",
      "Epoch: [1][28900/36908] Elapsed 398m 4s (remain 110m 17s) Loss: 0.0001(0.0073) Grad: 249.9751  LR: 0.000005  \n",
      "Epoch: [1][29000/36908] Elapsed 399m 28s (remain 108m 54s) Loss: 0.0042(0.0073) Grad: 73654.3906  LR: 0.000005  \n",
      "Epoch: [1][29100/36908] Elapsed 400m 51s (remain 107m 32s) Loss: 0.0028(0.0072) Grad: 57473.6875  LR: 0.000005  \n",
      "Epoch: [1][29200/36908] Elapsed 402m 14s (remain 106m 9s) Loss: 0.0001(0.0072) Grad: 292.4721  LR: 0.000005  \n",
      "Epoch: [1][29300/36908] Elapsed 403m 37s (remain 104m 47s) Loss: 0.0026(0.0072) Grad: 38337.1133  LR: 0.000005  \n",
      "Epoch: [1][29400/36908] Elapsed 405m 1s (remain 103m 24s) Loss: 0.0041(0.0072) Grad: 32620.6016  LR: 0.000005  \n",
      "Epoch: [1][29500/36908] Elapsed 406m 24s (remain 102m 2s) Loss: 0.0000(0.0072) Grad: 114.1405  LR: 0.000004  \n",
      "Epoch: [1][29600/36908] Elapsed 407m 46s (remain 100m 39s) Loss: 0.0021(0.0072) Grad: 37320.9023  LR: 0.000004  \n",
      "Epoch: [1][29700/36908] Elapsed 409m 9s (remain 99m 17s) Loss: 0.0064(0.0072) Grad: 25176.9219  LR: 0.000004  \n",
      "Epoch: [1][29800/36908] Elapsed 410m 32s (remain 97m 54s) Loss: 0.0029(0.0071) Grad: 79901.4531  LR: 0.000004  \n",
      "Epoch: [1][29900/36908] Elapsed 411m 55s (remain 96m 31s) Loss: 0.0056(0.0071) Grad: 116960.1953  LR: 0.000004  \n",
      "Epoch: [1][30000/36908] Elapsed 413m 17s (remain 95m 9s) Loss: 0.0012(0.0071) Grad: 38148.1133  LR: 0.000004  \n",
      "Epoch: [1][30100/36908] Elapsed 414m 42s (remain 93m 46s) Loss: 0.0087(0.0071) Grad: 27719.7441  LR: 0.000004  \n",
      "Epoch: [1][30200/36908] Elapsed 416m 4s (remain 92m 24s) Loss: 0.0004(0.0071) Grad: 1456.5681  LR: 0.000004  \n",
      "Epoch: [1][30300/36908] Elapsed 417m 26s (remain 91m 1s) Loss: 0.0005(0.0071) Grad: 29269.3867  LR: 0.000004  \n",
      "Epoch: [1][30400/36908] Elapsed 418m 49s (remain 89m 38s) Loss: 0.0004(0.0070) Grad: 8364.9863  LR: 0.000004  \n",
      "Epoch: [1][30500/36908] Elapsed 420m 12s (remain 88m 16s) Loss: 0.0028(0.0070) Grad: 30499.7305  LR: 0.000004  \n",
      "Epoch: [1][30600/36908] Elapsed 421m 35s (remain 86m 53s) Loss: 0.0008(0.0070) Grad: 6595.3379  LR: 0.000004  \n",
      "Epoch: [1][30700/36908] Elapsed 422m 59s (remain 85m 31s) Loss: 0.0063(0.0070) Grad: 23105.0449  LR: 0.000004  \n",
      "Epoch: [1][30800/36908] Elapsed 424m 22s (remain 84m 8s) Loss: 0.0002(0.0070) Grad: 76.2934  LR: 0.000004  \n",
      "Epoch: [1][30900/36908] Elapsed 425m 45s (remain 82m 46s) Loss: 0.0002(0.0070) Grad: 86.8720  LR: 0.000004  \n",
      "Epoch: [1][31000/36908] Elapsed 427m 9s (remain 81m 23s) Loss: 0.0034(0.0070) Grad: 28929.0195  LR: 0.000004  \n",
      "Epoch: [1][31100/36908] Elapsed 428m 31s (remain 80m 0s) Loss: 0.0002(0.0069) Grad: 525.1293  LR: 0.000003  \n",
      "Epoch: [1][31200/36908] Elapsed 429m 54s (remain 78m 38s) Loss: 0.0010(0.0069) Grad: 6936.8809  LR: 0.000003  \n",
      "Epoch: [1][31300/36908] Elapsed 431m 17s (remain 77m 15s) Loss: 0.0011(0.0069) Grad: 10956.7686  LR: 0.000003  \n",
      "Epoch: [1][31400/36908] Elapsed 432m 39s (remain 75m 52s) Loss: 0.0000(0.0069) Grad: 64.8117  LR: 0.000003  \n",
      "Epoch: [1][31500/36908] Elapsed 434m 1s (remain 74m 29s) Loss: 0.0082(0.0069) Grad: 75967.6562  LR: 0.000003  \n",
      "Epoch: [1][31600/36908] Elapsed 435m 23s (remain 73m 7s) Loss: 0.0001(0.0069) Grad: 186.7613  LR: 0.000003  \n",
      "Epoch: [1][31700/36908] Elapsed 436m 46s (remain 71m 44s) Loss: 0.0010(0.0069) Grad: 7488.6982  LR: 0.000003  \n",
      "Epoch: [1][31800/36908] Elapsed 438m 8s (remain 70m 21s) Loss: 0.0011(0.0069) Grad: 11587.2783  LR: 0.000003  \n",
      "Epoch: [1][31900/36908] Elapsed 439m 30s (remain 68m 58s) Loss: 0.0055(0.0068) Grad: 73700.1016  LR: 0.000003  \n",
      "Epoch: [1][32000/36908] Elapsed 440m 52s (remain 67m 36s) Loss: 0.0024(0.0068) Grad: 23624.5801  LR: 0.000003  \n",
      "Epoch: [1][32100/36908] Elapsed 442m 16s (remain 66m 13s) Loss: 0.0112(0.0068) Grad: 96645.7109  LR: 0.000003  \n",
      "Epoch: [1][32200/36908] Elapsed 443m 39s (remain 64m 51s) Loss: 0.0003(0.0068) Grad: 580.0339  LR: 0.000003  \n",
      "Epoch: [1][32300/36908] Elapsed 445m 3s (remain 63m 28s) Loss: 0.0051(0.0068) Grad: 62663.3711  LR: 0.000003  \n",
      "Epoch: [1][32400/36908] Elapsed 446m 25s (remain 62m 5s) Loss: 0.0001(0.0068) Grad: 217.4153  LR: 0.000003  \n",
      "Epoch: [1][32500/36908] Elapsed 447m 47s (remain 60m 43s) Loss: 0.0017(0.0068) Grad: 3125.9785  LR: 0.000003  \n",
      "Epoch: [1][32600/36908] Elapsed 449m 9s (remain 59m 20s) Loss: 0.0006(0.0067) Grad: 1419.4271  LR: 0.000003  \n",
      "Epoch: [1][32700/36908] Elapsed 450m 33s (remain 57m 57s) Loss: 0.0000(0.0067) Grad: 48.6893  LR: 0.000003  \n",
      "Epoch: [1][32800/36908] Elapsed 451m 56s (remain 56m 35s) Loss: 0.0002(0.0067) Grad: 214.4691  LR: 0.000002  \n",
      "Epoch: [1][32900/36908] Elapsed 453m 19s (remain 55m 12s) Loss: 0.0014(0.0067) Grad: 32420.8711  LR: 0.000002  \n",
      "Epoch: [1][33000/36908] Elapsed 454m 43s (remain 53m 50s) Loss: 0.0005(0.0067) Grad: 11580.9229  LR: 0.000002  \n",
      "Epoch: [1][33100/36908] Elapsed 456m 6s (remain 52m 27s) Loss: 0.0022(0.0067) Grad: 149798.7656  LR: 0.000002  \n",
      "Epoch: [1][33200/36908] Elapsed 457m 29s (remain 51m 4s) Loss: 0.0055(0.0067) Grad: 126784.7422  LR: 0.000002  \n",
      "Epoch: [1][33300/36908] Elapsed 458m 51s (remain 49m 42s) Loss: 0.0001(0.0067) Grad: 14685.2773  LR: 0.000002  \n",
      "Epoch: [1][33400/36908] Elapsed 460m 14s (remain 48m 19s) Loss: 0.0003(0.0066) Grad: 896.5462  LR: 0.000002  \n",
      "Epoch: [1][33500/36908] Elapsed 461m 36s (remain 46m 56s) Loss: 0.0193(0.0066) Grad: 623377.6875  LR: 0.000002  \n",
      "Epoch: [1][33600/36908] Elapsed 463m 0s (remain 45m 34s) Loss: 0.0001(0.0066) Grad: 90.4092  LR: 0.000002  \n",
      "Epoch: [1][33700/36908] Elapsed 464m 22s (remain 44m 11s) Loss: 0.0002(0.0066) Grad: 495.3870  LR: 0.000002  \n",
      "Epoch: [1][33800/36908] Elapsed 465m 45s (remain 42m 48s) Loss: 0.0021(0.0066) Grad: 35361.8711  LR: 0.000002  \n",
      "Epoch: [1][33900/36908] Elapsed 467m 8s (remain 41m 26s) Loss: 0.0002(0.0066) Grad: 221.5184  LR: 0.000002  \n",
      "Epoch: [1][34000/36908] Elapsed 468m 30s (remain 40m 3s) Loss: 0.0012(0.0066) Grad: 7014.5420  LR: 0.000002  \n",
      "Epoch: [1][34100/36908] Elapsed 469m 52s (remain 38m 40s) Loss: 0.0003(0.0066) Grad: 19424.1777  LR: 0.000002  \n",
      "Epoch: [1][34200/36908] Elapsed 471m 16s (remain 37m 18s) Loss: 0.0016(0.0065) Grad: 38190.2891  LR: 0.000002  \n",
      "Epoch: [1][34300/36908] Elapsed 472m 40s (remain 35m 55s) Loss: 0.0001(0.0065) Grad: 320.9594  LR: 0.000002  \n",
      "Epoch: [1][34400/36908] Elapsed 474m 3s (remain 34m 32s) Loss: 0.0088(0.0065) Grad: 499002.1250  LR: 0.000002  \n",
      "Epoch: [1][34500/36908] Elapsed 475m 26s (remain 33m 10s) Loss: 0.0206(0.0065) Grad: 339622.6562  LR: 0.000001  \n",
      "Epoch: [1][34600/36908] Elapsed 476m 50s (remain 31m 47s) Loss: 0.0002(0.0065) Grad: 16038.7617  LR: 0.000001  \n",
      "Epoch: [1][34700/36908] Elapsed 478m 13s (remain 30m 24s) Loss: 0.0016(0.0065) Grad: 90295.6484  LR: 0.000001  \n",
      "Epoch: [1][34800/36908] Elapsed 479m 36s (remain 29m 2s) Loss: 0.0000(0.0065) Grad: 143.4520  LR: 0.000001  \n",
      "Epoch: [1][34900/36908] Elapsed 481m 1s (remain 27m 39s) Loss: 0.0013(0.0065) Grad: 36202.4766  LR: 0.000001  \n",
      "Epoch: [1][35000/36908] Elapsed 482m 24s (remain 26m 17s) Loss: 0.0003(0.0065) Grad: 5575.5083  LR: 0.000001  \n",
      "Epoch: [1][35100/36908] Elapsed 483m 48s (remain 24m 54s) Loss: 0.0037(0.0064) Grad: 133293.0625  LR: 0.000001  \n",
      "Epoch: [1][35200/36908] Elapsed 485m 12s (remain 23m 31s) Loss: 0.0002(0.0064) Grad: 441.4973  LR: 0.000001  \n",
      "Epoch: [1][35300/36908] Elapsed 486m 36s (remain 22m 9s) Loss: 0.0002(0.0064) Grad: 171.2269  LR: 0.000001  \n",
      "Epoch: [1][35400/36908] Elapsed 487m 59s (remain 20m 46s) Loss: 0.0023(0.0064) Grad: 74361.8672  LR: 0.000001  \n",
      "Epoch: [1][35500/36908] Elapsed 489m 21s (remain 19m 23s) Loss: 0.0009(0.0064) Grad: 374.3090  LR: 0.000001  \n",
      "Epoch: [1][35600/36908] Elapsed 490m 46s (remain 18m 1s) Loss: 0.0001(0.0064) Grad: 103.7152  LR: 0.000001  \n",
      "Epoch: [1][35700/36908] Elapsed 492m 10s (remain 16m 38s) Loss: 0.0078(0.0064) Grad: 63284.4180  LR: 0.000001  \n",
      "Epoch: [1][35800/36908] Elapsed 493m 33s (remain 15m 15s) Loss: 0.0008(0.0064) Grad: 5065.9297  LR: 0.000001  \n",
      "Epoch: [1][35900/36908] Elapsed 494m 56s (remain 13m 52s) Loss: 0.0030(0.0064) Grad: 52653.0273  LR: 0.000001  \n",
      "Epoch: [1][36000/36908] Elapsed 496m 19s (remain 12m 30s) Loss: 0.0002(0.0063) Grad: 81.2381  LR: 0.000001  \n",
      "Epoch: [1][36100/36908] Elapsed 497m 41s (remain 11m 7s) Loss: 0.0000(0.0063) Grad: 11.8049  LR: 0.000000  \n",
      "Epoch: [1][36200/36908] Elapsed 499m 6s (remain 9m 44s) Loss: 0.0051(0.0063) Grad: 22973.7852  LR: 0.000000  \n",
      "Epoch: [1][36300/36908] Elapsed 500m 28s (remain 8m 22s) Loss: 0.0113(0.0063) Grad: 212018.6875  LR: 0.000000  \n",
      "Epoch: [1][36400/36908] Elapsed 501m 51s (remain 6m 59s) Loss: 0.0001(0.0063) Grad: 142.8445  LR: 0.000000  \n",
      "Epoch: [1][36500/36908] Elapsed 503m 12s (remain 5m 36s) Loss: 0.0004(0.0063) Grad: 1823.9938  LR: 0.000000  \n",
      "Epoch: [1][36600/36908] Elapsed 504m 36s (remain 4m 13s) Loss: 0.0002(0.0063) Grad: 1145.7465  LR: 0.000000  \n",
      "Epoch: [1][36700/36908] Elapsed 505m 59s (remain 2m 51s) Loss: 0.0009(0.0063) Grad: 11182.6182  LR: 0.000000  \n",
      "Epoch: [1][36800/36908] Elapsed 507m 22s (remain 1m 28s) Loss: 0.0039(0.0063) Grad: 72864.3047  LR: 0.000000  \n",
      "Epoch: [1][36900/36908] Elapsed 508m 44s (remain 0m 5s) Loss: 0.0006(0.0063) Grad: 13224.4121  LR: 0.000000  \n",
      "Epoch: [1][36907/36908] Elapsed 508m 50s (remain 0m 0s) Loss: 0.0000(0.0063) Grad: 998.5626  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 11m 30s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 29s (remain 5m 19s) Loss: 0.0151(0.0057) \n",
      "EVAL: [200/1192] Elapsed 0m 59s (remain 4m 53s) Loss: 0.0080(0.0063) \n",
      "EVAL: [300/1192] Elapsed 1m 28s (remain 4m 22s) Loss: 0.0054(0.0068) \n",
      "EVAL: [400/1192] Elapsed 1m 58s (remain 3m 53s) Loss: 0.0082(0.0072) \n",
      "EVAL: [500/1192] Elapsed 2m 27s (remain 3m 24s) Loss: 0.0107(0.0066) \n",
      "EVAL: [600/1192] Elapsed 2m 56s (remain 2m 54s) Loss: 0.0006(0.0068) \n",
      "EVAL: [700/1192] Elapsed 3m 26s (remain 2m 24s) Loss: 0.1036(0.0083) \n",
      "EVAL: [800/1192] Elapsed 3m 56s (remain 1m 55s) Loss: 0.0028(0.0085) \n",
      "EVAL: [900/1192] Elapsed 4m 26s (remain 1m 26s) Loss: 0.0017(0.0084) \n",
      "EVAL: [1000/1192] Elapsed 4m 55s (remain 0m 56s) Loss: 0.0000(0.0083) \n",
      "EVAL: [1100/1192] Elapsed 5m 25s (remain 0m 26s) Loss: 0.0015(0.0080) \n",
      "EVAL: [1191/1192] Elapsed 5m 51s (remain 0m 0s) Loss: 0.0000(0.0078) \n",
      "Epoch 1 - avg_train_loss: 0.0063  avg_val_loss: 0.0078  time: 30885s\n",
      "Epoch 1 - Score: 0.8877\n",
      "Epoch 1 - Save Best Score: 0.8877 Model\n",
      "========== fold: 1 training ==========\n",
      "get pseudo plain from ../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_1.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_1.npy\n",
      "(612602, 6) (612602, 950)\n",
      "(100000, 7)\n",
      "(110725, 11)\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/36908] Elapsed 0m 1s (remain 657m 31s) Loss: 0.3658(0.3658) Grad: 102483.1172  LR: 0.000000  \n",
      "Epoch: [1][100/36908] Elapsed 1m 24s (remain 514m 50s) Loss: 0.3429(0.3601) Grad: 108044.7969  LR: 0.000001  \n",
      "Epoch: [1][200/36908] Elapsed 2m 48s (remain 514m 13s) Loss: 0.2700(0.3379) Grad: 45390.2500  LR: 0.000001  \n",
      "Epoch: [1][300/36908] Elapsed 4m 11s (remain 509m 58s) Loss: 0.1616(0.2970) Grad: 15450.2656  LR: 0.000002  \n",
      "Epoch: [1][400/36908] Elapsed 5m 35s (remain 509m 5s) Loss: 0.0311(0.2459) Grad: 5472.6328  LR: 0.000002  \n",
      "Epoch: [1][500/36908] Elapsed 6m 58s (remain 507m 1s) Loss: 0.0354(0.2046) Grad: 707.6978  LR: 0.000003  \n",
      "Epoch: [1][600/36908] Elapsed 8m 21s (remain 505m 24s) Loss: 0.0687(0.1774) Grad: 4197.2539  LR: 0.000003  \n",
      "Epoch: [1][700/36908] Elapsed 9m 45s (remain 503m 36s) Loss: 0.0343(0.1572) Grad: 1006.9555  LR: 0.000004  \n",
      "Epoch: [1][800/36908] Elapsed 11m 8s (remain 501m 53s) Loss: 0.0215(0.1420) Grad: 1309.2972  LR: 0.000004  \n",
      "Epoch: [1][900/36908] Elapsed 12m 31s (remain 500m 48s) Loss: 0.0220(0.1291) Grad: 5901.5039  LR: 0.000005  \n",
      "Epoch: [1][1000/36908] Elapsed 13m 55s (remain 499m 22s) Loss: 0.0326(0.1183) Grad: 11794.6816  LR: 0.000005  \n",
      "Epoch: [1][1100/36908] Elapsed 15m 18s (remain 497m 50s) Loss: 0.0227(0.1090) Grad: 8400.7256  LR: 0.000006  \n",
      "Epoch: [1][1200/36908] Elapsed 16m 41s (remain 496m 8s) Loss: 0.0260(0.1014) Grad: 11555.1406  LR: 0.000007  \n",
      "Epoch: [1][1300/36908] Elapsed 18m 4s (remain 494m 54s) Loss: 0.0170(0.0949) Grad: 4923.6904  LR: 0.000007  \n",
      "Epoch: [1][1400/36908] Elapsed 19m 30s (remain 494m 13s) Loss: 0.0119(0.0892) Grad: 4983.5557  LR: 0.000008  \n",
      "Epoch: [1][1500/36908] Elapsed 20m 54s (remain 493m 3s) Loss: 0.0094(0.0841) Grad: 1724.7495  LR: 0.000008  \n",
      "Epoch: [1][1600/36908] Elapsed 22m 17s (remain 491m 41s) Loss: 0.0103(0.0796) Grad: 1399.7814  LR: 0.000009  \n",
      "Epoch: [1][1700/36908] Elapsed 23m 43s (remain 490m 54s) Loss: 0.0067(0.0755) Grad: 220.1825  LR: 0.000009  \n",
      "Epoch: [1][1800/36908] Elapsed 25m 7s (remain 489m 42s) Loss: 0.0047(0.0718) Grad: 482.5148  LR: 0.000010  \n",
      "Epoch: [1][1900/36908] Elapsed 26m 30s (remain 488m 4s) Loss: 0.0161(0.0685) Grad: 1713.1831  LR: 0.000010  \n",
      "Epoch: [1][2000/36908] Elapsed 27m 54s (remain 486m 44s) Loss: 0.0021(0.0655) Grad: 332.0821  LR: 0.000011  \n",
      "Epoch: [1][2100/36908] Elapsed 29m 16s (remain 485m 4s) Loss: 0.0058(0.0627) Grad: 220.0565  LR: 0.000011  \n",
      "Epoch: [1][2200/36908] Elapsed 30m 39s (remain 483m 31s) Loss: 0.0012(0.0602) Grad: 22.9477  LR: 0.000012  \n",
      "Epoch: [1][2300/36908] Elapsed 32m 2s (remain 481m 47s) Loss: 0.0553(0.0580) Grad: 19938.5625  LR: 0.000012  \n",
      "Epoch: [1][2400/36908] Elapsed 33m 25s (remain 480m 22s) Loss: 0.0049(0.0559) Grad: 440.1583  LR: 0.000013  \n",
      "Epoch: [1][2500/36908] Elapsed 34m 48s (remain 478m 50s) Loss: 0.0026(0.0539) Grad: 193.8672  LR: 0.000014  \n",
      "Epoch: [1][2600/36908] Elapsed 36m 11s (remain 477m 20s) Loss: 0.0091(0.0521) Grad: 619.0775  LR: 0.000014  \n",
      "Epoch: [1][2700/36908] Elapsed 37m 34s (remain 475m 57s) Loss: 0.0076(0.0505) Grad: 936.3582  LR: 0.000015  \n",
      "Epoch: [1][2800/36908] Elapsed 38m 59s (remain 474m 49s) Loss: 0.0006(0.0489) Grad: 37.4298  LR: 0.000015  \n",
      "Epoch: [1][2900/36908] Elapsed 40m 23s (remain 473m 33s) Loss: 0.0008(0.0475) Grad: 68.2895  LR: 0.000016  \n",
      "Epoch: [1][3000/36908] Elapsed 41m 47s (remain 472m 6s) Loss: 0.0042(0.0461) Grad: 338.9416  LR: 0.000016  \n",
      "Epoch: [1][3100/36908] Elapsed 43m 10s (remain 470m 39s) Loss: 0.0109(0.0448) Grad: 1391.4293  LR: 0.000017  \n",
      "Epoch: [1][3200/36908] Elapsed 44m 34s (remain 469m 21s) Loss: 0.0285(0.0436) Grad: 815.8936  LR: 0.000017  \n",
      "Epoch: [1][3300/36908] Elapsed 45m 56s (remain 467m 48s) Loss: 0.0035(0.0425) Grad: 641.5995  LR: 0.000018  \n",
      "Epoch: [1][3400/36908] Elapsed 47m 20s (remain 466m 21s) Loss: 0.0022(0.0414) Grad: 306.3661  LR: 0.000018  \n",
      "Epoch: [1][3500/36908] Elapsed 48m 42s (remain 464m 48s) Loss: 0.0012(0.0403) Grad: 60.0969  LR: 0.000019  \n",
      "Epoch: [1][3600/36908] Elapsed 50m 4s (remain 463m 12s) Loss: 0.0021(0.0394) Grad: 333.3763  LR: 0.000020  \n",
      "Epoch: [1][3700/36908] Elapsed 51m 28s (remain 461m 51s) Loss: 0.0032(0.0385) Grad: 404.0739  LR: 0.000020  \n",
      "Epoch: [1][3800/36908] Elapsed 52m 50s (remain 460m 16s) Loss: 0.0038(0.0376) Grad: 213.9382  LR: 0.000020  \n",
      "Epoch: [1][3900/36908] Elapsed 54m 12s (remain 458m 38s) Loss: 0.0038(0.0368) Grad: 198.9611  LR: 0.000020  \n",
      "Epoch: [1][4000/36908] Elapsed 55m 34s (remain 457m 4s) Loss: 0.0013(0.0360) Grad: 152.8291  LR: 0.000020  \n",
      "Epoch: [1][4100/36908] Elapsed 56m 57s (remain 455m 38s) Loss: 0.0082(0.0352) Grad: 3384.4080  LR: 0.000020  \n",
      "Epoch: [1][4200/36908] Elapsed 58m 19s (remain 454m 3s) Loss: 0.0008(0.0345) Grad: 70.1370  LR: 0.000020  \n",
      "Epoch: [1][4300/36908] Elapsed 59m 40s (remain 452m 25s) Loss: 0.0013(0.0338) Grad: 294.5988  LR: 0.000020  \n",
      "Epoch: [1][4400/36908] Elapsed 61m 4s (remain 451m 10s) Loss: 0.0067(0.0332) Grad: 1359.2140  LR: 0.000020  \n",
      "Epoch: [1][4500/36908] Elapsed 62m 27s (remain 449m 44s) Loss: 0.0022(0.0326) Grad: 105.4963  LR: 0.000020  \n",
      "Epoch: [1][4600/36908] Elapsed 63m 49s (remain 448m 12s) Loss: 0.0109(0.0320) Grad: 863.1283  LR: 0.000019  \n",
      "Epoch: [1][4700/36908] Elapsed 65m 12s (remain 446m 45s) Loss: 0.0005(0.0314) Grad: 19.5202  LR: 0.000019  \n",
      "Epoch: [1][4800/36908] Elapsed 66m 35s (remain 445m 22s) Loss: 0.0031(0.0308) Grad: 1316.8236  LR: 0.000019  \n",
      "Epoch: [1][4900/36908] Elapsed 67m 57s (remain 443m 52s) Loss: 0.0015(0.0303) Grad: 91.6852  LR: 0.000019  \n",
      "Epoch: [1][5000/36908] Elapsed 69m 19s (remain 442m 19s) Loss: 0.0338(0.0298) Grad: 2597.1965  LR: 0.000019  \n",
      "Epoch: [1][5100/36908] Elapsed 70m 41s (remain 440m 50s) Loss: 0.0034(0.0293) Grad: 403.0923  LR: 0.000019  \n",
      "Epoch: [1][5200/36908] Elapsed 72m 5s (remain 439m 28s) Loss: 0.0005(0.0288) Grad: 85.4547  LR: 0.000019  \n",
      "Epoch: [1][5300/36908] Elapsed 73m 26s (remain 437m 55s) Loss: 0.0052(0.0284) Grad: 1358.6390  LR: 0.000019  \n",
      "Epoch: [1][5400/36908] Elapsed 74m 48s (remain 436m 25s) Loss: 0.0003(0.0279) Grad: 24.8257  LR: 0.000019  \n",
      "Epoch: [1][5500/36908] Elapsed 76m 11s (remain 435m 0s) Loss: 0.0003(0.0275) Grad: 25.7319  LR: 0.000019  \n",
      "Epoch: [1][5600/36908] Elapsed 77m 33s (remain 433m 29s) Loss: 0.0033(0.0271) Grad: 355.6542  LR: 0.000019  \n",
      "Epoch: [1][5700/36908] Elapsed 78m 55s (remain 432m 2s) Loss: 0.0063(0.0267) Grad: 441.7896  LR: 0.000019  \n",
      "Epoch: [1][5800/36908] Elapsed 80m 18s (remain 430m 37s) Loss: 0.0022(0.0263) Grad: 1629.3695  LR: 0.000019  \n",
      "Epoch: [1][5900/36908] Elapsed 81m 40s (remain 429m 10s) Loss: 0.0007(0.0259) Grad: 121.9215  LR: 0.000019  \n",
      "Epoch: [1][6000/36908] Elapsed 83m 2s (remain 427m 43s) Loss: 0.0023(0.0255) Grad: 767.4755  LR: 0.000019  \n",
      "Epoch: [1][6100/36908] Elapsed 84m 25s (remain 426m 18s) Loss: 0.0003(0.0252) Grad: 60.6486  LR: 0.000019  \n",
      "Epoch: [1][6200/36908] Elapsed 85m 49s (remain 425m 0s) Loss: 0.0007(0.0248) Grad: 20.7705  LR: 0.000018  \n",
      "Epoch: [1][6300/36908] Elapsed 87m 13s (remain 423m 39s) Loss: 0.0178(0.0245) Grad: 4743.2437  LR: 0.000018  \n",
      "Epoch: [1][6400/36908] Elapsed 88m 36s (remain 422m 18s) Loss: 0.0011(0.0242) Grad: 413.9761  LR: 0.000018  \n",
      "Epoch: [1][6500/36908] Elapsed 89m 59s (remain 420m 53s) Loss: 0.0002(0.0239) Grad: 75.9422  LR: 0.000018  \n",
      "Epoch: [1][6600/36908] Elapsed 91m 21s (remain 419m 26s) Loss: 0.0015(0.0236) Grad: 229.5102  LR: 0.000018  \n",
      "Epoch: [1][6700/36908] Elapsed 92m 43s (remain 418m 0s) Loss: 0.0027(0.0233) Grad: 250.6497  LR: 0.000018  \n",
      "Epoch: [1][6800/36908] Elapsed 94m 7s (remain 416m 38s) Loss: 0.0020(0.0230) Grad: 1024.8583  LR: 0.000018  \n",
      "Epoch: [1][6900/36908] Elapsed 95m 30s (remain 415m 19s) Loss: 0.0021(0.0227) Grad: 264.8438  LR: 0.000018  \n",
      "Epoch: [1][7000/36908] Elapsed 96m 53s (remain 413m 54s) Loss: 0.0072(0.0224) Grad: 961.6773  LR: 0.000018  \n",
      "Epoch: [1][7100/36908] Elapsed 98m 16s (remain 412m 29s) Loss: 0.0009(0.0222) Grad: 322.9367  LR: 0.000018  \n",
      "Epoch: [1][7200/36908] Elapsed 99m 39s (remain 411m 9s) Loss: 0.0006(0.0219) Grad: 91.8254  LR: 0.000018  \n",
      "Epoch: [1][7300/36908] Elapsed 101m 2s (remain 409m 45s) Loss: 0.0003(0.0217) Grad: 60.8603  LR: 0.000018  \n",
      "Epoch: [1][7400/36908] Elapsed 102m 25s (remain 408m 21s) Loss: 0.0002(0.0214) Grad: 52.7257  LR: 0.000018  \n",
      "Epoch: [1][7500/36908] Elapsed 103m 49s (remain 407m 3s) Loss: 0.0039(0.0212) Grad: 925.6077  LR: 0.000018  \n",
      "Epoch: [1][7600/36908] Elapsed 105m 13s (remain 405m 42s) Loss: 0.0011(0.0210) Grad: 63.2341  LR: 0.000018  \n",
      "Epoch: [1][7700/36908] Elapsed 106m 35s (remain 404m 16s) Loss: 0.0018(0.0208) Grad: 235.3296  LR: 0.000018  \n",
      "Epoch: [1][7800/36908] Elapsed 107m 58s (remain 402m 52s) Loss: 0.0131(0.0205) Grad: 2155.4595  LR: 0.000018  \n",
      "Epoch: [1][7900/36908] Elapsed 109m 22s (remain 401m 31s) Loss: 0.0008(0.0203) Grad: 161.0083  LR: 0.000017  \n",
      "Epoch: [1][8000/36908] Elapsed 110m 44s (remain 400m 7s) Loss: 0.0014(0.0201) Grad: 126.3051  LR: 0.000017  \n",
      "Epoch: [1][8100/36908] Elapsed 112m 7s (remain 398m 42s) Loss: 0.0002(0.0199) Grad: 125.4217  LR: 0.000017  \n",
      "Epoch: [1][8200/36908] Elapsed 113m 30s (remain 397m 19s) Loss: 0.0004(0.0197) Grad: 27.2443  LR: 0.000017  \n",
      "Epoch: [1][8300/36908] Elapsed 114m 53s (remain 395m 55s) Loss: 0.0030(0.0195) Grad: 759.3065  LR: 0.000017  \n",
      "Epoch: [1][8400/36908] Elapsed 116m 17s (remain 394m 35s) Loss: 0.0020(0.0193) Grad: 481.8650  LR: 0.000017  \n",
      "Epoch: [1][8500/36908] Elapsed 117m 40s (remain 393m 11s) Loss: 0.0022(0.0192) Grad: 279.4050  LR: 0.000017  \n",
      "Epoch: [1][8600/36908] Elapsed 119m 2s (remain 391m 45s) Loss: 0.0014(0.0190) Grad: 27.5872  LR: 0.000017  \n",
      "Epoch: [1][8700/36908] Elapsed 120m 24s (remain 390m 20s) Loss: 0.0018(0.0188) Grad: 134.6744  LR: 0.000017  \n",
      "Epoch: [1][8800/36908] Elapsed 121m 48s (remain 389m 1s) Loss: 0.0033(0.0186) Grad: 186.3531  LR: 0.000017  \n",
      "Epoch: [1][8900/36908] Elapsed 123m 12s (remain 387m 40s) Loss: 0.0029(0.0185) Grad: 664.2110  LR: 0.000017  \n",
      "Epoch: [1][9000/36908] Elapsed 124m 35s (remain 386m 17s) Loss: 0.0057(0.0183) Grad: 318.0814  LR: 0.000017  \n",
      "Epoch: [1][9100/36908] Elapsed 125m 58s (remain 384m 54s) Loss: 0.0072(0.0181) Grad: 1380.3990  LR: 0.000017  \n",
      "Epoch: [1][9200/36908] Elapsed 127m 21s (remain 383m 30s) Loss: 0.0055(0.0180) Grad: 351.6421  LR: 0.000017  \n",
      "Epoch: [1][9300/36908] Elapsed 128m 44s (remain 382m 8s) Loss: 0.0019(0.0178) Grad: 234.2823  LR: 0.000017  \n",
      "Epoch: [1][9400/36908] Elapsed 130m 7s (remain 380m 43s) Loss: 0.0014(0.0177) Grad: 432.2993  LR: 0.000017  \n",
      "Epoch: [1][9500/36908] Elapsed 131m 28s (remain 379m 16s) Loss: 0.0001(0.0175) Grad: 57.0236  LR: 0.000017  \n",
      "Epoch: [1][9600/36908] Elapsed 132m 50s (remain 377m 50s) Loss: 0.0023(0.0174) Grad: 231.7191  LR: 0.000016  \n",
      "Epoch: [1][9700/36908] Elapsed 134m 13s (remain 376m 26s) Loss: 0.0035(0.0172) Grad: 898.1797  LR: 0.000016  \n",
      "Epoch: [1][9800/36908] Elapsed 135m 36s (remain 375m 4s) Loss: 0.0012(0.0171) Grad: 281.9763  LR: 0.000016  \n",
      "Epoch: [1][9900/36908] Elapsed 137m 0s (remain 373m 41s) Loss: 0.0042(0.0170) Grad: 2042.5172  LR: 0.000016  \n",
      "Epoch: [1][10000/36908] Elapsed 138m 23s (remain 372m 20s) Loss: 0.0067(0.0168) Grad: 1423.4607  LR: 0.000016  \n",
      "Epoch: [1][10100/36908] Elapsed 139m 45s (remain 370m 55s) Loss: 0.0093(0.0167) Grad: 2993.2197  LR: 0.000016  \n",
      "Epoch: [1][10200/36908] Elapsed 141m 8s (remain 369m 31s) Loss: 0.0046(0.0166) Grad: 1919.5227  LR: 0.000016  \n",
      "Epoch: [1][10300/36908] Elapsed 142m 31s (remain 368m 9s) Loss: 0.0022(0.0164) Grad: 747.9479  LR: 0.000016  \n",
      "Epoch: [1][10400/36908] Elapsed 143m 54s (remain 366m 46s) Loss: 0.0049(0.0163) Grad: 2299.3408  LR: 0.000016  \n",
      "Epoch: [1][10500/36908] Elapsed 145m 17s (remain 365m 22s) Loss: 0.0011(0.0162) Grad: 79.3583  LR: 0.000016  \n",
      "Epoch: [1][10600/36908] Elapsed 146m 42s (remain 364m 2s) Loss: 0.0045(0.0161) Grad: 69.0690  LR: 0.000016  \n",
      "Epoch: [1][10700/36908] Elapsed 148m 5s (remain 362m 39s) Loss: 0.0015(0.0159) Grad: 1199.3042  LR: 0.000016  \n",
      "Epoch: [1][10800/36908] Elapsed 149m 28s (remain 361m 18s) Loss: 0.0001(0.0158) Grad: 15.9058  LR: 0.000016  \n",
      "Epoch: [1][10900/36908] Elapsed 150m 50s (remain 359m 52s) Loss: 0.0025(0.0157) Grad: 1256.9463  LR: 0.000016  \n",
      "Epoch: [1][11000/36908] Elapsed 152m 12s (remain 358m 27s) Loss: 0.0013(0.0156) Grad: 270.5695  LR: 0.000016  \n",
      "Epoch: [1][11100/36908] Elapsed 153m 34s (remain 357m 2s) Loss: 0.0062(0.0155) Grad: 2170.2432  LR: 0.000016  \n",
      "Epoch: [1][11200/36908] Elapsed 154m 56s (remain 355m 37s) Loss: 0.0089(0.0154) Grad: 1826.5034  LR: 0.000015  \n",
      "Epoch: [1][11300/36908] Elapsed 156m 19s (remain 354m 13s) Loss: 0.0050(0.0153) Grad: 991.4084  LR: 0.000015  \n",
      "Epoch: [1][11400/36908] Elapsed 157m 42s (remain 352m 49s) Loss: 0.0014(0.0152) Grad: 16.0213  LR: 0.000015  \n",
      "Epoch: [1][11500/36908] Elapsed 159m 3s (remain 351m 23s) Loss: 0.0040(0.0151) Grad: 3727.4038  LR: 0.000015  \n",
      "Epoch: [1][11600/36908] Elapsed 160m 26s (remain 349m 59s) Loss: 0.0058(0.0150) Grad: 10559.1367  LR: 0.000015  \n",
      "Epoch: [1][11700/36908] Elapsed 161m 49s (remain 348m 37s) Loss: 0.0021(0.0149) Grad: 891.5568  LR: 0.000015  \n",
      "Epoch: [1][11800/36908] Elapsed 163m 11s (remain 347m 12s) Loss: 0.0010(0.0148) Grad: 203.5002  LR: 0.000015  \n",
      "Epoch: [1][11900/36908] Elapsed 164m 33s (remain 345m 46s) Loss: 0.0004(0.0147) Grad: 67.8772  LR: 0.000015  \n",
      "Epoch: [1][12000/36908] Elapsed 165m 56s (remain 344m 23s) Loss: 0.0170(0.0146) Grad: 4004.7207  LR: 0.000015  \n",
      "Epoch: [1][12100/36908] Elapsed 167m 19s (remain 343m 0s) Loss: 0.0107(0.0145) Grad: 2549.1689  LR: 0.000015  \n",
      "Epoch: [1][12200/36908] Elapsed 168m 42s (remain 341m 37s) Loss: 0.0025(0.0144) Grad: 594.5827  LR: 0.000015  \n",
      "Epoch: [1][12300/36908] Elapsed 170m 4s (remain 340m 14s) Loss: 0.0034(0.0143) Grad: 12.5990  LR: 0.000015  \n",
      "Epoch: [1][12400/36908] Elapsed 171m 29s (remain 338m 53s) Loss: 0.0103(0.0142) Grad: 7528.3848  LR: 0.000015  \n",
      "Epoch: [1][12500/36908] Elapsed 172m 53s (remain 337m 33s) Loss: 0.0024(0.0141) Grad: 662.8876  LR: 0.000015  \n",
      "Epoch: [1][12600/36908] Elapsed 174m 17s (remain 336m 12s) Loss: 0.0000(0.0140) Grad: 6.4347  LR: 0.000015  \n",
      "Epoch: [1][12700/36908] Elapsed 175m 39s (remain 334m 48s) Loss: 0.0117(0.0140) Grad: 22232.7773  LR: 0.000015  \n",
      "Epoch: [1][12800/36908] Elapsed 177m 2s (remain 333m 23s) Loss: 0.0048(0.0139) Grad: 1341.5687  LR: 0.000015  \n",
      "Epoch: [1][12900/36908] Elapsed 178m 25s (remain 332m 1s) Loss: 0.0030(0.0138) Grad: 1062.8046  LR: 0.000014  \n",
      "Epoch: [1][13000/36908] Elapsed 179m 47s (remain 330m 36s) Loss: 0.0008(0.0137) Grad: 82.1634  LR: 0.000014  \n",
      "Epoch: [1][13100/36908] Elapsed 181m 11s (remain 329m 14s) Loss: 0.0011(0.0136) Grad: 620.1231  LR: 0.000014  \n",
      "Epoch: [1][13200/36908] Elapsed 182m 34s (remain 327m 51s) Loss: 0.0046(0.0135) Grad: 2646.9575  LR: 0.000014  \n",
      "Epoch: [1][13300/36908] Elapsed 183m 56s (remain 326m 28s) Loss: 0.0016(0.0135) Grad: 561.0263  LR: 0.000014  \n",
      "Epoch: [1][13400/36908] Elapsed 185m 20s (remain 325m 5s) Loss: 0.0016(0.0134) Grad: 6.1676  LR: 0.000014  \n",
      "Epoch: [1][13500/36908] Elapsed 186m 43s (remain 323m 43s) Loss: 0.0004(0.0133) Grad: 91.7732  LR: 0.000014  \n",
      "Epoch: [1][13600/36908] Elapsed 188m 6s (remain 322m 20s) Loss: 0.0033(0.0132) Grad: 688.2679  LR: 0.000014  \n",
      "Epoch: [1][13700/36908] Elapsed 189m 30s (remain 320m 59s) Loss: 0.0002(0.0132) Grad: 13.3618  LR: 0.000014  \n",
      "Epoch: [1][13800/36908] Elapsed 190m 53s (remain 319m 36s) Loss: 0.0026(0.0131) Grad: 3161.4270  LR: 0.000014  \n",
      "Epoch: [1][13900/36908] Elapsed 192m 16s (remain 318m 13s) Loss: 0.0004(0.0130) Grad: 29.7275  LR: 0.000014  \n",
      "Epoch: [1][14000/36908] Elapsed 193m 39s (remain 316m 51s) Loss: 0.0072(0.0130) Grad: 323.1152  LR: 0.000014  \n",
      "Epoch: [1][14100/36908] Elapsed 195m 3s (remain 315m 28s) Loss: 0.0009(0.0129) Grad: 606.4089  LR: 0.000014  \n",
      "Epoch: [1][14200/36908] Elapsed 196m 25s (remain 314m 4s) Loss: 0.0035(0.0128) Grad: 612.3407  LR: 0.000014  \n",
      "Epoch: [1][14300/36908] Elapsed 197m 47s (remain 312m 40s) Loss: 0.0023(0.0128) Grad: 8405.4316  LR: 0.000014  \n",
      "Epoch: [1][14400/36908] Elapsed 199m 10s (remain 311m 16s) Loss: 0.0003(0.0127) Grad: 67.4437  LR: 0.000014  \n",
      "Epoch: [1][14500/36908] Elapsed 200m 34s (remain 309m 55s) Loss: 0.0023(0.0126) Grad: 2007.2465  LR: 0.000013  \n",
      "Epoch: [1][14600/36908] Elapsed 201m 58s (remain 308m 33s) Loss: 0.0018(0.0126) Grad: 546.9146  LR: 0.000013  \n",
      "Epoch: [1][14700/36908] Elapsed 203m 20s (remain 307m 10s) Loss: 0.0024(0.0125) Grad: 3396.8435  LR: 0.000013  \n",
      "Epoch: [1][14800/36908] Elapsed 204m 43s (remain 305m 46s) Loss: 0.0016(0.0124) Grad: 112.0116  LR: 0.000013  \n",
      "Epoch: [1][14900/36908] Elapsed 206m 7s (remain 304m 24s) Loss: 0.0004(0.0124) Grad: 31.6248  LR: 0.000013  \n",
      "Epoch: [1][15000/36908] Elapsed 207m 31s (remain 303m 4s) Loss: 0.0011(0.0123) Grad: 1162.0255  LR: 0.000013  \n",
      "Epoch: [1][15100/36908] Elapsed 208m 55s (remain 301m 42s) Loss: 0.0004(0.0123) Grad: 16.0557  LR: 0.000013  \n",
      "Epoch: [1][15200/36908] Elapsed 210m 18s (remain 300m 19s) Loss: 0.0003(0.0122) Grad: 82.2711  LR: 0.000013  \n",
      "Epoch: [1][15300/36908] Elapsed 211m 41s (remain 298m 55s) Loss: 0.0063(0.0121) Grad: 1657.7086  LR: 0.000013  \n",
      "Epoch: [1][15400/36908] Elapsed 213m 4s (remain 297m 32s) Loss: 0.0104(0.0121) Grad: 2130.4780  LR: 0.000013  \n",
      "Epoch: [1][15500/36908] Elapsed 214m 28s (remain 296m 11s) Loss: 0.0037(0.0120) Grad: 1982.3383  LR: 0.000013  \n",
      "Epoch: [1][15600/36908] Elapsed 215m 51s (remain 294m 47s) Loss: 0.0016(0.0120) Grad: 487.2911  LR: 0.000013  \n",
      "Epoch: [1][15700/36908] Elapsed 217m 14s (remain 293m 25s) Loss: 0.0015(0.0119) Grad: 759.0458  LR: 0.000013  \n",
      "Epoch: [1][15800/36908] Elapsed 218m 37s (remain 292m 2s) Loss: 0.0003(0.0119) Grad: 38.8339  LR: 0.000013  \n",
      "Epoch: [1][15900/36908] Elapsed 220m 1s (remain 290m 40s) Loss: 0.0002(0.0118) Grad: 79.6150  LR: 0.000013  \n",
      "Epoch: [1][16000/36908] Elapsed 221m 24s (remain 289m 17s) Loss: 0.0002(0.0118) Grad: 48.7556  LR: 0.000013  \n",
      "Epoch: [1][16100/36908] Elapsed 222m 47s (remain 287m 54s) Loss: 0.0001(0.0117) Grad: 8.6567  LR: 0.000013  \n",
      "Epoch: [1][16200/36908] Elapsed 224m 9s (remain 286m 29s) Loss: 0.0112(0.0117) Grad: 8968.5186  LR: 0.000012  \n",
      "Epoch: [1][16300/36908] Elapsed 225m 31s (remain 285m 6s) Loss: 0.0015(0.0116) Grad: 1090.9731  LR: 0.000012  \n",
      "Epoch: [1][16400/36908] Elapsed 226m 53s (remain 283m 42s) Loss: 0.0023(0.0116) Grad: 1951.3402  LR: 0.000012  \n",
      "Epoch: [1][16500/36908] Elapsed 228m 18s (remain 282m 20s) Loss: 0.0104(0.0115) Grad: 3433.6018  LR: 0.000012  \n",
      "Epoch: [1][16600/36908] Elapsed 229m 42s (remain 280m 58s) Loss: 0.0035(0.0115) Grad: 2096.9768  LR: 0.000012  \n",
      "Epoch: [1][16700/36908] Elapsed 231m 5s (remain 279m 35s) Loss: 0.0040(0.0114) Grad: 98.8392  LR: 0.000012  \n",
      "Epoch: [1][16800/36908] Elapsed 232m 28s (remain 278m 12s) Loss: 0.0021(0.0114) Grad: 761.6834  LR: 0.000012  \n",
      "Epoch: [1][16900/36908] Elapsed 233m 52s (remain 276m 50s) Loss: 0.0120(0.0113) Grad: 3469.2974  LR: 0.000012  \n",
      "Epoch: [1][17000/36908] Elapsed 235m 15s (remain 275m 28s) Loss: 0.0043(0.0113) Grad: 734.8066  LR: 0.000012  \n",
      "Epoch: [1][17100/36908] Elapsed 236m 39s (remain 274m 5s) Loss: 0.0017(0.0112) Grad: 1657.1515  LR: 0.000012  \n",
      "Epoch: [1][17200/36908] Elapsed 238m 2s (remain 272m 43s) Loss: 0.0011(0.0112) Grad: 1083.8899  LR: 0.000012  \n",
      "Epoch: [1][17300/36908] Elapsed 239m 25s (remain 271m 20s) Loss: 0.0050(0.0111) Grad: 2718.7258  LR: 0.000012  \n",
      "Epoch: [1][17400/36908] Elapsed 240m 49s (remain 269m 57s) Loss: 0.0010(0.0111) Grad: 399.9044  LR: 0.000012  \n",
      "Epoch: [1][17500/36908] Elapsed 242m 11s (remain 268m 34s) Loss: 0.0055(0.0110) Grad: 960.4706  LR: 0.000012  \n",
      "Epoch: [1][17600/36908] Elapsed 243m 34s (remain 267m 10s) Loss: 0.0125(0.0110) Grad: 4248.4131  LR: 0.000012  \n",
      "Epoch: [1][17700/36908] Elapsed 244m 58s (remain 265m 48s) Loss: 0.0007(0.0109) Grad: 81.2781  LR: 0.000012  \n",
      "Epoch: [1][17800/36908] Elapsed 246m 20s (remain 264m 25s) Loss: 0.0006(0.0109) Grad: 83.2032  LR: 0.000012  \n",
      "Epoch: [1][17900/36908] Elapsed 247m 43s (remain 263m 1s) Loss: 0.0003(0.0109) Grad: 225.0745  LR: 0.000011  \n",
      "Epoch: [1][18000/36908] Elapsed 249m 5s (remain 261m 37s) Loss: 0.0006(0.0108) Grad: 259.2398  LR: 0.000011  \n",
      "Epoch: [1][18100/36908] Elapsed 250m 29s (remain 260m 15s) Loss: 0.0051(0.0108) Grad: 1477.0328  LR: 0.000011  \n",
      "Epoch: [1][18200/36908] Elapsed 251m 53s (remain 258m 53s) Loss: 0.0001(0.0107) Grad: 243.6223  LR: 0.000011  \n",
      "Epoch: [1][18300/36908] Elapsed 253m 17s (remain 257m 31s) Loss: 0.0013(0.0107) Grad: 1200.7389  LR: 0.000011  \n",
      "Epoch: [1][18400/36908] Elapsed 254m 40s (remain 256m 8s) Loss: 0.0068(0.0106) Grad: 10726.5137  LR: 0.000011  \n",
      "Epoch: [1][18500/36908] Elapsed 256m 2s (remain 254m 44s) Loss: 0.0069(0.0106) Grad: 8397.8770  LR: 0.000011  \n",
      "Epoch: [1][18600/36908] Elapsed 257m 26s (remain 253m 22s) Loss: 0.0285(0.0106) Grad: 9924.9961  LR: 0.000011  \n",
      "Epoch: [1][18700/36908] Elapsed 258m 49s (remain 251m 59s) Loss: 0.0104(0.0105) Grad: 26811.0625  LR: 0.000011  \n",
      "Epoch: [1][18800/36908] Elapsed 260m 13s (remain 250m 37s) Loss: 0.0005(0.0105) Grad: 230.3925  LR: 0.000011  \n",
      "Epoch: [1][18900/36908] Elapsed 261m 36s (remain 249m 14s) Loss: 0.0196(0.0105) Grad: 6499.6880  LR: 0.000011  \n",
      "Epoch: [1][19000/36908] Elapsed 263m 0s (remain 247m 51s) Loss: 0.0001(0.0104) Grad: 28.1873  LR: 0.000011  \n",
      "Epoch: [1][19100/36908] Elapsed 264m 23s (remain 246m 28s) Loss: 0.0004(0.0104) Grad: 586.3240  LR: 0.000011  \n",
      "Epoch: [1][19200/36908] Elapsed 265m 45s (remain 245m 4s) Loss: 0.0001(0.0103) Grad: 6.7843  LR: 0.000011  \n",
      "Epoch: [1][19300/36908] Elapsed 267m 9s (remain 243m 42s) Loss: 0.0014(0.0103) Grad: 2518.0923  LR: 0.000011  \n",
      "Epoch: [1][19400/36908] Elapsed 268m 32s (remain 242m 19s) Loss: 0.0001(0.0103) Grad: 54.5842  LR: 0.000011  \n",
      "Epoch: [1][19500/36908] Elapsed 269m 56s (remain 240m 57s) Loss: 0.0254(0.0102) Grad: 13535.9580  LR: 0.000010  \n",
      "Epoch: [1][19600/36908] Elapsed 271m 21s (remain 239m 35s) Loss: 0.0007(0.0102) Grad: 244.6194  LR: 0.000010  \n",
      "Epoch: [1][19700/36908] Elapsed 272m 44s (remain 238m 12s) Loss: 0.0215(0.0102) Grad: 24968.4336  LR: 0.000010  \n",
      "Epoch: [1][19800/36908] Elapsed 274m 7s (remain 236m 49s) Loss: 0.0074(0.0101) Grad: 6937.3193  LR: 0.000010  \n",
      "Epoch: [1][19900/36908] Elapsed 275m 30s (remain 235m 26s) Loss: 0.0015(0.0101) Grad: 529.9059  LR: 0.000010  \n",
      "Epoch: [1][20000/36908] Elapsed 276m 54s (remain 234m 4s) Loss: 0.0155(0.0101) Grad: 61079.3008  LR: 0.000010  \n",
      "Epoch: [1][20100/36908] Elapsed 278m 17s (remain 232m 41s) Loss: 0.0019(0.0100) Grad: 1905.0435  LR: 0.000010  \n",
      "Epoch: [1][20200/36908] Elapsed 279m 39s (remain 231m 17s) Loss: 0.0025(0.0100) Grad: 4184.4634  LR: 0.000010  \n",
      "Epoch: [1][20300/36908] Elapsed 281m 3s (remain 229m 54s) Loss: 0.0137(0.0100) Grad: 25943.3184  LR: 0.000010  \n",
      "Epoch: [1][20400/36908] Elapsed 282m 27s (remain 228m 32s) Loss: 0.0005(0.0099) Grad: 754.2009  LR: 0.000010  \n",
      "Epoch: [1][20500/36908] Elapsed 283m 49s (remain 227m 8s) Loss: 0.0001(0.0099) Grad: 166.4555  LR: 0.000010  \n",
      "Epoch: [1][20600/36908] Elapsed 285m 13s (remain 225m 46s) Loss: 0.0043(0.0099) Grad: 48429.4297  LR: 0.000010  \n",
      "Epoch: [1][20700/36908] Elapsed 286m 37s (remain 224m 23s) Loss: 0.0039(0.0098) Grad: 4133.0640  LR: 0.000010  \n",
      "Epoch: [1][20800/36908] Elapsed 288m 0s (remain 223m 0s) Loss: 0.0005(0.0098) Grad: 825.0272  LR: 0.000010  \n",
      "Epoch: [1][20900/36908] Elapsed 289m 22s (remain 221m 36s) Loss: 0.0001(0.0098) Grad: 449.1595  LR: 0.000010  \n",
      "Epoch: [1][21000/36908] Elapsed 290m 44s (remain 220m 13s) Loss: 0.0034(0.0097) Grad: 2663.1196  LR: 0.000010  \n",
      "Epoch: [1][21100/36908] Elapsed 292m 6s (remain 218m 49s) Loss: 0.0002(0.0097) Grad: 18.9502  LR: 0.000010  \n",
      "Epoch: [1][21200/36908] Elapsed 293m 29s (remain 217m 26s) Loss: 0.0068(0.0097) Grad: 626.0486  LR: 0.000009  \n",
      "Epoch: [1][21300/36908] Elapsed 294m 53s (remain 216m 4s) Loss: 0.0003(0.0096) Grad: 87.6129  LR: 0.000009  \n",
      "Epoch: [1][21400/36908] Elapsed 296m 17s (remain 214m 41s) Loss: 0.0034(0.0096) Grad: 2168.5071  LR: 0.000009  \n",
      "Epoch: [1][21500/36908] Elapsed 297m 41s (remain 213m 19s) Loss: 0.0001(0.0096) Grad: 15.5363  LR: 0.000009  \n",
      "Epoch: [1][21600/36908] Elapsed 299m 4s (remain 211m 56s) Loss: 0.0021(0.0095) Grad: 2274.7573  LR: 0.000009  \n",
      "Epoch: [1][21700/36908] Elapsed 300m 28s (remain 210m 33s) Loss: 0.0011(0.0095) Grad: 462.6698  LR: 0.000009  \n",
      "Epoch: [1][21800/36908] Elapsed 301m 52s (remain 209m 11s) Loss: 0.0001(0.0095) Grad: 125.8466  LR: 0.000009  \n",
      "Epoch: [1][21900/36908] Elapsed 303m 15s (remain 207m 48s) Loss: 0.0013(0.0094) Grad: 327.5087  LR: 0.000009  \n",
      "Epoch: [1][22000/36908] Elapsed 304m 40s (remain 206m 26s) Loss: 0.0001(0.0094) Grad: 29.4321  LR: 0.000009  \n",
      "Epoch: [1][22100/36908] Elapsed 306m 3s (remain 205m 2s) Loss: 0.0021(0.0094) Grad: 2553.3787  LR: 0.000009  \n",
      "Epoch: [1][22200/36908] Elapsed 307m 25s (remain 203m 39s) Loss: 0.0032(0.0094) Grad: 3110.9963  LR: 0.000009  \n",
      "Epoch: [1][22300/36908] Elapsed 308m 49s (remain 202m 16s) Loss: 0.0010(0.0093) Grad: 265.6139  LR: 0.000009  \n",
      "Epoch: [1][22400/36908] Elapsed 310m 13s (remain 200m 54s) Loss: 0.0101(0.0093) Grad: 8804.4355  LR: 0.000009  \n",
      "Epoch: [1][22500/36908] Elapsed 311m 37s (remain 199m 31s) Loss: 0.0006(0.0093) Grad: 1461.3169  LR: 0.000009  \n",
      "Epoch: [1][22600/36908] Elapsed 313m 0s (remain 198m 8s) Loss: 0.0039(0.0092) Grad: 20548.4043  LR: 0.000009  \n",
      "Epoch: [1][22700/36908] Elapsed 314m 26s (remain 196m 46s) Loss: 0.0003(0.0092) Grad: 397.9913  LR: 0.000009  \n",
      "Epoch: [1][22800/36908] Elapsed 315m 50s (remain 195m 24s) Loss: 0.0012(0.0092) Grad: 164.9835  LR: 0.000008  \n",
      "Epoch: [1][22900/36908] Elapsed 317m 14s (remain 194m 1s) Loss: 0.0022(0.0092) Grad: 1477.2645  LR: 0.000008  \n",
      "Epoch: [1][23000/36908] Elapsed 318m 38s (remain 192m 39s) Loss: 0.0008(0.0091) Grad: 1899.7305  LR: 0.000008  \n",
      "Epoch: [1][23100/36908] Elapsed 320m 2s (remain 191m 16s) Loss: 0.0022(0.0091) Grad: 2410.9487  LR: 0.000008  \n",
      "Epoch: [1][23200/36908] Elapsed 321m 25s (remain 189m 53s) Loss: 0.0075(0.0091) Grad: 17341.9570  LR: 0.000008  \n",
      "Epoch: [1][23300/36908] Elapsed 322m 48s (remain 188m 30s) Loss: 0.0024(0.0091) Grad: 2516.4126  LR: 0.000008  \n",
      "Epoch: [1][23400/36908] Elapsed 324m 12s (remain 187m 7s) Loss: 0.0014(0.0090) Grad: 1469.9500  LR: 0.000008  \n",
      "Epoch: [1][23500/36908] Elapsed 325m 34s (remain 185m 44s) Loss: 0.0002(0.0090) Grad: 88.8861  LR: 0.000008  \n",
      "Epoch: [1][23600/36908] Elapsed 326m 57s (remain 184m 21s) Loss: 0.0002(0.0090) Grad: 92.2716  LR: 0.000008  \n",
      "Epoch: [1][23700/36908] Elapsed 328m 20s (remain 182m 57s) Loss: 0.0001(0.0090) Grad: 477.5485  LR: 0.000008  \n",
      "Epoch: [1][23800/36908] Elapsed 329m 45s (remain 181m 35s) Loss: 0.0010(0.0089) Grad: 3294.0610  LR: 0.000008  \n",
      "Epoch: [1][23900/36908] Elapsed 331m 9s (remain 180m 13s) Loss: 0.0013(0.0089) Grad: 7137.7188  LR: 0.000008  \n",
      "Epoch: [1][24000/36908] Elapsed 332m 33s (remain 178m 50s) Loss: 0.0009(0.0089) Grad: 1324.9198  LR: 0.000008  \n",
      "Epoch: [1][24100/36908] Elapsed 333m 57s (remain 177m 27s) Loss: 0.0017(0.0089) Grad: 6147.0845  LR: 0.000008  \n",
      "Epoch: [1][24200/36908] Elapsed 335m 21s (remain 176m 5s) Loss: 0.0009(0.0088) Grad: 1514.8402  LR: 0.000008  \n",
      "Epoch: [1][24300/36908] Elapsed 336m 44s (remain 174m 41s) Loss: 0.0047(0.0088) Grad: 6183.7588  LR: 0.000008  \n",
      "Epoch: [1][24400/36908] Elapsed 338m 8s (remain 173m 18s) Loss: 0.0152(0.0088) Grad: 18046.0391  LR: 0.000008  \n",
      "Epoch: [1][24500/36908] Elapsed 339m 32s (remain 171m 56s) Loss: 0.0005(0.0088) Grad: 132.8163  LR: 0.000007  \n",
      "Epoch: [1][24600/36908] Elapsed 340m 56s (remain 170m 33s) Loss: 0.0008(0.0087) Grad: 1071.3159  LR: 0.000007  \n",
      "Epoch: [1][24700/36908] Elapsed 342m 19s (remain 169m 10s) Loss: 0.0006(0.0087) Grad: 101.8660  LR: 0.000007  \n",
      "Epoch: [1][24800/36908] Elapsed 343m 43s (remain 167m 47s) Loss: 0.0014(0.0087) Grad: 1516.0391  LR: 0.000007  \n",
      "Epoch: [1][24900/36908] Elapsed 345m 7s (remain 166m 24s) Loss: 0.0062(0.0087) Grad: 1879.5885  LR: 0.000007  \n",
      "Epoch: [1][25000/36908] Elapsed 346m 31s (remain 165m 1s) Loss: 0.0025(0.0086) Grad: 5791.7949  LR: 0.000007  \n",
      "Epoch: [1][25100/36908] Elapsed 347m 54s (remain 163m 38s) Loss: 0.0005(0.0086) Grad: 1013.2838  LR: 0.000007  \n",
      "Epoch: [1][25200/36908] Elapsed 349m 17s (remain 162m 15s) Loss: 0.0022(0.0086) Grad: 39711.8711  LR: 0.000007  \n",
      "Epoch: [1][25300/36908] Elapsed 350m 40s (remain 160m 52s) Loss: 0.0081(0.0086) Grad: 24246.0547  LR: 0.000007  \n",
      "Epoch: [1][25400/36908] Elapsed 352m 5s (remain 159m 30s) Loss: 0.0035(0.0085) Grad: 1755.6769  LR: 0.000007  \n",
      "Epoch: [1][25500/36908] Elapsed 353m 28s (remain 158m 6s) Loss: 0.0002(0.0085) Grad: 36.7725  LR: 0.000007  \n",
      "Epoch: [1][25600/36908] Elapsed 354m 53s (remain 156m 44s) Loss: 0.0010(0.0085) Grad: 878.8773  LR: 0.000007  \n",
      "Epoch: [1][25700/36908] Elapsed 356m 18s (remain 155m 21s) Loss: 0.0005(0.0085) Grad: 124.1169  LR: 0.000007  \n",
      "Epoch: [1][25800/36908] Elapsed 357m 41s (remain 153m 59s) Loss: 0.0008(0.0085) Grad: 1337.9695  LR: 0.000007  \n",
      "Epoch: [1][25900/36908] Elapsed 359m 4s (remain 152m 35s) Loss: 0.0007(0.0084) Grad: 1002.1295  LR: 0.000007  \n",
      "Epoch: [1][26000/36908] Elapsed 360m 28s (remain 151m 12s) Loss: 0.0072(0.0084) Grad: 11665.5332  LR: 0.000007  \n",
      "Epoch: [1][26100/36908] Elapsed 361m 52s (remain 149m 50s) Loss: 0.0037(0.0084) Grad: 8842.1680  LR: 0.000007  \n",
      "Epoch: [1][26200/36908] Elapsed 363m 15s (remain 148m 26s) Loss: 0.0008(0.0084) Grad: 575.0217  LR: 0.000006  \n",
      "Epoch: [1][26300/36908] Elapsed 364m 39s (remain 147m 3s) Loss: 0.0057(0.0083) Grad: 7153.7871  LR: 0.000006  \n",
      "Epoch: [1][26400/36908] Elapsed 366m 3s (remain 145m 40s) Loss: 0.0009(0.0083) Grad: 788.7203  LR: 0.000006  \n",
      "Epoch: [1][26500/36908] Elapsed 367m 26s (remain 144m 17s) Loss: 0.0052(0.0083) Grad: 21074.0801  LR: 0.000006  \n",
      "Epoch: [1][26600/36908] Elapsed 368m 49s (remain 142m 54s) Loss: 0.0056(0.0083) Grad: 5672.2632  LR: 0.000006  \n",
      "Epoch: [1][26700/36908] Elapsed 370m 11s (remain 141m 30s) Loss: 0.0001(0.0083) Grad: 1818.5337  LR: 0.000006  \n",
      "Epoch: [1][26800/36908] Elapsed 371m 34s (remain 140m 7s) Loss: 0.0193(0.0082) Grad: 222016.1406  LR: 0.000006  \n",
      "Epoch: [1][26900/36908] Elapsed 372m 56s (remain 138m 43s) Loss: 0.0007(0.0082) Grad: 301.6106  LR: 0.000006  \n",
      "Epoch: [1][27000/36908] Elapsed 374m 19s (remain 137m 20s) Loss: 0.0003(0.0082) Grad: 209.9613  LR: 0.000006  \n",
      "Epoch: [1][27100/36908] Elapsed 375m 42s (remain 135m 57s) Loss: 0.0129(0.0082) Grad: 22016.4590  LR: 0.000006  \n",
      "Epoch: [1][27200/36908] Elapsed 377m 5s (remain 134m 34s) Loss: 0.0005(0.0082) Grad: 243.7472  LR: 0.000006  \n",
      "Epoch: [1][27300/36908] Elapsed 378m 28s (remain 133m 11s) Loss: 0.0047(0.0081) Grad: 13396.9512  LR: 0.000006  \n",
      "Epoch: [1][27400/36908] Elapsed 379m 53s (remain 131m 48s) Loss: 0.0005(0.0081) Grad: 265.0360  LR: 0.000006  \n",
      "Epoch: [1][27500/36908] Elapsed 381m 17s (remain 130m 25s) Loss: 0.0015(0.0081) Grad: 739.3554  LR: 0.000006  \n",
      "Epoch: [1][27600/36908] Elapsed 382m 40s (remain 129m 2s) Loss: 0.0002(0.0081) Grad: 505.4620  LR: 0.000006  \n",
      "Epoch: [1][27700/36908] Elapsed 384m 4s (remain 127m 39s) Loss: 0.0002(0.0081) Grad: 100.8914  LR: 0.000006  \n",
      "Epoch: [1][27800/36908] Elapsed 385m 27s (remain 126m 16s) Loss: 0.0016(0.0080) Grad: 1016.2654  LR: 0.000005  \n",
      "Epoch: [1][27900/36908] Elapsed 386m 50s (remain 124m 52s) Loss: 0.0001(0.0080) Grad: 40.4923  LR: 0.000005  \n",
      "Epoch: [1][28000/36908] Elapsed 388m 14s (remain 123m 29s) Loss: 0.0014(0.0080) Grad: 6728.1099  LR: 0.000005  \n",
      "Epoch: [1][28100/36908] Elapsed 389m 39s (remain 122m 7s) Loss: 0.0034(0.0080) Grad: 28680.4141  LR: 0.000005  \n",
      "Epoch: [1][28200/36908] Elapsed 391m 2s (remain 120m 43s) Loss: 0.0044(0.0080) Grad: 6944.8789  LR: 0.000005  \n",
      "Epoch: [1][28300/36908] Elapsed 392m 26s (remain 119m 21s) Loss: 0.0103(0.0080) Grad: 37229.3047  LR: 0.000005  \n",
      "Epoch: [1][28400/36908] Elapsed 393m 50s (remain 117m 58s) Loss: 0.0055(0.0079) Grad: 14129.1016  LR: 0.000005  \n",
      "Epoch: [1][28500/36908] Elapsed 395m 13s (remain 116m 34s) Loss: 0.0015(0.0079) Grad: 3911.1687  LR: 0.000005  \n",
      "Epoch: [1][28600/36908] Elapsed 396m 36s (remain 115m 11s) Loss: 0.0010(0.0079) Grad: 2794.9578  LR: 0.000005  \n",
      "Epoch: [1][28700/36908] Elapsed 398m 1s (remain 113m 48s) Loss: 0.0002(0.0079) Grad: 72.7538  LR: 0.000005  \n",
      "Epoch: [1][28800/36908] Elapsed 399m 23s (remain 112m 25s) Loss: 0.0002(0.0079) Grad: 504.2671  LR: 0.000005  \n",
      "Epoch: [1][28900/36908] Elapsed 400m 46s (remain 111m 2s) Loss: 0.0036(0.0078) Grad: 6626.4434  LR: 0.000005  \n",
      "Epoch: [1][29000/36908] Elapsed 402m 10s (remain 109m 39s) Loss: 0.0048(0.0078) Grad: 19424.7246  LR: 0.000005  \n",
      "Epoch: [1][29100/36908] Elapsed 403m 33s (remain 108m 15s) Loss: 0.0009(0.0078) Grad: 487.1216  LR: 0.000005  \n",
      "Epoch: [1][29200/36908] Elapsed 404m 57s (remain 106m 52s) Loss: 0.0001(0.0078) Grad: 53.6845  LR: 0.000005  \n",
      "Epoch: [1][29300/36908] Elapsed 406m 20s (remain 105m 29s) Loss: 0.0028(0.0078) Grad: 29482.4492  LR: 0.000005  \n",
      "Epoch: [1][29400/36908] Elapsed 407m 44s (remain 104m 6s) Loss: 0.0005(0.0078) Grad: 1551.0656  LR: 0.000005  \n",
      "Epoch: [1][29500/36908] Elapsed 409m 8s (remain 102m 43s) Loss: 0.0008(0.0077) Grad: 9196.8643  LR: 0.000004  \n",
      "Epoch: [1][29600/36908] Elapsed 410m 30s (remain 101m 20s) Loss: 0.0002(0.0077) Grad: 1212.9702  LR: 0.000004  \n",
      "Epoch: [1][29700/36908] Elapsed 411m 52s (remain 99m 56s) Loss: 0.0026(0.0077) Grad: 21323.9980  LR: 0.000004  \n",
      "Epoch: [1][29800/36908] Elapsed 413m 15s (remain 98m 33s) Loss: 0.0004(0.0077) Grad: 3490.2632  LR: 0.000004  \n",
      "Epoch: [1][29900/36908] Elapsed 414m 37s (remain 97m 9s) Loss: 0.0058(0.0077) Grad: 48722.5859  LR: 0.000004  \n",
      "Epoch: [1][30000/36908] Elapsed 416m 0s (remain 95m 46s) Loss: 0.0022(0.0077) Grad: 4758.7393  LR: 0.000004  \n",
      "Epoch: [1][30100/36908] Elapsed 417m 24s (remain 94m 23s) Loss: 0.0002(0.0076) Grad: 1052.3154  LR: 0.000004  \n",
      "Epoch: [1][30200/36908] Elapsed 418m 48s (remain 93m 0s) Loss: 0.0009(0.0076) Grad: 13144.6260  LR: 0.000004  \n",
      "Epoch: [1][30300/36908] Elapsed 420m 12s (remain 91m 37s) Loss: 0.0009(0.0076) Grad: 3502.3489  LR: 0.000004  \n",
      "Epoch: [1][30400/36908] Elapsed 421m 35s (remain 90m 14s) Loss: 0.0003(0.0076) Grad: 4704.9009  LR: 0.000004  \n",
      "Epoch: [1][30500/36908] Elapsed 423m 0s (remain 88m 51s) Loss: 0.0083(0.0076) Grad: 156320.7188  LR: 0.000004  \n",
      "Epoch: [1][30600/36908] Elapsed 424m 23s (remain 87m 28s) Loss: 0.0028(0.0076) Grad: 7607.6689  LR: 0.000004  \n",
      "Epoch: [1][30700/36908] Elapsed 425m 47s (remain 86m 5s) Loss: 0.0001(0.0076) Grad: 306.1517  LR: 0.000004  \n",
      "Epoch: [1][30800/36908] Elapsed 427m 11s (remain 84m 42s) Loss: 0.0011(0.0075) Grad: 4083.7402  LR: 0.000004  \n",
      "Epoch: [1][30900/36908] Elapsed 428m 35s (remain 83m 19s) Loss: 0.0000(0.0075) Grad: 33.1421  LR: 0.000004  \n",
      "Epoch: [1][31000/36908] Elapsed 429m 59s (remain 81m 55s) Loss: 0.0005(0.0075) Grad: 2636.4539  LR: 0.000004  \n",
      "Epoch: [1][31100/36908] Elapsed 431m 22s (remain 80m 32s) Loss: 0.0002(0.0075) Grad: 3024.1995  LR: 0.000003  \n",
      "Epoch: [1][31200/36908] Elapsed 432m 46s (remain 79m 9s) Loss: 0.0028(0.0075) Grad: 26887.3750  LR: 0.000003  \n",
      "Epoch: [1][31300/36908] Elapsed 434m 10s (remain 77m 46s) Loss: 0.0018(0.0075) Grad: 19065.1289  LR: 0.000003  \n",
      "Epoch: [1][31400/36908] Elapsed 435m 34s (remain 76m 23s) Loss: 0.0064(0.0074) Grad: 61123.4883  LR: 0.000003  \n",
      "Epoch: [1][31500/36908] Elapsed 436m 58s (remain 75m 0s) Loss: 0.0003(0.0074) Grad: 576.2013  LR: 0.000003  \n",
      "Epoch: [1][31600/36908] Elapsed 438m 22s (remain 73m 37s) Loss: 0.0097(0.0074) Grad: 53551.2148  LR: 0.000003  \n",
      "Epoch: [1][31700/36908] Elapsed 439m 46s (remain 72m 13s) Loss: 0.0904(0.0074) Grad: 741701.0000  LR: 0.000003  \n",
      "Epoch: [1][31800/36908] Elapsed 441m 8s (remain 70m 50s) Loss: 0.0034(0.0074) Grad: 13194.4512  LR: 0.000003  \n",
      "Epoch: [1][31900/36908] Elapsed 442m 31s (remain 69m 27s) Loss: 0.0050(0.0074) Grad: 148121.5625  LR: 0.000003  \n",
      "Epoch: [1][32000/36908] Elapsed 443m 55s (remain 68m 4s) Loss: 0.0001(0.0074) Grad: 54.2399  LR: 0.000003  \n",
      "Epoch: [1][32100/36908] Elapsed 445m 20s (remain 66m 41s) Loss: 0.0057(0.0074) Grad: 43941.4297  LR: 0.000003  \n",
      "Epoch: [1][32200/36908] Elapsed 446m 43s (remain 65m 18s) Loss: 0.0019(0.0073) Grad: 41548.2578  LR: 0.000003  \n",
      "Epoch: [1][32300/36908] Elapsed 448m 8s (remain 63m 55s) Loss: 0.0091(0.0073) Grad: 166431.7812  LR: 0.000003  \n",
      "Epoch: [1][32400/36908] Elapsed 449m 32s (remain 62m 31s) Loss: 0.0016(0.0073) Grad: 42824.2305  LR: 0.000003  \n",
      "Epoch: [1][32500/36908] Elapsed 450m 56s (remain 61m 8s) Loss: 0.0002(0.0073) Grad: 246.2269  LR: 0.000003  \n",
      "Epoch: [1][32600/36908] Elapsed 452m 19s (remain 59m 45s) Loss: 0.0054(0.0073) Grad: 63165.0078  LR: 0.000003  \n",
      "Epoch: [1][32700/36908] Elapsed 453m 44s (remain 58m 22s) Loss: 0.0015(0.0073) Grad: 4907.8647  LR: 0.000003  \n",
      "Epoch: [1][32800/36908] Elapsed 455m 7s (remain 56m 59s) Loss: 0.0003(0.0073) Grad: 437.1704  LR: 0.000002  \n",
      "Epoch: [1][32900/36908] Elapsed 456m 30s (remain 55m 35s) Loss: 0.0005(0.0072) Grad: 4381.8447  LR: 0.000002  \n",
      "Epoch: [1][33000/36908] Elapsed 457m 54s (remain 54m 12s) Loss: 0.0020(0.0072) Grad: 44878.7031  LR: 0.000002  \n",
      "Epoch: [1][33100/36908] Elapsed 459m 18s (remain 52m 49s) Loss: 0.0002(0.0072) Grad: 342.8195  LR: 0.000002  \n",
      "Epoch: [1][33200/36908] Elapsed 460m 41s (remain 51m 26s) Loss: 0.0021(0.0072) Grad: 4069.5369  LR: 0.000002  \n",
      "Epoch: [1][33300/36908] Elapsed 462m 3s (remain 50m 2s) Loss: 0.0009(0.0072) Grad: 6313.5630  LR: 0.000002  \n",
      "Epoch: [1][33400/36908] Elapsed 463m 27s (remain 48m 39s) Loss: 0.0002(0.0072) Grad: 326.9547  LR: 0.000002  \n",
      "Epoch: [1][33500/36908] Elapsed 464m 50s (remain 47m 16s) Loss: 0.0020(0.0072) Grad: 12080.7812  LR: 0.000002  \n",
      "Epoch: [1][33600/36908] Elapsed 466m 13s (remain 45m 53s) Loss: 0.0000(0.0072) Grad: 223.0003  LR: 0.000002  \n",
      "Epoch: [1][33700/36908] Elapsed 467m 37s (remain 44m 30s) Loss: 0.0016(0.0071) Grad: 48646.3984  LR: 0.000002  \n",
      "Epoch: [1][33800/36908] Elapsed 469m 3s (remain 43m 6s) Loss: 0.0013(0.0071) Grad: 2419.4421  LR: 0.000002  \n",
      "Epoch: [1][33900/36908] Elapsed 470m 27s (remain 41m 43s) Loss: 0.0009(0.0071) Grad: 5759.1226  LR: 0.000002  \n",
      "Epoch: [1][34000/36908] Elapsed 471m 51s (remain 40m 20s) Loss: 0.0005(0.0071) Grad: 1559.7683  LR: 0.000002  \n",
      "Epoch: [1][34100/36908] Elapsed 473m 15s (remain 38m 57s) Loss: 0.0029(0.0071) Grad: 262403.6562  LR: 0.000002  \n",
      "Epoch: [1][34200/36908] Elapsed 474m 40s (remain 37m 34s) Loss: 0.0073(0.0071) Grad: 8335.6387  LR: 0.000002  \n",
      "Epoch: [1][34300/36908] Elapsed 476m 3s (remain 36m 10s) Loss: 0.0037(0.0071) Grad: 51029.2344  LR: 0.000002  \n",
      "Epoch: [1][34400/36908] Elapsed 477m 27s (remain 34m 47s) Loss: 0.0002(0.0071) Grad: 1463.0371  LR: 0.000002  \n",
      "Epoch: [1][34500/36908] Elapsed 478m 51s (remain 33m 24s) Loss: 0.0068(0.0070) Grad: 10552.1182  LR: 0.000001  \n",
      "Epoch: [1][34600/36908] Elapsed 480m 14s (remain 32m 1s) Loss: 0.0028(0.0070) Grad: 5292.5674  LR: 0.000001  \n",
      "Epoch: [1][34700/36908] Elapsed 481m 39s (remain 30m 38s) Loss: 0.0001(0.0070) Grad: 540.8120  LR: 0.000001  \n",
      "Epoch: [1][34800/36908] Elapsed 483m 4s (remain 29m 14s) Loss: 0.0003(0.0070) Grad: 2293.9258  LR: 0.000001  \n",
      "Epoch: [1][34900/36908] Elapsed 484m 28s (remain 27m 51s) Loss: 0.0020(0.0070) Grad: 72023.6797  LR: 0.000001  \n",
      "Epoch: [1][35000/36908] Elapsed 485m 52s (remain 26m 28s) Loss: 0.0018(0.0070) Grad: 46046.9141  LR: 0.000001  \n",
      "Epoch: [1][35100/36908] Elapsed 487m 17s (remain 25m 5s) Loss: 0.0001(0.0070) Grad: 437.4203  LR: 0.000001  \n",
      "Epoch: [1][35200/36908] Elapsed 488m 40s (remain 23m 41s) Loss: 0.0004(0.0070) Grad: 1774.8335  LR: 0.000001  \n",
      "Epoch: [1][35300/36908] Elapsed 490m 2s (remain 22m 18s) Loss: 0.0027(0.0069) Grad: 26083.1699  LR: 0.000001  \n",
      "Epoch: [1][35400/36908] Elapsed 491m 27s (remain 20m 55s) Loss: 0.0039(0.0069) Grad: 89415.7031  LR: 0.000001  \n",
      "Epoch: [1][35500/36908] Elapsed 492m 50s (remain 19m 31s) Loss: 0.0007(0.0069) Grad: 33396.9570  LR: 0.000001  \n",
      "Epoch: [1][35600/36908] Elapsed 494m 13s (remain 18m 8s) Loss: 0.0010(0.0069) Grad: 7005.7319  LR: 0.000001  \n",
      "Epoch: [1][35700/36908] Elapsed 495m 36s (remain 16m 45s) Loss: 0.0008(0.0069) Grad: 4346.9995  LR: 0.000001  \n",
      "Epoch: [1][35800/36908] Elapsed 497m 1s (remain 15m 22s) Loss: 0.0092(0.0069) Grad: 293595.1562  LR: 0.000001  \n",
      "Epoch: [1][35900/36908] Elapsed 498m 25s (remain 13m 58s) Loss: 0.0036(0.0069) Grad: 18923.9980  LR: 0.000001  \n",
      "Epoch: [1][36000/36908] Elapsed 499m 49s (remain 12m 35s) Loss: 0.0012(0.0069) Grad: 8322.5645  LR: 0.000001  \n",
      "Epoch: [1][36100/36908] Elapsed 501m 13s (remain 11m 12s) Loss: 0.0024(0.0069) Grad: 68752.7500  LR: 0.000000  \n",
      "Epoch: [1][36200/36908] Elapsed 502m 37s (remain 9m 48s) Loss: 0.0197(0.0069) Grad: inf  LR: 0.000000  \n",
      "Epoch: [1][36300/36908] Elapsed 504m 3s (remain 8m 25s) Loss: 0.0002(0.0068) Grad: 231.9758  LR: 0.000000  \n",
      "Epoch: [1][36400/36908] Elapsed 505m 27s (remain 7m 2s) Loss: 0.0042(0.0068) Grad: 3678.4465  LR: 0.000000  \n",
      "Epoch: [1][36500/36908] Elapsed 506m 50s (remain 5m 39s) Loss: 0.0023(0.0068) Grad: 3711.2297  LR: 0.000000  \n",
      "Epoch: [1][36600/36908] Elapsed 508m 12s (remain 4m 15s) Loss: 0.0011(0.0068) Grad: 2554.0608  LR: 0.000000  \n",
      "Epoch: [1][36700/36908] Elapsed 509m 37s (remain 2m 52s) Loss: 0.0002(0.0068) Grad: 1162.9025  LR: 0.000000  \n",
      "Epoch: [1][36800/36908] Elapsed 511m 1s (remain 1m 29s) Loss: 0.0011(0.0068) Grad: 5808.0962  LR: 0.000000  \n",
      "Epoch: [1][36900/36908] Elapsed 512m 24s (remain 0m 5s) Loss: 0.0077(0.0068) Grad: 139119.0781  LR: 0.000000  \n",
      "Epoch: [1][36907/36908] Elapsed 512m 30s (remain 0m 0s) Loss: 0.0009(0.0068) Grad: 399.0365  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 12m 55s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 30s (remain 5m 28s) Loss: 0.0001(0.0051) \n",
      "EVAL: [200/1192] Elapsed 0m 59s (remain 4m 53s) Loss: 0.0001(0.0063) \n",
      "EVAL: [300/1192] Elapsed 1m 29s (remain 4m 25s) Loss: 0.0012(0.0100) \n",
      "EVAL: [400/1192] Elapsed 1m 59s (remain 3m 55s) Loss: 0.0326(0.0098) \n",
      "EVAL: [500/1192] Elapsed 2m 28s (remain 3m 24s) Loss: 0.0353(0.0089) \n",
      "EVAL: [600/1192] Elapsed 2m 57s (remain 2m 55s) Loss: 0.1422(0.0089) \n",
      "EVAL: [700/1192] Elapsed 3m 27s (remain 2m 25s) Loss: 0.0054(0.0101) \n",
      "EVAL: [800/1192] Elapsed 3m 56s (remain 1m 55s) Loss: 0.0101(0.0099) \n",
      "EVAL: [900/1192] Elapsed 4m 26s (remain 1m 25s) Loss: 0.0014(0.0095) \n",
      "EVAL: [1000/1192] Elapsed 4m 55s (remain 0m 56s) Loss: 0.0000(0.0092) \n",
      "EVAL: [1100/1192] Elapsed 5m 24s (remain 0m 26s) Loss: 0.0070(0.0088) \n",
      "EVAL: [1191/1192] Elapsed 5m 51s (remain 0m 0s) Loss: 0.0100(0.0083) \n",
      "Epoch 1 - avg_train_loss: 0.0068  avg_val_loss: 0.0083  time: 31106s\n",
      "Epoch 1 - Score: 0.8864\n",
      "Epoch 1 - Save Best Score: 0.8864 Model\n",
      "========== fold: 2 training ==========\n",
      "get pseudo plain from ../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_2.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_2.npy\n",
      "(612602, 6) (612602, 950)\n",
      "(100000, 7)\n",
      "(110725, 11)\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/36908] Elapsed 0m 1s (remain 686m 20s) Loss: 0.3521(0.3521) Grad: 95587.7266  LR: 0.000000  \n",
      "Epoch: [1][100/36908] Elapsed 1m 24s (remain 510m 42s) Loss: 0.3305(0.3451) Grad: 44070.9609  LR: 0.000001  \n",
      "Epoch: [1][200/36908] Elapsed 2m 47s (remain 509m 47s) Loss: 0.2617(0.3217) Grad: 39294.2344  LR: 0.000001  \n",
      "Epoch: [1][300/36908] Elapsed 4m 12s (remain 511m 2s) Loss: 0.1645(0.2865) Grad: 32630.9141  LR: 0.000002  \n",
      "Epoch: [1][400/36908] Elapsed 5m 34s (remain 508m 7s) Loss: 0.0898(0.2441) Grad: 7343.1343  LR: 0.000002  \n",
      "Epoch: [1][500/36908] Elapsed 6m 58s (remain 506m 51s) Loss: 0.0384(0.2040) Grad: 795.6589  LR: 0.000003  \n",
      "Epoch: [1][600/36908] Elapsed 8m 22s (remain 505m 28s) Loss: 0.0368(0.1759) Grad: 654.7165  LR: 0.000003  \n",
      "Epoch: [1][700/36908] Elapsed 9m 44s (remain 503m 25s) Loss: 0.0226(0.1558) Grad: 468.0547  LR: 0.000004  \n",
      "Epoch: [1][800/36908] Elapsed 11m 7s (remain 501m 45s) Loss: 0.0508(0.1398) Grad: 6448.4229  LR: 0.000004  \n",
      "Epoch: [1][900/36908] Elapsed 12m 31s (remain 500m 43s) Loss: 0.0298(0.1268) Grad: 9943.4385  LR: 0.000005  \n",
      "Epoch: [1][1000/36908] Elapsed 13m 55s (remain 499m 19s) Loss: 0.0097(0.1161) Grad: 591.4811  LR: 0.000005  \n",
      "Epoch: [1][1100/36908] Elapsed 15m 18s (remain 497m 42s) Loss: 0.0076(0.1069) Grad: 3701.6655  LR: 0.000006  \n",
      "Epoch: [1][1200/36908] Elapsed 16m 40s (remain 495m 54s) Loss: 0.0093(0.0994) Grad: 1365.1056  LR: 0.000007  \n",
      "Epoch: [1][1300/36908] Elapsed 18m 5s (remain 494m 58s) Loss: 0.0047(0.0927) Grad: 773.6260  LR: 0.000007  \n",
      "Epoch: [1][1400/36908] Elapsed 19m 29s (remain 494m 6s) Loss: 0.0311(0.0871) Grad: 8427.0889  LR: 0.000008  \n",
      "Epoch: [1][1500/36908] Elapsed 20m 53s (remain 492m 42s) Loss: 0.0100(0.0819) Grad: 1574.9458  LR: 0.000008  \n",
      "Epoch: [1][1600/36908] Elapsed 22m 17s (remain 491m 25s) Loss: 0.0045(0.0775) Grad: 4049.6167  LR: 0.000009  \n",
      "Epoch: [1][1700/36908] Elapsed 23m 40s (remain 490m 6s) Loss: 0.0030(0.0735) Grad: 613.4923  LR: 0.000009  \n",
      "Epoch: [1][1800/36908] Elapsed 25m 4s (remain 488m 55s) Loss: 0.0001(0.0700) Grad: 37.7060  LR: 0.000010  \n",
      "Epoch: [1][1900/36908] Elapsed 26m 29s (remain 487m 43s) Loss: 0.0014(0.0668) Grad: 581.9473  LR: 0.000010  \n",
      "Epoch: [1][2000/36908] Elapsed 27m 52s (remain 486m 15s) Loss: 0.0049(0.0639) Grad: 1822.8661  LR: 0.000011  \n",
      "Epoch: [1][2100/36908] Elapsed 29m 15s (remain 484m 43s) Loss: 0.0020(0.0613) Grad: 486.6499  LR: 0.000011  \n",
      "Epoch: [1][2200/36908] Elapsed 30m 39s (remain 483m 26s) Loss: 0.0037(0.0590) Grad: 434.6460  LR: 0.000012  \n",
      "Epoch: [1][2300/36908] Elapsed 32m 2s (remain 482m 1s) Loss: 0.0087(0.0567) Grad: 2368.9805  LR: 0.000012  \n",
      "Epoch: [1][2400/36908] Elapsed 33m 26s (remain 480m 35s) Loss: 0.0011(0.0547) Grad: 787.9074  LR: 0.000013  \n",
      "Epoch: [1][2500/36908] Elapsed 34m 49s (remain 479m 2s) Loss: 0.0017(0.0527) Grad: 471.3582  LR: 0.000014  \n",
      "Epoch: [1][2600/36908] Elapsed 36m 12s (remain 477m 38s) Loss: 0.0036(0.0510) Grad: 551.2031  LR: 0.000014  \n",
      "Epoch: [1][2700/36908] Elapsed 37m 35s (remain 476m 8s) Loss: 0.0025(0.0493) Grad: 812.9386  LR: 0.000015  \n",
      "Epoch: [1][2800/36908] Elapsed 38m 58s (remain 474m 35s) Loss: 0.0005(0.0478) Grad: 129.6071  LR: 0.000015  \n",
      "Epoch: [1][2900/36908] Elapsed 40m 22s (remain 473m 21s) Loss: 0.0097(0.0464) Grad: 9411.9727  LR: 0.000016  \n",
      "Epoch: [1][3000/36908] Elapsed 41m 47s (remain 472m 6s) Loss: 0.0006(0.0450) Grad: 121.8575  LR: 0.000016  \n",
      "Epoch: [1][3100/36908] Elapsed 43m 10s (remain 470m 36s) Loss: 0.0066(0.0438) Grad: 1466.5192  LR: 0.000017  \n",
      "Epoch: [1][3200/36908] Elapsed 44m 32s (remain 468m 59s) Loss: 0.0010(0.0426) Grad: 296.7131  LR: 0.000017  \n",
      "Epoch: [1][3300/36908] Elapsed 45m 56s (remain 467m 41s) Loss: 0.0042(0.0415) Grad: 1125.2081  LR: 0.000018  \n",
      "Epoch: [1][3400/36908] Elapsed 47m 19s (remain 466m 17s) Loss: 0.0004(0.0405) Grad: 42.4652  LR: 0.000018  \n",
      "Epoch: [1][3500/36908] Elapsed 48m 41s (remain 464m 41s) Loss: 0.0172(0.0395) Grad: 2692.1978  LR: 0.000019  \n",
      "Epoch: [1][3600/36908] Elapsed 50m 4s (remain 463m 8s) Loss: 0.0005(0.0385) Grad: 100.9364  LR: 0.000020  \n",
      "Epoch: [1][3700/36908] Elapsed 51m 26s (remain 461m 31s) Loss: 0.0147(0.0377) Grad: 4185.0459  LR: 0.000020  \n",
      "Epoch: [1][3800/36908] Elapsed 52m 48s (remain 459m 57s) Loss: 0.0346(0.0368) Grad: 12030.9736  LR: 0.000020  \n",
      "Epoch: [1][3900/36908] Elapsed 54m 11s (remain 458m 35s) Loss: 0.0081(0.0360) Grad: 880.5052  LR: 0.000020  \n",
      "Epoch: [1][4000/36908] Elapsed 55m 35s (remain 457m 9s) Loss: 0.0029(0.0352) Grad: 1164.1931  LR: 0.000020  \n",
      "Epoch: [1][4100/36908] Elapsed 56m 58s (remain 455m 43s) Loss: 0.0046(0.0344) Grad: 903.5927  LR: 0.000020  \n",
      "Epoch: [1][4200/36908] Elapsed 58m 21s (remain 454m 18s) Loss: 0.0027(0.0337) Grad: 635.1486  LR: 0.000020  \n",
      "Epoch: [1][4300/36908] Elapsed 59m 43s (remain 452m 48s) Loss: 0.0013(0.0331) Grad: 91.4442  LR: 0.000020  \n",
      "Epoch: [1][4400/36908] Elapsed 61m 5s (remain 451m 17s) Loss: 0.0082(0.0324) Grad: 5053.2788  LR: 0.000020  \n",
      "Epoch: [1][4500/36908] Elapsed 62m 28s (remain 449m 47s) Loss: 0.0128(0.0318) Grad: 2508.5525  LR: 0.000020  \n",
      "Epoch: [1][4600/36908] Elapsed 63m 51s (remain 448m 26s) Loss: 0.0069(0.0312) Grad: 702.8420  LR: 0.000019  \n",
      "Epoch: [1][4700/36908] Elapsed 65m 15s (remain 447m 5s) Loss: 0.0000(0.0307) Grad: 7.9499  LR: 0.000019  \n",
      "Epoch: [1][4800/36908] Elapsed 66m 38s (remain 445m 37s) Loss: 0.0006(0.0301) Grad: 44.3602  LR: 0.000019  \n",
      "Epoch: [1][4900/36908] Elapsed 68m 0s (remain 444m 8s) Loss: 0.0009(0.0296) Grad: 272.0128  LR: 0.000019  \n",
      "Epoch: [1][5000/36908] Elapsed 69m 24s (remain 442m 49s) Loss: 0.0010(0.0291) Grad: 475.2032  LR: 0.000019  \n",
      "Epoch: [1][5100/36908] Elapsed 70m 48s (remain 441m 28s) Loss: 0.0004(0.0286) Grad: 57.8087  LR: 0.000019  \n",
      "Epoch: [1][5200/36908] Elapsed 72m 10s (remain 439m 59s) Loss: 0.0005(0.0282) Grad: 463.5536  LR: 0.000019  \n",
      "Epoch: [1][5300/36908] Elapsed 73m 33s (remain 438m 33s) Loss: 0.0025(0.0277) Grad: 1015.5806  LR: 0.000019  \n",
      "Epoch: [1][5400/36908] Elapsed 74m 55s (remain 437m 6s) Loss: 0.0041(0.0273) Grad: 1191.4827  LR: 0.000019  \n",
      "Epoch: [1][5500/36908] Elapsed 76m 18s (remain 435m 38s) Loss: 0.0004(0.0269) Grad: 254.9940  LR: 0.000019  \n",
      "Epoch: [1][5600/36908] Elapsed 77m 42s (remain 434m 20s) Loss: 0.0009(0.0265) Grad: 338.5721  LR: 0.000019  \n",
      "Epoch: [1][5700/36908] Elapsed 79m 5s (remain 432m 56s) Loss: 0.0006(0.0261) Grad: 82.0381  LR: 0.000019  \n",
      "Epoch: [1][5800/36908] Elapsed 80m 28s (remain 431m 29s) Loss: 0.0011(0.0257) Grad: 265.9063  LR: 0.000019  \n",
      "Epoch: [1][5900/36908] Elapsed 81m 50s (remain 430m 4s) Loss: 0.0002(0.0253) Grad: 107.0495  LR: 0.000019  \n",
      "Epoch: [1][6000/36908] Elapsed 83m 13s (remain 428m 37s) Loss: 0.0011(0.0250) Grad: 278.8624  LR: 0.000019  \n",
      "Epoch: [1][6100/36908] Elapsed 84m 37s (remain 427m 16s) Loss: 0.0011(0.0246) Grad: 120.4433  LR: 0.000019  \n",
      "Epoch: [1][6200/36908] Elapsed 85m 59s (remain 425m 50s) Loss: 0.0100(0.0243) Grad: 5433.8511  LR: 0.000018  \n",
      "Epoch: [1][6300/36908] Elapsed 87m 21s (remain 424m 20s) Loss: 0.0008(0.0240) Grad: 293.7741  LR: 0.000018  \n",
      "Epoch: [1][6400/36908] Elapsed 88m 45s (remain 423m 2s) Loss: 0.0105(0.0236) Grad: 3916.2036  LR: 0.000018  \n",
      "Epoch: [1][6500/36908] Elapsed 90m 8s (remain 421m 35s) Loss: 0.0003(0.0233) Grad: 20.9637  LR: 0.000018  \n",
      "Epoch: [1][6600/36908] Elapsed 91m 30s (remain 420m 7s) Loss: 0.0051(0.0230) Grad: 2264.3701  LR: 0.000018  \n",
      "Epoch: [1][6700/36908] Elapsed 92m 52s (remain 418m 41s) Loss: 0.0171(0.0227) Grad: 10654.3193  LR: 0.000018  \n",
      "Epoch: [1][6800/36908] Elapsed 94m 14s (remain 417m 13s) Loss: 0.0002(0.0225) Grad: 82.1841  LR: 0.000018  \n",
      "Epoch: [1][6900/36908] Elapsed 95m 38s (remain 415m 50s) Loss: 0.0002(0.0222) Grad: 168.2739  LR: 0.000018  \n",
      "Epoch: [1][7000/36908] Elapsed 97m 0s (remain 414m 22s) Loss: 0.0059(0.0220) Grad: 1921.4000  LR: 0.000018  \n",
      "Epoch: [1][7100/36908] Elapsed 98m 22s (remain 412m 58s) Loss: 0.0082(0.0217) Grad: 4864.8174  LR: 0.000018  \n",
      "Epoch: [1][7200/36908] Elapsed 99m 46s (remain 411m 37s) Loss: 0.0050(0.0214) Grad: 2651.8062  LR: 0.000018  \n",
      "Epoch: [1][7300/36908] Elapsed 101m 8s (remain 410m 10s) Loss: 0.0004(0.0212) Grad: 333.6972  LR: 0.000018  \n",
      "Epoch: [1][7400/36908] Elapsed 102m 32s (remain 408m 49s) Loss: 0.0003(0.0209) Grad: 132.6381  LR: 0.000018  \n",
      "Epoch: [1][7500/36908] Elapsed 103m 55s (remain 407m 24s) Loss: 0.0006(0.0207) Grad: 93.9870  LR: 0.000018  \n",
      "Epoch: [1][7600/36908] Elapsed 105m 17s (remain 405m 56s) Loss: 0.0067(0.0205) Grad: 2109.5874  LR: 0.000018  \n",
      "Epoch: [1][7700/36908] Elapsed 106m 38s (remain 404m 28s) Loss: 0.0005(0.0203) Grad: 89.9465  LR: 0.000018  \n",
      "Epoch: [1][7800/36908] Elapsed 108m 2s (remain 403m 5s) Loss: 0.0041(0.0200) Grad: 1268.7452  LR: 0.000018  \n",
      "Epoch: [1][7900/36908] Elapsed 109m 25s (remain 401m 44s) Loss: 0.0067(0.0198) Grad: 3974.1213  LR: 0.000017  \n",
      "Epoch: [1][8000/36908] Elapsed 110m 47s (remain 400m 18s) Loss: 0.0114(0.0196) Grad: 4513.4473  LR: 0.000017  \n",
      "Epoch: [1][8100/36908] Elapsed 112m 11s (remain 398m 56s) Loss: 0.0004(0.0194) Grad: 33.6686  LR: 0.000017  \n",
      "Epoch: [1][8200/36908] Elapsed 113m 33s (remain 397m 29s) Loss: 0.0004(0.0192) Grad: 67.5399  LR: 0.000017  \n",
      "Epoch: [1][8300/36908] Elapsed 114m 55s (remain 396m 2s) Loss: 0.0033(0.0191) Grad: 2566.9485  LR: 0.000017  \n",
      "Epoch: [1][8400/36908] Elapsed 116m 19s (remain 394m 43s) Loss: 0.0025(0.0189) Grad: 606.7093  LR: 0.000017  \n",
      "Epoch: [1][8500/36908] Elapsed 117m 42s (remain 393m 19s) Loss: 0.0013(0.0187) Grad: 1456.4551  LR: 0.000017  \n",
      "Epoch: [1][8600/36908] Elapsed 119m 6s (remain 391m 59s) Loss: 0.0006(0.0185) Grad: 576.9790  LR: 0.000017  \n",
      "Epoch: [1][8700/36908] Elapsed 120m 29s (remain 390m 36s) Loss: 0.0027(0.0183) Grad: 1604.5452  LR: 0.000017  \n",
      "Epoch: [1][8800/36908] Elapsed 121m 53s (remain 389m 15s) Loss: 0.0013(0.0182) Grad: 1463.3309  LR: 0.000017  \n",
      "Epoch: [1][8900/36908] Elapsed 123m 16s (remain 387m 53s) Loss: 0.0053(0.0180) Grad: 7833.6597  LR: 0.000017  \n",
      "Epoch: [1][9000/36908] Elapsed 124m 39s (remain 386m 29s) Loss: 0.0002(0.0178) Grad: 83.0365  LR: 0.000017  \n",
      "Epoch: [1][9100/36908] Elapsed 126m 2s (remain 385m 5s) Loss: 0.0003(0.0177) Grad: 156.1537  LR: 0.000017  \n",
      "Epoch: [1][9200/36908] Elapsed 127m 25s (remain 383m 42s) Loss: 0.0001(0.0175) Grad: 81.2297  LR: 0.000017  \n",
      "Epoch: [1][9300/36908] Elapsed 128m 48s (remain 382m 18s) Loss: 0.0015(0.0174) Grad: 722.3447  LR: 0.000017  \n",
      "Epoch: [1][9400/36908] Elapsed 130m 11s (remain 380m 55s) Loss: 0.0077(0.0172) Grad: 10438.1943  LR: 0.000017  \n",
      "Epoch: [1][9500/36908] Elapsed 131m 34s (remain 379m 33s) Loss: 0.0120(0.0171) Grad: 1708.3081  LR: 0.000017  \n",
      "Epoch: [1][9600/36908] Elapsed 132m 57s (remain 378m 8s) Loss: 0.0004(0.0169) Grad: 61.6986  LR: 0.000016  \n",
      "Epoch: [1][9700/36908] Elapsed 134m 19s (remain 376m 42s) Loss: 0.0080(0.0168) Grad: 4040.0393  LR: 0.000016  \n",
      "Epoch: [1][9800/36908] Elapsed 135m 42s (remain 375m 18s) Loss: 0.0100(0.0166) Grad: 7014.7432  LR: 0.000016  \n",
      "Epoch: [1][9900/36908] Elapsed 137m 4s (remain 373m 53s) Loss: 0.0142(0.0165) Grad: 24955.7207  LR: 0.000016  \n",
      "Epoch: [1][10000/36908] Elapsed 138m 27s (remain 372m 30s) Loss: 0.0013(0.0164) Grad: 1035.4763  LR: 0.000016  \n",
      "Epoch: [1][10100/36908] Elapsed 139m 51s (remain 371m 11s) Loss: 0.0122(0.0162) Grad: 13122.2344  LR: 0.000016  \n",
      "Epoch: [1][10200/36908] Elapsed 141m 14s (remain 369m 47s) Loss: 0.0008(0.0161) Grad: 893.7460  LR: 0.000016  \n",
      "Epoch: [1][10300/36908] Elapsed 142m 38s (remain 368m 27s) Loss: 0.0077(0.0160) Grad: 4142.0186  LR: 0.000016  \n",
      "Epoch: [1][10400/36908] Elapsed 144m 2s (remain 367m 5s) Loss: 0.0016(0.0159) Grad: 115.8980  LR: 0.000016  \n",
      "Epoch: [1][10500/36908] Elapsed 145m 25s (remain 365m 42s) Loss: 0.0000(0.0158) Grad: 47.8658  LR: 0.000016  \n",
      "Epoch: [1][10600/36908] Elapsed 146m 48s (remain 364m 18s) Loss: 0.0002(0.0156) Grad: 46.3382  LR: 0.000016  \n",
      "Epoch: [1][10700/36908] Elapsed 148m 13s (remain 362m 59s) Loss: 0.0004(0.0155) Grad: 58.1730  LR: 0.000016  \n",
      "Epoch: [1][10800/36908] Elapsed 149m 37s (remain 361m 38s) Loss: 0.0231(0.0154) Grad: 24514.2227  LR: 0.000016  \n",
      "Epoch: [1][10900/36908] Elapsed 151m 0s (remain 360m 17s) Loss: 0.0110(0.0153) Grad: 3038.3145  LR: 0.000016  \n",
      "Epoch: [1][11000/36908] Elapsed 152m 23s (remain 358m 52s) Loss: 0.0044(0.0152) Grad: 1987.3542  LR: 0.000016  \n",
      "Epoch: [1][11100/36908] Elapsed 153m 46s (remain 357m 29s) Loss: 0.0036(0.0151) Grad: 3204.2410  LR: 0.000016  \n",
      "Epoch: [1][11200/36908] Elapsed 155m 10s (remain 356m 8s) Loss: 0.0001(0.0150) Grad: 17.7719  LR: 0.000015  \n",
      "Epoch: [1][11300/36908] Elapsed 156m 33s (remain 354m 44s) Loss: 0.0002(0.0149) Grad: 307.6320  LR: 0.000015  \n",
      "Epoch: [1][11400/36908] Elapsed 157m 56s (remain 353m 22s) Loss: 0.0100(0.0148) Grad: 32355.8379  LR: 0.000015  \n",
      "Epoch: [1][11500/36908] Elapsed 159m 19s (remain 351m 58s) Loss: 0.0001(0.0147) Grad: 103.4328  LR: 0.000015  \n",
      "Epoch: [1][11600/36908] Elapsed 160m 43s (remain 350m 36s) Loss: 0.0009(0.0146) Grad: 455.0233  LR: 0.000015  \n",
      "Epoch: [1][11700/36908] Elapsed 162m 5s (remain 349m 11s) Loss: 0.0123(0.0145) Grad: 12815.8799  LR: 0.000015  \n",
      "Epoch: [1][11800/36908] Elapsed 163m 28s (remain 347m 48s) Loss: 0.0020(0.0144) Grad: 936.2132  LR: 0.000015  \n",
      "Epoch: [1][11900/36908] Elapsed 164m 52s (remain 346m 27s) Loss: 0.0076(0.0143) Grad: 5750.4170  LR: 0.000015  \n",
      "Epoch: [1][12000/36908] Elapsed 166m 16s (remain 345m 4s) Loss: 0.0042(0.0142) Grad: 6785.7031  LR: 0.000015  \n",
      "Epoch: [1][12100/36908] Elapsed 167m 39s (remain 343m 41s) Loss: 0.0000(0.0141) Grad: 8.5112  LR: 0.000015  \n",
      "Epoch: [1][12200/36908] Elapsed 169m 2s (remain 342m 18s) Loss: 0.0000(0.0140) Grad: 18.5493  LR: 0.000015  \n",
      "Epoch: [1][12300/36908] Elapsed 170m 24s (remain 340m 53s) Loss: 0.0043(0.0139) Grad: 1877.7251  LR: 0.000015  \n",
      "Epoch: [1][12400/36908] Elapsed 171m 47s (remain 339m 29s) Loss: 0.0032(0.0138) Grad: 1144.1655  LR: 0.000015  \n",
      "Epoch: [1][12500/36908] Elapsed 173m 11s (remain 338m 7s) Loss: 0.0070(0.0137) Grad: 3251.6970  LR: 0.000015  \n",
      "Epoch: [1][12600/36908] Elapsed 174m 34s (remain 336m 44s) Loss: 0.0008(0.0136) Grad: 2887.0725  LR: 0.000015  \n",
      "Epoch: [1][12700/36908] Elapsed 175m 56s (remain 335m 20s) Loss: 0.0002(0.0136) Grad: 43.3022  LR: 0.000015  \n",
      "Epoch: [1][12800/36908] Elapsed 177m 19s (remain 333m 55s) Loss: 0.0153(0.0135) Grad: 23676.0703  LR: 0.000015  \n",
      "Epoch: [1][12900/36908] Elapsed 178m 41s (remain 332m 31s) Loss: 0.0002(0.0134) Grad: 269.8430  LR: 0.000014  \n",
      "Epoch: [1][13000/36908] Elapsed 180m 4s (remain 331m 7s) Loss: 0.0005(0.0133) Grad: 970.1946  LR: 0.000014  \n",
      "Epoch: [1][13100/36908] Elapsed 181m 26s (remain 329m 42s) Loss: 0.0012(0.0132) Grad: 6076.1387  LR: 0.000014  \n",
      "Epoch: [1][13200/36908] Elapsed 182m 50s (remain 328m 20s) Loss: 0.0072(0.0132) Grad: 7665.3823  LR: 0.000014  \n",
      "Epoch: [1][13300/36908] Elapsed 184m 12s (remain 326m 56s) Loss: 0.0030(0.0131) Grad: 12211.1924  LR: 0.000014  \n",
      "Epoch: [1][13400/36908] Elapsed 185m 35s (remain 325m 32s) Loss: 0.0094(0.0130) Grad: 4162.1660  LR: 0.000014  \n",
      "Epoch: [1][13500/36908] Elapsed 186m 58s (remain 324m 10s) Loss: 0.0043(0.0129) Grad: 11223.2715  LR: 0.000014  \n",
      "Epoch: [1][13600/36908] Elapsed 188m 22s (remain 322m 48s) Loss: 0.0018(0.0129) Grad: 3071.2119  LR: 0.000014  \n",
      "Epoch: [1][13700/36908] Elapsed 189m 46s (remain 321m 27s) Loss: 0.0026(0.0128) Grad: 1487.1057  LR: 0.000014  \n",
      "Epoch: [1][13800/36908] Elapsed 191m 10s (remain 320m 5s) Loss: 0.0036(0.0127) Grad: 7139.6934  LR: 0.000014  \n",
      "Epoch: [1][13900/36908] Elapsed 192m 33s (remain 318m 41s) Loss: 0.0002(0.0127) Grad: 96.7287  LR: 0.000014  \n",
      "Epoch: [1][14000/36908] Elapsed 193m 56s (remain 317m 18s) Loss: 0.0000(0.0126) Grad: 23.5805  LR: 0.000014  \n",
      "Epoch: [1][14100/36908] Elapsed 195m 21s (remain 315m 57s) Loss: 0.0002(0.0125) Grad: 157.1780  LR: 0.000014  \n",
      "Epoch: [1][14200/36908] Elapsed 196m 44s (remain 314m 34s) Loss: 0.0007(0.0125) Grad: 6053.2866  LR: 0.000014  \n",
      "Epoch: [1][14300/36908] Elapsed 198m 7s (remain 313m 11s) Loss: 0.0038(0.0124) Grad: 2876.8499  LR: 0.000014  \n",
      "Epoch: [1][14400/36908] Elapsed 199m 31s (remain 311m 49s) Loss: 0.0024(0.0123) Grad: 4614.9668  LR: 0.000014  \n",
      "Epoch: [1][14500/36908] Elapsed 200m 53s (remain 310m 25s) Loss: 0.0004(0.0123) Grad: 139.8480  LR: 0.000013  \n",
      "Epoch: [1][14600/36908] Elapsed 202m 15s (remain 309m 0s) Loss: 0.0001(0.0122) Grad: 39.1024  LR: 0.000013  \n",
      "Epoch: [1][14700/36908] Elapsed 203m 39s (remain 307m 38s) Loss: 0.0003(0.0121) Grad: 120.7990  LR: 0.000013  \n",
      "Epoch: [1][14800/36908] Elapsed 205m 2s (remain 306m 14s) Loss: 0.0009(0.0121) Grad: 360.5126  LR: 0.000013  \n",
      "Epoch: [1][14900/36908] Elapsed 206m 24s (remain 304m 49s) Loss: 0.0044(0.0120) Grad: 4688.3359  LR: 0.000013  \n",
      "Epoch: [1][15000/36908] Elapsed 207m 46s (remain 303m 26s) Loss: 0.0025(0.0120) Grad: 8340.1641  LR: 0.000013  \n",
      "Epoch: [1][15100/36908] Elapsed 209m 10s (remain 302m 4s) Loss: 0.0037(0.0119) Grad: 1060.3328  LR: 0.000013  \n",
      "Epoch: [1][15200/36908] Elapsed 210m 34s (remain 300m 41s) Loss: 0.0029(0.0118) Grad: 6459.3970  LR: 0.000013  \n",
      "Epoch: [1][15300/36908] Elapsed 211m 57s (remain 299m 18s) Loss: 0.0007(0.0118) Grad: 353.1256  LR: 0.000013  \n",
      "Epoch: [1][15400/36908] Elapsed 213m 19s (remain 297m 53s) Loss: 0.0004(0.0117) Grad: 390.2330  LR: 0.000013  \n",
      "Epoch: [1][15500/36908] Elapsed 214m 41s (remain 296m 29s) Loss: 0.0016(0.0116) Grad: 3762.2979  LR: 0.000013  \n",
      "Epoch: [1][15600/36908] Elapsed 216m 4s (remain 295m 5s) Loss: 0.0003(0.0116) Grad: 295.2817  LR: 0.000013  \n",
      "Epoch: [1][15700/36908] Elapsed 217m 26s (remain 293m 41s) Loss: 0.0001(0.0115) Grad: 57.6585  LR: 0.000013  \n",
      "Epoch: [1][15800/36908] Elapsed 218m 48s (remain 292m 17s) Loss: 0.0006(0.0115) Grad: 296.9486  LR: 0.000013  \n",
      "Epoch: [1][15900/36908] Elapsed 220m 9s (remain 290m 51s) Loss: 0.0015(0.0114) Grad: 453.4528  LR: 0.000013  \n",
      "Epoch: [1][16000/36908] Elapsed 221m 31s (remain 289m 27s) Loss: 0.0188(0.0114) Grad: 22969.7656  LR: 0.000013  \n",
      "Epoch: [1][16100/36908] Elapsed 222m 55s (remain 288m 4s) Loss: 0.0006(0.0113) Grad: 486.5692  LR: 0.000013  \n",
      "Epoch: [1][16200/36908] Elapsed 224m 18s (remain 286m 41s) Loss: 0.0001(0.0113) Grad: 33.0950  LR: 0.000012  \n",
      "Epoch: [1][16300/36908] Elapsed 225m 41s (remain 285m 18s) Loss: 0.0003(0.0112) Grad: 85.3422  LR: 0.000012  \n",
      "Epoch: [1][16400/36908] Elapsed 227m 4s (remain 283m 55s) Loss: 0.0001(0.0112) Grad: 35.4731  LR: 0.000012  \n",
      "Epoch: [1][16500/36908] Elapsed 228m 28s (remain 282m 33s) Loss: 0.0001(0.0111) Grad: 20.9319  LR: 0.000012  \n",
      "Epoch: [1][16600/36908] Elapsed 229m 50s (remain 281m 9s) Loss: 0.0034(0.0111) Grad: 2814.8455  LR: 0.000012  \n",
      "Epoch: [1][16700/36908] Elapsed 231m 14s (remain 279m 47s) Loss: 0.0010(0.0110) Grad: 2039.0232  LR: 0.000012  \n",
      "Epoch: [1][16800/36908] Elapsed 232m 37s (remain 278m 23s) Loss: 0.0001(0.0110) Grad: 65.0789  LR: 0.000012  \n",
      "Epoch: [1][16900/36908] Elapsed 234m 0s (remain 277m 1s) Loss: 0.0001(0.0109) Grad: 16.9183  LR: 0.000012  \n",
      "Epoch: [1][17000/36908] Elapsed 235m 25s (remain 275m 39s) Loss: 0.0045(0.0109) Grad: 1884.5533  LR: 0.000012  \n",
      "Epoch: [1][17100/36908] Elapsed 236m 49s (remain 274m 17s) Loss: 0.0089(0.0108) Grad: 9739.5869  LR: 0.000012  \n",
      "Epoch: [1][17200/36908] Elapsed 238m 13s (remain 272m 55s) Loss: 0.0026(0.0108) Grad: 2340.0437  LR: 0.000012  \n",
      "Epoch: [1][17300/36908] Elapsed 239m 36s (remain 271m 32s) Loss: 0.0078(0.0107) Grad: 2470.3669  LR: 0.000012  \n",
      "Epoch: [1][17400/36908] Elapsed 240m 59s (remain 270m 9s) Loss: 0.0011(0.0107) Grad: 1308.1665  LR: 0.000012  \n",
      "Epoch: [1][17500/36908] Elapsed 242m 22s (remain 268m 45s) Loss: 0.0015(0.0106) Grad: 1488.7396  LR: 0.000012  \n",
      "Epoch: [1][17600/36908] Elapsed 243m 44s (remain 267m 21s) Loss: 0.0054(0.0106) Grad: 5353.6660  LR: 0.000012  \n",
      "Epoch: [1][17700/36908] Elapsed 245m 7s (remain 265m 58s) Loss: 0.0057(0.0105) Grad: 1174.9200  LR: 0.000012  \n",
      "Epoch: [1][17800/36908] Elapsed 246m 29s (remain 264m 34s) Loss: 0.0080(0.0105) Grad: 7934.3999  LR: 0.000012  \n",
      "Epoch: [1][17900/36908] Elapsed 247m 52s (remain 263m 10s) Loss: 0.0036(0.0105) Grad: 2314.5417  LR: 0.000011  \n",
      "Epoch: [1][18000/36908] Elapsed 249m 15s (remain 261m 48s) Loss: 0.0003(0.0104) Grad: 161.6922  LR: 0.000011  \n",
      "Epoch: [1][18100/36908] Elapsed 250m 38s (remain 260m 25s) Loss: 0.0016(0.0104) Grad: 320.8348  LR: 0.000011  \n",
      "Epoch: [1][18200/36908] Elapsed 252m 1s (remain 259m 1s) Loss: 0.0097(0.0103) Grad: 8127.5190  LR: 0.000011  \n",
      "Epoch: [1][18300/36908] Elapsed 253m 23s (remain 257m 37s) Loss: 0.0004(0.0103) Grad: 284.8004  LR: 0.000011  \n",
      "Epoch: [1][18400/36908] Elapsed 254m 46s (remain 256m 14s) Loss: 0.0106(0.0102) Grad: 3280.0764  LR: 0.000011  \n",
      "Epoch: [1][18500/36908] Elapsed 256m 9s (remain 254m 51s) Loss: 0.0001(0.0102) Grad: 131.4366  LR: 0.000011  \n",
      "Epoch: [1][18600/36908] Elapsed 257m 33s (remain 253m 29s) Loss: 0.0132(0.0102) Grad: 9089.8252  LR: 0.000011  \n",
      "Epoch: [1][18700/36908] Elapsed 258m 56s (remain 252m 6s) Loss: 0.0002(0.0101) Grad: 101.3810  LR: 0.000011  \n",
      "Epoch: [1][18800/36908] Elapsed 260m 20s (remain 250m 44s) Loss: 0.0025(0.0101) Grad: 2629.3914  LR: 0.000011  \n",
      "Epoch: [1][18900/36908] Elapsed 261m 43s (remain 249m 20s) Loss: 0.0006(0.0101) Grad: 503.2357  LR: 0.000011  \n",
      "Epoch: [1][19000/36908] Elapsed 263m 6s (remain 247m 57s) Loss: 0.0007(0.0100) Grad: 86.7253  LR: 0.000011  \n",
      "Epoch: [1][19100/36908] Elapsed 264m 29s (remain 246m 34s) Loss: 0.0064(0.0100) Grad: 1615.5140  LR: 0.000011  \n",
      "Epoch: [1][19200/36908] Elapsed 265m 51s (remain 245m 10s) Loss: 0.0006(0.0099) Grad: 89.6726  LR: 0.000011  \n",
      "Epoch: [1][19300/36908] Elapsed 267m 14s (remain 243m 47s) Loss: 0.0120(0.0099) Grad: 9966.7969  LR: 0.000011  \n",
      "Epoch: [1][19400/36908] Elapsed 268m 38s (remain 242m 24s) Loss: 0.0000(0.0099) Grad: 56.2237  LR: 0.000011  \n",
      "Epoch: [1][19500/36908] Elapsed 270m 0s (remain 241m 1s) Loss: 0.0019(0.0098) Grad: 2051.7800  LR: 0.000010  \n",
      "Epoch: [1][19600/36908] Elapsed 271m 23s (remain 239m 37s) Loss: 0.0072(0.0098) Grad: 12159.2451  LR: 0.000010  \n",
      "Epoch: [1][19700/36908] Elapsed 272m 47s (remain 238m 15s) Loss: 0.0020(0.0098) Grad: 6216.4697  LR: 0.000010  \n",
      "Epoch: [1][19800/36908] Elapsed 274m 11s (remain 236m 52s) Loss: 0.0001(0.0097) Grad: 11.1949  LR: 0.000010  \n",
      "Epoch: [1][19900/36908] Elapsed 275m 33s (remain 235m 29s) Loss: 0.0104(0.0097) Grad: 14945.5908  LR: 0.000010  \n",
      "Epoch: [1][20000/36908] Elapsed 276m 57s (remain 234m 6s) Loss: 0.0232(0.0096) Grad: 35068.7617  LR: 0.000010  \n",
      "Epoch: [1][20100/36908] Elapsed 278m 19s (remain 232m 42s) Loss: 0.0037(0.0096) Grad: 5667.2969  LR: 0.000010  \n",
      "Epoch: [1][20200/36908] Elapsed 279m 41s (remain 231m 19s) Loss: 0.0012(0.0096) Grad: 1256.6536  LR: 0.000010  \n",
      "Epoch: [1][20300/36908] Elapsed 281m 3s (remain 229m 55s) Loss: 0.0048(0.0095) Grad: 5919.9097  LR: 0.000010  \n",
      "Epoch: [1][20400/36908] Elapsed 282m 26s (remain 228m 31s) Loss: 0.0000(0.0095) Grad: 5.6500  LR: 0.000010  \n",
      "Epoch: [1][20500/36908] Elapsed 283m 50s (remain 227m 9s) Loss: 0.0021(0.0095) Grad: 1828.0676  LR: 0.000010  \n",
      "Epoch: [1][20600/36908] Elapsed 285m 13s (remain 225m 46s) Loss: 0.0007(0.0094) Grad: 2758.0986  LR: 0.000010  \n",
      "Epoch: [1][20700/36908] Elapsed 286m 35s (remain 224m 22s) Loss: 0.0034(0.0094) Grad: 4773.5117  LR: 0.000010  \n",
      "Epoch: [1][20800/36908] Elapsed 287m 57s (remain 222m 58s) Loss: 0.0023(0.0094) Grad: 8475.5312  LR: 0.000010  \n",
      "Epoch: [1][20900/36908] Elapsed 289m 20s (remain 221m 35s) Loss: 0.0003(0.0093) Grad: 296.0933  LR: 0.000010  \n",
      "Epoch: [1][21000/36908] Elapsed 290m 44s (remain 220m 12s) Loss: 0.0042(0.0093) Grad: 5817.0815  LR: 0.000010  \n",
      "Epoch: [1][21100/36908] Elapsed 292m 7s (remain 218m 50s) Loss: 0.0013(0.0093) Grad: 803.0259  LR: 0.000010  \n",
      "Epoch: [1][21200/36908] Elapsed 293m 30s (remain 217m 26s) Loss: 0.0053(0.0092) Grad: 4752.7397  LR: 0.000009  \n",
      "Epoch: [1][21300/36908] Elapsed 294m 52s (remain 216m 3s) Loss: 0.0001(0.0092) Grad: 437.5026  LR: 0.000009  \n",
      "Epoch: [1][21400/36908] Elapsed 296m 15s (remain 214m 39s) Loss: 0.0000(0.0092) Grad: 61.6761  LR: 0.000009  \n",
      "Epoch: [1][21500/36908] Elapsed 297m 39s (remain 213m 17s) Loss: 0.0001(0.0091) Grad: 20.9993  LR: 0.000009  \n",
      "Epoch: [1][21600/36908] Elapsed 299m 2s (remain 211m 54s) Loss: 0.0041(0.0091) Grad: 6076.5688  LR: 0.000009  \n",
      "Epoch: [1][21700/36908] Elapsed 300m 24s (remain 210m 30s) Loss: 0.0004(0.0091) Grad: 2354.3718  LR: 0.000009  \n",
      "Epoch: [1][21800/36908] Elapsed 301m 47s (remain 209m 7s) Loss: 0.0000(0.0091) Grad: 7.9838  LR: 0.000009  \n",
      "Epoch: [1][21900/36908] Elapsed 303m 11s (remain 207m 45s) Loss: 0.0060(0.0090) Grad: 3242.6934  LR: 0.000009  \n",
      "Epoch: [1][22000/36908] Elapsed 304m 35s (remain 206m 22s) Loss: 0.0048(0.0090) Grad: 2332.1655  LR: 0.000009  \n",
      "Epoch: [1][22100/36908] Elapsed 305m 58s (remain 204m 59s) Loss: 0.0000(0.0090) Grad: 13.4572  LR: 0.000009  \n",
      "Epoch: [1][22200/36908] Elapsed 307m 23s (remain 203m 37s) Loss: 0.0001(0.0090) Grad: 26.4151  LR: 0.000009  \n",
      "Epoch: [1][22300/36908] Elapsed 308m 46s (remain 202m 14s) Loss: 0.0016(0.0089) Grad: 8530.2314  LR: 0.000009  \n",
      "Epoch: [1][22400/36908] Elapsed 310m 9s (remain 200m 51s) Loss: 0.0005(0.0089) Grad: 998.6113  LR: 0.000009  \n",
      "Epoch: [1][22500/36908] Elapsed 311m 34s (remain 199m 29s) Loss: 0.0003(0.0089) Grad: 1535.3529  LR: 0.000009  \n",
      "Epoch: [1][22600/36908] Elapsed 312m 57s (remain 198m 6s) Loss: 0.0011(0.0088) Grad: 2143.8713  LR: 0.000009  \n",
      "Epoch: [1][22700/36908] Elapsed 314m 20s (remain 196m 43s) Loss: 0.0024(0.0088) Grad: 1368.1931  LR: 0.000009  \n",
      "Epoch: [1][22800/36908] Elapsed 315m 44s (remain 195m 20s) Loss: 0.0022(0.0088) Grad: 1347.1654  LR: 0.000008  \n",
      "Epoch: [1][22900/36908] Elapsed 317m 7s (remain 193m 57s) Loss: 0.0000(0.0088) Grad: 85.7406  LR: 0.000008  \n",
      "Epoch: [1][23000/36908] Elapsed 318m 31s (remain 192m 35s) Loss: 0.0008(0.0087) Grad: 1328.8707  LR: 0.000008  \n",
      "Epoch: [1][23100/36908] Elapsed 319m 55s (remain 191m 12s) Loss: 0.0050(0.0087) Grad: 6347.6797  LR: 0.000008  \n",
      "Epoch: [1][23200/36908] Elapsed 321m 19s (remain 189m 50s) Loss: 0.0027(0.0087) Grad: 488.0168  LR: 0.000008  \n",
      "Epoch: [1][23300/36908] Elapsed 322m 42s (remain 188m 27s) Loss: 0.0025(0.0086) Grad: 3508.5720  LR: 0.000008  \n",
      "Epoch: [1][23400/36908] Elapsed 324m 5s (remain 187m 4s) Loss: 0.0024(0.0086) Grad: 9182.1562  LR: 0.000008  \n",
      "Epoch: [1][23500/36908] Elapsed 325m 30s (remain 185m 41s) Loss: 0.0076(0.0086) Grad: 18594.6484  LR: 0.000008  \n",
      "Epoch: [1][23600/36908] Elapsed 326m 54s (remain 184m 19s) Loss: 0.0049(0.0086) Grad: 46225.9805  LR: 0.000008  \n",
      "Epoch: [1][23700/36908] Elapsed 328m 17s (remain 182m 56s) Loss: 0.0033(0.0085) Grad: 11032.8994  LR: 0.000008  \n",
      "Epoch: [1][23800/36908] Elapsed 329m 40s (remain 181m 32s) Loss: 0.0002(0.0085) Grad: 123.4201  LR: 0.000008  \n",
      "Epoch: [1][23900/36908] Elapsed 331m 2s (remain 180m 9s) Loss: 0.0000(0.0085) Grad: 318.1504  LR: 0.000008  \n",
      "Epoch: [1][24000/36908] Elapsed 332m 27s (remain 178m 46s) Loss: 0.0002(0.0085) Grad: 76.1104  LR: 0.000008  \n",
      "Epoch: [1][24100/36908] Elapsed 333m 50s (remain 177m 23s) Loss: 0.0000(0.0084) Grad: 8.8572  LR: 0.000008  \n",
      "Epoch: [1][24200/36908] Elapsed 335m 12s (remain 176m 0s) Loss: 0.0003(0.0084) Grad: 295.2547  LR: 0.000008  \n",
      "Epoch: [1][24300/36908] Elapsed 336m 37s (remain 174m 37s) Loss: 0.0007(0.0084) Grad: 1085.2112  LR: 0.000008  \n",
      "Epoch: [1][24400/36908] Elapsed 338m 1s (remain 173m 15s) Loss: 0.0015(0.0084) Grad: 1158.3997  LR: 0.000008  \n",
      "Epoch: [1][24500/36908] Elapsed 339m 23s (remain 171m 52s) Loss: 0.0041(0.0083) Grad: 13503.1660  LR: 0.000007  \n",
      "Epoch: [1][24600/36908] Elapsed 340m 46s (remain 170m 28s) Loss: 0.0054(0.0083) Grad: 10135.4766  LR: 0.000007  \n",
      "Epoch: [1][24700/36908] Elapsed 342m 10s (remain 169m 6s) Loss: 0.0012(0.0083) Grad: 6421.3633  LR: 0.000007  \n",
      "Epoch: [1][24800/36908] Elapsed 343m 32s (remain 167m 42s) Loss: 0.0012(0.0083) Grad: 690.9598  LR: 0.000007  \n",
      "Epoch: [1][24900/36908] Elapsed 344m 55s (remain 166m 19s) Loss: 0.0005(0.0082) Grad: 520.7712  LR: 0.000007  \n",
      "Epoch: [1][25000/36908] Elapsed 346m 17s (remain 164m 55s) Loss: 0.0004(0.0082) Grad: 426.1979  LR: 0.000007  \n",
      "Epoch: [1][25100/36908] Elapsed 347m 42s (remain 163m 33s) Loss: 0.0007(0.0082) Grad: 171.9150  LR: 0.000007  \n",
      "Epoch: [1][25200/36908] Elapsed 349m 6s (remain 162m 10s) Loss: 0.0041(0.0082) Grad: 5604.1836  LR: 0.000007  \n",
      "Epoch: [1][25300/36908] Elapsed 350m 30s (remain 160m 47s) Loss: 0.0001(0.0081) Grad: 19.8656  LR: 0.000007  \n",
      "Epoch: [1][25400/36908] Elapsed 351m 53s (remain 159m 24s) Loss: 0.0051(0.0081) Grad: 4895.4038  LR: 0.000007  \n",
      "Epoch: [1][25500/36908] Elapsed 353m 18s (remain 158m 2s) Loss: 0.0096(0.0081) Grad: 31600.3535  LR: 0.000007  \n",
      "Epoch: [1][25600/36908] Elapsed 354m 42s (remain 156m 39s) Loss: 0.0006(0.0081) Grad: 265.9623  LR: 0.000007  \n",
      "Epoch: [1][25700/36908] Elapsed 356m 5s (remain 155m 16s) Loss: 0.0000(0.0081) Grad: 250.2723  LR: 0.000007  \n",
      "Epoch: [1][25800/36908] Elapsed 357m 28s (remain 153m 53s) Loss: 0.0007(0.0080) Grad: 2555.0505  LR: 0.000007  \n",
      "Epoch: [1][25900/36908] Elapsed 358m 53s (remain 152m 30s) Loss: 0.0001(0.0080) Grad: 10.3934  LR: 0.000007  \n",
      "Epoch: [1][26000/36908] Elapsed 360m 17s (remain 151m 8s) Loss: 0.0002(0.0080) Grad: 323.8732  LR: 0.000007  \n",
      "Epoch: [1][26100/36908] Elapsed 361m 40s (remain 149m 45s) Loss: 0.0021(0.0080) Grad: 4696.0024  LR: 0.000007  \n",
      "Epoch: [1][26200/36908] Elapsed 363m 4s (remain 148m 22s) Loss: 0.0001(0.0080) Grad: 196.2614  LR: 0.000006  \n",
      "Epoch: [1][26300/36908] Elapsed 364m 27s (remain 146m 59s) Loss: 0.0001(0.0079) Grad: 41.5085  LR: 0.000006  \n",
      "Epoch: [1][26400/36908] Elapsed 365m 51s (remain 145m 36s) Loss: 0.0001(0.0079) Grad: 747.6443  LR: 0.000006  \n",
      "Epoch: [1][26500/36908] Elapsed 367m 15s (remain 144m 13s) Loss: 0.0025(0.0079) Grad: 7917.3193  LR: 0.000006  \n",
      "Epoch: [1][26600/36908] Elapsed 368m 39s (remain 142m 50s) Loss: 0.0009(0.0079) Grad: 697.4981  LR: 0.000006  \n",
      "Epoch: [1][26700/36908] Elapsed 370m 1s (remain 141m 26s) Loss: 0.0025(0.0079) Grad: 17723.0273  LR: 0.000006  \n",
      "Epoch: [1][26800/36908] Elapsed 371m 25s (remain 140m 4s) Loss: 0.0001(0.0078) Grad: 114.8487  LR: 0.000006  \n",
      "Epoch: [1][26900/36908] Elapsed 372m 49s (remain 138m 41s) Loss: 0.0024(0.0078) Grad: 5954.2866  LR: 0.000006  \n",
      "Epoch: [1][27000/36908] Elapsed 374m 13s (remain 137m 18s) Loss: 0.0008(0.0078) Grad: 190.3563  LR: 0.000006  \n",
      "Epoch: [1][27100/36908] Elapsed 375m 36s (remain 135m 55s) Loss: 0.0008(0.0078) Grad: 900.6870  LR: 0.000006  \n",
      "Epoch: [1][27200/36908] Elapsed 377m 0s (remain 134m 32s) Loss: 0.0002(0.0078) Grad: 95.9900  LR: 0.000006  \n",
      "Epoch: [1][27300/36908] Elapsed 378m 24s (remain 133m 9s) Loss: 0.0001(0.0077) Grad: 110.4958  LR: 0.000006  \n",
      "Epoch: [1][27400/36908] Elapsed 379m 47s (remain 131m 46s) Loss: 0.0034(0.0077) Grad: 4420.5342  LR: 0.000006  \n",
      "Epoch: [1][27500/36908] Elapsed 381m 10s (remain 130m 22s) Loss: 0.0035(0.0077) Grad: 5535.6211  LR: 0.000006  \n",
      "Epoch: [1][27600/36908] Elapsed 382m 32s (remain 128m 59s) Loss: 0.0016(0.0077) Grad: 5965.7100  LR: 0.000006  \n",
      "Epoch: [1][27700/36908] Elapsed 383m 55s (remain 127m 36s) Loss: 0.0011(0.0077) Grad: 14716.3643  LR: 0.000006  \n",
      "Epoch: [1][27800/36908] Elapsed 385m 20s (remain 126m 13s) Loss: 0.0069(0.0077) Grad: 7894.3115  LR: 0.000005  \n",
      "Epoch: [1][27900/36908] Elapsed 386m 43s (remain 124m 50s) Loss: 0.0021(0.0076) Grad: 13343.3730  LR: 0.000005  \n",
      "Epoch: [1][28000/36908] Elapsed 388m 8s (remain 123m 27s) Loss: 0.0001(0.0076) Grad: 77.5155  LR: 0.000005  \n",
      "Epoch: [1][28100/36908] Elapsed 389m 32s (remain 122m 4s) Loss: 0.0018(0.0076) Grad: 6026.3281  LR: 0.000005  \n",
      "Epoch: [1][28200/36908] Elapsed 390m 55s (remain 120m 41s) Loss: 0.0002(0.0076) Grad: 73.6270  LR: 0.000005  \n",
      "Epoch: [1][28300/36908] Elapsed 392m 19s (remain 119m 18s) Loss: 0.0093(0.0076) Grad: 26883.0293  LR: 0.000005  \n",
      "Epoch: [1][28400/36908] Elapsed 393m 42s (remain 117m 55s) Loss: 0.0002(0.0075) Grad: 2930.2146  LR: 0.000005  \n",
      "Epoch: [1][28500/36908] Elapsed 395m 5s (remain 116m 32s) Loss: 0.0013(0.0075) Grad: 155.2301  LR: 0.000005  \n",
      "Epoch: [1][28600/36908] Elapsed 396m 28s (remain 115m 9s) Loss: 0.0007(0.0075) Grad: 475.3693  LR: 0.000005  \n",
      "Epoch: [1][28700/36908] Elapsed 397m 52s (remain 113m 46s) Loss: 0.0001(0.0075) Grad: 419.9826  LR: 0.000005  \n",
      "Epoch: [1][28800/36908] Elapsed 399m 16s (remain 112m 23s) Loss: 0.0016(0.0075) Grad: 6774.0645  LR: 0.000005  \n",
      "Epoch: [1][28900/36908] Elapsed 400m 39s (remain 111m 0s) Loss: 0.0021(0.0075) Grad: 10701.8301  LR: 0.000005  \n",
      "Epoch: [1][29000/36908] Elapsed 402m 3s (remain 109m 37s) Loss: 0.0038(0.0074) Grad: 9264.1406  LR: 0.000005  \n",
      "Epoch: [1][29100/36908] Elapsed 403m 27s (remain 108m 14s) Loss: 0.0003(0.0074) Grad: 5878.8896  LR: 0.000005  \n",
      "Epoch: [1][29200/36908] Elapsed 404m 50s (remain 106m 50s) Loss: 0.0025(0.0074) Grad: 16356.3115  LR: 0.000005  \n",
      "Epoch: [1][29300/36908] Elapsed 406m 14s (remain 105m 27s) Loss: 0.0032(0.0074) Grad: 10718.7295  LR: 0.000005  \n",
      "Epoch: [1][29400/36908] Elapsed 407m 37s (remain 104m 4s) Loss: 0.0065(0.0074) Grad: 27390.8828  LR: 0.000005  \n",
      "Epoch: [1][29500/36908] Elapsed 408m 59s (remain 102m 41s) Loss: 0.0001(0.0074) Grad: 113.7438  LR: 0.000004  \n",
      "Epoch: [1][29600/36908] Elapsed 410m 23s (remain 101m 18s) Loss: 0.0110(0.0073) Grad: 46385.4141  LR: 0.000004  \n",
      "Epoch: [1][29700/36908] Elapsed 411m 46s (remain 99m 54s) Loss: 0.0002(0.0073) Grad: 220.7399  LR: 0.000004  \n",
      "Epoch: [1][29800/36908] Elapsed 413m 8s (remain 98m 31s) Loss: 0.0006(0.0073) Grad: 2615.6038  LR: 0.000004  \n",
      "Epoch: [1][29900/36908] Elapsed 414m 32s (remain 97m 8s) Loss: 0.0002(0.0073) Grad: 2400.3931  LR: 0.000004  \n",
      "Epoch: [1][30000/36908] Elapsed 415m 57s (remain 95m 45s) Loss: 0.0000(0.0073) Grad: 21.3232  LR: 0.000004  \n",
      "Epoch: [1][30100/36908] Elapsed 417m 20s (remain 94m 22s) Loss: 0.0012(0.0073) Grad: 1883.9929  LR: 0.000004  \n",
      "Epoch: [1][30200/36908] Elapsed 418m 43s (remain 92m 59s) Loss: 0.0002(0.0072) Grad: 108.0861  LR: 0.000004  \n",
      "Epoch: [1][30300/36908] Elapsed 420m 6s (remain 91m 36s) Loss: 0.0049(0.0072) Grad: 24061.1777  LR: 0.000004  \n",
      "Epoch: [1][30400/36908] Elapsed 421m 31s (remain 90m 13s) Loss: 0.0002(0.0072) Grad: 263.6543  LR: 0.000004  \n",
      "Epoch: [1][30500/36908] Elapsed 422m 54s (remain 88m 50s) Loss: 0.0015(0.0072) Grad: 14910.0352  LR: 0.000004  \n",
      "Epoch: [1][30600/36908] Elapsed 424m 17s (remain 87m 26s) Loss: 0.0051(0.0072) Grad: 21929.9766  LR: 0.000004  \n",
      "Epoch: [1][30700/36908] Elapsed 425m 40s (remain 86m 3s) Loss: 0.0048(0.0072) Grad: 31281.8105  LR: 0.000004  \n",
      "Epoch: [1][30800/36908] Elapsed 427m 4s (remain 84m 40s) Loss: 0.0021(0.0072) Grad: 2008.1532  LR: 0.000004  \n",
      "Epoch: [1][30900/36908] Elapsed 428m 27s (remain 83m 17s) Loss: 0.0014(0.0071) Grad: 6738.5820  LR: 0.000004  \n",
      "Epoch: [1][31000/36908] Elapsed 429m 50s (remain 81m 54s) Loss: 0.0081(0.0071) Grad: 55642.3594  LR: 0.000004  \n",
      "Epoch: [1][31100/36908] Elapsed 431m 13s (remain 80m 30s) Loss: 0.0002(0.0071) Grad: 44.7467  LR: 0.000003  \n",
      "Epoch: [1][31200/36908] Elapsed 432m 37s (remain 79m 7s) Loss: 0.0029(0.0071) Grad: 6648.3711  LR: 0.000003  \n",
      "Epoch: [1][31300/36908] Elapsed 434m 0s (remain 77m 44s) Loss: 0.0001(0.0071) Grad: 548.4649  LR: 0.000003  \n",
      "Epoch: [1][31400/36908] Elapsed 435m 23s (remain 76m 21s) Loss: 0.0062(0.0071) Grad: 179266.4375  LR: 0.000003  \n",
      "Epoch: [1][31500/36908] Elapsed 436m 45s (remain 74m 58s) Loss: 0.0006(0.0070) Grad: 997.4884  LR: 0.000003  \n",
      "Epoch: [1][31600/36908] Elapsed 438m 9s (remain 73m 34s) Loss: 0.0019(0.0070) Grad: 3065.0984  LR: 0.000003  \n",
      "Epoch: [1][31700/36908] Elapsed 439m 32s (remain 72m 11s) Loss: 0.0002(0.0070) Grad: 128.1803  LR: 0.000003  \n",
      "Epoch: [1][31800/36908] Elapsed 440m 56s (remain 70m 48s) Loss: 0.0006(0.0070) Grad: 519.9471  LR: 0.000003  \n",
      "Epoch: [1][31900/36908] Elapsed 442m 20s (remain 69m 25s) Loss: 0.0001(0.0070) Grad: 41.8666  LR: 0.000003  \n",
      "Epoch: [1][32000/36908] Elapsed 443m 43s (remain 68m 2s) Loss: 0.0023(0.0070) Grad: 28529.3223  LR: 0.000003  \n",
      "Epoch: [1][32100/36908] Elapsed 445m 7s (remain 66m 39s) Loss: 0.0001(0.0070) Grad: 70.9074  LR: 0.000003  \n",
      "Epoch: [1][32200/36908] Elapsed 446m 30s (remain 65m 16s) Loss: 0.0001(0.0069) Grad: 2091.7092  LR: 0.000003  \n",
      "Epoch: [1][32300/36908] Elapsed 447m 55s (remain 63m 53s) Loss: 0.0015(0.0069) Grad: 33480.8008  LR: 0.000003  \n",
      "Epoch: [1][32400/36908] Elapsed 449m 18s (remain 62m 29s) Loss: 0.0086(0.0069) Grad: 201134.4688  LR: 0.000003  \n",
      "Epoch: [1][32500/36908] Elapsed 450m 40s (remain 61m 6s) Loss: 0.0005(0.0069) Grad: 16764.6973  LR: 0.000003  \n",
      "Epoch: [1][32600/36908] Elapsed 452m 4s (remain 59m 43s) Loss: 0.0033(0.0069) Grad: 47080.9961  LR: 0.000003  \n",
      "Epoch: [1][32700/36908] Elapsed 453m 27s (remain 58m 20s) Loss: 0.0027(0.0069) Grad: 67900.9453  LR: 0.000003  \n",
      "Epoch: [1][32800/36908] Elapsed 454m 51s (remain 56m 57s) Loss: 0.0019(0.0069) Grad: 3829.8711  LR: 0.000002  \n",
      "Epoch: [1][32900/36908] Elapsed 456m 15s (remain 55m 34s) Loss: 0.0002(0.0068) Grad: 427.7039  LR: 0.000002  \n",
      "Epoch: [1][33000/36908] Elapsed 457m 38s (remain 54m 10s) Loss: 0.0081(0.0068) Grad: 217491.2344  LR: 0.000002  \n",
      "Epoch: [1][33100/36908] Elapsed 459m 1s (remain 52m 47s) Loss: 0.0054(0.0068) Grad: 143506.0781  LR: 0.000002  \n",
      "Epoch: [1][33200/36908] Elapsed 460m 23s (remain 51m 24s) Loss: 0.0001(0.0068) Grad: 224.4050  LR: 0.000002  \n",
      "Epoch: [1][33300/36908] Elapsed 461m 48s (remain 50m 1s) Loss: 0.0000(0.0068) Grad: 321.2480  LR: 0.000002  \n",
      "Epoch: [1][33400/36908] Elapsed 463m 12s (remain 48m 38s) Loss: 0.0041(0.0068) Grad: 117888.3828  LR: 0.000002  \n",
      "Epoch: [1][33500/36908] Elapsed 464m 35s (remain 47m 14s) Loss: 0.0000(0.0068) Grad: 572.9258  LR: 0.000002  \n",
      "Epoch: [1][33600/36908] Elapsed 465m 59s (remain 45m 51s) Loss: 0.0080(0.0068) Grad: 193138.1875  LR: 0.000002  \n",
      "Epoch: [1][33700/36908] Elapsed 467m 23s (remain 44m 28s) Loss: 0.0009(0.0067) Grad: 36218.9219  LR: 0.000002  \n",
      "Epoch: [1][33800/36908] Elapsed 468m 47s (remain 43m 5s) Loss: 0.0080(0.0067) Grad: 87858.8359  LR: 0.000002  \n",
      "Epoch: [1][33900/36908] Elapsed 470m 10s (remain 41m 42s) Loss: 0.0004(0.0067) Grad: 2165.6641  LR: 0.000002  \n",
      "Epoch: [1][34000/36908] Elapsed 471m 33s (remain 40m 19s) Loss: 0.0025(0.0067) Grad: 38328.5273  LR: 0.000002  \n",
      "Epoch: [1][34100/36908] Elapsed 472m 57s (remain 38m 55s) Loss: 0.0040(0.0067) Grad: 63850.3359  LR: 0.000002  \n",
      "Epoch: [1][34200/36908] Elapsed 474m 21s (remain 37m 32s) Loss: 0.0014(0.0067) Grad: 6850.4419  LR: 0.000002  \n",
      "Epoch: [1][34300/36908] Elapsed 475m 44s (remain 36m 9s) Loss: 0.0005(0.0067) Grad: 4693.7715  LR: 0.000002  \n",
      "Epoch: [1][34400/36908] Elapsed 477m 8s (remain 34m 46s) Loss: 0.0002(0.0067) Grad: 1338.9265  LR: 0.000002  \n",
      "Epoch: [1][34500/36908] Elapsed 478m 31s (remain 33m 23s) Loss: 0.0112(0.0066) Grad: 169850.2812  LR: 0.000001  \n",
      "Epoch: [1][34600/36908] Elapsed 479m 54s (remain 31m 59s) Loss: 0.0048(0.0066) Grad: 34010.5664  LR: 0.000001  \n",
      "Epoch: [1][34700/36908] Elapsed 481m 19s (remain 30m 36s) Loss: 0.0083(0.0066) Grad: 34714.4336  LR: 0.000001  \n",
      "Epoch: [1][34800/36908] Elapsed 482m 42s (remain 29m 13s) Loss: 0.0004(0.0066) Grad: 885.1570  LR: 0.000001  \n",
      "Epoch: [1][34900/36908] Elapsed 484m 6s (remain 27m 50s) Loss: 0.0005(0.0066) Grad: 1585.9658  LR: 0.000001  \n",
      "Epoch: [1][35000/36908] Elapsed 485m 30s (remain 26m 27s) Loss: 0.0007(0.0066) Grad: 2851.9282  LR: 0.000001  \n",
      "Epoch: [1][35100/36908] Elapsed 486m 54s (remain 25m 3s) Loss: 0.0001(0.0066) Grad: 218.7582  LR: 0.000001  \n",
      "Epoch: [1][35200/36908] Elapsed 488m 18s (remain 23m 40s) Loss: 0.0009(0.0066) Grad: 3649.1548  LR: 0.000001  \n",
      "Epoch: [1][35300/36908] Elapsed 489m 41s (remain 22m 17s) Loss: 0.0006(0.0065) Grad: 28247.7363  LR: 0.000001  \n",
      "Epoch: [1][35400/36908] Elapsed 491m 5s (remain 20m 54s) Loss: 0.0001(0.0065) Grad: 85.2871  LR: 0.000001  \n",
      "Epoch: [1][35500/36908] Elapsed 492m 29s (remain 19m 31s) Loss: 0.0054(0.0065) Grad: 130016.7969  LR: 0.000001  \n",
      "Epoch: [1][35600/36908] Elapsed 493m 52s (remain 18m 7s) Loss: 0.0001(0.0065) Grad: 4659.4214  LR: 0.000001  \n",
      "Epoch: [1][35700/36908] Elapsed 495m 16s (remain 16m 44s) Loss: 0.0132(0.0065) Grad: 69074.7812  LR: 0.000001  \n",
      "Epoch: [1][35800/36908] Elapsed 496m 39s (remain 15m 21s) Loss: 0.0004(0.0065) Grad: 447.3079  LR: 0.000001  \n",
      "Epoch: [1][35900/36908] Elapsed 498m 1s (remain 13m 58s) Loss: 0.0014(0.0065) Grad: 16892.2031  LR: 0.000001  \n",
      "Epoch: [1][36000/36908] Elapsed 499m 25s (remain 12m 34s) Loss: 0.0000(0.0065) Grad: 25.0383  LR: 0.000001  \n",
      "Epoch: [1][36100/36908] Elapsed 500m 49s (remain 11m 11s) Loss: 0.0033(0.0065) Grad: 26999.0527  LR: 0.000000  \n",
      "Epoch: [1][36200/36908] Elapsed 502m 11s (remain 9m 48s) Loss: 0.0026(0.0065) Grad: 37331.0977  LR: 0.000000  \n",
      "Epoch: [1][36300/36908] Elapsed 503m 34s (remain 8m 25s) Loss: 0.0008(0.0064) Grad: 14865.0254  LR: 0.000000  \n",
      "Epoch: [1][36400/36908] Elapsed 504m 58s (remain 7m 2s) Loss: 0.0106(0.0064) Grad: 99408.4844  LR: 0.000000  \n",
      "Epoch: [1][36500/36908] Elapsed 506m 21s (remain 5m 38s) Loss: 0.0034(0.0064) Grad: 6488.0278  LR: 0.000000  \n",
      "Epoch: [1][36600/36908] Elapsed 507m 43s (remain 4m 15s) Loss: 0.0006(0.0064) Grad: 1316.7953  LR: 0.000000  \n",
      "Epoch: [1][36700/36908] Elapsed 509m 7s (remain 2m 52s) Loss: 0.0000(0.0064) Grad: 25.4113  LR: 0.000000  \n",
      "Epoch: [1][36800/36908] Elapsed 510m 30s (remain 1m 29s) Loss: 0.0008(0.0064) Grad: 1374.7275  LR: 0.000000  \n",
      "Epoch: [1][36900/36908] Elapsed 511m 52s (remain 0m 5s) Loss: 0.0001(0.0064) Grad: 145.7059  LR: 0.000000  \n",
      "Epoch: [1][36907/36908] Elapsed 511m 58s (remain 0m 0s) Loss: 0.0010(0.0064) Grad: 6962.0386  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 11m 51s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 29s (remain 5m 18s) Loss: 0.0359(0.0078) \n",
      "EVAL: [200/1192] Elapsed 0m 58s (remain 4m 50s) Loss: 0.0050(0.0070) \n",
      "EVAL: [300/1192] Elapsed 1m 27s (remain 4m 19s) Loss: 0.0045(0.0069) \n",
      "EVAL: [400/1192] Elapsed 1m 56s (remain 3m 50s) Loss: 0.0000(0.0074) \n",
      "EVAL: [500/1192] Elapsed 2m 27s (remain 3m 22s) Loss: 0.0001(0.0069) \n",
      "EVAL: [600/1192] Elapsed 2m 56s (remain 2m 53s) Loss: 0.0031(0.0070) \n",
      "EVAL: [700/1192] Elapsed 3m 24s (remain 2m 23s) Loss: 0.0056(0.0077) \n",
      "EVAL: [800/1192] Elapsed 3m 53s (remain 1m 54s) Loss: 0.0000(0.0078) \n",
      "EVAL: [900/1192] Elapsed 4m 23s (remain 1m 25s) Loss: 0.0138(0.0079) \n",
      "EVAL: [1000/1192] Elapsed 4m 52s (remain 0m 55s) Loss: 0.0123(0.0078) \n",
      "EVAL: [1100/1192] Elapsed 5m 21s (remain 0m 26s) Loss: 0.0382(0.0075) \n",
      "EVAL: [1191/1192] Elapsed 5m 48s (remain 0m 0s) Loss: 0.0000(0.0072) \n",
      "Epoch 1 - avg_train_loss: 0.0064  avg_val_loss: 0.0072  time: 31070s\n",
      "Epoch 1 - Score: 0.8930\n",
      "Epoch 1 - Save Best Score: 0.8930 Model\n",
      "========== fold: 3 training ==========\n",
      "get pseudo plain from ../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_3.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_3.npy\n",
      "(612602, 6) (612602, 950)\n",
      "(100000, 7)\n",
      "(110725, 11)\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/36908] Elapsed 0m 1s (remain 705m 17s) Loss: 0.3318(0.3318) Grad: 104023.7031  LR: 0.000000  \n",
      "Epoch: [1][100/36908] Elapsed 1m 25s (remain 517m 35s) Loss: 0.3116(0.3203) Grad: 93897.4844  LR: 0.000001  \n",
      "Epoch: [1][200/36908] Elapsed 2m 48s (remain 512m 40s) Loss: 0.2400(0.3002) Grad: 40553.1484  LR: 0.000001  \n",
      "Epoch: [1][300/36908] Elapsed 4m 12s (remain 511m 27s) Loss: 0.1443(0.2644) Grad: 31916.8320  LR: 0.000002  \n",
      "Epoch: [1][400/36908] Elapsed 5m 36s (remain 511m 2s) Loss: 0.0714(0.2216) Grad: 9797.6074  LR: 0.000002  \n",
      "Epoch: [1][500/36908] Elapsed 7m 1s (remain 510m 17s) Loss: 0.0240(0.1862) Grad: 1936.0504  LR: 0.000003  \n",
      "Epoch: [1][600/36908] Elapsed 8m 26s (remain 509m 40s) Loss: 0.0297(0.1616) Grad: 2185.6802  LR: 0.000003  \n",
      "Epoch: [1][700/36908] Elapsed 9m 49s (remain 507m 11s) Loss: 0.0226(0.1441) Grad: 1789.2241  LR: 0.000004  \n",
      "Epoch: [1][800/36908] Elapsed 11m 12s (remain 505m 1s) Loss: 0.0261(0.1303) Grad: 2259.2305  LR: 0.000004  \n",
      "Epoch: [1][900/36908] Elapsed 12m 35s (remain 503m 23s) Loss: 0.0162(0.1190) Grad: 6617.6094  LR: 0.000005  \n",
      "Epoch: [1][1000/36908] Elapsed 14m 0s (remain 502m 22s) Loss: 0.0460(0.1098) Grad: 8881.7256  LR: 0.000005  \n",
      "Epoch: [1][1100/36908] Elapsed 15m 23s (remain 500m 28s) Loss: 0.0280(0.1014) Grad: 4271.7651  LR: 0.000006  \n",
      "Epoch: [1][1200/36908] Elapsed 16m 46s (remain 498m 40s) Loss: 0.0162(0.0944) Grad: 9663.5732  LR: 0.000007  \n",
      "Epoch: [1][1300/36908] Elapsed 18m 9s (remain 496m 57s) Loss: 0.0082(0.0883) Grad: 3583.1399  LR: 0.000007  \n",
      "Epoch: [1][1400/36908] Elapsed 19m 32s (remain 495m 24s) Loss: 0.0270(0.0830) Grad: 10485.1885  LR: 0.000008  \n",
      "Epoch: [1][1500/36908] Elapsed 20m 57s (remain 494m 29s) Loss: 0.0315(0.0785) Grad: 11289.7549  LR: 0.000008  \n",
      "Epoch: [1][1600/36908] Elapsed 22m 21s (remain 493m 5s) Loss: 0.0002(0.0742) Grad: 201.1037  LR: 0.000009  \n",
      "Epoch: [1][1700/36908] Elapsed 23m 45s (remain 491m 47s) Loss: 0.0029(0.0706) Grad: 3015.1318  LR: 0.000009  \n",
      "Epoch: [1][1800/36908] Elapsed 25m 8s (remain 490m 11s) Loss: 0.0082(0.0672) Grad: 4557.2188  LR: 0.000010  \n",
      "Epoch: [1][1900/36908] Elapsed 26m 31s (remain 488m 29s) Loss: 0.0092(0.0643) Grad: 6742.6040  LR: 0.000010  \n",
      "Epoch: [1][2000/36908] Elapsed 27m 55s (remain 487m 7s) Loss: 0.0221(0.0616) Grad: 5005.5044  LR: 0.000011  \n",
      "Epoch: [1][2100/36908] Elapsed 29m 19s (remain 485m 51s) Loss: 0.0152(0.0591) Grad: 4969.0029  LR: 0.000011  \n",
      "Epoch: [1][2200/36908] Elapsed 30m 43s (remain 484m 29s) Loss: 0.0097(0.0568) Grad: 4330.3638  LR: 0.000012  \n",
      "Epoch: [1][2300/36908] Elapsed 32m 7s (remain 483m 16s) Loss: 0.0032(0.0547) Grad: 1952.6454  LR: 0.000012  \n",
      "Epoch: [1][2400/36908] Elapsed 33m 31s (remain 481m 47s) Loss: 0.0137(0.0527) Grad: 14867.2861  LR: 0.000013  \n",
      "Epoch: [1][2500/36908] Elapsed 34m 54s (remain 480m 19s) Loss: 0.0041(0.0509) Grad: 4922.9341  LR: 0.000014  \n",
      "Epoch: [1][2600/36908] Elapsed 36m 19s (remain 479m 4s) Loss: 0.0025(0.0493) Grad: 1362.8356  LR: 0.000014  \n",
      "Epoch: [1][2700/36908] Elapsed 37m 44s (remain 477m 56s) Loss: 0.0156(0.0478) Grad: 4605.1846  LR: 0.000015  \n",
      "Epoch: [1][2800/36908] Elapsed 39m 9s (remain 476m 46s) Loss: 0.0027(0.0463) Grad: 3551.3264  LR: 0.000015  \n",
      "Epoch: [1][2900/36908] Elapsed 40m 33s (remain 475m 24s) Loss: 0.0056(0.0450) Grad: 10734.8398  LR: 0.000016  \n",
      "Epoch: [1][3000/36908] Elapsed 41m 56s (remain 473m 54s) Loss: 0.0037(0.0437) Grad: 1683.7086  LR: 0.000016  \n",
      "Epoch: [1][3100/36908] Elapsed 43m 20s (remain 472m 33s) Loss: 0.0005(0.0426) Grad: 513.2897  LR: 0.000017  \n",
      "Epoch: [1][3200/36908] Elapsed 44m 44s (remain 471m 11s) Loss: 0.0061(0.0415) Grad: 4074.4172  LR: 0.000017  \n",
      "Epoch: [1][3300/36908] Elapsed 46m 8s (remain 469m 44s) Loss: 0.0090(0.0405) Grad: 3095.9290  LR: 0.000018  \n",
      "Epoch: [1][3400/36908] Elapsed 47m 31s (remain 468m 8s) Loss: 0.0055(0.0395) Grad: 1268.5276  LR: 0.000018  \n",
      "Epoch: [1][3500/36908] Elapsed 48m 53s (remain 466m 31s) Loss: 0.0129(0.0386) Grad: 3393.6843  LR: 0.000019  \n",
      "Epoch: [1][3600/36908] Elapsed 50m 17s (remain 465m 12s) Loss: 0.0029(0.0377) Grad: 1208.7949  LR: 0.000020  \n",
      "Epoch: [1][3700/36908] Elapsed 51m 41s (remain 463m 43s) Loss: 0.0092(0.0368) Grad: 2800.5852  LR: 0.000020  \n",
      "Epoch: [1][3800/36908] Elapsed 53m 3s (remain 462m 9s) Loss: 0.0116(0.0361) Grad: 3430.6304  LR: 0.000020  \n",
      "Epoch: [1][3900/36908] Elapsed 54m 27s (remain 460m 43s) Loss: 0.0007(0.0353) Grad: 300.3918  LR: 0.000020  \n",
      "Epoch: [1][4000/36908] Elapsed 55m 50s (remain 459m 18s) Loss: 0.0038(0.0345) Grad: 558.8502  LR: 0.000020  \n",
      "Epoch: [1][4100/36908] Elapsed 57m 15s (remain 458m 0s) Loss: 0.0015(0.0338) Grad: 1296.1473  LR: 0.000020  \n",
      "Epoch: [1][4200/36908] Elapsed 58m 38s (remain 456m 34s) Loss: 0.0017(0.0331) Grad: 504.3802  LR: 0.000020  \n",
      "Epoch: [1][4300/36908] Elapsed 60m 3s (remain 455m 15s) Loss: 0.0018(0.0325) Grad: 1620.1521  LR: 0.000020  \n",
      "Epoch: [1][4400/36908] Elapsed 61m 26s (remain 453m 48s) Loss: 0.0092(0.0318) Grad: 4172.0898  LR: 0.000020  \n",
      "Epoch: [1][4500/36908] Elapsed 62m 49s (remain 452m 19s) Loss: 0.0002(0.0312) Grad: 42.9676  LR: 0.000020  \n",
      "Epoch: [1][4600/36908] Elapsed 64m 13s (remain 451m 1s) Loss: 0.0052(0.0307) Grad: 5821.2383  LR: 0.000019  \n",
      "Epoch: [1][4700/36908] Elapsed 65m 38s (remain 449m 41s) Loss: 0.0002(0.0302) Grad: 49.3142  LR: 0.000019  \n",
      "Epoch: [1][4800/36908] Elapsed 67m 2s (remain 448m 18s) Loss: 0.0038(0.0296) Grad: 1079.3412  LR: 0.000019  \n",
      "Epoch: [1][4900/36908] Elapsed 68m 25s (remain 446m 49s) Loss: 0.0030(0.0291) Grad: 1173.9481  LR: 0.000019  \n",
      "Epoch: [1][5000/36908] Elapsed 69m 47s (remain 445m 17s) Loss: 0.0052(0.0287) Grad: 1578.0339  LR: 0.000019  \n",
      "Epoch: [1][5100/36908] Elapsed 71m 11s (remain 443m 53s) Loss: 0.0060(0.0282) Grad: 5798.9146  LR: 0.000019  \n",
      "Epoch: [1][5200/36908] Elapsed 72m 34s (remain 442m 27s) Loss: 0.0047(0.0277) Grad: 11683.6309  LR: 0.000019  \n",
      "Epoch: [1][5300/36908] Elapsed 73m 57s (remain 440m 58s) Loss: 0.0019(0.0273) Grad: 1403.0712  LR: 0.000019  \n",
      "Epoch: [1][5400/36908] Elapsed 75m 20s (remain 439m 30s) Loss: 0.0088(0.0269) Grad: 9699.9619  LR: 0.000019  \n",
      "Epoch: [1][5500/36908] Elapsed 76m 44s (remain 438m 8s) Loss: 0.0001(0.0265) Grad: 25.7705  LR: 0.000019  \n",
      "Epoch: [1][5600/36908] Elapsed 78m 7s (remain 436m 39s) Loss: 0.0097(0.0261) Grad: 10272.0771  LR: 0.000019  \n",
      "Epoch: [1][5700/36908] Elapsed 79m 29s (remain 435m 10s) Loss: 0.0019(0.0257) Grad: 5865.8618  LR: 0.000019  \n",
      "Epoch: [1][5800/36908] Elapsed 80m 54s (remain 433m 50s) Loss: 0.0003(0.0253) Grad: 165.3418  LR: 0.000019  \n",
      "Epoch: [1][5900/36908] Elapsed 82m 18s (remain 432m 30s) Loss: 0.0048(0.0250) Grad: 6605.0415  LR: 0.000019  \n",
      "Epoch: [1][6000/36908] Elapsed 83m 41s (remain 431m 2s) Loss: 0.0016(0.0246) Grad: 4773.6592  LR: 0.000019  \n",
      "Epoch: [1][6100/36908] Elapsed 85m 4s (remain 429m 36s) Loss: 0.0005(0.0243) Grad: 564.7920  LR: 0.000019  \n",
      "Epoch: [1][6200/36908] Elapsed 86m 28s (remain 428m 14s) Loss: 0.0005(0.0240) Grad: 108.5128  LR: 0.000018  \n",
      "Epoch: [1][6300/36908] Elapsed 87m 51s (remain 426m 46s) Loss: 0.0025(0.0236) Grad: 729.5898  LR: 0.000018  \n",
      "Epoch: [1][6400/36908] Elapsed 89m 15s (remain 425m 24s) Loss: 0.0006(0.0233) Grad: 101.2579  LR: 0.000018  \n",
      "Epoch: [1][6500/36908] Elapsed 90m 37s (remain 423m 54s) Loss: 0.0039(0.0230) Grad: 1409.1803  LR: 0.000018  \n",
      "Epoch: [1][6600/36908] Elapsed 92m 1s (remain 422m 31s) Loss: 0.0003(0.0228) Grad: 198.5444  LR: 0.000018  \n",
      "Epoch: [1][6700/36908] Elapsed 93m 24s (remain 421m 5s) Loss: 0.0208(0.0225) Grad: 11712.4932  LR: 0.000018  \n",
      "Epoch: [1][6800/36908] Elapsed 94m 47s (remain 419m 38s) Loss: 0.0007(0.0222) Grad: 579.4113  LR: 0.000018  \n",
      "Epoch: [1][6900/36908] Elapsed 96m 11s (remain 418m 15s) Loss: 0.0006(0.0219) Grad: 872.1252  LR: 0.000018  \n",
      "Epoch: [1][7000/36908] Elapsed 97m 35s (remain 416m 55s) Loss: 0.0002(0.0217) Grad: 131.0344  LR: 0.000018  \n",
      "Epoch: [1][7100/36908] Elapsed 98m 59s (remain 415m 29s) Loss: 0.0012(0.0214) Grad: 3215.2764  LR: 0.000018  \n",
      "Epoch: [1][7200/36908] Elapsed 100m 22s (remain 414m 5s) Loss: 0.0030(0.0212) Grad: 7132.5737  LR: 0.000018  \n",
      "Epoch: [1][7300/36908] Elapsed 101m 45s (remain 412m 40s) Loss: 0.0001(0.0209) Grad: 26.4739  LR: 0.000018  \n",
      "Epoch: [1][7400/36908] Elapsed 103m 8s (remain 411m 14s) Loss: 0.0013(0.0207) Grad: 248.5866  LR: 0.000018  \n",
      "Epoch: [1][7500/36908] Elapsed 104m 32s (remain 409m 50s) Loss: 0.0031(0.0205) Grad: 3299.1333  LR: 0.000018  \n",
      "Epoch: [1][7600/36908] Elapsed 105m 55s (remain 408m 24s) Loss: 0.0008(0.0203) Grad: 295.1803  LR: 0.000018  \n",
      "Epoch: [1][7700/36908] Elapsed 107m 19s (remain 407m 1s) Loss: 0.0089(0.0201) Grad: 15786.2715  LR: 0.000018  \n",
      "Epoch: [1][7800/36908] Elapsed 108m 43s (remain 405m 40s) Loss: 0.0008(0.0199) Grad: 1288.0488  LR: 0.000018  \n",
      "Epoch: [1][7900/36908] Elapsed 110m 6s (remain 404m 15s) Loss: 0.0003(0.0196) Grad: 588.0323  LR: 0.000017  \n",
      "Epoch: [1][8000/36908] Elapsed 111m 30s (remain 402m 53s) Loss: 0.0011(0.0195) Grad: 640.8094  LR: 0.000017  \n",
      "Epoch: [1][8100/36908] Elapsed 112m 53s (remain 401m 28s) Loss: 0.0094(0.0193) Grad: 24229.6191  LR: 0.000017  \n",
      "Epoch: [1][8200/36908] Elapsed 114m 16s (remain 400m 0s) Loss: 0.0008(0.0191) Grad: 844.0596  LR: 0.000017  \n",
      "Epoch: [1][8300/36908] Elapsed 115m 39s (remain 398m 33s) Loss: 0.0016(0.0189) Grad: 3117.8848  LR: 0.000017  \n",
      "Epoch: [1][8400/36908] Elapsed 117m 2s (remain 397m 10s) Loss: 0.0049(0.0187) Grad: 5336.1699  LR: 0.000017  \n",
      "Epoch: [1][8500/36908] Elapsed 118m 25s (remain 395m 42s) Loss: 0.0000(0.0185) Grad: 31.8553  LR: 0.000017  \n",
      "Epoch: [1][8600/36908] Elapsed 119m 47s (remain 394m 15s) Loss: 0.0070(0.0184) Grad: 7014.6338  LR: 0.000017  \n",
      "Epoch: [1][8700/36908] Elapsed 121m 10s (remain 392m 48s) Loss: 0.0001(0.0182) Grad: 30.6483  LR: 0.000017  \n",
      "Epoch: [1][8800/36908] Elapsed 122m 33s (remain 391m 24s) Loss: 0.0063(0.0180) Grad: 5186.8364  LR: 0.000017  \n",
      "Epoch: [1][8900/36908] Elapsed 123m 57s (remain 390m 1s) Loss: 0.0058(0.0179) Grad: 3429.0320  LR: 0.000017  \n",
      "Epoch: [1][9000/36908] Elapsed 125m 19s (remain 388m 35s) Loss: 0.0095(0.0177) Grad: 13083.4307  LR: 0.000017  \n",
      "Epoch: [1][9100/36908] Elapsed 126m 42s (remain 387m 8s) Loss: 0.0001(0.0175) Grad: 75.1579  LR: 0.000017  \n",
      "Epoch: [1][9200/36908] Elapsed 128m 6s (remain 385m 45s) Loss: 0.0001(0.0174) Grad: 30.4539  LR: 0.000017  \n",
      "Epoch: [1][9300/36908] Elapsed 129m 29s (remain 384m 22s) Loss: 0.0054(0.0172) Grad: 5997.3037  LR: 0.000017  \n",
      "Epoch: [1][9400/36908] Elapsed 130m 52s (remain 382m 55s) Loss: 0.0001(0.0171) Grad: 14.7609  LR: 0.000017  \n",
      "Epoch: [1][9500/36908] Elapsed 132m 15s (remain 381m 31s) Loss: 0.0003(0.0169) Grad: 417.9245  LR: 0.000017  \n",
      "Epoch: [1][9600/36908] Elapsed 133m 39s (remain 380m 8s) Loss: 0.0087(0.0168) Grad: 103931.2969  LR: 0.000016  \n",
      "Epoch: [1][9700/36908] Elapsed 135m 2s (remain 378m 42s) Loss: 0.0082(0.0167) Grad: 17069.3359  LR: 0.000016  \n",
      "Epoch: [1][9800/36908] Elapsed 136m 25s (remain 377m 17s) Loss: 0.0029(0.0165) Grad: 15142.2295  LR: 0.000016  \n",
      "Epoch: [1][9900/36908] Elapsed 137m 48s (remain 375m 53s) Loss: 0.0000(0.0164) Grad: 23.6945  LR: 0.000016  \n",
      "Epoch: [1][10000/36908] Elapsed 139m 13s (remain 374m 33s) Loss: 0.0001(0.0163) Grad: 15.8587  LR: 0.000016  \n",
      "Epoch: [1][10100/36908] Elapsed 140m 36s (remain 373m 9s) Loss: 0.0040(0.0161) Grad: 27097.3945  LR: 0.000016  \n",
      "Epoch: [1][10200/36908] Elapsed 141m 59s (remain 371m 45s) Loss: 0.0032(0.0160) Grad: 2035.5050  LR: 0.000016  \n",
      "Epoch: [1][10300/36908] Elapsed 143m 24s (remain 370m 24s) Loss: 0.0010(0.0159) Grad: 1203.5477  LR: 0.000016  \n",
      "Epoch: [1][10400/36908] Elapsed 144m 48s (remain 369m 1s) Loss: 0.0031(0.0157) Grad: 694.3013  LR: 0.000016  \n",
      "Epoch: [1][10500/36908] Elapsed 146m 10s (remain 367m 36s) Loss: 0.0030(0.0156) Grad: 4154.1045  LR: 0.000016  \n",
      "Epoch: [1][10600/36908] Elapsed 147m 32s (remain 366m 9s) Loss: 0.0011(0.0155) Grad: 1306.2972  LR: 0.000016  \n",
      "Epoch: [1][10700/36908] Elapsed 148m 57s (remain 364m 46s) Loss: 0.0104(0.0154) Grad: 19967.7949  LR: 0.000016  \n",
      "Epoch: [1][10800/36908] Elapsed 150m 21s (remain 363m 25s) Loss: 0.0016(0.0153) Grad: 1225.5187  LR: 0.000016  \n",
      "Epoch: [1][10900/36908] Elapsed 151m 44s (remain 362m 1s) Loss: 0.0006(0.0152) Grad: 993.7222  LR: 0.000016  \n",
      "Epoch: [1][11000/36908] Elapsed 153m 7s (remain 360m 35s) Loss: 0.0002(0.0151) Grad: 145.2179  LR: 0.000016  \n",
      "Epoch: [1][11100/36908] Elapsed 154m 30s (remain 359m 11s) Loss: 0.0002(0.0150) Grad: 241.6084  LR: 0.000016  \n",
      "Epoch: [1][11200/36908] Elapsed 155m 54s (remain 357m 49s) Loss: 0.0032(0.0149) Grad: 2612.3274  LR: 0.000015  \n",
      "Epoch: [1][11300/36908] Elapsed 157m 17s (remain 356m 23s) Loss: 0.0096(0.0148) Grad: 12747.1797  LR: 0.000015  \n",
      "Epoch: [1][11400/36908] Elapsed 158m 39s (remain 354m 57s) Loss: 0.0002(0.0146) Grad: 136.5763  LR: 0.000015  \n",
      "Epoch: [1][11500/36908] Elapsed 160m 1s (remain 353m 30s) Loss: 0.0022(0.0146) Grad: 7059.3936  LR: 0.000015  \n",
      "Epoch: [1][11600/36908] Elapsed 161m 24s (remain 352m 5s) Loss: 0.0012(0.0144) Grad: 3935.1040  LR: 0.000015  \n",
      "Epoch: [1][11700/36908] Elapsed 162m 47s (remain 350m 42s) Loss: 0.0002(0.0143) Grad: 555.9279  LR: 0.000015  \n",
      "Epoch: [1][11800/36908] Elapsed 164m 10s (remain 349m 17s) Loss: 0.0015(0.0143) Grad: 6330.5977  LR: 0.000015  \n",
      "Epoch: [1][11900/36908] Elapsed 165m 33s (remain 347m 52s) Loss: 0.0107(0.0142) Grad: 24461.9941  LR: 0.000015  \n",
      "Epoch: [1][12000/36908] Elapsed 166m 56s (remain 346m 28s) Loss: 0.0072(0.0141) Grad: 21512.1895  LR: 0.000015  \n",
      "Epoch: [1][12100/36908] Elapsed 168m 19s (remain 345m 3s) Loss: 0.0004(0.0140) Grad: 1427.7264  LR: 0.000015  \n",
      "Epoch: [1][12200/36908] Elapsed 169m 42s (remain 343m 39s) Loss: 0.0001(0.0139) Grad: 396.5957  LR: 0.000015  \n",
      "Epoch: [1][12300/36908] Elapsed 171m 4s (remain 342m 14s) Loss: 0.0003(0.0138) Grad: 2234.9858  LR: 0.000015  \n",
      "Epoch: [1][12400/36908] Elapsed 172m 27s (remain 340m 49s) Loss: 0.0035(0.0137) Grad: 6560.0366  LR: 0.000015  \n",
      "Epoch: [1][12500/36908] Elapsed 173m 51s (remain 339m 27s) Loss: 0.0001(0.0136) Grad: 65.2468  LR: 0.000015  \n",
      "Epoch: [1][12600/36908] Elapsed 175m 14s (remain 338m 2s) Loss: 0.0011(0.0135) Grad: 247.3391  LR: 0.000015  \n",
      "Epoch: [1][12700/36908] Elapsed 176m 36s (remain 336m 36s) Loss: 0.0001(0.0135) Grad: 28.2300  LR: 0.000015  \n",
      "Epoch: [1][12800/36908] Elapsed 178m 1s (remain 335m 15s) Loss: 0.0009(0.0134) Grad: 1273.9933  LR: 0.000015  \n",
      "Epoch: [1][12900/36908] Elapsed 179m 26s (remain 333m 54s) Loss: 0.0002(0.0133) Grad: 269.2599  LR: 0.000014  \n",
      "Epoch: [1][13000/36908] Elapsed 180m 49s (remain 332m 30s) Loss: 0.0008(0.0132) Grad: 327.8914  LR: 0.000014  \n",
      "Epoch: [1][13100/36908] Elapsed 182m 13s (remain 331m 7s) Loss: 0.0008(0.0132) Grad: 6827.8462  LR: 0.000014  \n",
      "Epoch: [1][13200/36908] Elapsed 183m 36s (remain 329m 43s) Loss: 0.0001(0.0131) Grad: 72.8682  LR: 0.000014  \n",
      "Epoch: [1][13300/36908] Elapsed 184m 58s (remain 328m 17s) Loss: 0.0025(0.0130) Grad: 12051.6953  LR: 0.000014  \n",
      "Epoch: [1][13400/36908] Elapsed 186m 21s (remain 326m 53s) Loss: 0.0001(0.0129) Grad: 452.7955  LR: 0.000014  \n",
      "Epoch: [1][13500/36908] Elapsed 187m 44s (remain 325m 29s) Loss: 0.0005(0.0129) Grad: 1308.9954  LR: 0.000014  \n",
      "Epoch: [1][13600/36908] Elapsed 189m 6s (remain 324m 4s) Loss: 0.0047(0.0128) Grad: 27507.2871  LR: 0.000014  \n",
      "Epoch: [1][13700/36908] Elapsed 190m 30s (remain 322m 40s) Loss: 0.0034(0.0127) Grad: 3086.2119  LR: 0.000014  \n",
      "Epoch: [1][13800/36908] Elapsed 191m 52s (remain 321m 15s) Loss: 0.0000(0.0126) Grad: 17.1135  LR: 0.000014  \n",
      "Epoch: [1][13900/36908] Elapsed 193m 14s (remain 319m 49s) Loss: 0.0015(0.0126) Grad: 984.7119  LR: 0.000014  \n",
      "Epoch: [1][14000/36908] Elapsed 194m 37s (remain 318m 25s) Loss: 0.0005(0.0125) Grad: 393.5126  LR: 0.000014  \n",
      "Epoch: [1][14100/36908] Elapsed 196m 1s (remain 317m 3s) Loss: 0.0014(0.0124) Grad: 1938.0441  LR: 0.000014  \n",
      "Epoch: [1][14200/36908] Elapsed 197m 24s (remain 315m 39s) Loss: 0.0002(0.0124) Grad: 263.0612  LR: 0.000014  \n",
      "Epoch: [1][14300/36908] Elapsed 198m 47s (remain 314m 14s) Loss: 0.0248(0.0123) Grad: 75165.2812  LR: 0.000014  \n",
      "Epoch: [1][14400/36908] Elapsed 200m 11s (remain 312m 51s) Loss: 0.0001(0.0122) Grad: 279.1873  LR: 0.000014  \n",
      "Epoch: [1][14500/36908] Elapsed 201m 34s (remain 311m 28s) Loss: 0.0007(0.0122) Grad: 17260.6973  LR: 0.000013  \n",
      "Epoch: [1][14600/36908] Elapsed 202m 57s (remain 310m 4s) Loss: 0.0052(0.0121) Grad: 30704.6855  LR: 0.000013  \n",
      "Epoch: [1][14700/36908] Elapsed 204m 22s (remain 308m 43s) Loss: 0.0000(0.0120) Grad: 8.9691  LR: 0.000013  \n",
      "Epoch: [1][14800/36908] Elapsed 205m 46s (remain 307m 20s) Loss: 0.0000(0.0120) Grad: 23.4523  LR: 0.000013  \n",
      "Epoch: [1][14900/36908] Elapsed 207m 10s (remain 305m 58s) Loss: 0.0025(0.0119) Grad: 13003.3672  LR: 0.000013  \n",
      "Epoch: [1][15000/36908] Elapsed 208m 35s (remain 304m 37s) Loss: 0.0001(0.0119) Grad: 115.1011  LR: 0.000013  \n",
      "Epoch: [1][15100/36908] Elapsed 209m 59s (remain 303m 14s) Loss: 0.0032(0.0118) Grad: 2569.5713  LR: 0.000013  \n",
      "Epoch: [1][15200/36908] Elapsed 211m 21s (remain 301m 48s) Loss: 0.0001(0.0117) Grad: 127.4563  LR: 0.000013  \n",
      "Epoch: [1][15300/36908] Elapsed 212m 45s (remain 300m 26s) Loss: 0.0145(0.0117) Grad: 42761.3555  LR: 0.000013  \n",
      "Epoch: [1][15400/36908] Elapsed 214m 8s (remain 299m 2s) Loss: 0.0003(0.0116) Grad: 82.0311  LR: 0.000013  \n",
      "Epoch: [1][15500/36908] Elapsed 215m 31s (remain 297m 39s) Loss: 0.0000(0.0116) Grad: 79.5731  LR: 0.000013  \n",
      "Epoch: [1][15600/36908] Elapsed 216m 55s (remain 296m 15s) Loss: 0.0001(0.0115) Grad: 104.1748  LR: 0.000013  \n",
      "Epoch: [1][15700/36908] Elapsed 218m 18s (remain 294m 51s) Loss: 0.0015(0.0115) Grad: 2550.4973  LR: 0.000013  \n",
      "Epoch: [1][15800/36908] Elapsed 219m 40s (remain 293m 27s) Loss: 0.0007(0.0114) Grad: 4311.9775  LR: 0.000013  \n",
      "Epoch: [1][15900/36908] Elapsed 221m 3s (remain 292m 2s) Loss: 0.0056(0.0113) Grad: 8330.9453  LR: 0.000013  \n",
      "Epoch: [1][16000/36908] Elapsed 222m 26s (remain 290m 38s) Loss: 0.0001(0.0113) Grad: 77.1249  LR: 0.000013  \n",
      "Epoch: [1][16100/36908] Elapsed 223m 49s (remain 289m 14s) Loss: 0.0000(0.0112) Grad: 344.2491  LR: 0.000013  \n",
      "Epoch: [1][16200/36908] Elapsed 225m 12s (remain 287m 50s) Loss: 0.0039(0.0112) Grad: 10991.7471  LR: 0.000012  \n",
      "Epoch: [1][16300/36908] Elapsed 226m 35s (remain 286m 27s) Loss: 0.0022(0.0111) Grad: 22298.9258  LR: 0.000012  \n",
      "Epoch: [1][16400/36908] Elapsed 227m 59s (remain 285m 3s) Loss: 0.0069(0.0111) Grad: 14857.6895  LR: 0.000012  \n",
      "Epoch: [1][16500/36908] Elapsed 229m 22s (remain 283m 40s) Loss: 0.0155(0.0110) Grad: 25689.0020  LR: 0.000012  \n",
      "Epoch: [1][16600/36908] Elapsed 230m 45s (remain 282m 16s) Loss: 0.0001(0.0110) Grad: 34.2693  LR: 0.000012  \n",
      "Epoch: [1][16700/36908] Elapsed 232m 8s (remain 280m 52s) Loss: 0.0026(0.0109) Grad: 3775.8467  LR: 0.000012  \n",
      "Epoch: [1][16800/36908] Elapsed 233m 30s (remain 279m 27s) Loss: 0.0003(0.0109) Grad: 93.9923  LR: 0.000012  \n",
      "Epoch: [1][16900/36908] Elapsed 234m 54s (remain 278m 4s) Loss: 0.0002(0.0108) Grad: 146.3933  LR: 0.000012  \n",
      "Epoch: [1][17000/36908] Elapsed 236m 17s (remain 276m 40s) Loss: 0.0009(0.0108) Grad: 5845.4854  LR: 0.000012  \n",
      "Epoch: [1][17100/36908] Elapsed 237m 40s (remain 275m 17s) Loss: 0.0068(0.0107) Grad: 22024.0371  LR: 0.000012  \n",
      "Epoch: [1][17200/36908] Elapsed 239m 3s (remain 273m 53s) Loss: 0.0003(0.0107) Grad: 2237.9399  LR: 0.000012  \n",
      "Epoch: [1][17300/36908] Elapsed 240m 26s (remain 272m 29s) Loss: 0.0050(0.0107) Grad: 96173.8906  LR: 0.000012  \n",
      "Epoch: [1][17400/36908] Elapsed 241m 50s (remain 271m 7s) Loss: 0.0025(0.0106) Grad: 17231.2344  LR: 0.000012  \n",
      "Epoch: [1][17500/36908] Elapsed 243m 13s (remain 269m 43s) Loss: 0.0011(0.0106) Grad: 3670.5217  LR: 0.000012  \n",
      "Epoch: [1][17600/36908] Elapsed 244m 36s (remain 268m 19s) Loss: 0.0104(0.0105) Grad: 65836.2500  LR: 0.000012  \n",
      "Epoch: [1][17700/36908] Elapsed 245m 59s (remain 266m 55s) Loss: 0.0013(0.0105) Grad: 2497.2512  LR: 0.000012  \n",
      "Epoch: [1][17800/36908] Elapsed 247m 24s (remain 265m 33s) Loss: 0.0000(0.0104) Grad: 33.8360  LR: 0.000012  \n",
      "Epoch: [1][17900/36908] Elapsed 248m 47s (remain 264m 9s) Loss: 0.0078(0.0104) Grad: 37586.7617  LR: 0.000011  \n",
      "Epoch: [1][18000/36908] Elapsed 250m 11s (remain 262m 46s) Loss: 0.0001(0.0103) Grad: 166.7673  LR: 0.000011  \n",
      "Epoch: [1][18100/36908] Elapsed 251m 36s (remain 261m 24s) Loss: 0.0001(0.0103) Grad: 111.5162  LR: 0.000011  \n",
      "Epoch: [1][18200/36908] Elapsed 253m 0s (remain 260m 2s) Loss: 0.0019(0.0103) Grad: 7412.8911  LR: 0.000011  \n",
      "Epoch: [1][18300/36908] Elapsed 254m 23s (remain 258m 38s) Loss: 0.0001(0.0102) Grad: 35.9956  LR: 0.000011  \n",
      "Epoch: [1][18400/36908] Elapsed 255m 47s (remain 257m 15s) Loss: 0.0023(0.0102) Grad: 8241.3936  LR: 0.000011  \n",
      "Epoch: [1][18500/36908] Elapsed 257m 10s (remain 255m 51s) Loss: 0.0017(0.0101) Grad: 14729.8281  LR: 0.000011  \n",
      "Epoch: [1][18600/36908] Elapsed 258m 34s (remain 254m 28s) Loss: 0.0068(0.0101) Grad: 100839.7500  LR: 0.000011  \n",
      "Epoch: [1][18700/36908] Elapsed 259m 57s (remain 253m 5s) Loss: 0.0022(0.0101) Grad: 18268.4883  LR: 0.000011  \n",
      "Epoch: [1][18800/36908] Elapsed 261m 20s (remain 251m 41s) Loss: 0.0002(0.0100) Grad: 1111.3472  LR: 0.000011  \n",
      "Epoch: [1][18900/36908] Elapsed 262m 44s (remain 250m 19s) Loss: 0.0011(0.0100) Grad: 7625.5674  LR: 0.000011  \n",
      "Epoch: [1][19000/36908] Elapsed 264m 8s (remain 248m 55s) Loss: 0.0002(0.0099) Grad: 301.0856  LR: 0.000011  \n",
      "Epoch: [1][19100/36908] Elapsed 265m 31s (remain 247m 32s) Loss: 0.0000(0.0099) Grad: 173.5390  LR: 0.000011  \n",
      "Epoch: [1][19200/36908] Elapsed 266m 56s (remain 246m 10s) Loss: 0.0006(0.0099) Grad: 1886.0846  LR: 0.000011  \n",
      "Epoch: [1][19300/36908] Elapsed 268m 19s (remain 244m 46s) Loss: 0.0017(0.0098) Grad: 10495.3486  LR: 0.000011  \n",
      "Epoch: [1][19400/36908] Elapsed 269m 41s (remain 243m 21s) Loss: 0.0115(0.0098) Grad: 151638.6250  LR: 0.000011  \n",
      "Epoch: [1][19500/36908] Elapsed 271m 4s (remain 241m 58s) Loss: 0.0006(0.0097) Grad: 1466.2166  LR: 0.000010  \n",
      "Epoch: [1][19600/36908] Elapsed 272m 28s (remain 240m 35s) Loss: 0.0007(0.0097) Grad: 7268.0435  LR: 0.000010  \n",
      "Epoch: [1][19700/36908] Elapsed 273m 51s (remain 239m 11s) Loss: 0.0050(0.0097) Grad: 20216.2227  LR: 0.000010  \n",
      "Epoch: [1][19800/36908] Elapsed 275m 13s (remain 237m 47s) Loss: 0.0005(0.0096) Grad: 366.8353  LR: 0.000010  \n",
      "Epoch: [1][19900/36908] Elapsed 276m 37s (remain 236m 23s) Loss: 0.0072(0.0096) Grad: 22684.8516  LR: 0.000010  \n",
      "Epoch: [1][20000/36908] Elapsed 278m 1s (remain 235m 0s) Loss: 0.0005(0.0096) Grad: 446.2597  LR: 0.000010  \n",
      "Epoch: [1][20100/36908] Elapsed 279m 23s (remain 233m 36s) Loss: 0.0004(0.0095) Grad: 4611.3657  LR: 0.000010  \n",
      "Epoch: [1][20200/36908] Elapsed 280m 46s (remain 232m 12s) Loss: 0.0003(0.0095) Grad: 5705.2114  LR: 0.000010  \n",
      "Epoch: [1][20300/36908] Elapsed 282m 8s (remain 230m 48s) Loss: 0.0003(0.0095) Grad: 7496.1196  LR: 0.000010  \n",
      "Epoch: [1][20400/36908] Elapsed 283m 31s (remain 229m 24s) Loss: 0.0016(0.0094) Grad: 7550.5537  LR: 0.000010  \n",
      "Epoch: [1][20500/36908] Elapsed 284m 55s (remain 228m 1s) Loss: 0.0006(0.0094) Grad: 801.4783  LR: 0.000010  \n",
      "Epoch: [1][20600/36908] Elapsed 286m 17s (remain 226m 37s) Loss: 0.0014(0.0094) Grad: 11384.8643  LR: 0.000010  \n",
      "Epoch: [1][20700/36908] Elapsed 287m 41s (remain 225m 14s) Loss: 0.0037(0.0093) Grad: 15277.2559  LR: 0.000010  \n",
      "Epoch: [1][20800/36908] Elapsed 289m 4s (remain 223m 50s) Loss: 0.0041(0.0093) Grad: 38341.2969  LR: 0.000010  \n",
      "Epoch: [1][20900/36908] Elapsed 290m 28s (remain 222m 27s) Loss: 0.0047(0.0093) Grad: 27468.5742  LR: 0.000010  \n",
      "Epoch: [1][21000/36908] Elapsed 291m 51s (remain 221m 3s) Loss: 0.0001(0.0092) Grad: 25.1869  LR: 0.000010  \n",
      "Epoch: [1][21100/36908] Elapsed 293m 14s (remain 219m 40s) Loss: 0.0001(0.0092) Grad: 114.5387  LR: 0.000010  \n",
      "Epoch: [1][21200/36908] Elapsed 294m 37s (remain 218m 16s) Loss: 0.0170(0.0092) Grad: 626559.5000  LR: 0.000009  \n",
      "Epoch: [1][21300/36908] Elapsed 296m 1s (remain 216m 53s) Loss: 0.0001(0.0092) Grad: 377.8553  LR: 0.000009  \n",
      "Epoch: [1][21400/36908] Elapsed 297m 25s (remain 215m 30s) Loss: 0.0044(0.0091) Grad: 15148.3398  LR: 0.000009  \n",
      "Epoch: [1][21500/36908] Elapsed 298m 49s (remain 214m 7s) Loss: 0.0004(0.0091) Grad: 756.0234  LR: 0.000009  \n",
      "Epoch: [1][21600/36908] Elapsed 300m 12s (remain 212m 44s) Loss: 0.0001(0.0091) Grad: 382.2577  LR: 0.000009  \n",
      "Epoch: [1][21700/36908] Elapsed 301m 36s (remain 211m 21s) Loss: 0.0014(0.0090) Grad: 5253.7798  LR: 0.000009  \n",
      "Epoch: [1][21800/36908] Elapsed 302m 59s (remain 209m 57s) Loss: 0.0027(0.0090) Grad: 12743.9990  LR: 0.000009  \n",
      "Epoch: [1][21900/36908] Elapsed 304m 23s (remain 208m 34s) Loss: 0.0009(0.0090) Grad: 10033.8213  LR: 0.000009  \n",
      "Epoch: [1][22000/36908] Elapsed 305m 47s (remain 207m 11s) Loss: 0.0000(0.0090) Grad: 86.5155  LR: 0.000009  \n",
      "Epoch: [1][22100/36908] Elapsed 307m 10s (remain 205m 47s) Loss: 0.0119(0.0089) Grad: 75426.8984  LR: 0.000009  \n",
      "Epoch: [1][22200/36908] Elapsed 308m 33s (remain 204m 24s) Loss: 0.0013(0.0089) Grad: 10734.1826  LR: 0.000009  \n",
      "Epoch: [1][22300/36908] Elapsed 309m 58s (remain 203m 1s) Loss: 0.0002(0.0089) Grad: 151.5782  LR: 0.000009  \n",
      "Epoch: [1][22400/36908] Elapsed 311m 21s (remain 201m 38s) Loss: 0.0002(0.0088) Grad: 351.7347  LR: 0.000009  \n",
      "Epoch: [1][22500/36908] Elapsed 312m 45s (remain 200m 15s) Loss: 0.0068(0.0088) Grad: 54627.2070  LR: 0.000009  \n",
      "Epoch: [1][22600/36908] Elapsed 314m 7s (remain 198m 51s) Loss: 0.0053(0.0088) Grad: 27048.7891  LR: 0.000009  \n",
      "Epoch: [1][22700/36908] Elapsed 315m 30s (remain 197m 27s) Loss: 0.0032(0.0088) Grad: 7042.3921  LR: 0.000009  \n",
      "Epoch: [1][22800/36908] Elapsed 316m 53s (remain 196m 3s) Loss: 0.0001(0.0087) Grad: 37.8304  LR: 0.000008  \n",
      "Epoch: [1][22900/36908] Elapsed 318m 16s (remain 194m 39s) Loss: 0.0006(0.0087) Grad: 1589.7837  LR: 0.000008  \n",
      "Epoch: [1][23000/36908] Elapsed 319m 39s (remain 193m 16s) Loss: 0.0018(0.0087) Grad: 15261.2422  LR: 0.000008  \n",
      "Epoch: [1][23100/36908] Elapsed 321m 3s (remain 191m 53s) Loss: 0.0001(0.0087) Grad: 421.5736  LR: 0.000008  \n",
      "Epoch: [1][23200/36908] Elapsed 322m 26s (remain 190m 29s) Loss: 0.0005(0.0086) Grad: 378.9021  LR: 0.000008  \n",
      "Epoch: [1][23300/36908] Elapsed 323m 50s (remain 189m 7s) Loss: 0.0005(0.0086) Grad: 758.0409  LR: 0.000008  \n",
      "Epoch: [1][23400/36908] Elapsed 325m 14s (remain 187m 43s) Loss: 0.0027(0.0086) Grad: 7119.3550  LR: 0.000008  \n",
      "Epoch: [1][23500/36908] Elapsed 326m 38s (remain 186m 20s) Loss: 0.0023(0.0086) Grad: 12369.0439  LR: 0.000008  \n",
      "Epoch: [1][23600/36908] Elapsed 328m 2s (remain 184m 57s) Loss: 0.0007(0.0085) Grad: 1533.5229  LR: 0.000008  \n",
      "Epoch: [1][23700/36908] Elapsed 329m 25s (remain 183m 34s) Loss: 0.0047(0.0085) Grad: 40153.8945  LR: 0.000008  \n",
      "Epoch: [1][23800/36908] Elapsed 330m 48s (remain 182m 10s) Loss: 0.0066(0.0085) Grad: 81850.3672  LR: 0.000008  \n",
      "Epoch: [1][23900/36908] Elapsed 332m 11s (remain 180m 46s) Loss: 0.0010(0.0085) Grad: 523.1412  LR: 0.000008  \n",
      "Epoch: [1][24000/36908] Elapsed 333m 35s (remain 179m 23s) Loss: 0.0020(0.0084) Grad: 2602.0193  LR: 0.000008  \n",
      "Epoch: [1][24100/36908] Elapsed 334m 58s (remain 178m 0s) Loss: 0.0018(0.0084) Grad: 18242.5352  LR: 0.000008  \n",
      "Epoch: [1][24200/36908] Elapsed 336m 21s (remain 176m 36s) Loss: 0.0015(0.0084) Grad: 11109.6377  LR: 0.000008  \n",
      "Epoch: [1][24300/36908] Elapsed 337m 43s (remain 175m 12s) Loss: 0.0022(0.0084) Grad: 7948.7207  LR: 0.000008  \n",
      "Epoch: [1][24400/36908] Elapsed 339m 7s (remain 173m 49s) Loss: 0.0014(0.0083) Grad: 7061.4771  LR: 0.000008  \n",
      "Epoch: [1][24500/36908] Elapsed 340m 32s (remain 172m 26s) Loss: 0.0040(0.0083) Grad: 82240.5469  LR: 0.000007  \n",
      "Epoch: [1][24600/36908] Elapsed 341m 56s (remain 171m 3s) Loss: 0.0104(0.0083) Grad: 22949.8477  LR: 0.000007  \n",
      "Epoch: [1][24700/36908] Elapsed 343m 20s (remain 169m 40s) Loss: 0.0075(0.0083) Grad: 16950.4531  LR: 0.000007  \n",
      "Epoch: [1][24800/36908] Elapsed 344m 43s (remain 168m 17s) Loss: 0.0001(0.0082) Grad: 151.5529  LR: 0.000007  \n",
      "Epoch: [1][24900/36908] Elapsed 346m 7s (remain 166m 53s) Loss: 0.0085(0.0082) Grad: 36679.0117  LR: 0.000007  \n",
      "Epoch: [1][25000/36908] Elapsed 347m 29s (remain 165m 30s) Loss: 0.0006(0.0082) Grad: 29021.2129  LR: 0.000007  \n",
      "Epoch: [1][25100/36908] Elapsed 348m 54s (remain 164m 7s) Loss: 0.0001(0.0082) Grad: 89.5446  LR: 0.000007  \n",
      "Epoch: [1][25200/36908] Elapsed 350m 18s (remain 162m 44s) Loss: 0.0010(0.0082) Grad: 3248.9319  LR: 0.000007  \n",
      "Epoch: [1][25300/36908] Elapsed 351m 41s (remain 161m 20s) Loss: 0.0139(0.0081) Grad: 22836.2051  LR: 0.000007  \n",
      "Epoch: [1][25400/36908] Elapsed 353m 4s (remain 159m 56s) Loss: 0.0015(0.0081) Grad: 1979.9965  LR: 0.000007  \n",
      "Epoch: [1][25500/36908] Elapsed 354m 26s (remain 158m 33s) Loss: 0.0083(0.0081) Grad: 146976.6562  LR: 0.000007  \n",
      "Epoch: [1][25600/36908] Elapsed 355m 49s (remain 157m 9s) Loss: 0.0013(0.0081) Grad: 23174.3496  LR: 0.000007  \n",
      "Epoch: [1][25700/36908] Elapsed 357m 13s (remain 155m 46s) Loss: 0.0037(0.0080) Grad: 138766.7344  LR: 0.000007  \n",
      "Epoch: [1][25800/36908] Elapsed 358m 36s (remain 154m 22s) Loss: 0.0042(0.0080) Grad: 37272.1641  LR: 0.000007  \n",
      "Epoch: [1][25900/36908] Elapsed 359m 58s (remain 152m 58s) Loss: 0.0009(0.0080) Grad: 6185.3867  LR: 0.000007  \n",
      "Epoch: [1][26000/36908] Elapsed 361m 20s (remain 151m 34s) Loss: 0.0015(0.0080) Grad: 11635.4922  LR: 0.000007  \n",
      "Epoch: [1][26100/36908] Elapsed 362m 43s (remain 150m 11s) Loss: 0.0024(0.0080) Grad: 36466.0859  LR: 0.000007  \n",
      "Epoch: [1][26200/36908] Elapsed 364m 7s (remain 148m 47s) Loss: 0.0002(0.0079) Grad: 237.2373  LR: 0.000006  \n",
      "Epoch: [1][26300/36908] Elapsed 365m 30s (remain 147m 24s) Loss: 0.0006(0.0079) Grad: 18165.3496  LR: 0.000006  \n",
      "Epoch: [1][26400/36908] Elapsed 366m 52s (remain 146m 0s) Loss: 0.0000(0.0079) Grad: 211.1826  LR: 0.000006  \n",
      "Epoch: [1][26500/36908] Elapsed 368m 16s (remain 144m 37s) Loss: 0.0004(0.0079) Grad: 7187.1050  LR: 0.000006  \n",
      "Epoch: [1][26600/36908] Elapsed 369m 39s (remain 143m 13s) Loss: 0.0003(0.0079) Grad: 6235.0234  LR: 0.000006  \n",
      "Epoch: [1][26700/36908] Elapsed 371m 1s (remain 141m 50s) Loss: 0.0001(0.0078) Grad: 84.6756  LR: 0.000006  \n",
      "Epoch: [1][26800/36908] Elapsed 372m 24s (remain 140m 26s) Loss: 0.0001(0.0078) Grad: 71.4999  LR: 0.000006  \n",
      "Epoch: [1][26900/36908] Elapsed 373m 47s (remain 139m 2s) Loss: 0.0003(0.0078) Grad: 1968.2079  LR: 0.000006  \n",
      "Epoch: [1][27000/36908] Elapsed 375m 10s (remain 137m 39s) Loss: 0.0053(0.0078) Grad: 115943.4297  LR: 0.000006  \n",
      "Epoch: [1][27100/36908] Elapsed 376m 34s (remain 136m 16s) Loss: 0.0000(0.0078) Grad: 24.3947  LR: 0.000006  \n",
      "Epoch: [1][27200/36908] Elapsed 377m 58s (remain 134m 52s) Loss: 0.0013(0.0077) Grad: 5789.7383  LR: 0.000006  \n",
      "Epoch: [1][27300/36908] Elapsed 379m 20s (remain 133m 29s) Loss: 0.0039(0.0077) Grad: 100216.7422  LR: 0.000006  \n",
      "Epoch: [1][27400/36908] Elapsed 380m 43s (remain 132m 5s) Loss: 0.0019(0.0077) Grad: 28848.0234  LR: 0.000006  \n",
      "Epoch: [1][27500/36908] Elapsed 382m 7s (remain 130m 42s) Loss: 0.0104(0.0077) Grad: 83721.3125  LR: 0.000006  \n",
      "Epoch: [1][27600/36908] Elapsed 383m 30s (remain 129m 19s) Loss: 0.0003(0.0077) Grad: 688.3706  LR: 0.000006  \n",
      "Epoch: [1][27700/36908] Elapsed 384m 53s (remain 127m 55s) Loss: 0.0010(0.0076) Grad: 6771.3145  LR: 0.000006  \n",
      "Epoch: [1][27800/36908] Elapsed 386m 17s (remain 126m 32s) Loss: 0.0000(0.0076) Grad: 949.4008  LR: 0.000005  \n",
      "Epoch: [1][27900/36908] Elapsed 387m 39s (remain 125m 8s) Loss: 0.0080(0.0076) Grad: 14384.6279  LR: 0.000005  \n",
      "Epoch: [1][28000/36908] Elapsed 389m 3s (remain 123m 45s) Loss: 0.0002(0.0076) Grad: 1168.9764  LR: 0.000005  \n",
      "Epoch: [1][28100/36908] Elapsed 390m 27s (remain 122m 22s) Loss: 0.0055(0.0076) Grad: 378820.5938  LR: 0.000005  \n",
      "Epoch: [1][28200/36908] Elapsed 391m 51s (remain 120m 59s) Loss: 0.0045(0.0076) Grad: 55617.8242  LR: 0.000005  \n",
      "Epoch: [1][28300/36908] Elapsed 393m 16s (remain 119m 36s) Loss: 0.0032(0.0075) Grad: 38269.4922  LR: 0.000005  \n",
      "Epoch: [1][28400/36908] Elapsed 394m 39s (remain 118m 12s) Loss: 0.0036(0.0075) Grad: 17512.5488  LR: 0.000005  \n",
      "Epoch: [1][28500/36908] Elapsed 396m 4s (remain 116m 49s) Loss: 0.0030(0.0075) Grad: 16569.1074  LR: 0.000005  \n",
      "Epoch: [1][28600/36908] Elapsed 397m 27s (remain 115m 26s) Loss: 0.0004(0.0075) Grad: 3952.0464  LR: 0.000005  \n",
      "Epoch: [1][28700/36908] Elapsed 398m 50s (remain 114m 2s) Loss: 0.0028(0.0075) Grad: 37725.3320  LR: 0.000005  \n",
      "Epoch: [1][28800/36908] Elapsed 400m 12s (remain 112m 39s) Loss: 0.0015(0.0075) Grad: 6490.7886  LR: 0.000005  \n",
      "Epoch: [1][28900/36908] Elapsed 401m 35s (remain 111m 15s) Loss: 0.0001(0.0075) Grad: 38.3925  LR: 0.000005  \n",
      "Epoch: [1][29000/36908] Elapsed 402m 59s (remain 109m 52s) Loss: 0.0001(0.0074) Grad: 92.4511  LR: 0.000005  \n",
      "Epoch: [1][29100/36908] Elapsed 404m 22s (remain 108m 29s) Loss: 0.0003(0.0074) Grad: 4056.9690  LR: 0.000005  \n",
      "Epoch: [1][29200/36908] Elapsed 405m 45s (remain 107m 5s) Loss: 0.0151(0.0074) Grad: 155080.4844  LR: 0.000005  \n",
      "Epoch: [1][29300/36908] Elapsed 407m 8s (remain 105m 42s) Loss: 0.0005(0.0074) Grad: 9526.8535  LR: 0.000005  \n",
      "Epoch: [1][29400/36908] Elapsed 408m 32s (remain 104m 18s) Loss: 0.0001(0.0074) Grad: 54.9549  LR: 0.000005  \n",
      "Epoch: [1][29500/36908] Elapsed 409m 55s (remain 102m 55s) Loss: 0.0038(0.0073) Grad: 84847.1797  LR: 0.000004  \n",
      "Epoch: [1][29600/36908] Elapsed 411m 18s (remain 101m 31s) Loss: 0.0250(0.0073) Grad: 1169579.6250  LR: 0.000004  \n",
      "Epoch: [1][29700/36908] Elapsed 412m 40s (remain 100m 8s) Loss: 0.0001(0.0073) Grad: 1688.9698  LR: 0.000004  \n",
      "Epoch: [1][29800/36908] Elapsed 414m 4s (remain 98m 44s) Loss: 0.0044(0.0073) Grad: 148879.3906  LR: 0.000004  \n",
      "Epoch: [1][29900/36908] Elapsed 415m 27s (remain 97m 21s) Loss: 0.0013(0.0073) Grad: 10258.3652  LR: 0.000004  \n",
      "Epoch: [1][30000/36908] Elapsed 416m 50s (remain 95m 58s) Loss: 0.0033(0.0073) Grad: 50582.0859  LR: 0.000004  \n",
      "Epoch: [1][30100/36908] Elapsed 418m 13s (remain 94m 34s) Loss: 0.0030(0.0072) Grad: 27547.6426  LR: 0.000004  \n",
      "Epoch: [1][30200/36908] Elapsed 419m 36s (remain 93m 11s) Loss: 0.0113(0.0072) Grad: 489057.7812  LR: 0.000004  \n",
      "Epoch: [1][30300/36908] Elapsed 421m 0s (remain 91m 47s) Loss: 0.0018(0.0072) Grad: 63767.1289  LR: 0.000004  \n",
      "Epoch: [1][30400/36908] Elapsed 422m 23s (remain 90m 24s) Loss: 0.0009(0.0072) Grad: 2275.9260  LR: 0.000004  \n",
      "Epoch: [1][30500/36908] Elapsed 423m 48s (remain 89m 1s) Loss: 0.0002(0.0072) Grad: 2849.2271  LR: 0.000004  \n",
      "Epoch: [1][30600/36908] Elapsed 425m 11s (remain 87m 37s) Loss: 0.0001(0.0072) Grad: 801.7211  LR: 0.000004  \n",
      "Epoch: [1][30700/36908] Elapsed 426m 34s (remain 86m 14s) Loss: 0.0019(0.0072) Grad: 27066.1172  LR: 0.000004  \n",
      "Epoch: [1][30800/36908] Elapsed 427m 57s (remain 84m 51s) Loss: 0.0000(0.0071) Grad: 102.3423  LR: 0.000004  \n",
      "Epoch: [1][30900/36908] Elapsed 429m 20s (remain 83m 27s) Loss: 0.0004(0.0071) Grad: 1009.7349  LR: 0.000004  \n",
      "Epoch: [1][31000/36908] Elapsed 430m 44s (remain 82m 4s) Loss: 0.0013(0.0071) Grad: 65591.5938  LR: 0.000004  \n",
      "Epoch: [1][31100/36908] Elapsed 432m 8s (remain 80m 41s) Loss: 0.0009(0.0071) Grad: 2700.6147  LR: 0.000003  \n",
      "Epoch: [1][31200/36908] Elapsed 433m 31s (remain 79m 17s) Loss: 0.0039(0.0071) Grad: 243879.5156  LR: 0.000003  \n",
      "Epoch: [1][31300/36908] Elapsed 434m 55s (remain 77m 54s) Loss: 0.0001(0.0071) Grad: 11.9875  LR: 0.000003  \n",
      "Epoch: [1][31400/36908] Elapsed 436m 18s (remain 76m 31s) Loss: 0.0009(0.0070) Grad: 2745.3123  LR: 0.000003  \n",
      "Epoch: [1][31500/36908] Elapsed 437m 41s (remain 75m 7s) Loss: 0.0004(0.0070) Grad: 10355.3203  LR: 0.000003  \n",
      "Epoch: [1][31600/36908] Elapsed 439m 4s (remain 73m 44s) Loss: 0.0103(0.0070) Grad: 112107.4766  LR: 0.000003  \n",
      "Epoch: [1][31700/36908] Elapsed 440m 29s (remain 72m 21s) Loss: 0.0005(0.0070) Grad: 557.6013  LR: 0.000003  \n",
      "Epoch: [1][31800/36908] Elapsed 441m 52s (remain 70m 57s) Loss: 0.0019(0.0070) Grad: 23136.0273  LR: 0.000003  \n",
      "Epoch: [1][31900/36908] Elapsed 443m 17s (remain 69m 34s) Loss: 0.0011(0.0070) Grad: 1984.5763  LR: 0.000003  \n",
      "Epoch: [1][32000/36908] Elapsed 444m 40s (remain 68m 11s) Loss: 0.0000(0.0070) Grad: 37.0786  LR: 0.000003  \n",
      "Epoch: [1][32100/36908] Elapsed 446m 3s (remain 66m 47s) Loss: 0.0003(0.0069) Grad: 1131.1060  LR: 0.000003  \n",
      "Epoch: [1][32200/36908] Elapsed 447m 27s (remain 65m 24s) Loss: 0.0000(0.0069) Grad: 247.0660  LR: 0.000003  \n",
      "Epoch: [1][32300/36908] Elapsed 448m 50s (remain 64m 0s) Loss: 0.0040(0.0069) Grad: 55773.5547  LR: 0.000003  \n",
      "Epoch: [1][32400/36908] Elapsed 450m 14s (remain 62m 37s) Loss: 0.0046(0.0069) Grad: 41490.8125  LR: 0.000003  \n",
      "Epoch: [1][32500/36908] Elapsed 451m 38s (remain 61m 14s) Loss: 0.0041(0.0069) Grad: 23087.6211  LR: 0.000003  \n",
      "Epoch: [1][32600/36908] Elapsed 453m 0s (remain 59m 50s) Loss: 0.0059(0.0069) Grad: 114630.2891  LR: 0.000003  \n",
      "Epoch: [1][32700/36908] Elapsed 454m 24s (remain 58m 27s) Loss: 0.0090(0.0069) Grad: 126349.5156  LR: 0.000003  \n",
      "Epoch: [1][32800/36908] Elapsed 455m 47s (remain 57m 4s) Loss: 0.0022(0.0069) Grad: 15678.2119  LR: 0.000002  \n",
      "Epoch: [1][32900/36908] Elapsed 457m 10s (remain 55m 40s) Loss: 0.0039(0.0068) Grad: 31150.4629  LR: 0.000002  \n",
      "Epoch: [1][33000/36908] Elapsed 458m 34s (remain 54m 17s) Loss: 0.0058(0.0068) Grad: 49012.4844  LR: 0.000002  \n",
      "Epoch: [1][33100/36908] Elapsed 459m 58s (remain 52m 54s) Loss: 0.0000(0.0068) Grad: 168.7983  LR: 0.000002  \n",
      "Epoch: [1][33200/36908] Elapsed 461m 21s (remain 51m 30s) Loss: 0.0001(0.0068) Grad: 548.9786  LR: 0.000002  \n",
      "Epoch: [1][33300/36908] Elapsed 462m 45s (remain 50m 7s) Loss: 0.0008(0.0068) Grad: 17722.4727  LR: 0.000002  \n",
      "Epoch: [1][33400/36908] Elapsed 464m 9s (remain 48m 44s) Loss: 0.0001(0.0068) Grad: 1174.4489  LR: 0.000002  \n",
      "Epoch: [1][33500/36908] Elapsed 465m 31s (remain 47m 20s) Loss: 0.0015(0.0068) Grad: 12840.2354  LR: 0.000002  \n",
      "Epoch: [1][33600/36908] Elapsed 466m 55s (remain 45m 57s) Loss: 0.0001(0.0067) Grad: 31.5581  LR: 0.000002  \n",
      "Epoch: [1][33700/36908] Elapsed 468m 18s (remain 44m 33s) Loss: 0.0032(0.0067) Grad: 24640.4473  LR: 0.000002  \n",
      "Epoch: [1][33800/36908] Elapsed 469m 41s (remain 43m 10s) Loss: 0.0021(0.0067) Grad: 34667.8164  LR: 0.000002  \n",
      "Epoch: [1][33900/36908] Elapsed 471m 6s (remain 41m 47s) Loss: 0.0008(0.0067) Grad: 6446.3936  LR: 0.000002  \n",
      "Epoch: [1][34000/36908] Elapsed 472m 29s (remain 40m 23s) Loss: 0.0001(0.0067) Grad: 78.5167  LR: 0.000002  \n",
      "Epoch: [1][34100/36908] Elapsed 473m 52s (remain 39m 0s) Loss: 0.0001(0.0067) Grad: 101.9617  LR: 0.000002  \n",
      "Epoch: [1][34200/36908] Elapsed 475m 17s (remain 37m 37s) Loss: 0.0025(0.0067) Grad: 17597.8477  LR: 0.000002  \n",
      "Epoch: [1][34300/36908] Elapsed 476m 41s (remain 36m 13s) Loss: 0.0004(0.0067) Grad: 180.3852  LR: 0.000002  \n",
      "Epoch: [1][34400/36908] Elapsed 478m 3s (remain 34m 50s) Loss: 0.0061(0.0067) Grad: 187008.5469  LR: 0.000002  \n",
      "Epoch: [1][34500/36908] Elapsed 479m 27s (remain 33m 27s) Loss: 0.0001(0.0066) Grad: 99.6913  LR: 0.000001  \n",
      "Epoch: [1][34600/36908] Elapsed 480m 50s (remain 32m 3s) Loss: 0.0026(0.0066) Grad: 29118.3965  LR: 0.000001  \n",
      "Epoch: [1][34700/36908] Elapsed 482m 13s (remain 30m 40s) Loss: 0.0001(0.0066) Grad: 100.2278  LR: 0.000001  \n",
      "Epoch: [1][34800/36908] Elapsed 483m 36s (remain 29m 16s) Loss: 0.0111(0.0066) Grad: 114894.6953  LR: 0.000001  \n",
      "Epoch: [1][34900/36908] Elapsed 485m 1s (remain 27m 53s) Loss: 0.0033(0.0066) Grad: 78426.1016  LR: 0.000001  \n",
      "Epoch: [1][35000/36908] Elapsed 486m 24s (remain 26m 30s) Loss: 0.0010(0.0066) Grad: 2200.1707  LR: 0.000001  \n",
      "Epoch: [1][35100/36908] Elapsed 487m 47s (remain 25m 6s) Loss: 0.0014(0.0066) Grad: 4299.2339  LR: 0.000001  \n",
      "Epoch: [1][35200/36908] Elapsed 489m 10s (remain 23m 43s) Loss: 0.0034(0.0066) Grad: 71764.8594  LR: 0.000001  \n",
      "Epoch: [1][35300/36908] Elapsed 490m 33s (remain 22m 19s) Loss: 0.0005(0.0065) Grad: 3027.5088  LR: 0.000001  \n",
      "Epoch: [1][35400/36908] Elapsed 491m 57s (remain 20m 56s) Loss: 0.0008(0.0065) Grad: 19722.3535  LR: 0.000001  \n",
      "Epoch: [1][35500/36908] Elapsed 493m 20s (remain 19m 33s) Loss: 0.0001(0.0065) Grad: 87.6646  LR: 0.000001  \n",
      "Epoch: [1][35600/36908] Elapsed 494m 43s (remain 18m 9s) Loss: 0.0001(0.0065) Grad: 445.6416  LR: 0.000001  \n",
      "Epoch: [1][35700/36908] Elapsed 496m 7s (remain 16m 46s) Loss: 0.0080(0.0065) Grad: 148630.0625  LR: 0.000001  \n",
      "Epoch: [1][35800/36908] Elapsed 497m 30s (remain 15m 22s) Loss: 0.0031(0.0065) Grad: 13893.1094  LR: 0.000001  \n",
      "Epoch: [1][35900/36908] Elapsed 498m 53s (remain 13m 59s) Loss: 0.0053(0.0065) Grad: 73350.9688  LR: 0.000001  \n",
      "Epoch: [1][36000/36908] Elapsed 500m 17s (remain 12m 36s) Loss: 0.0014(0.0065) Grad: 34074.5938  LR: 0.000001  \n",
      "Epoch: [1][36100/36908] Elapsed 501m 41s (remain 11m 12s) Loss: 0.0067(0.0065) Grad: 34051.0859  LR: 0.000000  \n",
      "Epoch: [1][36200/36908] Elapsed 503m 4s (remain 9m 49s) Loss: 0.0062(0.0064) Grad: 58943.5469  LR: 0.000000  \n",
      "Epoch: [1][36300/36908] Elapsed 504m 27s (remain 8m 26s) Loss: 0.0004(0.0064) Grad: 258.1983  LR: 0.000000  \n",
      "Epoch: [1][36400/36908] Elapsed 505m 49s (remain 7m 2s) Loss: 0.0009(0.0064) Grad: 8946.3867  LR: 0.000000  \n",
      "Epoch: [1][36500/36908] Elapsed 507m 13s (remain 5m 39s) Loss: 0.0001(0.0064) Grad: 15.4052  LR: 0.000000  \n",
      "Epoch: [1][36600/36908] Elapsed 508m 37s (remain 4m 15s) Loss: 0.0023(0.0064) Grad: 126966.2031  LR: 0.000000  \n",
      "Epoch: [1][36700/36908] Elapsed 510m 0s (remain 2m 52s) Loss: 0.0024(0.0064) Grad: 52946.3633  LR: 0.000000  \n",
      "Epoch: [1][36800/36908] Elapsed 511m 22s (remain 1m 29s) Loss: 0.0036(0.0064) Grad: 25483.6074  LR: 0.000000  \n",
      "Epoch: [1][36900/36908] Elapsed 512m 45s (remain 0m 5s) Loss: 0.0001(0.0064) Grad: 210.3655  LR: 0.000000  \n",
      "Epoch: [1][36907/36908] Elapsed 512m 51s (remain 0m 0s) Loss: 0.0002(0.0064) Grad: 1082.8247  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 12m 7s) Loss: 0.0002(0.0002) \n",
      "EVAL: [100/1192] Elapsed 0m 30s (remain 5m 27s) Loss: 0.0509(0.0054) \n",
      "EVAL: [200/1192] Elapsed 0m 59s (remain 4m 52s) Loss: 0.0052(0.0056) \n",
      "EVAL: [300/1192] Elapsed 1m 28s (remain 4m 21s) Loss: 0.0075(0.0061) \n",
      "EVAL: [400/1192] Elapsed 1m 57s (remain 3m 52s) Loss: 0.0000(0.0058) \n",
      "EVAL: [500/1192] Elapsed 2m 27s (remain 3m 23s) Loss: 0.0703(0.0058) \n",
      "EVAL: [600/1192] Elapsed 2m 56s (remain 2m 54s) Loss: 0.0096(0.0060) \n",
      "EVAL: [700/1192] Elapsed 3m 26s (remain 2m 24s) Loss: 0.0055(0.0068) \n",
      "EVAL: [800/1192] Elapsed 3m 56s (remain 1m 55s) Loss: 0.0166(0.0069) \n",
      "EVAL: [900/1192] Elapsed 4m 25s (remain 1m 25s) Loss: 0.0096(0.0071) \n",
      "EVAL: [1000/1192] Elapsed 4m 54s (remain 0m 56s) Loss: 0.0000(0.0069) \n",
      "EVAL: [1100/1192] Elapsed 5m 25s (remain 0m 26s) Loss: 0.0230(0.0068) \n",
      "EVAL: [1191/1192] Elapsed 5m 51s (remain 0m 0s) Loss: 0.0000(0.0067) \n",
      "Epoch 1 - avg_train_loss: 0.0064  avg_val_loss: 0.0067  time: 31127s\n",
      "Epoch 1 - Score: 0.8980\n",
      "Epoch 1 - Save Best Score: 0.8980 Model\n",
      "best_thres: 0.52  score: 0.89130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n",
      "load weights from ../output/nbme-score-clinical-patient-notes/nbme-exp070/fold0_best.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9356685f6e6f4aa9b1ff99253bcf06dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x7fdb6d9288c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 132, in __del__\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n",
      "load weights from ../output/nbme-score-clinical-patient-notes/nbme-exp070/fold1_best.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf3639816da45d3b3fe58d336629667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x7fdb6d9288c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 132, in __del__\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n",
      "load weights from ../output/nbme-score-clinical-patient-notes/nbme-exp070/fold2_best.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a6bf862d6c48a59137dde940a15a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n",
      "load weights from ../output/nbme-score-clinical-patient-notes/nbme-exp070/fold3_best.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d8f06652ee42c68636f7ebaf63e253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "name": "nbme-exp068.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 641.572862,
   "end_time": "2022-02-27T11:39:50.972497",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-27T11:29:09.399635",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02633c7de1ea4b7ca2fd899ae0c6d209": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "032d2a594c45472e9681d2d7557ab93d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "09da6c904a3344ad9d7d0f17c646c8b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_66daca2dd30d44efbcd88adcbb1c2725",
      "placeholder": "​",
      "style": "IPY_MODEL_02633c7de1ea4b7ca2fd899ae0c6d209",
      "value": " 446k/446k [00:00&lt;00:00, 4.84MB/s]"
     }
    },
    "0c748174ece64ca6992b434a2d92e1e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_827b5e8189d242c9b62bb8d6a084de72",
       "IPY_MODEL_0f910e441d334b10bc666e6ef47de941",
       "IPY_MODEL_09da6c904a3344ad9d7d0f17c646c8b1"
      ],
      "layout": "IPY_MODEL_d6ba1f9a002c49268799fc4a17f793ee"
     }
    },
    "0f0864ef340b49aaa9b4596e032163a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0f910e441d334b10bc666e6ef47de941": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_760cf6427cdc4b05affb72378d92a0f3",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9a660863781b487392504ab3302a5f93",
      "value": 456318
     }
    },
    "13133c559d1b4a14ba19c157c169b532": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13873604cb644df0b4b5e8e9ea57d645": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c619fc144a44ae8ba6d091c236a20f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c64feffa14d4a56b3bf4c25f6d05c51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1fc34ac85840429e8d5d62785817dd03",
      "placeholder": "​",
      "style": "IPY_MODEL_c7dbd264bb804aabbb9a3a3922cdcc00",
      "value": " 52.0/52.0 [00:00&lt;00:00, 2.12kB/s]"
     }
    },
    "1fc34ac85840429e8d5d62785817dd03": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "263650ffbf3145048f331827b49ef11b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_886eee1754fd42e185a7548aa44871f1",
      "placeholder": "​",
      "style": "IPY_MODEL_ec232b9c334c4987b35fde837fac77da",
      "value": " 878k/878k [00:00&lt;00:00, 3.84MB/s]"
     }
    },
    "2c0468cceac144b890d30b510202deba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2df59efc32cf4642a4cfff7db709db5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb191fc8e771447ab389bbb57fe22d2f",
      "max": 143,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ced0845b868342c29d4e2d830a48f203",
      "value": 143
     }
    },
    "323239c2e16449c088afd6eb5eeaa73f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b5f8f50bdaf843f1a938f6129848cd83",
      "max": 898825,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0f0864ef340b49aaa9b4596e032163a7",
      "value": 898825
     }
    },
    "323befd254f741a7b041d124d4073817": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "33bbf6ff0a9847fc9900a16ea8247120": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_61d290b99d78422494bd99ba7f4ce0b4",
      "max": 52,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_511fe7d1f0db48f5848e3539ddb29fff",
      "value": 52
     }
    },
    "3d505cf1fb134928953da8d79d68665a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c276b71af2f4301b4048e4f8dad36d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_563519b534874ec8a145d292258b2dad",
       "IPY_MODEL_535a8156a6da43928fd0d7a33c624540",
       "IPY_MODEL_ef3dd8c9fb4a46bfa6c30489d2d75b02"
      ],
      "layout": "IPY_MODEL_5d985ee5c7d84bf9a135972d46830ad0"
     }
    },
    "511fe7d1f0db48f5848e3539ddb29fff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "530069c0165e4b3d8e077e9c6a4a1d21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c0468cceac144b890d30b510202deba",
      "placeholder": "​",
      "style": "IPY_MODEL_b4c22ff3f7064b5c82b501058fa367a6",
      "value": "Downloading: 100%"
     }
    },
    "535a8156a6da43928fd0d7a33c624540": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8fa83fd7255f43ff9126aa633ed1b663",
      "max": 42146,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9febd109d3a24bef886c8d24808347ba",
      "value": 42146
     }
    },
    "5504bc5bee8c4c189614fd398d1b7fef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "563519b534874ec8a145d292258b2dad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5bc25dd928614ba999cefcde5efa9197",
      "placeholder": "​",
      "style": "IPY_MODEL_5ccb74cb082845d7ade9962f741a2910",
      "value": "100%"
     }
    },
    "5a47b531ee814b1eba201f0aa79212db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5bb3e7fc97c64d59a20a7ebb29a39800": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5bc25dd928614ba999cefcde5efa9197": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ccb74cb082845d7ade9962f741a2910": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5d985ee5c7d84bf9a135972d46830ad0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "61d290b99d78422494bd99ba7f4ce0b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "638cf831bc0443658b4b455200f9b990": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f0a30d22d004fd6866556696346fb74",
      "max": 475,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5504bc5bee8c4c189614fd398d1b7fef",
      "value": 475
     }
    },
    "6639cae9d2844ea3b7d9a777b43b2181": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6e5be1d244794198afee6c8fefd3a191",
       "IPY_MODEL_2df59efc32cf4642a4cfff7db709db5a",
       "IPY_MODEL_72d201f9470a4d04b1e89f7d0018c0d7"
      ],
      "layout": "IPY_MODEL_cebcd629be2340bfa4ca1780d70dd364"
     }
    },
    "66a3685d05e1493c987e6cb1d9ed00a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "66daca2dd30d44efbcd88adcbb1c2725": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e5be1d244794198afee6c8fefd3a191": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da9bce59ac4845cda340d840b79be311",
      "placeholder": "​",
      "style": "IPY_MODEL_323befd254f741a7b041d124d4073817",
      "value": "100%"
     }
    },
    "72d201f9470a4d04b1e89f7d0018c0d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13873604cb644df0b4b5e8e9ea57d645",
      "placeholder": "​",
      "style": "IPY_MODEL_f129267b3ae6422e8d345c28b73f5516",
      "value": " 143/143 [00:00&lt;00:00, 2848.65it/s]"
     }
    },
    "760cf6427cdc4b05affb72378d92a0f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c1442db3949418184bfb68266910d91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7e01c08e3af148f580fcdc6ef6ebf5b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7ebb3f8b10154ef4badb2d6856140d18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "808441188c1a4b5788ae84fa3edf27db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d505cf1fb134928953da8d79d68665a",
      "placeholder": "​",
      "style": "IPY_MODEL_81714296902f4d26a32aa05da8890693",
      "value": "Downloading: 100%"
     }
    },
    "80be6bc8b1c8454187599fe6f05cdcba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "811a7f7f9bc34108b113d084a7d488f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81714296902f4d26a32aa05da8890693": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8254bfa8a6fa47dba57db5ba29dccaeb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "827b5e8189d242c9b62bb8d6a084de72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc036c5ffc0a4a0fb2d0f504cf4264f3",
      "placeholder": "​",
      "style": "IPY_MODEL_80be6bc8b1c8454187599fe6f05cdcba",
      "value": "Downloading: 100%"
     }
    },
    "886eee1754fd42e185a7548aa44871f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e1aca2f17c6476a855cbe11a1d661a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aa1104924af9458eb0d5443fd617cf29",
       "IPY_MODEL_b94dd392dc874911b1deeb39f77b342f",
       "IPY_MODEL_be8e2f5fc8eb4029a4aece3d1776054d"
      ],
      "layout": "IPY_MODEL_032d2a594c45472e9681d2d7557ab93d"
     }
    },
    "8f0a30d22d004fd6866556696346fb74": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8fa83fd7255f43ff9126aa633ed1b663": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9826121100004ec49f1cd7ed26023d9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cfc527e5b7e84c45af32ba7e49275790",
       "IPY_MODEL_33bbf6ff0a9847fc9900a16ea8247120",
       "IPY_MODEL_1c64feffa14d4a56b3bf4c25f6d05c51"
      ],
      "layout": "IPY_MODEL_b42a1221467e43648126dc9432be0b28"
     }
    },
    "9a660863781b487392504ab3302a5f93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9febd109d3a24bef886c8d24808347ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a2fc47910e5a4b158eddf7faa455d683": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_808441188c1a4b5788ae84fa3edf27db",
       "IPY_MODEL_638cf831bc0443658b4b455200f9b990",
       "IPY_MODEL_c5ad324c87914e26ad665efcc418f354"
      ],
      "layout": "IPY_MODEL_8254bfa8a6fa47dba57db5ba29dccaeb"
     }
    },
    "aa1104924af9458eb0d5443fd617cf29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd01049905654fe6bee6c4c602b89ed9",
      "placeholder": "​",
      "style": "IPY_MODEL_66a3685d05e1493c987e6cb1d9ed00a1",
      "value": "100%"
     }
    },
    "b42a1221467e43648126dc9432be0b28": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4c22ff3f7064b5c82b501058fa367a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b5f8f50bdaf843f1a938f6129848cd83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b94dd392dc874911b1deeb39f77b342f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13133c559d1b4a14ba19c157c169b532",
      "max": 42146,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7e01c08e3af148f580fcdc6ef6ebf5b4",
      "value": 42146
     }
    },
    "be8e2f5fc8eb4029a4aece3d1776054d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e464d1ba21a1489183fda5c01bbc0cca",
      "placeholder": "​",
      "style": "IPY_MODEL_5bb3e7fc97c64d59a20a7ebb29a39800",
      "value": " 42146/42146 [00:22&lt;00:00, 2038.25it/s]"
     }
    },
    "c5ad324c87914e26ad665efcc418f354": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a47b531ee814b1eba201f0aa79212db",
      "placeholder": "​",
      "style": "IPY_MODEL_7c1442db3949418184bfb68266910d91",
      "value": " 475/475 [00:00&lt;00:00, 18.8kB/s]"
     }
    },
    "c7dbd264bb804aabbb9a3a3922cdcc00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd01049905654fe6bee6c4c602b89ed9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cebcd629be2340bfa4ca1780d70dd364": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ced0845b868342c29d4e2d830a48f203": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cfc527e5b7e84c45af32ba7e49275790": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c619fc144a44ae8ba6d091c236a20f6",
      "placeholder": "​",
      "style": "IPY_MODEL_f3dcb10079874c84a85896aae581b1dc",
      "value": "Downloading: 100%"
     }
    },
    "d6ba1f9a002c49268799fc4a17f793ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da9bce59ac4845cda340d840b79be311": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc036c5ffc0a4a0fb2d0f504cf4264f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e464d1ba21a1489183fda5c01bbc0cca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb191fc8e771447ab389bbb57fe22d2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec232b9c334c4987b35fde837fac77da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ecfdf18519e34e2d9a0b112b0815cba5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ef3dd8c9fb4a46bfa6c30489d2d75b02": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_811a7f7f9bc34108b113d084a7d488f4",
      "placeholder": "​",
      "style": "IPY_MODEL_ecfdf18519e34e2d9a0b112b0815cba5",
      "value": " 42146/42146 [00:00&lt;00:00, 687236.90it/s]"
     }
    },
    "f129267b3ae6422e8d345c28b73f5516": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f3dcb10079874c84a85896aae581b1dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f61172e130474199999fe10c8470b1e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_530069c0165e4b3d8e077e9c6a4a1d21",
       "IPY_MODEL_323239c2e16449c088afd6eb5eeaa73f",
       "IPY_MODEL_263650ffbf3145048f331827b49ef11b"
      ],
      "layout": "IPY_MODEL_7ebb3f8b10154ef4badb2d6856140d18"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
