{"cells":[{"cell_type":"markdown","id":"incredible-principle","metadata":{"id":"incredible-principle"},"source":["## References"]},{"cell_type":"markdown","id":"simplified-tract","metadata":{"id":"simplified-tract"},"source":["- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"]},{"cell_type":"markdown","id":"boolean-shame","metadata":{"id":"boolean-shame"},"source":["## Configurations"]},{"cell_type":"code","execution_count":1,"id":"needed-consistency","metadata":{"id":"needed-consistency","executionInfo":{"status":"ok","timestamp":1645614518536,"user_tz":-540,"elapsed":3,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["EXP_NAME = \"nbme-exp002\"\n","ENV = \"colab\"\n","DEBUG_MODE = False\n","SUBMISSION_MODE = False"]},{"cell_type":"code","execution_count":2,"id":"operational-trader","metadata":{"id":"operational-trader","executionInfo":{"status":"ok","timestamp":1645614518536,"user_tz":-540,"elapsed":3,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class CFG:\n","    env=ENV\n","    exp_name=EXP_NAME\n","    debug=DEBUG_MODE\n","    submission=SUBMISSION_MODE\n","    apex=True\n","    input_dir=None\n","    output_dir=None\n","    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n","    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n","    competition_name=\"nbme-score-clinical-patient-notes\"\n","    id_col=\"id\"\n","    target_col=\"location\"\n","    pretrained_model_name=\"microsoft/deberta-base\"\n","    tokenizer=None\n","    max_len=None\n","    output_dim=1\n","    dropout=0.2\n","    num_workers=4\n","    batch_size=8\n","    lr=2e-5\n","    betas=(0.9, 0.98)\n","    weight_decay=0.1\n","    num_warmup_steps_rate=0.1\n","    batch_scheduler=True\n","    epochs=5\n","    n_fold=5\n","    train_fold=[0, 1, 2, 3, 4]\n","    seed=71\n","    gradient_accumulation_steps=1\n","    max_grad_norm=1000\n","    print_freq=100\n","    train=True\n","    inference=True"]},{"cell_type":"code","execution_count":3,"id":"seasonal-consistency","metadata":{"id":"seasonal-consistency","executionInfo":{"status":"ok","timestamp":1645614518536,"user_tz":-540,"elapsed":3,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.train_fold = [0, 1]\n","\n","if CFG.submission:\n","    CFG.train = False\n","    CFG.inference = True"]},{"cell_type":"markdown","id":"billion-composite","metadata":{"id":"billion-composite"},"source":["## Directory Settings"]},{"cell_type":"code","execution_count":4,"id":"desperate-collect","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"desperate-collect","executionInfo":{"status":"ok","timestamp":1645614525321,"user_tz":-540,"elapsed":6788,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}},"outputId":"b7937ca9-5fe1-4423-b295-9f8737b8983e"},"outputs":[{"output_type":"stream","name":"stdout","text":["colab\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"]}],"source":["import sys\n","from pathlib import Path\n","\n","\n","print(CFG.env)\n","if CFG.env == \"colab\":\n","    # colab環境\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","    # install packages\n","    !pip install transformers\n","\n","elif CFG.env == \"local\":\n","    # ローカルサーバ\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","\n","elif CFG.env == \"kaggle\":\n","    # kaggle環境\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./\")"]},{"cell_type":"code","execution_count":5,"id":"acute-pregnancy","metadata":{"id":"acute-pregnancy","executionInfo":{"status":"ok","timestamp":1645614530411,"user_tz":-540,"elapsed":5096,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["import gc\n","import os\n","import ast\n","import time\n","import math\n","import random\n","import itertools\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","from scipy.optimize import minimize\n","from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as T\n","from torchvision.io import read_image\n","from torch.utils.data import DataLoader, Dataset\n","\n","from transformers import BartModel,BertModel,BertTokenizer\n","from transformers import DebertaModel,DebertaTokenizer\n","from transformers import RobertaModel,RobertaTokenizer\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","id":"generous-raleigh","metadata":{"id":"generous-raleigh"},"source":["## Utilities"]},{"cell_type":"code","execution_count":6,"id":"controlling-headset","metadata":{"id":"controlling-headset","executionInfo":{"status":"ok","timestamp":1645614530411,"user_tz":-540,"elapsed":5,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on binary arrays.\n","\n","    Args:\n","        preds (list of lists of ints): Predictions.\n","        truths (list of lists of ints): Ground truths.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    # Micro : aggregating over all instances\n","    preds = np.concatenate(preds)\n","    truths = np.concatenate(truths)\n","    return f1_score(truths, preds)\n","\n","\n","def spans_to_binary(spans, length=None):\n","    \"\"\"\n","    Converts spans to a binary array indicating whether each character is in the span.\n","\n","    Args:\n","        spans (list of lists of two ints): Spans.\n","\n","    Returns:\n","        np array [length]: Binarized spans.\n","    \"\"\"\n","    length = np.max(spans) if length is None else length\n","    binary = np.zeros(length)\n","    for start, end in spans:\n","        binary[start:end] = 1\n","    return binary\n","\n","\n","def span_micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on spans.\n","\n","    Args:\n","        preds (list of lists of two ints): Prediction spans.\n","        truths (list of lists of two ints): Ground truth spans.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    bin_preds = []\n","    bin_truths = []\n","    for pred, truth in zip(preds, truths):\n","        if not len(pred) and not len(truth):\n","            continue\n","        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n","        bin_preds.append(spans_to_binary(pred, length))\n","        bin_truths.append(spans_to_binary(truth, length))\n","    return micro_f1(bin_preds, bin_truths)\n","\n","\n","def get_score(y_true, y_pred):\n","    score = span_micro_f1(y_true, y_pred)\n","    return score"]},{"cell_type":"code","source":["def create_labels_for_scoring(df):\n","    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n","    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n","    for i in range(len(df)):\n","        lst = df.loc[i, \"location\"]\n","        if lst:\n","            new_lst = \";\".join(lst)\n","            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n","\n","    # create labels\n","    truths = []\n","    for location_list in df[\"location_for_create_labels\"].values:\n","        truth = []\n","        if len(location_list) > 0:\n","            location = location_list[0]\n","            for loc in [s.split() for s in location.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                truth.append([start, end])\n","        truths.append(truth)\n","\n","    return truths\n","\n","\n","def get_char_probs(texts, token_probs, tokenizer):\n","    res = [np.zeros(len(t)) for t in texts]\n","    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n","        encoded = tokenizer(\n","            text=text,\n","            max_length=CFG.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n","            start, end = offset_mapping\n","            res[i][start:end] = pred\n","    return res\n","\n","\n","def get_predicted_location_str(char_probs, th=0.5):\n","    results = []\n","    for char_prob in char_probs:\n","        result = np.where(char_prob >= th)[0] + 1\n","        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n","        result = [f\"{min(r)} {max(r)}\" for r in result]\n","        result = \";\".join(result)\n","        results.append(result)\n","    return results\n","\n","\n","def get_predictions(results):\n","    predictions = []\n","    for result in results:\n","        prediction = []\n","        if result != \"\":\n","            for loc in [s.split() for s in result.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                prediction.append([start, end])\n","        predictions.append(prediction)\n","    return predictions\n","\n","\n","def scoring(df, th=0.5):\n","    labels = create_labels_for_scoring(df)\n","\n","    token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n","    char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n","    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n","    preds = get_predictions(predicted_location_str)\n","\n","    score = get_score(labels, preds)\n","    return score\n","\n","\n","def get_best_thres(oof_df):\n","    def f1_opt(x):\n","        return -1 * scoring(oof_df, th=x)\n","\n","    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n","    return best_thres"],"metadata":{"id":"UOscbQSt4Cqo","executionInfo":{"status":"ok","timestamp":1645614530411,"user_tz":-540,"elapsed":4,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"id":"UOscbQSt4Cqo","execution_count":7,"outputs":[]},{"cell_type":"code","source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return \"%dm %ds\" % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"],"metadata":{"id":"mmZHVaPkh0Qc","executionInfo":{"status":"ok","timestamp":1645614530411,"user_tz":-540,"elapsed":4,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"id":"mmZHVaPkh0Qc","execution_count":8,"outputs":[]},{"cell_type":"code","source":["seed_everything()"],"metadata":{"id":"IydDnpFyh4PX","executionInfo":{"status":"ok","timestamp":1645614530412,"user_tz":-540,"elapsed":5,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"id":"IydDnpFyh4PX","execution_count":9,"outputs":[]},{"cell_type":"markdown","id":"formed-handbook","metadata":{"id":"formed-handbook"},"source":["## Data Loading"]},{"cell_type":"code","execution_count":10,"id":"vanilla-register","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vanilla-register","executionInfo":{"status":"ok","timestamp":1645614531212,"user_tz":-540,"elapsed":805,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}},"outputId":"8b7600fa-a8c4-4eb7-f240-db8916b9bf7b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14300, 6), (143, 3), (42146, 3), (5, 4))"]},"metadata":{},"execution_count":10}],"source":["train = pd.read_csv(CFG.input_dir / \"train.csv\")\n","features = pd.read_csv(CFG.input_dir / \"features.csv\")\n","patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n","test = pd.read_csv(CFG.input_dir / \"test.csv\")\n","\n","train.shape, features.shape, patient_notes.shape, test.shape"]},{"cell_type":"code","execution_count":11,"id":"approximate-transmission","metadata":{"id":"approximate-transmission","executionInfo":{"status":"ok","timestamp":1645614531213,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["if CFG.debug:\n","    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n","    print(train.shape)"]},{"cell_type":"markdown","id":"civic-advisory","metadata":{"id":"civic-advisory"},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":12,"id":"irish-nature","metadata":{"id":"irish-nature","executionInfo":{"status":"ok","timestamp":1645614531213,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def preprocess_features(features):\n","    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n","    return features\n","\n","\n","features = preprocess_features(features)"]},{"cell_type":"code","execution_count":13,"id":"parliamentary-stupid","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"parliamentary-stupid","executionInfo":{"status":"ok","timestamp":1645614531213,"user_tz":-540,"elapsed":8,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}},"outputId":"e8d8f977-b2c5-42d4-9d08-a3ed4f34f423"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14300, 8), (5, 6))"]},"metadata":{},"execution_count":13}],"source":["train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","\n","train.shape, test.shape"]},{"cell_type":"code","execution_count":14,"id":"vietnamese-spare","metadata":{"id":"vietnamese-spare","executionInfo":{"status":"ok","timestamp":1645614531213,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def fix_anno(df, target_id, annotation, location):\n","    idx = df[\"id\"] == target_id\n","    df.loc[idx, \"annotation\"] = annotation\n","    df.loc[idx, \"location\"] = location"]},{"cell_type":"code","execution_count":15,"id":"tutorial-soldier","metadata":{"id":"tutorial-soldier","executionInfo":{"status":"ok","timestamp":1645614531214,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["fix_anno(train, \"00669_000\", \"['father heart attack']\", \"['764 783']\")\n","fix_anno(train, \"01110_010\", \"['for the last 2-3 months', 'over the last 2 months']\", \"['77 100', '398 420']\")\n","fix_anno(train, \"01146_005\", \"['no heat intolerance', 'no cold intolerance']\", \"['285 292;301 312', '285 287;296 312']\")\n","fix_anno(train, \"02428_001\", \"['mother thyroid problem']\", \"['551 557;565 580']\")\n","fix_anno(train, \"02428_004\", '[\\'felt like he was going to \"pass out\"\\']', \"['131 135;181 212']\")\n","fix_anno(train, \"10047_105\", \"['stool , with no blood']\", \"['259 280']\")\n","fix_anno(train, \"10196_105\", \"['diarrhoe non blooody']\", \"['176 184;201 212']\")\n","fix_anno(train, \"10206_103\", \"['diarrhea for last 2-3 days']\", \"['249 257;271 288']\")\n","fix_anno(train, \"10228_100\", \"['no vaginal discharge']\", \"['822 824;907 924']\")\n","fix_anno(train, \"10268_111\", \"['started about 8-10 hours ago']\", \"['101 129']\")\n","fix_anno(train, \"10459_105\", \"['no blood in the stool']\", \"['531 539;549 561']\")\n","fix_anno(train, \"10620_102\", \"['last sexually active 9 months ago']\", \"['540 560;581 593']\")\n","fix_anno(train, \"10646_107\", \"['right lower quadrant pain']\", \"['32 57']\")\n","fix_anno(train, \"10968_105\", \"['diarrhoea no blood']\", \"['308 317;376 384']\")\n","fix_anno(train, \"20747_214\", \"['sweating']\", \"['549 557']\")\n","fix_anno(\n","    train,\n","    \"21686_200\",\n","    \"['previously as regular', 'previously eveyr 28-29 days', 'previously lasting 5 days', 'previously regular flow']\",\n","    \"['102 123', '102 112;125 141', '102 112;143 157', '102 112;159 171']\",\n",")\n","fix_anno(train, \"30437_309\", \"['for 2 months']\", \"['33 45']\")\n","fix_anno(train, \"32657_315\", \"['35 year old']\", \"['5 16']\")\n","fix_anno(train, \"32996_302\", \"['darker brown stools']\", \"['175 194']\")\n","fix_anno(train, \"33531_300\", \"['uncle with peptic ulcer']\", \"['700 723']\")\n","fix_anno(train, \"40974_406\", \"['difficulty falling asleep']\", \"['225 250']\")\n","fix_anno(train, \"41825_402\", \"['helps to take care of aging mother and in-laws']\", \"['197 218;236 260']\")\n","fix_anno(\n","    train,\n","    \"42625_400\",\n","    \"['No hair changes', 'No skin changes', 'No GI changes', 'No palpitations', 'No excessive sweating']\",\n","    \"['480 482;507 519', '480 482;499 503;512 519', '480 482;521 531', '480 482;533 545', '480 482;564 582']\",\n",")\n","fix_anno(\n","    train,\n","    \"43451_402\",\n","    \"['stressed due to taking care of her mother', 'stressed due to taking care of husbands parents']\",\n","    \"['290 320;327 337', '290 320;342 358']\",\n",")\n","fix_anno(train, \"44958_402\", \"['stressor taking care of many sick family members']\", \"['288 296;324 363']\")\n","fix_anno(train, \"50574_514\", \"['heart started racing and felt numbness for the 1st time in her finger tips']\", \"['108 182']\")\n","fix_anno(train, \"52512_500\", \"['first started 5 yrs']\", \"['102 121']\")\n","fix_anno(train, \"60235_608\", \"['No shortness of breath']\", \"['481 483;533 552']\")\n","fix_anno(train, \"60469_603\", \"['recent URI', 'nasal stuffines, rhinorrhea, for 3-4 days']\", \"['92 102', '123 164']\")\n","fix_anno(\n","    train,\n","    \"70255_702\",\n","    \"['irregularity with her cycles', 'heavier bleeding', 'changes her pad every couple hours']\",\n","    \"['89 117', '122 138', '368 402']\",\n",")\n","fix_anno(train, \"70412_701\", \"['gaining 10-15 lbs']\", \"['344 361']\")\n","fix_anno(train, \"72660_701\", \"['weight gain', 'gain of 10-16lbs']\", \"['600 611', '607 623']\")\n","fix_anno(train, \"81856_813\", \"['seeing her son knows are not real']\", \"['386 400;443 461']\")\n","fix_anno(train, \"81985_813\", \"['saw him once in the kitchen after he died']\", \"['160 201']\")\n","fix_anno(train, \"83199_810\", \"['tried Ambien but it didnt work']\", \"['325 337;349 366']\")\n","fix_anno(train, \"83757_803\", \"['heard what she described as a party later than evening these things did not actually happen']\", \"['405 459;488 524']\")\n","fix_anno(train, \"83757_813\", \"['experienced seeing her son at the kitchen table these things did not actually happen']\", \"['353 400;488 524']\")\n","fix_anno(train, \"92224_909\", \"['SCRACHY THROAT', 'RUNNY NOSE']\", \"['293 307', '321 331']\")\n","fix_anno(train, \"92385_900\", \"['without improvement when taking tylenol', 'without improvement when taking ibuprofen']\", \"['182 221', '182 213;225 234']\")\n","fix_anno(train, \"92385_902\", \"['yesterday', 'yesterday']\", \"['79 88', '409 418']\")\n","fix_anno(train, \"93988_904\", \"['headache global', 'headache throughout her head']\", \"['86 94;230 236', '86 94;237 256']\")\n","fix_anno(train, \"94656_904\", \"['headache generalized in her head']\", \"['56 64;156 179']\")"]},{"cell_type":"code","execution_count":16,"id":"imposed-encyclopedia","metadata":{"id":"imposed-encyclopedia","executionInfo":{"status":"ok","timestamp":1645614531214,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n","train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"]},{"cell_type":"code","execution_count":17,"id":"collected-princeton","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":191},"id":"collected-princeton","executionInfo":{"status":"ok","timestamp":1645614531214,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}},"outputId":"c6e9ceac-112b-4337-bf29-6bcca3fedd09"},"outputs":[{"output_type":"display_data","data":{"text/plain":["0    4399\n","1    8184\n","2    1293\n","3     287\n","4      99\n","5      27\n","6       9\n","7       1\n","8       1\n","Name: annotation_length, dtype: int64"]},"metadata":{}}],"source":["train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n","display(train['annotation_length'].value_counts().sort_index())"]},{"cell_type":"markdown","id":"downtown-frame","metadata":{"id":"downtown-frame"},"source":["## CV split"]},{"cell_type":"code","execution_count":18,"id":"detailed-drive","metadata":{"id":"detailed-drive","executionInfo":{"status":"ok","timestamp":1645614531214,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def get_groupkfold(df, group_name):\n","    groups = df[group_name].unique()\n","\n","    kf = KFold(\n","        n_splits=CFG.n_fold,\n","        shuffle=True,\n","        random_state=CFG.seed,\n","    )\n","    folds_ids = []\n","    for i_fold, (_, val_group_idx) in enumerate(kf.split(groups)):\n","        val_group = groups[val_group_idx]\n","        is_val = df[group_name].isin(val_group)\n","        val_idx = df[is_val].index\n","        df.loc[val_idx, \"fold\"] = int(i_fold)\n","\n","    df[\"fold\"] = df[\"fold\"].astype(int)\n","    return df"]},{"cell_type":"code","execution_count":19,"id":"other-satisfaction","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139},"id":"other-satisfaction","executionInfo":{"status":"ok","timestamp":1645614531214,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}},"outputId":"b7af118e-6961-4fcd-aef0-2995b602730a"},"outputs":[{"output_type":"display_data","data":{"text/plain":["fold\n","0    2902\n","1    2894\n","2    2813\n","3    2791\n","4    2900\n","dtype: int64"]},"metadata":{}}],"source":["train = get_groupkfold(train, \"pn_num\")\n","display(train.groupby(\"fold\").size())"]},{"cell_type":"markdown","id":"senior-wichita","metadata":{"id":"senior-wichita"},"source":["## Setup tokenizer"]},{"cell_type":"code","execution_count":20,"id":"thrown-theology","metadata":{"id":"thrown-theology","executionInfo":{"status":"ok","timestamp":1645614540268,"user_tz":-540,"elapsed":9060,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["if CFG.submission:\n","    tokenizer = AutoTokenizer.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n","else:\n","    tokenizer = AutoTokenizer.from_pretrained(CFG.pretrained_model_name)\n","    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n","\n","CFG.tokenizer = tokenizer"]},{"cell_type":"markdown","id":"varying-tourism","metadata":{"id":"varying-tourism"},"source":["## Create dataset"]},{"cell_type":"code","execution_count":21,"id":"white-integral","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["1bf8493b04954d95adaa9556b1c26ec5","02013be4b6c54ad5b15b3c42dfb63f26","3012c34221dd47bcbeef7182d37cb9a9","9db50339f80b4d5dbb02dacfa54cc098","e5cfce60f4dc4892a127e0893df4cd77","9e28131aa525418cb798eade8db9d198","e55b4dff79574f2990d420a59440ae60","097ea871087d4ac08e85a97cb97911bf","d0b32231f103460ebb8418986c8bae2d","8d05be7c219c4564b98aa8d044be471e","b7b67a16aedc4824b779facb0a58a2ce"]},"id":"white-integral","executionInfo":{"status":"ok","timestamp":1645614573085,"user_tz":-540,"elapsed":32838,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}},"outputId":"ba6ffea1-c99a-41c8-fcb8-65b8ae3b9afa"},"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1bf8493b04954d95adaa9556b1c26ec5","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/42146 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 433\n"]}],"source":["pn_history_lengths = []\n","tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    pn_history_lengths.append(length)\n","\n","print(\"max length:\", np.max(pn_history_lengths))"]},{"cell_type":"code","execution_count":22,"id":"demonstrated-version","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["3e9a4a45fec04036a7e4150b3e9bd057","f6d595e260f24414938d6547bba5fb81","35d081ebff274bcc8e2a8759a1a22991","7e7e11038d0d4b688c8f0e9405fe8eb7","3fa1fe37240c4b639dec024bf5ba7e8e","ff301805e5a840b9a5ecd86bd113c4f4","2a1d95e09ef44017b9c857f417921c07","b5c169f1d9574a71956e4c9ed93e3389","e5b75a0e3974447e8c4e4bf8bb1ef9ae","6389bd084c6c4c10b35255d3bbc78b32","50e67d9ac46a4238b110490c32d8895b"]},"id":"demonstrated-version","executionInfo":{"status":"ok","timestamp":1645614573483,"user_tz":-540,"elapsed":10,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}},"outputId":"6203abd7-9992-483d-bb9a-b413b5748382"},"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3e9a4a45fec04036a7e4150b3e9bd057","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/143 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 30\n"]}],"source":["feature_text_lengths = []\n","tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    feature_text_lengths.append(length)\n","\n","print(\"max length:\", np.max(feature_text_lengths))"]},{"cell_type":"code","execution_count":23,"id":"posted-miami","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"posted-miami","executionInfo":{"status":"ok","timestamp":1645614573483,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}},"outputId":"0af1d59f-a430-4734-b786-04660b5eb3b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["max length: 466\n"]}],"source":["CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n","\n","print(\"max length:\", CFG.max_len)"]},{"cell_type":"code","execution_count":24,"id":"fossil-supply","metadata":{"id":"fossil-supply","executionInfo":{"status":"ok","timestamp":1645614573483,"user_tz":-540,"elapsed":5,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class TrainingDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","        self.annotation_lengths = self.df[\"annotation_length\"].values\n","        self.locations = self.df[\"location\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def _create_label(self, pn_history, annotation_length, location_list):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        offset_mapping = encoded[\"offset_mapping\"]\n","        ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n","        label = np.zeros(len(offset_mapping))\n","        label[ignore_idxes] = -1\n","\n","        if annotation_length > 0:\n","            for location in location_list:\n","                for loc in [s.split() for s in location.split(\";\")]:\n","                    start, end = int(loc[0]), int(loc[1])\n","                    start_idx = -1\n","                    end_idx = -1\n","                    for idx in range(len(offset_mapping)):\n","                        if (start_idx == -1) & (start < offset_mapping[idx][0]):\n","                            start_idx = idx - 1\n","                        if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n","                            end_idx = idx + 1\n","                    if start_idx == -1:\n","                        start_idx = end_idx\n","                    if (start_idx != -1) & (end_idx != -1):\n","                        label[start_idx:end_idx] = 1\n","\n","        return torch.tensor(label, dtype=torch.float)\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n","        return input_, label"]},{"cell_type":"code","execution_count":25,"id":"apparent-norfolk","metadata":{"id":"apparent-norfolk","executionInfo":{"status":"ok","timestamp":1645614573484,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class TestDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        return input_"]},{"cell_type":"markdown","id":"motivated-bread","metadata":{"id":"motivated-bread"},"source":["## Model"]},{"cell_type":"code","execution_count":26,"id":"minute-virginia","metadata":{"id":"minute-virginia","executionInfo":{"status":"ok","timestamp":1645614573484,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class CustomModel(nn.Module):\n","    def __init__(self, cfg, model_config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        if model_config_path is None:\n","            self.model_config = AutoConfig.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                output_hidden_states=True,\n","            )\n","        else:\n","            self.model_config = torch.load(model_config_path)\n","\n","        if pretrained:\n","            self.backbone = AutoModel.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                config=self.model_config,\n","            )\n","        else:\n","            self.backbone = AutoModel.from_config(self.model_config)\n","\n","        self.fc = nn.Sequential(\n","            nn.Dropout(self.cfg.dropout),\n","            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n","        )\n","\n","    def forward(self, inputs):\n","        h = self.backbone(**inputs)[\"last_hidden_state\"]\n","        output = self.fc(h)\n","        return output"]},{"cell_type":"markdown","id":"seventh-configuration","metadata":{"id":"seventh-configuration"},"source":["## Training"]},{"cell_type":"code","execution_count":27,"id":"rocky-lexington","metadata":{"id":"rocky-lexington","executionInfo":{"status":"ok","timestamp":1645614573484,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def train_fn(\n","    train_dataloader,\n","    model,\n","    criterion,\n","    optimizer,\n","    epoch,\n","    scheduler,\n","    device,\n","):\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(train_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","\n","        if CFG.batch_scheduler:\n","            scheduler.step()\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n","            print(\n","                \"Epoch: [{0}][{1}/{2}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                \"Grad: {grad_norm:.4f}  \"\n","                \"LR: {lr:.6f}  \"\n","                .format(\n","                    epoch+1,\n","                    step,\n","                    len(train_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n","                    loss=losses,\n","                     grad_norm=grad_norm,\n","                     lr=scheduler.get_lr()[0],\n","                )\n","            )\n","    return losses.avg"]},{"cell_type":"code","execution_count":28,"id":"honest-programming","metadata":{"id":"honest-programming","executionInfo":{"status":"ok","timestamp":1645614573484,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def valid_fn(\n","    val_dataloader,\n","    model,\n","    criterion,\n","    device,\n","):\n","    model.eval()\n","    preds = []\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(val_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.no_grad():\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(output.sigmoid().squeeze().detach().cpu().numpy())\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n","            print(\n","                \"EVAL: [{0}/{1}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                .format(\n","                    step, len(val_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n","                    loss=losses,\n","                )\n","            )\n","    preds = np.concatenate(preds)\n","    return losses.avg, preds"]},{"cell_type":"code","execution_count":29,"id":"junior-international","metadata":{"id":"junior-international","executionInfo":{"status":"ok","timestamp":1645614573485,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def inference_fn(test_dataloader, model, device):\n","    model.eval()\n","    model.to(device)\n","    preds = []\n","    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n","    for inputs in tk0:\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        with torch.no_grad():\n","            output = model(inputs)\n","        preds.append(output.sigmoid().squeeze().detach().cpu().numpy())\n","    preds = np.concatenate(preds)\n","    return preds"]},{"cell_type":"code","execution_count":30,"id":"complicated-testament","metadata":{"id":"complicated-testament","executionInfo":{"status":"ok","timestamp":1645614573485,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def train_loop(df, i_fold, device):\n","    print(f\"========== fold: {i_fold} training ==========\")\n","    train_idx = df[df[\"fold\"] != i_fold].index\n","    val_idx = df[df[\"fold\"] == i_fold].index\n","\n","    train_folds = df.loc[train_idx].reset_index(drop=True)\n","    val_folds = df.loc[val_idx].reset_index(drop=True)\n","\n","    train_dataset = TrainingDataset(CFG, train_folds)\n","    val_dataset = TrainingDataset(CFG, val_folds)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=True,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=True,\n","    )\n","    val_dataloader = DataLoader(\n","        val_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=False,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=False,\n","    )\n","\n","    model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n","    model.to(device)\n","\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\"params\": [p for n, p in param_optimizer if not any(\n","            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n","        {\"params\": [p for n, p in param_optimizer if any(\n","            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n","    ]\n","    optimizer = AdamW(\n","        optimizer_grouped_parameters,\n","        lr=CFG.lr,\n","        betas=CFG.betas,\n","        weight_decay=CFG.weight_decay,\n","    )\n","    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n","    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        num_training_steps=num_train_optimization_steps,\n","    )\n","\n","    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    best_score = -1 * np.inf\n","\n","    for epoch in range(CFG.epochs):\n","        start_time = time.time()\n","        avg_loss = train_fn(\n","            train_dataloader,\n","            model,\n","            criterion,\n","            optimizer,\n","            epoch,\n","            scheduler,\n","            device,\n","        )\n","        avg_val_loss, val_preds = valid_fn(\n","            val_dataloader,\n","            model,\n","            criterion,\n","            device,\n","        )\n","\n","        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n","            scheduler.step()\n","\n","        # scoring\n","        val_folds[[str(i) for i in range(CFG.max_len)]] = val_preds\n","        score = scoring(val_folds, th=0.5)\n","\n","        elapsed = time.time() - start_time\n","\n","        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n","        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n","        if score > best_score:\n","            best_score = score\n","            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n","            torch.save({\n","                \"model\": model.state_dict(),\n","                \"predictions\": val_preds,\n","                },\n","                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","            )\n","\n","    predictions = torch.load(\n","        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","        map_location=torch.device(\"cpu\"),\n","    )[\"predictions\"]\n","    val_folds[[str(i) for i in range(CFG.max_len)]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return val_folds"]},{"cell_type":"markdown","id":"returning-banner","metadata":{"id":"returning-banner"},"source":["## Main"]},{"cell_type":"code","execution_count":31,"id":"ongoing-budget","metadata":{"id":"ongoing-budget","executionInfo":{"status":"ok","timestamp":1645614573485,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def main():\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    if CFG.train:\n","        oof_df = pd.DataFrame()\n","        for i_fold in range(CFG.n_fold):\n","            if i_fold in CFG.train_fold:\n","                _oof_df = train_loop(train, i_fold, device)\n","                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n","        #oof_df.to_csv(CFG.output_dir / \"oof_df.csv\", index=False)\n","        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    if CFG.submission:\n","        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n","    else:\n","        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    score = scoring(oof_df, th=0.5)\n","    print(f\"Best thres: 0.5, Score: {score:.4f}\")\n","    best_thres = get_best_thres(oof_df)\n","    score = scoring(oof_df, th=best_thres)\n","    print(f\"Best thres: {best_thres}, Score: {score:.4f}\")\n","\n","    if CFG.inference:\n","        test_dataset = TestDataset(CFG, test)\n","        test_dataloader = DataLoader(\n","            test_dataset,\n","            batch_size=CFG.batch_size,\n","            shuffle=False,\n","            num_workers=CFG.num_workers,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","        predictions = []\n","        for i_fold in CFG.train_fold:\n","            if CFG.submission:\n","                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n","                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n","            else:\n","                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n","\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            model.load_state_dict(state[\"model\"])\n","            test_token_probs = inference_fn(test_dataloader, model, device)\n","            test[[f\"fold{i_fold}_{i}\" for i in range(CFG.max_len)]] = test_token_probs\n","            test_char_probs = get_char_probs(test[\"pn_history\"].values, test_token_probs, CFG.tokenizer)\n","            predictions.append(test_char_probs)\n","\n","            del state, test_token_probs, model; gc.collect()\n","            torch.cuda.empty_cache()\n","\n","        predictions = np.mean(predictions, axis=0)\n","        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n","        test[CFG.target_col] = predicted_location_str\n","        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n","        test[[CFG.id_col, CFG.target_col]].to_csv(\n","            CFG.output_dir / \"submission.csv\", index=False\n","        )"]},{"cell_type":"code","execution_count":32,"id":"nearby-ultimate","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["0d9ebef7937448ea8f674a0be608eac0","80ed937bfa8b4d6583e75192e5751f60","e71730d0da3345a08306022932a3fc44","835e194f801c43ebb3e7f288efbade19","fa9e6c6ee4264abba8d76e4a3487c92f","f794d8da46ab4e0b8845189e2afb9445","548b2bc7cc9244a194b159c382f63769","a5aa63e0b6464ec7a6a1c2673b1b70ef","6653d082286742d39916093a01865a55","dae4cc2c197a403fbfdc337a3006994c","5b5a918c526d4c9c97aefced65b33236","7d826574b7c94347947097cf95a07ba8","ae88d591d71e4e1eaf903f54ec2fa378","c4df2f9fdaee4cb0baa4cdb61ad8509c","68a1b3e79cb84b8794b8db5fdfd6258d","6e2c4d74af6d4a31b0c9c8fe41bc990c","3687acea47364461ad976dc759b9fd37","670906e53bb4491e91cf2b5149f58e09","65d48eb6ea2b4579bd03995bc3d30e6b","829949a4fce1433ebf4488238f539302","c312abd88bb94935a7d6c8cd6cf08d9f","0146c65dbfe84fd88d60fcd71dc22e88","726bd7fa25a444b39a54ebd2f1074906","31e3d1f11e4b445382ed586eabf11541","9c2edd474604437d9f30bf055eedfc42","5ba75f0e32d940ad9a811ca3131b0db6","a878d1591583475785b9b99ffd1bfe86","9b9ff8855fe34930bec9bd9feb8ab719","d44e74bb4cda4685bdee0246a1709a75","6f2d4ec95069450380784810a1bdbee1","ebfd69f8fb2044c8a2a80b56684f28b6","0be5f0ced8d54aef862b91e6af484bc3","34b57bf63a674edc917adfffebc9ff39","faa0b9e403854d628755f5fc755df0e6","8b377c36c9f6411caf96ae4979ca8523","7d3d9eb12bdb41de97259cfa7108de59","3f4536b288164897a387d16638269fcf","2de09571d5bf41d69b0b9aef57fc3b18","a2b6ef66e28841dc9f426be385d91c61","dbb220a47b384d2690d82401a28c4d03","e10ef4d69bea46d9978058b42d1128f0","bd01ce71fa5640a9a63c999932069c3d","82f58599156841e6a4300ac0a7f85369","690568c77ac24786b7e44c83bb222298","18591825905749078d7be86d5d8f40b7","a1f7a0bed23f4566a2848ec8fa27f700","8c8e9d7990cd4c6495ff4655e83a5539","b890a083095544e682a8970000dd98f6","191b5134210f49efbbf847b7bd3bd3e4","6dafb06ce46c4d12986651c11737fcd7","e76c4daafd154830adac192c70cbd2b2","a39e68b36c934f1cb16e709f7b4f5e94","ff497745794d4775988c38bd324d308a","d122b49dd1774adeb9e0429537855fb9","fd1ef1f6bacb497fbc5e2b47c2e05e92"]},"id":"nearby-ultimate","executionInfo":{"status":"ok","timestamp":1645624949290,"user_tz":-540,"elapsed":10375312,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}},"outputId":"55d9f0c3-b73a-462f-f09b-0f4416ea38c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["========== fold: 0 training ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: [1][0/1424] Elapsed 0m 1s (remain 24m 15s) Loss: 0.7841(0.7841) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/1424] Elapsed 0m 22s (remain 5m 0s) Loss: 0.1131(0.3776) Grad: 1992.6510  LR: 0.000003  \n","Epoch: [1][200/1424] Elapsed 0m 44s (remain 4m 33s) Loss: 0.0269(0.2211) Grad: 3103.3484  LR: 0.000006  \n","Epoch: [1][300/1424] Elapsed 1m 6s (remain 4m 9s) Loss: 0.0132(0.1612) Grad: 2843.2793  LR: 0.000008  \n","Epoch: [1][400/1424] Elapsed 1m 29s (remain 3m 48s) Loss: 0.0341(0.1293) Grad: 3110.3171  LR: 0.000011  \n","Epoch: [1][500/1424] Elapsed 1m 51s (remain 3m 25s) Loss: 0.0445(0.1092) Grad: 5013.7622  LR: 0.000014  \n","Epoch: [1][600/1424] Elapsed 2m 13s (remain 3m 2s) Loss: 0.0402(0.0950) Grad: 4871.8579  LR: 0.000017  \n","Epoch: [1][700/1424] Elapsed 2m 35s (remain 2m 40s) Loss: 0.0260(0.0849) Grad: 3865.8916  LR: 0.000020  \n","Epoch: [1][800/1424] Elapsed 2m 57s (remain 2m 18s) Loss: 0.0061(0.0772) Grad: 1086.5222  LR: 0.000020  \n","Epoch: [1][900/1424] Elapsed 3m 19s (remain 1m 55s) Loss: 0.0226(0.0714) Grad: 2173.0564  LR: 0.000019  \n","Epoch: [1][1000/1424] Elapsed 3m 41s (remain 1m 33s) Loss: 0.0550(0.0661) Grad: 5199.4399  LR: 0.000019  \n","Epoch: [1][1100/1424] Elapsed 4m 3s (remain 1m 11s) Loss: 0.0169(0.0619) Grad: 1735.8433  LR: 0.000019  \n","Epoch: [1][1200/1424] Elapsed 4m 25s (remain 0m 49s) Loss: 0.0202(0.0584) Grad: 4131.6763  LR: 0.000018  \n","Epoch: [1][1300/1424] Elapsed 4m 47s (remain 0m 27s) Loss: 0.0077(0.0551) Grad: 1778.2361  LR: 0.000018  \n","Epoch: [1][1400/1424] Elapsed 5m 9s (remain 0m 5s) Loss: 0.0065(0.0526) Grad: 3314.7175  LR: 0.000018  \n","Epoch: [1][1423/1424] Elapsed 5m 14s (remain 0m 0s) Loss: 0.0300(0.0522) Grad: 4019.8428  LR: 0.000018  \n","EVAL: [0/363] Elapsed 0m 0s (remain 2m 32s) Loss: 0.0146(0.0146) \n","EVAL: [100/363] Elapsed 0m 11s (remain 0m 30s) Loss: 0.0187(0.0144) \n","EVAL: [200/363] Elapsed 0m 23s (remain 0m 18s) Loss: 0.0202(0.0167) \n","EVAL: [300/363] Elapsed 0m 34s (remain 0m 7s) Loss: 0.0116(0.0163) \n","EVAL: [362/363] Elapsed 0m 41s (remain 0m 0s) Loss: 0.0032(0.0151) \n","Epoch 1 - avg_train_loss: 0.0522  avg_val_loss: 0.0151  time: 362s\n","Epoch 1 - Score: 0.8243\n","Epoch 1 - Save Best Score: 0.8243 Model\n","Epoch: [2][0/1424] Elapsed 0m 0s (remain 15m 28s) Loss: 0.0247(0.0247) Grad: 38792.8477  LR: 0.000018  \n","Epoch: [2][100/1424] Elapsed 0m 23s (remain 5m 2s) Loss: 0.0153(0.0144) Grad: 37225.3047  LR: 0.000017  \n","Epoch: [2][200/1424] Elapsed 0m 45s (remain 4m 34s) Loss: 0.0033(0.0135) Grad: 10804.8242  LR: 0.000017  \n","Epoch: [2][300/1424] Elapsed 1m 7s (remain 4m 10s) Loss: 0.0080(0.0131) Grad: 14410.0586  LR: 0.000017  \n","Epoch: [2][400/1424] Elapsed 1m 29s (remain 3m 47s) Loss: 0.0043(0.0125) Grad: 12212.2158  LR: 0.000017  \n","Epoch: [2][500/1424] Elapsed 1m 51s (remain 3m 24s) Loss: 0.0322(0.0128) Grad: 127623.4531  LR: 0.000016  \n","Epoch: [2][600/1424] Elapsed 2m 13s (remain 3m 2s) Loss: 0.0180(0.0126) Grad: 64223.0664  LR: 0.000016  \n","Epoch: [2][700/1424] Elapsed 2m 35s (remain 2m 40s) Loss: 0.0125(0.0127) Grad: 20553.3320  LR: 0.000016  \n","Epoch: [2][800/1424] Elapsed 2m 57s (remain 2m 17s) Loss: 0.0211(0.0125) Grad: 46793.5312  LR: 0.000015  \n","Epoch: [2][900/1424] Elapsed 3m 19s (remain 1m 55s) Loss: 0.0052(0.0124) Grad: 10452.2305  LR: 0.000015  \n","Epoch: [2][1000/1424] Elapsed 3m 41s (remain 1m 33s) Loss: 0.0055(0.0125) Grad: 7373.5229  LR: 0.000015  \n","Epoch: [2][1100/1424] Elapsed 4m 3s (remain 1m 11s) Loss: 0.0158(0.0127) Grad: 28493.7285  LR: 0.000014  \n","Epoch: [2][1200/1424] Elapsed 4m 25s (remain 0m 49s) Loss: 0.0125(0.0126) Grad: 53120.9766  LR: 0.000014  \n","Epoch: [2][1300/1424] Elapsed 4m 47s (remain 0m 27s) Loss: 0.0087(0.0125) Grad: 25594.3535  LR: 0.000014  \n","Epoch: [2][1400/1424] Elapsed 5m 9s (remain 0m 5s) Loss: 0.0012(0.0124) Grad: 6205.5405  LR: 0.000013  \n","Epoch: [2][1423/1424] Elapsed 5m 14s (remain 0m 0s) Loss: 0.0093(0.0124) Grad: 14828.3730  LR: 0.000013  \n","EVAL: [0/363] Elapsed 0m 0s (remain 2m 28s) Loss: 0.0051(0.0051) \n","EVAL: [100/363] Elapsed 0m 11s (remain 0m 30s) Loss: 0.0085(0.0126) \n","EVAL: [200/363] Elapsed 0m 23s (remain 0m 18s) Loss: 0.0119(0.0144) \n","EVAL: [300/363] Elapsed 0m 34s (remain 0m 7s) Loss: 0.0039(0.0139) \n","EVAL: [362/363] Elapsed 0m 41s (remain 0m 0s) Loss: 0.0022(0.0129) \n","Epoch 2 - avg_train_loss: 0.0124  avg_val_loss: 0.0129  time: 361s\n","Epoch 2 - Score: 0.8551\n","Epoch 2 - Save Best Score: 0.8551 Model\n","Epoch: [3][0/1424] Elapsed 0m 0s (remain 16m 4s) Loss: 0.0016(0.0016) Grad: 5275.7646  LR: 0.000013  \n","Epoch: [3][100/1424] Elapsed 0m 23s (remain 5m 4s) Loss: 0.0029(0.0083) Grad: 8540.4043  LR: 0.000013  \n","Epoch: [3][200/1424] Elapsed 0m 45s (remain 4m 35s) Loss: 0.0165(0.0087) Grad: 18268.7227  LR: 0.000013  \n","Epoch: [3][300/1424] Elapsed 1m 7s (remain 4m 10s) Loss: 0.0049(0.0090) Grad: 19081.4375  LR: 0.000012  \n","Epoch: [3][400/1424] Elapsed 1m 29s (remain 3m 47s) Loss: 0.0234(0.0090) Grad: 73979.6641  LR: 0.000012  \n","Epoch: [3][500/1424] Elapsed 1m 51s (remain 3m 25s) Loss: 0.0164(0.0094) Grad: 50943.4961  LR: 0.000012  \n","Epoch: [3][600/1424] Elapsed 2m 13s (remain 3m 2s) Loss: 0.0161(0.0096) Grad: 103930.2656  LR: 0.000011  \n","Epoch: [3][700/1424] Elapsed 2m 35s (remain 2m 40s) Loss: 0.0165(0.0095) Grad: 43376.4453  LR: 0.000011  \n","Epoch: [3][800/1424] Elapsed 2m 57s (remain 2m 17s) Loss: 0.0069(0.0095) Grad: 18287.1816  LR: 0.000011  \n","Epoch: [3][900/1424] Elapsed 3m 19s (remain 1m 55s) Loss: 0.0121(0.0096) Grad: 24613.8926  LR: 0.000011  \n","Epoch: [3][1000/1424] Elapsed 3m 41s (remain 1m 33s) Loss: 0.0117(0.0096) Grad: 52006.5195  LR: 0.000010  \n","Epoch: [3][1100/1424] Elapsed 4m 3s (remain 1m 11s) Loss: 0.0003(0.0095) Grad: 1546.7694  LR: 0.000010  \n","Epoch: [3][1200/1424] Elapsed 4m 25s (remain 0m 49s) Loss: 0.0034(0.0095) Grad: 15994.1504  LR: 0.000010  \n","Epoch: [3][1300/1424] Elapsed 4m 47s (remain 0m 27s) Loss: 0.0033(0.0095) Grad: 17275.8750  LR: 0.000009  \n","Epoch: [3][1400/1424] Elapsed 5m 9s (remain 0m 5s) Loss: 0.0057(0.0096) Grad: 28530.2207  LR: 0.000009  \n","Epoch: [3][1423/1424] Elapsed 5m 14s (remain 0m 0s) Loss: 0.0144(0.0095) Grad: 20637.3086  LR: 0.000009  \n","EVAL: [0/363] Elapsed 0m 0s (remain 2m 31s) Loss: 0.0036(0.0036) \n","EVAL: [100/363] Elapsed 0m 11s (remain 0m 30s) Loss: 0.0057(0.0116) \n","EVAL: [200/363] Elapsed 0m 23s (remain 0m 18s) Loss: 0.0048(0.0137) \n","EVAL: [300/363] Elapsed 0m 34s (remain 0m 7s) Loss: 0.0026(0.0133) \n","EVAL: [362/363] Elapsed 0m 41s (remain 0m 0s) Loss: 0.0022(0.0122) \n","Epoch 3 - avg_train_loss: 0.0095  avg_val_loss: 0.0122  time: 362s\n","Epoch 3 - Score: 0.8677\n","Epoch 3 - Save Best Score: 0.8677 Model\n","Epoch: [4][0/1424] Elapsed 0m 0s (remain 14m 36s) Loss: 0.0084(0.0084) Grad: 13715.0762  LR: 0.000009  \n","Epoch: [4][100/1424] Elapsed 0m 23s (remain 5m 3s) Loss: 0.0032(0.0060) Grad: 14612.0146  LR: 0.000009  \n","Epoch: [4][200/1424] Elapsed 0m 45s (remain 4m 34s) Loss: 0.0008(0.0072) Grad: 3586.5559  LR: 0.000008  \n","Epoch: [4][300/1424] Elapsed 1m 7s (remain 4m 10s) Loss: 0.0003(0.0070) Grad: 4403.3955  LR: 0.000008  \n","Epoch: [4][400/1424] Elapsed 1m 29s (remain 3m 47s) Loss: 0.0080(0.0074) Grad: 17024.7305  LR: 0.000008  \n","Epoch: [4][500/1424] Elapsed 1m 51s (remain 3m 24s) Loss: 0.0051(0.0077) Grad: 26819.1504  LR: 0.000007  \n","Epoch: [4][600/1424] Elapsed 2m 13s (remain 3m 2s) Loss: 0.0009(0.0077) Grad: 16844.8398  LR: 0.000007  \n","Epoch: [4][700/1424] Elapsed 2m 35s (remain 2m 40s) Loss: 0.0122(0.0079) Grad: 33724.9648  LR: 0.000007  \n","Epoch: [4][800/1424] Elapsed 2m 57s (remain 2m 17s) Loss: 0.0091(0.0079) Grad: 19151.7480  LR: 0.000006  \n","Epoch: [4][900/1424] Elapsed 3m 19s (remain 1m 55s) Loss: 0.0034(0.0079) Grad: 13578.1934  LR: 0.000006  \n","Epoch: [4][1000/1424] Elapsed 3m 41s (remain 1m 33s) Loss: 0.0052(0.0078) Grad: 27388.4688  LR: 0.000006  \n","Epoch: [4][1100/1424] Elapsed 4m 3s (remain 1m 11s) Loss: 0.0025(0.0078) Grad: 12064.5293  LR: 0.000005  \n","Epoch: [4][1200/1424] Elapsed 4m 25s (remain 0m 49s) Loss: 0.0033(0.0077) Grad: 15660.2969  LR: 0.000005  \n","Epoch: [4][1300/1424] Elapsed 4m 47s (remain 0m 27s) Loss: 0.0004(0.0077) Grad: 2000.4032  LR: 0.000005  \n","Epoch: [4][1400/1424] Elapsed 5m 9s (remain 0m 5s) Loss: 0.0023(0.0076) Grad: 8405.8467  LR: 0.000005  \n","Epoch: [4][1423/1424] Elapsed 5m 14s (remain 0m 0s) Loss: 0.0002(0.0076) Grad: 2670.7905  LR: 0.000004  \n","EVAL: [0/363] Elapsed 0m 0s (remain 2m 31s) Loss: 0.0047(0.0047) \n","EVAL: [100/363] Elapsed 0m 11s (remain 0m 30s) Loss: 0.0039(0.0133) \n","EVAL: [200/363] Elapsed 0m 23s (remain 0m 18s) Loss: 0.0027(0.0160) \n","EVAL: [300/363] Elapsed 0m 34s (remain 0m 7s) Loss: 0.0011(0.0148) \n","EVAL: [362/363] Elapsed 0m 41s (remain 0m 0s) Loss: 0.0021(0.0137) \n","Epoch 4 - avg_train_loss: 0.0076  avg_val_loss: 0.0137  time: 362s\n","Epoch 4 - Score: 0.8722\n","Epoch 4 - Save Best Score: 0.8722 Model\n","Epoch: [5][0/1424] Elapsed 0m 0s (remain 14m 31s) Loss: 0.0079(0.0079) Grad: 43065.6641  LR: 0.000004  \n","Epoch: [5][100/1424] Elapsed 0m 23s (remain 5m 4s) Loss: 0.0219(0.0052) Grad: 148176.8281  LR: 0.000004  \n","Epoch: [5][200/1424] Elapsed 0m 45s (remain 4m 35s) Loss: 0.0095(0.0058) Grad: 19159.8848  LR: 0.000004  \n","Epoch: [5][300/1424] Elapsed 1m 7s (remain 4m 10s) Loss: 0.0009(0.0062) Grad: 6155.7695  LR: 0.000004  \n","Epoch: [5][400/1424] Elapsed 1m 29s (remain 3m 47s) Loss: 0.0003(0.0064) Grad: 7852.0464  LR: 0.000003  \n","Epoch: [5][500/1424] Elapsed 1m 51s (remain 3m 24s) Loss: 0.0015(0.0063) Grad: 17674.6680  LR: 0.000003  \n","Epoch: [5][600/1424] Elapsed 2m 13s (remain 3m 2s) Loss: 0.0041(0.0061) Grad: 32918.6562  LR: 0.000003  \n","Epoch: [5][700/1424] Elapsed 2m 35s (remain 2m 40s) Loss: 0.0038(0.0061) Grad: 29465.6797  LR: 0.000002  \n","Epoch: [5][800/1424] Elapsed 2m 57s (remain 2m 17s) Loss: 0.0162(0.0061) Grad: 29742.6621  LR: 0.000002  \n","Epoch: [5][900/1424] Elapsed 3m 24s (remain 1m 58s) Loss: 0.0006(0.0061) Grad: 3049.5056  LR: 0.000002  \n","Epoch: [5][1000/1424] Elapsed 3m 46s (remain 1m 35s) Loss: 0.0012(0.0060) Grad: 13337.1768  LR: 0.000001  \n","Epoch: [5][1100/1424] Elapsed 4m 8s (remain 1m 12s) Loss: 0.0018(0.0060) Grad: 11605.8623  LR: 0.000001  \n","Epoch: [5][1200/1424] Elapsed 4m 30s (remain 0m 50s) Loss: 0.0007(0.0060) Grad: 3795.5337  LR: 0.000001  \n","Epoch: [5][1300/1424] Elapsed 4m 53s (remain 0m 27s) Loss: 0.0112(0.0059) Grad: 41367.9141  LR: 0.000000  \n","Epoch: [5][1400/1424] Elapsed 5m 15s (remain 0m 5s) Loss: 0.0113(0.0060) Grad: 24221.8711  LR: 0.000000  \n","Epoch: [5][1423/1424] Elapsed 5m 20s (remain 0m 0s) Loss: 0.0025(0.0060) Grad: 10543.9512  LR: 0.000000  \n","EVAL: [0/363] Elapsed 0m 0s (remain 2m 53s) Loss: 0.0019(0.0019) \n","EVAL: [100/363] Elapsed 0m 11s (remain 0m 30s) Loss: 0.0026(0.0143) \n","EVAL: [200/363] Elapsed 0m 23s (remain 0m 18s) Loss: 0.0029(0.0169) \n","EVAL: [300/363] Elapsed 0m 34s (remain 0m 7s) Loss: 0.0010(0.0157) \n","EVAL: [362/363] Elapsed 0m 42s (remain 0m 0s) Loss: 0.0018(0.0145) \n","Epoch 5 - avg_train_loss: 0.0060  avg_val_loss: 0.0145  time: 368s\n","Epoch 5 - Score: 0.8731\n","Epoch 5 - Save Best Score: 0.8731 Model\n","========== fold: 1 training ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: [1][0/1425] Elapsed 0m 0s (remain 16m 2s) Loss: 0.3971(0.3971) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/1425] Elapsed 0m 23s (remain 5m 3s) Loss: 0.1334(0.2113) Grad: 4675.3442  LR: 0.000003  \n","Epoch: [1][200/1425] Elapsed 0m 45s (remain 4m 35s) Loss: 0.0897(0.1375) Grad: 10432.7158  LR: 0.000006  \n","Epoch: [1][300/1425] Elapsed 1m 7s (remain 4m 10s) Loss: 0.0215(0.1049) Grad: 5375.0044  LR: 0.000008  \n","Epoch: [1][400/1425] Elapsed 1m 29s (remain 3m 47s) Loss: 0.0320(0.0881) Grad: 2461.2358  LR: 0.000011  \n","Epoch: [1][500/1425] Elapsed 1m 51s (remain 3m 25s) Loss: 0.0117(0.0764) Grad: 3291.5432  LR: 0.000014  \n","Epoch: [1][600/1425] Elapsed 2m 13s (remain 3m 2s) Loss: 0.0228(0.0680) Grad: 5703.9058  LR: 0.000017  \n","Epoch: [1][700/1425] Elapsed 2m 35s (remain 2m 40s) Loss: 0.0076(0.0620) Grad: 2155.9717  LR: 0.000020  \n","Epoch: [1][800/1425] Elapsed 2m 57s (remain 2m 18s) Loss: 0.0224(0.0572) Grad: 4415.9048  LR: 0.000020  \n","Epoch: [1][900/1425] Elapsed 3m 19s (remain 1m 55s) Loss: 0.0400(0.0532) Grad: 6436.4727  LR: 0.000019  \n","Epoch: [1][1000/1425] Elapsed 3m 41s (remain 1m 33s) Loss: 0.0310(0.0496) Grad: 10960.8262  LR: 0.000019  \n","Epoch: [1][1100/1425] Elapsed 4m 3s (remain 1m 11s) Loss: 0.0106(0.0472) Grad: 6533.8296  LR: 0.000019  \n","Epoch: [1][1200/1425] Elapsed 4m 25s (remain 0m 49s) Loss: 0.0203(0.0447) Grad: 5647.3091  LR: 0.000018  \n","Epoch: [1][1300/1425] Elapsed 4m 47s (remain 0m 27s) Loss: 0.0501(0.0427) Grad: 6055.4966  LR: 0.000018  \n","Epoch: [1][1400/1425] Elapsed 5m 9s (remain 0m 5s) Loss: 0.0122(0.0409) Grad: 4049.6992  LR: 0.000018  \n","Epoch: [1][1424/1425] Elapsed 5m 14s (remain 0m 0s) Loss: 0.0078(0.0405) Grad: 1506.5768  LR: 0.000018  \n","EVAL: [0/362] Elapsed 0m 0s (remain 3m 0s) Loss: 0.0038(0.0038) \n","EVAL: [100/362] Elapsed 0m 11s (remain 0m 30s) Loss: 0.0060(0.0143) \n","EVAL: [200/362] Elapsed 0m 23s (remain 0m 18s) Loss: 0.0410(0.0147) \n","EVAL: [300/362] Elapsed 0m 34s (remain 0m 7s) Loss: 0.0067(0.0148) \n","EVAL: [361/362] Elapsed 0m 41s (remain 0m 0s) Loss: 0.0080(0.0139) \n","Epoch 1 - avg_train_loss: 0.0405  avg_val_loss: 0.0139  time: 361s\n","Epoch 1 - Score: 0.8255\n","Epoch 1 - Save Best Score: 0.8255 Model\n","Epoch: [2][0/1425] Elapsed 0m 0s (remain 15m 20s) Loss: 0.0109(0.0109) Grad: 13454.7295  LR: 0.000018  \n","Epoch: [2][100/1425] Elapsed 0m 22s (remain 5m 1s) Loss: 0.0120(0.0145) Grad: 36542.9844  LR: 0.000017  \n","Epoch: [2][200/1425] Elapsed 0m 45s (remain 4m 34s) Loss: 0.0147(0.0132) Grad: 45828.5859  LR: 0.000017  \n","Epoch: [2][300/1425] Elapsed 1m 7s (remain 4m 10s) Loss: 0.0107(0.0129) Grad: 57923.1250  LR: 0.000017  \n","Epoch: [2][400/1425] Elapsed 1m 29s (remain 3m 47s) Loss: 0.0175(0.0128) Grad: 37439.4023  LR: 0.000017  \n","Epoch: [2][500/1425] Elapsed 1m 50s (remain 3m 24s) Loss: 0.0171(0.0132) Grad: 32046.6328  LR: 0.000016  \n","Epoch: [2][600/1425] Elapsed 2m 12s (remain 3m 2s) Loss: 0.0035(0.0133) Grad: 15766.0693  LR: 0.000016  \n","Epoch: [2][700/1425] Elapsed 2m 34s (remain 2m 40s) Loss: 0.0043(0.0132) Grad: 8000.8506  LR: 0.000016  \n","Epoch: [2][800/1425] Elapsed 2m 56s (remain 2m 17s) Loss: 0.0107(0.0130) Grad: 32808.4219  LR: 0.000015  \n","Epoch: [2][900/1425] Elapsed 3m 18s (remain 1m 55s) Loss: 0.0471(0.0130) Grad: 106851.8828  LR: 0.000015  \n","Epoch: [2][1000/1425] Elapsed 3m 40s (remain 1m 33s) Loss: 0.0231(0.0130) Grad: 41555.1680  LR: 0.000015  \n","Epoch: [2][1100/1425] Elapsed 4m 2s (remain 1m 11s) Loss: 0.0046(0.0128) Grad: 11040.8291  LR: 0.000014  \n","Epoch: [2][1200/1425] Elapsed 4m 24s (remain 0m 49s) Loss: 0.0033(0.0128) Grad: 10704.1016  LR: 0.000014  \n","Epoch: [2][1300/1425] Elapsed 4m 46s (remain 0m 27s) Loss: 0.0037(0.0126) Grad: 23111.6113  LR: 0.000014  \n","Epoch: [2][1400/1425] Elapsed 5m 8s (remain 0m 5s) Loss: 0.0076(0.0125) Grad: 20280.7637  LR: 0.000013  \n","Epoch: [2][1424/1425] Elapsed 5m 13s (remain 0m 0s) Loss: 0.0096(0.0125) Grad: 40643.3555  LR: 0.000013  \n","EVAL: [0/362] Elapsed 0m 0s (remain 2m 53s) Loss: 0.0031(0.0031) \n","EVAL: [100/362] Elapsed 0m 11s (remain 0m 30s) Loss: 0.0013(0.0139) \n","EVAL: [200/362] Elapsed 0m 23s (remain 0m 18s) Loss: 0.0255(0.0138) \n","EVAL: [300/362] Elapsed 0m 34s (remain 0m 7s) Loss: 0.0070(0.0142) \n","EVAL: [361/362] Elapsed 0m 41s (remain 0m 0s) Loss: 0.0063(0.0131) \n","Epoch 2 - avg_train_loss: 0.0125  avg_val_loss: 0.0131  time: 361s\n","Epoch 2 - Score: 0.8529\n","Epoch 2 - Save Best Score: 0.8529 Model\n","Epoch: [3][0/1425] Elapsed 0m 0s (remain 14m 30s) Loss: 0.0018(0.0018) Grad: 6840.8462  LR: 0.000013  \n","Epoch: [3][100/1425] Elapsed 0m 22s (remain 5m 1s) Loss: 0.0035(0.0095) Grad: 17893.3711  LR: 0.000013  \n","Epoch: [3][200/1425] Elapsed 0m 44s (remain 4m 33s) Loss: 0.0003(0.0096) Grad: 1467.7416  LR: 0.000013  \n","Epoch: [3][300/1425] Elapsed 1m 6s (remain 4m 10s) Loss: 0.0097(0.0101) Grad: 44416.1055  LR: 0.000012  \n","Epoch: [3][400/1425] Elapsed 1m 29s (remain 3m 47s) Loss: 0.0067(0.0098) Grad: 20670.8887  LR: 0.000012  \n","Epoch: [3][500/1425] Elapsed 1m 51s (remain 3m 24s) Loss: 0.0011(0.0096) Grad: 9143.1094  LR: 0.000012  \n","Epoch: [3][600/1425] Elapsed 2m 12s (remain 3m 2s) Loss: 0.0217(0.0099) Grad: 29020.0723  LR: 0.000011  \n","Epoch: [3][700/1425] Elapsed 2m 35s (remain 2m 40s) Loss: 0.0135(0.0098) Grad: 57096.3711  LR: 0.000011  \n","Epoch: [3][800/1425] Elapsed 2m 57s (remain 2m 17s) Loss: 0.0046(0.0096) Grad: 16700.9844  LR: 0.000011  \n","Epoch: [3][900/1425] Elapsed 3m 19s (remain 1m 55s) Loss: 0.0015(0.0097) Grad: 5308.7207  LR: 0.000011  \n","Epoch: [3][1000/1425] Elapsed 3m 41s (remain 1m 33s) Loss: 0.0175(0.0097) Grad: 25682.0645  LR: 0.000010  \n","Epoch: [3][1100/1425] Elapsed 4m 3s (remain 1m 11s) Loss: 0.0088(0.0096) Grad: 48230.9062  LR: 0.000010  \n","Epoch: [3][1200/1425] Elapsed 4m 25s (remain 0m 49s) Loss: 0.0014(0.0097) Grad: 4443.3799  LR: 0.000010  \n","Epoch: [3][1300/1425] Elapsed 4m 47s (remain 0m 27s) Loss: 0.0064(0.0096) Grad: 17331.3066  LR: 0.000009  \n","Epoch: [3][1400/1425] Elapsed 5m 9s (remain 0m 5s) Loss: 0.0115(0.0095) Grad: 33951.3203  LR: 0.000009  \n","Epoch: [3][1424/1425] Elapsed 5m 14s (remain 0m 0s) Loss: 0.0086(0.0095) Grad: 25015.2422  LR: 0.000009  \n","EVAL: [0/362] Elapsed 0m 0s (remain 2m 44s) Loss: 0.0023(0.0023) \n","EVAL: [100/362] Elapsed 0m 11s (remain 0m 30s) Loss: 0.0008(0.0159) \n","EVAL: [200/362] Elapsed 0m 23s (remain 0m 18s) Loss: 0.0475(0.0157) \n","EVAL: [300/362] Elapsed 0m 34s (remain 0m 7s) Loss: 0.0067(0.0157) \n","EVAL: [361/362] Elapsed 0m 41s (remain 0m 0s) Loss: 0.0047(0.0144) \n","Epoch 3 - avg_train_loss: 0.0095  avg_val_loss: 0.0144  time: 361s\n","Epoch 3 - Score: 0.8511\n","Epoch: [4][0/1425] Elapsed 0m 0s (remain 13m 20s) Loss: 0.0024(0.0024) Grad: 6861.6699  LR: 0.000009  \n","Epoch: [4][100/1425] Elapsed 0m 22s (remain 4m 55s) Loss: 0.0099(0.0062) Grad: 35666.9961  LR: 0.000009  \n","Epoch: [4][200/1425] Elapsed 0m 44s (remain 4m 31s) Loss: 0.0013(0.0070) Grad: 6836.2817  LR: 0.000008  \n","Epoch: [4][300/1425] Elapsed 1m 6s (remain 4m 8s) Loss: 0.0001(0.0070) Grad: 695.0681  LR: 0.000008  \n","Epoch: [4][400/1425] Elapsed 1m 28s (remain 3m 46s) Loss: 0.0025(0.0068) Grad: 31776.0625  LR: 0.000008  \n","Epoch: [4][500/1425] Elapsed 1m 50s (remain 3m 24s) Loss: 0.0006(0.0068) Grad: 3312.1987  LR: 0.000007  \n","Epoch: [4][600/1425] Elapsed 2m 12s (remain 3m 1s) Loss: 0.0086(0.0069) Grad: 21223.8359  LR: 0.000007  \n","Epoch: [4][700/1425] Elapsed 2m 34s (remain 2m 39s) Loss: 0.0013(0.0070) Grad: 55727.9336  LR: 0.000007  \n","Epoch: [4][800/1425] Elapsed 2m 56s (remain 2m 17s) Loss: 0.0021(0.0074) Grad: 7231.4365  LR: 0.000006  \n","Epoch: [4][900/1425] Elapsed 3m 18s (remain 1m 55s) Loss: 0.0373(0.0075) Grad: 284378.0312  LR: 0.000006  \n","Epoch: [4][1000/1425] Elapsed 3m 40s (remain 1m 33s) Loss: 0.0011(0.0075) Grad: 21682.8594  LR: 0.000006  \n","Epoch: [4][1100/1425] Elapsed 4m 2s (remain 1m 11s) Loss: 0.0081(0.0075) Grad: 27611.3672  LR: 0.000005  \n","Epoch: [4][1200/1425] Elapsed 4m 24s (remain 0m 49s) Loss: 0.0153(0.0075) Grad: 69355.6250  LR: 0.000005  \n","Epoch: [4][1300/1425] Elapsed 4m 46s (remain 0m 27s) Loss: 0.0041(0.0075) Grad: 14253.3301  LR: 0.000005  \n","Epoch: [4][1400/1425] Elapsed 5m 8s (remain 0m 5s) Loss: 0.0066(0.0075) Grad: 26646.9258  LR: 0.000005  \n","Epoch: [4][1424/1425] Elapsed 5m 14s (remain 0m 0s) Loss: 0.0010(0.0074) Grad: 14736.9043  LR: 0.000004  \n","EVAL: [0/362] Elapsed 0m 0s (remain 2m 25s) Loss: 0.0026(0.0026) \n","EVAL: [100/362] Elapsed 0m 11s (remain 0m 30s) Loss: 0.0003(0.0148) \n","EVAL: [200/362] Elapsed 0m 23s (remain 0m 18s) Loss: 0.0409(0.0150) \n","EVAL: [300/362] Elapsed 0m 34s (remain 0m 7s) Loss: 0.0101(0.0158) \n","EVAL: [361/362] Elapsed 0m 41s (remain 0m 0s) Loss: 0.0040(0.0145) \n","Epoch 4 - avg_train_loss: 0.0074  avg_val_loss: 0.0145  time: 361s\n","Epoch 4 - Score: 0.8663\n","Epoch 4 - Save Best Score: 0.8663 Model\n","Epoch: [5][0/1425] Elapsed 0m 0s (remain 15m 6s) Loss: 0.0130(0.0130) Grad: 60973.5000  LR: 0.000004  \n","Epoch: [5][100/1425] Elapsed 0m 23s (remain 5m 4s) Loss: 0.0026(0.0068) Grad: 7249.1416  LR: 0.000004  \n","Epoch: [5][200/1425] Elapsed 0m 45s (remain 4m 35s) Loss: 0.0153(0.0060) Grad: 53473.0859  LR: 0.000004  \n","Epoch: [5][300/1425] Elapsed 1m 7s (remain 4m 11s) Loss: 0.0069(0.0064) Grad: 42592.5469  LR: 0.000004  \n","Epoch: [5][400/1425] Elapsed 1m 29s (remain 3m 47s) Loss: 0.0033(0.0063) Grad: 12135.9414  LR: 0.000003  \n","Epoch: [5][500/1425] Elapsed 1m 51s (remain 3m 25s) Loss: 0.0074(0.0063) Grad: 10743.8809  LR: 0.000003  \n","Epoch: [5][600/1425] Elapsed 2m 13s (remain 3m 2s) Loss: 0.0101(0.0062) Grad: 39935.6680  LR: 0.000003  \n","Epoch: [5][700/1425] Elapsed 2m 35s (remain 2m 40s) Loss: 0.0029(0.0063) Grad: 9356.6875  LR: 0.000002  \n","Epoch: [5][800/1425] Elapsed 2m 57s (remain 2m 18s) Loss: 0.0000(0.0061) Grad: 94.0826  LR: 0.000002  \n","Epoch: [5][900/1425] Elapsed 3m 19s (remain 1m 55s) Loss: 0.0052(0.0061) Grad: 24432.5000  LR: 0.000002  \n","Epoch: [5][1000/1425] Elapsed 3m 41s (remain 1m 33s) Loss: 0.0024(0.0060) Grad: 11965.0381  LR: 0.000001  \n","Epoch: [5][1100/1425] Elapsed 4m 3s (remain 1m 11s) Loss: 0.0024(0.0060) Grad: 11923.0254  LR: 0.000001  \n","Epoch: [5][1200/1425] Elapsed 4m 25s (remain 0m 49s) Loss: 0.0346(0.0060) Grad: 107221.3438  LR: 0.000001  \n","Epoch: [5][1300/1425] Elapsed 4m 47s (remain 0m 27s) Loss: 0.0247(0.0060) Grad: 84130.8516  LR: 0.000000  \n","Epoch: [5][1400/1425] Elapsed 5m 9s (remain 0m 5s) Loss: 0.0056(0.0060) Grad: 33831.6992  LR: 0.000000  \n","Epoch: [5][1424/1425] Elapsed 5m 14s (remain 0m 0s) Loss: 0.0098(0.0060) Grad: 21462.2207  LR: 0.000000  \n","EVAL: [0/362] Elapsed 0m 0s (remain 2m 48s) Loss: 0.0026(0.0026) \n","EVAL: [100/362] Elapsed 0m 11s (remain 0m 30s) Loss: 0.0002(0.0158) \n","EVAL: [200/362] Elapsed 0m 23s (remain 0m 18s) Loss: 0.0450(0.0160) \n","EVAL: [300/362] Elapsed 0m 34s (remain 0m 7s) Loss: 0.0147(0.0171) \n","EVAL: [361/362] Elapsed 0m 41s (remain 0m 0s) Loss: 0.0044(0.0156) \n","Epoch 5 - avg_train_loss: 0.0060  avg_val_loss: 0.0156  time: 362s\n","Epoch 5 - Score: 0.8667\n","Epoch 5 - Save Best Score: 0.8667 Model\n","========== fold: 2 training ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: [1][0/1435] Elapsed 0m 0s (remain 15m 21s) Loss: 1.2420(1.2420) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/1435] Elapsed 0m 23s (remain 5m 7s) Loss: 0.0329(0.6274) Grad: 572.4616  LR: 0.000003  \n","Epoch: [1][200/1435] Elapsed 0m 45s (remain 4m 38s) Loss: 0.0178(0.3453) Grad: 1442.6128  LR: 0.000006  \n","Epoch: [1][300/1435] Elapsed 1m 7s (remain 4m 13s) Loss: 0.0331(0.2436) Grad: 4928.6362  LR: 0.000008  \n","Epoch: [1][400/1435] Elapsed 1m 29s (remain 3m 50s) Loss: 0.0084(0.1908) Grad: 1225.8955  LR: 0.000011  \n","Epoch: [1][500/1435] Elapsed 1m 51s (remain 3m 27s) Loss: 0.0159(0.1586) Grad: 2368.3337  LR: 0.000014  \n","Epoch: [1][600/1435] Elapsed 2m 13s (remain 3m 5s) Loss: 0.0235(0.1367) Grad: 2449.5957  LR: 0.000017  \n","Epoch: [1][700/1435] Elapsed 2m 35s (remain 2m 42s) Loss: 0.0386(0.1209) Grad: 3049.7554  LR: 0.000020  \n","Epoch: [1][800/1435] Elapsed 2m 57s (remain 2m 20s) Loss: 0.0158(0.1085) Grad: 1226.0144  LR: 0.000020  \n","Epoch: [1][900/1435] Elapsed 3m 19s (remain 1m 58s) Loss: 0.0020(0.0991) Grad: 329.9468  LR: 0.000019  \n","Epoch: [1][1000/1435] Elapsed 3m 41s (remain 1m 35s) Loss: 0.0217(0.0911) Grad: 1994.5231  LR: 0.000019  \n","Epoch: [1][1100/1435] Elapsed 4m 3s (remain 1m 13s) Loss: 0.0049(0.0843) Grad: 780.4623  LR: 0.000019  \n","Epoch: [1][1200/1435] Elapsed 4m 25s (remain 0m 51s) Loss: 0.0129(0.0788) Grad: 2124.1995  LR: 0.000019  \n","Epoch: [1][1300/1435] Elapsed 4m 47s (remain 0m 29s) Loss: 0.0514(0.0742) Grad: 4642.5498  LR: 0.000018  \n","Epoch: [1][1400/1435] Elapsed 5m 9s (remain 0m 7s) Loss: 0.0110(0.0701) Grad: 1094.5743  LR: 0.000018  \n","Epoch: [1][1434/1435] Elapsed 5m 16s (remain 0m 0s) Loss: 0.0341(0.0689) Grad: 2845.1528  LR: 0.000018  \n","EVAL: [0/352] Elapsed 0m 0s (remain 2m 54s) Loss: 0.0417(0.0417) \n","EVAL: [100/352] Elapsed 0m 11s (remain 0m 29s) Loss: 0.0170(0.0173) \n","EVAL: [200/352] Elapsed 0m 23s (remain 0m 17s) Loss: 0.0437(0.0192) \n","EVAL: [300/352] Elapsed 0m 34s (remain 0m 5s) Loss: 0.0093(0.0203) \n","EVAL: [351/352] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0104(0.0194) \n","Epoch 1 - avg_train_loss: 0.0689  avg_val_loss: 0.0194  time: 362s\n","Epoch 1 - Score: 0.7842\n","Epoch 1 - Save Best Score: 0.7842 Model\n","Epoch: [2][0/1435] Elapsed 0m 0s (remain 15m 32s) Loss: 0.0073(0.0073) Grad: 15375.1602  LR: 0.000018  \n","Epoch: [2][100/1435] Elapsed 0m 23s (remain 5m 7s) Loss: 0.0181(0.0147) Grad: 40621.6133  LR: 0.000017  \n","Epoch: [2][200/1435] Elapsed 0m 45s (remain 4m 38s) Loss: 0.0101(0.0132) Grad: 20793.4512  LR: 0.000017  \n","Epoch: [2][300/1435] Elapsed 1m 7s (remain 4m 13s) Loss: 0.0083(0.0139) Grad: 27805.9375  LR: 0.000017  \n","Epoch: [2][400/1435] Elapsed 1m 29s (remain 3m 50s) Loss: 0.0259(0.0139) Grad: 18274.8066  LR: 0.000017  \n","Epoch: [2][500/1435] Elapsed 1m 51s (remain 3m 27s) Loss: 0.0016(0.0137) Grad: 7215.8154  LR: 0.000016  \n","Epoch: [2][600/1435] Elapsed 2m 13s (remain 3m 5s) Loss: 0.0053(0.0136) Grad: 12765.2236  LR: 0.000016  \n","Epoch: [2][700/1435] Elapsed 2m 35s (remain 2m 42s) Loss: 0.0092(0.0134) Grad: 45268.6719  LR: 0.000016  \n","Epoch: [2][800/1435] Elapsed 2m 57s (remain 2m 20s) Loss: 0.0030(0.0133) Grad: 7844.6353  LR: 0.000015  \n","Epoch: [2][900/1435] Elapsed 3m 19s (remain 1m 58s) Loss: 0.0075(0.0130) Grad: 35763.6055  LR: 0.000015  \n","Epoch: [2][1000/1435] Elapsed 3m 41s (remain 1m 36s) Loss: 0.0023(0.0132) Grad: 8178.6367  LR: 0.000015  \n","Epoch: [2][1100/1435] Elapsed 4m 3s (remain 1m 13s) Loss: 0.0029(0.0129) Grad: 11149.0459  LR: 0.000014  \n","Epoch: [2][1200/1435] Elapsed 4m 25s (remain 0m 51s) Loss: 0.0068(0.0128) Grad: 19999.1934  LR: 0.000014  \n","Epoch: [2][1300/1435] Elapsed 4m 47s (remain 0m 29s) Loss: 0.0082(0.0127) Grad: 21209.5918  LR: 0.000014  \n","Epoch: [2][1400/1435] Elapsed 5m 9s (remain 0m 7s) Loss: 0.0044(0.0128) Grad: 10795.6719  LR: 0.000013  \n","Epoch: [2][1434/1435] Elapsed 5m 17s (remain 0m 0s) Loss: 0.0067(0.0127) Grad: 16728.4355  LR: 0.000013  \n","EVAL: [0/352] Elapsed 0m 0s (remain 2m 41s) Loss: 0.0341(0.0341) \n","EVAL: [100/352] Elapsed 0m 11s (remain 0m 29s) Loss: 0.0098(0.0118) \n","EVAL: [200/352] Elapsed 0m 23s (remain 0m 17s) Loss: 0.0394(0.0129) \n","EVAL: [300/352] Elapsed 0m 34s (remain 0m 5s) Loss: 0.0012(0.0140) \n","EVAL: [351/352] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0068(0.0132) \n","Epoch 2 - avg_train_loss: 0.0127  avg_val_loss: 0.0132  time: 362s\n","Epoch 2 - Score: 0.8453\n","Epoch 2 - Save Best Score: 0.8453 Model\n","Epoch: [3][0/1435] Elapsed 0m 0s (remain 16m 8s) Loss: 0.0083(0.0083) Grad: 32413.8750  LR: 0.000013  \n","Epoch: [3][100/1435] Elapsed 0m 23s (remain 5m 5s) Loss: 0.0002(0.0083) Grad: 588.6271  LR: 0.000013  \n","Epoch: [3][200/1435] Elapsed 0m 45s (remain 4m 37s) Loss: 0.0002(0.0095) Grad: 1788.1681  LR: 0.000013  \n","Epoch: [3][300/1435] Elapsed 1m 7s (remain 4m 13s) Loss: 0.0067(0.0095) Grad: 39311.3242  LR: 0.000012  \n","Epoch: [3][400/1435] Elapsed 1m 29s (remain 3m 50s) Loss: 0.0078(0.0096) Grad: 12616.2480  LR: 0.000012  \n","Epoch: [3][500/1435] Elapsed 1m 51s (remain 3m 27s) Loss: 0.0025(0.0102) Grad: 8326.8018  LR: 0.000012  \n","Epoch: [3][600/1435] Elapsed 2m 13s (remain 3m 5s) Loss: 0.0046(0.0101) Grad: 23650.3242  LR: 0.000011  \n","Epoch: [3][700/1435] Elapsed 2m 35s (remain 2m 42s) Loss: 0.0017(0.0100) Grad: 4191.0752  LR: 0.000011  \n","Epoch: [3][800/1435] Elapsed 2m 57s (remain 2m 20s) Loss: 0.0047(0.0097) Grad: 30899.7559  LR: 0.000011  \n","Epoch: [3][900/1435] Elapsed 3m 19s (remain 1m 58s) Loss: 0.0142(0.0097) Grad: 18915.7676  LR: 0.000011  \n","Epoch: [3][1000/1435] Elapsed 3m 41s (remain 1m 35s) Loss: 0.0161(0.0097) Grad: 59315.6523  LR: 0.000010  \n","Epoch: [3][1100/1435] Elapsed 4m 3s (remain 1m 13s) Loss: 0.0120(0.0096) Grad: 19767.6016  LR: 0.000010  \n","Epoch: [3][1200/1435] Elapsed 4m 25s (remain 0m 51s) Loss: 0.0050(0.0097) Grad: 19559.4824  LR: 0.000010  \n","Epoch: [3][1300/1435] Elapsed 4m 47s (remain 0m 29s) Loss: 0.0039(0.0098) Grad: 10342.2314  LR: 0.000009  \n","Epoch: [3][1400/1435] Elapsed 5m 9s (remain 0m 7s) Loss: 0.0089(0.0097) Grad: 18854.2363  LR: 0.000009  \n","Epoch: [3][1434/1435] Elapsed 5m 16s (remain 0m 0s) Loss: 0.0023(0.0097) Grad: 38436.2188  LR: 0.000009  \n","EVAL: [0/352] Elapsed 0m 0s (remain 2m 36s) Loss: 0.0064(0.0064) \n","EVAL: [100/352] Elapsed 0m 11s (remain 0m 29s) Loss: 0.0115(0.0111) \n","EVAL: [200/352] Elapsed 0m 23s (remain 0m 17s) Loss: 0.0302(0.0125) \n","EVAL: [300/352] Elapsed 0m 34s (remain 0m 5s) Loss: 0.0014(0.0138) \n","EVAL: [351/352] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0017(0.0129) \n","Epoch 3 - avg_train_loss: 0.0097  avg_val_loss: 0.0129  time: 362s\n","Epoch 3 - Score: 0.8583\n","Epoch 3 - Save Best Score: 0.8583 Model\n","Epoch: [4][0/1435] Elapsed 0m 0s (remain 15m 3s) Loss: 0.0010(0.0010) Grad: 6870.6377  LR: 0.000009  \n","Epoch: [4][100/1435] Elapsed 0m 23s (remain 5m 5s) Loss: 0.0434(0.0077) Grad: 161282.4844  LR: 0.000009  \n","Epoch: [4][200/1435] Elapsed 0m 45s (remain 4m 37s) Loss: 0.0167(0.0077) Grad: 84257.3125  LR: 0.000008  \n","Epoch: [4][300/1435] Elapsed 1m 7s (remain 4m 13s) Loss: 0.0073(0.0079) Grad: 21396.8730  LR: 0.000008  \n","Epoch: [4][400/1435] Elapsed 1m 29s (remain 3m 50s) Loss: 0.0098(0.0078) Grad: 82957.1250  LR: 0.000008  \n","Epoch: [4][500/1435] Elapsed 1m 51s (remain 3m 27s) Loss: 0.0016(0.0078) Grad: 8174.9238  LR: 0.000007  \n","Epoch: [4][600/1435] Elapsed 2m 13s (remain 3m 4s) Loss: 0.0049(0.0076) Grad: 34619.2539  LR: 0.000007  \n","Epoch: [4][700/1435] Elapsed 2m 35s (remain 2m 42s) Loss: 0.0006(0.0075) Grad: 7813.9717  LR: 0.000007  \n","Epoch: [4][800/1435] Elapsed 2m 57s (remain 2m 20s) Loss: 0.0005(0.0075) Grad: 4494.0737  LR: 0.000006  \n","Epoch: [4][900/1435] Elapsed 3m 19s (remain 1m 58s) Loss: 0.0131(0.0074) Grad: 88575.5234  LR: 0.000006  \n","Epoch: [4][1000/1435] Elapsed 3m 41s (remain 1m 35s) Loss: 0.0066(0.0073) Grad: 38603.3477  LR: 0.000006  \n","Epoch: [4][1100/1435] Elapsed 4m 3s (remain 1m 13s) Loss: 0.0166(0.0073) Grad: 72878.2500  LR: 0.000005  \n","Epoch: [4][1200/1435] Elapsed 4m 25s (remain 0m 51s) Loss: 0.0096(0.0073) Grad: 19183.3691  LR: 0.000005  \n","Epoch: [4][1300/1435] Elapsed 4m 47s (remain 0m 29s) Loss: 0.0012(0.0074) Grad: 8081.0322  LR: 0.000005  \n","Epoch: [4][1400/1435] Elapsed 5m 9s (remain 0m 7s) Loss: 0.0448(0.0075) Grad: 102880.5000  LR: 0.000005  \n","Epoch: [4][1434/1435] Elapsed 5m 16s (remain 0m 0s) Loss: 0.0107(0.0075) Grad: 42865.8125  LR: 0.000004  \n","EVAL: [0/352] Elapsed 0m 0s (remain 2m 46s) Loss: 0.0020(0.0020) \n","EVAL: [100/352] Elapsed 0m 11s (remain 0m 29s) Loss: 0.0066(0.0117) \n","EVAL: [200/352] Elapsed 0m 23s (remain 0m 17s) Loss: 0.0404(0.0133) \n","EVAL: [300/352] Elapsed 0m 34s (remain 0m 5s) Loss: 0.0004(0.0151) \n","EVAL: [351/352] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0004(0.0141) \n","Epoch 4 - avg_train_loss: 0.0075  avg_val_loss: 0.0141  time: 362s\n","Epoch 4 - Score: 0.8632\n","Epoch 4 - Save Best Score: 0.8632 Model\n","Epoch: [5][0/1435] Elapsed 0m 0s (remain 14m 45s) Loss: 0.0007(0.0007) Grad: 4759.7983  LR: 0.000004  \n","Epoch: [5][100/1435] Elapsed 0m 23s (remain 5m 4s) Loss: 0.0016(0.0049) Grad: 6104.5801  LR: 0.000004  \n","Epoch: [5][200/1435] Elapsed 0m 45s (remain 4m 36s) Loss: 0.0051(0.0050) Grad: 19276.0273  LR: 0.000004  \n","Epoch: [5][300/1435] Elapsed 1m 7s (remain 4m 12s) Loss: 0.0078(0.0057) Grad: 92267.1094  LR: 0.000004  \n","Epoch: [5][400/1435] Elapsed 1m 29s (remain 3m 49s) Loss: 0.0038(0.0056) Grad: 21754.1953  LR: 0.000003  \n","Epoch: [5][500/1435] Elapsed 1m 51s (remain 3m 27s) Loss: 0.0014(0.0058) Grad: 6462.6113  LR: 0.000003  \n","Epoch: [5][600/1435] Elapsed 2m 13s (remain 3m 4s) Loss: 0.0300(0.0061) Grad: 16224.6270  LR: 0.000003  \n","Epoch: [5][700/1435] Elapsed 2m 35s (remain 2m 42s) Loss: 0.0104(0.0061) Grad: 65269.9336  LR: 0.000002  \n","Epoch: [5][800/1435] Elapsed 2m 57s (remain 2m 20s) Loss: 0.0010(0.0062) Grad: 6544.4312  LR: 0.000002  \n","Epoch: [5][900/1435] Elapsed 3m 19s (remain 1m 58s) Loss: 0.0022(0.0061) Grad: 7628.5396  LR: 0.000002  \n","Epoch: [5][1000/1435] Elapsed 3m 41s (remain 1m 35s) Loss: 0.0073(0.0061) Grad: 35573.5312  LR: 0.000001  \n","Epoch: [5][1100/1435] Elapsed 4m 3s (remain 1m 13s) Loss: 0.0006(0.0060) Grad: 2575.7407  LR: 0.000001  \n","Epoch: [5][1200/1435] Elapsed 4m 25s (remain 0m 51s) Loss: 0.0038(0.0061) Grad: 18202.3691  LR: 0.000001  \n","Epoch: [5][1300/1435] Elapsed 4m 47s (remain 0m 29s) Loss: 0.0014(0.0060) Grad: 6528.3950  LR: 0.000000  \n","Epoch: [5][1400/1435] Elapsed 5m 9s (remain 0m 7s) Loss: 0.0016(0.0060) Grad: 21399.1133  LR: 0.000000  \n","Epoch: [5][1434/1435] Elapsed 5m 16s (remain 0m 0s) Loss: 0.0078(0.0060) Grad: 17288.1035  LR: 0.000000  \n","EVAL: [0/352] Elapsed 0m 0s (remain 2m 38s) Loss: 0.0017(0.0017) \n","EVAL: [100/352] Elapsed 0m 11s (remain 0m 29s) Loss: 0.0117(0.0130) \n","EVAL: [200/352] Elapsed 0m 23s (remain 0m 17s) Loss: 0.0411(0.0144) \n","EVAL: [300/352] Elapsed 0m 34s (remain 0m 5s) Loss: 0.0002(0.0162) \n","EVAL: [351/352] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0003(0.0151) \n","Epoch 5 - avg_train_loss: 0.0060  avg_val_loss: 0.0151  time: 362s\n","Epoch 5 - Score: 0.8672\n","Epoch 5 - Save Best Score: 0.8672 Model\n","========== fold: 3 training ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: [1][0/1438] Elapsed 0m 0s (remain 14m 41s) Loss: 1.0292(1.0292) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/1438] Elapsed 0m 23s (remain 5m 10s) Loss: 0.0620(0.5202) Grad: 774.8645  LR: 0.000003  \n","Epoch: [1][200/1438] Elapsed 0m 45s (remain 4m 39s) Loss: 0.0600(0.2938) Grad: 184538.2188  LR: 0.000006  \n","Epoch: [1][300/1438] Elapsed 1m 7s (remain 4m 15s) Loss: 0.0509(0.2104) Grad: 4400.4370  LR: 0.000008  \n","Epoch: [1][400/1438] Elapsed 1m 29s (remain 3m 51s) Loss: 0.0126(0.1660) Grad: 1064.3547  LR: 0.000011  \n","Epoch: [1][500/1438] Elapsed 1m 51s (remain 3m 28s) Loss: 0.0080(0.1387) Grad: 873.7530  LR: 0.000014  \n","Epoch: [1][600/1438] Elapsed 2m 13s (remain 3m 5s) Loss: 0.0131(0.1199) Grad: 1913.4095  LR: 0.000017  \n","Epoch: [1][700/1438] Elapsed 2m 35s (remain 2m 43s) Loss: 0.0103(0.1061) Grad: 1435.3951  LR: 0.000019  \n","Epoch: [1][800/1438] Elapsed 2m 57s (remain 2m 21s) Loss: 0.0115(0.0957) Grad: 1772.2068  LR: 0.000020  \n","Epoch: [1][900/1438] Elapsed 3m 19s (remain 1m 58s) Loss: 0.0248(0.0873) Grad: 3887.3350  LR: 0.000019  \n","Epoch: [1][1000/1438] Elapsed 3m 41s (remain 1m 36s) Loss: 0.0064(0.0805) Grad: 876.6718  LR: 0.000019  \n","Epoch: [1][1100/1438] Elapsed 4m 3s (remain 1m 14s) Loss: 0.0164(0.0749) Grad: 2758.9087  LR: 0.000019  \n","Epoch: [1][1200/1438] Elapsed 4m 25s (remain 0m 52s) Loss: 0.0117(0.0702) Grad: 1275.9569  LR: 0.000019  \n","Epoch: [1][1300/1438] Elapsed 4m 47s (remain 0m 30s) Loss: 0.0137(0.0663) Grad: 2322.6968  LR: 0.000018  \n","Epoch: [1][1400/1438] Elapsed 5m 9s (remain 0m 8s) Loss: 0.0054(0.0627) Grad: 1777.9375  LR: 0.000018  \n","Epoch: [1][1437/1438] Elapsed 5m 17s (remain 0m 0s) Loss: 0.0085(0.0616) Grad: 842.8226  LR: 0.000018  \n","EVAL: [0/349] Elapsed 0m 0s (remain 2m 50s) Loss: 0.0082(0.0082) \n","EVAL: [100/349] Elapsed 0m 11s (remain 0m 29s) Loss: 0.0233(0.0136) \n","EVAL: [200/349] Elapsed 0m 23s (remain 0m 17s) Loss: 0.0117(0.0147) \n","EVAL: [300/349] Elapsed 0m 34s (remain 0m 5s) Loss: 0.0114(0.0151) \n","EVAL: [348/349] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0024(0.0144) \n","Epoch 1 - avg_train_loss: 0.0616  avg_val_loss: 0.0144  time: 362s\n","Epoch 1 - Score: 0.8196\n","Epoch 1 - Save Best Score: 0.8196 Model\n","Epoch: [2][0/1438] Elapsed 0m 0s (remain 15m 50s) Loss: 0.0052(0.0052) Grad: 8418.1846  LR: 0.000018  \n","Epoch: [2][100/1438] Elapsed 0m 23s (remain 5m 6s) Loss: 0.0045(0.0132) Grad: 15249.0889  LR: 0.000017  \n","Epoch: [2][200/1438] Elapsed 0m 45s (remain 4m 37s) Loss: 0.0131(0.0134) Grad: 35081.8516  LR: 0.000017  \n","Epoch: [2][300/1438] Elapsed 1m 7s (remain 4m 13s) Loss: 0.0032(0.0130) Grad: 7531.3706  LR: 0.000017  \n","Epoch: [2][400/1438] Elapsed 1m 29s (remain 3m 50s) Loss: 0.0190(0.0131) Grad: 48232.8945  LR: 0.000017  \n","Epoch: [2][500/1438] Elapsed 1m 51s (remain 3m 27s) Loss: 0.0206(0.0132) Grad: 249414.9531  LR: 0.000016  \n","Epoch: [2][600/1438] Elapsed 2m 13s (remain 3m 5s) Loss: 0.0171(0.0129) Grad: 55395.3672  LR: 0.000016  \n","Epoch: [2][700/1438] Elapsed 2m 35s (remain 2m 42s) Loss: 0.0233(0.0132) Grad: 48902.8477  LR: 0.000016  \n","Epoch: [2][800/1438] Elapsed 2m 57s (remain 2m 20s) Loss: 0.0034(0.0133) Grad: 8288.1836  LR: 0.000015  \n","Epoch: [2][900/1438] Elapsed 3m 18s (remain 1m 58s) Loss: 0.0122(0.0131) Grad: 31351.2207  LR: 0.000015  \n","Epoch: [2][1000/1438] Elapsed 3m 40s (remain 1m 36s) Loss: 0.0130(0.0129) Grad: 33732.2383  LR: 0.000015  \n","Epoch: [2][1100/1438] Elapsed 4m 2s (remain 1m 14s) Loss: 0.0104(0.0127) Grad: 26384.4492  LR: 0.000014  \n","Epoch: [2][1200/1438] Elapsed 4m 24s (remain 0m 52s) Loss: 0.0029(0.0125) Grad: 10203.1719  LR: 0.000014  \n","Epoch: [2][1300/1438] Elapsed 4m 46s (remain 0m 30s) Loss: 0.0438(0.0125) Grad: 118792.7812  LR: 0.000014  \n","Epoch: [2][1400/1438] Elapsed 5m 8s (remain 0m 8s) Loss: 0.0077(0.0124) Grad: 13143.3711  LR: 0.000013  \n","Epoch: [2][1437/1438] Elapsed 5m 17s (remain 0m 0s) Loss: 0.0333(0.0124) Grad: 47456.3750  LR: 0.000013  \n","EVAL: [0/349] Elapsed 0m 0s (remain 2m 30s) Loss: 0.0071(0.0071) \n","EVAL: [100/349] Elapsed 0m 11s (remain 0m 29s) Loss: 0.0135(0.0107) \n","EVAL: [200/349] Elapsed 0m 23s (remain 0m 17s) Loss: 0.0142(0.0117) \n","EVAL: [300/349] Elapsed 0m 34s (remain 0m 5s) Loss: 0.0065(0.0120) \n","EVAL: [348/349] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0006(0.0114) \n","Epoch 2 - avg_train_loss: 0.0124  avg_val_loss: 0.0114  time: 363s\n","Epoch 2 - Score: 0.8531\n","Epoch 2 - Save Best Score: 0.8531 Model\n","Epoch: [3][0/1438] Elapsed 0m 0s (remain 14m 24s) Loss: 0.0094(0.0094) Grad: 18070.8887  LR: 0.000013  \n","Epoch: [3][100/1438] Elapsed 0m 23s (remain 5m 5s) Loss: 0.0115(0.0108) Grad: 22300.3691  LR: 0.000013  \n","Epoch: [3][200/1438] Elapsed 0m 45s (remain 4m 37s) Loss: 0.0103(0.0108) Grad: 32507.4414  LR: 0.000013  \n","Epoch: [3][300/1438] Elapsed 1m 7s (remain 4m 13s) Loss: 0.0047(0.0101) Grad: 9389.1377  LR: 0.000012  \n","Epoch: [3][400/1438] Elapsed 1m 29s (remain 3m 50s) Loss: 0.0678(0.0097) Grad: 103590.0781  LR: 0.000012  \n","Epoch: [3][500/1438] Elapsed 1m 50s (remain 3m 27s) Loss: 0.0397(0.0095) Grad: 49309.0625  LR: 0.000012  \n","Epoch: [3][600/1438] Elapsed 2m 12s (remain 3m 5s) Loss: 0.0253(0.0096) Grad: 305852.3125  LR: 0.000011  \n","Epoch: [3][700/1438] Elapsed 2m 34s (remain 2m 42s) Loss: 0.0150(0.0094) Grad: 47196.2070  LR: 0.000011  \n","Epoch: [3][800/1438] Elapsed 2m 56s (remain 2m 20s) Loss: 0.0111(0.0093) Grad: 31087.6270  LR: 0.000011  \n","Epoch: [3][900/1438] Elapsed 3m 18s (remain 1m 58s) Loss: 0.0371(0.0092) Grad: 203377.0938  LR: 0.000011  \n","Epoch: [3][1000/1438] Elapsed 3m 40s (remain 1m 36s) Loss: 0.0017(0.0092) Grad: 11320.5449  LR: 0.000010  \n","Epoch: [3][1100/1438] Elapsed 4m 2s (remain 1m 14s) Loss: 0.0019(0.0091) Grad: 7044.6543  LR: 0.000010  \n","Epoch: [3][1200/1438] Elapsed 4m 24s (remain 0m 52s) Loss: 0.0013(0.0092) Grad: 29612.8613  LR: 0.000010  \n","Epoch: [3][1300/1438] Elapsed 4m 46s (remain 0m 30s) Loss: 0.0043(0.0093) Grad: 6030.6333  LR: 0.000009  \n","Epoch: [3][1400/1438] Elapsed 5m 8s (remain 0m 8s) Loss: 0.0043(0.0094) Grad: 9955.5361  LR: 0.000009  \n","Epoch: [3][1437/1438] Elapsed 5m 16s (remain 0m 0s) Loss: 0.0085(0.0094) Grad: 63177.5117  LR: 0.000009  \n","EVAL: [0/349] Elapsed 0m 0s (remain 2m 26s) Loss: 0.0055(0.0055) \n","EVAL: [100/349] Elapsed 0m 11s (remain 0m 29s) Loss: 0.0095(0.0112) \n","EVAL: [200/349] Elapsed 0m 23s (remain 0m 17s) Loss: 0.0137(0.0118) \n","EVAL: [300/349] Elapsed 0m 34s (remain 0m 5s) Loss: 0.0169(0.0122) \n","EVAL: [348/349] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0006(0.0115) \n","Epoch 3 - avg_train_loss: 0.0094  avg_val_loss: 0.0115  time: 362s\n","Epoch 3 - Score: 0.8627\n","Epoch 3 - Save Best Score: 0.8627 Model\n","Epoch: [4][0/1438] Elapsed 0m 0s (remain 14m 59s) Loss: 0.0031(0.0031) Grad: 20154.8789  LR: 0.000009  \n","Epoch: [4][100/1438] Elapsed 0m 22s (remain 5m 4s) Loss: 0.0129(0.0067) Grad: 36975.6914  LR: 0.000009  \n","Epoch: [4][200/1438] Elapsed 0m 44s (remain 4m 36s) Loss: 0.0051(0.0070) Grad: 43571.2305  LR: 0.000008  \n","Epoch: [4][300/1438] Elapsed 1m 6s (remain 4m 12s) Loss: 0.0007(0.0073) Grad: 3299.0486  LR: 0.000008  \n","Epoch: [4][400/1438] Elapsed 1m 28s (remain 3m 49s) Loss: 0.0003(0.0074) Grad: 1288.7104  LR: 0.000008  \n","Epoch: [4][500/1438] Elapsed 1m 50s (remain 3m 27s) Loss: 0.0313(0.0076) Grad: 97988.7266  LR: 0.000007  \n","Epoch: [4][600/1438] Elapsed 2m 12s (remain 3m 5s) Loss: 0.0040(0.0075) Grad: 21037.8926  LR: 0.000007  \n","Epoch: [4][700/1438] Elapsed 2m 34s (remain 2m 42s) Loss: 0.0045(0.0076) Grad: 20296.2988  LR: 0.000007  \n","Epoch: [4][800/1438] Elapsed 2m 56s (remain 2m 20s) Loss: 0.0037(0.0075) Grad: 16508.4785  LR: 0.000006  \n","Epoch: [4][900/1438] Elapsed 3m 18s (remain 1m 58s) Loss: 0.0104(0.0075) Grad: 23065.7891  LR: 0.000006  \n","Epoch: [4][1000/1438] Elapsed 3m 40s (remain 1m 36s) Loss: 0.0018(0.0074) Grad: 11868.7471  LR: 0.000006  \n","Epoch: [4][1100/1438] Elapsed 4m 2s (remain 1m 14s) Loss: 0.0013(0.0074) Grad: 66601.4922  LR: 0.000005  \n","Epoch: [4][1200/1438] Elapsed 4m 24s (remain 0m 52s) Loss: 0.0092(0.0075) Grad: 38786.5430  LR: 0.000005  \n","Epoch: [4][1300/1438] Elapsed 4m 46s (remain 0m 30s) Loss: 0.0084(0.0074) Grad: 31461.9922  LR: 0.000005  \n","Epoch: [4][1400/1438] Elapsed 5m 8s (remain 0m 8s) Loss: 0.0065(0.0074) Grad: 125351.0547  LR: 0.000005  \n","Epoch: [4][1437/1438] Elapsed 5m 16s (remain 0m 0s) Loss: 0.0011(0.0074) Grad: 17964.2734  LR: 0.000004  \n","EVAL: [0/349] Elapsed 0m 0s (remain 2m 25s) Loss: 0.0017(0.0017) \n","EVAL: [100/349] Elapsed 0m 11s (remain 0m 29s) Loss: 0.0101(0.0119) \n","EVAL: [200/349] Elapsed 0m 23s (remain 0m 17s) Loss: 0.0184(0.0127) \n","EVAL: [300/349] Elapsed 0m 34s (remain 0m 5s) Loss: 0.0153(0.0133) \n","EVAL: [348/349] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0004(0.0127) \n","Epoch 4 - avg_train_loss: 0.0074  avg_val_loss: 0.0127  time: 362s\n","Epoch 4 - Score: 0.8660\n","Epoch 4 - Save Best Score: 0.8660 Model\n","Epoch: [5][0/1438] Elapsed 0m 0s (remain 14m 52s) Loss: 0.0024(0.0024) Grad: 9009.8926  LR: 0.000004  \n","Epoch: [5][100/1438] Elapsed 0m 23s (remain 5m 5s) Loss: 0.0081(0.0046) Grad: 29900.4961  LR: 0.000004  \n","Epoch: [5][200/1438] Elapsed 0m 45s (remain 4m 37s) Loss: 0.0048(0.0056) Grad: 8940.8330  LR: 0.000004  \n","Epoch: [5][300/1438] Elapsed 1m 7s (remain 4m 13s) Loss: 0.0042(0.0060) Grad: 12997.0078  LR: 0.000004  \n","Epoch: [5][400/1438] Elapsed 1m 29s (remain 3m 50s) Loss: 0.0052(0.0060) Grad: 17162.6523  LR: 0.000003  \n","Epoch: [5][500/1438] Elapsed 1m 51s (remain 3m 27s) Loss: 0.0017(0.0061) Grad: 6650.3003  LR: 0.000003  \n","Epoch: [5][600/1438] Elapsed 2m 13s (remain 3m 5s) Loss: 0.0081(0.0061) Grad: 24488.9688  LR: 0.000003  \n","Epoch: [5][700/1438] Elapsed 2m 35s (remain 2m 42s) Loss: 0.0000(0.0061) Grad: 62.7570  LR: 0.000002  \n","Epoch: [5][800/1438] Elapsed 2m 56s (remain 2m 20s) Loss: 0.0009(0.0061) Grad: 7366.4951  LR: 0.000002  \n","Epoch: [5][900/1438] Elapsed 3m 18s (remain 1m 58s) Loss: 0.0086(0.0061) Grad: 15884.7217  LR: 0.000002  \n","Epoch: [5][1000/1438] Elapsed 3m 40s (remain 1m 36s) Loss: 0.0007(0.0060) Grad: 6469.0044  LR: 0.000001  \n","Epoch: [5][1100/1438] Elapsed 4m 2s (remain 1m 14s) Loss: 0.0163(0.0061) Grad: 17253.0742  LR: 0.000001  \n","Epoch: [5][1200/1438] Elapsed 4m 24s (remain 0m 52s) Loss: 0.0013(0.0060) Grad: 29957.6484  LR: 0.000001  \n","Epoch: [5][1300/1438] Elapsed 4m 46s (remain 0m 30s) Loss: 0.0017(0.0060) Grad: 32144.8438  LR: 0.000000  \n","Epoch: [5][1400/1438] Elapsed 5m 8s (remain 0m 8s) Loss: 0.0026(0.0060) Grad: 13603.1787  LR: 0.000000  \n","Epoch: [5][1437/1438] Elapsed 5m 17s (remain 0m 0s) Loss: 0.0073(0.0060) Grad: 27393.9453  LR: 0.000000  \n","EVAL: [0/349] Elapsed 0m 0s (remain 2m 15s) Loss: 0.0043(0.0043) \n","EVAL: [100/349] Elapsed 0m 11s (remain 0m 29s) Loss: 0.0111(0.0133) \n","EVAL: [200/349] Elapsed 0m 23s (remain 0m 17s) Loss: 0.0204(0.0142) \n","EVAL: [300/349] Elapsed 0m 34s (remain 0m 5s) Loss: 0.0151(0.0148) \n","EVAL: [348/349] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0002(0.0141) \n","Epoch 5 - avg_train_loss: 0.0060  avg_val_loss: 0.0141  time: 362s\n","Epoch 5 - Score: 0.8709\n","Epoch 5 - Save Best Score: 0.8709 Model\n","========== fold: 4 training ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: [1][0/1425] Elapsed 0m 0s (remain 14m 24s) Loss: 1.0857(1.0857) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/1425] Elapsed 0m 23s (remain 5m 4s) Loss: 0.0819(0.5536) Grad: 2765.7539  LR: 0.000003  \n","Epoch: [1][200/1425] Elapsed 0m 45s (remain 4m 35s) Loss: 0.0587(0.3063) Grad: 5891.1353  LR: 0.000006  \n","Epoch: [1][300/1425] Elapsed 1m 7s (remain 4m 11s) Loss: 0.0881(0.2188) Grad: 9443.4424  LR: 0.000008  \n","Epoch: [1][400/1425] Elapsed 1m 29s (remain 3m 48s) Loss: 0.0208(0.1731) Grad: 2249.7583  LR: 0.000011  \n","Epoch: [1][500/1425] Elapsed 1m 51s (remain 3m 25s) Loss: 0.0101(0.1441) Grad: 2044.3568  LR: 0.000014  \n","Epoch: [1][600/1425] Elapsed 2m 13s (remain 3m 3s) Loss: 0.0196(0.1244) Grad: 1927.8291  LR: 0.000017  \n","Epoch: [1][700/1425] Elapsed 2m 35s (remain 2m 40s) Loss: 0.0206(0.1099) Grad: 2645.6074  LR: 0.000020  \n","Epoch: [1][800/1425] Elapsed 2m 57s (remain 2m 18s) Loss: 0.0062(0.0989) Grad: 1454.6267  LR: 0.000020  \n","Epoch: [1][900/1425] Elapsed 3m 19s (remain 1m 56s) Loss: 0.0053(0.0905) Grad: 1154.7043  LR: 0.000019  \n","Epoch: [1][1000/1425] Elapsed 3m 41s (remain 1m 34s) Loss: 0.0188(0.0833) Grad: 2635.9595  LR: 0.000019  \n","Epoch: [1][1100/1425] Elapsed 4m 4s (remain 1m 11s) Loss: 0.0237(0.0776) Grad: 1209.7020  LR: 0.000019  \n","Epoch: [1][1200/1425] Elapsed 4m 26s (remain 0m 49s) Loss: 0.0343(0.0728) Grad: 1027.1421  LR: 0.000018  \n","Epoch: [1][1300/1425] Elapsed 4m 48s (remain 0m 27s) Loss: 0.0110(0.0686) Grad: 1086.7834  LR: 0.000018  \n","Epoch: [1][1400/1425] Elapsed 5m 10s (remain 0m 5s) Loss: 0.0647(0.0650) Grad: 4280.2217  LR: 0.000018  \n","Epoch: [1][1424/1425] Elapsed 5m 15s (remain 0m 0s) Loss: 0.0127(0.0642) Grad: 2318.3628  LR: 0.000018  \n","EVAL: [0/363] Elapsed 0m 0s (remain 2m 41s) Loss: 0.0189(0.0189) \n","EVAL: [100/363] Elapsed 0m 11s (remain 0m 30s) Loss: 0.0160(0.0169) \n","EVAL: [200/363] Elapsed 0m 23s (remain 0m 18s) Loss: 0.0608(0.0175) \n","EVAL: [300/363] Elapsed 0m 34s (remain 0m 7s) Loss: 0.0042(0.0171) \n","EVAL: [362/363] Elapsed 0m 41s (remain 0m 0s) Loss: 0.0073(0.0157) \n","Epoch 1 - avg_train_loss: 0.0642  avg_val_loss: 0.0157  time: 362s\n","Epoch 1 - Score: 0.8130\n","Epoch 1 - Save Best Score: 0.8130 Model\n","Epoch: [2][0/1425] Elapsed 0m 0s (remain 15m 28s) Loss: 0.0173(0.0173) Grad: 24072.3145  LR: 0.000018  \n","Epoch: [2][100/1425] Elapsed 0m 23s (remain 5m 3s) Loss: 0.0211(0.0113) Grad: 27357.9570  LR: 0.000017  \n","Epoch: [2][200/1425] Elapsed 0m 45s (remain 4m 35s) Loss: 0.0047(0.0121) Grad: 17209.8984  LR: 0.000017  \n","Epoch: [2][300/1425] Elapsed 1m 7s (remain 4m 10s) Loss: 0.0101(0.0124) Grad: 18205.7109  LR: 0.000017  \n","Epoch: [2][400/1425] Elapsed 1m 29s (remain 3m 47s) Loss: 0.0548(0.0123) Grad: 97270.7969  LR: 0.000017  \n","Epoch: [2][500/1425] Elapsed 1m 51s (remain 3m 25s) Loss: 0.0080(0.0121) Grad: 13424.8330  LR: 0.000016  \n","Epoch: [2][600/1425] Elapsed 2m 13s (remain 3m 2s) Loss: 0.0039(0.0124) Grad: 15869.9814  LR: 0.000016  \n","Epoch: [2][700/1425] Elapsed 2m 35s (remain 2m 40s) Loss: 0.0375(0.0126) Grad: 100063.3281  LR: 0.000016  \n","Epoch: [2][800/1425] Elapsed 2m 57s (remain 2m 18s) Loss: 0.0207(0.0129) Grad: 118864.3516  LR: 0.000015  \n","Epoch: [2][900/1425] Elapsed 3m 19s (remain 1m 56s) Loss: 0.0075(0.0128) Grad: 15414.0010  LR: 0.000015  \n","Epoch: [2][1000/1425] Elapsed 3m 41s (remain 1m 33s) Loss: 0.0099(0.0126) Grad: 31088.4980  LR: 0.000015  \n","Epoch: [2][1100/1425] Elapsed 4m 3s (remain 1m 11s) Loss: 0.0197(0.0125) Grad: 108006.7422  LR: 0.000014  \n","Epoch: [2][1200/1425] Elapsed 4m 25s (remain 0m 49s) Loss: 0.0033(0.0124) Grad: 10259.3701  LR: 0.000014  \n","Epoch: [2][1300/1425] Elapsed 4m 47s (remain 0m 27s) Loss: 0.0079(0.0124) Grad: 16988.7246  LR: 0.000014  \n","Epoch: [2][1400/1425] Elapsed 5m 9s (remain 0m 5s) Loss: 0.0049(0.0124) Grad: 13123.9248  LR: 0.000013  \n","Epoch: [2][1424/1425] Elapsed 5m 14s (remain 0m 0s) Loss: 0.0070(0.0124) Grad: 21437.3066  LR: 0.000013  \n","EVAL: [0/363] Elapsed 0m 0s (remain 2m 36s) Loss: 0.0341(0.0341) \n","EVAL: [100/363] Elapsed 0m 11s (remain 0m 30s) Loss: 0.0075(0.0152) \n","EVAL: [200/363] Elapsed 0m 23s (remain 0m 18s) Loss: 0.1028(0.0156) \n","EVAL: [300/363] Elapsed 0m 34s (remain 0m 7s) Loss: 0.0032(0.0150) \n","EVAL: [362/363] Elapsed 0m 41s (remain 0m 0s) Loss: 0.0071(0.0135) \n","Epoch 2 - avg_train_loss: 0.0124  avg_val_loss: 0.0135  time: 361s\n","Epoch 2 - Score: 0.8506\n","Epoch 2 - Save Best Score: 0.8506 Model\n","Epoch: [3][0/1425] Elapsed 0m 0s (remain 15m 32s) Loss: 0.0104(0.0104) Grad: 48263.6953  LR: 0.000013  \n","Epoch: [3][100/1425] Elapsed 0m 23s (remain 5m 2s) Loss: 0.0167(0.0110) Grad: 64324.3164  LR: 0.000013  \n","Epoch: [3][200/1425] Elapsed 0m 45s (remain 4m 34s) Loss: 0.0313(0.0105) Grad: 79287.5938  LR: 0.000013  \n","Epoch: [3][300/1425] Elapsed 1m 7s (remain 4m 10s) Loss: 0.0044(0.0100) Grad: 12458.9834  LR: 0.000012  \n","Epoch: [3][400/1425] Elapsed 1m 29s (remain 3m 47s) Loss: 0.0094(0.0094) Grad: 13731.5898  LR: 0.000012  \n","Epoch: [3][500/1425] Elapsed 1m 51s (remain 3m 25s) Loss: 0.0060(0.0094) Grad: 15801.1992  LR: 0.000012  \n","Epoch: [3][600/1425] Elapsed 2m 13s (remain 3m 2s) Loss: 0.0034(0.0094) Grad: 8146.3770  LR: 0.000011  \n","Epoch: [3][700/1425] Elapsed 2m 35s (remain 2m 40s) Loss: 0.0093(0.0094) Grad: 20030.6680  LR: 0.000011  \n","Epoch: [3][800/1425] Elapsed 2m 57s (remain 2m 18s) Loss: 0.0071(0.0095) Grad: 23440.7461  LR: 0.000011  \n","Epoch: [3][900/1425] Elapsed 3m 19s (remain 1m 55s) Loss: 0.0062(0.0095) Grad: 19516.3594  LR: 0.000011  \n","Epoch: [3][1000/1425] Elapsed 3m 41s (remain 1m 33s) Loss: 0.0089(0.0096) Grad: 15142.1670  LR: 0.000010  \n","Epoch: [3][1100/1425] Elapsed 4m 3s (remain 1m 11s) Loss: 0.0065(0.0095) Grad: 25312.7266  LR: 0.000010  \n","Epoch: [3][1200/1425] Elapsed 4m 25s (remain 0m 49s) Loss: 0.0082(0.0095) Grad: 42637.6523  LR: 0.000010  \n","Epoch: [3][1300/1425] Elapsed 4m 47s (remain 0m 27s) Loss: 0.0110(0.0094) Grad: 23238.8047  LR: 0.000009  \n","Epoch: [3][1400/1425] Elapsed 5m 9s (remain 0m 5s) Loss: 0.0027(0.0094) Grad: 18481.6191  LR: 0.000009  \n","Epoch: [3][1424/1425] Elapsed 5m 14s (remain 0m 0s) Loss: 0.0079(0.0094) Grad: 28927.3770  LR: 0.000009  \n","EVAL: [0/363] Elapsed 0m 0s (remain 2m 30s) Loss: 0.0188(0.0188) \n","EVAL: [100/363] Elapsed 0m 11s (remain 0m 30s) Loss: 0.0115(0.0163) \n","EVAL: [200/363] Elapsed 0m 23s (remain 0m 18s) Loss: 0.0652(0.0162) \n","EVAL: [300/363] Elapsed 0m 34s (remain 0m 7s) Loss: 0.0053(0.0156) \n","EVAL: [362/363] Elapsed 0m 41s (remain 0m 0s) Loss: 0.0076(0.0139) \n","Epoch 3 - avg_train_loss: 0.0094  avg_val_loss: 0.0139  time: 361s\n","Epoch 3 - Score: 0.8563\n","Epoch 3 - Save Best Score: 0.8563 Model\n","Epoch: [4][0/1425] Elapsed 0m 0s (remain 16m 37s) Loss: 0.0116(0.0116) Grad: 27679.7461  LR: 0.000009  \n","Epoch: [4][100/1425] Elapsed 0m 23s (remain 5m 4s) Loss: 0.0060(0.0082) Grad: 36211.6875  LR: 0.000009  \n","Epoch: [4][200/1425] Elapsed 0m 45s (remain 4m 35s) Loss: 0.0054(0.0077) Grad: 32198.4902  LR: 0.000008  \n","Epoch: [4][300/1425] Elapsed 1m 7s (remain 4m 11s) Loss: 0.0276(0.0075) Grad: 58347.9141  LR: 0.000008  \n","Epoch: [4][400/1425] Elapsed 1m 29s (remain 3m 48s) Loss: 0.0119(0.0072) Grad: 45656.7031  LR: 0.000008  \n","Epoch: [4][500/1425] Elapsed 1m 51s (remain 3m 25s) Loss: 0.0098(0.0071) Grad: 19732.3262  LR: 0.000007  \n","Epoch: [4][600/1425] Elapsed 2m 13s (remain 3m 2s) Loss: 0.0067(0.0073) Grad: 22475.8691  LR: 0.000007  \n","Epoch: [4][700/1425] Elapsed 2m 35s (remain 2m 40s) Loss: 0.0041(0.0071) Grad: 41617.8867  LR: 0.000007  \n","Epoch: [4][800/1425] Elapsed 2m 57s (remain 2m 18s) Loss: 0.0001(0.0071) Grad: 514.0217  LR: 0.000006  \n","Epoch: [4][900/1425] Elapsed 3m 19s (remain 1m 56s) Loss: 0.0112(0.0071) Grad: 90902.1094  LR: 0.000006  \n","Epoch: [4][1000/1425] Elapsed 3m 41s (remain 1m 33s) Loss: 0.0021(0.0072) Grad: 7234.2153  LR: 0.000006  \n","Epoch: [4][1100/1425] Elapsed 4m 3s (remain 1m 11s) Loss: 0.0098(0.0071) Grad: 11063.9336  LR: 0.000005  \n","Epoch: [4][1200/1425] Elapsed 4m 25s (remain 0m 49s) Loss: 0.0028(0.0073) Grad: 11661.4277  LR: 0.000005  \n","Epoch: [4][1300/1425] Elapsed 4m 47s (remain 0m 27s) Loss: 0.0008(0.0072) Grad: 6410.6245  LR: 0.000005  \n","Epoch: [4][1400/1425] Elapsed 5m 9s (remain 0m 5s) Loss: 0.0014(0.0074) Grad: 8704.6885  LR: 0.000005  \n","Epoch: [4][1424/1425] Elapsed 5m 14s (remain 0m 0s) Loss: 0.0054(0.0074) Grad: 16115.7041  LR: 0.000004  \n","EVAL: [0/363] Elapsed 0m 0s (remain 2m 47s) Loss: 0.0111(0.0111) \n","EVAL: [100/363] Elapsed 0m 11s (remain 0m 30s) Loss: 0.0086(0.0160) \n","EVAL: [200/363] Elapsed 0m 23s (remain 0m 18s) Loss: 0.0691(0.0160) \n","EVAL: [300/363] Elapsed 0m 34s (remain 0m 7s) Loss: 0.0084(0.0154) \n","EVAL: [362/363] Elapsed 0m 41s (remain 0m 0s) Loss: 0.0078(0.0138) \n","Epoch 4 - avg_train_loss: 0.0074  avg_val_loss: 0.0138  time: 361s\n","Epoch 4 - Score: 0.8644\n","Epoch 4 - Save Best Score: 0.8644 Model\n","Epoch: [5][0/1425] Elapsed 0m 0s (remain 15m 5s) Loss: 0.0001(0.0001) Grad: 431.8732  LR: 0.000004  \n","Epoch: [5][100/1425] Elapsed 0m 23s (remain 5m 3s) Loss: 0.0060(0.0060) Grad: 16794.5879  LR: 0.000004  \n","Epoch: [5][200/1425] Elapsed 0m 45s (remain 4m 35s) Loss: 0.0006(0.0056) Grad: 6593.2241  LR: 0.000004  \n","Epoch: [5][300/1425] Elapsed 1m 7s (remain 4m 10s) Loss: 0.0040(0.0058) Grad: 10466.0127  LR: 0.000004  \n","Epoch: [5][400/1425] Elapsed 1m 29s (remain 3m 47s) Loss: 0.0123(0.0059) Grad: 40171.7773  LR: 0.000003  \n","Epoch: [5][500/1425] Elapsed 1m 51s (remain 3m 25s) Loss: 0.0085(0.0059) Grad: 23433.5938  LR: 0.000003  \n","Epoch: [5][600/1425] Elapsed 2m 13s (remain 3m 2s) Loss: 0.0091(0.0059) Grad: 87091.8594  LR: 0.000003  \n","Epoch: [5][700/1425] Elapsed 2m 35s (remain 2m 40s) Loss: 0.0137(0.0061) Grad: 37065.5273  LR: 0.000002  \n","Epoch: [5][800/1425] Elapsed 2m 57s (remain 2m 18s) Loss: 0.0002(0.0061) Grad: 1107.9355  LR: 0.000002  \n","Epoch: [5][900/1425] Elapsed 3m 19s (remain 1m 55s) Loss: 0.0026(0.0061) Grad: 21594.2051  LR: 0.000002  \n","Epoch: [5][1000/1425] Elapsed 3m 41s (remain 1m 33s) Loss: 0.0061(0.0061) Grad: 19892.6777  LR: 0.000001  \n","Epoch: [5][1100/1425] Elapsed 4m 3s (remain 1m 11s) Loss: 0.0083(0.0061) Grad: 38390.6055  LR: 0.000001  \n","Epoch: [5][1200/1425] Elapsed 4m 25s (remain 0m 49s) Loss: 0.0144(0.0061) Grad: 64765.9609  LR: 0.000001  \n","Epoch: [5][1300/1425] Elapsed 4m 47s (remain 0m 27s) Loss: 0.0050(0.0061) Grad: 17561.6934  LR: 0.000000  \n","Epoch: [5][1400/1425] Elapsed 5m 9s (remain 0m 5s) Loss: 0.0014(0.0061) Grad: 7181.2090  LR: 0.000000  \n","Epoch: [5][1424/1425] Elapsed 5m 14s (remain 0m 0s) Loss: 0.0057(0.0061) Grad: 25710.4492  LR: 0.000000  \n","EVAL: [0/363] Elapsed 0m 0s (remain 2m 31s) Loss: 0.0234(0.0234) \n","EVAL: [100/363] Elapsed 0m 11s (remain 0m 30s) Loss: 0.0075(0.0177) \n","EVAL: [200/363] Elapsed 0m 23s (remain 0m 18s) Loss: 0.0758(0.0174) \n","EVAL: [300/363] Elapsed 0m 34s (remain 0m 7s) Loss: 0.0111(0.0171) \n","EVAL: [362/363] Elapsed 0m 41s (remain 0m 0s) Loss: 0.0079(0.0153) \n","Epoch 5 - avg_train_loss: 0.0061  avg_val_loss: 0.0153  time: 361s\n","Epoch 5 - Score: 0.8634\n","Best thres: 0.5, Score: 0.8684\n","Best thres: 0.52509765625, Score: 0.8685\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0d9ebef7937448ea8f674a0be608eac0","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d826574b7c94347947097cf95a07ba8","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception ignored in: <function _ConnectionBase.__del__ at 0x7fb978010b00>\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 132, in __del__\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"726bd7fa25a444b39a54ebd2f1074906","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception ignored in: <function _ConnectionBase.__del__ at 0x7fb978010b00>\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 132, in __del__\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"faa0b9e403854d628755f5fc755df0e6","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception ignored in: <function _ConnectionBase.__del__ at 0x7fb978010b00>\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 132, in __del__\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18591825905749078d7be86d5d8f40b7","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception ignored in: <function _ConnectionBase.__del__ at 0x7fb978010b00>\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 132, in __del__\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n"]}],"source":["if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"colab":{"name":"nbme-exp002.ipynb","provenance":[],"collapsed_sections":[],"background_execution":"on"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1bf8493b04954d95adaa9556b1c26ec5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_02013be4b6c54ad5b15b3c42dfb63f26","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3012c34221dd47bcbeef7182d37cb9a9","IPY_MODEL_9db50339f80b4d5dbb02dacfa54cc098","IPY_MODEL_e5cfce60f4dc4892a127e0893df4cd77"]}},"02013be4b6c54ad5b15b3c42dfb63f26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3012c34221dd47bcbeef7182d37cb9a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9e28131aa525418cb798eade8db9d198","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e55b4dff79574f2990d420a59440ae60"}},"9db50339f80b4d5dbb02dacfa54cc098":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_097ea871087d4ac08e85a97cb97911bf","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":42146,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":42146,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d0b32231f103460ebb8418986c8bae2d"}},"e5cfce60f4dc4892a127e0893df4cd77":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8d05be7c219c4564b98aa8d044be471e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 42146/42146 [00:33&lt;00:00, 2010.92it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b7b67a16aedc4824b779facb0a58a2ce"}},"9e28131aa525418cb798eade8db9d198":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e55b4dff79574f2990d420a59440ae60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"097ea871087d4ac08e85a97cb97911bf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d0b32231f103460ebb8418986c8bae2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8d05be7c219c4564b98aa8d044be471e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b7b67a16aedc4824b779facb0a58a2ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3e9a4a45fec04036a7e4150b3e9bd057":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f6d595e260f24414938d6547bba5fb81","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_35d081ebff274bcc8e2a8759a1a22991","IPY_MODEL_7e7e11038d0d4b688c8f0e9405fe8eb7","IPY_MODEL_3fa1fe37240c4b639dec024bf5ba7e8e"]}},"f6d595e260f24414938d6547bba5fb81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"35d081ebff274bcc8e2a8759a1a22991":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ff301805e5a840b9a5ecd86bd113c4f4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2a1d95e09ef44017b9c857f417921c07"}},"7e7e11038d0d4b688c8f0e9405fe8eb7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b5c169f1d9574a71956e4c9ed93e3389","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":143,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":143,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e5b75a0e3974447e8c4e4bf8bb1ef9ae"}},"3fa1fe37240c4b639dec024bf5ba7e8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6389bd084c6c4c10b35255d3bbc78b32","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 143/143 [00:00&lt;00:00, 2297.37it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_50e67d9ac46a4238b110490c32d8895b"}},"ff301805e5a840b9a5ecd86bd113c4f4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2a1d95e09ef44017b9c857f417921c07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b5c169f1d9574a71956e4c9ed93e3389":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e5b75a0e3974447e8c4e4bf8bb1ef9ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6389bd084c6c4c10b35255d3bbc78b32":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"50e67d9ac46a4238b110490c32d8895b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0d9ebef7937448ea8f674a0be608eac0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_80ed937bfa8b4d6583e75192e5751f60","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e71730d0da3345a08306022932a3fc44","IPY_MODEL_835e194f801c43ebb3e7f288efbade19","IPY_MODEL_fa9e6c6ee4264abba8d76e4a3487c92f"]}},"80ed937bfa8b4d6583e75192e5751f60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e71730d0da3345a08306022932a3fc44":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f794d8da46ab4e0b8845189e2afb9445","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_548b2bc7cc9244a194b159c382f63769"}},"835e194f801c43ebb3e7f288efbade19":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a5aa63e0b6464ec7a6a1c2673b1b70ef","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6653d082286742d39916093a01865a55"}},"fa9e6c6ee4264abba8d76e4a3487c92f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_dae4cc2c197a403fbfdc337a3006994c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:01&lt;00:00,  1.28it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5b5a918c526d4c9c97aefced65b33236"}},"f794d8da46ab4e0b8845189e2afb9445":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"548b2bc7cc9244a194b159c382f63769":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a5aa63e0b6464ec7a6a1c2673b1b70ef":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6653d082286742d39916093a01865a55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dae4cc2c197a403fbfdc337a3006994c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5b5a918c526d4c9c97aefced65b33236":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7d826574b7c94347947097cf95a07ba8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ae88d591d71e4e1eaf903f54ec2fa378","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c4df2f9fdaee4cb0baa4cdb61ad8509c","IPY_MODEL_68a1b3e79cb84b8794b8db5fdfd6258d","IPY_MODEL_6e2c4d74af6d4a31b0c9c8fe41bc990c"]}},"ae88d591d71e4e1eaf903f54ec2fa378":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c4df2f9fdaee4cb0baa4cdb61ad8509c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3687acea47364461ad976dc759b9fd37","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_670906e53bb4491e91cf2b5149f58e09"}},"68a1b3e79cb84b8794b8db5fdfd6258d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_65d48eb6ea2b4579bd03995bc3d30e6b","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_829949a4fce1433ebf4488238f539302"}},"6e2c4d74af6d4a31b0c9c8fe41bc990c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c312abd88bb94935a7d6c8cd6cf08d9f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:01&lt;00:00,  1.22it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0146c65dbfe84fd88d60fcd71dc22e88"}},"3687acea47364461ad976dc759b9fd37":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"670906e53bb4491e91cf2b5149f58e09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"65d48eb6ea2b4579bd03995bc3d30e6b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"829949a4fce1433ebf4488238f539302":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c312abd88bb94935a7d6c8cd6cf08d9f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0146c65dbfe84fd88d60fcd71dc22e88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"726bd7fa25a444b39a54ebd2f1074906":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_31e3d1f11e4b445382ed586eabf11541","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9c2edd474604437d9f30bf055eedfc42","IPY_MODEL_5ba75f0e32d940ad9a811ca3131b0db6","IPY_MODEL_a878d1591583475785b9b99ffd1bfe86"]}},"31e3d1f11e4b445382ed586eabf11541":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9c2edd474604437d9f30bf055eedfc42":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9b9ff8855fe34930bec9bd9feb8ab719","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d44e74bb4cda4685bdee0246a1709a75"}},"5ba75f0e32d940ad9a811ca3131b0db6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6f2d4ec95069450380784810a1bdbee1","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ebfd69f8fb2044c8a2a80b56684f28b6"}},"a878d1591583475785b9b99ffd1bfe86":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0be5f0ced8d54aef862b91e6af484bc3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:01&lt;00:00,  1.19it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_34b57bf63a674edc917adfffebc9ff39"}},"9b9ff8855fe34930bec9bd9feb8ab719":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d44e74bb4cda4685bdee0246a1709a75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6f2d4ec95069450380784810a1bdbee1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ebfd69f8fb2044c8a2a80b56684f28b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0be5f0ced8d54aef862b91e6af484bc3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"34b57bf63a674edc917adfffebc9ff39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"faa0b9e403854d628755f5fc755df0e6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8b377c36c9f6411caf96ae4979ca8523","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7d3d9eb12bdb41de97259cfa7108de59","IPY_MODEL_3f4536b288164897a387d16638269fcf","IPY_MODEL_2de09571d5bf41d69b0b9aef57fc3b18"]}},"8b377c36c9f6411caf96ae4979ca8523":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7d3d9eb12bdb41de97259cfa7108de59":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a2b6ef66e28841dc9f426be385d91c61","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dbb220a47b384d2690d82401a28c4d03"}},"3f4536b288164897a387d16638269fcf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e10ef4d69bea46d9978058b42d1128f0","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bd01ce71fa5640a9a63c999932069c3d"}},"2de09571d5bf41d69b0b9aef57fc3b18":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_82f58599156841e6a4300ac0a7f85369","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:01&lt;00:00,  1.03it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_690568c77ac24786b7e44c83bb222298"}},"a2b6ef66e28841dc9f426be385d91c61":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"dbb220a47b384d2690d82401a28c4d03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e10ef4d69bea46d9978058b42d1128f0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"bd01ce71fa5640a9a63c999932069c3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"82f58599156841e6a4300ac0a7f85369":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"690568c77ac24786b7e44c83bb222298":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"18591825905749078d7be86d5d8f40b7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a1f7a0bed23f4566a2848ec8fa27f700","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8c8e9d7990cd4c6495ff4655e83a5539","IPY_MODEL_b890a083095544e682a8970000dd98f6","IPY_MODEL_191b5134210f49efbbf847b7bd3bd3e4"]}},"a1f7a0bed23f4566a2848ec8fa27f700":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8c8e9d7990cd4c6495ff4655e83a5539":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6dafb06ce46c4d12986651c11737fcd7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e76c4daafd154830adac192c70cbd2b2"}},"b890a083095544e682a8970000dd98f6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a39e68b36c934f1cb16e709f7b4f5e94","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ff497745794d4775988c38bd324d308a"}},"191b5134210f49efbbf847b7bd3bd3e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d122b49dd1774adeb9e0429537855fb9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:01&lt;00:00,  1.03s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fd1ef1f6bacb497fbc5e2b47c2e05e92"}},"6dafb06ce46c4d12986651c11737fcd7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e76c4daafd154830adac192c70cbd2b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a39e68b36c934f1cb16e709f7b4f5e94":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ff497745794d4775988c38bd324d308a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d122b49dd1774adeb9e0429537855fb9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fd1ef1f6bacb497fbc5e2b47c2e05e92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":5}