{"cells":[{"cell_type":"markdown","metadata":{"id":"colored-security"},"source":["## References"]},{"cell_type":"markdown","metadata":{"id":"educational-operator"},"source":["- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"]},{"cell_type":"markdown","metadata":{"id":"incorrect-greek"},"source":["## Configurations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"alive-granny"},"outputs":[],"source":["EXP_NAME = \"nbme-exp068\"\n","ENV = \"colab\"\n","DEBUG_MODE = False\n","SUBMISSION_MODE = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"heavy-prophet"},"outputs":[],"source":["class CFG:\n","    env=ENV\n","    exp_name=EXP_NAME\n","    debug=DEBUG_MODE\n","    submission=SUBMISSION_MODE\n","    apex=True\n","    input_dir=None\n","    output_dir=None\n","    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n","    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n","    competition_name=\"nbme-score-clinical-patient-notes\"\n","    id_col=\"id\"\n","    target_col=\"location\"\n","    pretrained_model_name=\"microsoft/deberta-large\"\n","    tokenizer=None\n","    max_len=None\n","    max_char_len=None\n","    output_dim=1\n","    dropout=0.2\n","    num_workers=4\n","    batch_size=3\n","    lr=2e-5\n","    betas=(0.9, 0.98)\n","    weight_decay=0.1\n","    num_warmup_steps_rate=0.1\n","    batch_scheduler=True\n","    epochs=5\n","    n_fold=4\n","    train_fold=[1, 2, 3]  # [0, 1, 2, 3]\n","    seed=71\n","    gradient_accumulation_steps=2\n","    max_grad_norm=1000\n","    print_freq=100\n","    train=True\n","    inference=True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vocational-coating"},"outputs":[],"source":["if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.train_fold = [0, 1]\n","\n","if CFG.submission:\n","    CFG.train = False\n","    CFG.inference = True"]},{"cell_type":"markdown","metadata":{"id":"private-moderator"},"source":["## Directory Settings"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27657,"status":"ok","timestamp":1648598822381,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"married-tokyo","outputId":"9c0fba66-759b-4354-898f-1afb47256d96"},"outputs":[{"name":"stdout","output_type":"stream","text":["colab\n","Mounted at /content/drive\n","Collecting transformers==4.16.2\n","  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 3.7 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.63.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.11.3)\n","Collecting pyyaml\u003e=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 77.0 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 79.1 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,\u003e=0.10.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 77.7 MB/s \n","\u001b[?25hCollecting huggingface-hub\u003c1.0,\u003e=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 6.8 MB/s \n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (1.21.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2019.12.20)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (3.6.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.1.0-\u003etransformers==4.16.2) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=20.0-\u003etransformers==4.16.2) (3.0.7)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003etransformers==4.16.2) (3.7.0)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==4.16.2) (3.0.4)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==4.16.2) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==4.16.2) (1.24.3)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==4.16.2) (2021.10.8)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers==4.16.2) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers==4.16.2) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers==4.16.2) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.16.2\n"]}],"source":["import sys\n","from pathlib import Path\n","\n","\n","print(CFG.env)\n","if CFG.env == \"colab\":\n","    # colab環境\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","    # install packages\n","    !pip install transformers==4.16.2\n","\n","elif CFG.env == \"local\":\n","    # ローカルサーバ\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","\n","elif CFG.env == \"kaggle\":\n","    # kaggle環境\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"blank-pierre"},"outputs":[],"source":["import gc\n","import os\n","import ast\n","import time\n","import math\n","import random\n","import itertools\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","from scipy.optimize import minimize\n","from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as T\n","from torchvision.io import read_image\n","from torch.utils.data import DataLoader, Dataset\n","\n","from transformers import AutoModelForMaskedLM\n","from transformers import BartModel,BertModel,BertTokenizer\n","from transformers import DebertaModel,DebertaTokenizer\n","from transformers import RobertaModel,RobertaTokenizer\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"sound-still"},"source":["## Utilities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"surprised-commercial"},"outputs":[],"source":["def micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on binary arrays.\n","\n","    Args:\n","        preds (list of lists of ints): Predictions.\n","        truths (list of lists of ints): Ground truths.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    # Micro : aggregating over all instances\n","    preds = np.concatenate(preds)\n","    truths = np.concatenate(truths)\n","    return f1_score(truths, preds)\n","\n","\n","def spans_to_binary(spans, length=None):\n","    \"\"\"\n","    Converts spans to a binary array indicating whether each character is in the span.\n","\n","    Args:\n","        spans (list of lists of two ints): Spans.\n","\n","    Returns:\n","        np array [length]: Binarized spans.\n","    \"\"\"\n","    length = np.max(spans) if length is None else length\n","    binary = np.zeros(length)\n","    for start, end in spans:\n","        binary[start:end] = 1\n","    return binary\n","\n","\n","def span_micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on spans.\n","\n","    Args:\n","        preds (list of lists of two ints): Prediction spans.\n","        truths (list of lists of two ints): Ground truth spans.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    bin_preds = []\n","    bin_truths = []\n","    for pred, truth in zip(preds, truths):\n","        if not len(pred) and not len(truth):\n","            continue\n","        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n","        bin_preds.append(spans_to_binary(pred, length))\n","        bin_truths.append(spans_to_binary(truth, length))\n","    return micro_f1(bin_preds, bin_truths)\n","\n","\n","def get_score(y_true, y_pred):\n","    score = span_micro_f1(y_true, y_pred)\n","    return score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"interstate-accident"},"outputs":[],"source":["def create_labels_for_scoring(df):\n","    # example: ['48 61', '111 128'] -\u003e [[48, 61], [111, 128]]\n","    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n","    for i in range(len(df)):\n","        lst = df.loc[i, \"location\"]\n","        if lst:\n","            new_lst = \";\".join(lst)\n","            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n","\n","    # create labels\n","    truths = []\n","    for location_list in df[\"location_for_create_labels\"].values:\n","        truth = []\n","        if len(location_list) \u003e 0:\n","            location = location_list[0]\n","            for loc in [s.split() for s in location.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                truth.append([start, end])\n","        truths.append(truth)\n","\n","    return truths\n","\n","\n","def get_char_probs(texts, token_probs, tokenizer):\n","    res = [np.zeros(len(t)) for t in texts]\n","    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n","        encoded = tokenizer(\n","            text=text,\n","            max_length=CFG.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n","            start, end = offset_mapping\n","            res[i][start:end] = pred\n","    return res\n","\n","\n","def get_predicted_location_str(char_probs, th=0.5):\n","    results = []\n","    for char_prob in char_probs:\n","        # result = np.where(char_prob \u003e= th)[0] + 1\n","        result = np.where(char_prob \u003e= th)[0]\n","        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n","        # result = [f\"{min(r)} {max(r)}\" for r in result]\n","        result = [f\"{min(r)} {max(r) + 1}\" for r in result]\n","        result = \";\".join(result)\n","        results.append(result)\n","    return results\n","\n","\n","def get_predictions(results):\n","    predictions = []\n","    for result in results:\n","        prediction = []\n","        if result != \"\":\n","            for loc in [s.split() for s in result.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                prediction.append([start, end])\n","        predictions.append(prediction)\n","    return predictions\n","\n","\n","def scoring(df, th=0.5, use_token_prob=True):\n","    labels = create_labels_for_scoring(df)\n","\n","    if use_token_prob:\n","        token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n","        char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n","    else:\n","        char_probs = df[[str(i) for i in range(CFG.max_char_len)]].values\n","        char_probs = [char_probs[i] for i in range(len(char_probs))]\n","\n","    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n","    preds = get_predictions(predicted_location_str)\n","\n","    score = get_score(labels, preds)\n","    return score\n","\n","\n","def get_best_thres(oof_df):\n","    def f1_opt(x):\n","        return -1 * scoring(oof_df, th=x)\n","\n","    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n","    return best_thres"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"coated-pioneer"},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return \"%dm %ds\" % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nervous-delaware"},"outputs":[],"source":["seed_everything()"]},{"cell_type":"markdown","metadata":{"id":"functioning-destruction"},"source":["## Data Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1943,"status":"ok","timestamp":1648598832931,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"global-monte","outputId":"77675ef9-9cb4-44a4-ebeb-ae9e74d7ebd7"},"outputs":[{"data":{"text/plain":["((14300, 6), (143, 3), (42146, 3), (5, 4))"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["train = pd.read_csv(CFG.input_dir / \"train.csv\")\n","features = pd.read_csv(CFG.input_dir / \"features.csv\")\n","patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n","test = pd.read_csv(CFG.input_dir / \"test.csv\")\n","\n","train.shape, features.shape, patient_notes.shape, test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"independent-airfare"},"outputs":[],"source":["if CFG.debug:\n","    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n","    print(train.shape)"]},{"cell_type":"markdown","metadata":{"id":"silent-locator"},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"unusual-fifty"},"outputs":[],"source":["def preprocess_features(features):\n","    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n","    return features\n","\n","\n","features = preprocess_features(features)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":273,"status":"ok","timestamp":1648598833200,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"decreased-mustang","outputId":"61c7d744-fde7-4d3c-e0c2-8393e24159b9"},"outputs":[{"data":{"text/plain":["((14300, 8), (5, 6))"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","\n","train.shape, test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"boolean-trade"},"outputs":[],"source":["train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n","train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1648598833878,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"accomplished-dakota","outputId":"0c1b9ec8-3c61-4a44-bafe-fb9ea649d668"},"outputs":[{"data":{"text/plain":["0    4399\n","1    8181\n","2    1296\n","3     287\n","4      99\n","5      27\n","6       9\n","7       1\n","8       1\n","Name: annotation_length, dtype: int64"]},"metadata":{},"output_type":"display_data"}],"source":["train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n","display(train['annotation_length'].value_counts().sort_index())"]},{"cell_type":"markdown","metadata":{"id":"funded-elizabeth"},"source":["## CV split"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1648598833878,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"unexpected-columbia","outputId":"e4b2bd12-a470-45e9-89f8-18f01bfb8836"},"outputs":[{"data":{"text/plain":["fold\n","0    3575\n","1    3575\n","2    3575\n","3    3575\n","dtype: int64"]},"metadata":{},"output_type":"display_data"}],"source":["Fold = GroupKFold(n_splits=CFG.n_fold)\n","groups = train['pn_num'].values\n","for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n","    train.loc[val_index, 'fold'] = int(n)\n","train['fold'] = train['fold'].astype(int)\n","display(train.groupby('fold').size())"]},{"cell_type":"markdown","metadata":{"id":"critical-archive"},"source":["## Setup tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145},"executionInfo":{"elapsed":3043,"status":"ok","timestamp":1648598836917,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"broken-generator","outputId":"9ed5f1df-2a6b-4b0d-bbfb-dd49474e17ea"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9826121100004ec49f1cd7ed26023d9f","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/52.0 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a2fc47910e5a4b158eddf7faa455d683","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/475 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f61172e130474199999fe10c8470b1e8","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/878k [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0c748174ece64ca6992b434a2d92e1e4","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/446k [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["if CFG.submission:\n","    tokenizer = AutoTokenizer.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n","else:\n","    tokenizer = AutoTokenizer.from_pretrained(CFG.pretrained_model_name)\n","    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n","\n","CFG.tokenizer = tokenizer"]},{"cell_type":"markdown","metadata":{"id":"compatible-lincoln"},"source":["## Create dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"elapsed":22110,"status":"ok","timestamp":1648598859021,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"fluid-nancy","outputId":"9b844ea4-2568-4dea-dd39-867dcb402cfe"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8e1aca2f17c6476a855cbe11a1d661a9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/42146 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["max length: 433\n"]}],"source":["pn_history_lengths = []\n","tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    pn_history_lengths.append(length)\n","\n","print(\"max length:\", np.max(pn_history_lengths))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1648598859021,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"posted-humidity","outputId":"c23cdcb5-4093-47d7-d95c-a35eaf72ea9d"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6639cae9d2844ea3b7d9a777b43b2181","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/143 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["max length: 30\n"]}],"source":["feature_text_lengths = []\n","tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    feature_text_lengths.append(length)\n","\n","print(\"max length:\", np.max(feature_text_lengths))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1648598859022,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"resistant-amount","outputId":"8bc2659a-d25a-40a1-f85d-37d57b1f3fc5"},"outputs":[{"name":"stdout","output_type":"stream","text":["max length: 466\n"]}],"source":["CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls \u0026 sep \u0026 sep\n","\n","print(\"max length:\", CFG.max_len)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"elapsed":408,"status":"ok","timestamp":1648598859416,"user":{"displayName":"Shuhei Goda","userId":"08246931244224045522"},"user_tz":-540},"id":"be6XpsR0aIWS","outputId":"1af87d7f-23bc-4035-d4c2-08d1f778e070"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4c276b71af2f4301b4048e4f8dad36d3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/42146 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["max length: 950\n"]}],"source":["pn_history_lengths = []\n","tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n","for text in tk0:\n","    length = len(text)\n","    pn_history_lengths.append(length)\n","\n","CFG.max_char_len = max(pn_history_lengths)\n","\n","print(\"max length:\", CFG.max_char_len)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fIzpppqiaMRn"},"outputs":[],"source":["class TrainingDataset(Dataset):\n","    def __init__(self, cfg, df, pseudo_label=None):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.max_char_len = self.cfg.max_char_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","        self.annotation_lengths = self.df[\"annotation_length\"].values\n","        self.locations = self.df[\"location\"].values\n","        if \"pseudo_idx\" in df.columns:\n","            self.pseudo_idx = self.df[\"pseudo_idx\"].values\n","            self.pseudo_label = pseudo_label\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def _create_mapping_from_token_to_char(self, pn_history):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        mapping_from_token_to_char = np.zeros(self.max_char_len)\n","        offset_mapping = encoded[\"offset_mapping\"]\n","        for i, offset in enumerate(offset_mapping):\n","            start_idx, end_idx = offset\n","            mapping_from_token_to_char[start_idx:end_idx] = i\n","        return torch.tensor(mapping_from_token_to_char, dtype=torch.long)\n","\n","    def _create_label(self, pn_history, annotation_length, location_list):\n","        label = np.zeros(self.max_char_len)\n","        label[len(pn_history):] = -1\n","        if annotation_length \u003e 0:\n","            for location in location_list:\n","                for loc in [s.split() for s in location.split(\";\")]:\n","                    start, end = int(loc[0]), int(loc[1])\n","                    label[start:end] = 1\n","        return torch.tensor(label, dtype=torch.float)\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        if not np.isnan(self.annotation_lengths[idx]):\n","            label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n","        else:\n","            p_idx = int(self.pseudo_idx[idx])\n","            label = torch.tensor(self.pseudo_label[p_idx], dtype=torch.float)\n","        mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n","        return input_, label, mapping_from_token_to_char"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"weird-interaction"},"outputs":[],"source":["class TestDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.max_char_len = self.cfg.max_char_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def _create_mapping_from_token_to_char(self, pn_history):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        mapping_from_token_to_char = np.zeros(self.max_char_len)\n","        offset_mapping = encoded[\"offset_mapping\"]\n","        for i, offset in enumerate(offset_mapping):\n","            start_idx, end_idx = offset\n","            mapping_from_token_to_char[start_idx:end_idx] = i\n","        return torch.tensor(mapping_from_token_to_char, dtype=torch.long)\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n","        return input_, mapping_from_token_to_char"]},{"cell_type":"markdown","metadata":{"id":"upper-mobility"},"source":["## Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"spanish-destruction"},"outputs":[],"source":["class CustomModel(nn.Module):\n","    def __init__(self, cfg, model_config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        if model_config_path is None:\n","            self.model_config = AutoConfig.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                output_hidden_states=True,\n","            )\n","        else:\n","            self.model_config = torch.load(model_config_path)\n","\n","        if pretrained:\n","            self.backbone = AutoModel.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                config=self.model_config,\n","            )\n","            print(f\"Load weight from pretrained\")\n","        else:\n","            #self.backbone = AutoModel.from_config(self.model_config)\n","            itpt = AutoModelForMaskedLM.from_config(self.model_config)\n","            path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp010/checkpoint-130170/pytorch_model.bin\")\n","            # path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n","            state_dict = torch.load(path)\n","            itpt.load_state_dict(state_dict)\n","            self.backbone = itpt.deberta\n","            print(f\"Load weight from {path}\")\n","\n","        self.lstm = nn.GRU(\n","            input_size=self.model_config.hidden_size,\n","            bidirectional=True,\n","            hidden_size=self.model_config.hidden_size // 2,\n","            num_layers=4,\n","            dropout=self.cfg.dropout,\n","            batch_first=True,\n","        )\n","        self.fc = nn.Sequential(\n","            nn.Dropout(self.cfg.dropout),\n","            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n","        )\n","\n","    def forward(self, inputs, mappings_from_token_to_char):\n","        h = self.backbone(**inputs)[\"last_hidden_state\"]  # [batch, seq_len, d_model]\n","        mappings_from_token_to_char = mappings_from_token_to_char.unsqueeze(2).expand(-1, -1, self.model_config.hidden_size)\n","        h = torch.gather(h, 1, mappings_from_token_to_char)    # [batch, seq_len, d_model]\n","        h, _ = self.lstm(h)\n","        output = self.fc(h)\n","\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"chronic-bullet"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"biological-hunger"},"outputs":[],"source":["def train_fn(\n","    train_dataloader,\n","    model,\n","    criterion,\n","    optimizer,\n","    epoch,\n","    scheduler,\n","    device,\n","):\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels, mappings_from_token_to_char) in enumerate(train_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device) \n","        batch_size = labels.size(0)\n","        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            output = model(inputs, mappings_from_token_to_char)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","        loss = loss.mean()\n","\n","        if CFG.gradient_accumulation_steps \u003e 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","\n","        if CFG.batch_scheduler:\n","            scheduler.step()\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n","            print(\n","                \"Epoch: [{0}][{1}/{2}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                \"Grad: {grad_norm:.4f}  \"\n","                \"LR: {lr:.6f}  \"\n","                .format(\n","                    epoch+1,\n","                    step,\n","                    len(train_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n","                    loss=losses,\n","                     grad_norm=grad_norm,\n","                     lr=scheduler.get_lr()[0],\n","                )\n","            )\n","    del output, loss, inputs, labels, mappings_from_token_to_char, scaler, grad_norm; gc.collect()\n","    torch.cuda.empty_cache()\n","    return losses.avg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"satisfied-sterling"},"outputs":[],"source":["def valid_fn(\n","    val_dataloader,\n","    model,\n","    criterion,\n","    device,\n","):\n","    model.eval()\n","    preds = []\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels, mappings_from_token_to_char) in enumerate(val_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device) \n","        batch_size = labels.size(0)\n","        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            output = model(inputs, mappings_from_token_to_char)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","        loss = loss.mean()\n","    \n","        if CFG.gradient_accumulation_steps \u003e 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n","            print(\n","                \"EVAL: [{0}/{1}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                .format(\n","                    step, len(val_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n","                    loss=losses,\n","                )\n","            )\n","    preds = np.concatenate(preds)\n","    return losses.avg, preds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"incorporate-viking"},"outputs":[],"source":["def inference_fn(test_dataloader, model, device):\n","    model.eval()\n","    model.to(device)\n","    preds = []\n","    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n","    for (inputs, mappings_from_token_to_char) in tk0:\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n","\n","        with torch.no_grad():\n","            output = model(inputs, mappings_from_token_to_char)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","    preds = np.concatenate(preds)\n","    return preds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dental-sunset"},"outputs":[],"source":["def train_loop(df, i_fold, device):\n","    print(f\"========== fold: {i_fold} training ==========\")\n","    train_idx = df[df[\"fold\"] != i_fold].index\n","    val_idx = df[df[\"fold\"] == i_fold].index\n","\n","    train_folds = df.loc[train_idx].reset_index(drop=True)\n","    val_folds = df.loc[val_idx].reset_index(drop=True)\n","\n","    pseudo_plain_path = './drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl'\n","    pseudo_plain = pd.read_pickle(pseudo_plain_path)\n","    pseudo_label_path = f'./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_{i_fold}.npy'\n","    pseudo_label = np.load(pseudo_label_path)\n","    print(f\"get pseudo plain from {pseudo_plain_path}\")\n","    print(f\"get pseudo labels from {pseudo_label_path}\")\n","    print(pseudo_plain.shape, pseudo_label.shape)\n","    pseudo_plain[\"pseudo_idx\"] = np.arange(len(pseudo_plain))\n","    pseudo_plain = pseudo_plain.sample(n=100000, random_state=i_fold)\n","    print(pseudo_plain.shape)\n","    train_folds = pd.concat([train_folds, pseudo_plain], axis=0, ignore_index=True)\n","\n","    train_dataset = TrainingDataset(CFG, train_folds, pseudo_label)\n","    val_dataset = TrainingDataset(CFG, val_folds)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=True,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=True,\n","    )\n","    val_dataloader = DataLoader(\n","        val_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=False,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=False,\n","    )\n","\n","    # model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","    model = CustomModel(CFG, model_config_path=None, pretrained=False)   # itptを使うため\n","    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n","    model.to(device)\n","\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\"params\": [p for n, p in param_optimizer if not any(\n","            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n","        {\"params\": [p for n, p in param_optimizer if any(\n","            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n","    ]\n","    optimizer = AdamW(\n","        optimizer_grouped_parameters,\n","        lr=CFG.lr,\n","        betas=CFG.betas,\n","        weight_decay=CFG.weight_decay,\n","    )\n","    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n","    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        num_training_steps=num_train_optimization_steps,\n","    )\n","\n","    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    best_score = -1 * np.inf\n","\n","    for epoch in range(CFG.epochs):\n","        start_time = time.time()\n","        avg_loss = train_fn(\n","            train_dataloader,\n","            model,\n","            criterion,\n","            optimizer,\n","            epoch,\n","            scheduler,\n","            device,\n","        )\n","        avg_val_loss, val_preds = valid_fn(\n","            val_dataloader,\n","            model,\n","            criterion,\n","            device,\n","        )\n","\n","        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n","            scheduler.step()\n","\n","        # scoring\n","        val_folds[[str(i) for i in range(CFG.max_char_len)]] = val_preds\n","        score = scoring(val_folds, th=0.5, use_token_prob=False)\n","\n","        elapsed = time.time() - start_time\n","\n","        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n","        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n","        if score \u003e best_score:\n","            best_score = score\n","            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n","            torch.save({\n","                \"model\": model.state_dict(),\n","                \"predictions\": val_preds,\n","                },\n","                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","            )\n","        ##################################### debug #####################################\n","        break\n","\n","    predictions = torch.load(\n","        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","        map_location=torch.device(\"cpu\"),\n","    )[\"predictions\"]\n","    val_folds[[str(i) for i in range(CFG.max_char_len)]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return val_folds"]},{"cell_type":"markdown","metadata":{"id":"brazilian-graphics"},"source":["## Main"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"connected-protein"},"outputs":[],"source":["def main():\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    if CFG.train:\n","        oof_df = pd.DataFrame()\n","        for i_fold in range(CFG.n_fold):\n","            if i_fold in CFG.train_fold:\n","                _oof_df = train_loop(train, i_fold, device)\n","                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n","        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    if CFG.submission:\n","        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n","    else:\n","        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    best_thres = 0.5\n","    best_score = 0.\n","    for th in np.arange(0.45, 0.55, 0.01):\n","        th = np.round(th, 2)\n","        score = scoring(oof_df, th=th, use_token_prob=False)\n","        if best_score \u003c score:\n","            best_thres = th\n","            best_score = score\n","    print(f\"best_thres: {best_thres}  score: {best_score:.5f}\")\n","\n","    if CFG.inference:\n","        test_dataset = TestDataset(CFG, test)\n","        test_dataloader = DataLoader(\n","            test_dataset,\n","            batch_size=CFG.batch_size,\n","            shuffle=False,\n","            num_workers=CFG.num_workers,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","        predictions = []\n","        for i_fold in CFG.train_fold:\n","            if CFG.submission:\n","                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n","                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n","            else:\n","                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n","\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            model.load_state_dict(state[\"model\"])\n","            print(f\"load weights from {path}\")\n","            test_char_probs = inference_fn(test_dataloader, model, device)\n","            predictions.append(test_char_probs)\n","\n","            del state, test_char_probs, model; gc.collect()\n","            torch.cuda.empty_cache()\n","\n","        predictions = np.mean(predictions, axis=0)\n","        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n","        test[CFG.target_col] = predicted_location_str\n","        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n","        test[[CFG.id_col, CFG.target_col]].to_csv(\n","            CFG.output_dir / \"submission.csv\", index=False\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"serious-bunny"},"outputs":[{"name":"stdout","output_type":"stream","text":["========== fold: 1 training ==========\n","(612602, 6) (612602, 950)\n","(100000, 7)\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n","Epoch: [1][0/36908] Elapsed 0m 1s (remain 727m 3s) Loss: 0.3519(0.3519) Grad: 96061.2422  LR: 0.000000  \n","Epoch: [1][100/36908] Elapsed 1m 3s (remain 384m 42s) Loss: 0.3462(0.3513) Grad: 49920.2930  LR: 0.000000  \n","Epoch: [1][200/36908] Elapsed 2m 5s (remain 382m 53s) Loss: 0.3365(0.3464) Grad: 46849.4219  LR: 0.000000  \n","Epoch: [1][300/36908] Elapsed 3m 7s (remain 381m 1s) Loss: 0.3130(0.3387) Grad: 43863.5742  LR: 0.000000  \n","Epoch: [1][400/36908] Elapsed 4m 10s (remain 379m 25s) Loss: 0.2748(0.3279) Grad: 44519.1445  LR: 0.000000  \n","Epoch: [1][500/36908] Elapsed 5m 12s (remain 378m 24s) Loss: 0.2428(0.3146) Grad: 39728.5000  LR: 0.000001  \n","Epoch: [1][600/36908] Elapsed 6m 14s (remain 376m 56s) Loss: 0.1825(0.2988) Grad: 28520.2441  LR: 0.000001  \n","Epoch: [1][700/36908] Elapsed 7m 16s (remain 375m 36s) Loss: 0.1315(0.2795) Grad: 14702.3545  LR: 0.000001  \n","Epoch: [1][800/36908] Elapsed 8m 18s (remain 374m 17s) Loss: 0.0926(0.2586) Grad: 10316.3691  LR: 0.000001  \n","Epoch: [1][900/36908] Elapsed 9m 19s (remain 372m 58s) Loss: 0.0654(0.2379) Grad: 4933.2739  LR: 0.000001  \n","Epoch: [1][1000/36908] Elapsed 10m 21s (remain 371m 43s) Loss: 0.0592(0.2188) Grad: 1094.9108  LR: 0.000001  \n","Epoch: [1][1100/36908] Elapsed 11m 23s (remain 370m 30s) Loss: 0.0304(0.2023) Grad: 728.3980  LR: 0.000001  \n","Epoch: [1][1200/36908] Elapsed 12m 25s (remain 369m 23s) Loss: 0.0562(0.1887) Grad: 1367.6158  LR: 0.000001  \n","Epoch: [1][1300/36908] Elapsed 13m 27s (remain 368m 19s) Loss: 0.0439(0.1769) Grad: 884.8626  LR: 0.000001  \n","Epoch: [1][1400/36908] Elapsed 14m 29s (remain 367m 12s) Loss: 0.0307(0.1669) Grad: 573.9481  LR: 0.000002  \n","Epoch: [1][1500/36908] Elapsed 15m 31s (remain 366m 1s) Loss: 0.0509(0.1582) Grad: 1724.0654  LR: 0.000002  \n","Epoch: [1][1600/36908] Elapsed 16m 32s (remain 364m 54s) Loss: 0.0228(0.1506) Grad: 1343.7600  LR: 0.000002  \n","Epoch: [1][1700/36908] Elapsed 17m 34s (remain 363m 44s) Loss: 0.0251(0.1438) Grad: 3299.3257  LR: 0.000002  \n","Epoch: [1][1800/36908] Elapsed 18m 36s (remain 362m 38s) Loss: 0.0161(0.1372) Grad: 2960.9790  LR: 0.000002  \n","Epoch: [1][1900/36908] Elapsed 19m 38s (remain 361m 33s) Loss: 0.0520(0.1313) Grad: 15169.5781  LR: 0.000002  \n","Epoch: [1][2000/36908] Elapsed 20m 40s (remain 360m 32s) Loss: 0.0018(0.1257) Grad: 734.2032  LR: 0.000002  \n","Epoch: [1][2100/36908] Elapsed 21m 41s (remain 359m 27s) Loss: 0.0203(0.1208) Grad: 10615.4014  LR: 0.000002  \n","Epoch: [1][2200/36908] Elapsed 22m 43s (remain 358m 22s) Loss: 0.0213(0.1163) Grad: 11763.1367  LR: 0.000002  \n","Epoch: [1][2300/36908] Elapsed 23m 45s (remain 357m 18s) Loss: 0.0119(0.1119) Grad: 4679.0229  LR: 0.000002  \n","Epoch: [1][2400/36908] Elapsed 24m 47s (remain 356m 14s) Loss: 0.0299(0.1080) Grad: 21745.3457  LR: 0.000003  \n","Epoch: [1][2500/36908] Elapsed 25m 49s (remain 355m 10s) Loss: 0.0183(0.1043) Grad: 2165.8381  LR: 0.000003  \n","Epoch: [1][2600/36908] Elapsed 26m 50s (remain 354m 7s) Loss: 0.0081(0.1009) Grad: 2290.0422  LR: 0.000003  \n","Epoch: [1][2700/36908] Elapsed 27m 52s (remain 353m 6s) Loss: 0.0018(0.0977) Grad: 442.9733  LR: 0.000003  \n","Epoch: [1][2800/36908] Elapsed 28m 54s (remain 352m 3s) Loss: 0.0054(0.0947) Grad: 1912.9821  LR: 0.000003  \n","Epoch: [1][2900/36908] Elapsed 29m 56s (remain 350m 59s) Loss: 0.0096(0.0919) Grad: 14329.5566  LR: 0.000003  \n","Epoch: [1][3000/36908] Elapsed 30m 58s (remain 349m 54s) Loss: 0.0161(0.0892) Grad: 3215.2402  LR: 0.000003  \n","Epoch: [1][3100/36908] Elapsed 31m 59s (remain 348m 50s) Loss: 0.0011(0.0867) Grad: 841.2111  LR: 0.000003  \n","Epoch: [1][3200/36908] Elapsed 33m 1s (remain 347m 48s) Loss: 0.0016(0.0843) Grad: 333.2820  LR: 0.000003  \n","Epoch: [1][3300/36908] Elapsed 34m 3s (remain 346m 43s) Loss: 0.0201(0.0822) Grad: 5511.9277  LR: 0.000004  \n","Epoch: [1][3400/36908] Elapsed 35m 5s (remain 345m 41s) Loss: 0.0152(0.0801) Grad: 5877.9883  LR: 0.000004  \n","Epoch: [1][3500/36908] Elapsed 36m 7s (remain 344m 39s) Loss: 0.0044(0.0781) Grad: 2149.4792  LR: 0.000004  \n","Epoch: [1][3600/36908] Elapsed 37m 8s (remain 343m 36s) Loss: 0.0022(0.0762) Grad: 804.9498  LR: 0.000004  \n","Epoch: [1][3700/36908] Elapsed 38m 10s (remain 342m 31s) Loss: 0.0019(0.0744) Grad: 676.3548  LR: 0.000004  \n","Epoch: [1][3800/36908] Elapsed 39m 12s (remain 341m 27s) Loss: 0.0573(0.0727) Grad: 10583.4766  LR: 0.000004  \n","Epoch: [1][3900/36908] Elapsed 40m 13s (remain 340m 23s) Loss: 0.0034(0.0711) Grad: 1357.7224  LR: 0.000004  \n","Epoch: [1][4000/36908] Elapsed 41m 15s (remain 339m 20s) Loss: 0.0146(0.0695) Grad: 5555.6162  LR: 0.000004  \n","Epoch: [1][4100/36908] Elapsed 42m 17s (remain 338m 17s) Loss: 0.0099(0.0681) Grad: 3467.6672  LR: 0.000004  \n","Epoch: [1][4200/36908] Elapsed 43m 19s (remain 337m 15s) Loss: 0.0337(0.0666) Grad: 8997.2656  LR: 0.000005  \n","Epoch: [1][4300/36908] Elapsed 44m 21s (remain 336m 16s) Loss: 0.0181(0.0653) Grad: 6835.0952  LR: 0.000005  \n","Epoch: [1][4400/36908] Elapsed 45m 23s (remain 335m 16s) Loss: 0.0009(0.0640) Grad: 905.9886  LR: 0.000005  \n","Epoch: [1][4500/36908] Elapsed 46m 25s (remain 334m 17s) Loss: 0.0027(0.0627) Grad: 2089.1406  LR: 0.000005  \n","Epoch: [1][4600/36908] Elapsed 47m 27s (remain 333m 14s) Loss: 0.0221(0.0615) Grad: 6265.2510  LR: 0.000005  \n","Epoch: [1][4700/36908] Elapsed 48m 29s (remain 332m 16s) Loss: 0.0042(0.0603) Grad: 1010.7195  LR: 0.000005  \n","Epoch: [1][4800/36908] Elapsed 49m 32s (remain 331m 20s) Loss: 0.0087(0.0591) Grad: 3183.1240  LR: 0.000005  \n","Epoch: [1][4900/36908] Elapsed 50m 34s (remain 330m 19s) Loss: 0.0007(0.0581) Grad: 148.0766  LR: 0.000005  \n","Epoch: [1][5000/36908] Elapsed 51m 36s (remain 329m 19s) Loss: 0.0110(0.0571) Grad: 6949.0444  LR: 0.000005  \n","Epoch: [1][5100/36908] Elapsed 52m 38s (remain 328m 17s) Loss: 0.0006(0.0561) Grad: 221.8972  LR: 0.000006  \n","Epoch: [1][5200/36908] Elapsed 53m 40s (remain 327m 14s) Loss: 0.0009(0.0551) Grad: 92.2672  LR: 0.000006  \n","Epoch: [1][5300/36908] Elapsed 54m 42s (remain 326m 12s) Loss: 0.0088(0.0542) Grad: 3132.1750  LR: 0.000006  \n","Epoch: [1][5400/36908] Elapsed 55m 44s (remain 325m 9s) Loss: 0.0211(0.0533) Grad: 6466.0444  LR: 0.000006  \n","Epoch: [1][5500/36908] Elapsed 56m 46s (remain 324m 7s) Loss: 0.0041(0.0524) Grad: 1391.0121  LR: 0.000006  \n","Epoch: [1][5600/36908] Elapsed 57m 48s (remain 323m 4s) Loss: 0.0043(0.0516) Grad: 1391.9795  LR: 0.000006  \n","Epoch: [1][5700/36908] Elapsed 58m 49s (remain 322m 2s) Loss: 0.0125(0.0508) Grad: 5584.2915  LR: 0.000006  \n","Epoch: [1][5800/36908] Elapsed 59m 51s (remain 321m 0s) Loss: 0.0055(0.0501) Grad: 6122.9771  LR: 0.000006  \n","Epoch: [1][5900/36908] Elapsed 60m 53s (remain 319m 59s) Loss: 0.0082(0.0494) Grad: 3278.5261  LR: 0.000006  \n","Epoch: [1][6000/36908] Elapsed 61m 55s (remain 318m 57s) Loss: 0.0006(0.0486) Grad: 78.3660  LR: 0.000007  \n","Epoch: [1][6100/36908] Elapsed 62m 57s (remain 317m 55s) Loss: 0.0017(0.0479) Grad: 1522.4131  LR: 0.000007  \n","Epoch: [1][6200/36908] Elapsed 63m 59s (remain 316m 53s) Loss: 0.0015(0.0472) Grad: 1642.2253  LR: 0.000007  \n","Epoch: [1][6300/36908] Elapsed 65m 1s (remain 315m 50s) Loss: 0.0013(0.0466) Grad: 974.2676  LR: 0.000007  \n","Epoch: [1][6400/36908] Elapsed 66m 3s (remain 314m 47s) Loss: 0.0070(0.0459) Grad: 986.1126  LR: 0.000007  \n","Epoch: [1][6500/36908] Elapsed 67m 4s (remain 313m 44s) Loss: 0.0130(0.0453) Grad: 7842.4551  LR: 0.000007  \n","Epoch: [1][6600/36908] Elapsed 68m 6s (remain 312m 42s) Loss: 0.0015(0.0447) Grad: 1735.1781  LR: 0.000007  \n","Epoch: [1][6700/36908] Elapsed 69m 8s (remain 311m 39s) Loss: 0.0013(0.0441) Grad: 604.4842  LR: 0.000007  \n","Epoch: [1][6800/36908] Elapsed 70m 9s (remain 310m 37s) Loss: 0.0058(0.0435) Grad: 11298.0244  LR: 0.000007  \n","Epoch: [1][6900/36908] Elapsed 71m 11s (remain 309m 34s) Loss: 0.0005(0.0430) Grad: 267.4236  LR: 0.000007  \n","Epoch: [1][7000/36908] Elapsed 72m 13s (remain 308m 31s) Loss: 0.0011(0.0424) Grad: 1868.5585  LR: 0.000008  \n","Epoch: [1][7100/36908] Elapsed 73m 15s (remain 307m 30s) Loss: 0.0003(0.0419) Grad: 138.4667  LR: 0.000008  \n","Epoch: [1][7200/36908] Elapsed 74m 17s (remain 306m 28s) Loss: 0.0017(0.0414) Grad: 1258.4175  LR: 0.000008  \n","Epoch: [1][7300/36908] Elapsed 75m 19s (remain 305m 26s) Loss: 0.0007(0.0409) Grad: 269.3532  LR: 0.000008  \n","Epoch: [1][7400/36908] Elapsed 76m 21s (remain 304m 24s) Loss: 0.0217(0.0404) Grad: 8243.2881  LR: 0.000008  \n","Epoch: [1][7500/36908] Elapsed 77m 23s (remain 303m 22s) Loss: 0.0009(0.0400) Grad: 1459.8643  LR: 0.000008  \n","Epoch: [1][7600/36908] Elapsed 78m 24s (remain 302m 20s) Loss: 0.0012(0.0395) Grad: 647.6413  LR: 0.000008  \n","Epoch: [1][7700/36908] Elapsed 79m 26s (remain 301m 18s) Loss: 0.0002(0.0391) Grad: 24.1121  LR: 0.000008  \n","Epoch: [1][7800/36908] Elapsed 80m 28s (remain 300m 16s) Loss: 0.0056(0.0386) Grad: 8706.2217  LR: 0.000008  \n","Epoch: [1][7900/36908] Elapsed 81m 30s (remain 299m 14s) Loss: 0.0005(0.0382) Grad: 167.9264  LR: 0.000009  \n","Epoch: [1][8000/36908] Elapsed 82m 32s (remain 298m 12s) Loss: 0.0010(0.0378) Grad: 310.5167  LR: 0.000009  \n","Epoch: [1][8100/36908] Elapsed 83m 34s (remain 297m 10s) Loss: 0.0025(0.0374) Grad: 3582.1172  LR: 0.000009  \n","Epoch: [1][8200/36908] Elapsed 84m 36s (remain 296m 9s) Loss: 0.0164(0.0370) Grad: 7110.9326  LR: 0.000009  \n","Epoch: [1][8300/36908] Elapsed 85m 38s (remain 295m 7s) Loss: 0.0008(0.0366) Grad: 1165.3416  LR: 0.000009  \n","Epoch: [1][8400/36908] Elapsed 86m 40s (remain 294m 5s) Loss: 0.0107(0.0362) Grad: 3905.0615  LR: 0.000009  \n","Epoch: [1][8500/36908] Elapsed 87m 42s (remain 293m 3s) Loss: 0.0036(0.0358) Grad: 3924.7402  LR: 0.000009  \n","Epoch: [1][8600/36908] Elapsed 88m 43s (remain 292m 1s) Loss: 0.0012(0.0355) Grad: 623.0325  LR: 0.000009  \n","Epoch: [1][8700/36908] Elapsed 89m 45s (remain 290m 59s) Loss: 0.0001(0.0351) Grad: 50.8783  LR: 0.000009  \n","Epoch: [1][8800/36908] Elapsed 90m 47s (remain 289m 56s) Loss: 0.0024(0.0348) Grad: 2155.3855  LR: 0.000010  \n","Epoch: [1][8900/36908] Elapsed 91m 49s (remain 288m 54s) Loss: 0.0004(0.0344) Grad: 409.4146  LR: 0.000010  \n","Epoch: [1][9000/36908] Elapsed 92m 50s (remain 287m 52s) Loss: 0.0007(0.0341) Grad: 412.5681  LR: 0.000010  \n","Epoch: [1][9100/36908] Elapsed 93m 52s (remain 286m 50s) Loss: 0.0109(0.0337) Grad: 4455.8057  LR: 0.000010  \n","Epoch: [1][9200/36908] Elapsed 94m 54s (remain 285m 48s) Loss: 0.0031(0.0334) Grad: 9547.2900  LR: 0.000010  \n","Epoch: [1][9300/36908] Elapsed 95m 56s (remain 284m 46s) Loss: 0.0036(0.0331) Grad: 2370.1885  LR: 0.000010  \n","Epoch: [1][9400/36908] Elapsed 96m 58s (remain 283m 44s) Loss: 0.0003(0.0328) Grad: 737.5096  LR: 0.000010  \n","Epoch: [1][9500/36908] Elapsed 98m 0s (remain 282m 42s) Loss: 0.0240(0.0325) Grad: 5350.0669  LR: 0.000010  \n","Epoch: [1][9600/36908] Elapsed 99m 2s (remain 281m 40s) Loss: 0.0008(0.0322) Grad: 1363.4226  LR: 0.000010  \n","Epoch: [1][9700/36908] Elapsed 100m 4s (remain 280m 38s) Loss: 0.0129(0.0319) Grad: 10323.3379  LR: 0.000011  \n","Epoch: [1][9800/36908] Elapsed 101m 5s (remain 279m 36s) Loss: 0.0014(0.0316) Grad: 1971.1211  LR: 0.000011  \n","Epoch: [1][9900/36908] Elapsed 102m 7s (remain 278m 34s) Loss: 0.0001(0.0314) Grad: 37.7717  LR: 0.000011  \n","Epoch: [1][10000/36908] Elapsed 103m 9s (remain 277m 32s) Loss: 0.0041(0.0311) Grad: 3323.8386  LR: 0.000011  \n","Epoch: [1][10100/36908] Elapsed 104m 11s (remain 276m 30s) Loss: 0.0014(0.0308) Grad: 755.5029  LR: 0.000011  \n","Epoch: [1][10200/36908] Elapsed 105m 13s (remain 275m 29s) Loss: 0.0202(0.0306) Grad: 15173.2275  LR: 0.000011  \n","Epoch: [1][10300/36908] Elapsed 106m 15s (remain 274m 27s) Loss: 0.0086(0.0303) Grad: 4851.5913  LR: 0.000011  \n","Epoch: [1][10400/36908] Elapsed 107m 17s (remain 273m 25s) Loss: 0.0028(0.0301) Grad: 777.8820  LR: 0.000011  \n","Epoch: [1][10500/36908] Elapsed 108m 19s (remain 272m 23s) Loss: 0.0012(0.0298) Grad: 351.0536  LR: 0.000011  \n","Epoch: [1][10600/36908] Elapsed 109m 20s (remain 271m 21s) Loss: 0.0033(0.0296) Grad: 3412.6516  LR: 0.000011  \n","Epoch: [1][10700/36908] Elapsed 110m 22s (remain 270m 19s) Loss: 0.0001(0.0293) Grad: 95.0322  LR: 0.000012  \n","Epoch: [1][10800/36908] Elapsed 111m 24s (remain 269m 17s) Loss: 0.0004(0.0291) Grad: 87.7049  LR: 0.000012  \n","Epoch: [1][10900/36908] Elapsed 112m 26s (remain 268m 15s) Loss: 0.0031(0.0289) Grad: 6084.0664  LR: 0.000012  \n","Epoch: [1][11000/36908] Elapsed 113m 28s (remain 267m 13s) Loss: 0.0006(0.0286) Grad: 553.2914  LR: 0.000012  \n","Epoch: [1][11100/36908] Elapsed 114m 30s (remain 266m 11s) Loss: 0.0028(0.0284) Grad: 5381.9663  LR: 0.000012  \n","Epoch: [1][11200/36908] Elapsed 115m 32s (remain 265m 9s) Loss: 0.0004(0.0282) Grad: 57.7690  LR: 0.000012  \n","Epoch: [1][11300/36908] Elapsed 116m 34s (remain 264m 8s) Loss: 0.0094(0.0280) Grad: 13475.0410  LR: 0.000012  \n","Epoch: [1][11400/36908] Elapsed 117m 36s (remain 263m 6s) Loss: 0.0002(0.0278) Grad: 23.9111  LR: 0.000012  \n","Epoch: [1][11500/36908] Elapsed 118m 38s (remain 262m 4s) Loss: 0.0111(0.0276) Grad: 18543.3145  LR: 0.000012  \n","Epoch: [1][11600/36908] Elapsed 119m 40s (remain 261m 2s) Loss: 0.0001(0.0274) Grad: 79.9881  LR: 0.000013  \n","Epoch: [1][11700/36908] Elapsed 120m 41s (remain 260m 1s) Loss: 0.0212(0.0272) Grad: 21894.6602  LR: 0.000013  \n","Epoch: [1][11800/36908] Elapsed 121m 44s (remain 258m 59s) Loss: 0.0068(0.0270) Grad: 3499.7175  LR: 0.000013  \n","Epoch: [1][11900/36908] Elapsed 122m 46s (remain 257m 57s) Loss: 0.0004(0.0268) Grad: 127.3931  LR: 0.000013  \n","Epoch: [1][12000/36908] Elapsed 123m 47s (remain 256m 56s) Loss: 0.0003(0.0266) Grad: 34.2037  LR: 0.000013  \n","Epoch: [1][12100/36908] Elapsed 124m 49s (remain 255m 54s) Loss: 0.0001(0.0264) Grad: 24.8877  LR: 0.000013  \n","Epoch: [1][12200/36908] Elapsed 125m 51s (remain 254m 52s) Loss: 0.0002(0.0262) Grad: 67.7125  LR: 0.000013  \n","Epoch: [1][12300/36908] Elapsed 126m 53s (remain 253m 50s) Loss: 0.0057(0.0260) Grad: 4542.9473  LR: 0.000013  \n","Epoch: [1][12400/36908] Elapsed 127m 55s (remain 252m 48s) Loss: 0.0000(0.0258) Grad: 11.1154  LR: 0.000013  \n","Epoch: [1][12500/36908] Elapsed 128m 57s (remain 251m 47s) Loss: 0.0018(0.0257) Grad: 3913.4321  LR: 0.000014  \n","Epoch: [1][12600/36908] Elapsed 130m 0s (remain 250m 46s) Loss: 0.0031(0.0255) Grad: 7709.9072  LR: 0.000014  \n","Epoch: [1][12700/36908] Elapsed 131m 2s (remain 249m 45s) Loss: 0.0001(0.0253) Grad: 30.3809  LR: 0.000014  \n","Epoch: [1][12800/36908] Elapsed 132m 4s (remain 248m 44s) Loss: 0.0010(0.0251) Grad: 6446.9976  LR: 0.000014  \n","Epoch: [1][12900/36908] Elapsed 133m 6s (remain 247m 42s) Loss: 0.0016(0.0250) Grad: 658.4635  LR: 0.000014  \n","Epoch: [1][13000/36908] Elapsed 134m 9s (remain 246m 40s) Loss: 0.0042(0.0248) Grad: 5534.8955  LR: 0.000014  \n","Epoch: [1][13100/36908] Elapsed 135m 11s (remain 245m 39s) Loss: 0.0002(0.0246) Grad: 76.9704  LR: 0.000014  \n","Epoch: [1][13200/36908] Elapsed 136m 12s (remain 244m 37s) Loss: 0.0002(0.0245) Grad: 166.9554  LR: 0.000014  \n","Epoch: [1][13300/36908] Elapsed 137m 14s (remain 243m 35s) Loss: 0.0088(0.0243) Grad: 2759.6003  LR: 0.000014  \n","Epoch: [1][13400/36908] Elapsed 138m 16s (remain 242m 33s) Loss: 0.0089(0.0242) Grad: 5665.2559  LR: 0.000015  \n","Epoch: [1][13500/36908] Elapsed 139m 18s (remain 241m 31s) Loss: 0.0008(0.0240) Grad: 811.4979  LR: 0.000015  \n","Epoch: [1][13600/36908] Elapsed 140m 20s (remain 240m 29s) Loss: 0.0036(0.0239) Grad: 3139.4639  LR: 0.000015  \n","Epoch: [1][13700/36908] Elapsed 141m 22s (remain 239m 27s) Loss: 0.0002(0.0237) Grad: 49.3018  LR: 0.000015  \n","Epoch: [1][13800/36908] Elapsed 142m 24s (remain 238m 25s) Loss: 0.0092(0.0236) Grad: 33514.0039  LR: 0.000015  \n","Epoch: [1][13900/36908] Elapsed 143m 26s (remain 237m 24s) Loss: 0.0010(0.0234) Grad: 136.5574  LR: 0.000015  \n","Epoch: [1][14000/36908] Elapsed 144m 28s (remain 236m 22s) Loss: 0.0003(0.0233) Grad: 107.7943  LR: 0.000015  \n","Epoch: [1][14100/36908] Elapsed 145m 30s (remain 235m 20s) Loss: 0.0003(0.0231) Grad: 202.4138  LR: 0.000015  \n","Epoch: [1][14200/36908] Elapsed 146m 32s (remain 234m 18s) Loss: 0.0079(0.0230) Grad: 9959.5518  LR: 0.000015  \n","Epoch: [1][14300/36908] Elapsed 147m 34s (remain 233m 16s) Loss: 0.0184(0.0229) Grad: 19784.0371  LR: 0.000015  \n","Epoch: [1][14400/36908] Elapsed 148m 36s (remain 232m 14s) Loss: 0.0142(0.0227) Grad: 29967.1035  LR: 0.000016  \n","Epoch: [1][14500/36908] Elapsed 149m 38s (remain 231m 13s) Loss: 0.0007(0.0226) Grad: 3693.8088  LR: 0.000016  \n","Epoch: [1][14600/36908] Elapsed 150m 40s (remain 230m 11s) Loss: 0.0000(0.0225) Grad: 21.7694  LR: 0.000016  \n","Epoch: [1][14700/36908] Elapsed 151m 42s (remain 229m 9s) Loss: 0.0071(0.0223) Grad: 19280.3809  LR: 0.000016  \n","Epoch: [1][14800/36908] Elapsed 152m 44s (remain 228m 7s) Loss: 0.0052(0.0222) Grad: 8431.9893  LR: 0.000016  \n","Epoch: [1][14900/36908] Elapsed 153m 46s (remain 227m 5s) Loss: 0.0003(0.0221) Grad: 7700.6689  LR: 0.000016  \n","Epoch: [1][15000/36908] Elapsed 154m 48s (remain 226m 4s) Loss: 0.0000(0.0220) Grad: 6.3905  LR: 0.000016  \n","Epoch: [1][15100/36908] Elapsed 155m 50s (remain 225m 2s) Loss: 0.0042(0.0218) Grad: 16268.8369  LR: 0.000016  \n","Epoch: [1][15200/36908] Elapsed 156m 52s (remain 224m 0s) Loss: 0.0001(0.0217) Grad: 33.8987  LR: 0.000016  \n","Epoch: [1][15300/36908] Elapsed 157m 54s (remain 222m 58s) Loss: 0.0007(0.0216) Grad: 1410.5715  LR: 0.000017  \n","Epoch: [1][15400/36908] Elapsed 158m 56s (remain 221m 57s) Loss: 0.0002(0.0215) Grad: 87.7934  LR: 0.000017  \n","Epoch: [1][15500/36908] Elapsed 159m 58s (remain 220m 55s) Loss: 0.0004(0.0213) Grad: 2194.4519  LR: 0.000017  \n","Epoch: [1][15600/36908] Elapsed 161m 0s (remain 219m 54s) Loss: 0.0003(0.0212) Grad: 88.1059  LR: 0.000017  \n","Epoch: [1][15700/36908] Elapsed 162m 3s (remain 218m 52s) Loss: 0.0001(0.0211) Grad: 15.6979  LR: 0.000017  \n","Epoch: [1][15800/36908] Elapsed 163m 5s (remain 217m 50s) Loss: 0.0044(0.0210) Grad: 7863.5049  LR: 0.000017  \n","Epoch: [1][15900/36908] Elapsed 164m 7s (remain 216m 49s) Loss: 0.0054(0.0209) Grad: 5801.8940  LR: 0.000017  \n","Epoch: [1][16000/36908] Elapsed 165m 9s (remain 215m 47s) Loss: 0.0021(0.0208) Grad: 6890.3120  LR: 0.000017  \n","Epoch: [1][16100/36908] Elapsed 166m 11s (remain 214m 45s) Loss: 0.0032(0.0207) Grad: 2468.1677  LR: 0.000017  \n","Epoch: [1][16200/36908] Elapsed 167m 13s (remain 213m 43s) Loss: 0.0003(0.0206) Grad: 60.7406  LR: 0.000018  \n","Epoch: [1][16300/36908] Elapsed 168m 15s (remain 212m 42s) Loss: 0.0013(0.0204) Grad: 740.0039  LR: 0.000018  \n","Epoch: [1][16400/36908] Elapsed 169m 17s (remain 211m 40s) Loss: 0.0001(0.0203) Grad: 63.9917  LR: 0.000018  \n","Epoch: [1][16500/36908] Elapsed 170m 19s (remain 210m 38s) Loss: 0.0015(0.0202) Grad: 5460.6680  LR: 0.000018  \n","Epoch: [1][16600/36908] Elapsed 171m 21s (remain 209m 36s) Loss: 0.0003(0.0201) Grad: 968.0132  LR: 0.000018  \n","Epoch: [1][16700/36908] Elapsed 172m 23s (remain 208m 34s) Loss: 0.0066(0.0200) Grad: 22125.8711  LR: 0.000018  \n","Epoch: [1][16800/36908] Elapsed 173m 25s (remain 207m 33s) Loss: 0.0057(0.0199) Grad: 31148.8457  LR: 0.000018  \n","Epoch: [1][16900/36908] Elapsed 174m 27s (remain 206m 31s) Loss: 0.0036(0.0198) Grad: 6905.8550  LR: 0.000018  \n","Epoch: [1][17000/36908] Elapsed 175m 29s (remain 205m 29s) Loss: 0.0002(0.0197) Grad: 256.9590  LR: 0.000018  \n","Epoch: [1][17100/36908] Elapsed 176m 31s (remain 204m 27s) Loss: 0.0172(0.0196) Grad: 25606.5098  LR: 0.000019  \n","Epoch: [1][17200/36908] Elapsed 177m 33s (remain 203m 25s) Loss: 0.0012(0.0195) Grad: 2189.3359  LR: 0.000019  \n","Epoch: [1][17300/36908] Elapsed 178m 35s (remain 202m 24s) Loss: 0.0064(0.0194) Grad: 27732.2148  LR: 0.000019  \n","Epoch: [1][17400/36908] Elapsed 179m 38s (remain 201m 22s) Loss: 0.0004(0.0193) Grad: 203.8492  LR: 0.000019  \n","Epoch: [1][17500/36908] Elapsed 180m 39s (remain 200m 20s) Loss: 0.0026(0.0192) Grad: 5779.1064  LR: 0.000019  \n","Epoch: [1][17600/36908] Elapsed 181m 41s (remain 199m 18s) Loss: 0.0002(0.0192) Grad: 101.5150  LR: 0.000019  \n","Epoch: [1][17700/36908] Elapsed 182m 43s (remain 198m 16s) Loss: 0.0045(0.0191) Grad: 3119.7285  LR: 0.000019  \n","Epoch: [1][17800/36908] Elapsed 183m 45s (remain 197m 14s) Loss: 0.0026(0.0190) Grad: 4630.0728  LR: 0.000019  \n","Epoch: [1][17900/36908] Elapsed 184m 47s (remain 196m 12s) Loss: 0.0008(0.0189) Grad: 214.5449  LR: 0.000019  \n","Epoch: [1][18000/36908] Elapsed 185m 49s (remain 195m 10s) Loss: 0.0010(0.0188) Grad: 592.5223  LR: 0.000020  \n","Epoch: [1][18100/36908] Elapsed 186m 51s (remain 194m 8s) Loss: 0.0007(0.0187) Grad: 799.9906  LR: 0.000020  \n","Epoch: [1][18200/36908] Elapsed 187m 53s (remain 193m 7s) Loss: 0.0001(0.0186) Grad: 357.7609  LR: 0.000020  \n","Epoch: [1][18300/36908] Elapsed 188m 55s (remain 192m 5s) Loss: 0.0025(0.0185) Grad: 10175.5928  LR: 0.000020  \n","Epoch: [1][18400/36908] Elapsed 189m 58s (remain 191m 3s) Loss: 0.0031(0.0184) Grad: 7475.9863  LR: 0.000020  \n","Epoch: [1][18500/36908] Elapsed 191m 0s (remain 190m 1s) Loss: 0.0053(0.0184) Grad: 24170.6621  LR: 0.000020  \n","Epoch: [1][18600/36908] Elapsed 192m 2s (remain 189m 0s) Loss: 0.0016(0.0183) Grad: 2917.2910  LR: 0.000020  \n","Epoch: [1][18700/36908] Elapsed 193m 3s (remain 187m 57s) Loss: 0.0045(0.0182) Grad: 48929.7930  LR: 0.000020  \n","Epoch: [1][18800/36908] Elapsed 194m 5s (remain 186m 56s) Loss: 0.0001(0.0181) Grad: 243.7568  LR: 0.000020  \n","Epoch: [1][18900/36908] Elapsed 195m 7s (remain 185m 54s) Loss: 0.0010(0.0180) Grad: 1448.4427  LR: 0.000020  \n","Epoch: [1][19000/36908] Elapsed 196m 9s (remain 184m 52s) Loss: 0.0000(0.0180) Grad: 15.6146  LR: 0.000020  \n","Epoch: [1][19100/36908] Elapsed 197m 11s (remain 183m 50s) Loss: 0.0002(0.0179) Grad: 166.0456  LR: 0.000020  \n","Epoch: [1][19200/36908] Elapsed 198m 13s (remain 182m 48s) Loss: 0.0045(0.0178) Grad: 27663.7598  LR: 0.000020  \n","Epoch: [1][19300/36908] Elapsed 199m 15s (remain 181m 46s) Loss: 0.0009(0.0177) Grad: 1582.7108  LR: 0.000020  \n","Epoch: [1][19400/36908] Elapsed 200m 17s (remain 180m 44s) Loss: 0.0075(0.0177) Grad: 23155.6113  LR: 0.000020  \n","Epoch: [1][19500/36908] Elapsed 201m 19s (remain 179m 42s) Loss: 0.0004(0.0176) Grad: 1503.0229  LR: 0.000020  \n","Epoch: [1][19600/36908] Elapsed 202m 20s (remain 178m 40s) Loss: 0.0053(0.0175) Grad: 39907.5117  LR: 0.000020  \n","Epoch: [1][19700/36908] Elapsed 203m 22s (remain 177m 38s) Loss: 0.0001(0.0174) Grad: 955.7444  LR: 0.000020  \n","Epoch: [1][19800/36908] Elapsed 204m 24s (remain 176m 36s) Loss: 0.0050(0.0174) Grad: 102473.6328  LR: 0.000020  \n","Epoch: [1][19900/36908] Elapsed 205m 26s (remain 175m 34s) Loss: 0.0003(0.0173) Grad: 467.5329  LR: 0.000020  \n","Epoch: [1][20000/36908] Elapsed 206m 28s (remain 174m 32s) Loss: 0.0005(0.0172) Grad: 4464.7529  LR: 0.000020  \n","Epoch: [1][20100/36908] Elapsed 207m 30s (remain 173m 30s) Loss: 0.0017(0.0171) Grad: 6157.5015  LR: 0.000020  \n","Epoch: [1][20200/36908] Elapsed 208m 32s (remain 172m 28s) Loss: 0.0001(0.0171) Grad: 59.9964  LR: 0.000020  \n","Epoch: [1][20300/36908] Elapsed 209m 34s (remain 171m 26s) Loss: 0.0002(0.0170) Grad: 756.0613  LR: 0.000020  \n","Epoch: [1][20400/36908] Elapsed 210m 36s (remain 170m 24s) Loss: 0.0038(0.0169) Grad: 30336.6094  LR: 0.000020  \n","Epoch: [1][20500/36908] Elapsed 211m 38s (remain 169m 22s) Loss: 0.0002(0.0169) Grad: 33.6848  LR: 0.000020  \n","Epoch: [1][20600/36908] Elapsed 212m 40s (remain 168m 20s) Loss: 0.0100(0.0168) Grad: 283793.0625  LR: 0.000020  \n","Epoch: [1][20700/36908] Elapsed 213m 41s (remain 167m 18s) Loss: 0.0112(0.0167) Grad: 41092.0742  LR: 0.000020  \n","Epoch: [1][20800/36908] Elapsed 214m 43s (remain 166m 16s) Loss: 0.0053(0.0167) Grad: 12118.9238  LR: 0.000020  \n","Epoch: [1][20900/36908] Elapsed 215m 45s (remain 165m 14s) Loss: 0.0003(0.0166) Grad: 243.0813  LR: 0.000020  \n","Epoch: [1][21000/36908] Elapsed 216m 47s (remain 164m 12s) Loss: 0.0001(0.0165) Grad: 15.3289  LR: 0.000020  \n","Epoch: [1][21100/36908] Elapsed 217m 49s (remain 163m 10s) Loss: 0.0026(0.0165) Grad: 3017.1489  LR: 0.000020  \n","Epoch: [1][21200/36908] Elapsed 218m 51s (remain 162m 8s) Loss: 0.0017(0.0164) Grad: 9443.1836  LR: 0.000020  \n","Epoch: [1][21300/36908] Elapsed 219m 53s (remain 161m 6s) Loss: 0.0005(0.0164) Grad: 539.5471  LR: 0.000020  \n","Epoch: [1][21400/36908] Elapsed 220m 55s (remain 160m 4s) Loss: 0.0011(0.0163) Grad: 1085.2698  LR: 0.000020  \n","Epoch: [1][21500/36908] Elapsed 221m 57s (remain 159m 2s) Loss: 0.0007(0.0162) Grad: 11058.8320  LR: 0.000020  \n","Epoch: [1][21600/36908] Elapsed 222m 58s (remain 158m 0s) Loss: 0.0052(0.0162) Grad: 21809.1973  LR: 0.000020  \n","Epoch: [1][21700/36908] Elapsed 224m 0s (remain 156m 58s) Loss: 0.0001(0.0161) Grad: 13.8229  LR: 0.000020  \n","Epoch: [1][21800/36908] Elapsed 225m 2s (remain 155m 56s) Loss: 0.0025(0.0160) Grad: 9603.0859  LR: 0.000020  \n","Epoch: [1][21900/36908] Elapsed 226m 4s (remain 154m 54s) Loss: 0.0001(0.0160) Grad: 34.3916  LR: 0.000020  \n","Epoch: [1][22000/36908] Elapsed 227m 6s (remain 153m 52s) Loss: 0.0004(0.0159) Grad: 324.2096  LR: 0.000020  \n","Epoch: [1][22100/36908] Elapsed 228m 8s (remain 152m 50s) Loss: 0.0111(0.0159) Grad: 10461.6924  LR: 0.000020  \n","Epoch: [1][22200/36908] Elapsed 229m 10s (remain 151m 48s) Loss: 0.0033(0.0158) Grad: 23260.0156  LR: 0.000020  \n","Epoch: [1][22300/36908] Elapsed 230m 12s (remain 150m 46s) Loss: 0.0005(0.0157) Grad: 595.8345  LR: 0.000020  \n","Epoch: [1][22400/36908] Elapsed 231m 14s (remain 149m 44s) Loss: 0.0069(0.0157) Grad: 28601.6406  LR: 0.000020  \n","Epoch: [1][22500/36908] Elapsed 232m 15s (remain 148m 42s) Loss: 0.0001(0.0156) Grad: 198.6898  LR: 0.000020  \n","Epoch: [1][22600/36908] Elapsed 233m 17s (remain 147m 40s) Loss: 0.0005(0.0156) Grad: 2369.4961  LR: 0.000020  \n","Epoch: [1][22700/36908] Elapsed 234m 19s (remain 146m 39s) Loss: 0.0005(0.0155) Grad: 999.9149  LR: 0.000019  \n","Epoch: [1][22800/36908] Elapsed 235m 21s (remain 145m 37s) Loss: 0.0056(0.0155) Grad: 73753.9766  LR: 0.000019  \n","Epoch: [1][22900/36908] Elapsed 236m 23s (remain 144m 35s) Loss: 0.0016(0.0154) Grad: 19059.0391  LR: 0.000019  \n","Epoch: [1][23000/36908] Elapsed 237m 25s (remain 143m 33s) Loss: 0.0001(0.0154) Grad: 84.7498  LR: 0.000019  \n","Epoch: [1][23100/36908] Elapsed 238m 27s (remain 142m 31s) Loss: 0.0001(0.0153) Grad: 33.9125  LR: 0.000019  \n","Epoch: [1][23200/36908] Elapsed 239m 28s (remain 141m 29s) Loss: 0.0003(0.0153) Grad: 833.7825  LR: 0.000019  \n","Epoch: [1][23300/36908] Elapsed 240m 30s (remain 140m 27s) Loss: 0.0006(0.0152) Grad: 4113.0737  LR: 0.000019  \n","Epoch: [1][23400/36908] Elapsed 241m 32s (remain 139m 24s) Loss: 0.0006(0.0151) Grad: 1063.2675  LR: 0.000019  \n","Epoch: [1][23500/36908] Elapsed 242m 34s (remain 138m 22s) Loss: 0.0004(0.0151) Grad: 77148.3594  LR: 0.000019  \n","Epoch: [1][23600/36908] Elapsed 243m 35s (remain 137m 20s) Loss: 0.0019(0.0150) Grad: 19433.0664  LR: 0.000019  \n","Epoch: [1][23700/36908] Elapsed 244m 37s (remain 136m 18s) Loss: 0.0002(0.0150) Grad: 365.4958  LR: 0.000019  \n","Epoch: [1][23800/36908] Elapsed 245m 39s (remain 135m 16s) Loss: 0.0017(0.0149) Grad: 19086.9746  LR: 0.000019  \n","Epoch: [1][23900/36908] Elapsed 246m 40s (remain 134m 14s) Loss: 0.0005(0.0149) Grad: 1185.9086  LR: 0.000019  \n","Epoch: [1][24000/36908] Elapsed 247m 42s (remain 133m 12s) Loss: 0.0013(0.0148) Grad: 12560.6611  LR: 0.000019  \n","Epoch: [1][24100/36908] Elapsed 248m 44s (remain 132m 10s) Loss: 0.0008(0.0148) Grad: 2509.2065  LR: 0.000019  \n","Epoch: [1][24200/36908] Elapsed 249m 46s (remain 131m 8s) Loss: 0.0143(0.0147) Grad: 123492.0781  LR: 0.000019  \n","Epoch: [1][24300/36908] Elapsed 250m 48s (remain 130m 6s) Loss: 0.0015(0.0147) Grad: 1522.1106  LR: 0.000019  \n","Epoch: [1][24400/36908] Elapsed 251m 49s (remain 129m 4s) Loss: 0.0053(0.0146) Grad: 47223.4297  LR: 0.000019  \n","Epoch: [1][24500/36908] Elapsed 252m 51s (remain 128m 2s) Loss: 0.0153(0.0146) Grad: 208428.1250  LR: 0.000019  \n","Epoch: [1][24600/36908] Elapsed 253m 53s (remain 127m 0s) Loss: 0.0002(0.0145) Grad: 1340.9453  LR: 0.000019  \n","Epoch: [1][24700/36908] Elapsed 254m 54s (remain 125m 58s) Loss: 0.0002(0.0145) Grad: 360.7762  LR: 0.000019  \n","Epoch: [1][24800/36908] Elapsed 255m 56s (remain 124m 56s) Loss: 0.0072(0.0144) Grad: 53517.9844  LR: 0.000019  \n","Epoch: [1][24900/36908] Elapsed 256m 58s (remain 123m 54s) Loss: 0.0011(0.0144) Grad: 2215.4197  LR: 0.000019  \n","Epoch: [1][25000/36908] Elapsed 258m 0s (remain 122m 52s) Loss: 0.0006(0.0143) Grad: 1182.2106  LR: 0.000019  \n","Epoch: [1][25100/36908] Elapsed 259m 2s (remain 121m 50s) Loss: 0.0005(0.0143) Grad: 1545.9302  LR: 0.000019  \n","Epoch: [1][25200/36908] Elapsed 260m 4s (remain 120m 48s) Loss: 0.0001(0.0142) Grad: 87.7314  LR: 0.000019  \n","Epoch: [1][25300/36908] Elapsed 261m 6s (remain 119m 47s) Loss: 0.0002(0.0142) Grad: 6239.7861  LR: 0.000019  \n","Epoch: [1][25400/36908] Elapsed 262m 8s (remain 118m 45s) Loss: 0.0006(0.0142) Grad: 2095.4045  LR: 0.000019  \n","Epoch: [1][25500/36908] Elapsed 263m 9s (remain 117m 43s) Loss: 0.0001(0.0141) Grad: 155.9128  LR: 0.000019  \n","Epoch: [1][25600/36908] Elapsed 264m 11s (remain 116m 41s) Loss: 0.0030(0.0141) Grad: 83854.4766  LR: 0.000019  \n","Epoch: [1][25700/36908] Elapsed 265m 13s (remain 115m 39s) Loss: 0.0016(0.0140) Grad: 27351.2402  LR: 0.000019  \n","Epoch: [1][25800/36908] Elapsed 266m 15s (remain 114m 37s) Loss: 0.0001(0.0140) Grad: 57.7388  LR: 0.000019  \n","Epoch: [1][25900/36908] Elapsed 267m 16s (remain 113m 35s) Loss: 0.0001(0.0139) Grad: 57.9924  LR: 0.000019  \n","Epoch: [1][26000/36908] Elapsed 268m 18s (remain 112m 33s) Loss: 0.0021(0.0139) Grad: 2887.9231  LR: 0.000019  \n","Epoch: [1][26100/36908] Elapsed 269m 20s (remain 111m 31s) Loss: 0.0002(0.0139) Grad: 87.4454  LR: 0.000019  \n","Epoch: [1][26200/36908] Elapsed 270m 22s (remain 110m 29s) Loss: 0.0004(0.0138) Grad: 664.8928  LR: 0.000019  \n","Epoch: [1][26300/36908] Elapsed 271m 24s (remain 109m 27s) Loss: 0.0030(0.0138) Grad: 13892.8691  LR: 0.000019  \n","Epoch: [1][26400/36908] Elapsed 272m 26s (remain 108m 25s) Loss: 0.0014(0.0137) Grad: 16717.1660  LR: 0.000019  \n","Epoch: [1][26500/36908] Elapsed 273m 28s (remain 107m 23s) Loss: 0.0001(0.0137) Grad: 547.9139  LR: 0.000019  \n","Epoch: [1][26600/36908] Elapsed 274m 29s (remain 106m 21s) Loss: 0.0000(0.0136) Grad: 75.8098  LR: 0.000019  \n","Epoch: [1][26700/36908] Elapsed 275m 31s (remain 105m 19s) Loss: 0.0011(0.0136) Grad: 19037.3789  LR: 0.000019  \n","Epoch: [1][26800/36908] Elapsed 276m 33s (remain 104m 17s) Loss: 0.0001(0.0136) Grad: 856.7568  LR: 0.000019  \n","Epoch: [1][26900/36908] Elapsed 277m 35s (remain 103m 15s) Loss: 0.0005(0.0135) Grad: 2982.9412  LR: 0.000019  \n","Epoch: [1][27000/36908] Elapsed 278m 36s (remain 102m 13s) Loss: 0.0024(0.0135) Grad: 42192.5195  LR: 0.000019  \n","Epoch: [1][27100/36908] Elapsed 279m 38s (remain 101m 11s) Loss: 0.0010(0.0134) Grad: 11483.5654  LR: 0.000019  \n","Epoch: [1][27200/36908] Elapsed 280m 39s (remain 100m 9s) Loss: 0.0030(0.0134) Grad: 88896.2266  LR: 0.000019  \n","Epoch: [1][27300/36908] Elapsed 281m 41s (remain 99m 7s) Loss: 0.0023(0.0133) Grad: 6942.9951  LR: 0.000019  \n","Epoch: [1][27400/36908] Elapsed 282m 43s (remain 98m 5s) Loss: 0.0008(0.0133) Grad: 10114.0459  LR: 0.000019  \n","Epoch: [1][27500/36908] Elapsed 283m 44s (remain 97m 3s) Loss: 0.0004(0.0133) Grad: 588.9075  LR: 0.000019  \n","Epoch: [1][27600/36908] Elapsed 284m 46s (remain 96m 1s) Loss: 0.0012(0.0132) Grad: 8155.5527  LR: 0.000019  \n","Epoch: [1][27700/36908] Elapsed 285m 48s (remain 94m 59s) Loss: 0.0002(0.0132) Grad: 137.6199  LR: 0.000019  \n","Epoch: [1][27800/36908] Elapsed 286m 49s (remain 93m 57s) Loss: 0.0039(0.0132) Grad: 53655.1562  LR: 0.000019  \n","Epoch: [1][27900/36908] Elapsed 287m 51s (remain 92m 55s) Loss: 0.0009(0.0131) Grad: 1538.2085  LR: 0.000019  \n","Epoch: [1][28000/36908] Elapsed 288m 53s (remain 91m 53s) Loss: 0.0034(0.0131) Grad: 65663.8672  LR: 0.000019  \n","Epoch: [1][28100/36908] Elapsed 289m 55s (remain 90m 51s) Loss: 0.0008(0.0130) Grad: 930.4383  LR: 0.000019  \n","Epoch: [1][28200/36908] Elapsed 290m 57s (remain 89m 49s) Loss: 0.0000(0.0130) Grad: 17.4835  LR: 0.000019  \n","Epoch: [1][28300/36908] Elapsed 291m 58s (remain 88m 47s) Loss: 0.0017(0.0130) Grad: 39043.6914  LR: 0.000019  \n","Epoch: [1][28400/36908] Elapsed 293m 0s (remain 87m 45s) Loss: 0.0010(0.0129) Grad: 1240.2906  LR: 0.000019  \n","Epoch: [1][28500/36908] Elapsed 294m 2s (remain 86m 43s) Loss: 0.0030(0.0129) Grad: 18501.5234  LR: 0.000019  \n","Epoch: [1][28600/36908] Elapsed 295m 3s (remain 85m 41s) Loss: 0.0001(0.0129) Grad: 1518.8314  LR: 0.000019  \n","Epoch: [1][28700/36908] Elapsed 296m 5s (remain 84m 39s) Loss: 0.0034(0.0128) Grad: 59568.0586  LR: 0.000019  \n","Epoch: [1][28800/36908] Elapsed 297m 7s (remain 83m 38s) Loss: 0.0002(0.0128) Grad: 77.1993  LR: 0.000019  \n","Epoch: [1][28900/36908] Elapsed 298m 8s (remain 82m 36s) Loss: 0.0001(0.0128) Grad: 33.9356  LR: 0.000019  \n","Epoch: [1][29000/36908] Elapsed 299m 10s (remain 81m 34s) Loss: 0.0007(0.0127) Grad: 9926.7627  LR: 0.000019  \n","Epoch: [1][29100/36908] Elapsed 300m 12s (remain 80m 32s) Loss: 0.0001(0.0127) Grad: 22.5808  LR: 0.000019  \n","Epoch: [1][29200/36908] Elapsed 301m 14s (remain 79m 30s) Loss: 0.0009(0.0127) Grad: 2535.0288  LR: 0.000019  \n","Epoch: [1][29300/36908] Elapsed 302m 16s (remain 78m 28s) Loss: 0.0015(0.0126) Grad: 5598.2891  LR: 0.000019  \n","Epoch: [1][29400/36908] Elapsed 303m 18s (remain 77m 26s) Loss: 0.0018(0.0126) Grad: 11926.4414  LR: 0.000019  \n","Epoch: [1][29500/36908] Elapsed 304m 20s (remain 76m 24s) Loss: 0.0002(0.0126) Grad: 111.8828  LR: 0.000019  \n","Epoch: [1][29600/36908] Elapsed 305m 21s (remain 75m 22s) Loss: 0.0009(0.0125) Grad: 1887.7625  LR: 0.000019  \n","Epoch: [1][29700/36908] Elapsed 306m 23s (remain 74m 20s) Loss: 0.0028(0.0125) Grad: 48774.9766  LR: 0.000019  \n","Epoch: [1][29800/36908] Elapsed 307m 25s (remain 73m 18s) Loss: 0.0022(0.0125) Grad: 13109.9111  LR: 0.000019  \n","Epoch: [1][29900/36908] Elapsed 308m 26s (remain 72m 16s) Loss: 0.0028(0.0124) Grad: 19444.1895  LR: 0.000019  \n","Epoch: [1][30000/36908] Elapsed 309m 28s (remain 71m 15s) Loss: 0.0004(0.0124) Grad: 558.2027  LR: 0.000019  \n","Epoch: [1][30100/36908] Elapsed 310m 30s (remain 70m 13s) Loss: 0.0017(0.0124) Grad: 54394.1016  LR: 0.000019  \n","Epoch: [1][30200/36908] Elapsed 311m 32s (remain 69m 11s) Loss: 0.0058(0.0123) Grad: 56993.2344  LR: 0.000019  \n","Epoch: [1][30300/36908] Elapsed 312m 34s (remain 68m 9s) Loss: 0.0014(0.0123) Grad: 14610.7021  LR: 0.000019  \n","Epoch: [1][30400/36908] Elapsed 313m 36s (remain 67m 7s) Loss: 0.0069(0.0123) Grad: 73011.6875  LR: 0.000019  \n","Epoch: [1][30500/36908] Elapsed 314m 38s (remain 66m 5s) Loss: 0.0029(0.0122) Grad: 21583.1836  LR: 0.000019  \n","Epoch: [1][30600/36908] Elapsed 315m 40s (remain 65m 3s) Loss: 0.0022(0.0122) Grad: 17189.1816  LR: 0.000019  \n","Epoch: [1][30700/36908] Elapsed 316m 42s (remain 64m 1s) Loss: 0.0001(0.0122) Grad: 504.4310  LR: 0.000019  \n","Epoch: [1][30800/36908] Elapsed 317m 43s (remain 62m 59s) Loss: 0.0001(0.0121) Grad: 112.5337  LR: 0.000019  \n","Epoch: [1][30900/36908] Elapsed 318m 45s (remain 61m 57s) Loss: 0.0005(0.0121) Grad: 6359.8613  LR: 0.000019  \n","Epoch: [1][31000/36908] Elapsed 319m 47s (remain 60m 56s) Loss: 0.0042(0.0121) Grad: 60148.0469  LR: 0.000018  \n","Epoch: [1][31100/36908] Elapsed 320m 49s (remain 59m 54s) Loss: 0.0003(0.0120) Grad: 628.5964  LR: 0.000018  \n","Epoch: [1][31200/36908] Elapsed 321m 51s (remain 58m 52s) Loss: 0.0002(0.0120) Grad: 296.1950  LR: 0.000018  \n","Epoch: [1][31300/36908] Elapsed 322m 53s (remain 57m 50s) Loss: 0.0022(0.0120) Grad: 37798.6211  LR: 0.000018  \n","Epoch: [1][31400/36908] Elapsed 323m 55s (remain 56m 48s) Loss: 0.0008(0.0119) Grad: 2040.1464  LR: 0.000018  \n","Epoch: [1][31500/36908] Elapsed 324m 57s (remain 55m 46s) Loss: 0.0001(0.0119) Grad: 1098.9794  LR: 0.000018  \n","Epoch: [1][31600/36908] Elapsed 325m 59s (remain 54m 44s) Loss: 0.0007(0.0119) Grad: 5292.6572  LR: 0.000018  \n","Epoch: [1][31700/36908] Elapsed 327m 1s (remain 53m 42s) Loss: 0.0051(0.0119) Grad: 21875.4609  LR: 0.000018  \n","Epoch: [1][31800/36908] Elapsed 328m 3s (remain 52m 41s) Loss: 0.0001(0.0118) Grad: 1046.9354  LR: 0.000018  \n","Epoch: [1][31900/36908] Elapsed 329m 5s (remain 51m 39s) Loss: 0.0001(0.0118) Grad: 22.3051  LR: 0.000018  \n","Epoch: [1][32000/36908] Elapsed 330m 7s (remain 50m 37s) Loss: 0.0029(0.0118) Grad: 95981.2109  LR: 0.000018  \n","Epoch: [1][32100/36908] Elapsed 331m 9s (remain 49m 35s) Loss: 0.0059(0.0117) Grad: 33776.4102  LR: 0.000018  \n","Epoch: [1][32200/36908] Elapsed 332m 11s (remain 48m 33s) Loss: 0.0026(0.0117) Grad: 40916.9727  LR: 0.000018  \n","Epoch: [1][32300/36908] Elapsed 333m 12s (remain 47m 31s) Loss: 0.0003(0.0117) Grad: 1601.7169  LR: 0.000018  \n","Epoch: [1][32400/36908] Elapsed 334m 15s (remain 46m 29s) Loss: 0.0001(0.0116) Grad: 32.1587  LR: 0.000018  \n","Epoch: [1][32500/36908] Elapsed 335m 17s (remain 45m 27s) Loss: 0.0002(0.0116) Grad: 96.2184  LR: 0.000018  \n","Epoch: [1][32600/36908] Elapsed 336m 19s (remain 44m 25s) Loss: 0.0001(0.0116) Grad: 60.0430  LR: 0.000018  \n","Epoch: [1][32700/36908] Elapsed 337m 20s (remain 43m 23s) Loss: 0.0000(0.0116) Grad: 35.2634  LR: 0.000018  \n","Epoch: [1][32800/36908] Elapsed 338m 22s (remain 42m 22s) Loss: 0.0002(0.0115) Grad: 268.0734  LR: 0.000018  \n","Epoch: [1][32900/36908] Elapsed 339m 24s (remain 41m 20s) Loss: 0.0028(0.0115) Grad: 43084.5586  LR: 0.000018  \n","Epoch: [1][33000/36908] Elapsed 340m 26s (remain 40m 18s) Loss: 0.0005(0.0115) Grad: 1857.2551  LR: 0.000018  \n","Epoch: [1][33100/36908] Elapsed 341m 28s (remain 39m 16s) Loss: 0.0076(0.0114) Grad: 100636.8828  LR: 0.000018  \n","Epoch: [1][33200/36908] Elapsed 342m 30s (remain 38m 14s) Loss: 0.0011(0.0114) Grad: 9441.7305  LR: 0.000018  \n","Epoch: [1][33300/36908] Elapsed 343m 32s (remain 37m 12s) Loss: 0.0000(0.0114) Grad: 201.7200  LR: 0.000018  \n","Epoch: [1][33400/36908] Elapsed 344m 35s (remain 36m 10s) Loss: 0.0035(0.0114) Grad: 31915.4688  LR: 0.000018  \n","Epoch: [1][33500/36908] Elapsed 345m 36s (remain 35m 8s) Loss: 0.0007(0.0113) Grad: 2168.9055  LR: 0.000018  \n","Epoch: [1][33600/36908] Elapsed 346m 38s (remain 34m 7s) Loss: 0.0001(0.0113) Grad: 20.4150  LR: 0.000018  \n","Epoch: [1][33700/36908] Elapsed 347m 40s (remain 33m 5s) Loss: 0.0013(0.0113) Grad: 2270.7805  LR: 0.000018  \n","Epoch: [1][33800/36908] Elapsed 348m 42s (remain 32m 3s) Loss: 0.0170(0.0113) Grad: 274191.0000  LR: 0.000018  \n","Epoch: [1][33900/36908] Elapsed 349m 44s (remain 31m 1s) Loss: 0.0051(0.0112) Grad: 85603.2344  LR: 0.000018  \n","Epoch: [1][34000/36908] Elapsed 350m 46s (remain 29m 59s) Loss: 0.0020(0.0112) Grad: 88497.2969  LR: 0.000018  \n","Epoch: [1][34100/36908] Elapsed 351m 48s (remain 28m 57s) Loss: 0.0004(0.0112) Grad: 1047.2815  LR: 0.000018  \n","Epoch: [1][34200/36908] Elapsed 352m 50s (remain 27m 55s) Loss: 0.0000(0.0112) Grad: 16.2745  LR: 0.000018  \n","Epoch: [1][34300/36908] Elapsed 353m 52s (remain 26m 53s) Loss: 0.0002(0.0111) Grad: 178.8630  LR: 0.000018  \n","Epoch: [1][34400/36908] Elapsed 354m 54s (remain 25m 51s) Loss: 0.0085(0.0111) Grad: 233964.5469  LR: 0.000018  \n","Epoch: [1][34500/36908] Elapsed 355m 56s (remain 24m 49s) Loss: 0.0070(0.0111) Grad: 60047.1211  LR: 0.000018  \n","Epoch: [1][34600/36908] Elapsed 356m 58s (remain 23m 48s) Loss: 0.0065(0.0111) Grad: 355328.4688  LR: 0.000018  \n","Epoch: [1][34700/36908] Elapsed 358m 0s (remain 22m 46s) Loss: 0.0018(0.0110) Grad: 41935.6797  LR: 0.000018  \n","Epoch: [1][34800/36908] Elapsed 359m 2s (remain 21m 44s) Loss: 0.0065(0.0110) Grad: 351829.6250  LR: 0.000018  \n","Epoch: [1][34900/36908] Elapsed 360m 5s (remain 20m 42s) Loss: 0.0006(0.0110) Grad: 6218.0269  LR: 0.000018  \n","Epoch: [1][35000/36908] Elapsed 361m 7s (remain 19m 40s) Loss: 0.0004(0.0110) Grad: 7118.4854  LR: 0.000018  \n","Epoch: [1][35100/36908] Elapsed 362m 8s (remain 18m 38s) Loss: 0.0037(0.0109) Grad: 72089.2812  LR: 0.000018  \n","Epoch: [1][35200/36908] Elapsed 363m 10s (remain 17m 36s) Loss: 0.0083(0.0109) Grad: 203618.2031  LR: 0.000018  \n","Epoch: [1][35300/36908] Elapsed 364m 12s (remain 16m 34s) Loss: 0.0001(0.0109) Grad: 78.5009  LR: 0.000018  \n","Epoch: [1][35400/36908] Elapsed 365m 14s (remain 15m 32s) Loss: 0.0028(0.0109) Grad: 5924.3848  LR: 0.000018  \n","Epoch: [1][35500/36908] Elapsed 366m 16s (remain 14m 30s) Loss: 0.0001(0.0108) Grad: 118.0219  LR: 0.000018  \n","Epoch: [1][35600/36908] Elapsed 367m 18s (remain 13m 29s) Loss: 0.0029(0.0108) Grad: 47077.0234  LR: 0.000018  \n","Epoch: [1][35700/36908] Elapsed 368m 20s (remain 12m 27s) Loss: 0.0068(0.0108) Grad: 243994.9844  LR: 0.000018  \n","Epoch: [1][35800/36908] Elapsed 369m 22s (remain 11m 25s) Loss: 0.0014(0.0108) Grad: 2776.6260  LR: 0.000018  \n","Epoch: [1][35900/36908] Elapsed 370m 24s (remain 10m 23s) Loss: 0.0006(0.0107) Grad: 28911.3027  LR: 0.000018  \n","Epoch: [1][36000/36908] Elapsed 371m 26s (remain 9m 21s) Loss: 0.0005(0.0107) Grad: 4157.0264  LR: 0.000018  \n","Epoch: [1][36100/36908] Elapsed 372m 28s (remain 8m 19s) Loss: 0.0054(0.0107) Grad: 99623.1094  LR: 0.000018  \n","Epoch: [1][36200/36908] Elapsed 373m 30s (remain 7m 17s) Loss: 0.0002(0.0107) Grad: 807.7932  LR: 0.000018  \n","Epoch: [1][36300/36908] Elapsed 374m 32s (remain 6m 15s) Loss: 0.0049(0.0106) Grad: 54574.1719  LR: 0.000018  \n","Epoch: [1][36400/36908] Elapsed 375m 35s (remain 5m 13s) Loss: 0.0001(0.0106) Grad: 87.2435  LR: 0.000018  \n","Epoch: [1][36500/36908] Elapsed 376m 36s (remain 4m 11s) Loss: 0.0031(0.0106) Grad: 48255.0977  LR: 0.000018  \n","Epoch: [1][36600/36908] Elapsed 377m 38s (remain 3m 10s) Loss: 0.0012(0.0106) Grad: 2191.1658  LR: 0.000018  \n","Epoch: [1][36700/36908] Elapsed 378m 40s (remain 2m 8s) Loss: 0.0009(0.0106) Grad: 16455.9727  LR: 0.000018  \n","Epoch: [1][36800/36908] Elapsed 379m 42s (remain 1m 6s) Loss: 0.0010(0.0105) Grad: 9910.5625  LR: 0.000018  \n","Epoch: [1][36900/36908] Elapsed 380m 44s (remain 0m 4s) Loss: 0.0011(0.0105) Grad: 3517.8481  LR: 0.000018  \n","Epoch: [1][36907/36908] Elapsed 380m 48s (remain 0m 0s) Loss: 0.0299(0.0105) Grad: 613216.5625  LR: 0.000018  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 10m 45s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 22s (remain 4m 7s) Loss: 0.0002(0.0058) \n","EVAL: [200/1192] Elapsed 0m 45s (remain 3m 43s) Loss: 0.0000(0.0069) \n","EVAL: [300/1192] Elapsed 1m 7s (remain 3m 19s) Loss: 0.0013(0.0111) \n","EVAL: [400/1192] Elapsed 1m 29s (remain 2m 57s) Loss: 0.0294(0.0111) \n","EVAL: [500/1192] Elapsed 1m 52s (remain 2m 34s) Loss: 0.0315(0.0101) \n","EVAL: [600/1192] Elapsed 2m 14s (remain 2m 12s) Loss: 0.1249(0.0101) \n","EVAL: [700/1192] Elapsed 2m 36s (remain 1m 49s) Loss: 0.0049(0.0113) \n","EVAL: [800/1192] Elapsed 2m 58s (remain 1m 27s) Loss: 0.0049(0.0110) \n","EVAL: [900/1192] Elapsed 3m 21s (remain 1m 4s) Loss: 0.0025(0.0106) \n","EVAL: [1000/1192] Elapsed 3m 43s (remain 0m 42s) Loss: 0.0000(0.0103) \n","EVAL: [1100/1192] Elapsed 4m 6s (remain 0m 20s) Loss: 0.0073(0.0099) \n","EVAL: [1191/1192] Elapsed 4m 26s (remain 0m 0s) Loss: 0.0107(0.0093) \n","Epoch 1 - avg_train_loss: 0.0105  avg_val_loss: 0.0093  time: 23117s\n","Epoch 1 - Score: 0.8822\n","Epoch 1 - Save Best Score: 0.8822 Model\n","========== fold: 2 training ==========\n","(612602, 6) (612602, 950)\n","(100000, 7)\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n","Epoch: [1][0/36908] Elapsed 0m 0s (remain 563m 28s) Loss: 0.3619(0.3619) Grad: 109071.6094  LR: 0.000000  \n","Epoch: [1][100/36908] Elapsed 1m 3s (remain 383m 55s) Loss: 0.3475(0.3572) Grad: 25342.5684  LR: 0.000000  \n","Epoch: [1][200/36908] Elapsed 2m 5s (remain 381m 24s) Loss: 0.3371(0.3509) Grad: 26400.8047  LR: 0.000000  \n","Epoch: [1][300/36908] Elapsed 3m 7s (remain 379m 36s) Loss: 0.3132(0.3411) Grad: 12445.1396  LR: 0.000000  \n","Epoch: [1][400/36908] Elapsed 4m 9s (remain 378m 35s) Loss: 0.2575(0.3265) Grad: 10555.5283  LR: 0.000000  \n","Epoch: [1][500/36908] Elapsed 5m 11s (remain 377m 27s) Loss: 0.2128(0.3080) Grad: 9967.2109  LR: 0.000001  \n","Epoch: [1][600/36908] Elapsed 6m 13s (remain 376m 10s) Loss: 0.1465(0.2868) Grad: 9339.3301  LR: 0.000001  \n","Epoch: [1][700/36908] Elapsed 7m 15s (remain 374m 51s) Loss: 0.0893(0.2641) Grad: 6558.7817  LR: 0.000001  \n","Epoch: [1][800/36908] Elapsed 8m 17s (remain 373m 35s) Loss: 0.0455(0.2410) Grad: 3964.8962  LR: 0.000001  \n","Epoch: [1][900/36908] Elapsed 9m 19s (remain 372m 20s) Loss: 0.0277(0.2199) Grad: 1553.8997  LR: 0.000001  \n","Epoch: [1][1000/36908] Elapsed 10m 20s (remain 371m 9s) Loss: 0.0268(0.2020) Grad: 515.7057  LR: 0.000001  \n","Epoch: [1][1100/36908] Elapsed 11m 22s (remain 370m 0s) Loss: 0.0143(0.1869) Grad: 614.4888  LR: 0.000001  \n","Epoch: [1][1200/36908] Elapsed 12m 24s (remain 368m 57s) Loss: 0.0224(0.1746) Grad: 478.6185  LR: 0.000001  \n","Epoch: [1][1300/36908] Elapsed 13m 26s (remain 367m 56s) Loss: 0.0431(0.1641) Grad: 729.3839  LR: 0.000001  \n","Epoch: [1][1400/36908] Elapsed 14m 28s (remain 366m 51s) Loss: 0.0137(0.1550) Grad: 564.4598  LR: 0.000002  \n","Epoch: [1][1500/36908] Elapsed 15m 30s (remain 365m 50s) Loss: 0.0233(0.1471) Grad: 445.4814  LR: 0.000002  \n","Epoch: [1][1600/36908] Elapsed 16m 32s (remain 364m 52s) Loss: 0.0858(0.1401) Grad: 2197.4661  LR: 0.000002  \n","Epoch: [1][1700/36908] Elapsed 17m 34s (remain 363m 51s) Loss: 0.0421(0.1339) Grad: 789.8650  LR: 0.000002  \n","Epoch: [1][1800/36908] Elapsed 18m 36s (remain 362m 47s) Loss: 0.0064(0.1281) Grad: 1020.5696  LR: 0.000002  \n","Epoch: [1][1900/36908] Elapsed 19m 38s (remain 361m 48s) Loss: 0.0207(0.1231) Grad: 1688.5304  LR: 0.000002  \n","Epoch: [1][2000/36908] Elapsed 20m 40s (remain 360m 46s) Loss: 0.0253(0.1180) Grad: 4363.5420  LR: 0.000002  \n","Epoch: [1][2100/36908] Elapsed 21m 42s (remain 359m 43s) Loss: 0.0076(0.1132) Grad: 3706.9507  LR: 0.000002  \n","Epoch: [1][2200/36908] Elapsed 22m 44s (remain 358m 39s) Loss: 0.0065(0.1087) Grad: 1626.9066  LR: 0.000002  \n","Epoch: [1][2300/36908] Elapsed 23m 46s (remain 357m 36s) Loss: 0.0250(0.1048) Grad: 10738.1445  LR: 0.000002  \n","Epoch: [1][2400/36908] Elapsed 24m 48s (remain 356m 33s) Loss: 0.0094(0.1009) Grad: 2894.2842  LR: 0.000003  \n","Epoch: [1][2500/36908] Elapsed 25m 50s (remain 355m 30s) Loss: 0.0173(0.0974) Grad: 5594.5669  LR: 0.000003  \n","Epoch: [1][2600/36908] Elapsed 26m 52s (remain 354m 28s) Loss: 0.0029(0.0942) Grad: 760.7898  LR: 0.000003  \n","Epoch: [1][2700/36908] Elapsed 27m 54s (remain 353m 28s) Loss: 0.0157(0.0912) Grad: 6184.4297  LR: 0.000003  \n","Epoch: [1][2800/36908] Elapsed 28m 56s (remain 352m 26s) Loss: 0.0111(0.0884) Grad: 3748.1885  LR: 0.000003  \n","Epoch: [1][2900/36908] Elapsed 29m 58s (remain 351m 24s) Loss: 0.0055(0.0857) Grad: 1811.9302  LR: 0.000003  \n","Epoch: [1][3000/36908] Elapsed 31m 0s (remain 350m 25s) Loss: 0.0048(0.0831) Grad: 996.0707  LR: 0.000003  \n","Epoch: [1][3100/36908] Elapsed 32m 2s (remain 349m 23s) Loss: 0.0101(0.0808) Grad: 2358.4917  LR: 0.000003  \n","Epoch: [1][3200/36908] Elapsed 33m 4s (remain 348m 20s) Loss: 0.0053(0.0785) Grad: 2505.3220  LR: 0.000003  \n","Epoch: [1][3300/36908] Elapsed 34m 6s (remain 347m 18s) Loss: 0.0085(0.0764) Grad: 2127.4497  LR: 0.000004  \n","Epoch: [1][3400/36908] Elapsed 35m 9s (remain 346m 18s) Loss: 0.0020(0.0744) Grad: 866.5940  LR: 0.000004  \n","Epoch: [1][3500/36908] Elapsed 36m 11s (remain 345m 18s) Loss: 0.0134(0.0726) Grad: 9588.3467  LR: 0.000004  \n","Epoch: [1][3600/36908] Elapsed 37m 13s (remain 344m 17s) Loss: 0.0054(0.0708) Grad: 2534.6094  LR: 0.000004  \n","Epoch: [1][3700/36908] Elapsed 38m 15s (remain 343m 16s) Loss: 0.0227(0.0691) Grad: 6225.2134  LR: 0.000004  \n","Epoch: [1][3800/36908] Elapsed 39m 17s (remain 342m 14s) Loss: 0.0097(0.0675) Grad: 3130.4504  LR: 0.000004  \n","Epoch: [1][3900/36908] Elapsed 40m 19s (remain 341m 11s) Loss: 0.0022(0.0660) Grad: 448.2492  LR: 0.000004  \n","Epoch: [1][4000/36908] Elapsed 41m 21s (remain 340m 9s) Loss: 0.0037(0.0645) Grad: 1891.0236  LR: 0.000004  \n","Epoch: [1][4100/36908] Elapsed 42m 23s (remain 339m 5s) Loss: 0.0031(0.0631) Grad: 563.4766  LR: 0.000004  \n","Epoch: [1][4200/36908] Elapsed 43m 25s (remain 338m 2s) Loss: 0.0256(0.0618) Grad: 8118.1030  LR: 0.000005  \n","Epoch: [1][4300/36908] Elapsed 44m 26s (remain 336m 59s) Loss: 0.0020(0.0605) Grad: 747.6895  LR: 0.000005  \n","Epoch: [1][4400/36908] Elapsed 45m 28s (remain 335m 56s) Loss: 0.0021(0.0594) Grad: 2307.8425  LR: 0.000005  \n","Epoch: [1][4500/36908] Elapsed 46m 30s (remain 334m 54s) Loss: 0.0096(0.0582) Grad: 21487.3047  LR: 0.000005  \n","Epoch: [1][4600/36908] Elapsed 47m 32s (remain 333m 52s) Loss: 0.0017(0.0571) Grad: 1056.4259  LR: 0.000005  \n","Epoch: [1][4700/36908] Elapsed 48m 34s (remain 332m 49s) Loss: 0.0091(0.0560) Grad: 7965.1421  LR: 0.000005  \n","Epoch: [1][4800/36908] Elapsed 49m 36s (remain 331m 46s) Loss: 0.0000(0.0550) Grad: 24.0038  LR: 0.000005  \n","Epoch: [1][4900/36908] Elapsed 50m 38s (remain 330m 45s) Loss: 0.0026(0.0541) Grad: 2822.6970  LR: 0.000005  \n","Epoch: [1][5000/36908] Elapsed 51m 41s (remain 329m 44s) Loss: 0.0012(0.0532) Grad: 1007.0776  LR: 0.000005  \n","Epoch: [1][5100/36908] Elapsed 52m 42s (remain 328m 41s) Loss: 0.0026(0.0522) Grad: 3163.1880  LR: 0.000006  \n","Epoch: [1][5200/36908] Elapsed 53m 44s (remain 327m 38s) Loss: 0.0013(0.0513) Grad: 318.6159  LR: 0.000006  \n","Epoch: [1][5300/36908] Elapsed 54m 46s (remain 326m 35s) Loss: 0.0041(0.0505) Grad: 4119.5752  LR: 0.000006  \n","Epoch: [1][5400/36908] Elapsed 55m 48s (remain 325m 33s) Loss: 0.0040(0.0497) Grad: 5081.9517  LR: 0.000006  \n","Epoch: [1][5500/36908] Elapsed 56m 50s (remain 324m 29s) Loss: 0.0019(0.0489) Grad: 883.2197  LR: 0.000006  \n","Epoch: [1][5600/36908] Elapsed 57m 52s (remain 323m 27s) Loss: 0.0004(0.0481) Grad: 510.5233  LR: 0.000006  \n","Epoch: [1][5700/36908] Elapsed 58m 53s (remain 322m 24s) Loss: 0.0014(0.0474) Grad: 3254.6714  LR: 0.000006  \n","Epoch: [1][5800/36908] Elapsed 59m 55s (remain 321m 22s) Loss: 0.0013(0.0466) Grad: 2150.3262  LR: 0.000006  \n","Epoch: [1][5900/36908] Elapsed 60m 58s (remain 320m 21s) Loss: 0.0005(0.0460) Grad: 195.6614  LR: 0.000006  \n","Epoch: [1][6000/36908] Elapsed 62m 0s (remain 319m 21s) Loss: 0.0193(0.0453) Grad: 10697.5088  LR: 0.000007  \n","Epoch: [1][6100/36908] Elapsed 63m 2s (remain 318m 19s) Loss: 0.0062(0.0446) Grad: 19320.9375  LR: 0.000007  \n","Epoch: [1][6200/36908] Elapsed 64m 4s (remain 317m 17s) Loss: 0.0004(0.0440) Grad: 105.7028  LR: 0.000007  \n","Epoch: [1][6300/36908] Elapsed 65m 6s (remain 316m 14s) Loss: 0.0051(0.0434) Grad: 9627.8447  LR: 0.000007  \n","Epoch: [1][6400/36908] Elapsed 66m 8s (remain 315m 13s) Loss: 0.0051(0.0428) Grad: 5423.4614  LR: 0.000007  \n","Epoch: [1][6500/36908] Elapsed 67m 10s (remain 314m 12s) Loss: 0.0103(0.0423) Grad: 7047.2017  LR: 0.000007  \n","Epoch: [1][6600/36908] Elapsed 68m 12s (remain 313m 9s) Loss: 0.0076(0.0417) Grad: 3503.6589  LR: 0.000007  \n","Epoch: [1][6700/36908] Elapsed 69m 14s (remain 312m 7s) Loss: 0.0029(0.0412) Grad: 2266.5623  LR: 0.000007  \n","Epoch: [1][6800/36908] Elapsed 70m 16s (remain 311m 6s) Loss: 0.0039(0.0406) Grad: 1689.9841  LR: 0.000007  \n","Epoch: [1][6900/36908] Elapsed 71m 18s (remain 310m 3s) Loss: 0.0002(0.0401) Grad: 83.2037  LR: 0.000007  \n","Epoch: [1][7000/36908] Elapsed 72m 20s (remain 309m 1s) Loss: 0.0182(0.0396) Grad: 12730.0498  LR: 0.000008  \n","Epoch: [1][7100/36908] Elapsed 73m 22s (remain 307m 59s) Loss: 0.0020(0.0391) Grad: 2526.2346  LR: 0.000008  \n","Epoch: [1][7200/36908] Elapsed 74m 24s (remain 306m 58s) Loss: 0.0013(0.0387) Grad: 499.5522  LR: 0.000008  \n","Epoch: [1][7300/36908] Elapsed 75m 26s (remain 305m 56s) Loss: 0.0192(0.0382) Grad: 2016.4531  LR: 0.000008  \n","Epoch: [1][7400/36908] Elapsed 76m 28s (remain 304m 55s) Loss: 0.0008(0.0378) Grad: 440.7208  LR: 0.000008  \n","Epoch: [1][7500/36908] Elapsed 77m 30s (remain 303m 53s) Loss: 0.0002(0.0373) Grad: 40.2307  LR: 0.000008  \n","Epoch: [1][7600/36908] Elapsed 78m 32s (remain 302m 51s) Loss: 0.0052(0.0369) Grad: 5096.2646  LR: 0.000008  \n","Epoch: [1][7700/36908] Elapsed 79m 34s (remain 301m 49s) Loss: 0.0094(0.0365) Grad: 13276.9385  LR: 0.000008  \n","Epoch: [1][7800/36908] Elapsed 80m 37s (remain 300m 47s) Loss: 0.0012(0.0361) Grad: 1602.4039  LR: 0.000008  \n","Epoch: [1][7900/36908] Elapsed 81m 39s (remain 299m 46s) Loss: 0.0004(0.0357) Grad: 80.2701  LR: 0.000009  \n","Epoch: [1][8000/36908] Elapsed 82m 41s (remain 298m 45s) Loss: 0.0010(0.0353) Grad: 212.1022  LR: 0.000009  \n","Epoch: [1][8100/36908] Elapsed 83m 43s (remain 297m 43s) Loss: 0.0035(0.0349) Grad: 1172.0986  LR: 0.000009  \n","Epoch: [1][8200/36908] Elapsed 84m 45s (remain 296m 42s) Loss: 0.0060(0.0346) Grad: 2093.1348  LR: 0.000009  \n","Epoch: [1][8300/36908] Elapsed 85m 47s (remain 295m 40s) Loss: 0.0008(0.0342) Grad: 2944.4622  LR: 0.000009  \n","Epoch: [1][8400/36908] Elapsed 86m 49s (remain 294m 38s) Loss: 0.0008(0.0339) Grad: 1557.6796  LR: 0.000009  \n","Epoch: [1][8500/36908] Elapsed 87m 51s (remain 293m 36s) Loss: 0.0050(0.0335) Grad: 15119.6064  LR: 0.000009  \n","Epoch: [1][8600/36908] Elapsed 88m 53s (remain 292m 34s) Loss: 0.0010(0.0332) Grad: 1027.7003  LR: 0.000009  \n","Epoch: [1][8700/36908] Elapsed 89m 55s (remain 291m 32s) Loss: 0.0159(0.0328) Grad: 10802.1895  LR: 0.000009  \n","Epoch: [1][8800/36908] Elapsed 90m 58s (remain 290m 31s) Loss: 0.0053(0.0325) Grad: 12926.4902  LR: 0.000010  \n","Epoch: [1][8900/36908] Elapsed 92m 0s (remain 289m 29s) Loss: 0.0001(0.0322) Grad: 258.8123  LR: 0.000010  \n","Epoch: [1][9000/36908] Elapsed 93m 2s (remain 288m 28s) Loss: 0.0153(0.0319) Grad: 33717.6797  LR: 0.000010  \n","Epoch: [1][9100/36908] Elapsed 94m 4s (remain 287m 26s) Loss: 0.0074(0.0316) Grad: 6127.1357  LR: 0.000010  \n","Epoch: [1][9200/36908] Elapsed 95m 6s (remain 286m 24s) Loss: 0.0002(0.0313) Grad: 122.8814  LR: 0.000010  \n","Epoch: [1][9300/36908] Elapsed 96m 8s (remain 285m 22s) Loss: 0.0003(0.0310) Grad: 599.9828  LR: 0.000010  \n","Epoch: [1][9400/36908] Elapsed 97m 10s (remain 284m 20s) Loss: 0.0054(0.0307) Grad: 30647.0234  LR: 0.000010  \n","Epoch: [1][9500/36908] Elapsed 98m 13s (remain 283m 19s) Loss: 0.0052(0.0304) Grad: 10555.6914  LR: 0.000010  \n","Epoch: [1][9600/36908] Elapsed 99m 15s (remain 282m 17s) Loss: 0.0016(0.0302) Grad: 1915.1669  LR: 0.000010  \n","Epoch: [1][9700/36908] Elapsed 100m 17s (remain 281m 15s) Loss: 0.0034(0.0299) Grad: 3711.5454  LR: 0.000011  \n","Epoch: [1][9800/36908] Elapsed 101m 18s (remain 280m 12s) Loss: 0.0005(0.0296) Grad: 660.9085  LR: 0.000011  \n","Epoch: [1][9900/36908] Elapsed 102m 20s (remain 279m 10s) Loss: 0.0016(0.0294) Grad: 6231.3052  LR: 0.000011  \n","Epoch: [1][10000/36908] Elapsed 103m 22s (remain 278m 8s) Loss: 0.0001(0.0291) Grad: 41.6860  LR: 0.000011  \n","Epoch: [1][10100/36908] Elapsed 104m 24s (remain 277m 6s) Loss: 0.0003(0.0289) Grad: 1188.6771  LR: 0.000011  \n","Epoch: [1][10200/36908] Elapsed 105m 27s (remain 276m 4s) Loss: 0.0022(0.0286) Grad: 4912.4878  LR: 0.000011  \n","Epoch: [1][10300/36908] Elapsed 106m 29s (remain 275m 3s) Loss: 0.0033(0.0284) Grad: 4800.2573  LR: 0.000011  \n","Epoch: [1][10400/36908] Elapsed 107m 31s (remain 274m 1s) Loss: 0.0059(0.0282) Grad: 18659.9297  LR: 0.000011  \n","Epoch: [1][10500/36908] Elapsed 108m 33s (remain 272m 59s) Loss: 0.0001(0.0279) Grad: 483.7764  LR: 0.000011  \n","Epoch: [1][10600/36908] Elapsed 109m 35s (remain 271m 57s) Loss: 0.0007(0.0277) Grad: 466.4833  LR: 0.000011  \n","Epoch: [1][10700/36908] Elapsed 110m 37s (remain 270m 55s) Loss: 0.0002(0.0275) Grad: 206.9803  LR: 0.000012  \n","Epoch: [1][10800/36908] Elapsed 111m 39s (remain 269m 53s) Loss: 0.0053(0.0273) Grad: 13761.5137  LR: 0.000012  \n","Epoch: [1][10900/36908] Elapsed 112m 41s (remain 268m 51s) Loss: 0.0373(0.0270) Grad: 39131.6445  LR: 0.000012  \n","Epoch: [1][11000/36908] Elapsed 113m 43s (remain 267m 49s) Loss: 0.0013(0.0268) Grad: 6758.3076  LR: 0.000012  \n","Epoch: [1][11100/36908] Elapsed 114m 45s (remain 266m 48s) Loss: 0.0196(0.0266) Grad: 12349.9512  LR: 0.000012  \n","Epoch: [1][11200/36908] Elapsed 115m 48s (remain 265m 46s) Loss: 0.0158(0.0264) Grad: 7597.9658  LR: 0.000012  \n","Epoch: [1][11300/36908] Elapsed 116m 50s (remain 264m 44s) Loss: 0.0001(0.0262) Grad: 12.9844  LR: 0.000012  \n","Epoch: [1][11400/36908] Elapsed 117m 52s (remain 263m 42s) Loss: 0.0026(0.0260) Grad: 9928.6445  LR: 0.000012  \n","Epoch: [1][11500/36908] Elapsed 118m 54s (remain 262m 40s) Loss: 0.0109(0.0259) Grad: 7681.2993  LR: 0.000012  \n","Epoch: [1][11600/36908] Elapsed 119m 56s (remain 261m 38s) Loss: 0.0004(0.0257) Grad: 2585.9353  LR: 0.000013  \n","Epoch: [1][11700/36908] Elapsed 120m 58s (remain 260m 36s) Loss: 0.0056(0.0255) Grad: 1650.3163  LR: 0.000013  \n","Epoch: [1][11800/36908] Elapsed 122m 0s (remain 259m 34s) Loss: 0.0046(0.0253) Grad: 3881.3821  LR: 0.000013  \n","Epoch: [1][11900/36908] Elapsed 123m 2s (remain 258m 32s) Loss: 0.0001(0.0251) Grad: 31.7299  LR: 0.000013  \n","Epoch: [1][12000/36908] Elapsed 124m 4s (remain 257m 30s) Loss: 0.0000(0.0249) Grad: 34.8963  LR: 0.000013  \n","Epoch: [1][12100/36908] Elapsed 125m 6s (remain 256m 28s) Loss: 0.0012(0.0247) Grad: 383.6290  LR: 0.000013  \n","Epoch: [1][12200/36908] Elapsed 126m 8s (remain 255m 26s) Loss: 0.0002(0.0246) Grad: 258.9939  LR: 0.000013  \n","Epoch: [1][12300/36908] Elapsed 127m 10s (remain 254m 24s) Loss: 0.0040(0.0244) Grad: 14160.4541  LR: 0.000013  \n","Epoch: [1][12400/36908] Elapsed 128m 12s (remain 253m 22s) Loss: 0.0000(0.0242) Grad: 16.5273  LR: 0.000013  \n","Epoch: [1][12500/36908] Elapsed 129m 14s (remain 252m 20s) Loss: 0.0002(0.0241) Grad: 64.0979  LR: 0.000014  \n","Epoch: [1][12600/36908] Elapsed 130m 16s (remain 251m 18s) Loss: 0.0015(0.0239) Grad: 1203.3376  LR: 0.000014  \n","Epoch: [1][12700/36908] Elapsed 131m 18s (remain 250m 15s) Loss: 0.0007(0.0238) Grad: 723.4194  LR: 0.000014  \n","Epoch: [1][12800/36908] Elapsed 132m 20s (remain 249m 13s) Loss: 0.0036(0.0236) Grad: 28427.1230  LR: 0.000014  \n","Epoch: [1][12900/36908] Elapsed 133m 22s (remain 248m 10s) Loss: 0.0077(0.0234) Grad: 41397.9336  LR: 0.000014  \n","Epoch: [1][13000/36908] Elapsed 134m 23s (remain 247m 8s) Loss: 0.0026(0.0233) Grad: 10250.8008  LR: 0.000014  \n","Epoch: [1][13100/36908] Elapsed 135m 25s (remain 246m 6s) Loss: 0.0082(0.0231) Grad: 6827.9966  LR: 0.000014  \n","Epoch: [1][13200/36908] Elapsed 136m 28s (remain 245m 4s) Loss: 0.0000(0.0230) Grad: 11.3599  LR: 0.000014  \n","Epoch: [1][13300/36908] Elapsed 137m 30s (remain 244m 2s) Loss: 0.0420(0.0228) Grad: 13651.7549  LR: 0.000014  \n","Epoch: [1][13400/36908] Elapsed 138m 32s (remain 243m 0s) Loss: 0.0005(0.0227) Grad: 147.8754  LR: 0.000015  \n","Epoch: [1][13500/36908] Elapsed 139m 34s (remain 241m 58s) Loss: 0.0002(0.0226) Grad: 717.1267  LR: 0.000015  \n","Epoch: [1][13600/36908] Elapsed 140m 36s (remain 240m 56s) Loss: 0.0002(0.0224) Grad: 64.9132  LR: 0.000015  \n","Epoch: [1][13700/36908] Elapsed 141m 38s (remain 239m 54s) Loss: 0.0003(0.0223) Grad: 163.8075  LR: 0.000015  \n","Epoch: [1][13800/36908] Elapsed 142m 40s (remain 238m 52s) Loss: 0.0017(0.0221) Grad: 11258.6768  LR: 0.000015  \n","Epoch: [1][13900/36908] Elapsed 143m 42s (remain 237m 50s) Loss: 0.0003(0.0220) Grad: 544.4069  LR: 0.000015  \n","Epoch: [1][14000/36908] Elapsed 144m 44s (remain 236m 48s) Loss: 0.0001(0.0219) Grad: 166.9951  LR: 0.000015  \n","Epoch: [1][14100/36908] Elapsed 145m 46s (remain 235m 46s) Loss: 0.0121(0.0218) Grad: 77441.4766  LR: 0.000015  \n","Epoch: [1][14200/36908] Elapsed 146m 48s (remain 234m 44s) Loss: 0.0037(0.0216) Grad: 4052.1562  LR: 0.000015  \n","Epoch: [1][14300/36908] Elapsed 147m 50s (remain 233m 42s) Loss: 0.0001(0.0215) Grad: 27.8108  LR: 0.000015  \n","Epoch: [1][14400/36908] Elapsed 148m 52s (remain 232m 40s) Loss: 0.0037(0.0214) Grad: 14689.4375  LR: 0.000016  \n","Epoch: [1][14500/36908] Elapsed 149m 54s (remain 231m 37s) Loss: 0.0034(0.0213) Grad: 8629.1416  LR: 0.000016  \n","Epoch: [1][14600/36908] Elapsed 150m 56s (remain 230m 36s) Loss: 0.0149(0.0211) Grad: 26744.5898  LR: 0.000016  \n","Epoch: [1][14700/36908] Elapsed 151m 58s (remain 229m 34s) Loss: 0.0002(0.0210) Grad: 713.5720  LR: 0.000016  \n","Epoch: [1][14800/36908] Elapsed 153m 0s (remain 228m 32s) Loss: 0.0005(0.0209) Grad: 616.5857  LR: 0.000016  \n","Epoch: [1][14900/36908] Elapsed 154m 2s (remain 227m 30s) Loss: 0.0003(0.0208) Grad: 70.0994  LR: 0.000016  \n","Epoch: [1][15000/36908] Elapsed 155m 4s (remain 226m 28s) Loss: 0.0020(0.0207) Grad: 37246.6016  LR: 0.000016  \n","Epoch: [1][15100/36908] Elapsed 156m 6s (remain 225m 26s) Loss: 0.0094(0.0205) Grad: 24536.7402  LR: 0.000016  \n","Epoch: [1][15200/36908] Elapsed 157m 8s (remain 224m 24s) Loss: 0.0033(0.0204) Grad: 4702.4644  LR: 0.000016  \n","Epoch: [1][15300/36908] Elapsed 158m 10s (remain 223m 22s) Loss: 0.0046(0.0203) Grad: 9507.5518  LR: 0.000017  \n","Epoch: [1][15400/36908] Elapsed 159m 12s (remain 222m 20s) Loss: 0.0029(0.0202) Grad: 11144.5889  LR: 0.000017  \n","Epoch: [1][15500/36908] Elapsed 160m 14s (remain 221m 18s) Loss: 0.0004(0.0201) Grad: 273.8698  LR: 0.000017  \n","Epoch: [1][15600/36908] Elapsed 161m 16s (remain 220m 16s) Loss: 0.0001(0.0200) Grad: 41.4086  LR: 0.000017  \n","Epoch: [1][15700/36908] Elapsed 162m 18s (remain 219m 13s) Loss: 0.0092(0.0199) Grad: 18510.6914  LR: 0.000017  \n","Epoch: [1][15800/36908] Elapsed 163m 20s (remain 218m 11s) Loss: 0.0068(0.0198) Grad: 41353.7812  LR: 0.000017  \n","Epoch: [1][15900/36908] Elapsed 164m 22s (remain 217m 9s) Loss: 0.0012(0.0197) Grad: 2869.2151  LR: 0.000017  \n","Epoch: [1][16000/36908] Elapsed 165m 24s (remain 216m 6s) Loss: 0.0002(0.0196) Grad: 1097.0070  LR: 0.000017  \n","Epoch: [1][16100/36908] Elapsed 166m 26s (remain 215m 5s) Loss: 0.0001(0.0195) Grad: 829.0236  LR: 0.000017  \n","Epoch: [1][16200/36908] Elapsed 167m 28s (remain 214m 3s) Loss: 0.0107(0.0194) Grad: 36490.3867  LR: 0.000018  \n","Epoch: [1][16300/36908] Elapsed 168m 30s (remain 213m 1s) Loss: 0.0002(0.0193) Grad: 337.3331  LR: 0.000018  \n","Epoch: [1][16400/36908] Elapsed 169m 32s (remain 211m 58s) Loss: 0.0002(0.0192) Grad: 158.5028  LR: 0.000018  \n","Epoch: [1][16500/36908] Elapsed 170m 34s (remain 210m 56s) Loss: 0.0012(0.0191) Grad: 12843.2461  LR: 0.000018  \n","Epoch: [1][16600/36908] Elapsed 171m 35s (remain 209m 54s) Loss: 0.0011(0.0190) Grad: 15432.8340  LR: 0.000018  \n","Epoch: [1][16700/36908] Elapsed 172m 37s (remain 208m 52s) Loss: 0.0001(0.0189) Grad: 95.1488  LR: 0.000018  \n","Epoch: [1][16800/36908] Elapsed 173m 39s (remain 207m 49s) Loss: 0.0001(0.0188) Grad: 185.9302  LR: 0.000018  \n","Epoch: [1][16900/36908] Elapsed 174m 41s (remain 206m 47s) Loss: 0.0027(0.0187) Grad: 11103.8174  LR: 0.000018  \n","Epoch: [1][17000/36908] Elapsed 175m 43s (remain 205m 46s) Loss: 0.0001(0.0186) Grad: 181.1739  LR: 0.000018  \n","Epoch: [1][17100/36908] Elapsed 176m 46s (remain 204m 44s) Loss: 0.0115(0.0185) Grad: 29279.5723  LR: 0.000019  \n","Epoch: [1][17200/36908] Elapsed 177m 48s (remain 203m 42s) Loss: 0.0001(0.0184) Grad: 137.4887  LR: 0.000019  \n","Epoch: [1][17300/36908] Elapsed 178m 49s (remain 202m 40s) Loss: 0.0032(0.0184) Grad: 14354.4639  LR: 0.000019  \n","Epoch: [1][17400/36908] Elapsed 179m 51s (remain 201m 38s) Loss: 0.0000(0.0183) Grad: 31.9541  LR: 0.000019  \n","Epoch: [1][17500/36908] Elapsed 180m 53s (remain 200m 36s) Loss: 0.0002(0.0182) Grad: 441.1705  LR: 0.000019  \n","Epoch: [1][17600/36908] Elapsed 181m 56s (remain 199m 34s) Loss: 0.0001(0.0181) Grad: 31.1720  LR: 0.000019  \n","Epoch: [1][17700/36908] Elapsed 182m 58s (remain 198m 32s) Loss: 0.0060(0.0180) Grad: 31782.6445  LR: 0.000019  \n","Epoch: [1][17800/36908] Elapsed 184m 0s (remain 197m 30s) Loss: 0.0169(0.0179) Grad: 165668.8438  LR: 0.000019  \n","Epoch: [1][17900/36908] Elapsed 185m 2s (remain 196m 28s) Loss: 0.0007(0.0179) Grad: 3270.7683  LR: 0.000019  \n","Epoch: [1][18000/36908] Elapsed 186m 4s (remain 195m 26s) Loss: 0.0004(0.0178) Grad: 4483.6094  LR: 0.000020  \n","Epoch: [1][18100/36908] Elapsed 187m 6s (remain 194m 23s) Loss: 0.0025(0.0177) Grad: 13366.1357  LR: 0.000020  \n","Epoch: [1][18200/36908] Elapsed 188m 7s (remain 193m 21s) Loss: 0.0001(0.0176) Grad: 43.7099  LR: 0.000020  \n","Epoch: [1][18300/36908] Elapsed 189m 9s (remain 192m 19s) Loss: 0.0136(0.0175) Grad: 335965.5625  LR: 0.000020  \n","Epoch: [1][18400/36908] Elapsed 190m 11s (remain 191m 17s) Loss: 0.0024(0.0175) Grad: 5127.1660  LR: 0.000020  \n","Epoch: [1][18500/36908] Elapsed 191m 13s (remain 190m 15s) Loss: 0.0036(0.0174) Grad: 70316.3828  LR: 0.000020  \n","Epoch: [1][18600/36908] Elapsed 192m 16s (remain 189m 13s) Loss: 0.0013(0.0173) Grad: 8591.4111  LR: 0.000020  \n","Epoch: [1][18700/36908] Elapsed 193m 18s (remain 188m 12s) Loss: 0.0051(0.0172) Grad: 20254.4766  LR: 0.000020  \n","Epoch: [1][18800/36908] Elapsed 194m 20s (remain 187m 10s) Loss: 0.0007(0.0172) Grad: 829.6351  LR: 0.000020  \n","Epoch: [1][18900/36908] Elapsed 195m 22s (remain 186m 8s) Loss: 0.0021(0.0171) Grad: 13267.8643  LR: 0.000020  \n","Epoch: [1][19000/36908] Elapsed 196m 24s (remain 185m 6s) Loss: 0.0001(0.0170) Grad: 121.4614  LR: 0.000020  \n","Epoch: [1][19100/36908] Elapsed 197m 26s (remain 184m 4s) Loss: 0.0010(0.0169) Grad: 4270.2793  LR: 0.000020  \n","Epoch: [1][19200/36908] Elapsed 198m 28s (remain 183m 2s) Loss: 0.0003(0.0169) Grad: 974.0635  LR: 0.000020  \n","Epoch: [1][19300/36908] Elapsed 199m 30s (remain 182m 0s) Loss: 0.0000(0.0168) Grad: 32.4385  LR: 0.000020  \n","Epoch: [1][19400/36908] Elapsed 200m 32s (remain 180m 58s) Loss: 0.0010(0.0167) Grad: 2430.6914  LR: 0.000020  \n","Epoch: [1][19500/36908] Elapsed 201m 35s (remain 179m 56s) Loss: 0.0169(0.0167) Grad: 59054.6445  LR: 0.000020  \n","Epoch: [1][19600/36908] Elapsed 202m 37s (remain 178m 54s) Loss: 0.0180(0.0166) Grad: 100482.1797  LR: 0.000020  \n","Epoch: [1][19700/36908] Elapsed 203m 39s (remain 177m 52s) Loss: 0.0002(0.0165) Grad: 275.6051  LR: 0.000020  \n","Epoch: [1][19800/36908] Elapsed 204m 41s (remain 176m 50s) Loss: 0.0053(0.0165) Grad: 22226.2402  LR: 0.000020  \n","Epoch: [1][19900/36908] Elapsed 205m 43s (remain 175m 48s) Loss: 0.0022(0.0164) Grad: 8644.1270  LR: 0.000020  \n","Epoch: [1][20000/36908] Elapsed 206m 45s (remain 174m 46s) Loss: 0.0013(0.0163) Grad: 3585.7798  LR: 0.000020  \n","Epoch: [1][20100/36908] Elapsed 207m 47s (remain 173m 44s) Loss: 0.0002(0.0163) Grad: 958.4651  LR: 0.000020  \n","Epoch: [1][20200/36908] Elapsed 208m 49s (remain 172m 42s) Loss: 0.0001(0.0162) Grad: 30.8481  LR: 0.000020  \n","Epoch: [1][20300/36908] Elapsed 209m 51s (remain 171m 40s) Loss: 0.0001(0.0162) Grad: 152.3677  LR: 0.000020  \n","Epoch: [1][20400/36908] Elapsed 210m 53s (remain 170m 38s) Loss: 0.0225(0.0161) Grad: 91988.3594  LR: 0.000020  \n","Epoch: [1][20500/36908] Elapsed 211m 56s (remain 169m 36s) Loss: 0.0001(0.0160) Grad: 135.6000  LR: 0.000020  \n","Epoch: [1][20600/36908] Elapsed 212m 58s (remain 168m 34s) Loss: 0.0001(0.0160) Grad: 97.6906  LR: 0.000020  \n","Epoch: [1][20700/36908] Elapsed 213m 59s (remain 167m 32s) Loss: 0.0006(0.0159) Grad: 12921.0205  LR: 0.000020  \n","Epoch: [1][20800/36908] Elapsed 215m 1s (remain 166m 30s) Loss: 0.0001(0.0158) Grad: 33.2077  LR: 0.000020  \n","Epoch: [1][20900/36908] Elapsed 216m 3s (remain 165m 28s) Loss: 0.0084(0.0158) Grad: 79139.6250  LR: 0.000020  \n","Epoch: [1][21000/36908] Elapsed 217m 5s (remain 164m 25s) Loss: 0.0001(0.0157) Grad: 110.0631  LR: 0.000020  \n","Epoch: [1][21100/36908] Elapsed 218m 7s (remain 163m 23s) Loss: 0.0001(0.0157) Grad: 75.0034  LR: 0.000020  \n","Epoch: [1][21200/36908] Elapsed 219m 8s (remain 162m 21s) Loss: 0.0010(0.0156) Grad: 5333.8052  LR: 0.000020  \n","Epoch: [1][21300/36908] Elapsed 220m 10s (remain 161m 19s) Loss: 0.0007(0.0155) Grad: 14562.7373  LR: 0.000020  \n","Epoch: [1][21400/36908] Elapsed 221m 12s (remain 160m 17s) Loss: 0.0001(0.0155) Grad: 37.0283  LR: 0.000020  \n","Epoch: [1][21500/36908] Elapsed 222m 14s (remain 159m 15s) Loss: 0.0024(0.0154) Grad: 8764.3867  LR: 0.000020  \n","Epoch: [1][21600/36908] Elapsed 223m 16s (remain 158m 13s) Loss: 0.0010(0.0154) Grad: 5297.2715  LR: 0.000020  \n","Epoch: [1][21700/36908] Elapsed 224m 18s (remain 157m 11s) Loss: 0.0035(0.0153) Grad: 18572.3926  LR: 0.000020  \n","Epoch: [1][21800/36908] Elapsed 225m 20s (remain 156m 9s) Loss: 0.0005(0.0153) Grad: 1626.8567  LR: 0.000020  \n","Epoch: [1][21900/36908] Elapsed 226m 22s (remain 155m 7s) Loss: 0.0202(0.0152) Grad: 190990.6875  LR: 0.000020  \n","Epoch: [1][22000/36908] Elapsed 227m 24s (remain 154m 5s) Loss: 0.0000(0.0151) Grad: 28.4236  LR: 0.000020  \n","Epoch: [1][22100/36908] Elapsed 228m 26s (remain 153m 2s) Loss: 0.0006(0.0151) Grad: 41086.4258  LR: 0.000020  \n","Epoch: [1][22200/36908] Elapsed 229m 28s (remain 152m 0s) Loss: 0.0030(0.0150) Grad: 10111.5635  LR: 0.000020  \n","Epoch: [1][22300/36908] Elapsed 230m 30s (remain 150m 58s) Loss: 0.0016(0.0150) Grad: 14631.5703  LR: 0.000020  \n","Epoch: [1][22400/36908] Elapsed 231m 32s (remain 149m 56s) Loss: 0.0015(0.0149) Grad: 28710.8223  LR: 0.000020  \n","Epoch: [1][22500/36908] Elapsed 232m 34s (remain 148m 54s) Loss: 0.0035(0.0149) Grad: 144158.3906  LR: 0.000020  \n","Epoch: [1][22600/36908] Elapsed 233m 35s (remain 147m 52s) Loss: 0.0004(0.0148) Grad: 35080.6562  LR: 0.000020  \n","Epoch: [1][22700/36908] Elapsed 234m 37s (remain 146m 50s) Loss: 0.0059(0.0148) Grad: 49508.1055  LR: 0.000019  \n","Epoch: [1][22800/36908] Elapsed 235m 39s (remain 145m 48s) Loss: 0.0001(0.0147) Grad: 159.4834  LR: 0.000019  \n","Epoch: [1][22900/36908] Elapsed 236m 41s (remain 144m 46s) Loss: 0.0182(0.0147) Grad: 84696.2422  LR: 0.000019  \n","Epoch: [1][23000/36908] Elapsed 237m 43s (remain 143m 44s) Loss: 0.0002(0.0146) Grad: 237.4194  LR: 0.000019  \n","Epoch: [1][23100/36908] Elapsed 238m 46s (remain 142m 42s) Loss: 0.0002(0.0146) Grad: 9508.1465  LR: 0.000019  \n","Epoch: [1][23200/36908] Elapsed 239m 48s (remain 141m 40s) Loss: 0.0001(0.0145) Grad: 281.0410  LR: 0.000019  \n","Epoch: [1][23300/36908] Elapsed 240m 50s (remain 140m 38s) Loss: 0.0002(0.0145) Grad: 584.4692  LR: 0.000019  \n","Epoch: [1][23400/36908] Elapsed 241m 52s (remain 139m 36s) Loss: 0.0310(0.0144) Grad: 116526.8359  LR: 0.000019  \n","Epoch: [1][23500/36908] Elapsed 242m 54s (remain 138m 34s) Loss: 0.0107(0.0144) Grad: 107588.6719  LR: 0.000019  \n","Epoch: [1][23600/36908] Elapsed 243m 56s (remain 137m 32s) Loss: 0.0000(0.0143) Grad: 77.5298  LR: 0.000019  \n","Epoch: [1][23700/36908] Elapsed 244m 58s (remain 136m 30s) Loss: 0.0000(0.0143) Grad: 241.4594  LR: 0.000019  \n","Epoch: [1][23800/36908] Elapsed 246m 0s (remain 135m 28s) Loss: 0.0021(0.0142) Grad: 16263.1318  LR: 0.000019  \n","Epoch: [1][23900/36908] Elapsed 247m 2s (remain 134m 26s) Loss: 0.0000(0.0142) Grad: 42.0904  LR: 0.000019  \n","Epoch: [1][24000/36908] Elapsed 248m 4s (remain 133m 24s) Loss: 0.0000(0.0141) Grad: 21.6395  LR: 0.000019  \n","Epoch: [1][24100/36908] Elapsed 249m 5s (remain 132m 22s) Loss: 0.0001(0.0141) Grad: 110.2132  LR: 0.000019  \n","Epoch: [1][24200/36908] Elapsed 250m 7s (remain 131m 20s) Loss: 0.0129(0.0140) Grad: 103320.7031  LR: 0.000019  \n","Epoch: [1][24300/36908] Elapsed 251m 9s (remain 130m 18s) Loss: 0.0123(0.0140) Grad: 147045.8906  LR: 0.000019  \n","Epoch: [1][24400/36908] Elapsed 252m 11s (remain 129m 15s) Loss: 0.0004(0.0139) Grad: 2600.7808  LR: 0.000019  \n","Epoch: [1][24500/36908] Elapsed 253m 13s (remain 128m 13s) Loss: 0.0005(0.0139) Grad: 15008.1123  LR: 0.000019  \n","Epoch: [1][24600/36908] Elapsed 254m 16s (remain 127m 12s) Loss: 0.0004(0.0138) Grad: 1087.5233  LR: 0.000019  \n","Epoch: [1][24700/36908] Elapsed 255m 18s (remain 126m 10s) Loss: 0.0000(0.0138) Grad: 8.3846  LR: 0.000019  \n","Epoch: [1][24800/36908] Elapsed 256m 20s (remain 125m 7s) Loss: 0.0006(0.0137) Grad: 10316.0186  LR: 0.000019  \n","Epoch: [1][24900/36908] Elapsed 257m 21s (remain 124m 5s) Loss: 0.0042(0.0137) Grad: 214220.0000  LR: 0.000019  \n","Epoch: [1][25000/36908] Elapsed 258m 23s (remain 123m 3s) Loss: 0.0004(0.0137) Grad: 2990.5691  LR: 0.000019  \n","Epoch: [1][25100/36908] Elapsed 259m 25s (remain 122m 1s) Loss: 0.0278(0.0136) Grad: 217872.2969  LR: 0.000019  \n","Epoch: [1][25200/36908] Elapsed 260m 27s (remain 120m 59s) Loss: 0.0007(0.0136) Grad: 4448.6353  LR: 0.000019  \n","Epoch: [1][25300/36908] Elapsed 261m 28s (remain 119m 57s) Loss: 0.0003(0.0135) Grad: 10299.0518  LR: 0.000019  \n","Epoch: [1][25400/36908] Elapsed 262m 30s (remain 118m 55s) Loss: 0.0126(0.0135) Grad: 199215.0938  LR: 0.000019  \n","Epoch: [1][25500/36908] Elapsed 263m 32s (remain 117m 53s) Loss: 0.0082(0.0134) Grad: 391080.0625  LR: 0.000019  \n","Epoch: [1][25600/36908] Elapsed 264m 34s (remain 116m 51s) Loss: 0.0011(0.0134) Grad: 30920.5410  LR: 0.000019  \n","Epoch: [1][25700/36908] Elapsed 265m 36s (remain 115m 49s) Loss: 0.0119(0.0134) Grad: 345500.5938  LR: 0.000019  \n","Epoch: [1][25800/36908] Elapsed 266m 38s (remain 114m 47s) Loss: 0.0002(0.0133) Grad: 492.4799  LR: 0.000019  \n","Epoch: [1][25900/36908] Elapsed 267m 40s (remain 113m 44s) Loss: 0.0102(0.0133) Grad: 256769.2188  LR: 0.000019  \n","Epoch: [1][26000/36908] Elapsed 268m 41s (remain 112m 42s) Loss: 0.0039(0.0132) Grad: 55083.5742  LR: 0.000019  \n","Epoch: [1][26100/36908] Elapsed 269m 43s (remain 111m 40s) Loss: 0.0066(0.0132) Grad: 46263.2969  LR: 0.000019  \n","Epoch: [1][26200/36908] Elapsed 270m 45s (remain 110m 38s) Loss: 0.0008(0.0131) Grad: 163583.8125  LR: 0.000019  \n","Epoch: [1][26300/36908] Elapsed 271m 46s (remain 109m 36s) Loss: 0.0028(0.0131) Grad: 86598.4688  LR: 0.000019  \n","Epoch: [1][26400/36908] Elapsed 272m 48s (remain 108m 34s) Loss: 0.0017(0.0131) Grad: 14266.8428  LR: 0.000019  \n","Epoch: [1][26500/36908] Elapsed 273m 50s (remain 107m 32s) Loss: 0.0024(0.0130) Grad: 38653.7930  LR: 0.000019  \n","Epoch: [1][26600/36908] Elapsed 274m 52s (remain 106m 30s) Loss: 0.0004(0.0130) Grad: 4111.6543  LR: 0.000019  \n","Epoch: [1][26700/36908] Elapsed 275m 53s (remain 105m 27s) Loss: 0.0016(0.0129) Grad: 47000.1953  LR: 0.000019  \n","Epoch: [1][26800/36908] Elapsed 276m 55s (remain 104m 25s) Loss: 0.0046(0.0129) Grad: 362891.7500  LR: 0.000019  \n","Epoch: [1][26900/36908] Elapsed 277m 57s (remain 103m 23s) Loss: 0.0089(0.0129) Grad: 255910.6406  LR: 0.000019  \n","Epoch: [1][27000/36908] Elapsed 278m 58s (remain 102m 21s) Loss: 0.0071(0.0128) Grad: 562057.4375  LR: 0.000019  \n","Epoch: [1][27100/36908] Elapsed 280m 0s (remain 101m 19s) Loss: 0.0000(0.0128) Grad: 124.5624  LR: 0.000019  \n","Epoch: [1][27200/36908] Elapsed 281m 2s (remain 100m 17s) Loss: 0.0001(0.0128) Grad: 99.0419  LR: 0.000019  \n","Epoch: [1][27300/36908] Elapsed 282m 4s (remain 99m 15s) Loss: 0.0121(0.0127) Grad: 174096.1875  LR: 0.000019  \n","Epoch: [1][27400/36908] Elapsed 283m 5s (remain 98m 13s) Loss: 0.0001(0.0127) Grad: 126.0675  LR: 0.000019  \n","Epoch: [1][27500/36908] Elapsed 284m 7s (remain 97m 11s) Loss: 0.0001(0.0127) Grad: 50.1536  LR: 0.000019  \n","Epoch: [1][27600/36908] Elapsed 285m 9s (remain 96m 9s) Loss: 0.0012(0.0126) Grad: 5153.2290  LR: 0.000019  \n","Epoch: [1][27700/36908] Elapsed 286m 11s (remain 95m 7s) Loss: 0.0003(0.0126) Grad: 517.5522  LR: 0.000019  \n","Epoch: [1][27800/36908] Elapsed 287m 13s (remain 94m 5s) Loss: 0.0001(0.0125) Grad: 30.4026  LR: 0.000019  \n","Epoch: [1][27900/36908] Elapsed 288m 14s (remain 93m 3s) Loss: 0.0000(0.0125) Grad: 30.3072  LR: 0.000019  \n","Epoch: [1][28000/36908] Elapsed 289m 16s (remain 92m 1s) Loss: 0.0049(0.0125) Grad: 55440.5391  LR: 0.000019  \n","Epoch: [1][28100/36908] Elapsed 290m 18s (remain 90m 59s) Loss: 0.0001(0.0124) Grad: 239.9672  LR: 0.000019  \n","Epoch: [1][28200/36908] Elapsed 291m 20s (remain 89m 57s) Loss: 0.0008(0.0124) Grad: 15587.8311  LR: 0.000019  \n","Epoch: [1][28300/36908] Elapsed 292m 22s (remain 88m 55s) Loss: 0.0000(0.0124) Grad: 6.6761  LR: 0.000019  \n","Epoch: [1][28400/36908] Elapsed 293m 24s (remain 87m 53s) Loss: 0.0001(0.0123) Grad: 187.3026  LR: 0.000019  \n","Epoch: [1][28500/36908] Elapsed 294m 26s (remain 86m 51s) Loss: 0.0003(0.0123) Grad: 1233.9906  LR: 0.000019  \n","Epoch: [1][28600/36908] Elapsed 295m 28s (remain 85m 49s) Loss: 0.0039(0.0123) Grad: 30551.9629  LR: 0.000019  \n","Epoch: [1][28700/36908] Elapsed 296m 30s (remain 84m 47s) Loss: 0.0081(0.0122) Grad: 52836.4961  LR: 0.000019  \n","Epoch: [1][28800/36908] Elapsed 297m 32s (remain 83m 45s) Loss: 0.0026(0.0122) Grad: 6882.7891  LR: 0.000019  \n","Epoch: [1][28900/36908] Elapsed 298m 34s (remain 82m 43s) Loss: 0.0015(0.0122) Grad: 2844.2507  LR: 0.000019  \n","Epoch: [1][29000/36908] Elapsed 299m 35s (remain 81m 41s) Loss: 0.0000(0.0121) Grad: 77.5941  LR: 0.000019  \n","Epoch: [1][29100/36908] Elapsed 300m 37s (remain 80m 39s) Loss: 0.0001(0.0121) Grad: 41.9221  LR: 0.000019  \n","Epoch: [1][29200/36908] Elapsed 301m 39s (remain 79m 37s) Loss: 0.0021(0.0121) Grad: 2221.1570  LR: 0.000019  \n","Epoch: [1][29300/36908] Elapsed 302m 41s (remain 78m 35s) Loss: 0.0002(0.0120) Grad: 931.6075  LR: 0.000019  \n","Epoch: [1][29400/36908] Elapsed 303m 43s (remain 77m 33s) Loss: 0.0001(0.0120) Grad: 276.4597  LR: 0.000019  \n","Epoch: [1][29500/36908] Elapsed 304m 45s (remain 76m 31s) Loss: 0.0011(0.0120) Grad: 2415.9082  LR: 0.000019  \n","Epoch: [1][29600/36908] Elapsed 305m 47s (remain 75m 29s) Loss: 0.0004(0.0119) Grad: 225.7886  LR: 0.000019  \n","Epoch: [1][29700/36908] Elapsed 306m 48s (remain 74m 26s) Loss: 0.0026(0.0119) Grad: 19267.9121  LR: 0.000019  \n","Epoch: [1][29800/36908] Elapsed 307m 50s (remain 73m 24s) Loss: 0.0011(0.0119) Grad: 16456.3945  LR: 0.000019  \n","Epoch: [1][29900/36908] Elapsed 308m 52s (remain 72m 22s) Loss: 0.0006(0.0119) Grad: 7701.6396  LR: 0.000019  \n","Epoch: [1][30000/36908] Elapsed 309m 53s (remain 71m 20s) Loss: 0.0003(0.0118) Grad: 6276.2539  LR: 0.000019  \n","Epoch: [1][30100/36908] Elapsed 310m 55s (remain 70m 18s) Loss: 0.0004(0.0118) Grad: 7791.3999  LR: 0.000019  \n","Epoch: [1][30200/36908] Elapsed 311m 57s (remain 69m 16s) Loss: 0.0003(0.0118) Grad: 2574.2808  LR: 0.000019  \n","Epoch: [1][30300/36908] Elapsed 312m 58s (remain 68m 14s) Loss: 0.0075(0.0117) Grad: 111910.5547  LR: 0.000019  \n","Epoch: [1][30400/36908] Elapsed 314m 0s (remain 67m 12s) Loss: 0.0004(0.0117) Grad: 2987.7051  LR: 0.000019  \n","Epoch: [1][30500/36908] Elapsed 315m 3s (remain 66m 10s) Loss: 0.0008(0.0117) Grad: 703.5192  LR: 0.000019  \n","Epoch: [1][30600/36908] Elapsed 316m 5s (remain 65m 8s) Loss: 0.0001(0.0116) Grad: 556.6882  LR: 0.000019  \n","Epoch: [1][30700/36908] Elapsed 317m 7s (remain 64m 6s) Loss: 0.0009(0.0116) Grad: 62926.7070  LR: 0.000019  \n","Epoch: [1][30800/36908] Elapsed 318m 8s (remain 63m 4s) Loss: 0.0004(0.0116) Grad: 10311.1582  LR: 0.000019  \n","Epoch: [1][30900/36908] Elapsed 319m 10s (remain 62m 2s) Loss: 0.0013(0.0116) Grad: 2580.1245  LR: 0.000019  \n","Epoch: [1][31000/36908] Elapsed 320m 12s (remain 61m 0s) Loss: 0.0018(0.0115) Grad: 8887.3760  LR: 0.000018  \n","Epoch: [1][31100/36908] Elapsed 321m 14s (remain 59m 58s) Loss: 0.0003(0.0115) Grad: 514.5156  LR: 0.000018  \n","Epoch: [1][31200/36908] Elapsed 322m 16s (remain 58m 56s) Loss: 0.0001(0.0115) Grad: 90.3669  LR: 0.000018  \n","Epoch: [1][31300/36908] Elapsed 323m 18s (remain 57m 54s) Loss: 0.0020(0.0114) Grad: 23707.4062  LR: 0.000018  \n","Epoch: [1][31400/36908] Elapsed 324m 20s (remain 56m 52s) Loss: 0.0016(0.0114) Grad: 139031.2188  LR: 0.000018  \n","Epoch: [1][31500/36908] Elapsed 325m 22s (remain 55m 50s) Loss: 0.0052(0.0114) Grad: 94258.2734  LR: 0.000018  \n","Epoch: [1][31600/36908] Elapsed 326m 24s (remain 54m 49s) Loss: 0.0004(0.0114) Grad: 50570.1016  LR: 0.000018  \n","Epoch: [1][31700/36908] Elapsed 327m 26s (remain 53m 47s) Loss: 0.0034(0.0113) Grad: 22117.2031  LR: 0.000018  \n","Epoch: [1][31800/36908] Elapsed 328m 28s (remain 52m 45s) Loss: 0.0000(0.0113) Grad: 43.5587  LR: 0.000018  \n","Epoch: [1][31900/36908] Elapsed 329m 30s (remain 51m 43s) Loss: 0.0027(0.0113) Grad: 37726.4531  LR: 0.000018  \n","Epoch: [1][32000/36908] Elapsed 330m 32s (remain 50m 41s) Loss: 0.0231(0.0112) Grad: 330062.5625  LR: 0.000018  \n","Epoch: [1][32100/36908] Elapsed 331m 34s (remain 49m 39s) Loss: 0.0023(0.0112) Grad: 22483.8594  LR: 0.000018  \n","Epoch: [1][32200/36908] Elapsed 332m 36s (remain 48m 37s) Loss: 0.0156(0.0112) Grad: 1079603.6250  LR: 0.000018  \n","Epoch: [1][32300/36908] Elapsed 333m 38s (remain 47m 35s) Loss: 0.0001(0.0112) Grad: 259.7159  LR: 0.000018  \n","Epoch: [1][32400/36908] Elapsed 334m 40s (remain 46m 33s) Loss: 0.0005(0.0111) Grad: 2803.1406  LR: 0.000018  \n","Epoch: [1][32500/36908] Elapsed 335m 42s (remain 45m 31s) Loss: 0.0003(0.0111) Grad: 23683.6758  LR: 0.000018  \n","Epoch: [1][32600/36908] Elapsed 336m 44s (remain 44m 29s) Loss: 0.0033(0.0111) Grad: 54352.5781  LR: 0.000018  \n","Epoch: [1][32700/36908] Elapsed 337m 45s (remain 43m 27s) Loss: 0.0004(0.0111) Grad: 2114.0583  LR: 0.000018  \n","Epoch: [1][32800/36908] Elapsed 338m 47s (remain 42m 25s) Loss: 0.0007(0.0110) Grad: 4410.6050  LR: 0.000018  \n","Epoch: [1][32900/36908] Elapsed 339m 49s (remain 41m 23s) Loss: 0.0015(0.0110) Grad: 3505.3040  LR: 0.000018  \n","Epoch: [1][33000/36908] Elapsed 340m 51s (remain 40m 21s) Loss: 0.0003(0.0110) Grad: 1140.4221  LR: 0.000018  \n","Epoch: [1][33100/36908] Elapsed 341m 53s (remain 39m 19s) Loss: 0.0012(0.0109) Grad: 10227.4150  LR: 0.000018  \n","Epoch: [1][33200/36908] Elapsed 342m 55s (remain 38m 17s) Loss: 0.0002(0.0109) Grad: 18.8858  LR: 0.000018  \n","Epoch: [1][33300/36908] Elapsed 343m 57s (remain 37m 15s) Loss: 0.0000(0.0109) Grad: 14.9754  LR: 0.000018  \n","Epoch: [1][33400/36908] Elapsed 344m 59s (remain 36m 13s) Loss: 0.0052(0.0109) Grad: 33729.2656  LR: 0.000018  \n","Epoch: [1][33500/36908] Elapsed 346m 1s (remain 35m 11s) Loss: 0.0001(0.0108) Grad: 42.4596  LR: 0.000018  \n","Epoch: [1][33600/36908] Elapsed 347m 3s (remain 34m 9s) Loss: 0.0201(0.0108) Grad: 99681.0703  LR: 0.000018  \n","Epoch: [1][33700/36908] Elapsed 348m 5s (remain 33m 7s) Loss: 0.0020(0.0108) Grad: 61915.0703  LR: 0.000018  \n","Epoch: [1][33800/36908] Elapsed 349m 7s (remain 32m 5s) Loss: 0.0007(0.0108) Grad: 3803.6980  LR: 0.000018  \n","Epoch: [1][33900/36908] Elapsed 350m 9s (remain 31m 3s) Loss: 0.0101(0.0107) Grad: 76232.0859  LR: 0.000018  \n","Epoch: [1][34000/36908] Elapsed 351m 11s (remain 30m 1s) Loss: 0.0001(0.0107) Grad: 45.2669  LR: 0.000018  \n","Epoch: [1][34100/36908] Elapsed 352m 13s (remain 28m 59s) Loss: 0.0003(0.0107) Grad: 392.1394  LR: 0.000018  \n","Epoch: [1][34200/36908] Elapsed 353m 15s (remain 27m 57s) Loss: 0.0005(0.0107) Grad: 2114.1006  LR: 0.000018  \n","Epoch: [1][34300/36908] Elapsed 354m 17s (remain 26m 55s) Loss: 0.0001(0.0107) Grad: 93.4039  LR: 0.000018  \n","Epoch: [1][34400/36908] Elapsed 355m 19s (remain 25m 53s) Loss: 0.0001(0.0106) Grad: 116.8253  LR: 0.000018  \n","Epoch: [1][34500/36908] Elapsed 356m 21s (remain 24m 51s) Loss: 0.0012(0.0106) Grad: 15464.0654  LR: 0.000018  \n","Epoch: [1][34600/36908] Elapsed 357m 22s (remain 23m 49s) Loss: 0.0001(0.0106) Grad: 1592.0629  LR: 0.000018  \n","Epoch: [1][34700/36908] Elapsed 358m 24s (remain 22m 47s) Loss: 0.0001(0.0106) Grad: 404.5669  LR: 0.000018  \n","Epoch: [1][34800/36908] Elapsed 359m 26s (remain 21m 45s) Loss: 0.0042(0.0105) Grad: 64426.8477  LR: 0.000018  \n","Epoch: [1][34900/36908] Elapsed 360m 28s (remain 20m 43s) Loss: 0.0034(0.0105) Grad: 7632.3271  LR: 0.000018  \n","Epoch: [1][35000/36908] Elapsed 361m 30s (remain 19m 41s) Loss: 0.0001(0.0105) Grad: 491.0213  LR: 0.000018  \n","Epoch: [1][35100/36908] Elapsed 362m 32s (remain 18m 39s) Loss: 0.0027(0.0105) Grad: 58377.1992  LR: 0.000018  \n","Epoch: [1][35200/36908] Elapsed 363m 34s (remain 17m 37s) Loss: 0.0063(0.0104) Grad: 62996.7930  LR: 0.000018  \n","Epoch: [1][35300/36908] Elapsed 364m 35s (remain 16m 35s) Loss: 0.0005(0.0104) Grad: 2430.1782  LR: 0.000018  \n","Epoch: [1][35400/36908] Elapsed 365m 37s (remain 15m 33s) Loss: 0.0007(0.0104) Grad: 2798.8765  LR: 0.000018  \n","Epoch: [1][35500/36908] Elapsed 366m 39s (remain 14m 31s) Loss: 0.0005(0.0104) Grad: 947.1331  LR: 0.000018  \n","Epoch: [1][35600/36908] Elapsed 367m 41s (remain 13m 29s) Loss: 0.0008(0.0104) Grad: 1244.4785  LR: 0.000018  \n","Epoch: [1][35700/36908] Elapsed 368m 42s (remain 12m 27s) Loss: 0.0031(0.0103) Grad: 5710.4077  LR: 0.000018  \n","Epoch: [1][35800/36908] Elapsed 369m 44s (remain 11m 25s) Loss: 0.0044(0.0103) Grad: 24730.5273  LR: 0.000018  \n","Epoch: [1][35900/36908] Elapsed 370m 46s (remain 10m 23s) Loss: 0.0003(0.0103) Grad: 1287.7887  LR: 0.000018  \n","Epoch: [1][36000/36908] Elapsed 371m 48s (remain 9m 22s) Loss: 0.0001(0.0103) Grad: 1067.5978  LR: 0.000018  \n","Epoch: [1][36100/36908] Elapsed 372m 49s (remain 8m 20s) Loss: 0.0084(0.0103) Grad: 244010.6406  LR: 0.000018  \n","Epoch: [1][36200/36908] Elapsed 373m 51s (remain 7m 18s) Loss: 0.0015(0.0102) Grad: 14569.9980  LR: 0.000018  \n","Epoch: [1][36300/36908] Elapsed 374m 53s (remain 6m 16s) Loss: 0.0093(0.0102) Grad: 107106.8984  LR: 0.000018  \n","Epoch: [1][36400/36908] Elapsed 375m 55s (remain 5m 14s) Loss: 0.0001(0.0102) Grad: 435.7976  LR: 0.000018  \n","Epoch: [1][36500/36908] Elapsed 376m 56s (remain 4m 12s) Loss: 0.0067(0.0102) Grad: 41696.5977  LR: 0.000018  \n","Epoch: [1][36600/36908] Elapsed 377m 58s (remain 3m 10s) Loss: 0.0001(0.0101) Grad: 1283.8232  LR: 0.000018  \n","Epoch: [1][36700/36908] Elapsed 379m 0s (remain 2m 8s) Loss: 0.0015(0.0101) Grad: 182274.2656  LR: 0.000018  \n","Epoch: [1][36800/36908] Elapsed 380m 1s (remain 1m 6s) Loss: 0.0000(0.0101) Grad: 18.6051  LR: 0.000018  \n","Epoch: [1][36900/36908] Elapsed 381m 3s (remain 0m 4s) Loss: 0.0001(0.0101) Grad: 69.3187  LR: 0.000018  \n","Epoch: [1][36907/36908] Elapsed 381m 8s (remain 0m 0s) Loss: 0.0001(0.0101) Grad: 477.4245  LR: 0.000018  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 11m 13s) Loss: 0.0000(0.0000) \n","EVAL: [100/1192] Elapsed 0m 22s (remain 4m 6s) Loss: 0.0479(0.0086) \n","EVAL: [200/1192] Elapsed 0m 45s (remain 3m 41s) Loss: 0.0073(0.0081) \n","EVAL: [300/1192] Elapsed 1m 7s (remain 3m 18s) Loss: 0.0063(0.0078) \n","EVAL: [400/1192] Elapsed 1m 29s (remain 2m 56s) Loss: 0.0000(0.0084) \n","EVAL: [500/1192] Elapsed 1m 51s (remain 2m 33s) Loss: 0.0000(0.0078) \n","EVAL: [600/1192] Elapsed 2m 13s (remain 2m 11s) Loss: 0.0121(0.0079) \n","EVAL: [700/1192] Elapsed 2m 35s (remain 1m 49s) Loss: 0.0068(0.0087) \n","EVAL: [800/1192] Elapsed 2m 58s (remain 1m 26s) Loss: 0.0000(0.0087) \n","EVAL: [900/1192] Elapsed 3m 20s (remain 1m 4s) Loss: 0.0169(0.0088) \n","EVAL: [1000/1192] Elapsed 3m 42s (remain 0m 42s) Loss: 0.0028(0.0085) \n","EVAL: [1100/1192] Elapsed 4m 4s (remain 0m 20s) Loss: 0.0301(0.0082) \n","EVAL: [1191/1192] Elapsed 4m 24s (remain 0m 0s) Loss: 0.0000(0.0079) \n","Epoch 1 - avg_train_loss: 0.0101  avg_val_loss: 0.0079  time: 23135s\n","Epoch 1 - Score: 0.8910\n","Epoch 1 - Save Best Score: 0.8910 Model\n","========== fold: 3 training ==========\n","(612602, 6) (612602, 950)\n","(100000, 7)\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n","Epoch: [1][0/36908] Elapsed 0m 0s (remain 567m 22s) Loss: 0.3374(0.3374) Grad: 95890.4844  LR: 0.000000  \n","Epoch: [1][100/36908] Elapsed 1m 2s (remain 381m 23s) Loss: 0.3314(0.3355) Grad: 106039.0781  LR: 0.000000  \n","Epoch: [1][200/36908] Elapsed 2m 4s (remain 379m 16s) Loss: 0.3134(0.3310) Grad: 49948.3008  LR: 0.000000  \n","Epoch: [1][300/36908] Elapsed 3m 6s (remain 377m 48s) Loss: 0.2824(0.3219) Grad: 23354.8750  LR: 0.000000  \n","Epoch: [1][400/36908] Elapsed 4m 8s (remain 376m 29s) Loss: 0.2351(0.3076) Grad: 10567.0566  LR: 0.000000  \n","Epoch: [1][500/36908] Elapsed 5m 9s (remain 375m 25s) Loss: 0.1826(0.2887) Grad: 9295.8301  LR: 0.000001  \n","Epoch: [1][600/36908] Elapsed 6m 11s (remain 374m 22s) Loss: 0.1275(0.2680) Grad: 13196.3926  LR: 0.000001  \n","Epoch: [1][700/36908] Elapsed 7m 13s (remain 373m 24s) Loss: 0.0964(0.2460) Grad: 5561.6230  LR: 0.000001  \n","Epoch: [1][800/36908] Elapsed 8m 15s (remain 372m 27s) Loss: 0.0599(0.2247) Grad: 1416.3992  LR: 0.000001  \n","Epoch: [1][900/36908] Elapsed 9m 17s (remain 371m 33s) Loss: 0.0242(0.2050) Grad: 494.3935  LR: 0.000001  \n","Epoch: [1][1000/36908] Elapsed 10m 19s (remain 370m 34s) Loss: 0.0387(0.1884) Grad: 149.5299  LR: 0.000001  \n","Epoch: [1][1100/36908] Elapsed 11m 21s (remain 369m 32s) Loss: 0.0234(0.1743) Grad: 194.5960  LR: 0.000001  \n","Epoch: [1][1200/36908] Elapsed 12m 23s (remain 368m 32s) Loss: 0.0363(0.1628) Grad: 274.5549  LR: 0.000001  \n","Epoch: [1][1300/36908] Elapsed 13m 25s (remain 367m 38s) Loss: 0.0824(0.1533) Grad: 890.1354  LR: 0.000001  \n","Epoch: [1][1400/36908] Elapsed 14m 27s (remain 366m 38s) Loss: 0.0079(0.1448) Grad: 464.8308  LR: 0.000002  \n","Epoch: [1][1500/36908] Elapsed 15m 29s (remain 365m 35s) Loss: 0.0209(0.1375) Grad: 263.8711  LR: 0.000002  \n","Epoch: [1][1600/36908] Elapsed 16m 31s (remain 364m 31s) Loss: 0.0537(0.1311) Grad: 485.5076  LR: 0.000002  \n","Epoch: [1][1700/36908] Elapsed 17m 33s (remain 363m 27s) Loss: 0.0217(0.1255) Grad: 385.0901  LR: 0.000002  \n","Epoch: [1][1800/36908] Elapsed 18m 35s (remain 362m 22s) Loss: 0.0138(0.1205) Grad: 528.6103  LR: 0.000002  \n","Epoch: [1][1900/36908] Elapsed 19m 37s (remain 361m 16s) Loss: 0.0483(0.1155) Grad: 9210.2168  LR: 0.000002  \n","Epoch: [1][2000/36908] Elapsed 20m 38s (remain 360m 9s) Loss: 0.0151(0.1108) Grad: 3787.4985  LR: 0.000002  \n","Epoch: [1][2100/36908] Elapsed 21m 40s (remain 359m 4s) Loss: 0.0080(0.1065) Grad: 1500.2139  LR: 0.000002  \n","Epoch: [1][2200/36908] Elapsed 22m 42s (remain 357m 59s) Loss: 0.0236(0.1025) Grad: 2764.1167  LR: 0.000002  \n","Epoch: [1][2300/36908] Elapsed 23m 43s (remain 356m 55s) Loss: 0.0025(0.0987) Grad: 518.9813  LR: 0.000002  \n","Epoch: [1][2400/36908] Elapsed 24m 45s (remain 355m 51s) Loss: 0.0156(0.0952) Grad: 1466.8077  LR: 0.000003  \n","Epoch: [1][2500/36908] Elapsed 25m 47s (remain 354m 47s) Loss: 0.0015(0.0919) Grad: 255.9912  LR: 0.000003  \n","Epoch: [1][2600/36908] Elapsed 26m 49s (remain 353m 42s) Loss: 0.0108(0.0889) Grad: 1180.4752  LR: 0.000003  \n","Epoch: [1][2700/36908] Elapsed 27m 50s (remain 352m 40s) Loss: 0.0177(0.0861) Grad: 2957.8250  LR: 0.000003  \n","Epoch: [1][2800/36908] Elapsed 28m 52s (remain 351m 35s) Loss: 0.0042(0.0835) Grad: 401.5098  LR: 0.000003  \n","Epoch: [1][2900/36908] Elapsed 29m 54s (remain 350m 31s) Loss: 0.0011(0.0810) Grad: 121.5422  LR: 0.000003  \n","Epoch: [1][3000/36908] Elapsed 30m 55s (remain 349m 28s) Loss: 0.0140(0.0786) Grad: 3432.0623  LR: 0.000003  \n","Epoch: [1][3100/36908] Elapsed 31m 57s (remain 348m 26s) Loss: 0.0025(0.0764) Grad: 293.4048  LR: 0.000003  \n","Epoch: [1][3200/36908] Elapsed 32m 59s (remain 347m 23s) Loss: 0.0117(0.0744) Grad: 2559.6643  LR: 0.000003  \n","Epoch: [1][3300/36908] Elapsed 34m 1s (remain 346m 19s) Loss: 0.0015(0.0724) Grad: 109.5020  LR: 0.000004  \n","Epoch: [1][3400/36908] Elapsed 35m 2s (remain 345m 16s) Loss: 0.0282(0.0706) Grad: 2055.2351  LR: 0.000004  \n","Epoch: [1][3500/36908] Elapsed 36m 4s (remain 344m 13s) Loss: 0.0126(0.0688) Grad: 998.2405  LR: 0.000004  \n","Epoch: [1][3600/36908] Elapsed 37m 6s (remain 343m 9s) Loss: 0.0144(0.0671) Grad: 2898.9751  LR: 0.000004  \n","Epoch: [1][3700/36908] Elapsed 38m 7s (remain 342m 6s) Loss: 0.0037(0.0655) Grad: 726.5517  LR: 0.000004  \n","Epoch: [1][3800/36908] Elapsed 39m 9s (remain 341m 2s) Loss: 0.0016(0.0640) Grad: 307.1359  LR: 0.000004  \n","Epoch: [1][3900/36908] Elapsed 40m 10s (remain 339m 59s) Loss: 0.0139(0.0626) Grad: 1644.2642  LR: 0.000004  \n","Epoch: [1][4000/36908] Elapsed 41m 12s (remain 338m 57s) Loss: 0.0069(0.0612) Grad: 357.5913  LR: 0.000004  \n","Epoch: [1][4100/36908] Elapsed 42m 14s (remain 337m 55s) Loss: 0.0127(0.0599) Grad: 1661.2944  LR: 0.000004  \n","Epoch: [1][4200/36908] Elapsed 43m 16s (remain 336m 52s) Loss: 0.0020(0.0586) Grad: 818.8564  LR: 0.000005  \n","Epoch: [1][4300/36908] Elapsed 44m 17s (remain 335m 49s) Loss: 0.0029(0.0574) Grad: 137.8782  LR: 0.000005  \n","Epoch: [1][4400/36908] Elapsed 45m 19s (remain 334m 45s) Loss: 0.0012(0.0563) Grad: 596.1041  LR: 0.000005  \n","Epoch: [1][4500/36908] Elapsed 46m 20s (remain 333m 42s) Loss: 0.0126(0.0552) Grad: 2109.5669  LR: 0.000005  \n","Epoch: [1][4600/36908] Elapsed 47m 22s (remain 332m 39s) Loss: 0.0051(0.0542) Grad: 612.5946  LR: 0.000005  \n","Epoch: [1][4700/36908] Elapsed 48m 24s (remain 331m 36s) Loss: 0.0039(0.0532) Grad: 1408.7523  LR: 0.000005  \n","Epoch: [1][4800/36908] Elapsed 49m 25s (remain 330m 34s) Loss: 0.0020(0.0522) Grad: 1646.4712  LR: 0.000005  \n","Epoch: [1][4900/36908] Elapsed 50m 27s (remain 329m 32s) Loss: 0.0084(0.0513) Grad: 3736.4258  LR: 0.000005  \n","Epoch: [1][5000/36908] Elapsed 51m 29s (remain 328m 30s) Loss: 0.0018(0.0503) Grad: 658.7138  LR: 0.000005  \n","Epoch: [1][5100/36908] Elapsed 52m 31s (remain 327m 28s) Loss: 0.0052(0.0495) Grad: 1728.0225  LR: 0.000006  \n","Epoch: [1][5200/36908] Elapsed 53m 32s (remain 326m 25s) Loss: 0.0011(0.0487) Grad: 327.0110  LR: 0.000006  \n","Epoch: [1][5300/36908] Elapsed 54m 34s (remain 325m 23s) Loss: 0.0058(0.0479) Grad: 3457.7131  LR: 0.000006  \n","Epoch: [1][5400/36908] Elapsed 55m 36s (remain 324m 22s) Loss: 0.0016(0.0471) Grad: 756.2781  LR: 0.000006  \n","Epoch: [1][5500/36908] Elapsed 56m 38s (remain 323m 20s) Loss: 0.0046(0.0464) Grad: 1285.4995  LR: 0.000006  \n","Epoch: [1][5600/36908] Elapsed 57m 39s (remain 322m 19s) Loss: 0.0014(0.0457) Grad: 332.4005  LR: 0.000006  \n","Epoch: [1][5700/36908] Elapsed 58m 41s (remain 321m 16s) Loss: 0.0211(0.0449) Grad: 9164.1729  LR: 0.000006  \n","Epoch: [1][5800/36908] Elapsed 59m 43s (remain 320m 14s) Loss: 0.0025(0.0443) Grad: 431.6147  LR: 0.000006  \n","Epoch: [1][5900/36908] Elapsed 60m 44s (remain 319m 12s) Loss: 0.0015(0.0436) Grad: 825.9300  LR: 0.000006  \n","Epoch: [1][6000/36908] Elapsed 61m 46s (remain 318m 9s) Loss: 0.0010(0.0430) Grad: 548.4174  LR: 0.000007  \n","Epoch: [1][6100/36908] Elapsed 62m 48s (remain 317m 7s) Loss: 0.0021(0.0423) Grad: 918.5331  LR: 0.000007  \n","Epoch: [1][6200/36908] Elapsed 63m 49s (remain 316m 4s) Loss: 0.0030(0.0418) Grad: 733.0585  LR: 0.000007  \n","Epoch: [1][6300/36908] Elapsed 64m 51s (remain 315m 2s) Loss: 0.0027(0.0412) Grad: 3651.1096  LR: 0.000007  \n","Epoch: [1][6400/36908] Elapsed 65m 52s (remain 313m 59s) Loss: 0.0000(0.0406) Grad: 9.1692  LR: 0.000007  \n","Epoch: [1][6500/36908] Elapsed 66m 54s (remain 312m 58s) Loss: 0.0057(0.0401) Grad: 1046.9006  LR: 0.000007  \n","Epoch: [1][6600/36908] Elapsed 67m 56s (remain 311m 56s) Loss: 0.0004(0.0395) Grad: 51.6185  LR: 0.000007  \n","Epoch: [1][6700/36908] Elapsed 68m 58s (remain 310m 55s) Loss: 0.0002(0.0390) Grad: 26.8556  LR: 0.000007  \n","Epoch: [1][6800/36908] Elapsed 70m 0s (remain 309m 53s) Loss: 0.0000(0.0385) Grad: 6.5352  LR: 0.000007  \n","Epoch: [1][6900/36908] Elapsed 71m 2s (remain 308m 52s) Loss: 0.0127(0.0380) Grad: 3900.7043  LR: 0.000007  \n","Epoch: [1][7000/36908] Elapsed 72m 4s (remain 307m 51s) Loss: 0.0012(0.0375) Grad: 359.9307  LR: 0.000008  \n","Epoch: [1][7100/36908] Elapsed 73m 5s (remain 306m 50s) Loss: 0.0007(0.0371) Grad: 174.0731  LR: 0.000008  \n","Epoch: [1][7200/36908] Elapsed 74m 7s (remain 305m 49s) Loss: 0.0030(0.0366) Grad: 1859.2452  LR: 0.000008  \n","Epoch: [1][7300/36908] Elapsed 75m 9s (remain 304m 48s) Loss: 0.0081(0.0362) Grad: 4445.4316  LR: 0.000008  \n","Epoch: [1][7400/36908] Elapsed 76m 11s (remain 303m 48s) Loss: 0.0006(0.0358) Grad: 383.5706  LR: 0.000008  \n","Epoch: [1][7500/36908] Elapsed 77m 13s (remain 302m 46s) Loss: 0.0024(0.0354) Grad: 2286.7400  LR: 0.000008  \n","Epoch: [1][7600/36908] Elapsed 78m 15s (remain 301m 44s) Loss: 0.0004(0.0349) Grad: 29.4226  LR: 0.000008  \n","Epoch: [1][7700/36908] Elapsed 79m 17s (remain 300m 43s) Loss: 0.0010(0.0346) Grad: 598.3731  LR: 0.000008  \n","Epoch: [1][7800/36908] Elapsed 80m 19s (remain 299m 41s) Loss: 0.0005(0.0342) Grad: 143.1192  LR: 0.000008  \n","Epoch: [1][7900/36908] Elapsed 81m 20s (remain 298m 39s) Loss: 0.0032(0.0338) Grad: 780.9625  LR: 0.000009  \n","Epoch: [1][8000/36908] Elapsed 82m 22s (remain 297m 37s) Loss: 0.0003(0.0335) Grad: 180.8349  LR: 0.000009  \n","Epoch: [1][8100/36908] Elapsed 83m 24s (remain 296m 36s) Loss: 0.0002(0.0331) Grad: 58.6966  LR: 0.000009  \n","Epoch: [1][8200/36908] Elapsed 84m 26s (remain 295m 35s) Loss: 0.0091(0.0327) Grad: 8086.8418  LR: 0.000009  \n","Epoch: [1][8300/36908] Elapsed 85m 28s (remain 294m 33s) Loss: 0.0022(0.0324) Grad: 654.2057  LR: 0.000009  \n","Epoch: [1][8400/36908] Elapsed 86m 30s (remain 293m 32s) Loss: 0.0012(0.0321) Grad: 730.6080  LR: 0.000009  \n","Epoch: [1][8500/36908] Elapsed 87m 32s (remain 292m 31s) Loss: 0.0013(0.0317) Grad: 498.7051  LR: 0.000009  \n","Epoch: [1][8600/36908] Elapsed 88m 34s (remain 291m 30s) Loss: 0.0011(0.0314) Grad: 385.0623  LR: 0.000009  \n","Epoch: [1][8700/36908] Elapsed 89m 36s (remain 290m 29s) Loss: 0.0000(0.0311) Grad: 7.4692  LR: 0.000009  \n","Epoch: [1][8800/36908] Elapsed 90m 38s (remain 289m 28s) Loss: 0.0072(0.0308) Grad: 6545.8579  LR: 0.000010  \n","Epoch: [1][8900/36908] Elapsed 91m 40s (remain 288m 27s) Loss: 0.0019(0.0305) Grad: 414.4955  LR: 0.000010  \n","Epoch: [1][9000/36908] Elapsed 92m 42s (remain 287m 26s) Loss: 0.0002(0.0302) Grad: 70.6869  LR: 0.000010  \n","Epoch: [1][9100/36908] Elapsed 93m 44s (remain 286m 25s) Loss: 0.0006(0.0299) Grad: 406.7650  LR: 0.000010  \n","Epoch: [1][9200/36908] Elapsed 94m 46s (remain 285m 24s) Loss: 0.1043(0.0296) Grad: 179729.4219  LR: 0.000010  \n","Epoch: [1][9300/36908] Elapsed 95m 48s (remain 284m 22s) Loss: 0.0055(0.0294) Grad: 1980.4865  LR: 0.000010  \n","Epoch: [1][9400/36908] Elapsed 96m 50s (remain 283m 21s) Loss: 0.0042(0.0291) Grad: 4146.7876  LR: 0.000010  \n","Epoch: [1][9500/36908] Elapsed 97m 52s (remain 282m 19s) Loss: 0.0016(0.0288) Grad: 3493.3909  LR: 0.000010  \n","Epoch: [1][9600/36908] Elapsed 98m 54s (remain 281m 18s) Loss: 0.0017(0.0286) Grad: 1740.2223  LR: 0.000010  \n","Epoch: [1][9700/36908] Elapsed 99m 56s (remain 280m 16s) Loss: 0.0024(0.0283) Grad: 1643.5244  LR: 0.000011  \n","Epoch: [1][9800/36908] Elapsed 100m 58s (remain 279m 15s) Loss: 0.0097(0.0281) Grad: 2942.3687  LR: 0.000011  \n","Epoch: [1][9900/36908] Elapsed 102m 0s (remain 278m 14s) Loss: 0.0016(0.0278) Grad: 5358.5312  LR: 0.000011  \n","Epoch: [1][10000/36908] Elapsed 103m 2s (remain 277m 12s) Loss: 0.0005(0.0276) Grad: 240.2163  LR: 0.000011  \n","Epoch: [1][10100/36908] Elapsed 104m 3s (remain 276m 10s) Loss: 0.0012(0.0274) Grad: 1195.8446  LR: 0.000011  \n","Epoch: [1][10200/36908] Elapsed 105m 5s (remain 275m 9s) Loss: 0.0000(0.0271) Grad: 4.2977  LR: 0.000011  \n","Epoch: [1][10300/36908] Elapsed 106m 7s (remain 274m 7s) Loss: 0.0018(0.0269) Grad: 3636.5850  LR: 0.000011  \n","Epoch: [1][10400/36908] Elapsed 107m 9s (remain 273m 5s) Loss: 0.0061(0.0267) Grad: 3915.8450  LR: 0.000011  \n","Epoch: [1][10500/36908] Elapsed 108m 11s (remain 272m 3s) Loss: 0.0010(0.0265) Grad: 661.8269  LR: 0.000011  \n","Epoch: [1][10600/36908] Elapsed 109m 13s (remain 271m 2s) Loss: 0.0014(0.0262) Grad: 828.4364  LR: 0.000011  \n","Epoch: [1][10700/36908] Elapsed 110m 15s (remain 270m 0s) Loss: 0.0112(0.0260) Grad: 14054.6172  LR: 0.000012  \n","Epoch: [1][10800/36908] Elapsed 111m 17s (remain 268m 59s) Loss: 0.0001(0.0258) Grad: 27.9473  LR: 0.000012  \n","Epoch: [1][10900/36908] Elapsed 112m 19s (remain 267m 58s) Loss: 0.0063(0.0256) Grad: 4617.3232  LR: 0.000012  \n","Epoch: [1][11000/36908] Elapsed 113m 21s (remain 266m 57s) Loss: 0.0002(0.0254) Grad: 399.2826  LR: 0.000012  \n","Epoch: [1][11100/36908] Elapsed 114m 23s (remain 265m 56s) Loss: 0.0001(0.0252) Grad: 12.8668  LR: 0.000012  \n","Epoch: [1][11200/36908] Elapsed 115m 25s (remain 264m 54s) Loss: 0.0095(0.0250) Grad: 14178.3320  LR: 0.000012  \n","Epoch: [1][11300/36908] Elapsed 116m 27s (remain 263m 53s) Loss: 0.0063(0.0248) Grad: 3017.3076  LR: 0.000012  \n","Epoch: [1][11400/36908] Elapsed 117m 29s (remain 262m 52s) Loss: 0.0001(0.0247) Grad: 65.4947  LR: 0.000012  \n","Epoch: [1][11500/36908] Elapsed 118m 32s (remain 261m 51s) Loss: 0.0030(0.0245) Grad: 1081.9717  LR: 0.000012  \n","Epoch: [1][11600/36908] Elapsed 119m 34s (remain 260m 50s) Loss: 0.0141(0.0243) Grad: 12871.9990  LR: 0.000013  \n","Epoch: [1][11700/36908] Elapsed 120m 36s (remain 259m 48s) Loss: 0.0158(0.0241) Grad: 26027.8613  LR: 0.000013  \n","Epoch: [1][11800/36908] Elapsed 121m 38s (remain 258m 46s) Loss: 0.0007(0.0239) Grad: 976.3068  LR: 0.000013  \n","Epoch: [1][11900/36908] Elapsed 122m 40s (remain 257m 45s) Loss: 0.0002(0.0238) Grad: 392.4986  LR: 0.000013  \n","Epoch: [1][12000/36908] Elapsed 123m 42s (remain 256m 43s) Loss: 0.0035(0.0236) Grad: 2074.9622  LR: 0.000013  \n","Epoch: [1][12100/36908] Elapsed 124m 44s (remain 255m 42s) Loss: 0.0001(0.0234) Grad: 14.1239  LR: 0.000013  \n","Epoch: [1][12200/36908] Elapsed 125m 46s (remain 254m 41s) Loss: 0.0073(0.0233) Grad: 4615.7183  LR: 0.000013  \n","Epoch: [1][12300/36908] Elapsed 126m 48s (remain 253m 39s) Loss: 0.0037(0.0231) Grad: 1061.8027  LR: 0.000013  \n","Epoch: [1][12400/36908] Elapsed 127m 50s (remain 252m 38s) Loss: 0.0014(0.0230) Grad: 1597.5614  LR: 0.000013  \n","Epoch: [1][12500/36908] Elapsed 128m 52s (remain 251m 36s) Loss: 0.0035(0.0228) Grad: 2161.8157  LR: 0.000014  \n","Epoch: [1][12600/36908] Elapsed 129m 54s (remain 250m 35s) Loss: 0.0001(0.0227) Grad: 26.1077  LR: 0.000014  \n","Epoch: [1][12700/36908] Elapsed 130m 56s (remain 249m 33s) Loss: 0.0003(0.0225) Grad: 86.0180  LR: 0.000014  \n","Epoch: [1][12800/36908] Elapsed 131m 58s (remain 248m 32s) Loss: 0.0134(0.0224) Grad: 11755.9092  LR: 0.000014  \n","Epoch: [1][12900/36908] Elapsed 133m 0s (remain 247m 30s) Loss: 0.0042(0.0222) Grad: 6895.8188  LR: 0.000014  \n","Epoch: [1][13000/36908] Elapsed 134m 2s (remain 246m 28s) Loss: 0.0067(0.0221) Grad: 9515.7744  LR: 0.000014  \n","Epoch: [1][13100/36908] Elapsed 135m 4s (remain 245m 27s) Loss: 0.0003(0.0219) Grad: 79.1671  LR: 0.000014  \n","Epoch: [1][13200/36908] Elapsed 136m 6s (remain 244m 25s) Loss: 0.0034(0.0218) Grad: 5213.3618  LR: 0.000014  \n","Epoch: [1][13300/36908] Elapsed 137m 8s (remain 243m 23s) Loss: 0.0007(0.0217) Grad: 661.9839  LR: 0.000014  \n","Epoch: [1][13400/36908] Elapsed 138m 10s (remain 242m 21s) Loss: 0.0007(0.0215) Grad: 6884.9395  LR: 0.000015  \n","Epoch: [1][13500/36908] Elapsed 139m 12s (remain 241m 20s) Loss: 0.0179(0.0214) Grad: 13501.3164  LR: 0.000015  \n","Epoch: [1][13600/36908] Elapsed 140m 14s (remain 240m 18s) Loss: 0.0181(0.0213) Grad: 117327.5703  LR: 0.000015  \n","Epoch: [1][13700/36908] Elapsed 141m 16s (remain 239m 17s) Loss: 0.0004(0.0211) Grad: 64.6719  LR: 0.000015  \n","Epoch: [1][13800/36908] Elapsed 142m 18s (remain 238m 15s) Loss: 0.0048(0.0210) Grad: 11083.6631  LR: 0.000015  \n","Epoch: [1][13900/36908] Elapsed 143m 20s (remain 237m 13s) Loss: 0.0003(0.0209) Grad: 342.3782  LR: 0.000015  \n","Epoch: [1][14000/36908] Elapsed 144m 21s (remain 236m 11s) Loss: 0.0002(0.0208) Grad: 124.3913  LR: 0.000015  \n","Epoch: [1][14100/36908] Elapsed 145m 23s (remain 235m 10s) Loss: 0.0017(0.0207) Grad: 5964.0586  LR: 0.000015  \n","Epoch: [1][14200/36908] Elapsed 146m 25s (remain 234m 8s) Loss: 0.0001(0.0205) Grad: 61.6511  LR: 0.000015  \n","Epoch: [1][14300/36908] Elapsed 147m 27s (remain 233m 6s) Loss: 0.0011(0.0204) Grad: 564.2748  LR: 0.000015  \n","Epoch: [1][14400/36908] Elapsed 148m 29s (remain 232m 4s) Loss: 0.0000(0.0203) Grad: 10.3617  LR: 0.000016  \n","Epoch: [1][14500/36908] Elapsed 149m 31s (remain 231m 2s) Loss: 0.0019(0.0202) Grad: 2467.8105  LR: 0.000016  \n","Epoch: [1][14600/36908] Elapsed 150m 33s (remain 230m 0s) Loss: 0.0010(0.0201) Grad: 634.4222  LR: 0.000016  \n","Epoch: [1][14700/36908] Elapsed 151m 35s (remain 228m 58s) Loss: 0.0006(0.0199) Grad: 410.7764  LR: 0.000016  \n","Epoch: [1][14800/36908] Elapsed 152m 37s (remain 227m 57s) Loss: 0.0004(0.0198) Grad: 70.3124  LR: 0.000016  \n","Epoch: [1][14900/36908] Elapsed 153m 38s (remain 226m 55s) Loss: 0.0005(0.0197) Grad: 349.8652  LR: 0.000016  \n","Epoch: [1][15000/36908] Elapsed 154m 40s (remain 225m 53s) Loss: 0.0002(0.0196) Grad: 27.8562  LR: 0.000016  \n","Epoch: [1][15100/36908] Elapsed 155m 42s (remain 224m 51s) Loss: 0.0002(0.0195) Grad: 238.8878  LR: 0.000016  \n","Epoch: [1][15200/36908] Elapsed 156m 44s (remain 223m 50s) Loss: 0.0002(0.0194) Grad: 125.6389  LR: 0.000016  \n","Epoch: [1][15300/36908] Elapsed 157m 46s (remain 222m 48s) Loss: 0.0056(0.0193) Grad: 18031.5371  LR: 0.000017  \n","Epoch: [1][15400/36908] Elapsed 158m 48s (remain 221m 46s) Loss: 0.0010(0.0192) Grad: 2093.8618  LR: 0.000017  \n","Epoch: [1][15500/36908] Elapsed 159m 50s (remain 220m 45s) Loss: 0.0001(0.0191) Grad: 41.5066  LR: 0.000017  \n","Epoch: [1][15600/36908] Elapsed 160m 52s (remain 219m 43s) Loss: 0.0001(0.0190) Grad: 55.6136  LR: 0.000017  \n","Epoch: [1][15700/36908] Elapsed 161m 55s (remain 218m 41s) Loss: 0.0023(0.0189) Grad: 2061.1248  LR: 0.000017  \n","Epoch: [1][15800/36908] Elapsed 162m 57s (remain 217m 40s) Loss: 0.0001(0.0188) Grad: 22.6982  LR: 0.000017  \n","Epoch: [1][15900/36908] Elapsed 163m 59s (remain 216m 38s) Loss: 0.0019(0.0187) Grad: 1627.4290  LR: 0.000017  \n","Epoch: [1][16000/36908] Elapsed 165m 1s (remain 215m 37s) Loss: 0.0063(0.0186) Grad: 7340.2290  LR: 0.000017  \n","Epoch: [1][16100/36908] Elapsed 166m 3s (remain 214m 35s) Loss: 0.0097(0.0185) Grad: 6879.9639  LR: 0.000017  \n","Epoch: [1][16200/36908] Elapsed 167m 5s (remain 213m 33s) Loss: 0.0008(0.0184) Grad: 2302.0300  LR: 0.000018  \n","Epoch: [1][16300/36908] Elapsed 168m 7s (remain 212m 31s) Loss: 0.0000(0.0183) Grad: 9.7631  LR: 0.000018  \n","Epoch: [1][16400/36908] Elapsed 169m 9s (remain 211m 30s) Loss: 0.0001(0.0182) Grad: 100.0606  LR: 0.000018  \n","Epoch: [1][16500/36908] Elapsed 170m 11s (remain 210m 28s) Loss: 0.0078(0.0181) Grad: 5477.4600  LR: 0.000018  \n","Epoch: [1][16600/36908] Elapsed 171m 12s (remain 209m 26s) Loss: 0.0001(0.0180) Grad: 60.1579  LR: 0.000018  \n","Epoch: [1][16700/36908] Elapsed 172m 14s (remain 208m 24s) Loss: 0.0018(0.0179) Grad: 979.6052  LR: 0.000018  \n","Epoch: [1][16800/36908] Elapsed 173m 16s (remain 207m 22s) Loss: 0.0046(0.0179) Grad: 10650.0547  LR: 0.000018  \n","Epoch: [1][16900/36908] Elapsed 174m 18s (remain 206m 20s) Loss: 0.0003(0.0178) Grad: 68.8962  LR: 0.000018  \n","Epoch: [1][17000/36908] Elapsed 175m 20s (remain 205m 18s) Loss: 0.0001(0.0177) Grad: 15.8417  LR: 0.000018  \n","Epoch: [1][17100/36908] Elapsed 176m 22s (remain 204m 17s) Loss: 0.0149(0.0176) Grad: 37889.8398  LR: 0.000019  \n","Epoch: [1][17200/36908] Elapsed 177m 24s (remain 203m 15s) Loss: 0.0030(0.0175) Grad: 1358.6583  LR: 0.000019  \n","Epoch: [1][17300/36908] Elapsed 178m 26s (remain 202m 13s) Loss: 0.0030(0.0174) Grad: 2457.3782  LR: 0.000019  \n","Epoch: [1][17400/36908] Elapsed 179m 29s (remain 201m 12s) Loss: 0.0005(0.0174) Grad: 135.7731  LR: 0.000019  \n","Epoch: [1][17500/36908] Elapsed 180m 31s (remain 200m 11s) Loss: 0.0029(0.0173) Grad: 2748.5562  LR: 0.000019  \n","Epoch: [1][17600/36908] Elapsed 181m 33s (remain 199m 9s) Loss: 0.0061(0.0172) Grad: 10528.3174  LR: 0.000019  \n","Epoch: [1][17700/36908] Elapsed 182m 35s (remain 198m 7s) Loss: 0.0038(0.0171) Grad: 29101.7715  LR: 0.000019  \n","Epoch: [1][17800/36908] Elapsed 183m 38s (remain 197m 6s) Loss: 0.0001(0.0170) Grad: 13.1552  LR: 0.000019  \n","Epoch: [1][17900/36908] Elapsed 184m 40s (remain 196m 4s) Loss: 0.0039(0.0170) Grad: 12680.8555  LR: 0.000019  \n","Epoch: [1][18000/36908] Elapsed 185m 42s (remain 195m 2s) Loss: 0.0002(0.0169) Grad: 72.4146  LR: 0.000020  \n","Epoch: [1][18100/36908] Elapsed 186m 44s (remain 194m 1s) Loss: 0.0001(0.0168) Grad: 557.0574  LR: 0.000020  \n","Epoch: [1][18200/36908] Elapsed 187m 46s (remain 192m 59s) Loss: 0.0001(0.0167) Grad: 41.8479  LR: 0.000020  \n","Epoch: [1][18300/36908] Elapsed 188m 48s (remain 191m 57s) Loss: 0.0083(0.0167) Grad: 6510.9404  LR: 0.000020  \n","Epoch: [1][18400/36908] Elapsed 189m 50s (remain 190m 55s) Loss: 0.0154(0.0166) Grad: 100788.0703  LR: 0.000020  \n","Epoch: [1][18500/36908] Elapsed 190m 52s (remain 189m 54s) Loss: 0.0008(0.0165) Grad: 3210.9656  LR: 0.000020  \n","Epoch: [1][18600/36908] Elapsed 191m 54s (remain 188m 52s) Loss: 0.0105(0.0165) Grad: 24711.5059  LR: 0.000020  \n","Epoch: [1][18700/36908] Elapsed 192m 56s (remain 187m 50s) Loss: 0.0001(0.0164) Grad: 412.9440  LR: 0.000020  \n","Epoch: [1][18800/36908] Elapsed 193m 58s (remain 186m 48s) Loss: 0.0012(0.0163) Grad: 1542.7578  LR: 0.000020  \n","Epoch: [1][18900/36908] Elapsed 195m 0s (remain 185m 46s) Loss: 0.0001(0.0163) Grad: 197.5778  LR: 0.000020  \n","Epoch: [1][19000/36908] Elapsed 196m 2s (remain 184m 45s) Loss: 0.0001(0.0162) Grad: 121.3628  LR: 0.000020  \n","Epoch: [1][19100/36908] Elapsed 197m 4s (remain 183m 43s) Loss: 0.0002(0.0161) Grad: 36.1479  LR: 0.000020  \n","Epoch: [1][19200/36908] Elapsed 198m 6s (remain 182m 41s) Loss: 0.0001(0.0160) Grad: 19.2742  LR: 0.000020  \n","Epoch: [1][19300/36908] Elapsed 199m 8s (remain 181m 39s) Loss: 0.0028(0.0160) Grad: 3658.2715  LR: 0.000020  \n","Epoch: [1][19400/36908] Elapsed 200m 10s (remain 180m 38s) Loss: 0.0003(0.0159) Grad: 235.9466  LR: 0.000020  \n","Epoch: [1][19500/36908] Elapsed 201m 12s (remain 179m 36s) Loss: 0.0006(0.0159) Grad: 128.1191  LR: 0.000020  \n","Epoch: [1][19600/36908] Elapsed 202m 14s (remain 178m 34s) Loss: 0.0001(0.0158) Grad: 45.1616  LR: 0.000020  \n","Epoch: [1][19700/36908] Elapsed 203m 16s (remain 177m 32s) Loss: 0.0031(0.0157) Grad: 13673.8691  LR: 0.000020  \n","Epoch: [1][19800/36908] Elapsed 204m 18s (remain 176m 30s) Loss: 0.0001(0.0157) Grad: 612.8412  LR: 0.000020  \n","Epoch: [1][19900/36908] Elapsed 205m 20s (remain 175m 28s) Loss: 0.0008(0.0156) Grad: 1519.7615  LR: 0.000020  \n","Epoch: [1][20000/36908] Elapsed 206m 22s (remain 174m 27s) Loss: 0.0001(0.0155) Grad: 814.2499  LR: 0.000020  \n","Epoch: [1][20100/36908] Elapsed 207m 24s (remain 173m 25s) Loss: 0.0012(0.0155) Grad: 8229.1865  LR: 0.000020  \n","Epoch: [1][20200/36908] Elapsed 208m 26s (remain 172m 23s) Loss: 0.0001(0.0154) Grad: 21.8424  LR: 0.000020  \n","Epoch: [1][20300/36908] Elapsed 209m 29s (remain 171m 22s) Loss: 0.0040(0.0153) Grad: 1206.2423  LR: 0.000020  \n","Epoch: [1][20400/36908] Elapsed 210m 31s (remain 170m 20s) Loss: 0.0000(0.0153) Grad: 36.9480  LR: 0.000020  \n","Epoch: [1][20500/36908] Elapsed 211m 33s (remain 169m 18s) Loss: 0.0034(0.0152) Grad: 13064.6865  LR: 0.000020  \n","Epoch: [1][20600/36908] Elapsed 212m 35s (remain 168m 16s) Loss: 0.0234(0.0152) Grad: 32313.3672  LR: 0.000020  \n","Epoch: [1][20700/36908] Elapsed 213m 37s (remain 167m 14s) Loss: 0.0002(0.0151) Grad: 52.6401  LR: 0.000020  \n","Epoch: [1][20800/36908] Elapsed 214m 39s (remain 166m 12s) Loss: 0.0006(0.0150) Grad: 1076.6376  LR: 0.000020  \n","Epoch: [1][20900/36908] Elapsed 215m 41s (remain 165m 11s) Loss: 0.0047(0.0150) Grad: 10757.4746  LR: 0.000020  \n","Epoch: [1][21000/36908] Elapsed 216m 43s (remain 164m 9s) Loss: 0.0014(0.0149) Grad: 98.6664  LR: 0.000020  \n","Epoch: [1][21100/36908] Elapsed 217m 45s (remain 163m 7s) Loss: 0.0021(0.0149) Grad: 3255.9924  LR: 0.000020  \n","Epoch: [1][21200/36908] Elapsed 218m 47s (remain 162m 5s) Loss: 0.0001(0.0148) Grad: 53.4986  LR: 0.000020  \n","Epoch: [1][21300/36908] Elapsed 219m 49s (remain 161m 3s) Loss: 0.0002(0.0148) Grad: 111.9968  LR: 0.000020  \n","Epoch: [1][21400/36908] Elapsed 220m 51s (remain 160m 1s) Loss: 0.0023(0.0147) Grad: 4929.0620  LR: 0.000020  \n","Epoch: [1][21500/36908] Elapsed 221m 53s (remain 159m 0s) Loss: 0.0010(0.0147) Grad: 1274.1293  LR: 0.000020  \n","Epoch: [1][21600/36908] Elapsed 222m 55s (remain 157m 58s) Loss: 0.0077(0.0146) Grad: 2510.4890  LR: 0.000020  \n","Epoch: [1][21700/36908] Elapsed 223m 57s (remain 156m 56s) Loss: 0.0002(0.0146) Grad: 10.5884  LR: 0.000020  \n","Epoch: [1][21800/36908] Elapsed 224m 59s (remain 155m 54s) Loss: 0.0001(0.0145) Grad: 8.2855  LR: 0.000020  \n","Epoch: [1][21900/36908] Elapsed 226m 1s (remain 154m 52s) Loss: 0.0004(0.0145) Grad: 4094.0337  LR: 0.000020  \n","Epoch: [1][22000/36908] Elapsed 227m 3s (remain 153m 50s) Loss: 0.0001(0.0144) Grad: 119.4129  LR: 0.000020  \n","Epoch: [1][22100/36908] Elapsed 228m 5s (remain 152m 48s) Loss: 0.0005(0.0144) Grad: 2002.8553  LR: 0.000020  \n","Epoch: [1][22200/36908] Elapsed 229m 7s (remain 151m 47s) Loss: 0.0043(0.0143) Grad: 1204.2059  LR: 0.000020  \n","Epoch: [1][22300/36908] Elapsed 230m 9s (remain 150m 45s) Loss: 0.0001(0.0143) Grad: 23.1546  LR: 0.000020  \n","Epoch: [1][22400/36908] Elapsed 231m 11s (remain 149m 43s) Loss: 0.0000(0.0142) Grad: 26.5633  LR: 0.000020  \n","Epoch: [1][22500/36908] Elapsed 232m 13s (remain 148m 41s) Loss: 0.0007(0.0142) Grad: 1789.4532  LR: 0.000020  \n","Epoch: [1][22600/36908] Elapsed 233m 15s (remain 147m 39s) Loss: 0.0003(0.0141) Grad: 13.3528  LR: 0.000020  \n","Epoch: [1][22700/36908] Elapsed 234m 17s (remain 146m 37s) Loss: 0.0034(0.0141) Grad: 1480.4763  LR: 0.000019  \n","Epoch: [1][22800/36908] Elapsed 235m 19s (remain 145m 35s) Loss: 0.0032(0.0140) Grad: 2772.9343  LR: 0.000019  \n","Epoch: [1][22900/36908] Elapsed 236m 21s (remain 144m 33s) Loss: 0.0001(0.0140) Grad: 6.9863  LR: 0.000019  \n","Epoch: [1][23000/36908] Elapsed 237m 23s (remain 143m 32s) Loss: 0.0014(0.0139) Grad: 593.9903  LR: 0.000019  \n","Epoch: [1][23100/36908] Elapsed 238m 25s (remain 142m 30s) Loss: 0.0013(0.0139) Grad: 545.0143  LR: 0.000019  \n","Epoch: [1][23200/36908] Elapsed 239m 27s (remain 141m 28s) Loss: 0.0009(0.0138) Grad: 552.8515  LR: 0.000019  \n","Epoch: [1][23300/36908] Elapsed 240m 29s (remain 140m 26s) Loss: 0.0062(0.0138) Grad: 2862.2573  LR: 0.000019  \n","Epoch: [1][23400/36908] Elapsed 241m 31s (remain 139m 24s) Loss: 0.0002(0.0137) Grad: 66.4128  LR: 0.000019  \n","Epoch: [1][23500/36908] Elapsed 242m 33s (remain 138m 22s) Loss: 0.0000(0.0137) Grad: 4.6189  LR: 0.000019  \n","Epoch: [1][23600/36908] Elapsed 243m 35s (remain 137m 20s) Loss: 0.0021(0.0136) Grad: 2688.4956  LR: 0.000019  \n","Epoch: [1][23700/36908] Elapsed 244m 37s (remain 136m 18s) Loss: 0.0006(0.0136) Grad: 256.1982  LR: 0.000019  \n","Epoch: [1][23800/36908] Elapsed 245m 39s (remain 135m 16s) Loss: 0.0003(0.0136) Grad: 2093.9641  LR: 0.000019  \n","Epoch: [1][23900/36908] Elapsed 246m 41s (remain 134m 14s) Loss: 0.0001(0.0135) Grad: 21.2570  LR: 0.000019  \n","Epoch: [1][24000/36908] Elapsed 247m 43s (remain 133m 12s) Loss: 0.0001(0.0135) Grad: 4.3294  LR: 0.000019  \n","Epoch: [1][24100/36908] Elapsed 248m 45s (remain 132m 11s) Loss: 0.0023(0.0134) Grad: 495.6244  LR: 0.000019  \n","Epoch: [1][24200/36908] Elapsed 249m 47s (remain 131m 9s) Loss: 0.0031(0.0134) Grad: 1255.1561  LR: 0.000019  \n","Epoch: [1][24300/36908] Elapsed 250m 49s (remain 130m 7s) Loss: 0.0010(0.0133) Grad: 304.5907  LR: 0.000019  \n","Epoch: [1][24400/36908] Elapsed 251m 51s (remain 129m 5s) Loss: 0.0002(0.0133) Grad: 40.2629  LR: 0.000019  \n","Epoch: [1][24500/36908] Elapsed 252m 53s (remain 128m 3s) Loss: 0.0359(0.0133) Grad: 9802.7871  LR: 0.000019  \n","Epoch: [1][24600/36908] Elapsed 253m 55s (remain 127m 1s) Loss: 0.0001(0.0132) Grad: 37.8454  LR: 0.000019  \n","Epoch: [1][24700/36908] Elapsed 254m 57s (remain 125m 59s) Loss: 0.0013(0.0132) Grad: 1031.8010  LR: 0.000019  \n","Epoch: [1][24800/36908] Elapsed 255m 59s (remain 124m 57s) Loss: 0.0007(0.0131) Grad: 139.6905  LR: 0.000019  \n","Epoch: [1][24900/36908] Elapsed 257m 1s (remain 123m 56s) Loss: 0.0011(0.0131) Grad: 486.8431  LR: 0.000019  \n","Epoch: [1][25000/36908] Elapsed 258m 3s (remain 122m 54s) Loss: 0.0052(0.0130) Grad: 1021.2568  LR: 0.000019  \n","Epoch: [1][25100/36908] Elapsed 259m 5s (remain 121m 52s) Loss: 0.0009(0.0130) Grad: 112.4822  LR: 0.000019  \n","Epoch: [1][25200/36908] Elapsed 260m 7s (remain 120m 50s) Loss: 0.0047(0.0130) Grad: 1936.4768  LR: 0.000019  \n","Epoch: [1][25300/36908] Elapsed 261m 9s (remain 119m 48s) Loss: 0.0004(0.0129) Grad: 72.4554  LR: 0.000019  \n","Epoch: [1][25400/36908] Elapsed 262m 11s (remain 118m 46s) Loss: 0.0009(0.0129) Grad: 127.4921  LR: 0.000019  \n","Epoch: [1][25500/36908] Elapsed 263m 13s (remain 117m 44s) Loss: 0.0002(0.0128) Grad: 88.7419  LR: 0.000019  \n","Epoch: [1][25600/36908] Elapsed 264m 15s (remain 116m 42s) Loss: 0.0000(0.0128) Grad: 10.1703  LR: 0.000019  \n","Epoch: [1][25700/36908] Elapsed 265m 17s (remain 115m 40s) Loss: 0.0002(0.0128) Grad: 34.8587  LR: 0.000019  \n","Epoch: [1][25800/36908] Elapsed 266m 19s (remain 114m 38s) Loss: 0.0002(0.0127) Grad: 172.5519  LR: 0.000019  \n","Epoch: [1][25900/36908] Elapsed 267m 21s (remain 113m 36s) Loss: 0.0001(0.0127) Grad: 20.7642  LR: 0.000019  \n","Epoch: [1][26000/36908] Elapsed 268m 23s (remain 112m 35s) Loss: 0.0000(0.0127) Grad: 25.9994  LR: 0.000019  \n","Epoch: [1][26100/36908] Elapsed 269m 25s (remain 111m 33s) Loss: 0.0080(0.0126) Grad: 10214.3516  LR: 0.000019  \n","Epoch: [1][26200/36908] Elapsed 270m 27s (remain 110m 31s) Loss: 0.0005(0.0126) Grad: 68.1795  LR: 0.000019  \n","Epoch: [1][26300/36908] Elapsed 271m 29s (remain 109m 29s) Loss: 0.0001(0.0126) Grad: 2.9326  LR: 0.000019  \n","Epoch: [1][26400/36908] Elapsed 272m 31s (remain 108m 27s) Loss: 0.0021(0.0125) Grad: 435.3401  LR: 0.000019  \n","Epoch: [1][26500/36908] Elapsed 273m 33s (remain 107m 25s) Loss: 0.0020(0.0125) Grad: 644.8132  LR: 0.000019  \n","Epoch: [1][26600/36908] Elapsed 274m 34s (remain 106m 23s) Loss: 0.0048(0.0124) Grad: 3411.2742  LR: 0.000019  \n","Epoch: [1][26700/36908] Elapsed 275m 36s (remain 105m 21s) Loss: 0.0004(0.0124) Grad: 2250.3110  LR: 0.000019  \n","Epoch: [1][26800/36908] Elapsed 276m 39s (remain 104m 19s) Loss: 0.0023(0.0124) Grad: 2218.5886  LR: 0.000019  \n","Epoch: [1][26900/36908] Elapsed 277m 40s (remain 103m 17s) Loss: 0.0001(0.0123) Grad: 84.7058  LR: 0.000019  \n","Epoch: [1][27000/36908] Elapsed 278m 42s (remain 102m 15s) Loss: 0.0001(0.0123) Grad: 143.3281  LR: 0.000019  \n","Epoch: [1][27100/36908] Elapsed 279m 44s (remain 101m 13s) Loss: 0.0009(0.0123) Grad: 203.7030  LR: 0.000019  \n","Epoch: [1][27200/36908] Elapsed 280m 46s (remain 100m 11s) Loss: 0.0002(0.0123) Grad: 53.2364  LR: 0.000019  \n","Epoch: [1][27300/36908] Elapsed 281m 48s (remain 99m 10s) Loss: 0.0046(0.0122) Grad: 3365.3445  LR: 0.000019  \n","Epoch: [1][27400/36908] Elapsed 282m 51s (remain 98m 8s) Loss: 0.0001(0.0122) Grad: 16.4043  LR: 0.000019  \n","Epoch: [1][27500/36908] Elapsed 283m 53s (remain 97m 6s) Loss: 0.0097(0.0122) Grad: 9740.3320  LR: 0.000019  \n","Epoch: [1][27600/36908] Elapsed 284m 55s (remain 96m 4s) Loss: 0.0033(0.0121) Grad: 2568.0618  LR: 0.000019  \n","Epoch: [1][27700/36908] Elapsed 285m 56s (remain 95m 2s) Loss: 0.0002(0.0121) Grad: 185.1969  LR: 0.000019  \n","Epoch: [1][27800/36908] Elapsed 286m 58s (remain 94m 0s) Loss: 0.0005(0.0121) Grad: 224.4977  LR: 0.000019  \n","Epoch: [1][27900/36908] Elapsed 288m 0s (remain 92m 58s) Loss: 0.0001(0.0120) Grad: 9.8650  LR: 0.000019  \n","Epoch: [1][28000/36908] Elapsed 289m 2s (remain 91m 56s) Loss: 0.0059(0.0120) Grad: 10792.1357  LR: 0.000019  \n","Epoch: [1][28100/36908] Elapsed 290m 4s (remain 90m 54s) Loss: 0.0041(0.0120) Grad: 2968.9749  LR: 0.000019  \n","Epoch: [1][28200/36908] Elapsed 291m 6s (remain 89m 52s) Loss: 0.0029(0.0119) Grad: 3346.0068  LR: 0.000019  \n","Epoch: [1][28300/36908] Elapsed 292m 8s (remain 88m 50s) Loss: 0.0001(0.0119) Grad: 78.7239  LR: 0.000019  \n","Epoch: [1][28400/36908] Elapsed 293m 10s (remain 87m 48s) Loss: 0.0001(0.0119) Grad: 7.0601  LR: 0.000019  \n","Epoch: [1][28500/36908] Elapsed 294m 12s (remain 86m 47s) Loss: 0.0001(0.0118) Grad: 36.3387  LR: 0.000019  \n","Epoch: [1][28600/36908] Elapsed 295m 14s (remain 85m 45s) Loss: 0.0004(0.0118) Grad: 78.9053  LR: 0.000019  \n","Epoch: [1][28700/36908] Elapsed 296m 16s (remain 84m 43s) Loss: 0.0009(0.0118) Grad: 4399.8315  LR: 0.000019  \n","Epoch: [1][28800/36908] Elapsed 297m 18s (remain 83m 41s) Loss: 0.0081(0.0117) Grad: 5219.7690  LR: 0.000019  \n","Epoch: [1][28900/36908] Elapsed 298m 20s (remain 82m 39s) Loss: 0.0017(0.0117) Grad: 751.1309  LR: 0.000019  \n","Epoch: [1][29000/36908] Elapsed 299m 22s (remain 81m 37s) Loss: 0.0012(0.0117) Grad: 529.1607  LR: 0.000019  \n","Epoch: [1][29100/36908] Elapsed 300m 24s (remain 80m 35s) Loss: 0.0001(0.0116) Grad: 102.2536  LR: 0.000019  \n","Epoch: [1][29200/36908] Elapsed 301m 26s (remain 79m 33s) Loss: 0.0001(0.0116) Grad: 173.7510  LR: 0.000019  \n","Epoch: [1][29300/36908] Elapsed 302m 28s (remain 78m 31s) Loss: 0.0037(0.0116) Grad: 2002.3851  LR: 0.000019  \n","Epoch: [1][29400/36908] Elapsed 303m 31s (remain 77m 29s) Loss: 0.0122(0.0116) Grad: 7236.3550  LR: 0.000019  \n","Epoch: [1][29500/36908] Elapsed 304m 33s (remain 76m 28s) Loss: 0.0024(0.0115) Grad: 6346.9854  LR: 0.000019  \n","Epoch: [1][29600/36908] Elapsed 305m 35s (remain 75m 26s) Loss: 0.0000(0.0115) Grad: 14.4292  LR: 0.000019  \n","Epoch: [1][29700/36908] Elapsed 306m 37s (remain 74m 24s) Loss: 0.0000(0.0115) Grad: 15.7610  LR: 0.000019  \n","Epoch: [1][29800/36908] Elapsed 307m 38s (remain 73m 22s) Loss: 0.0008(0.0114) Grad: 1990.3359  LR: 0.000019  \n","Epoch: [1][29900/36908] Elapsed 308m 40s (remain 72m 20s) Loss: 0.0001(0.0114) Grad: 73.7508  LR: 0.000019  \n","Epoch: [1][30000/36908] Elapsed 309m 42s (remain 71m 18s) Loss: 0.0116(0.0114) Grad: 20525.6875  LR: 0.000019  \n","Epoch: [1][30100/36908] Elapsed 310m 44s (remain 70m 16s) Loss: 0.0004(0.0113) Grad: 255.4346  LR: 0.000019  \n","Epoch: [1][30200/36908] Elapsed 311m 46s (remain 69m 14s) Loss: 0.0137(0.0113) Grad: 5294.5396  LR: 0.000019  \n","Epoch: [1][30300/36908] Elapsed 312m 48s (remain 68m 12s) Loss: 0.0041(0.0113) Grad: 3450.4780  LR: 0.000019  \n","Epoch: [1][30400/36908] Elapsed 313m 50s (remain 67m 10s) Loss: 0.0003(0.0113) Grad: 24.5686  LR: 0.000019  \n","Epoch: [1][30500/36908] Elapsed 314m 51s (remain 66m 8s) Loss: 0.0076(0.0112) Grad: 27641.0391  LR: 0.000019  \n","Epoch: [1][30600/36908] Elapsed 315m 53s (remain 65m 6s) Loss: 0.0011(0.0112) Grad: 491.5782  LR: 0.000019  \n","Epoch: [1][30700/36908] Elapsed 316m 55s (remain 64m 4s) Loss: 0.0022(0.0112) Grad: 1562.6902  LR: 0.000019  \n","Epoch: [1][30800/36908] Elapsed 317m 56s (remain 63m 2s) Loss: 0.0001(0.0112) Grad: 36.0185  LR: 0.000019  \n","Epoch: [1][30900/36908] Elapsed 318m 58s (remain 62m 0s) Loss: 0.0006(0.0111) Grad: 1196.6340  LR: 0.000019  \n","Epoch: [1][31000/36908] Elapsed 320m 0s (remain 60m 58s) Loss: 0.0021(0.0111) Grad: 2934.8206  LR: 0.000018  \n","Epoch: [1][31100/36908] Elapsed 321m 1s (remain 59m 56s) Loss: 0.0026(0.0111) Grad: 2480.6260  LR: 0.000018  \n","Epoch: [1][31200/36908] Elapsed 322m 3s (remain 58m 54s) Loss: 0.0001(0.0110) Grad: 25.9344  LR: 0.000018  \n","Epoch: [1][31300/36908] Elapsed 323m 5s (remain 57m 52s) Loss: 0.0010(0.0110) Grad: 580.2743  LR: 0.000018  \n","Epoch: [1][31400/36908] Elapsed 324m 7s (remain 56m 50s) Loss: 0.0007(0.0110) Grad: 1451.7080  LR: 0.000018  \n","Epoch: [1][31500/36908] Elapsed 325m 9s (remain 55m 48s) Loss: 0.0008(0.0110) Grad: 947.8133  LR: 0.000018  \n","Epoch: [1][31600/36908] Elapsed 326m 11s (remain 54m 46s) Loss: 0.0001(0.0109) Grad: 31.6818  LR: 0.000018  \n","Epoch: [1][31700/36908] Elapsed 327m 12s (remain 53m 44s) Loss: 0.0021(0.0109) Grad: 5162.4326  LR: 0.000018  \n","Epoch: [1][31800/36908] Elapsed 328m 14s (remain 52m 42s) Loss: 0.0029(0.0109) Grad: 12813.7061  LR: 0.000018  \n","Epoch: [1][31900/36908] Elapsed 329m 16s (remain 51m 40s) Loss: 0.0005(0.0109) Grad: 609.1228  LR: 0.000018  \n","Epoch: [1][32000/36908] Elapsed 330m 18s (remain 50m 38s) Loss: 0.0002(0.0108) Grad: 168.7019  LR: 0.000018  \n","Epoch: [1][32100/36908] Elapsed 331m 20s (remain 49m 36s) Loss: 0.0026(0.0108) Grad: 2685.4341  LR: 0.000018  \n","Epoch: [1][32200/36908] Elapsed 332m 21s (remain 48m 35s) Loss: 0.0000(0.0108) Grad: 26.6320  LR: 0.000018  \n","Epoch: [1][32300/36908] Elapsed 333m 23s (remain 47m 33s) Loss: 0.0017(0.0108) Grad: 1422.7792  LR: 0.000018  \n","Epoch: [1][32400/36908] Elapsed 334m 25s (remain 46m 31s) Loss: 0.0012(0.0107) Grad: 772.1866  LR: 0.000018  \n","Epoch: [1][32500/36908] Elapsed 335m 27s (remain 45m 29s) Loss: 0.0004(0.0107) Grad: 991.4938  LR: 0.000018  \n","Epoch: [1][32600/36908] Elapsed 336m 28s (remain 44m 27s) Loss: 0.0014(0.0107) Grad: 2582.6470  LR: 0.000018  \n","Epoch: [1][32700/36908] Elapsed 337m 30s (remain 43m 25s) Loss: 0.0002(0.0106) Grad: 2281.0637  LR: 0.000018  \n","Epoch: [1][32800/36908] Elapsed 338m 32s (remain 42m 23s) Loss: 0.0081(0.0106) Grad: 10912.8223  LR: 0.000018  \n","Epoch: [1][32900/36908] Elapsed 339m 33s (remain 41m 21s) Loss: 0.0033(0.0106) Grad: 4240.4458  LR: 0.000018  \n","Epoch: [1][33000/36908] Elapsed 340m 35s (remain 40m 19s) Loss: 0.0002(0.0106) Grad: 128.7006  LR: 0.000018  \n","Epoch: [1][33100/36908] Elapsed 341m 37s (remain 39m 17s) Loss: 0.0003(0.0106) Grad: 148.8363  LR: 0.000018  \n","Epoch: [1][33200/36908] Elapsed 342m 38s (remain 38m 15s) Loss: 0.0006(0.0105) Grad: 782.4044  LR: 0.000018  \n","Epoch: [1][33300/36908] Elapsed 343m 40s (remain 37m 13s) Loss: 0.0028(0.0105) Grad: 14740.3184  LR: 0.000018  \n","Epoch: [1][33400/36908] Elapsed 344m 42s (remain 36m 11s) Loss: 0.0003(0.0105) Grad: 153.9202  LR: 0.000018  \n","Epoch: [1][33500/36908] Elapsed 345m 44s (remain 35m 9s) Loss: 0.0016(0.0105) Grad: 3177.1123  LR: 0.000018  \n","Epoch: [1][33600/36908] Elapsed 346m 45s (remain 34m 7s) Loss: 0.0042(0.0104) Grad: 13169.0791  LR: 0.000018  \n","Epoch: [1][33700/36908] Elapsed 347m 47s (remain 33m 5s) Loss: 0.0001(0.0104) Grad: 4.2666  LR: 0.000018  \n","Epoch: [1][33800/36908] Elapsed 348m 49s (remain 32m 3s) Loss: 0.0047(0.0104) Grad: 10768.4707  LR: 0.000018  \n","Epoch: [1][33900/36908] Elapsed 349m 51s (remain 31m 1s) Loss: 0.0035(0.0104) Grad: 4352.9243  LR: 0.000018  \n","Epoch: [1][34000/36908] Elapsed 350m 53s (remain 29m 59s) Loss: 0.0006(0.0103) Grad: 1244.3945  LR: 0.000018  \n","Epoch: [1][34100/36908] Elapsed 351m 54s (remain 28m 58s) Loss: 0.0010(0.0103) Grad: 2722.0952  LR: 0.000018  \n","Epoch: [1][34200/36908] Elapsed 352m 56s (remain 27m 56s) Loss: 0.0006(0.0103) Grad: 284.1189  LR: 0.000018  \n","Epoch: [1][34300/36908] Elapsed 353m 58s (remain 26m 54s) Loss: 0.0001(0.0103) Grad: 25.8394  LR: 0.000018  \n","Epoch: [1][34400/36908] Elapsed 354m 59s (remain 25m 52s) Loss: 0.0002(0.0102) Grad: 38.5476  LR: 0.000018  \n","Epoch: [1][34500/36908] Elapsed 356m 1s (remain 24m 50s) Loss: 0.0002(0.0102) Grad: 116.5164  LR: 0.000018  \n","Epoch: [1][34600/36908] Elapsed 357m 3s (remain 23m 48s) Loss: 0.0012(0.0102) Grad: 856.2292  LR: 0.000018  \n","Epoch: [1][34700/36908] Elapsed 358m 4s (remain 22m 46s) Loss: 0.0007(0.0102) Grad: 227.9890  LR: 0.000018  \n","Epoch: [1][34800/36908] Elapsed 359m 6s (remain 21m 44s) Loss: 0.0002(0.0102) Grad: 68.9926  LR: 0.000018  \n","Epoch: [1][34900/36908] Elapsed 360m 8s (remain 20m 42s) Loss: 0.0001(0.0101) Grad: 30.1244  LR: 0.000018  \n","Epoch: [1][35000/36908] Elapsed 361m 10s (remain 19m 40s) Loss: 0.0069(0.0101) Grad: 19134.3945  LR: 0.000018  \n","Epoch: [1][35100/36908] Elapsed 362m 11s (remain 18m 38s) Loss: 0.0004(0.0101) Grad: 213.9167  LR: 0.000018  \n","Epoch: [1][35200/36908] Elapsed 363m 13s (remain 17m 36s) Loss: 0.0032(0.0101) Grad: 15731.6543  LR: 0.000018  \n","Epoch: [1][35300/36908] Elapsed 364m 15s (remain 16m 34s) Loss: 0.0001(0.0101) Grad: 245.9862  LR: 0.000018  \n","Epoch: [1][35400/36908] Elapsed 365m 17s (remain 15m 32s) Loss: 0.0001(0.0100) Grad: 19.1495  LR: 0.000018  \n","Epoch: [1][35500/36908] Elapsed 366m 18s (remain 14m 31s) Loss: 0.0024(0.0100) Grad: 5628.5000  LR: 0.000018  \n","Epoch: [1][35600/36908] Elapsed 367m 20s (remain 13m 29s) Loss: 0.0001(0.0100) Grad: 18.1645  LR: 0.000018  \n","Epoch: [1][35700/36908] Elapsed 368m 22s (remain 12m 27s) Loss: 0.0027(0.0100) Grad: 25718.6855  LR: 0.000018  \n","Epoch: [1][35800/36908] Elapsed 369m 23s (remain 11m 25s) Loss: 0.0012(0.0099) Grad: 1806.9833  LR: 0.000018  \n","Epoch: [1][35900/36908] Elapsed 370m 25s (remain 10m 23s) Loss: 0.0033(0.0099) Grad: 16214.3008  LR: 0.000018  \n","Epoch: [1][36000/36908] Elapsed 371m 27s (remain 9m 21s) Loss: 0.0053(0.0099) Grad: 7362.3682  LR: 0.000018  \n","Epoch: [1][36100/36908] Elapsed 372m 28s (remain 8m 19s) Loss: 0.0002(0.0099) Grad: 229.3705  LR: 0.000018  \n","Epoch: [1][36200/36908] Elapsed 373m 30s (remain 7m 17s) Loss: 0.0023(0.0099) Grad: 5349.4658  LR: 0.000018  \n","Epoch: [1][36300/36908] Elapsed 374m 32s (remain 6m 15s) Loss: 0.0001(0.0098) Grad: 766.4781  LR: 0.000018  \n","Epoch: [1][36400/36908] Elapsed 375m 33s (remain 5m 13s) Loss: 0.0005(0.0098) Grad: 1037.1617  LR: 0.000018  \n","Epoch: [1][36500/36908] Elapsed 376m 35s (remain 4m 11s) Loss: 0.0119(0.0098) Grad: 60231.9297  LR: 0.000018  \n","Epoch: [1][36600/36908] Elapsed 377m 37s (remain 3m 10s) Loss: 0.0005(0.0098) Grad: 237.2656  LR: 0.000018  \n","Epoch: [1][36700/36908] Elapsed 378m 39s (remain 2m 8s) Loss: 0.0000(0.0098) Grad: 23.4268  LR: 0.000018  \n","Epoch: [1][36800/36908] Elapsed 379m 41s (remain 1m 6s) Loss: 0.0136(0.0097) Grad: 241421.4688  LR: 0.000018  \n","Epoch: [1][36900/36908] Elapsed 380m 42s (remain 0m 4s) Loss: 0.0000(0.0097) Grad: 14.5817  LR: 0.000018  \n","Epoch: [1][36907/36908] Elapsed 380m 47s (remain 0m 0s) Loss: 0.0021(0.0097) Grad: 2081.9121  LR: 0.000018  \n","EVAL: [0/1192] Elapsed 0m 0s (remain 11m 4s) Loss: 0.0001(0.0001) \n","EVAL: [100/1192] Elapsed 0m 22s (remain 4m 5s) Loss: 0.0569(0.0081) \n","EVAL: [200/1192] Elapsed 0m 44s (remain 3m 40s) Loss: 0.0102(0.0071) \n","EVAL: [300/1192] Elapsed 1m 6s (remain 3m 18s) Loss: 0.0091(0.0075) \n","EVAL: [400/1192] Elapsed 1m 28s (remain 2m 55s) Loss: 0.0000(0.0072) \n","EVAL: [500/1192] Elapsed 1m 51s (remain 2m 33s) Loss: 0.0644(0.0070) \n","EVAL: [600/1192] Elapsed 2m 13s (remain 2m 10s) Loss: 0.0119(0.0073) \n","EVAL: [700/1192] Elapsed 2m 35s (remain 1m 48s) Loss: 0.0044(0.0080) \n","EVAL: [800/1192] Elapsed 2m 57s (remain 1m 26s) Loss: 0.0185(0.0082) \n","EVAL: [900/1192] Elapsed 3m 19s (remain 1m 4s) Loss: 0.0119(0.0084) \n","EVAL: [1000/1192] Elapsed 3m 42s (remain 0m 42s) Loss: 0.0000(0.0081) \n","EVAL: [1100/1192] Elapsed 4m 4s (remain 0m 20s) Loss: 0.0220(0.0079) \n","EVAL: [1191/1192] Elapsed 4m 24s (remain 0m 0s) Loss: 0.0000(0.0077) \n","Epoch 1 - avg_train_loss: 0.0097  avg_val_loss: 0.0077  time: 23114s\n","Epoch 1 - Score: 0.8950\n","Epoch 1 - Save Best Score: 0.8950 Model\n","best_thres: 0.52  score: 0.88937\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"64961f62c5f94c3991e2b9f09c7c4782","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.52G [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Load weight from pretrained\n","load weights from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp068/fold1_best.pth\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8cc41478e6584c039ec440aa3c314e6d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Load weight from pretrained\n","load weights from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp068/fold2_best.pth\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5a5fdc30f42646058b9162b0e0974e3d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Load weight from pretrained\n","load weights from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp068/fold3_best.pth\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c74b2e0635904f98920a922b26b6541d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n"]}],"source":["if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"name":"nbme-exp068.ipynb","provenance":[{"file_id":"10yG4L3_nzpdL2CDwqxa9r-KWq6jYkWfl","timestamp":1648467991163}],"version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"papermill":{"default_parameters":{},"duration":641.572862,"end_time":"2022-02-27T11:39:50.972497","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-02-27T11:29:09.399635","version":"2.3.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"02633c7de1ea4b7ca2fd899ae0c6d209":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"032d2a594c45472e9681d2d7557ab93d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09da6c904a3344ad9d7d0f17c646c8b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_66daca2dd30d44efbcd88adcbb1c2725","placeholder":"​","style":"IPY_MODEL_02633c7de1ea4b7ca2fd899ae0c6d209","value":" 446k/446k [00:00\u0026lt;00:00, 4.84MB/s]"}},"0c748174ece64ca6992b434a2d92e1e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_827b5e8189d242c9b62bb8d6a084de72","IPY_MODEL_0f910e441d334b10bc666e6ef47de941","IPY_MODEL_09da6c904a3344ad9d7d0f17c646c8b1"],"layout":"IPY_MODEL_d6ba1f9a002c49268799fc4a17f793ee"}},"0f0864ef340b49aaa9b4596e032163a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0f910e441d334b10bc666e6ef47de941":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_760cf6427cdc4b05affb72378d92a0f3","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9a660863781b487392504ab3302a5f93","value":456318}},"13133c559d1b4a14ba19c157c169b532":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13873604cb644df0b4b5e8e9ea57d645":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c619fc144a44ae8ba6d091c236a20f6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c64feffa14d4a56b3bf4c25f6d05c51":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1fc34ac85840429e8d5d62785817dd03","placeholder":"​","style":"IPY_MODEL_c7dbd264bb804aabbb9a3a3922cdcc00","value":" 52.0/52.0 [00:00\u0026lt;00:00, 2.12kB/s]"}},"1fc34ac85840429e8d5d62785817dd03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"263650ffbf3145048f331827b49ef11b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_886eee1754fd42e185a7548aa44871f1","placeholder":"​","style":"IPY_MODEL_ec232b9c334c4987b35fde837fac77da","value":" 878k/878k [00:00\u0026lt;00:00, 3.84MB/s]"}},"2c0468cceac144b890d30b510202deba":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2df59efc32cf4642a4cfff7db709db5a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb191fc8e771447ab389bbb57fe22d2f","max":143,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ced0845b868342c29d4e2d830a48f203","value":143}},"323239c2e16449c088afd6eb5eeaa73f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5f8f50bdaf843f1a938f6129848cd83","max":898825,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0f0864ef340b49aaa9b4596e032163a7","value":898825}},"323befd254f741a7b041d124d4073817":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33bbf6ff0a9847fc9900a16ea8247120":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_61d290b99d78422494bd99ba7f4ce0b4","max":52,"min":0,"orientation":"horizontal","style":"IPY_MODEL_511fe7d1f0db48f5848e3539ddb29fff","value":52}},"3d505cf1fb134928953da8d79d68665a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c276b71af2f4301b4048e4f8dad36d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_563519b534874ec8a145d292258b2dad","IPY_MODEL_535a8156a6da43928fd0d7a33c624540","IPY_MODEL_ef3dd8c9fb4a46bfa6c30489d2d75b02"],"layout":"IPY_MODEL_5d985ee5c7d84bf9a135972d46830ad0"}},"511fe7d1f0db48f5848e3539ddb29fff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"530069c0165e4b3d8e077e9c6a4a1d21":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c0468cceac144b890d30b510202deba","placeholder":"​","style":"IPY_MODEL_b4c22ff3f7064b5c82b501058fa367a6","value":"Downloading: 100%"}},"535a8156a6da43928fd0d7a33c624540":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8fa83fd7255f43ff9126aa633ed1b663","max":42146,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9febd109d3a24bef886c8d24808347ba","value":42146}},"5504bc5bee8c4c189614fd398d1b7fef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"563519b534874ec8a145d292258b2dad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5bc25dd928614ba999cefcde5efa9197","placeholder":"​","style":"IPY_MODEL_5ccb74cb082845d7ade9962f741a2910","value":"100%"}},"5a47b531ee814b1eba201f0aa79212db":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5bb3e7fc97c64d59a20a7ebb29a39800":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5bc25dd928614ba999cefcde5efa9197":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ccb74cb082845d7ade9962f741a2910":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d985ee5c7d84bf9a135972d46830ad0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61d290b99d78422494bd99ba7f4ce0b4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"638cf831bc0443658b4b455200f9b990":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f0a30d22d004fd6866556696346fb74","max":475,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5504bc5bee8c4c189614fd398d1b7fef","value":475}},"6639cae9d2844ea3b7d9a777b43b2181":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6e5be1d244794198afee6c8fefd3a191","IPY_MODEL_2df59efc32cf4642a4cfff7db709db5a","IPY_MODEL_72d201f9470a4d04b1e89f7d0018c0d7"],"layout":"IPY_MODEL_cebcd629be2340bfa4ca1780d70dd364"}},"66a3685d05e1493c987e6cb1d9ed00a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66daca2dd30d44efbcd88adcbb1c2725":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e5be1d244794198afee6c8fefd3a191":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_da9bce59ac4845cda340d840b79be311","placeholder":"​","style":"IPY_MODEL_323befd254f741a7b041d124d4073817","value":"100%"}},"72d201f9470a4d04b1e89f7d0018c0d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_13873604cb644df0b4b5e8e9ea57d645","placeholder":"​","style":"IPY_MODEL_f129267b3ae6422e8d345c28b73f5516","value":" 143/143 [00:00\u0026lt;00:00, 2848.65it/s]"}},"760cf6427cdc4b05affb72378d92a0f3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c1442db3949418184bfb68266910d91":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e01c08e3af148f580fcdc6ef6ebf5b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7ebb3f8b10154ef4badb2d6856140d18":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"808441188c1a4b5788ae84fa3edf27db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d505cf1fb134928953da8d79d68665a","placeholder":"​","style":"IPY_MODEL_81714296902f4d26a32aa05da8890693","value":"Downloading: 100%"}},"80be6bc8b1c8454187599fe6f05cdcba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"811a7f7f9bc34108b113d084a7d488f4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81714296902f4d26a32aa05da8890693":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8254bfa8a6fa47dba57db5ba29dccaeb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"827b5e8189d242c9b62bb8d6a084de72":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc036c5ffc0a4a0fb2d0f504cf4264f3","placeholder":"​","style":"IPY_MODEL_80be6bc8b1c8454187599fe6f05cdcba","value":"Downloading: 100%"}},"886eee1754fd42e185a7548aa44871f1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e1aca2f17c6476a855cbe11a1d661a9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aa1104924af9458eb0d5443fd617cf29","IPY_MODEL_b94dd392dc874911b1deeb39f77b342f","IPY_MODEL_be8e2f5fc8eb4029a4aece3d1776054d"],"layout":"IPY_MODEL_032d2a594c45472e9681d2d7557ab93d"}},"8f0a30d22d004fd6866556696346fb74":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fa83fd7255f43ff9126aa633ed1b663":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9826121100004ec49f1cd7ed26023d9f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cfc527e5b7e84c45af32ba7e49275790","IPY_MODEL_33bbf6ff0a9847fc9900a16ea8247120","IPY_MODEL_1c64feffa14d4a56b3bf4c25f6d05c51"],"layout":"IPY_MODEL_b42a1221467e43648126dc9432be0b28"}},"9a660863781b487392504ab3302a5f93":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9febd109d3a24bef886c8d24808347ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a2fc47910e5a4b158eddf7faa455d683":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_808441188c1a4b5788ae84fa3edf27db","IPY_MODEL_638cf831bc0443658b4b455200f9b990","IPY_MODEL_c5ad324c87914e26ad665efcc418f354"],"layout":"IPY_MODEL_8254bfa8a6fa47dba57db5ba29dccaeb"}},"aa1104924af9458eb0d5443fd617cf29":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd01049905654fe6bee6c4c602b89ed9","placeholder":"​","style":"IPY_MODEL_66a3685d05e1493c987e6cb1d9ed00a1","value":"100%"}},"b42a1221467e43648126dc9432be0b28":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4c22ff3f7064b5c82b501058fa367a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5f8f50bdaf843f1a938f6129848cd83":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b94dd392dc874911b1deeb39f77b342f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_13133c559d1b4a14ba19c157c169b532","max":42146,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7e01c08e3af148f580fcdc6ef6ebf5b4","value":42146}},"be8e2f5fc8eb4029a4aece3d1776054d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e464d1ba21a1489183fda5c01bbc0cca","placeholder":"​","style":"IPY_MODEL_5bb3e7fc97c64d59a20a7ebb29a39800","value":" 42146/42146 [00:22\u0026lt;00:00, 2038.25it/s]"}},"c5ad324c87914e26ad665efcc418f354":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a47b531ee814b1eba201f0aa79212db","placeholder":"​","style":"IPY_MODEL_7c1442db3949418184bfb68266910d91","value":" 475/475 [00:00\u0026lt;00:00, 18.8kB/s]"}},"c7dbd264bb804aabbb9a3a3922cdcc00":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd01049905654fe6bee6c4c602b89ed9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cebcd629be2340bfa4ca1780d70dd364":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ced0845b868342c29d4e2d830a48f203":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cfc527e5b7e84c45af32ba7e49275790":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c619fc144a44ae8ba6d091c236a20f6","placeholder":"​","style":"IPY_MODEL_f3dcb10079874c84a85896aae581b1dc","value":"Downloading: 100%"}},"d6ba1f9a002c49268799fc4a17f793ee":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da9bce59ac4845cda340d840b79be311":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc036c5ffc0a4a0fb2d0f504cf4264f3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e464d1ba21a1489183fda5c01bbc0cca":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb191fc8e771447ab389bbb57fe22d2f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec232b9c334c4987b35fde837fac77da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ecfdf18519e34e2d9a0b112b0815cba5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef3dd8c9fb4a46bfa6c30489d2d75b02":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_811a7f7f9bc34108b113d084a7d488f4","placeholder":"​","style":"IPY_MODEL_ecfdf18519e34e2d9a0b112b0815cba5","value":" 42146/42146 [00:00\u0026lt;00:00, 687236.90it/s]"}},"f129267b3ae6422e8d345c28b73f5516":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f3dcb10079874c84a85896aae581b1dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f61172e130474199999fe10c8470b1e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_530069c0165e4b3d8e077e9c6a4a1d21","IPY_MODEL_323239c2e16449c088afd6eb5eeaa73f","IPY_MODEL_263650ffbf3145048f331827b49ef11b"],"layout":"IPY_MODEL_7ebb3f8b10154ef4badb2d6856140d18"}}}}},"nbformat":4,"nbformat_minor":5}