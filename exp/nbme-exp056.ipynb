{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "architectural-introduction",
   "metadata": {
    "id": "blind-kingdom"
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-praise",
   "metadata": {
    "id": "antique-glenn"
   },
   "source": [
    "- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-commodity",
   "metadata": {
    "id": "bored-ministry"
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exotic-genius",
   "metadata": {
    "id": "deadly-confidence"
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"nbme-exp056\"\n",
    "ENV = \"local\"\n",
    "DEBUG_MODE = False\n",
    "SUBMISSION_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vital-cricket",
   "metadata": {
    "id": "aware-worcester"
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    env=ENV\n",
    "    exp_name=EXP_NAME\n",
    "    debug=DEBUG_MODE\n",
    "    submission=SUBMISSION_MODE\n",
    "    apex=True\n",
    "    input_dir=None\n",
    "    output_dir=None\n",
    "    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n",
    "    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n",
    "    competition_name=\"nbme-score-clinical-patient-notes\"\n",
    "    id_col=\"id\"\n",
    "    target_col=\"location\"\n",
    "    pretrained_model_name=\"microsoft/deberta-large\"\n",
    "    tokenizer=None\n",
    "    max_len=None\n",
    "    max_char_len=None\n",
    "    output_dim=1\n",
    "    dropout=0.2\n",
    "    num_workers=4\n",
    "    batch_size=3\n",
    "    lr=2e-5\n",
    "    betas=(0.9, 0.98)\n",
    "    weight_decay=0.1\n",
    "    num_warmup_steps_rate=0.1\n",
    "    batch_scheduler=True\n",
    "    epochs=5\n",
    "    n_fold=4\n",
    "    train_fold=[0, 1, 2, 3]\n",
    "    seed=71\n",
    "    gradient_accumulation_steps=2\n",
    "    max_grad_norm=1000\n",
    "    print_freq=100\n",
    "    train=True\n",
    "    inference=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "smart-mustang",
   "metadata": {
    "id": "personalized-death"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.train_fold = [0, 1]\n",
    "\n",
    "if CFG.submission:\n",
    "    CFG.train = False\n",
    "    CFG.inference = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-howard",
   "metadata": {
    "id": "cardiovascular-neutral"
   },
   "source": [
    "## Directory Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "chief-robertson",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "checked-boards",
    "outputId": "cea3ff7c-73cb-479b-82dc-d4ad1b7d6719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "print(CFG.env)\n",
    "if CFG.env == \"colab\":\n",
    "    # colab環境\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "    # install packages\n",
    "    !pip install transformers==4.16.2\n",
    "\n",
    "elif CFG.env == \"local\":\n",
    "    # ローカルサーバ\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "\n",
    "elif CFG.env == \"kaggle\":\n",
    "    # kaggle環境\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "floating-inquiry",
   "metadata": {
    "id": "vital-mexico"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import BartModel,BertModel,BertTokenizer\n",
    "from transformers import DebertaModel,DebertaTokenizer\n",
    "from transformers import RobertaModel,RobertaTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-rating",
   "metadata": {
    "id": "economic-ladder"
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unauthorized-portsmouth",
   "metadata": {
    "id": "desperate-keyboard"
   },
   "outputs": [],
   "source": [
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "spatial-prisoner",
   "metadata": {
    "id": "flexible-wednesday"
   },
   "outputs": [],
   "source": [
    "def create_labels_for_scoring(df):\n",
    "    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n",
    "    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, \"location\"]\n",
    "        if lst:\n",
    "            new_lst = \";\".join(lst)\n",
    "            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n",
    "\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df[\"location_for_create_labels\"].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, token_probs, tokenizer):\n",
    "    res = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n",
    "        encoded = tokenizer(\n",
    "            text=text,\n",
    "            max_length=CFG.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n",
    "            start, end = offset_mapping\n",
    "            res[i][start:end] = pred\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_predicted_location_str(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        # result = np.where(char_prob >= th)[0] + 1\n",
    "        result = np.where(char_prob >= th)[0]\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        # result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = [f\"{min(r)} {max(r) + 1}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def scoring(df, th=0.5, use_token_prob=True):\n",
    "    labels = create_labels_for_scoring(df)\n",
    "\n",
    "    if use_token_prob:\n",
    "        token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n",
    "        char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n",
    "    else:\n",
    "        char_probs = df[[str(i) for i in range(CFG.max_char_len)]].values\n",
    "        char_probs = [char_probs[i] for i in range(len(char_probs))]\n",
    "\n",
    "    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n",
    "    preds = get_predictions(predicted_location_str)\n",
    "\n",
    "    score = get_score(labels, preds)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_best_thres(oof_df):\n",
    "    def f1_opt(x):\n",
    "        return -1 * scoring(oof_df, th=x)\n",
    "\n",
    "    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n",
    "    return best_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "objective-beatles",
   "metadata": {
    "id": "logical-chemistry"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "surface-heavy",
   "metadata": {
    "id": "gorgeous-record"
   },
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-function",
   "metadata": {
    "id": "frozen-africa"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "continent-thanksgiving",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "shaped-metallic",
    "outputId": "c2e09677-e2ae-4b51-b411-f1a0634026d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 6), (143, 3), (42146, 3), (5, 4))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(CFG.input_dir / \"train.csv\")\n",
    "features = pd.read_csv(CFG.input_dir / \"features.csv\")\n",
    "patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n",
    "test = pd.read_csv(CFG.input_dir / \"test.csv\")\n",
    "\n",
    "train.shape, features.shape, patient_notes.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "metric-error",
   "metadata": {
    "id": "visible-australia"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n",
    "    print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-heating",
   "metadata": {
    "id": "hydraulic-gibson"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dietary-worse",
   "metadata": {
    "id": "interpreted-northeast"
   },
   "outputs": [],
   "source": [
    "def preprocess_features(features):\n",
    "    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "\n",
    "\n",
    "features = preprocess_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "activated-glucose",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "martial-blind",
    "outputId": "5e7f2195-15c6-4e13-fc35-359b478f2af2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 8), (5, 6))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "raising-connecticut",
   "metadata": {
    "id": "electoral-favor"
   },
   "outputs": [],
   "source": [
    "train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n",
    "train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "demanding-providence",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "reported-parade",
    "outputId": "bf29b91f-33b5-4aea-ce84-2e4fb8e9bf40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4399\n",
       "1    8181\n",
       "2    1296\n",
       "3     287\n",
       "4      99\n",
       "5      27\n",
       "6       9\n",
       "7       1\n",
       "8       1\n",
       "Name: annotation_length, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n",
    "display(train['annotation_length'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-regard",
   "metadata": {
    "id": "enabling-relevance"
   },
   "source": [
    "## CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acting-personal",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "mature-coalition",
    "outputId": "ed39702e-822c-408e-ad92-eccd51f61c97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    3575\n",
       "1    3575\n",
       "2    3575\n",
       "3    3575\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Fold = GroupKFold(n_splits=CFG.n_fold)\n",
    "groups = train['pn_num'].values\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-doubt",
   "metadata": {
    "id": "subjective-entrance"
   },
   "source": [
    "## Setup tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "mighty-result",
   "metadata": {
    "id": "dramatic-afghanistan"
   },
   "outputs": [],
   "source": [
    "if CFG.submission:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG.pretrained_model_name)\n",
    "    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n",
    "\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-storm",
   "metadata": {
    "id": "divided-arrow"
   },
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "satellite-dictionary",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "8c7eb508782d4253af8cbff0a39e5d19",
      "33e4f48f7c8f40f48081c34bc992f215",
      "0c70395dca6341349fc883dc6b95090a",
      "0dfe4d3aa0354d95bb5d019420b510a9",
      "e6775e6fc2ad4b14b473b8a07e27426d",
      "40ae5d1c9e9f4feaa4207599ef17a1ec",
      "8cd3352e5e9342e4a814e9958ea6dc1d",
      "5a92171cb3d34fc8b1ed5119e32951ac",
      "a202501b76a748fe80180604dff32c7b",
      "c3d9ac191d9b4772ae4bca323a34ddd7",
      "9deae6afbd4740df8eb0289bc5f53ae7"
     ]
    },
    "id": "immune-campbell",
    "outputId": "dbda74a8-5a58-49cf-b86a-3e05c265f758"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c85cda4b89946c0b251a300e6025917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 433\n"
     ]
    }
   ],
   "source": [
    "pn_history_lengths = []\n",
    "tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    pn_history_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(pn_history_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "given-malpractice",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "47e8d82f628f4bd7b8b0ef0aa96852a5",
      "cc9a4ba21d664dafb8ae32a4e0a1c5e0",
      "f4421d3b3a844dbcb003f11902ee1898",
      "d02f186539954463873bb560b775894e",
      "14efac00edd349898e9fa95a63a2773d",
      "83d1b90076dc431893e0ab1c87e35f9c",
      "0b503e832e57492291cbf6e9ae66e343",
      "625abc68d2fb4fd4b8556c7cc1ae514a",
      "d2581946e9fd4f9a8dc56b3453cc70bd",
      "1f6c8df95c7845818253189f2e365ba9",
      "5e29db6be6284caa978ee223047f23c5"
     ]
    },
    "id": "northern-branch",
    "outputId": "e2c62ebe-aae9-413d-d424-6850759224a2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576587176e3b44cd97abd1d46ce22a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 30\n"
     ]
    }
   ],
   "source": [
    "feature_text_lengths = []\n",
    "tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    feature_text_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(feature_text_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "recognized-athletics",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oriental-jacksonville",
    "outputId": "293b5e4d-a369-433b-a5af-fbfd35f411a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 466\n"
     ]
    }
   ],
   "source": [
    "CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n",
    "\n",
    "print(\"max length:\", CFG.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "mature-silver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6b0006a4d145258cb76d11a9de990e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 950\n"
     ]
    }
   ],
   "source": [
    "pn_history_lengths = []\n",
    "tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n",
    "for text in tk0:\n",
    "    length = len(text)\n",
    "    pn_history_lengths.append(length)\n",
    "\n",
    "CFG.max_char_len = max(pn_history_lengths)\n",
    "\n",
    "print(\"max length:\", CFG.max_char_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "entire-salon",
   "metadata": {
    "id": "flexible-trainer"
   },
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.max_char_len = self.cfg.max_char_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "        self.annotation_lengths = self.df[\"annotation_length\"].values\n",
    "        self.locations = self.df[\"location\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def _create_mapping_from_token_to_char(self, pn_history):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        mapping_from_token_to_char = np.zeros(self.max_char_len)\n",
    "        offset_mapping = encoded[\"offset_mapping\"]\n",
    "        for i, offset in enumerate(offset_mapping):\n",
    "            start_idx, end_idx = offset\n",
    "            mapping_from_token_to_char[start_idx:end_idx] = i\n",
    "        return torch.tensor(mapping_from_token_to_char, dtype=torch.long)\n",
    "\n",
    "    def _create_label(self, pn_history, annotation_length, location_list):\n",
    "        label = np.zeros(self.max_char_len)\n",
    "        label[len(pn_history):] = -1\n",
    "        if annotation_length > 0:\n",
    "            for location in location_list:\n",
    "                for loc in [s.split() for s in location.split(\";\")]:\n",
    "                    start, end = int(loc[0]), int(loc[1])\n",
    "                    label[start:end] = 1\n",
    "        return torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n",
    "        mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n",
    "        return input_, label, mapping_from_token_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fancy-silicon",
   "metadata": {
    "id": "stock-robertson"
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def _create_mapping_from_token_to_char(self, pn_history):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        mapping_from_token_to_char = np.zeros(self.max_char_len)\n",
    "        offset_mapping = encoded[\"offset_mapping\"]\n",
    "        for i, offset in enumerate(offset_mapping):\n",
    "            start_idx, end_idx = offset\n",
    "            mapping_from_token_to_char[start_idx:end_idx] = i\n",
    "        return torch.tensor(mapping_from_token_to_char, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n",
    "        return input_, mapping_from_token_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-corrections",
   "metadata": {
    "id": "chemical-lucas"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "collect-insured",
   "metadata": {
    "id": "animated-array"
   },
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, model_config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if model_config_path is None:\n",
    "            self.model_config = AutoConfig.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "        else:\n",
    "            self.model_config = torch.load(model_config_path)\n",
    "\n",
    "        if pretrained:\n",
    "            self.backbone = AutoModel.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                config=self.model_config,\n",
    "            )\n",
    "            print(f\"Load weight from pretrained\")\n",
    "        else:\n",
    "            #self.backbone = AutoModel.from_config(self.model_config)\n",
    "            itpt = AutoModelForMaskedLM.from_config(self.model_config)\n",
    "            #path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp010/checkpoint-130170/pytorch_model.bin\")\n",
    "            path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n",
    "            state_dict = torch.load(path)\n",
    "            itpt.load_state_dict(state_dict)\n",
    "            self.backbone = itpt.deberta\n",
    "            print(f\"Load weight from {path}\")\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.model_config.hidden_size,\n",
    "            bidirectional=True,\n",
    "            hidden_size=self.model_config.hidden_size // 2,\n",
    "            num_layers=2,\n",
    "            dropout=self.cfg.dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(self.cfg.dropout),\n",
    "            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, mappings_from_token_to_char):\n",
    "        h = self.backbone(**inputs)[\"last_hidden_state\"]  # [batch, seq_len, d_model]\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.unsqueeze(2).expand(-1, -1, self.model_config.hidden_size)\n",
    "        h = torch.gather(h, 1, mappings_from_token_to_char)    # [batch, seq_len, d_model]\n",
    "        h, _ = self.lstm(h)\n",
    "        output = self.fc(h)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-stream",
   "metadata": {
    "id": "thorough-bristol"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "inappropriate-coordinator",
   "metadata": {
    "id": "talented-quantity"
   },
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    train_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    epoch,\n",
    "    scheduler,\n",
    "    device,\n",
    "):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels, mappings_from_token_to_char) in enumerate(train_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device) \n",
    "        batch_size = labels.size(0)\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            output = model(inputs, mappings_from_token_to_char)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if CFG.batch_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n",
    "            print(\n",
    "                \"Epoch: [{0}][{1}/{2}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                \"Grad: {grad_norm:.4f}  \"\n",
    "                \"LR: {lr:.6f}  \"\n",
    "                .format(\n",
    "                    epoch+1,\n",
    "                    step,\n",
    "                    len(train_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n",
    "                    loss=losses,\n",
    "                     grad_norm=grad_norm,\n",
    "                     lr=scheduler.get_lr()[0],\n",
    "                )\n",
    "            )\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "danish-ceremony",
   "metadata": {
    "id": "figured-cooperative"
   },
   "outputs": [],
   "source": [
    "def valid_fn(\n",
    "    val_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    device,\n",
    "):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels, mappings_from_token_to_char) in enumerate(val_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device) \n",
    "        batch_size = labels.size(0)\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            output = model(inputs, mappings_from_token_to_char)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "        loss = loss.mean()\n",
    "    \n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n",
    "            print(\n",
    "                \"EVAL: [{0}/{1}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                .format(\n",
    "                    step, len(val_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n",
    "                    loss=losses,\n",
    "                )\n",
    "            )\n",
    "    preds = np.concatenate(preds)\n",
    "    return losses.avg, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "consistent-chicken",
   "metadata": {
    "id": "played-pointer"
   },
   "outputs": [],
   "source": [
    "def inference_fn(test_dataloader, model, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preds = []\n",
    "    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n",
    "    for (inputs, mappings_from_token_to_char) in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs, mappings_from_token_to_char)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "colonial-gateway",
   "metadata": {
    "id": "brazilian-nigeria"
   },
   "outputs": [],
   "source": [
    "def train_loop(df, i_fold, device):\n",
    "    print(f\"========== fold: {i_fold} training ==========\")\n",
    "    train_idx = df[df[\"fold\"] != i_fold].index\n",
    "    val_idx = df[df[\"fold\"] == i_fold].index\n",
    "\n",
    "    train_folds = df.loc[train_idx].reset_index(drop=True)\n",
    "    val_folds = df.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = TrainingDataset(CFG, train_folds)\n",
    "    val_dataset = TrainingDataset(CFG, val_folds)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "    model = CustomModel(CFG, model_config_path=None, pretrained=False)   # itptを使うため\n",
    "    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n",
    "    model.to(device)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in param_optimizer if not any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n",
    "        {\"params\": [p for n, p in param_optimizer if any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=CFG.lr,\n",
    "        betas=CFG.betas,\n",
    "        weight_decay=CFG.weight_decay,\n",
    "    )\n",
    "    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n",
    "    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_train_optimization_steps,\n",
    "    )\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    best_score = -1 * np.inf\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "        avg_loss = train_fn(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            scheduler,\n",
    "            device,\n",
    "        )\n",
    "        avg_val_loss, val_preds = valid_fn(\n",
    "            val_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "\n",
    "        # scoring\n",
    "        val_folds[[str(i) for i in range(CFG.max_char_len)]] = val_preds\n",
    "        score = scoring(val_folds, th=0.5, use_token_prob=False)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n",
    "        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"predictions\": val_preds,\n",
    "                },\n",
    "                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "            )\n",
    "\n",
    "    predictions = torch.load(\n",
    "        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )[\"predictions\"]\n",
    "    val_folds[[str(i) for i in range(CFG.max_char_len)]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return val_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-empire",
   "metadata": {
    "id": "bearing-switch"
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "european-auction",
   "metadata": {
    "id": "desperate-crime"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for i_fold in range(CFG.n_fold):\n",
    "            if i_fold in CFG.train_fold:\n",
    "                _oof_df = train_loop(train, i_fold, device)\n",
    "                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n",
    "        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    if CFG.submission:\n",
    "        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n",
    "    else:\n",
    "        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    score = scoring(oof_df, th=0.5, use_token_prob=False)\n",
    "    print(f\"Best thres: 0.5, Score: {score:.4f}\")\n",
    "    best_thres = get_best_thres(oof_df)\n",
    "    score = scoring(oof_df, th=best_thres, use_token_prob=False)\n",
    "    print(f\"Best thres: {best_thres}, Score: {score:.4f}\")\n",
    "\n",
    "    if CFG.inference:\n",
    "        test_dataset = TestDataset(CFG, test)\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=CFG.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=CFG.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        predictions = []\n",
    "        for i_fold in CFG.train_fold:\n",
    "            if CFG.submission:\n",
    "                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n",
    "                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n",
    "            else:\n",
    "                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n",
    "\n",
    "            state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "            model.load_state_dict(state[\"model\"])\n",
    "            test_token_probs = inference_fn(test_dataloader, model, device)\n",
    "            test[[f\"fold{i_fold}_{i}\" for i in range(CFG.max_len)]] = test_token_probs\n",
    "            test_char_probs = get_char_probs(test[\"pn_history\"].values, test_token_probs, CFG.tokenizer)\n",
    "            predictions.append(test_char_probs)\n",
    "\n",
    "            del state, test_token_probs, model; gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        predictions = np.mean(predictions, axis=0)\n",
    "        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n",
    "        test[CFG.target_col] = predicted_location_str\n",
    "        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n",
    "        test[[CFG.id_col, CFG.target_col]].to_csv(\n",
    "            CFG.output_dir / \"submission.csv\", index=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "gentle-channels",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "graduate-vision",
    "outputId": "90d56101-1cd3-4eac-8d44-5be254003857"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/3575] Elapsed 0m 0s (remain 55m 42s) Loss: 0.3446(0.3446) Grad: 36554.5703  LR: 0.000000  \n",
      "Epoch: [1][100/3575] Elapsed 0m 59s (remain 33m 51s) Loss: 0.3331(0.3400) Grad: 35140.1133  LR: 0.000001  \n",
      "Epoch: [1][200/3575] Elapsed 1m 57s (remain 32m 45s) Loss: 0.2988(0.3292) Grad: 36022.1445  LR: 0.000002  \n",
      "Epoch: [1][300/3575] Elapsed 2m 56s (remain 32m 0s) Loss: 0.2315(0.3094) Grad: 38218.6953  LR: 0.000003  \n",
      "Epoch: [1][400/3575] Elapsed 3m 55s (remain 31m 5s) Loss: 0.1217(0.2774) Grad: 34972.6992  LR: 0.000004  \n",
      "Epoch: [1][500/3575] Elapsed 4m 54s (remain 30m 7s) Loss: 0.0738(0.2361) Grad: 1225.0009  LR: 0.000006  \n",
      "Epoch: [1][600/3575] Elapsed 5m 53s (remain 29m 8s) Loss: 0.0344(0.2037) Grad: 797.0891  LR: 0.000007  \n",
      "Epoch: [1][700/3575] Elapsed 6m 51s (remain 28m 7s) Loss: 0.0093(0.1804) Grad: 4360.2793  LR: 0.000008  \n",
      "Epoch: [1][800/3575] Elapsed 7m 50s (remain 27m 10s) Loss: 0.0115(0.1622) Grad: 6844.0620  LR: 0.000009  \n",
      "Epoch: [1][900/3575] Elapsed 8m 50s (remain 26m 13s) Loss: 0.0090(0.1473) Grad: 10013.4082  LR: 0.000010  \n",
      "Epoch: [1][1000/3575] Elapsed 9m 48s (remain 25m 13s) Loss: 0.0115(0.1348) Grad: 5056.9849  LR: 0.000011  \n",
      "Epoch: [1][1100/3575] Elapsed 10m 47s (remain 24m 15s) Loss: 0.0053(0.1246) Grad: 22330.8164  LR: 0.000012  \n",
      "Epoch: [1][1200/3575] Elapsed 11m 46s (remain 23m 17s) Loss: 0.0146(0.1160) Grad: 6219.0415  LR: 0.000013  \n",
      "Epoch: [1][1300/3575] Elapsed 12m 45s (remain 22m 18s) Loss: 0.0080(0.1085) Grad: 315.2312  LR: 0.000015  \n",
      "Epoch: [1][1400/3575] Elapsed 13m 45s (remain 21m 20s) Loss: 0.0613(0.1022) Grad: 40238.8086  LR: 0.000016  \n",
      "Epoch: [1][1500/3575] Elapsed 14m 44s (remain 20m 21s) Loss: 0.0052(0.0964) Grad: 2564.3230  LR: 0.000017  \n",
      "Epoch: [1][1600/3575] Elapsed 15m 44s (remain 19m 24s) Loss: 0.0026(0.0916) Grad: 3760.6470  LR: 0.000018  \n",
      "Epoch: [1][1700/3575] Elapsed 16m 44s (remain 18m 26s) Loss: 0.0043(0.0871) Grad: 21088.0234  LR: 0.000019  \n",
      "Epoch: [1][1800/3575] Elapsed 17m 42s (remain 17m 26s) Loss: 0.0137(0.0830) Grad: 19063.1035  LR: 0.000020  \n",
      "Epoch: [1][1900/3575] Elapsed 18m 42s (remain 16m 28s) Loss: 0.0082(0.0794) Grad: 12847.8760  LR: 0.000020  \n",
      "Epoch: [1][2000/3575] Elapsed 19m 42s (remain 15m 30s) Loss: 0.0050(0.0763) Grad: 3661.6072  LR: 0.000020  \n",
      "Epoch: [1][2100/3575] Elapsed 20m 42s (remain 14m 31s) Loss: 0.0279(0.0733) Grad: 3111.7771  LR: 0.000020  \n",
      "Epoch: [1][2200/3575] Elapsed 21m 40s (remain 13m 32s) Loss: 0.0009(0.0705) Grad: 12249.4541  LR: 0.000019  \n",
      "Epoch: [1][2300/3575] Elapsed 22m 39s (remain 12m 32s) Loss: 0.0077(0.0679) Grad: 10163.3936  LR: 0.000019  \n",
      "Epoch: [1][2400/3575] Elapsed 23m 39s (remain 11m 34s) Loss: 0.0178(0.0657) Grad: 25189.9316  LR: 0.000019  \n",
      "Epoch: [1][2500/3575] Elapsed 24m 39s (remain 10m 35s) Loss: 0.0133(0.0635) Grad: 11473.0693  LR: 0.000019  \n",
      "Epoch: [1][2600/3575] Elapsed 25m 39s (remain 9m 36s) Loss: 0.0016(0.0614) Grad: 552.9920  LR: 0.000019  \n",
      "Epoch: [1][2700/3575] Elapsed 26m 38s (remain 8m 37s) Loss: 0.0013(0.0597) Grad: 544.8447  LR: 0.000019  \n",
      "Epoch: [1][2800/3575] Elapsed 27m 37s (remain 7m 38s) Loss: 0.0021(0.0580) Grad: 149.1838  LR: 0.000019  \n",
      "Epoch: [1][2900/3575] Elapsed 28m 36s (remain 6m 38s) Loss: 0.0376(0.0564) Grad: 55044.9023  LR: 0.000019  \n",
      "Epoch: [1][3000/3575] Elapsed 29m 34s (remain 5m 39s) Loss: 0.0256(0.0548) Grad: 54079.7070  LR: 0.000018  \n",
      "Epoch: [1][3100/3575] Elapsed 30m 33s (remain 4m 40s) Loss: 0.0087(0.0533) Grad: 4937.2246  LR: 0.000018  \n",
      "Epoch: [1][3200/3575] Elapsed 31m 32s (remain 3m 41s) Loss: 0.0036(0.0519) Grad: 3008.6567  LR: 0.000018  \n",
      "Epoch: [1][3300/3575] Elapsed 32m 30s (remain 2m 41s) Loss: 0.0006(0.0506) Grad: 131.2037  LR: 0.000018  \n",
      "Epoch: [1][3400/3575] Elapsed 33m 28s (remain 1m 42s) Loss: 0.0005(0.0494) Grad: 463.7302  LR: 0.000018  \n",
      "Epoch: [1][3500/3575] Elapsed 34m 27s (remain 0m 43s) Loss: 0.0083(0.0482) Grad: 8099.1914  LR: 0.000018  \n",
      "Epoch: [1][3574/3575] Elapsed 35m 11s (remain 0m 0s) Loss: 0.0008(0.0474) Grad: 3530.9990  LR: 0.000018  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 7m 47s) Loss: 0.0041(0.0041) \n",
      "EVAL: [100/1192] Elapsed 0m 18s (remain 3m 23s) Loss: 0.0253(0.0078) \n",
      "EVAL: [200/1192] Elapsed 0m 37s (remain 3m 3s) Loss: 0.0207(0.0078) \n",
      "EVAL: [300/1192] Elapsed 0m 55s (remain 2m 45s) Loss: 0.0059(0.0082) \n",
      "EVAL: [400/1192] Elapsed 1m 14s (remain 2m 27s) Loss: 0.0035(0.0087) \n",
      "EVAL: [500/1192] Elapsed 1m 33s (remain 2m 9s) Loss: 0.0264(0.0083) \n",
      "EVAL: [600/1192] Elapsed 1m 52s (remain 1m 50s) Loss: 0.0126(0.0089) \n",
      "EVAL: [700/1192] Elapsed 2m 11s (remain 1m 32s) Loss: 0.0797(0.0103) \n",
      "EVAL: [800/1192] Elapsed 2m 30s (remain 1m 13s) Loss: 0.0047(0.0105) \n",
      "EVAL: [900/1192] Elapsed 2m 49s (remain 0m 54s) Loss: 0.0199(0.0104) \n",
      "EVAL: [1000/1192] Elapsed 3m 8s (remain 0m 35s) Loss: 0.0022(0.0102) \n",
      "EVAL: [1100/1192] Elapsed 3m 26s (remain 0m 17s) Loss: 0.0082(0.0099) \n",
      "EVAL: [1191/1192] Elapsed 3m 43s (remain 0m 0s) Loss: 0.0004(0.0096) \n",
      "Epoch 1 - avg_train_loss: 0.0474  avg_val_loss: 0.0096  time: 2339s\n",
      "Epoch 1 - Score: 0.8358\n",
      "Epoch 1 - Save Best Score: 0.8358 Model\n",
      "Epoch: [2][0/3575] Elapsed 0m 0s (remain 53m 50s) Loss: 0.0007(0.0007) Grad: 1059.2843  LR: 0.000018  \n",
      "Epoch: [2][100/3575] Elapsed 0m 59s (remain 34m 20s) Loss: 0.0018(0.0080) Grad: 7903.3779  LR: 0.000018  \n",
      "Epoch: [2][200/3575] Elapsed 1m 59s (remain 33m 25s) Loss: 0.0013(0.0074) Grad: 7372.3853  LR: 0.000018  \n",
      "Epoch: [2][300/3575] Elapsed 2m 59s (remain 32m 28s) Loss: 0.0810(0.0078) Grad: 209952.1250  LR: 0.000017  \n",
      "Epoch: [2][400/3575] Elapsed 3m 58s (remain 31m 26s) Loss: 0.0076(0.0080) Grad: 20127.2910  LR: 0.000017  \n",
      "Epoch: [2][500/3575] Elapsed 4m 57s (remain 30m 23s) Loss: 0.0043(0.0084) Grad: 16884.5156  LR: 0.000017  \n",
      "Epoch: [2][600/3575] Elapsed 5m 56s (remain 29m 25s) Loss: 0.0012(0.0084) Grad: 2917.4065  LR: 0.000017  \n",
      "Epoch: [2][700/3575] Elapsed 6m 55s (remain 28m 22s) Loss: 0.0004(0.0081) Grad: 1089.4895  LR: 0.000017  \n",
      "Epoch: [2][800/3575] Elapsed 7m 55s (remain 27m 25s) Loss: 0.0028(0.0083) Grad: 8260.0361  LR: 0.000017  \n",
      "Epoch: [2][900/3575] Elapsed 8m 55s (remain 26m 28s) Loss: 0.0023(0.0082) Grad: 833.5378  LR: 0.000017  \n",
      "Epoch: [2][1000/3575] Elapsed 9m 53s (remain 25m 26s) Loss: 0.0104(0.0084) Grad: 43128.8555  LR: 0.000017  \n",
      "Epoch: [2][1100/3575] Elapsed 10m 52s (remain 24m 25s) Loss: 0.0056(0.0085) Grad: 14052.7998  LR: 0.000016  \n",
      "Epoch: [2][1200/3575] Elapsed 11m 50s (remain 23m 24s) Loss: 0.0001(0.0087) Grad: 69.1659  LR: 0.000016  \n",
      "Epoch: [2][1300/3575] Elapsed 12m 48s (remain 22m 23s) Loss: 0.0085(0.0085) Grad: 8407.6055  LR: 0.000016  \n",
      "Epoch: [2][1400/3575] Elapsed 13m 48s (remain 21m 25s) Loss: 0.0062(0.0085) Grad: 18529.8105  LR: 0.000016  \n",
      "Epoch: [2][1500/3575] Elapsed 14m 47s (remain 20m 26s) Loss: 0.0041(0.0084) Grad: 18164.5332  LR: 0.000016  \n",
      "Epoch: [2][1600/3575] Elapsed 15m 46s (remain 19m 27s) Loss: 0.0132(0.0085) Grad: 53257.1484  LR: 0.000016  \n",
      "Epoch: [2][1700/3575] Elapsed 16m 44s (remain 18m 26s) Loss: 0.0006(0.0085) Grad: 820.5556  LR: 0.000016  \n",
      "Epoch: [2][1800/3575] Elapsed 17m 42s (remain 17m 26s) Loss: 0.0009(0.0084) Grad: 6787.4878  LR: 0.000016  \n",
      "Epoch: [2][1900/3575] Elapsed 18m 41s (remain 16m 27s) Loss: 0.0019(0.0085) Grad: 2126.6262  LR: 0.000015  \n",
      "Epoch: [2][2000/3575] Elapsed 19m 39s (remain 15m 27s) Loss: 0.0170(0.0084) Grad: 21514.4609  LR: 0.000015  \n",
      "Epoch: [2][2100/3575] Elapsed 20m 38s (remain 14m 28s) Loss: 0.0080(0.0084) Grad: 24234.9570  LR: 0.000015  \n",
      "Epoch: [2][2200/3575] Elapsed 21m 36s (remain 13m 29s) Loss: 0.0005(0.0083) Grad: 430.2757  LR: 0.000015  \n",
      "Epoch: [2][2300/3575] Elapsed 22m 36s (remain 12m 30s) Loss: 0.0441(0.0082) Grad: 237317.7656  LR: 0.000015  \n",
      "Epoch: [2][2400/3575] Elapsed 23m 36s (remain 11m 32s) Loss: 0.0005(0.0084) Grad: 310.5165  LR: 0.000015  \n",
      "Epoch: [2][2500/3575] Elapsed 24m 35s (remain 10m 33s) Loss: 0.0011(0.0083) Grad: 5351.0488  LR: 0.000015  \n",
      "Epoch: [2][2600/3575] Elapsed 25m 34s (remain 9m 34s) Loss: 0.0031(0.0082) Grad: 28551.1758  LR: 0.000015  \n",
      "Epoch: [2][2700/3575] Elapsed 26m 32s (remain 8m 35s) Loss: 0.0002(0.0082) Grad: 94.7826  LR: 0.000014  \n",
      "Epoch: [2][2800/3575] Elapsed 27m 32s (remain 7m 36s) Loss: 0.0019(0.0081) Grad: 6844.4468  LR: 0.000014  \n",
      "Epoch: [2][2900/3575] Elapsed 28m 31s (remain 6m 37s) Loss: 0.0164(0.0082) Grad: 6700.8501  LR: 0.000014  \n",
      "Epoch: [2][3000/3575] Elapsed 29m 31s (remain 5m 38s) Loss: 0.0228(0.0082) Grad: 11304.9688  LR: 0.000014  \n",
      "Epoch: [2][3100/3575] Elapsed 30m 30s (remain 4m 39s) Loss: 0.0010(0.0082) Grad: 2164.1096  LR: 0.000014  \n",
      "Epoch: [2][3200/3575] Elapsed 31m 28s (remain 3m 40s) Loss: 0.0048(0.0081) Grad: 30705.2539  LR: 0.000014  \n",
      "Epoch: [2][3300/3575] Elapsed 32m 27s (remain 2m 41s) Loss: 0.0002(0.0081) Grad: 109.9473  LR: 0.000014  \n",
      "Epoch: [2][3400/3575] Elapsed 33m 26s (remain 1m 42s) Loss: 0.0024(0.0081) Grad: 3440.7256  LR: 0.000014  \n",
      "Epoch: [2][3500/3575] Elapsed 34m 25s (remain 0m 43s) Loss: 0.0022(0.0080) Grad: 6470.4502  LR: 0.000013  \n",
      "Epoch: [2][3574/3575] Elapsed 35m 9s (remain 0m 0s) Loss: 0.0188(0.0080) Grad: 15414.0762  LR: 0.000013  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 8m 24s) Loss: 0.0009(0.0009) \n",
      "EVAL: [100/1192] Elapsed 0m 19s (remain 3m 27s) Loss: 0.0105(0.0054) \n",
      "EVAL: [200/1192] Elapsed 0m 38s (remain 3m 9s) Loss: 0.0186(0.0070) \n",
      "EVAL: [300/1192] Elapsed 0m 56s (remain 2m 48s) Loss: 0.0056(0.0077) \n",
      "EVAL: [400/1192] Elapsed 1m 15s (remain 2m 28s) Loss: 0.0041(0.0077) \n",
      "EVAL: [500/1192] Elapsed 1m 34s (remain 2m 9s) Loss: 0.0181(0.0073) \n",
      "EVAL: [600/1192] Elapsed 1m 53s (remain 1m 51s) Loss: 0.0086(0.0076) \n",
      "EVAL: [700/1192] Elapsed 2m 12s (remain 1m 32s) Loss: 0.0681(0.0087) \n",
      "EVAL: [800/1192] Elapsed 2m 30s (remain 1m 13s) Loss: 0.0067(0.0087) \n",
      "EVAL: [900/1192] Elapsed 2m 49s (remain 0m 54s) Loss: 0.0033(0.0086) \n",
      "EVAL: [1000/1192] Elapsed 3m 8s (remain 0m 35s) Loss: 0.0001(0.0084) \n",
      "EVAL: [1100/1192] Elapsed 3m 27s (remain 0m 17s) Loss: 0.0024(0.0080) \n",
      "EVAL: [1191/1192] Elapsed 3m 43s (remain 0m 0s) Loss: 0.0002(0.0077) \n",
      "Epoch 2 - avg_train_loss: 0.0080  avg_val_loss: 0.0077  time: 2337s\n",
      "Epoch 2 - Score: 0.8614\n",
      "Epoch 2 - Save Best Score: 0.8614 Model\n",
      "Epoch: [3][0/3575] Elapsed 0m 0s (remain 50m 2s) Loss: 0.0002(0.0002) Grad: 1022.9323  LR: 0.000013  \n",
      "Epoch: [3][100/3575] Elapsed 0m 59s (remain 34m 12s) Loss: 0.0304(0.0063) Grad: 32288.0645  LR: 0.000013  \n",
      "Epoch: [3][200/3575] Elapsed 1m 57s (remain 32m 56s) Loss: 0.0002(0.0051) Grad: 1065.3071  LR: 0.000013  \n",
      "Epoch: [3][300/3575] Elapsed 2m 56s (remain 31m 57s) Loss: 0.0250(0.0063) Grad: 75789.7891  LR: 0.000013  \n",
      "Epoch: [3][400/3575] Elapsed 3m 55s (remain 31m 6s) Loss: 0.0023(0.0069) Grad: 12063.8516  LR: 0.000013  \n",
      "Epoch: [3][500/3575] Elapsed 4m 54s (remain 30m 8s) Loss: 0.0000(0.0068) Grad: 398.0637  LR: 0.000013  \n",
      "Epoch: [3][600/3575] Elapsed 5m 53s (remain 29m 7s) Loss: 0.0005(0.0067) Grad: 2993.8740  LR: 0.000013  \n",
      "Epoch: [3][700/3575] Elapsed 6m 51s (remain 28m 6s) Loss: 0.0001(0.0068) Grad: 45.2403  LR: 0.000012  \n",
      "Epoch: [3][800/3575] Elapsed 7m 49s (remain 27m 7s) Loss: 0.0140(0.0069) Grad: 23571.3164  LR: 0.000012  \n",
      "Epoch: [3][900/3575] Elapsed 8m 48s (remain 26m 8s) Loss: 0.0095(0.0067) Grad: 29149.2871  LR: 0.000012  \n",
      "Epoch: [3][1000/3575] Elapsed 9m 48s (remain 25m 12s) Loss: 0.0002(0.0066) Grad: 313.6035  LR: 0.000012  \n",
      "Epoch: [3][1100/3575] Elapsed 10m 47s (remain 24m 14s) Loss: 0.0001(0.0065) Grad: 205.1547  LR: 0.000012  \n",
      "Epoch: [3][1200/3575] Elapsed 11m 45s (remain 23m 14s) Loss: 0.0009(0.0063) Grad: 5526.9629  LR: 0.000012  \n",
      "Epoch: [3][1300/3575] Elapsed 12m 44s (remain 22m 16s) Loss: 0.0000(0.0066) Grad: 31.3580  LR: 0.000012  \n",
      "Epoch: [3][1400/3575] Elapsed 13m 43s (remain 21m 17s) Loss: 0.0090(0.0065) Grad: 34148.4375  LR: 0.000012  \n",
      "Epoch: [3][1500/3575] Elapsed 14m 41s (remain 20m 18s) Loss: 0.0002(0.0063) Grad: 360.0237  LR: 0.000011  \n",
      "Epoch: [3][1600/3575] Elapsed 15m 40s (remain 19m 19s) Loss: 0.0037(0.0064) Grad: 87998.4766  LR: 0.000011  \n",
      "Epoch: [3][1700/3575] Elapsed 16m 39s (remain 18m 20s) Loss: 0.0064(0.0064) Grad: 4491.7866  LR: 0.000011  \n",
      "Epoch: [3][1800/3575] Elapsed 17m 37s (remain 17m 22s) Loss: 0.0045(0.0063) Grad: 11720.8311  LR: 0.000011  \n",
      "Epoch: [3][1900/3575] Elapsed 18m 36s (remain 16m 23s) Loss: 0.0159(0.0062) Grad: 229783.8750  LR: 0.000011  \n",
      "Epoch: [3][2000/3575] Elapsed 19m 34s (remain 15m 24s) Loss: 0.0426(0.0062) Grad: 21147.0742  LR: 0.000011  \n",
      "Epoch: [3][2100/3575] Elapsed 20m 32s (remain 14m 24s) Loss: 0.0001(0.0061) Grad: 91.4763  LR: 0.000011  \n",
      "Epoch: [3][2200/3575] Elapsed 21m 31s (remain 13m 26s) Loss: 0.0055(0.0061) Grad: 11116.2129  LR: 0.000011  \n",
      "Epoch: [3][2300/3575] Elapsed 22m 30s (remain 12m 27s) Loss: 0.0041(0.0061) Grad: 16726.8594  LR: 0.000010  \n",
      "Epoch: [3][2400/3575] Elapsed 23m 29s (remain 11m 29s) Loss: 0.0018(0.0061) Grad: 9600.5596  LR: 0.000010  \n",
      "Epoch: [3][2500/3575] Elapsed 24m 28s (remain 10m 30s) Loss: 0.0001(0.0061) Grad: 199.5671  LR: 0.000010  \n",
      "Epoch: [3][2600/3575] Elapsed 25m 27s (remain 9m 32s) Loss: 0.0048(0.0061) Grad: 5450.5815  LR: 0.000010  \n",
      "Epoch: [3][2700/3575] Elapsed 26m 26s (remain 8m 33s) Loss: 0.0108(0.0060) Grad: 40197.7539  LR: 0.000010  \n",
      "Epoch: [3][2800/3575] Elapsed 27m 26s (remain 7m 34s) Loss: 0.0002(0.0061) Grad: 517.2883  LR: 0.000010  \n",
      "Epoch: [3][2900/3575] Elapsed 28m 25s (remain 6m 36s) Loss: 0.0072(0.0060) Grad: 29451.6289  LR: 0.000010  \n",
      "Epoch: [3][3000/3575] Elapsed 29m 23s (remain 5m 37s) Loss: 0.0047(0.0060) Grad: 46995.6133  LR: 0.000010  \n",
      "Epoch: [3][3100/3575] Elapsed 30m 21s (remain 4m 38s) Loss: 0.0000(0.0060) Grad: 7.4699  LR: 0.000009  \n",
      "Epoch: [3][3200/3575] Elapsed 31m 20s (remain 3m 39s) Loss: 0.0008(0.0060) Grad: 1145.2571  LR: 0.000009  \n",
      "Epoch: [3][3300/3575] Elapsed 32m 20s (remain 2m 41s) Loss: 0.0004(0.0060) Grad: 1061.7603  LR: 0.000009  \n",
      "Epoch: [3][3400/3575] Elapsed 33m 20s (remain 1m 42s) Loss: 0.0001(0.0060) Grad: 62.4606  LR: 0.000009  \n",
      "Epoch: [3][3500/3575] Elapsed 34m 20s (remain 0m 43s) Loss: 0.0019(0.0061) Grad: 7559.9756  LR: 0.000009  \n",
      "Epoch: [3][3574/3575] Elapsed 35m 3s (remain 0m 0s) Loss: 0.0003(0.0061) Grad: 430.5189  LR: 0.000009  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 7m 56s) Loss: 0.0004(0.0004) \n",
      "EVAL: [100/1192] Elapsed 0m 19s (remain 3m 26s) Loss: 0.0198(0.0049) \n",
      "EVAL: [200/1192] Elapsed 0m 38s (remain 3m 7s) Loss: 0.0106(0.0058) \n",
      "EVAL: [300/1192] Elapsed 0m 56s (remain 2m 47s) Loss: 0.0069(0.0061) \n",
      "EVAL: [400/1192] Elapsed 1m 14s (remain 2m 27s) Loss: 0.0026(0.0066) \n",
      "EVAL: [500/1192] Elapsed 1m 33s (remain 2m 9s) Loss: 0.0127(0.0062) \n",
      "EVAL: [600/1192] Elapsed 1m 52s (remain 1m 50s) Loss: 0.0102(0.0064) \n",
      "EVAL: [700/1192] Elapsed 2m 11s (remain 1m 31s) Loss: 0.0707(0.0079) \n",
      "EVAL: [800/1192] Elapsed 2m 30s (remain 1m 13s) Loss: 0.0015(0.0079) \n",
      "EVAL: [900/1192] Elapsed 2m 48s (remain 0m 54s) Loss: 0.0057(0.0079) \n",
      "EVAL: [1000/1192] Elapsed 3m 7s (remain 0m 35s) Loss: 0.0000(0.0077) \n",
      "EVAL: [1100/1192] Elapsed 3m 26s (remain 0m 17s) Loss: 0.0035(0.0074) \n",
      "EVAL: [1191/1192] Elapsed 3m 42s (remain 0m 0s) Loss: 0.0001(0.0072) \n",
      "Epoch 3 - avg_train_loss: 0.0061  avg_val_loss: 0.0072  time: 2329s\n",
      "Epoch 3 - Score: 0.8744\n",
      "Epoch 3 - Save Best Score: 0.8744 Model\n",
      "Epoch: [4][0/3575] Elapsed 0m 0s (remain 51m 50s) Loss: 0.0078(0.0078) Grad: 23077.9980  LR: 0.000009  \n",
      "Epoch: [4][100/3575] Elapsed 0m 59s (remain 34m 19s) Loss: 0.0031(0.0036) Grad: 11434.0996  LR: 0.000009  \n",
      "Epoch: [4][200/3575] Elapsed 1m 58s (remain 33m 3s) Loss: 0.0002(0.0033) Grad: 100.0441  LR: 0.000009  \n",
      "Epoch: [4][300/3575] Elapsed 2m 56s (remain 31m 56s) Loss: 0.0018(0.0038) Grad: 38019.2656  LR: 0.000009  \n",
      "Epoch: [4][400/3575] Elapsed 3m 54s (remain 30m 54s) Loss: 0.0001(0.0038) Grad: 180.2239  LR: 0.000008  \n",
      "Epoch: [4][500/3575] Elapsed 4m 53s (remain 30m 1s) Loss: 0.0094(0.0040) Grad: 3361.6272  LR: 0.000008  \n",
      "Epoch: [4][600/3575] Elapsed 5m 51s (remain 29m 0s) Loss: 0.0001(0.0043) Grad: 214.4440  LR: 0.000008  \n",
      "Epoch: [4][700/3575] Elapsed 6m 49s (remain 28m 0s) Loss: 0.0035(0.0043) Grad: 7600.2354  LR: 0.000008  \n",
      "Epoch: [4][800/3575] Elapsed 7m 49s (remain 27m 6s) Loss: 0.0001(0.0043) Grad: 102.4558  LR: 0.000008  \n",
      "Epoch: [4][900/3575] Elapsed 8m 48s (remain 26m 9s) Loss: 0.0002(0.0043) Grad: 531.7124  LR: 0.000008  \n",
      "Epoch: [4][1000/3575] Elapsed 9m 47s (remain 25m 11s) Loss: 0.0241(0.0043) Grad: 72189.0938  LR: 0.000008  \n",
      "Epoch: [4][1100/3575] Elapsed 10m 46s (remain 24m 13s) Loss: 0.0008(0.0044) Grad: 4009.8159  LR: 0.000008  \n",
      "Epoch: [4][1200/3575] Elapsed 11m 45s (remain 23m 14s) Loss: 0.0250(0.0044) Grad: 10644.9062  LR: 0.000007  \n",
      "Epoch: [4][1300/3575] Elapsed 12m 45s (remain 22m 17s) Loss: 0.0004(0.0045) Grad: 1258.3630  LR: 0.000007  \n",
      "Epoch: [4][1400/3575] Elapsed 13m 45s (remain 21m 20s) Loss: 0.0002(0.0046) Grad: 614.7794  LR: 0.000007  \n",
      "Epoch: [4][1500/3575] Elapsed 14m 43s (remain 20m 21s) Loss: 0.0002(0.0047) Grad: 462.8505  LR: 0.000007  \n",
      "Epoch: [4][1600/3575] Elapsed 15m 42s (remain 19m 22s) Loss: 0.0001(0.0047) Grad: 45.7534  LR: 0.000007  \n",
      "Epoch: [4][1700/3575] Elapsed 16m 41s (remain 18m 23s) Loss: 0.0699(0.0048) Grad: 78571.7578  LR: 0.000007  \n",
      "Epoch: [4][1800/3575] Elapsed 17m 42s (remain 17m 26s) Loss: 0.0055(0.0048) Grad: 14370.0068  LR: 0.000007  \n",
      "Epoch: [4][1900/3575] Elapsed 18m 41s (remain 16m 27s) Loss: 0.0133(0.0048) Grad: 89217.0781  LR: 0.000007  \n",
      "Epoch: [4][2000/3575] Elapsed 19m 39s (remain 15m 28s) Loss: 0.0233(0.0048) Grad: 84847.2812  LR: 0.000006  \n",
      "Epoch: [4][2100/3575] Elapsed 20m 39s (remain 14m 29s) Loss: 0.0001(0.0047) Grad: 1203.2239  LR: 0.000006  \n",
      "Epoch: [4][2200/3575] Elapsed 21m 38s (remain 13m 30s) Loss: 0.0063(0.0048) Grad: 23511.5371  LR: 0.000006  \n",
      "Epoch: [4][2300/3575] Elapsed 22m 36s (remain 12m 31s) Loss: 0.0367(0.0049) Grad: 62274.1328  LR: 0.000006  \n",
      "Epoch: [4][2400/3575] Elapsed 23m 35s (remain 11m 32s) Loss: 0.0133(0.0049) Grad: 21025.4102  LR: 0.000006  \n",
      "Epoch: [4][2500/3575] Elapsed 24m 34s (remain 10m 33s) Loss: 0.0011(0.0049) Grad: 19745.1562  LR: 0.000006  \n",
      "Epoch: [4][2600/3575] Elapsed 25m 32s (remain 9m 34s) Loss: 0.0003(0.0049) Grad: 695.2405  LR: 0.000006  \n",
      "Epoch: [4][2700/3575] Elapsed 26m 31s (remain 8m 34s) Loss: 0.0000(0.0049) Grad: 36.2328  LR: 0.000006  \n",
      "Epoch: [4][2800/3575] Elapsed 27m 30s (remain 7m 36s) Loss: 0.0001(0.0049) Grad: 45.8029  LR: 0.000005  \n",
      "Epoch: [4][2900/3575] Elapsed 28m 29s (remain 6m 37s) Loss: 0.0038(0.0048) Grad: 31196.8008  LR: 0.000005  \n",
      "Epoch: [4][3000/3575] Elapsed 29m 28s (remain 5m 38s) Loss: 0.0000(0.0048) Grad: 29.9588  LR: 0.000005  \n",
      "Epoch: [4][3100/3575] Elapsed 30m 26s (remain 4m 39s) Loss: 0.0185(0.0048) Grad: 31400.3691  LR: 0.000005  \n",
      "Epoch: [4][3200/3575] Elapsed 31m 25s (remain 3m 40s) Loss: 0.0015(0.0048) Grad: 9175.8115  LR: 0.000005  \n",
      "Epoch: [4][3300/3575] Elapsed 32m 24s (remain 2m 41s) Loss: 0.0011(0.0048) Grad: 4136.0835  LR: 0.000005  \n",
      "Epoch: [4][3400/3575] Elapsed 33m 24s (remain 1m 42s) Loss: 0.0072(0.0048) Grad: 1686.0154  LR: 0.000005  \n",
      "Epoch: [4][3500/3575] Elapsed 34m 23s (remain 0m 43s) Loss: 0.0006(0.0048) Grad: 5609.7661  LR: 0.000005  \n",
      "Epoch: [4][3574/3575] Elapsed 35m 7s (remain 0m 0s) Loss: 0.0186(0.0048) Grad: 37633.2695  LR: 0.000004  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 8m 8s) Loss: 0.0001(0.0001) \n",
      "EVAL: [100/1192] Elapsed 0m 19s (remain 3m 31s) Loss: 0.0124(0.0062) \n",
      "EVAL: [200/1192] Elapsed 0m 38s (remain 3m 8s) Loss: 0.0129(0.0072) \n",
      "EVAL: [300/1192] Elapsed 0m 57s (remain 2m 51s) Loss: 0.0081(0.0072) \n",
      "EVAL: [400/1192] Elapsed 1m 16s (remain 2m 30s) Loss: 0.0044(0.0075) \n",
      "EVAL: [500/1192] Elapsed 1m 34s (remain 2m 10s) Loss: 0.0139(0.0072) \n",
      "EVAL: [600/1192] Elapsed 1m 54s (remain 1m 52s) Loss: 0.0068(0.0073) \n",
      "EVAL: [700/1192] Elapsed 2m 13s (remain 1m 33s) Loss: 0.0915(0.0088) \n",
      "EVAL: [800/1192] Elapsed 2m 32s (remain 1m 14s) Loss: 0.0024(0.0089) \n",
      "EVAL: [900/1192] Elapsed 2m 50s (remain 0m 55s) Loss: 0.0155(0.0088) \n",
      "EVAL: [1000/1192] Elapsed 3m 9s (remain 0m 36s) Loss: 0.0000(0.0086) \n",
      "EVAL: [1100/1192] Elapsed 3m 28s (remain 0m 17s) Loss: 0.0006(0.0082) \n",
      "EVAL: [1191/1192] Elapsed 3m 45s (remain 0m 0s) Loss: 0.0000(0.0079) \n",
      "Epoch 4 - avg_train_loss: 0.0048  avg_val_loss: 0.0079  time: 2336s\n",
      "Epoch 4 - Score: 0.8751\n",
      "Epoch 4 - Save Best Score: 0.8751 Model\n",
      "Epoch: [5][0/3575] Elapsed 0m 0s (remain 51m 14s) Loss: 0.0009(0.0009) Grad: 1544.8900  LR: 0.000004  \n",
      "Epoch: [5][100/3575] Elapsed 0m 59s (remain 34m 18s) Loss: 0.0000(0.0047) Grad: 16.9374  LR: 0.000004  \n",
      "Epoch: [5][200/3575] Elapsed 1m 57s (remain 32m 59s) Loss: 0.0242(0.0034) Grad: 110888.6094  LR: 0.000004  \n",
      "Epoch: [5][300/3575] Elapsed 2m 56s (remain 31m 55s) Loss: 0.0000(0.0036) Grad: 33.0361  LR: 0.000004  \n",
      "Epoch: [5][400/3575] Elapsed 3m 55s (remain 31m 0s) Loss: 0.0210(0.0041) Grad: 15503.1113  LR: 0.000004  \n",
      "Epoch: [5][500/3575] Elapsed 4m 53s (remain 30m 0s) Loss: 0.0104(0.0044) Grad: 52157.4023  LR: 0.000004  \n",
      "Epoch: [5][600/3575] Elapsed 5m 51s (remain 28m 59s) Loss: 0.0000(0.0043) Grad: 18.5259  LR: 0.000004  \n",
      "Epoch: [5][700/3575] Elapsed 6m 49s (remain 28m 0s) Loss: 0.0010(0.0041) Grad: 545.9407  LR: 0.000004  \n",
      "Epoch: [5][800/3575] Elapsed 7m 48s (remain 27m 3s) Loss: 0.0032(0.0040) Grad: 33579.2734  LR: 0.000003  \n",
      "Epoch: [5][900/3575] Elapsed 8m 47s (remain 26m 4s) Loss: 0.0004(0.0041) Grad: 2728.2639  LR: 0.000003  \n",
      "Epoch: [5][1000/3575] Elapsed 9m 45s (remain 25m 6s) Loss: 0.0286(0.0041) Grad: 72135.9375  LR: 0.000003  \n",
      "Epoch: [5][1100/3575] Elapsed 10m 44s (remain 24m 8s) Loss: 0.0001(0.0040) Grad: 75.0357  LR: 0.000003  \n",
      "Epoch: [5][1200/3575] Elapsed 11m 44s (remain 23m 11s) Loss: 0.0097(0.0041) Grad: 40268.3438  LR: 0.000003  \n",
      "Epoch: [5][1300/3575] Elapsed 12m 42s (remain 22m 13s) Loss: 0.0000(0.0042) Grad: 59.3685  LR: 0.000003  \n",
      "Epoch: [5][1400/3575] Elapsed 13m 40s (remain 21m 13s) Loss: 0.0001(0.0041) Grad: 64.3831  LR: 0.000003  \n",
      "Epoch: [5][1500/3575] Elapsed 14m 40s (remain 20m 16s) Loss: 0.0002(0.0041) Grad: 2226.4614  LR: 0.000003  \n",
      "Epoch: [5][1600/3575] Elapsed 15m 39s (remain 19m 17s) Loss: 0.0001(0.0040) Grad: 36.3191  LR: 0.000002  \n",
      "Epoch: [5][1700/3575] Elapsed 16m 38s (remain 18m 19s) Loss: 0.0000(0.0041) Grad: 18.7486  LR: 0.000002  \n",
      "Epoch: [5][1800/3575] Elapsed 17m 36s (remain 17m 20s) Loss: 0.0001(0.0042) Grad: 58.9291  LR: 0.000002  \n",
      "Epoch: [5][1900/3575] Elapsed 18m 36s (remain 16m 22s) Loss: 0.0009(0.0042) Grad: 7317.1274  LR: 0.000002  \n",
      "Epoch: [5][2000/3575] Elapsed 19m 35s (remain 15m 24s) Loss: 0.0025(0.0041) Grad: 24023.5508  LR: 0.000002  \n",
      "Epoch: [5][2100/3575] Elapsed 20m 33s (remain 14m 25s) Loss: 0.0133(0.0041) Grad: 103302.3516  LR: 0.000002  \n",
      "Epoch: [5][2200/3575] Elapsed 21m 32s (remain 13m 27s) Loss: 0.0034(0.0041) Grad: 47635.3242  LR: 0.000002  \n",
      "Epoch: [5][2300/3575] Elapsed 22m 30s (remain 12m 27s) Loss: 0.0001(0.0041) Grad: 81.0917  LR: 0.000002  \n",
      "Epoch: [5][2400/3575] Elapsed 23m 29s (remain 11m 29s) Loss: 0.0013(0.0040) Grad: 1771.8654  LR: 0.000001  \n",
      "Epoch: [5][2500/3575] Elapsed 24m 28s (remain 10m 30s) Loss: 0.0000(0.0040) Grad: 18.9461  LR: 0.000001  \n",
      "Epoch: [5][2600/3575] Elapsed 25m 27s (remain 9m 32s) Loss: 0.0001(0.0040) Grad: 62.1812  LR: 0.000001  \n",
      "Epoch: [5][2700/3575] Elapsed 26m 26s (remain 8m 33s) Loss: 0.0000(0.0040) Grad: 35.6186  LR: 0.000001  \n",
      "Epoch: [5][2800/3575] Elapsed 27m 24s (remain 7m 34s) Loss: 0.0153(0.0040) Grad: 55936.3242  LR: 0.000001  \n",
      "Epoch: [5][2900/3575] Elapsed 28m 23s (remain 6m 35s) Loss: 0.0000(0.0039) Grad: 10.4489  LR: 0.000001  \n",
      "Epoch: [5][3000/3575] Elapsed 29m 23s (remain 5m 37s) Loss: 0.0078(0.0039) Grad: 20359.4570  LR: 0.000001  \n",
      "Epoch: [5][3100/3575] Elapsed 30m 21s (remain 4m 38s) Loss: 0.0000(0.0039) Grad: 27.1343  LR: 0.000001  \n",
      "Epoch: [5][3200/3575] Elapsed 31m 19s (remain 3m 39s) Loss: 0.0102(0.0039) Grad: 19546.3125  LR: 0.000000  \n",
      "Epoch: [5][3300/3575] Elapsed 32m 18s (remain 2m 40s) Loss: 0.0051(0.0039) Grad: 11564.9385  LR: 0.000000  \n",
      "Epoch: [5][3400/3575] Elapsed 33m 16s (remain 1m 42s) Loss: 0.0042(0.0039) Grad: 15812.9951  LR: 0.000000  \n",
      "Epoch: [5][3500/3575] Elapsed 34m 14s (remain 0m 43s) Loss: 0.0001(0.0039) Grad: 309.8990  LR: 0.000000  \n",
      "Epoch: [5][3574/3575] Elapsed 34m 58s (remain 0m 0s) Loss: 0.0015(0.0038) Grad: 15705.8252  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 8m 36s) Loss: 0.0001(0.0001) \n",
      "EVAL: [100/1192] Elapsed 0m 19s (remain 3m 28s) Loss: 0.0152(0.0066) \n",
      "EVAL: [200/1192] Elapsed 0m 38s (remain 3m 7s) Loss: 0.0170(0.0081) \n",
      "EVAL: [300/1192] Elapsed 0m 57s (remain 2m 48s) Loss: 0.0084(0.0083) \n",
      "EVAL: [400/1192] Elapsed 1m 16s (remain 2m 29s) Loss: 0.0055(0.0089) \n",
      "EVAL: [500/1192] Elapsed 1m 34s (remain 2m 10s) Loss: 0.0144(0.0084) \n",
      "EVAL: [600/1192] Elapsed 1m 53s (remain 1m 51s) Loss: 0.0048(0.0086) \n",
      "EVAL: [700/1192] Elapsed 2m 13s (remain 1m 33s) Loss: 0.1052(0.0104) \n",
      "EVAL: [800/1192] Elapsed 2m 31s (remain 1m 14s) Loss: 0.0003(0.0106) \n",
      "EVAL: [900/1192] Elapsed 2m 50s (remain 0m 54s) Loss: 0.0112(0.0105) \n",
      "EVAL: [1000/1192] Elapsed 3m 8s (remain 0m 36s) Loss: 0.0000(0.0103) \n",
      "EVAL: [1100/1192] Elapsed 3m 27s (remain 0m 17s) Loss: 0.0005(0.0098) \n",
      "EVAL: [1191/1192] Elapsed 3m 44s (remain 0m 0s) Loss: 0.0000(0.0095) \n",
      "Epoch 5 - avg_train_loss: 0.0038  avg_val_loss: 0.0095  time: 2326s\n",
      "Epoch 5 - Score: 0.8779\n",
      "Epoch 5 - Save Best Score: 0.8779 Model\n",
      "========== fold: 1 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/3575] Elapsed 0m 0s (remain 48m 44s) Loss: 0.3569(0.3569) Grad: 38018.6133  LR: 0.000000  \n",
      "Epoch: [1][100/3575] Elapsed 1m 0s (remain 34m 32s) Loss: 0.3458(0.3526) Grad: 36850.1797  LR: 0.000001  \n",
      "Epoch: [1][200/3575] Elapsed 1m 59s (remain 33m 33s) Loss: 0.3084(0.3420) Grad: 37662.6836  LR: 0.000002  \n",
      "Epoch: [1][300/3575] Elapsed 2m 58s (remain 32m 19s) Loss: 0.2484(0.3229) Grad: 38989.0742  LR: 0.000003  \n",
      "Epoch: [1][400/3575] Elapsed 3m 57s (remain 31m 17s) Loss: 0.1348(0.2918) Grad: 16929.5098  LR: 0.000004  \n",
      "Epoch: [1][500/3575] Elapsed 4m 56s (remain 30m 16s) Loss: 0.0581(0.2479) Grad: 753.9301  LR: 0.000006  \n",
      "Epoch: [1][600/3575] Elapsed 5m 54s (remain 29m 15s) Loss: 0.0400(0.2144) Grad: 759.0989  LR: 0.000007  \n",
      "Epoch: [1][700/3575] Elapsed 6m 53s (remain 28m 15s) Loss: 0.1178(0.1897) Grad: 23605.2871  LR: 0.000008  \n",
      "Epoch: [1][800/3575] Elapsed 7m 53s (remain 27m 19s) Loss: 0.0406(0.1708) Grad: 10276.9619  LR: 0.000009  \n",
      "Epoch: [1][900/3575] Elapsed 8m 52s (remain 26m 20s) Loss: 0.0127(0.1547) Grad: 30938.0996  LR: 0.000010  \n",
      "Epoch: [1][1000/3575] Elapsed 9m 50s (remain 25m 19s) Loss: 0.0878(0.1410) Grad: 54474.8672  LR: 0.000011  \n",
      "Epoch: [1][1100/3575] Elapsed 10m 50s (remain 24m 20s) Loss: 0.0112(0.1302) Grad: 13656.8574  LR: 0.000012  \n",
      "Epoch: [1][1200/3575] Elapsed 11m 49s (remain 23m 22s) Loss: 0.0026(0.1209) Grad: 479.7081  LR: 0.000013  \n",
      "Epoch: [1][1300/3575] Elapsed 12m 47s (remain 22m 22s) Loss: 0.0045(0.1132) Grad: 1970.4227  LR: 0.000015  \n",
      "Epoch: [1][1400/3575] Elapsed 13m 47s (remain 21m 24s) Loss: 0.0136(0.1065) Grad: 20358.6953  LR: 0.000016  \n",
      "Epoch: [1][1500/3575] Elapsed 14m 47s (remain 20m 26s) Loss: 0.0057(0.1004) Grad: 6036.1221  LR: 0.000017  \n",
      "Epoch: [1][1600/3575] Elapsed 15m 46s (remain 19m 26s) Loss: 0.0095(0.0951) Grad: 17677.7676  LR: 0.000018  \n",
      "Epoch: [1][1700/3575] Elapsed 16m 44s (remain 18m 26s) Loss: 0.0098(0.0904) Grad: 13558.1641  LR: 0.000019  \n",
      "Epoch: [1][1800/3575] Elapsed 17m 45s (remain 17m 29s) Loss: 0.0015(0.0862) Grad: 2461.5984  LR: 0.000020  \n",
      "Epoch: [1][1900/3575] Elapsed 18m 45s (remain 16m 30s) Loss: 0.0059(0.0824) Grad: 7342.0977  LR: 0.000020  \n",
      "Epoch: [1][2000/3575] Elapsed 19m 43s (remain 15m 31s) Loss: 0.0062(0.0790) Grad: 5248.1396  LR: 0.000020  \n",
      "Epoch: [1][2100/3575] Elapsed 20m 42s (remain 14m 32s) Loss: 0.0023(0.0758) Grad: 3387.3083  LR: 0.000020  \n",
      "Epoch: [1][2200/3575] Elapsed 21m 42s (remain 13m 32s) Loss: 0.0648(0.0729) Grad: 27433.2734  LR: 0.000019  \n",
      "Epoch: [1][2300/3575] Elapsed 22m 41s (remain 12m 33s) Loss: 0.0026(0.0703) Grad: 11774.4561  LR: 0.000019  \n",
      "Epoch: [1][2400/3575] Elapsed 23m 39s (remain 11m 34s) Loss: 0.0080(0.0678) Grad: 5854.9985  LR: 0.000019  \n",
      "Epoch: [1][2500/3575] Elapsed 24m 39s (remain 10m 35s) Loss: 0.0022(0.0656) Grad: 2617.6118  LR: 0.000019  \n",
      "Epoch: [1][2600/3575] Elapsed 25m 38s (remain 9m 36s) Loss: 0.0192(0.0635) Grad: 32926.9648  LR: 0.000019  \n",
      "Epoch: [1][2700/3575] Elapsed 26m 37s (remain 8m 36s) Loss: 0.0050(0.0615) Grad: 12458.8682  LR: 0.000019  \n",
      "Epoch: [1][2800/3575] Elapsed 27m 36s (remain 7m 37s) Loss: 0.0953(0.0596) Grad: 30370.2227  LR: 0.000019  \n",
      "Epoch: [1][2900/3575] Elapsed 28m 34s (remain 6m 38s) Loss: 0.0336(0.0580) Grad: 41952.5781  LR: 0.000019  \n",
      "Epoch: [1][3000/3575] Elapsed 29m 33s (remain 5m 39s) Loss: 0.0052(0.0564) Grad: 21461.7812  LR: 0.000018  \n",
      "Epoch: [1][3100/3575] Elapsed 30m 32s (remain 4m 40s) Loss: 0.0006(0.0548) Grad: 302.8152  LR: 0.000018  \n",
      "Epoch: [1][3200/3575] Elapsed 31m 30s (remain 3m 40s) Loss: 0.0157(0.0534) Grad: 20586.6465  LR: 0.000018  \n",
      "Epoch: [1][3300/3575] Elapsed 32m 29s (remain 2m 41s) Loss: 0.0023(0.0521) Grad: 4140.8032  LR: 0.000018  \n",
      "Epoch: [1][3400/3575] Elapsed 33m 28s (remain 1m 42s) Loss: 0.0012(0.0508) Grad: 733.5819  LR: 0.000018  \n",
      "Epoch: [1][3500/3575] Elapsed 34m 27s (remain 0m 43s) Loss: 0.0258(0.0496) Grad: 76332.2812  LR: 0.000018  \n",
      "Epoch: [1][3574/3575] Elapsed 35m 11s (remain 0m 0s) Loss: 0.0079(0.0488) Grad: 3729.6587  LR: 0.000018  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 9m 9s) Loss: 0.0010(0.0010) \n",
      "EVAL: [100/1192] Elapsed 0m 19s (remain 3m 27s) Loss: 0.0011(0.0068) \n",
      "EVAL: [200/1192] Elapsed 0m 37s (remain 3m 5s) Loss: 0.0017(0.0085) \n",
      "EVAL: [300/1192] Elapsed 0m 56s (remain 2m 47s) Loss: 0.0095(0.0112) \n",
      "EVAL: [400/1192] Elapsed 1m 15s (remain 2m 28s) Loss: 0.0149(0.0109) \n",
      "EVAL: [500/1192] Elapsed 1m 33s (remain 2m 9s) Loss: 0.0257(0.0101) \n",
      "EVAL: [600/1192] Elapsed 1m 53s (remain 1m 51s) Loss: 0.1294(0.0110) \n",
      "EVAL: [700/1192] Elapsed 2m 12s (remain 1m 32s) Loss: 0.0028(0.0121) \n",
      "EVAL: [800/1192] Elapsed 2m 31s (remain 1m 13s) Loss: 0.0058(0.0118) \n",
      "EVAL: [900/1192] Elapsed 2m 50s (remain 0m 54s) Loss: 0.0071(0.0115) \n",
      "EVAL: [1000/1192] Elapsed 3m 8s (remain 0m 36s) Loss: 0.0007(0.0112) \n",
      "EVAL: [1100/1192] Elapsed 3m 28s (remain 0m 17s) Loss: 0.0027(0.0108) \n",
      "EVAL: [1191/1192] Elapsed 3m 45s (remain 0m 0s) Loss: 0.0041(0.0103) \n",
      "Epoch 1 - avg_train_loss: 0.0488  avg_val_loss: 0.0103  time: 2340s\n",
      "Epoch 1 - Score: 0.8334\n",
      "Epoch 1 - Save Best Score: 0.8334 Model\n",
      "Epoch: [2][0/3575] Elapsed 0m 0s (remain 52m 35s) Loss: 0.0016(0.0016) Grad: 10066.7588  LR: 0.000018  \n",
      "Epoch: [2][100/3575] Elapsed 0m 59s (remain 34m 16s) Loss: 0.0001(0.0076) Grad: 881.6412  LR: 0.000018  \n",
      "Epoch: [2][200/3575] Elapsed 1m 58s (remain 33m 12s) Loss: 0.0006(0.0072) Grad: 638.1073  LR: 0.000018  \n",
      "Epoch: [2][300/3575] Elapsed 3m 0s (remain 32m 40s) Loss: 0.0096(0.0074) Grad: 15006.4170  LR: 0.000017  \n",
      "Epoch: [2][400/3575] Elapsed 4m 0s (remain 31m 46s) Loss: 0.0005(0.0083) Grad: 366.7387  LR: 0.000017  \n",
      "Epoch: [2][500/3575] Elapsed 5m 0s (remain 30m 43s) Loss: 0.0348(0.0082) Grad: 62611.3633  LR: 0.000017  \n",
      "Epoch: [2][600/3575] Elapsed 5m 58s (remain 29m 36s) Loss: 0.0321(0.0081) Grad: 45620.5820  LR: 0.000017  \n",
      "Epoch: [2][700/3575] Elapsed 6m 57s (remain 28m 30s) Loss: 0.0463(0.0079) Grad: 89355.7969  LR: 0.000017  \n",
      "Epoch: [2][800/3575] Elapsed 7m 55s (remain 27m 26s) Loss: 0.0050(0.0082) Grad: 13153.3701  LR: 0.000017  \n",
      "Epoch: [2][900/3575] Elapsed 8m 53s (remain 26m 23s) Loss: 0.0052(0.0082) Grad: 11999.8379  LR: 0.000017  \n",
      "Epoch: [2][1000/3575] Elapsed 9m 55s (remain 25m 32s) Loss: 0.0039(0.0081) Grad: 6797.7173  LR: 0.000017  \n",
      "Epoch: [2][1100/3575] Elapsed 10m 55s (remain 24m 32s) Loss: 0.0066(0.0080) Grad: 17797.0254  LR: 0.000016  \n",
      "Epoch: [2][1200/3575] Elapsed 11m 54s (remain 23m 31s) Loss: 0.0002(0.0078) Grad: 91.0197  LR: 0.000016  \n",
      "Epoch: [2][1300/3575] Elapsed 12m 52s (remain 22m 29s) Loss: 0.0003(0.0078) Grad: 162.5880  LR: 0.000016  \n",
      "Epoch: [2][1400/3575] Elapsed 13m 50s (remain 21m 29s) Loss: 0.0102(0.0078) Grad: 13359.4092  LR: 0.000016  \n",
      "Epoch: [2][1500/3575] Elapsed 14m 47s (remain 20m 26s) Loss: 0.0000(0.0076) Grad: 13.7054  LR: 0.000016  \n",
      "Epoch: [2][1600/3575] Elapsed 15m 45s (remain 19m 26s) Loss: 0.0374(0.0077) Grad: 35772.1758  LR: 0.000016  \n",
      "Epoch: [2][1700/3575] Elapsed 16m 44s (remain 18m 26s) Loss: 0.0015(0.0076) Grad: 7695.3589  LR: 0.000016  \n",
      "Epoch: [2][1800/3575] Elapsed 17m 44s (remain 17m 28s) Loss: 0.0061(0.0075) Grad: 22484.1172  LR: 0.000016  \n",
      "Epoch: [2][1900/3575] Elapsed 18m 47s (remain 16m 32s) Loss: 0.0011(0.0076) Grad: 7604.5317  LR: 0.000015  \n",
      "Epoch: [2][2000/3575] Elapsed 19m 46s (remain 15m 33s) Loss: 0.0023(0.0075) Grad: 5762.4243  LR: 0.000015  \n",
      "Epoch: [2][2100/3575] Elapsed 20m 44s (remain 14m 33s) Loss: 0.0052(0.0075) Grad: 19895.5430  LR: 0.000015  \n",
      "Epoch: [2][2200/3575] Elapsed 21m 42s (remain 13m 33s) Loss: 0.0045(0.0075) Grad: 9617.5781  LR: 0.000015  \n",
      "Epoch: [2][2300/3575] Elapsed 22m 41s (remain 12m 33s) Loss: 0.0134(0.0075) Grad: 22974.3281  LR: 0.000015  \n",
      "Epoch: [2][2400/3575] Elapsed 23m 39s (remain 11m 34s) Loss: 0.0052(0.0075) Grad: 36387.7812  LR: 0.000015  \n",
      "Epoch: [2][2500/3575] Elapsed 24m 40s (remain 10m 35s) Loss: 0.0141(0.0075) Grad: 27199.0273  LR: 0.000015  \n",
      "Epoch: [2][2600/3575] Elapsed 25m 39s (remain 9m 36s) Loss: 0.0267(0.0075) Grad: 38287.5898  LR: 0.000015  \n",
      "Epoch: [2][2700/3575] Elapsed 26m 37s (remain 8m 36s) Loss: 0.0014(0.0075) Grad: 8493.7383  LR: 0.000014  \n",
      "Epoch: [2][2800/3575] Elapsed 27m 36s (remain 7m 37s) Loss: 0.0002(0.0075) Grad: 142.0762  LR: 0.000014  \n",
      "Epoch: [2][2900/3575] Elapsed 28m 37s (remain 6m 39s) Loss: 0.0006(0.0075) Grad: 804.4358  LR: 0.000014  \n",
      "Epoch: [2][3000/3575] Elapsed 29m 36s (remain 5m 39s) Loss: 0.0179(0.0075) Grad: 14746.3115  LR: 0.000014  \n",
      "Epoch: [2][3100/3575] Elapsed 30m 35s (remain 4m 40s) Loss: 0.0014(0.0074) Grad: 2564.1328  LR: 0.000014  \n",
      "Epoch: [2][3200/3575] Elapsed 31m 35s (remain 3m 41s) Loss: 0.0592(0.0075) Grad: 28614.6113  LR: 0.000014  \n",
      "Epoch: [2][3300/3575] Elapsed 32m 35s (remain 2m 42s) Loss: 0.0017(0.0074) Grad: 3970.3286  LR: 0.000014  \n",
      "Epoch: [2][3400/3575] Elapsed 33m 33s (remain 1m 43s) Loss: 0.0049(0.0075) Grad: 4072.5076  LR: 0.000014  \n",
      "Epoch: [2][3500/3575] Elapsed 34m 32s (remain 0m 43s) Loss: 0.0099(0.0075) Grad: 8222.4336  LR: 0.000013  \n",
      "Epoch: [2][3574/3575] Elapsed 35m 15s (remain 0m 0s) Loss: 0.0249(0.0075) Grad: 18422.1621  LR: 0.000013  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 9m 55s) Loss: 0.0003(0.0003) \n",
      "EVAL: [100/1192] Elapsed 0m 19s (remain 3m 34s) Loss: 0.0003(0.0053) \n",
      "EVAL: [200/1192] Elapsed 0m 39s (remain 3m 16s) Loss: 0.0003(0.0067) \n",
      "EVAL: [300/1192] Elapsed 0m 58s (remain 2m 54s) Loss: 0.0059(0.0104) \n",
      "EVAL: [400/1192] Elapsed 1m 17s (remain 2m 32s) Loss: 0.0226(0.0102) \n",
      "EVAL: [500/1192] Elapsed 1m 35s (remain 2m 12s) Loss: 0.0278(0.0094) \n",
      "EVAL: [600/1192] Elapsed 1m 54s (remain 1m 52s) Loss: 0.1203(0.0096) \n",
      "EVAL: [700/1192] Elapsed 2m 14s (remain 1m 34s) Loss: 0.0044(0.0107) \n",
      "EVAL: [800/1192] Elapsed 2m 33s (remain 1m 14s) Loss: 0.0029(0.0103) \n",
      "EVAL: [900/1192] Elapsed 2m 51s (remain 0m 55s) Loss: 0.0039(0.0100) \n",
      "EVAL: [1000/1192] Elapsed 3m 10s (remain 0m 36s) Loss: 0.0002(0.0098) \n",
      "EVAL: [1100/1192] Elapsed 3m 30s (remain 0m 17s) Loss: 0.0038(0.0094) \n",
      "EVAL: [1191/1192] Elapsed 3m 50s (remain 0m 0s) Loss: 0.0060(0.0089) \n",
      "Epoch 2 - avg_train_loss: 0.0075  avg_val_loss: 0.0089  time: 2349s\n",
      "Epoch 2 - Score: 0.8629\n",
      "Epoch 2 - Save Best Score: 0.8629 Model\n",
      "Epoch: [3][0/3575] Elapsed 0m 0s (remain 53m 28s) Loss: 0.0009(0.0009) Grad: 3331.2837  LR: 0.000013  \n",
      "Epoch: [3][100/3575] Elapsed 1m 0s (remain 34m 38s) Loss: 0.0070(0.0069) Grad: 2328.0339  LR: 0.000013  \n",
      "Epoch: [3][200/3575] Elapsed 2m 0s (remain 33m 50s) Loss: 0.0001(0.0057) Grad: 50.3208  LR: 0.000013  \n",
      "Epoch: [3][300/3575] Elapsed 2m 59s (remain 32m 35s) Loss: 0.0080(0.0057) Grad: 6066.1040  LR: 0.000013  \n",
      "Epoch: [3][400/3575] Elapsed 3m 59s (remain 31m 32s) Loss: 0.0001(0.0055) Grad: 60.4595  LR: 0.000013  \n",
      "Epoch: [3][500/3575] Elapsed 4m 57s (remain 30m 25s) Loss: 0.0025(0.0056) Grad: 8367.3457  LR: 0.000013  \n",
      "Epoch: [3][600/3575] Elapsed 5m 55s (remain 29m 21s) Loss: 0.0011(0.0057) Grad: 5985.3057  LR: 0.000013  \n",
      "Epoch: [3][700/3575] Elapsed 6m 54s (remain 28m 18s) Loss: 0.0002(0.0057) Grad: 89.2835  LR: 0.000012  \n",
      "Epoch: [3][800/3575] Elapsed 7m 53s (remain 27m 21s) Loss: 0.0003(0.0057) Grad: 833.4948  LR: 0.000012  \n",
      "Epoch: [3][900/3575] Elapsed 8m 54s (remain 26m 26s) Loss: 0.0001(0.0060) Grad: 58.5543  LR: 0.000012  \n",
      "Epoch: [3][1000/3575] Elapsed 9m 54s (remain 25m 27s) Loss: 0.0320(0.0059) Grad: 33295.8203  LR: 0.000012  \n",
      "Epoch: [3][1100/3575] Elapsed 10m 52s (remain 24m 27s) Loss: 0.0433(0.0059) Grad: 64065.1445  LR: 0.000012  \n",
      "Epoch: [3][1200/3575] Elapsed 11m 51s (remain 23m 27s) Loss: 0.0001(0.0060) Grad: 94.4408  LR: 0.000012  \n",
      "Epoch: [3][1300/3575] Elapsed 12m 50s (remain 22m 26s) Loss: 0.0013(0.0060) Grad: 10851.7051  LR: 0.000012  \n",
      "Epoch: [3][1400/3575] Elapsed 13m 49s (remain 21m 27s) Loss: 0.0016(0.0059) Grad: 3177.0981  LR: 0.000012  \n",
      "Epoch: [3][1500/3575] Elapsed 14m 49s (remain 20m 28s) Loss: 0.0002(0.0059) Grad: 740.1614  LR: 0.000011  \n",
      "Epoch: [3][1600/3575] Elapsed 15m 48s (remain 19m 28s) Loss: 0.0002(0.0059) Grad: 5332.2886  LR: 0.000011  \n",
      "Epoch: [3][1700/3575] Elapsed 16m 46s (remain 18m 28s) Loss: 0.0287(0.0059) Grad: 59479.2930  LR: 0.000011  \n",
      "Epoch: [3][1800/3575] Elapsed 17m 45s (remain 17m 29s) Loss: 0.0080(0.0058) Grad: 11117.6035  LR: 0.000011  \n",
      "Epoch: [3][1900/3575] Elapsed 18m 44s (remain 16m 30s) Loss: 0.0061(0.0058) Grad: 15817.8066  LR: 0.000011  \n",
      "Epoch: [3][2000/3575] Elapsed 19m 45s (remain 15m 32s) Loss: 0.0002(0.0058) Grad: 426.2142  LR: 0.000011  \n",
      "Epoch: [3][2100/3575] Elapsed 20m 43s (remain 14m 32s) Loss: 0.0170(0.0058) Grad: 10461.2852  LR: 0.000011  \n",
      "Epoch: [3][2200/3575] Elapsed 21m 42s (remain 13m 33s) Loss: 0.0021(0.0058) Grad: 5905.4170  LR: 0.000011  \n",
      "Epoch: [3][2300/3575] Elapsed 22m 42s (remain 12m 34s) Loss: 0.0052(0.0058) Grad: 13005.4922  LR: 0.000010  \n",
      "Epoch: [3][2400/3575] Elapsed 23m 43s (remain 11m 36s) Loss: 0.0001(0.0058) Grad: 48.8486  LR: 0.000010  \n",
      "Epoch: [3][2500/3575] Elapsed 24m 42s (remain 10m 36s) Loss: 0.0011(0.0058) Grad: 10098.0791  LR: 0.000010  \n",
      "Epoch: [3][2600/3575] Elapsed 25m 41s (remain 9m 37s) Loss: 0.0036(0.0058) Grad: 13956.9932  LR: 0.000010  \n",
      "Epoch: [3][2700/3575] Elapsed 26m 40s (remain 8m 37s) Loss: 0.0044(0.0057) Grad: 19847.1504  LR: 0.000010  \n",
      "Epoch: [3][2800/3575] Elapsed 27m 40s (remain 7m 38s) Loss: 0.0003(0.0058) Grad: 646.2342  LR: 0.000010  \n",
      "Epoch: [3][2900/3575] Elapsed 28m 42s (remain 6m 40s) Loss: 0.0006(0.0057) Grad: 13966.4951  LR: 0.000010  \n",
      "Epoch: [3][3000/3575] Elapsed 29m 41s (remain 5m 40s) Loss: 0.0001(0.0057) Grad: 102.7630  LR: 0.000010  \n",
      "Epoch: [3][3100/3575] Elapsed 30m 39s (remain 4m 41s) Loss: 0.0012(0.0057) Grad: 7038.9482  LR: 0.000009  \n",
      "Epoch: [3][3200/3575] Elapsed 31m 38s (remain 3m 41s) Loss: 0.0060(0.0057) Grad: 8022.7012  LR: 0.000009  \n",
      "Epoch: [3][3300/3575] Elapsed 32m 40s (remain 2m 42s) Loss: 0.0093(0.0058) Grad: 9876.7646  LR: 0.000009  \n",
      "Epoch: [3][3400/3575] Elapsed 33m 38s (remain 1m 43s) Loss: 0.0215(0.0058) Grad: 16387.9414  LR: 0.000009  \n",
      "Epoch: [3][3500/3575] Elapsed 34m 38s (remain 0m 43s) Loss: 0.0010(0.0058) Grad: 1560.5901  LR: 0.000009  \n",
      "Epoch: [3][3574/3575] Elapsed 35m 22s (remain 0m 0s) Loss: 0.0038(0.0058) Grad: 2428.2112  LR: 0.000009  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 9m 51s) Loss: 0.0001(0.0001) \n",
      "EVAL: [100/1192] Elapsed 0m 19s (remain 3m 32s) Loss: 0.0002(0.0052) \n",
      "EVAL: [200/1192] Elapsed 0m 38s (remain 3m 9s) Loss: 0.0002(0.0072) \n",
      "EVAL: [300/1192] Elapsed 0m 57s (remain 2m 48s) Loss: 0.0043(0.0107) \n",
      "EVAL: [400/1192] Elapsed 1m 15s (remain 2m 29s) Loss: 0.0211(0.0104) \n",
      "EVAL: [500/1192] Elapsed 1m 34s (remain 2m 9s) Loss: 0.0319(0.0094) \n",
      "EVAL: [600/1192] Elapsed 1m 52s (remain 1m 50s) Loss: 0.1134(0.0097) \n",
      "EVAL: [700/1192] Elapsed 2m 11s (remain 1m 31s) Loss: 0.0041(0.0109) \n",
      "EVAL: [800/1192] Elapsed 2m 29s (remain 1m 13s) Loss: 0.0087(0.0106) \n",
      "EVAL: [900/1192] Elapsed 2m 48s (remain 0m 54s) Loss: 0.0034(0.0103) \n",
      "EVAL: [1000/1192] Elapsed 3m 6s (remain 0m 35s) Loss: 0.0001(0.0100) \n",
      "EVAL: [1100/1192] Elapsed 3m 25s (remain 0m 16s) Loss: 0.0043(0.0095) \n",
      "EVAL: [1191/1192] Elapsed 3m 41s (remain 0m 0s) Loss: 0.0068(0.0090) \n",
      "Epoch 3 - avg_train_loss: 0.0058  avg_val_loss: 0.0090  time: 2348s\n",
      "Epoch 3 - Score: 0.8695\n",
      "Epoch 3 - Save Best Score: 0.8695 Model\n",
      "Epoch: [4][0/3575] Elapsed 0m 0s (remain 55m 19s) Loss: 0.0001(0.0001) Grad: 1024.4440  LR: 0.000009  \n",
      "Epoch: [4][100/3575] Elapsed 1m 0s (remain 34m 38s) Loss: 0.0032(0.0058) Grad: 8180.4966  LR: 0.000009  \n",
      "Epoch: [4][200/3575] Elapsed 2m 0s (remain 33m 37s) Loss: 0.0001(0.0048) Grad: 40.5770  LR: 0.000009  \n",
      "Epoch: [4][300/3575] Elapsed 2m 58s (remain 32m 26s) Loss: 0.0137(0.0048) Grad: 40231.4219  LR: 0.000009  \n",
      "Epoch: [4][400/3575] Elapsed 3m 57s (remain 31m 18s) Loss: 0.0114(0.0051) Grad: 40433.4688  LR: 0.000008  \n",
      "Epoch: [4][500/3575] Elapsed 4m 55s (remain 30m 14s) Loss: 0.0021(0.0051) Grad: 13555.3760  LR: 0.000008  \n",
      "Epoch: [4][600/3575] Elapsed 5m 54s (remain 29m 12s) Loss: 0.0032(0.0049) Grad: 2064.2251  LR: 0.000008  \n",
      "Epoch: [4][700/3575] Elapsed 6m 52s (remain 28m 11s) Loss: 0.0022(0.0048) Grad: 13876.1992  LR: 0.000008  \n",
      "Epoch: [4][800/3575] Elapsed 7m 52s (remain 27m 16s) Loss: 0.0001(0.0047) Grad: 396.6460  LR: 0.000008  \n",
      "Epoch: [4][900/3575] Elapsed 8m 52s (remain 26m 18s) Loss: 0.0000(0.0046) Grad: 130.4041  LR: 0.000008  \n",
      "Epoch: [4][1000/3575] Elapsed 9m 50s (remain 25m 19s) Loss: 0.0000(0.0045) Grad: 15.5042  LR: 0.000008  \n",
      "Epoch: [4][1100/3575] Elapsed 10m 50s (remain 24m 21s) Loss: 0.0010(0.0046) Grad: 24143.2363  LR: 0.000008  \n",
      "Epoch: [4][1200/3575] Elapsed 11m 53s (remain 23m 29s) Loss: 0.0001(0.0047) Grad: 159.5289  LR: 0.000007  \n",
      "Epoch: [4][1300/3575] Elapsed 12m 57s (remain 22m 38s) Loss: 0.0001(0.0046) Grad: 37.8408  LR: 0.000007  \n",
      "Epoch: [4][1400/3575] Elapsed 13m 56s (remain 21m 37s) Loss: 0.0003(0.0044) Grad: 612.8614  LR: 0.000007  \n",
      "Epoch: [4][1500/3575] Elapsed 14m 54s (remain 20m 36s) Loss: 0.0001(0.0044) Grad: 35.9630  LR: 0.000007  \n",
      "Epoch: [4][1600/3575] Elapsed 15m 54s (remain 19m 36s) Loss: 0.0001(0.0044) Grad: 231.4463  LR: 0.000007  \n",
      "Epoch: [4][1700/3575] Elapsed 16m 55s (remain 18m 38s) Loss: 0.0002(0.0044) Grad: 499.0406  LR: 0.000007  \n",
      "Epoch: [4][1800/3575] Elapsed 17m 57s (remain 17m 41s) Loss: 0.0002(0.0044) Grad: 660.0748  LR: 0.000007  \n",
      "Epoch: [4][1900/3575] Elapsed 18m 57s (remain 16m 41s) Loss: 0.0004(0.0044) Grad: 2024.7479  LR: 0.000007  \n",
      "Epoch: [4][2000/3575] Elapsed 19m 55s (remain 15m 40s) Loss: 0.0001(0.0045) Grad: 237.5428  LR: 0.000006  \n",
      "Epoch: [4][2100/3575] Elapsed 20m 54s (remain 14m 39s) Loss: 0.0027(0.0045) Grad: 10758.1055  LR: 0.000006  \n",
      "Epoch: [4][2200/3575] Elapsed 21m 53s (remain 13m 40s) Loss: 0.0002(0.0046) Grad: 695.1906  LR: 0.000006  \n",
      "Epoch: [4][2300/3575] Elapsed 22m 52s (remain 12m 40s) Loss: 0.0001(0.0046) Grad: 439.7200  LR: 0.000006  \n",
      "Epoch: [4][2400/3575] Elapsed 23m 51s (remain 11m 40s) Loss: 0.0001(0.0046) Grad: 94.3420  LR: 0.000006  \n",
      "Epoch: [4][2500/3575] Elapsed 24m 51s (remain 10m 40s) Loss: 0.0000(0.0045) Grad: 61.7099  LR: 0.000006  \n",
      "Epoch: [4][2600/3575] Elapsed 25m 54s (remain 9m 42s) Loss: 0.0001(0.0046) Grad: 32.5205  LR: 0.000006  \n",
      "Epoch: [4][2700/3575] Elapsed 26m 53s (remain 8m 42s) Loss: 0.0001(0.0045) Grad: 348.5925  LR: 0.000006  \n",
      "Epoch: [4][2800/3575] Elapsed 27m 51s (remain 7m 41s) Loss: 0.0024(0.0045) Grad: 14412.3857  LR: 0.000005  \n",
      "Epoch: [4][2900/3575] Elapsed 28m 51s (remain 6m 42s) Loss: 0.0000(0.0045) Grad: 22.2795  LR: 0.000005  \n",
      "Epoch: [4][3000/3575] Elapsed 29m 50s (remain 5m 42s) Loss: 0.0000(0.0045) Grad: 6.9931  LR: 0.000005  \n",
      "Epoch: [4][3100/3575] Elapsed 30m 49s (remain 4m 42s) Loss: 0.0080(0.0045) Grad: 20240.9336  LR: 0.000005  \n",
      "Epoch: [4][3200/3575] Elapsed 31m 49s (remain 3m 43s) Loss: 0.0050(0.0045) Grad: 23126.4570  LR: 0.000005  \n",
      "Epoch: [4][3300/3575] Elapsed 32m 50s (remain 2m 43s) Loss: 0.0010(0.0045) Grad: 28097.3672  LR: 0.000005  \n",
      "Epoch: [4][3400/3575] Elapsed 33m 49s (remain 1m 43s) Loss: 0.0001(0.0045) Grad: 69.4878  LR: 0.000005  \n",
      "Epoch: [4][3500/3575] Elapsed 34m 48s (remain 0m 44s) Loss: 0.0018(0.0045) Grad: 6009.8340  LR: 0.000005  \n",
      "Epoch: [4][3574/3575] Elapsed 35m 32s (remain 0m 0s) Loss: 0.0000(0.0045) Grad: 66.9608  LR: 0.000004  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 9m 26s) Loss: 0.0001(0.0001) \n",
      "EVAL: [100/1192] Elapsed 0m 20s (remain 3m 40s) Loss: 0.0002(0.0067) \n",
      "EVAL: [200/1192] Elapsed 0m 42s (remain 3m 31s) Loss: 0.0001(0.0079) \n",
      "EVAL: [300/1192] Elapsed 1m 5s (remain 3m 13s) Loss: 0.0017(0.0116) \n",
      "EVAL: [400/1192] Elapsed 1m 24s (remain 2m 46s) Loss: 0.0242(0.0113) \n",
      "EVAL: [500/1192] Elapsed 1m 43s (remain 2m 22s) Loss: 0.0341(0.0104) \n",
      "EVAL: [600/1192] Elapsed 2m 2s (remain 2m 0s) Loss: 0.1305(0.0105) \n",
      "EVAL: [700/1192] Elapsed 2m 21s (remain 1m 38s) Loss: 0.0054(0.0116) \n",
      "EVAL: [800/1192] Elapsed 2m 39s (remain 1m 17s) Loss: 0.0059(0.0112) \n",
      "EVAL: [900/1192] Elapsed 2m 58s (remain 0m 57s) Loss: 0.0093(0.0107) \n",
      "EVAL: [1000/1192] Elapsed 3m 17s (remain 0m 37s) Loss: 0.0001(0.0104) \n",
      "EVAL: [1100/1192] Elapsed 3m 36s (remain 0m 17s) Loss: 0.0047(0.0100) \n",
      "EVAL: [1191/1192] Elapsed 3m 55s (remain 0m 0s) Loss: 0.0073(0.0094) \n",
      "Epoch 4 - avg_train_loss: 0.0045  avg_val_loss: 0.0094  time: 2371s\n",
      "Epoch 4 - Score: 0.8756\n",
      "Epoch 4 - Save Best Score: 0.8756 Model\n",
      "Epoch: [5][0/3575] Elapsed 0m 0s (remain 54m 28s) Loss: 0.0009(0.0009) Grad: 4721.8730  LR: 0.000004  \n",
      "Epoch: [5][100/3575] Elapsed 0m 59s (remain 33m 49s) Loss: 0.0232(0.0035) Grad: 56788.8242  LR: 0.000004  \n",
      "Epoch: [5][200/3575] Elapsed 1m 57s (remain 32m 48s) Loss: 0.0011(0.0034) Grad: 2143.2256  LR: 0.000004  \n",
      "Epoch: [5][300/3575] Elapsed 2m 55s (remain 31m 48s) Loss: 0.0125(0.0032) Grad: 57774.5391  LR: 0.000004  \n",
      "Epoch: [5][400/3575] Elapsed 3m 53s (remain 30m 51s) Loss: 0.0069(0.0035) Grad: 17389.5605  LR: 0.000004  \n",
      "Epoch: [5][500/3575] Elapsed 4m 54s (remain 30m 5s) Loss: 0.0001(0.0035) Grad: 438.3916  LR: 0.000004  \n",
      "Epoch: [5][600/3575] Elapsed 5m 52s (remain 29m 6s) Loss: 0.0000(0.0034) Grad: 21.5011  LR: 0.000004  \n",
      "Epoch: [5][700/3575] Elapsed 6m 50s (remain 28m 3s) Loss: 0.0031(0.0035) Grad: 28770.2598  LR: 0.000004  \n",
      "Epoch: [5][800/3575] Elapsed 7m 48s (remain 27m 3s) Loss: 0.0016(0.0035) Grad: 1895.1306  LR: 0.000003  \n",
      "Epoch: [5][900/3575] Elapsed 8m 46s (remain 26m 2s) Loss: 0.0001(0.0038) Grad: 109.9001  LR: 0.000003  \n",
      "Epoch: [5][1000/3575] Elapsed 9m 44s (remain 25m 2s) Loss: 0.0194(0.0037) Grad: 154854.0781  LR: 0.000003  \n",
      "Epoch: [5][1100/3575] Elapsed 10m 42s (remain 24m 3s) Loss: 0.0001(0.0037) Grad: 199.3106  LR: 0.000003  \n",
      "Epoch: [5][1200/3575] Elapsed 11m 42s (remain 23m 8s) Loss: 0.0116(0.0036) Grad: 14309.9092  LR: 0.000003  \n",
      "Epoch: [5][1300/3575] Elapsed 12m 40s (remain 22m 8s) Loss: 0.0116(0.0036) Grad: 17787.1934  LR: 0.000003  \n",
      "Epoch: [5][1400/3575] Elapsed 13m 37s (remain 21m 8s) Loss: 0.0135(0.0036) Grad: 21820.5918  LR: 0.000003  \n",
      "Epoch: [5][1500/3575] Elapsed 14m 35s (remain 20m 9s) Loss: 0.0221(0.0037) Grad: 89050.8125  LR: 0.000003  \n",
      "Epoch: [5][1600/3575] Elapsed 15m 33s (remain 19m 10s) Loss: 0.0004(0.0035) Grad: 8245.1309  LR: 0.000002  \n",
      "Epoch: [5][1700/3575] Elapsed 16m 31s (remain 18m 11s) Loss: 0.0064(0.0035) Grad: 34546.8867  LR: 0.000002  \n",
      "Epoch: [5][1800/3575] Elapsed 17m 28s (remain 17m 13s) Loss: 0.0001(0.0036) Grad: 192.2220  LR: 0.000002  \n",
      "Epoch: [5][1900/3575] Elapsed 18m 27s (remain 16m 15s) Loss: 0.0000(0.0036) Grad: 25.7453  LR: 0.000002  \n",
      "Epoch: [5][2000/3575] Elapsed 19m 26s (remain 15m 17s) Loss: 0.0093(0.0036) Grad: 33980.2656  LR: 0.000002  \n",
      "Epoch: [5][2100/3575] Elapsed 20m 24s (remain 14m 19s) Loss: 0.0002(0.0036) Grad: 1058.5415  LR: 0.000002  \n",
      "Epoch: [5][2200/3575] Elapsed 21m 22s (remain 13m 20s) Loss: 0.0517(0.0038) Grad: 54790.9492  LR: 0.000002  \n",
      "Epoch: [5][2300/3575] Elapsed 22m 20s (remain 12m 22s) Loss: 0.0002(0.0038) Grad: 618.8844  LR: 0.000002  \n",
      "Epoch: [5][2400/3575] Elapsed 23m 19s (remain 11m 24s) Loss: 0.0001(0.0038) Grad: 349.1820  LR: 0.000001  \n",
      "Epoch: [5][2500/3575] Elapsed 24m 21s (remain 10m 27s) Loss: 0.0101(0.0038) Grad: 17689.3125  LR: 0.000001  \n",
      "Epoch: [5][2600/3575] Elapsed 25m 21s (remain 9m 29s) Loss: 0.0181(0.0038) Grad: 25472.4180  LR: 0.000001  \n",
      "Epoch: [5][2700/3575] Elapsed 26m 19s (remain 8m 31s) Loss: 0.0000(0.0037) Grad: 28.0829  LR: 0.000001  \n",
      "Epoch: [5][2800/3575] Elapsed 27m 17s (remain 7m 32s) Loss: 0.0087(0.0038) Grad: 17044.8398  LR: 0.000001  \n",
      "Epoch: [5][2900/3575] Elapsed 28m 15s (remain 6m 33s) Loss: 0.0000(0.0038) Grad: 31.6960  LR: 0.000001  \n",
      "Epoch: [5][3000/3575] Elapsed 29m 13s (remain 5m 35s) Loss: 0.0001(0.0038) Grad: 166.7674  LR: 0.000001  \n",
      "Epoch: [5][3100/3575] Elapsed 30m 13s (remain 4m 37s) Loss: 0.0038(0.0038) Grad: 22083.4570  LR: 0.000001  \n",
      "Epoch: [5][3200/3575] Elapsed 31m 14s (remain 3m 38s) Loss: 0.0059(0.0037) Grad: 38851.6680  LR: 0.000000  \n",
      "Epoch: [5][3300/3575] Elapsed 32m 12s (remain 2m 40s) Loss: 0.0178(0.0038) Grad: 38617.4023  LR: 0.000000  \n",
      "Epoch: [5][3400/3575] Elapsed 33m 10s (remain 1m 41s) Loss: 0.0001(0.0038) Grad: 1402.7932  LR: 0.000000  \n",
      "Epoch: [5][3500/3575] Elapsed 34m 8s (remain 0m 43s) Loss: 0.0017(0.0038) Grad: 16509.9180  LR: 0.000000  \n",
      "Epoch: [5][3574/3575] Elapsed 34m 52s (remain 0m 0s) Loss: 0.0002(0.0038) Grad: 847.7375  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 9m 23s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 19s (remain 3m 33s) Loss: 0.0002(0.0067) \n",
      "EVAL: [200/1192] Elapsed 0m 38s (remain 3m 9s) Loss: 0.0001(0.0082) \n",
      "EVAL: [300/1192] Elapsed 0m 57s (remain 2m 48s) Loss: 0.0017(0.0119) \n",
      "EVAL: [400/1192] Elapsed 1m 15s (remain 2m 28s) Loss: 0.0312(0.0119) \n",
      "EVAL: [500/1192] Elapsed 1m 34s (remain 2m 10s) Loss: 0.0352(0.0108) \n",
      "EVAL: [600/1192] Elapsed 1m 54s (remain 1m 52s) Loss: 0.1432(0.0109) \n",
      "EVAL: [700/1192] Elapsed 2m 13s (remain 1m 33s) Loss: 0.0057(0.0121) \n",
      "EVAL: [800/1192] Elapsed 2m 34s (remain 1m 15s) Loss: 0.0058(0.0118) \n",
      "EVAL: [900/1192] Elapsed 2m 52s (remain 0m 55s) Loss: 0.0105(0.0113) \n",
      "EVAL: [1000/1192] Elapsed 3m 11s (remain 0m 36s) Loss: 0.0000(0.0110) \n",
      "EVAL: [1100/1192] Elapsed 3m 30s (remain 0m 17s) Loss: 0.0053(0.0105) \n",
      "EVAL: [1191/1192] Elapsed 3m 47s (remain 0m 0s) Loss: 0.0085(0.0099) \n",
      "Epoch 5 - avg_train_loss: 0.0038  avg_val_loss: 0.0099  time: 2324s\n",
      "Epoch 5 - Score: 0.8651\n",
      "========== fold: 2 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/3575] Elapsed 0m 0s (remain 55m 40s) Loss: 0.3562(0.3562) Grad: 36461.1914  LR: 0.000000  \n",
      "Epoch: [1][100/3575] Elapsed 1m 0s (remain 34m 55s) Loss: 0.3459(0.3540) Grad: 37248.8906  LR: 0.000001  \n",
      "Epoch: [1][200/3575] Elapsed 2m 0s (remain 33m 34s) Loss: 0.3122(0.3430) Grad: 35702.3828  LR: 0.000002  \n",
      "Epoch: [1][300/3575] Elapsed 2m 59s (remain 32m 28s) Loss: 0.2324(0.3219) Grad: 18317.7559  LR: 0.000003  \n",
      "Epoch: [1][400/3575] Elapsed 3m 57s (remain 31m 22s) Loss: 0.0655(0.2805) Grad: 5594.4253  LR: 0.000004  \n",
      "Epoch: [1][500/3575] Elapsed 4m 57s (remain 30m 25s) Loss: 0.1295(0.2345) Grad: 16014.5342  LR: 0.000006  \n",
      "Epoch: [1][600/3575] Elapsed 5m 58s (remain 29m 32s) Loss: 0.0581(0.2028) Grad: 615.7625  LR: 0.000007  \n",
      "Epoch: [1][700/3575] Elapsed 6m 57s (remain 28m 31s) Loss: 0.0190(0.1792) Grad: 1845.9595  LR: 0.000008  \n",
      "Epoch: [1][800/3575] Elapsed 7m 56s (remain 27m 30s) Loss: 0.0115(0.1606) Grad: 7407.9697  LR: 0.000009  \n",
      "Epoch: [1][900/3575] Elapsed 8m 55s (remain 26m 28s) Loss: 0.0742(0.1456) Grad: 25383.8027  LR: 0.000010  \n",
      "Epoch: [1][1000/3575] Elapsed 9m 53s (remain 25m 26s) Loss: 0.0115(0.1329) Grad: 4126.6934  LR: 0.000011  \n",
      "Epoch: [1][1100/3575] Elapsed 10m 52s (remain 24m 25s) Loss: 0.0304(0.1227) Grad: 7393.5049  LR: 0.000012  \n",
      "Epoch: [1][1200/3575] Elapsed 11m 50s (remain 23m 24s) Loss: 0.0161(0.1139) Grad: 5913.3511  LR: 0.000013  \n",
      "Epoch: [1][1300/3575] Elapsed 12m 49s (remain 22m 24s) Loss: 0.0052(0.1066) Grad: 2310.6658  LR: 0.000015  \n",
      "Epoch: [1][1400/3575] Elapsed 13m 49s (remain 21m 27s) Loss: 0.0270(0.1001) Grad: 29305.9609  LR: 0.000016  \n",
      "Epoch: [1][1500/3575] Elapsed 14m 48s (remain 20m 28s) Loss: 0.0043(0.0943) Grad: 2750.7002  LR: 0.000017  \n",
      "Epoch: [1][1600/3575] Elapsed 15m 48s (remain 19m 29s) Loss: 0.0011(0.0893) Grad: 3459.8479  LR: 0.000018  \n",
      "Epoch: [1][1700/3575] Elapsed 16m 47s (remain 18m 29s) Loss: 0.0176(0.0848) Grad: 28414.0684  LR: 0.000019  \n",
      "Epoch: [1][1800/3575] Elapsed 17m 46s (remain 17m 30s) Loss: 0.0222(0.0808) Grad: 9015.0605  LR: 0.000020  \n",
      "Epoch: [1][1900/3575] Elapsed 18m 44s (remain 16m 30s) Loss: 0.0193(0.0772) Grad: 5153.9087  LR: 0.000020  \n",
      "Epoch: [1][2000/3575] Elapsed 19m 43s (remain 15m 31s) Loss: 0.0249(0.0740) Grad: 18860.0195  LR: 0.000020  \n",
      "Epoch: [1][2100/3575] Elapsed 20m 45s (remain 14m 33s) Loss: 0.0022(0.0710) Grad: 1766.9230  LR: 0.000020  \n",
      "Epoch: [1][2200/3575] Elapsed 21m 43s (remain 13m 33s) Loss: 0.0015(0.0683) Grad: 164.7072  LR: 0.000019  \n",
      "Epoch: [1][2300/3575] Elapsed 22m 43s (remain 12m 35s) Loss: 0.0015(0.0658) Grad: 723.3874  LR: 0.000019  \n",
      "Epoch: [1][2400/3575] Elapsed 23m 47s (remain 11m 38s) Loss: 0.0007(0.0636) Grad: 660.5313  LR: 0.000019  \n",
      "Epoch: [1][2500/3575] Elapsed 24m 46s (remain 10m 38s) Loss: 0.0145(0.0615) Grad: 1925.0785  LR: 0.000019  \n",
      "Epoch: [1][2600/3575] Elapsed 25m 45s (remain 9m 38s) Loss: 0.0009(0.0596) Grad: 266.4932  LR: 0.000019  \n",
      "Epoch: [1][2700/3575] Elapsed 26m 43s (remain 8m 38s) Loss: 0.0201(0.0578) Grad: 6155.9434  LR: 0.000019  \n",
      "Epoch: [1][2800/3575] Elapsed 27m 42s (remain 7m 39s) Loss: 0.0462(0.0561) Grad: 5155.0293  LR: 0.000019  \n",
      "Epoch: [1][2900/3575] Elapsed 28m 42s (remain 6m 40s) Loss: 0.0004(0.0545) Grad: 885.9050  LR: 0.000019  \n",
      "Epoch: [1][3000/3575] Elapsed 29m 41s (remain 5m 40s) Loss: 0.0068(0.0530) Grad: 4020.2654  LR: 0.000018  \n",
      "Epoch: [1][3100/3575] Elapsed 30m 40s (remain 4m 41s) Loss: 0.0000(0.0516) Grad: 363.8911  LR: 0.000018  \n",
      "Epoch: [1][3200/3575] Elapsed 31m 39s (remain 3m 41s) Loss: 0.0233(0.0503) Grad: 11090.5020  LR: 0.000018  \n",
      "Epoch: [1][3300/3575] Elapsed 32m 38s (remain 2m 42s) Loss: 0.0593(0.0491) Grad: 4966.5752  LR: 0.000018  \n",
      "Epoch: [1][3400/3575] Elapsed 33m 36s (remain 1m 43s) Loss: 0.0008(0.0480) Grad: 99.1584  LR: 0.000018  \n",
      "Epoch: [1][3500/3575] Elapsed 34m 37s (remain 0m 43s) Loss: 0.0008(0.0468) Grad: 341.4173  LR: 0.000018  \n",
      "Epoch: [1][3574/3575] Elapsed 35m 23s (remain 0m 0s) Loss: 0.0011(0.0460) Grad: 409.7289  LR: 0.000018  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 10m 17s) Loss: 0.0005(0.0005) \n",
      "EVAL: [100/1192] Elapsed 0m 21s (remain 3m 53s) Loss: 0.0205(0.0069) \n",
      "EVAL: [200/1192] Elapsed 0m 40s (remain 3m 18s) Loss: 0.0067(0.0077) \n",
      "EVAL: [300/1192] Elapsed 0m 58s (remain 2m 54s) Loss: 0.0131(0.0078) \n",
      "EVAL: [400/1192] Elapsed 1m 17s (remain 2m 32s) Loss: 0.0017(0.0082) \n",
      "EVAL: [500/1192] Elapsed 1m 36s (remain 2m 13s) Loss: 0.0006(0.0077) \n",
      "EVAL: [600/1192] Elapsed 1m 57s (remain 1m 55s) Loss: 0.0027(0.0084) \n",
      "EVAL: [700/1192] Elapsed 2m 16s (remain 1m 35s) Loss: 0.0077(0.0096) \n",
      "EVAL: [800/1192] Elapsed 2m 35s (remain 1m 15s) Loss: 0.0000(0.0095) \n",
      "EVAL: [900/1192] Elapsed 2m 53s (remain 0m 56s) Loss: 0.0041(0.0095) \n",
      "EVAL: [1000/1192] Elapsed 3m 12s (remain 0m 36s) Loss: 0.0077(0.0093) \n",
      "EVAL: [1100/1192] Elapsed 3m 30s (remain 0m 17s) Loss: 0.0097(0.0090) \n",
      "EVAL: [1191/1192] Elapsed 3m 49s (remain 0m 0s) Loss: 0.0010(0.0087) \n",
      "Epoch 1 - avg_train_loss: 0.0460  avg_val_loss: 0.0087  time: 2356s\n",
      "Epoch 1 - Score: 0.8234\n",
      "Epoch 1 - Save Best Score: 0.8234 Model\n",
      "Epoch: [2][0/3575] Elapsed 0m 0s (remain 52m 37s) Loss: 0.0042(0.0042) Grad: 6441.2993  LR: 0.000018  \n",
      "Epoch: [2][100/3575] Elapsed 0m 59s (remain 34m 10s) Loss: 0.0405(0.0077) Grad: 323343.2188  LR: 0.000018  \n",
      "Epoch: [2][200/3575] Elapsed 1m 58s (remain 33m 10s) Loss: 0.0240(0.0084) Grad: 24527.3340  LR: 0.000018  \n",
      "Epoch: [2][300/3575] Elapsed 2m 58s (remain 32m 18s) Loss: 0.0003(0.0087) Grad: 127.1002  LR: 0.000017  \n",
      "Epoch: [2][400/3575] Elapsed 3m 57s (remain 31m 17s) Loss: 0.0066(0.0085) Grad: 9557.1816  LR: 0.000017  \n",
      "Epoch: [2][500/3575] Elapsed 4m 56s (remain 30m 16s) Loss: 0.0002(0.0084) Grad: 66.3221  LR: 0.000017  \n",
      "Epoch: [2][600/3575] Elapsed 5m 56s (remain 29m 24s) Loss: 0.0067(0.0085) Grad: 19018.0723  LR: 0.000017  \n",
      "Epoch: [2][700/3575] Elapsed 6m 56s (remain 28m 28s) Loss: 0.0134(0.0085) Grad: 14228.9102  LR: 0.000017  \n",
      "Epoch: [2][800/3575] Elapsed 7m 55s (remain 27m 25s) Loss: 0.0011(0.0084) Grad: 2373.0684  LR: 0.000017  \n",
      "Epoch: [2][900/3575] Elapsed 8m 53s (remain 26m 24s) Loss: 0.0002(0.0085) Grad: 109.1029  LR: 0.000017  \n",
      "Epoch: [2][1000/3575] Elapsed 9m 53s (remain 25m 26s) Loss: 0.0123(0.0084) Grad: 8783.5166  LR: 0.000017  \n",
      "Epoch: [2][1100/3575] Elapsed 10m 54s (remain 24m 29s) Loss: 0.0010(0.0084) Grad: 1797.7825  LR: 0.000016  \n",
      "Epoch: [2][1200/3575] Elapsed 11m 55s (remain 23m 33s) Loss: 0.0062(0.0084) Grad: 16838.3223  LR: 0.000016  \n",
      "Epoch: [2][1300/3575] Elapsed 12m 53s (remain 22m 32s) Loss: 0.0097(0.0082) Grad: 27885.1738  LR: 0.000016  \n",
      "Epoch: [2][1400/3575] Elapsed 13m 54s (remain 21m 35s) Loss: 0.0108(0.0081) Grad: 40654.1250  LR: 0.000016  \n",
      "Epoch: [2][1500/3575] Elapsed 14m 58s (remain 20m 40s) Loss: 0.0002(0.0080) Grad: 97.0456  LR: 0.000016  \n",
      "Epoch: [2][1600/3575] Elapsed 15m 57s (remain 19m 40s) Loss: 0.0019(0.0081) Grad: 13302.0693  LR: 0.000016  \n",
      "Epoch: [2][1700/3575] Elapsed 16m 56s (remain 18m 39s) Loss: 0.0003(0.0080) Grad: 607.6721  LR: 0.000016  \n",
      "Epoch: [2][1800/3575] Elapsed 17m 55s (remain 17m 39s) Loss: 0.0093(0.0080) Grad: 56629.8984  LR: 0.000016  \n",
      "Epoch: [2][1900/3575] Elapsed 18m 55s (remain 16m 40s) Loss: 0.0014(0.0079) Grad: 4285.4531  LR: 0.000015  \n",
      "Epoch: [2][2000/3575] Elapsed 19m 56s (remain 15m 40s) Loss: 0.0009(0.0081) Grad: 3404.6240  LR: 0.000015  \n",
      "Epoch: [2][2100/3575] Elapsed 20m 57s (remain 14m 42s) Loss: 0.0525(0.0080) Grad: 47804.0273  LR: 0.000015  \n",
      "Epoch: [2][2200/3575] Elapsed 21m 59s (remain 13m 43s) Loss: 0.0048(0.0080) Grad: 14158.4014  LR: 0.000015  \n",
      "Epoch: [2][2300/3575] Elapsed 22m 59s (remain 12m 43s) Loss: 0.0026(0.0080) Grad: 1871.1874  LR: 0.000015  \n",
      "Epoch: [2][2400/3575] Elapsed 23m 58s (remain 11m 43s) Loss: 0.0103(0.0079) Grad: 50491.0312  LR: 0.000015  \n",
      "Epoch: [2][2500/3575] Elapsed 24m 56s (remain 10m 42s) Loss: 0.0060(0.0078) Grad: 15747.9990  LR: 0.000015  \n",
      "Epoch: [2][2600/3575] Elapsed 25m 54s (remain 9m 42s) Loss: 0.0086(0.0078) Grad: 1561.4059  LR: 0.000015  \n",
      "Epoch: [2][2700/3575] Elapsed 26m 55s (remain 8m 42s) Loss: 0.0555(0.0078) Grad: 27702.5508  LR: 0.000014  \n",
      "Epoch: [2][2800/3575] Elapsed 27m 53s (remain 7m 42s) Loss: 0.0014(0.0078) Grad: 8280.0605  LR: 0.000014  \n",
      "Epoch: [2][2900/3575] Elapsed 28m 52s (remain 6m 42s) Loss: 0.0003(0.0077) Grad: 646.0201  LR: 0.000014  \n",
      "Epoch: [2][3000/3575] Elapsed 29m 50s (remain 5m 42s) Loss: 0.0827(0.0077) Grad: 184616.4531  LR: 0.000014  \n",
      "Epoch: [2][3100/3575] Elapsed 30m 48s (remain 4m 42s) Loss: 0.0067(0.0077) Grad: 19646.2207  LR: 0.000014  \n",
      "Epoch: [2][3200/3575] Elapsed 31m 48s (remain 3m 42s) Loss: 0.0005(0.0077) Grad: 1590.8469  LR: 0.000014  \n",
      "Epoch: [2][3300/3575] Elapsed 32m 48s (remain 2m 43s) Loss: 0.0006(0.0076) Grad: 10226.7754  LR: 0.000014  \n",
      "Epoch: [2][3400/3575] Elapsed 33m 49s (remain 1m 43s) Loss: 0.0014(0.0076) Grad: 7059.4009  LR: 0.000014  \n",
      "Epoch: [2][3500/3575] Elapsed 34m 47s (remain 0m 44s) Loss: 0.0134(0.0077) Grad: 47401.1484  LR: 0.000013  \n",
      "Epoch: [2][3574/3575] Elapsed 35m 30s (remain 0m 0s) Loss: 0.0049(0.0077) Grad: 27158.4023  LR: 0.000013  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 9m 34s) Loss: 0.0001(0.0001) \n",
      "EVAL: [100/1192] Elapsed 0m 19s (remain 3m 26s) Loss: 0.0284(0.0061) \n",
      "EVAL: [200/1192] Elapsed 0m 37s (remain 3m 6s) Loss: 0.0076(0.0069) \n",
      "EVAL: [300/1192] Elapsed 0m 56s (remain 2m 47s) Loss: 0.0087(0.0070) \n",
      "EVAL: [400/1192] Elapsed 1m 16s (remain 2m 31s) Loss: 0.0006(0.0075) \n",
      "EVAL: [500/1192] Elapsed 1m 35s (remain 2m 11s) Loss: 0.0001(0.0071) \n",
      "EVAL: [600/1192] Elapsed 1m 53s (remain 1m 51s) Loss: 0.0101(0.0073) \n",
      "EVAL: [700/1192] Elapsed 2m 12s (remain 1m 32s) Loss: 0.0101(0.0079) \n",
      "EVAL: [800/1192] Elapsed 2m 31s (remain 1m 13s) Loss: 0.0000(0.0080) \n",
      "EVAL: [900/1192] Elapsed 2m 50s (remain 0m 55s) Loss: 0.0042(0.0081) \n",
      "EVAL: [1000/1192] Elapsed 3m 9s (remain 0m 36s) Loss: 0.0130(0.0079) \n",
      "EVAL: [1100/1192] Elapsed 3m 28s (remain 0m 17s) Loss: 0.0099(0.0076) \n",
      "EVAL: [1191/1192] Elapsed 3m 44s (remain 0m 0s) Loss: 0.0002(0.0073) \n",
      "Epoch 2 - avg_train_loss: 0.0077  avg_val_loss: 0.0073  time: 2358s\n",
      "Epoch 2 - Score: 0.8687\n",
      "Epoch 2 - Save Best Score: 0.8687 Model\n",
      "Epoch: [3][0/3575] Elapsed 0m 1s (remain 59m 45s) Loss: 0.0154(0.0154) Grad: 33903.5312  LR: 0.000013  \n",
      "Epoch: [3][100/3575] Elapsed 1m 1s (remain 34m 59s) Loss: 0.0285(0.0065) Grad: 43433.4180  LR: 0.000013  \n",
      "Epoch: [3][200/3575] Elapsed 1m 59s (remain 33m 28s) Loss: 0.0005(0.0062) Grad: 847.7272  LR: 0.000013  \n",
      "Epoch: [3][300/3575] Elapsed 2m 58s (remain 32m 16s) Loss: 0.0002(0.0064) Grad: 348.5275  LR: 0.000013  \n",
      "Epoch: [3][400/3575] Elapsed 3m 56s (remain 31m 10s) Loss: 0.0016(0.0062) Grad: 4147.5620  LR: 0.000013  \n",
      "Epoch: [3][500/3575] Elapsed 4m 56s (remain 30m 18s) Loss: 0.0001(0.0063) Grad: 148.2038  LR: 0.000013  \n",
      "Epoch: [3][600/3575] Elapsed 5m 57s (remain 29m 29s) Loss: 0.0765(0.0061) Grad: 187577.9688  LR: 0.000013  \n",
      "Epoch: [3][700/3575] Elapsed 6m 58s (remain 28m 34s) Loss: 0.0132(0.0060) Grad: 25308.9160  LR: 0.000012  \n",
      "Epoch: [3][800/3575] Elapsed 8m 1s (remain 27m 48s) Loss: 0.0016(0.0061) Grad: 2955.7832  LR: 0.000012  \n",
      "Epoch: [3][900/3575] Elapsed 9m 0s (remain 26m 45s) Loss: 0.0001(0.0060) Grad: 85.8728  LR: 0.000012  \n",
      "Epoch: [3][1000/3575] Elapsed 10m 0s (remain 25m 42s) Loss: 0.0002(0.0060) Grad: 256.9040  LR: 0.000012  \n",
      "Epoch: [3][1100/3575] Elapsed 10m 59s (remain 24m 42s) Loss: 0.0000(0.0060) Grad: 103.5416  LR: 0.000012  \n",
      "Epoch: [3][1200/3575] Elapsed 12m 0s (remain 23m 43s) Loss: 0.0011(0.0060) Grad: 1579.1199  LR: 0.000012  \n",
      "Epoch: [3][1300/3575] Elapsed 13m 0s (remain 22m 44s) Loss: 0.0000(0.0058) Grad: 11.0836  LR: 0.000012  \n",
      "Epoch: [3][1400/3575] Elapsed 14m 1s (remain 21m 45s) Loss: 0.0028(0.0058) Grad: 36702.3594  LR: 0.000012  \n",
      "Epoch: [3][1500/3575] Elapsed 15m 1s (remain 20m 45s) Loss: 0.0027(0.0060) Grad: 13733.0723  LR: 0.000011  \n",
      "Epoch: [3][1600/3575] Elapsed 16m 0s (remain 19m 44s) Loss: 0.0108(0.0059) Grad: 28241.4492  LR: 0.000011  \n",
      "Epoch: [3][1700/3575] Elapsed 16m 59s (remain 18m 42s) Loss: 0.0017(0.0058) Grad: 5012.7109  LR: 0.000011  \n",
      "Epoch: [3][1800/3575] Elapsed 17m 59s (remain 17m 43s) Loss: 0.0000(0.0058) Grad: 11.2152  LR: 0.000011  \n",
      "Epoch: [3][1900/3575] Elapsed 18m 59s (remain 16m 43s) Loss: 0.0002(0.0058) Grad: 139.8057  LR: 0.000011  \n",
      "Epoch: [3][2000/3575] Elapsed 19m 57s (remain 15m 42s) Loss: 0.0430(0.0059) Grad: 68267.6953  LR: 0.000011  \n",
      "Epoch: [3][2100/3575] Elapsed 20m 56s (remain 14m 41s) Loss: 0.0102(0.0059) Grad: 33606.1641  LR: 0.000011  \n",
      "Epoch: [3][2200/3575] Elapsed 21m 54s (remain 13m 40s) Loss: 0.0317(0.0059) Grad: 50517.4844  LR: 0.000011  \n",
      "Epoch: [3][2300/3575] Elapsed 22m 56s (remain 12m 42s) Loss: 0.0065(0.0059) Grad: 6316.5015  LR: 0.000010  \n",
      "Epoch: [3][2400/3575] Elapsed 24m 0s (remain 11m 44s) Loss: 0.0004(0.0059) Grad: 266.8891  LR: 0.000010  \n",
      "Epoch: [3][2500/3575] Elapsed 25m 0s (remain 10m 44s) Loss: 0.0007(0.0059) Grad: 2430.2456  LR: 0.000010  \n",
      "Epoch: [3][2600/3575] Elapsed 25m 59s (remain 9m 44s) Loss: 0.0077(0.0059) Grad: 2015.2321  LR: 0.000010  \n",
      "Epoch: [3][2700/3575] Elapsed 26m 57s (remain 8m 43s) Loss: 0.0007(0.0058) Grad: 6841.6182  LR: 0.000010  \n",
      "Epoch: [3][2800/3575] Elapsed 27m 56s (remain 7m 43s) Loss: 0.0193(0.0058) Grad: 25160.4941  LR: 0.000010  \n",
      "Epoch: [3][2900/3575] Elapsed 28m 56s (remain 6m 43s) Loss: 0.0317(0.0058) Grad: 5079.7461  LR: 0.000010  \n",
      "Epoch: [3][3000/3575] Elapsed 29m 58s (remain 5m 43s) Loss: 0.0257(0.0058) Grad: 60552.8594  LR: 0.000010  \n",
      "Epoch: [3][3100/3575] Elapsed 30m 57s (remain 4m 43s) Loss: 0.0001(0.0059) Grad: 38.2051  LR: 0.000009  \n",
      "Epoch: [3][3200/3575] Elapsed 31m 56s (remain 3m 43s) Loss: 0.0034(0.0058) Grad: 2797.8135  LR: 0.000009  \n",
      "Epoch: [3][3300/3575] Elapsed 32m 56s (remain 2m 44s) Loss: 0.0027(0.0058) Grad: 15495.0332  LR: 0.000009  \n",
      "Epoch: [3][3400/3575] Elapsed 33m 57s (remain 1m 44s) Loss: 0.0002(0.0057) Grad: 118.4465  LR: 0.000009  \n",
      "Epoch: [3][3500/3575] Elapsed 34m 56s (remain 0m 44s) Loss: 0.0039(0.0057) Grad: 8763.6006  LR: 0.000009  \n",
      "Epoch: [3][3574/3575] Elapsed 35m 40s (remain 0m 0s) Loss: 0.0021(0.0057) Grad: 4574.3726  LR: 0.000009  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 10m 3s) Loss: 0.0001(0.0001) \n",
      "EVAL: [100/1192] Elapsed 0m 20s (remain 3m 37s) Loss: 0.0306(0.0072) \n",
      "EVAL: [200/1192] Elapsed 0m 38s (remain 3m 11s) Loss: 0.0052(0.0071) \n",
      "EVAL: [300/1192] Elapsed 0m 57s (remain 2m 50s) Loss: 0.0030(0.0073) \n",
      "EVAL: [400/1192] Elapsed 1m 16s (remain 2m 31s) Loss: 0.0005(0.0077) \n",
      "EVAL: [500/1192] Elapsed 1m 36s (remain 2m 13s) Loss: 0.0001(0.0073) \n",
      "EVAL: [600/1192] Elapsed 1m 55s (remain 1m 53s) Loss: 0.0103(0.0076) \n",
      "EVAL: [700/1192] Elapsed 2m 14s (remain 1m 33s) Loss: 0.0121(0.0084) \n",
      "EVAL: [800/1192] Elapsed 2m 32s (remain 1m 14s) Loss: 0.0000(0.0084) \n",
      "EVAL: [900/1192] Elapsed 2m 51s (remain 0m 55s) Loss: 0.0172(0.0085) \n",
      "EVAL: [1000/1192] Elapsed 3m 9s (remain 0m 36s) Loss: 0.0166(0.0084) \n",
      "EVAL: [1100/1192] Elapsed 3m 28s (remain 0m 17s) Loss: 0.0398(0.0080) \n",
      "EVAL: [1191/1192] Elapsed 3m 46s (remain 0m 0s) Loss: 0.0001(0.0076) \n",
      "Epoch 3 - avg_train_loss: 0.0057  avg_val_loss: 0.0076  time: 2370s\n",
      "Epoch 3 - Score: 0.8537\n",
      "Epoch: [4][0/3575] Elapsed 0m 0s (remain 52m 46s) Loss: 0.0001(0.0001) Grad: 1007.3866  LR: 0.000009  \n",
      "Epoch: [4][100/3575] Elapsed 1m 0s (remain 34m 48s) Loss: 0.0223(0.0051) Grad: 26133.5566  LR: 0.000009  \n",
      "Epoch: [4][200/3575] Elapsed 1m 59s (remain 33m 22s) Loss: 0.0001(0.0055) Grad: 92.7818  LR: 0.000009  \n",
      "Epoch: [4][300/3575] Elapsed 2m 57s (remain 32m 14s) Loss: 0.0034(0.0049) Grad: 25938.1348  LR: 0.000009  \n",
      "Epoch: [4][400/3575] Elapsed 3m 57s (remain 31m 23s) Loss: 0.0007(0.0051) Grad: 6252.2510  LR: 0.000008  \n",
      "Epoch: [4][500/3575] Elapsed 4m 58s (remain 30m 29s) Loss: 0.0009(0.0050) Grad: 3457.7466  LR: 0.000008  \n",
      "Epoch: [4][600/3575] Elapsed 5m 56s (remain 29m 26s) Loss: 0.0041(0.0048) Grad: 8314.5244  LR: 0.000008  \n",
      "Epoch: [4][700/3575] Elapsed 6m 56s (remain 28m 25s) Loss: 0.0002(0.0045) Grad: 750.8967  LR: 0.000008  \n",
      "Epoch: [4][800/3575] Elapsed 7m 54s (remain 27m 23s) Loss: 0.0000(0.0045) Grad: 43.4666  LR: 0.000008  \n",
      "Epoch: [4][900/3575] Elapsed 8m 53s (remain 26m 22s) Loss: 0.0001(0.0044) Grad: 65.1969  LR: 0.000008  \n",
      "Epoch: [4][1000/3575] Elapsed 9m 52s (remain 25m 22s) Loss: 0.0001(0.0044) Grad: 81.6247  LR: 0.000008  \n",
      "Epoch: [4][1100/3575] Elapsed 10m 53s (remain 24m 29s) Loss: 0.0000(0.0043) Grad: 27.6316  LR: 0.000008  \n",
      "Epoch: [4][1200/3575] Elapsed 11m 54s (remain 23m 31s) Loss: 0.0240(0.0044) Grad: 42799.5391  LR: 0.000007  \n",
      "Epoch: [4][1300/3575] Elapsed 12m 53s (remain 22m 31s) Loss: 0.0008(0.0043) Grad: 493.9303  LR: 0.000007  \n",
      "Epoch: [4][1400/3575] Elapsed 13m 51s (remain 21m 30s) Loss: 0.0018(0.0043) Grad: 10971.5039  LR: 0.000007  \n",
      "Epoch: [4][1500/3575] Elapsed 14m 51s (remain 20m 31s) Loss: 0.0003(0.0043) Grad: 1384.4797  LR: 0.000007  \n",
      "Epoch: [4][1600/3575] Elapsed 15m 52s (remain 19m 33s) Loss: 0.0004(0.0043) Grad: 3024.5657  LR: 0.000007  \n",
      "Epoch: [4][1700/3575] Elapsed 16m 51s (remain 18m 34s) Loss: 0.0042(0.0044) Grad: 10853.5547  LR: 0.000007  \n",
      "Epoch: [4][1800/3575] Elapsed 17m 50s (remain 17m 34s) Loss: 0.0030(0.0044) Grad: 4411.0425  LR: 0.000007  \n",
      "Epoch: [4][1900/3575] Elapsed 18m 49s (remain 16m 34s) Loss: 0.0131(0.0044) Grad: 60355.4570  LR: 0.000007  \n",
      "Epoch: [4][2000/3575] Elapsed 19m 50s (remain 15m 36s) Loss: 0.0004(0.0043) Grad: 4342.7588  LR: 0.000006  \n",
      "Epoch: [4][2100/3575] Elapsed 20m 49s (remain 14m 36s) Loss: 0.0008(0.0045) Grad: 6357.6309  LR: 0.000006  \n",
      "Epoch: [4][2200/3575] Elapsed 21m 47s (remain 13m 36s) Loss: 0.0001(0.0045) Grad: 75.1161  LR: 0.000006  \n",
      "Epoch: [4][2300/3575] Elapsed 22m 47s (remain 12m 36s) Loss: 0.0001(0.0045) Grad: 107.9625  LR: 0.000006  \n",
      "Epoch: [4][2400/3575] Elapsed 23m 48s (remain 11m 38s) Loss: 0.0007(0.0046) Grad: 8461.5117  LR: 0.000006  \n",
      "Epoch: [4][2500/3575] Elapsed 24m 47s (remain 10m 38s) Loss: 0.0062(0.0046) Grad: 25358.9883  LR: 0.000006  \n",
      "Epoch: [4][2600/3575] Elapsed 25m 45s (remain 9m 38s) Loss: 0.0102(0.0047) Grad: 15872.7383  LR: 0.000006  \n",
      "Epoch: [4][2700/3575] Elapsed 26m 44s (remain 8m 39s) Loss: 0.0037(0.0046) Grad: 4154.1152  LR: 0.000006  \n",
      "Epoch: [4][2800/3575] Elapsed 27m 42s (remain 7m 39s) Loss: 0.0001(0.0046) Grad: 401.9195  LR: 0.000005  \n",
      "Epoch: [4][2900/3575] Elapsed 28m 40s (remain 6m 39s) Loss: 0.0001(0.0045) Grad: 113.9236  LR: 0.000005  \n",
      "Epoch: [4][3000/3575] Elapsed 29m 39s (remain 5m 40s) Loss: 0.0201(0.0046) Grad: 16508.9258  LR: 0.000005  \n",
      "Epoch: [4][3100/3575] Elapsed 30m 37s (remain 4m 40s) Loss: 0.0069(0.0046) Grad: 4716.0991  LR: 0.000005  \n",
      "Epoch: [4][3200/3575] Elapsed 31m 35s (remain 3m 41s) Loss: 0.0000(0.0046) Grad: 2.0850  LR: 0.000005  \n",
      "Epoch: [4][3300/3575] Elapsed 32m 34s (remain 2m 42s) Loss: 0.0011(0.0045) Grad: 2140.8887  LR: 0.000005  \n",
      "Epoch: [4][3400/3575] Elapsed 33m 34s (remain 1m 43s) Loss: 0.0002(0.0045) Grad: 118.0557  LR: 0.000005  \n",
      "Epoch: [4][3500/3575] Elapsed 34m 34s (remain 0m 43s) Loss: 0.0015(0.0045) Grad: 4935.3735  LR: 0.000005  \n",
      "Epoch: [4][3574/3575] Elapsed 35m 18s (remain 0m 0s) Loss: 0.0040(0.0045) Grad: 11851.1719  LR: 0.000004  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 9m 6s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 19s (remain 3m 32s) Loss: 0.0329(0.0066) \n",
      "EVAL: [200/1192] Elapsed 0m 38s (remain 3m 8s) Loss: 0.0076(0.0070) \n",
      "EVAL: [300/1192] Elapsed 0m 56s (remain 2m 47s) Loss: 0.0017(0.0071) \n",
      "EVAL: [400/1192] Elapsed 1m 16s (remain 2m 30s) Loss: 0.0003(0.0077) \n",
      "EVAL: [500/1192] Elapsed 1m 35s (remain 2m 11s) Loss: 0.0000(0.0073) \n",
      "EVAL: [600/1192] Elapsed 1m 53s (remain 1m 52s) Loss: 0.0074(0.0076) \n",
      "EVAL: [700/1192] Elapsed 2m 12s (remain 1m 32s) Loss: 0.0092(0.0083) \n",
      "EVAL: [800/1192] Elapsed 2m 30s (remain 1m 13s) Loss: 0.0000(0.0085) \n",
      "EVAL: [900/1192] Elapsed 2m 50s (remain 0m 54s) Loss: 0.0084(0.0085) \n",
      "EVAL: [1000/1192] Elapsed 3m 11s (remain 0m 36s) Loss: 0.0128(0.0084) \n",
      "EVAL: [1100/1192] Elapsed 3m 34s (remain 0m 17s) Loss: 0.0564(0.0080) \n",
      "EVAL: [1191/1192] Elapsed 3m 54s (remain 0m 0s) Loss: 0.0001(0.0076) \n",
      "Epoch 4 - avg_train_loss: 0.0045  avg_val_loss: 0.0076  time: 2356s\n",
      "Epoch 4 - Score: 0.8858\n",
      "Epoch 4 - Save Best Score: 0.8858 Model\n",
      "Epoch: [5][0/3575] Elapsed 0m 0s (remain 54m 9s) Loss: 0.0198(0.0198) Grad: 47041.5742  LR: 0.000004  \n",
      "Epoch: [5][100/3575] Elapsed 1m 0s (remain 34m 24s) Loss: 0.0003(0.0043) Grad: 2895.2583  LR: 0.000004  \n",
      "Epoch: [5][200/3575] Elapsed 2m 0s (remain 33m 50s) Loss: 0.0003(0.0040) Grad: 1898.9692  LR: 0.000004  \n",
      "Epoch: [5][300/3575] Elapsed 2m 59s (remain 32m 33s) Loss: 0.0006(0.0039) Grad: 926.1936  LR: 0.000004  \n",
      "Epoch: [5][400/3575] Elapsed 3m 57s (remain 31m 21s) Loss: 0.0076(0.0048) Grad: 87009.2344  LR: 0.000004  \n",
      "Epoch: [5][500/3575] Elapsed 4m 57s (remain 30m 23s) Loss: 0.0001(0.0044) Grad: 59.6776  LR: 0.000004  \n",
      "Epoch: [5][600/3575] Elapsed 5m 56s (remain 29m 23s) Loss: 0.0002(0.0043) Grad: 3548.2888  LR: 0.000004  \n",
      "Epoch: [5][700/3575] Elapsed 6m 54s (remain 28m 19s) Loss: 0.0002(0.0044) Grad: 786.9354  LR: 0.000004  \n",
      "Epoch: [5][800/3575] Elapsed 7m 53s (remain 27m 20s) Loss: 0.0001(0.0043) Grad: 153.1435  LR: 0.000003  \n",
      "Epoch: [5][900/3575] Elapsed 8m 52s (remain 26m 18s) Loss: 0.0001(0.0041) Grad: 144.0995  LR: 0.000003  \n",
      "Epoch: [5][1000/3575] Elapsed 9m 50s (remain 25m 17s) Loss: 0.0001(0.0040) Grad: 106.9641  LR: 0.000003  \n",
      "Epoch: [5][1100/3575] Elapsed 10m 48s (remain 24m 16s) Loss: 0.0001(0.0039) Grad: 53.0749  LR: 0.000003  \n",
      "Epoch: [5][1200/3575] Elapsed 11m 46s (remain 23m 15s) Loss: 0.0138(0.0039) Grad: 17993.7910  LR: 0.000003  \n",
      "Epoch: [5][1300/3575] Elapsed 12m 44s (remain 22m 15s) Loss: 0.0000(0.0039) Grad: 28.9841  LR: 0.000003  \n",
      "Epoch: [5][1400/3575] Elapsed 13m 43s (remain 21m 18s) Loss: 0.0101(0.0038) Grad: 75497.3828  LR: 0.000003  \n",
      "Epoch: [5][1500/3575] Elapsed 14m 44s (remain 20m 22s) Loss: 0.0000(0.0038) Grad: 107.2867  LR: 0.000003  \n",
      "Epoch: [5][1600/3575] Elapsed 15m 45s (remain 19m 25s) Loss: 0.0000(0.0037) Grad: 2.7799  LR: 0.000002  \n",
      "Epoch: [5][1700/3575] Elapsed 16m 43s (remain 18m 25s) Loss: 0.0001(0.0036) Grad: 33.1019  LR: 0.000002  \n",
      "Epoch: [5][1800/3575] Elapsed 17m 41s (remain 17m 25s) Loss: 0.0000(0.0037) Grad: 34.2616  LR: 0.000002  \n",
      "Epoch: [5][1900/3575] Elapsed 18m 39s (remain 16m 25s) Loss: 0.0003(0.0037) Grad: 10724.7295  LR: 0.000002  \n",
      "Epoch: [5][2000/3575] Elapsed 19m 36s (remain 15m 25s) Loss: 0.0019(0.0037) Grad: 3778.2905  LR: 0.000002  \n",
      "Epoch: [5][2100/3575] Elapsed 20m 33s (remain 14m 25s) Loss: 0.0000(0.0037) Grad: 3.2276  LR: 0.000002  \n",
      "Epoch: [5][2200/3575] Elapsed 21m 33s (remain 13m 27s) Loss: 0.0002(0.0037) Grad: 1077.5570  LR: 0.000002  \n",
      "Epoch: [5][2300/3575] Elapsed 22m 32s (remain 12m 28s) Loss: 0.0192(0.0037) Grad: 124894.7422  LR: 0.000002  \n",
      "Epoch: [5][2400/3575] Elapsed 23m 30s (remain 11m 29s) Loss: 0.0000(0.0036) Grad: 36.1380  LR: 0.000001  \n",
      "Epoch: [5][2500/3575] Elapsed 24m 28s (remain 10m 30s) Loss: 0.0013(0.0037) Grad: 17438.8242  LR: 0.000001  \n",
      "Epoch: [5][2600/3575] Elapsed 25m 26s (remain 9m 31s) Loss: 0.0003(0.0037) Grad: 855.0533  LR: 0.000001  \n",
      "Epoch: [5][2700/3575] Elapsed 26m 25s (remain 8m 32s) Loss: 0.0001(0.0037) Grad: 102.9423  LR: 0.000001  \n",
      "Epoch: [5][2800/3575] Elapsed 27m 24s (remain 7m 34s) Loss: 0.0000(0.0037) Grad: 22.1742  LR: 0.000001  \n",
      "Epoch: [5][2900/3575] Elapsed 28m 24s (remain 6m 36s) Loss: 0.0000(0.0037) Grad: 19.1886  LR: 0.000001  \n",
      "Epoch: [5][3000/3575] Elapsed 29m 23s (remain 5m 37s) Loss: 0.0005(0.0037) Grad: 203.2780  LR: 0.000001  \n",
      "Epoch: [5][3100/3575] Elapsed 30m 21s (remain 4m 38s) Loss: 0.0013(0.0036) Grad: 23630.8789  LR: 0.000001  \n",
      "Epoch: [5][3200/3575] Elapsed 31m 20s (remain 3m 39s) Loss: 0.0001(0.0036) Grad: 65.4512  LR: 0.000000  \n",
      "Epoch: [5][3300/3575] Elapsed 32m 19s (remain 2m 40s) Loss: 0.0001(0.0036) Grad: 158.0388  LR: 0.000000  \n",
      "Epoch: [5][3400/3575] Elapsed 33m 17s (remain 1m 42s) Loss: 0.0005(0.0036) Grad: 5664.4771  LR: 0.000000  \n",
      "Epoch: [5][3500/3575] Elapsed 34m 16s (remain 0m 43s) Loss: 0.0068(0.0036) Grad: 24810.3770  LR: 0.000000  \n",
      "Epoch: [5][3574/3575] Elapsed 35m 1s (remain 0m 0s) Loss: 0.0000(0.0036) Grad: 14.5040  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 9m 42s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 20s (remain 3m 40s) Loss: 0.0413(0.0081) \n",
      "EVAL: [200/1192] Elapsed 0m 39s (remain 3m 13s) Loss: 0.0122(0.0081) \n",
      "EVAL: [300/1192] Elapsed 0m 57s (remain 2m 50s) Loss: 0.0018(0.0082) \n",
      "EVAL: [400/1192] Elapsed 1m 16s (remain 2m 30s) Loss: 0.0001(0.0088) \n",
      "EVAL: [500/1192] Elapsed 1m 35s (remain 2m 11s) Loss: 0.0000(0.0083) \n",
      "EVAL: [600/1192] Elapsed 1m 54s (remain 1m 52s) Loss: 0.0066(0.0086) \n",
      "EVAL: [700/1192] Elapsed 2m 13s (remain 1m 33s) Loss: 0.0112(0.0094) \n",
      "EVAL: [800/1192] Elapsed 2m 32s (remain 1m 14s) Loss: 0.0000(0.0096) \n",
      "EVAL: [900/1192] Elapsed 2m 50s (remain 0m 55s) Loss: 0.0116(0.0096) \n",
      "EVAL: [1000/1192] Elapsed 3m 9s (remain 0m 36s) Loss: 0.0183(0.0094) \n",
      "EVAL: [1100/1192] Elapsed 3m 28s (remain 0m 17s) Loss: 0.0607(0.0090) \n",
      "EVAL: [1191/1192] Elapsed 3m 46s (remain 0m 0s) Loss: 0.0000(0.0086) \n",
      "Epoch 5 - avg_train_loss: 0.0036  avg_val_loss: 0.0086  time: 2331s\n",
      "Epoch 5 - Score: 0.8863\n",
      "Epoch 5 - Save Best Score: 0.8863 Model\n",
      "========== fold: 3 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Epoch: [1][0/3575] Elapsed 0m 0s (remain 54m 36s) Loss: 0.3519(0.3519) Grad: 36889.3125  LR: 0.000000  \n",
      "Epoch: [1][100/3575] Elapsed 0m 59s (remain 34m 13s) Loss: 0.3404(0.3479) Grad: 37585.8750  LR: 0.000001  \n",
      "Epoch: [1][200/3575] Elapsed 1m 58s (remain 33m 7s) Loss: 0.3050(0.3370) Grad: 34727.3477  LR: 0.000002  \n",
      "Epoch: [1][300/3575] Elapsed 3m 0s (remain 32m 46s) Loss: 0.2379(0.3176) Grad: 18535.1895  LR: 0.000003  \n",
      "Epoch: [1][400/3575] Elapsed 4m 5s (remain 32m 23s) Loss: 0.1065(0.2809) Grad: 13156.0234  LR: 0.000004  \n",
      "Epoch: [1][500/3575] Elapsed 5m 10s (remain 31m 43s) Loss: 0.0630(0.2374) Grad: 1010.5408  LR: 0.000006  \n",
      "Epoch: [1][600/3575] Elapsed 6m 10s (remain 30m 33s) Loss: 0.0243(0.2051) Grad: 1461.1826  LR: 0.000007  \n",
      "Epoch: [1][700/3575] Elapsed 7m 10s (remain 29m 26s) Loss: 0.0217(0.1817) Grad: 9567.6416  LR: 0.000008  \n",
      "Epoch: [1][800/3575] Elapsed 8m 11s (remain 28m 21s) Loss: 0.0300(0.1644) Grad: 9731.7305  LR: 0.000009  \n",
      "Epoch: [1][900/3575] Elapsed 9m 11s (remain 27m 17s) Loss: 0.0148(0.1493) Grad: 3440.3191  LR: 0.000010  \n",
      "Epoch: [1][1000/3575] Elapsed 10m 14s (remain 26m 20s) Loss: 0.0186(0.1365) Grad: 17744.8848  LR: 0.000011  \n",
      "Epoch: [1][1100/3575] Elapsed 11m 15s (remain 25m 18s) Loss: 0.0085(0.1259) Grad: 9920.3008  LR: 0.000012  \n",
      "Epoch: [1][1200/3575] Elapsed 12m 16s (remain 24m 15s) Loss: 0.0060(0.1169) Grad: 11892.7344  LR: 0.000013  \n",
      "Epoch: [1][1300/3575] Elapsed 13m 19s (remain 23m 16s) Loss: 0.0168(0.1093) Grad: 24741.0293  LR: 0.000015  \n",
      "Epoch: [1][1400/3575] Elapsed 14m 18s (remain 22m 11s) Loss: 0.0036(0.1027) Grad: 2232.2930  LR: 0.000016  \n",
      "Epoch: [1][1500/3575] Elapsed 15m 17s (remain 21m 7s) Loss: 0.0031(0.0969) Grad: 2313.7478  LR: 0.000017  \n",
      "Epoch: [1][1600/3575] Elapsed 16m 15s (remain 20m 3s) Loss: 0.0011(0.0920) Grad: 429.6354  LR: 0.000018  \n",
      "Epoch: [1][1700/3575] Elapsed 17m 14s (remain 19m 0s) Loss: 0.0273(0.0873) Grad: 12505.5078  LR: 0.000019  \n",
      "Epoch: [1][1800/3575] Elapsed 18m 14s (remain 17m 57s) Loss: 0.0280(0.0832) Grad: 30087.6875  LR: 0.000020  \n",
      "Epoch: [1][1900/3575] Elapsed 19m 12s (remain 16m 55s) Loss: 0.0058(0.0796) Grad: 5296.2939  LR: 0.000020  \n",
      "Epoch: [1][2000/3575] Elapsed 20m 11s (remain 15m 52s) Loss: 0.0018(0.0762) Grad: 658.3746  LR: 0.000020  \n",
      "Epoch: [1][2100/3575] Elapsed 21m 10s (remain 14m 51s) Loss: 0.0009(0.0732) Grad: 1039.8209  LR: 0.000020  \n",
      "Epoch: [1][2200/3575] Elapsed 22m 9s (remain 13m 49s) Loss: 0.0084(0.0704) Grad: 7854.8892  LR: 0.000019  \n",
      "Epoch: [1][2300/3575] Elapsed 23m 8s (remain 12m 48s) Loss: 0.0371(0.0678) Grad: 42529.3477  LR: 0.000019  \n",
      "Epoch: [1][2400/3575] Elapsed 24m 8s (remain 11m 48s) Loss: 0.0031(0.0655) Grad: 4095.9700  LR: 0.000019  \n",
      "Epoch: [1][2500/3575] Elapsed 25m 7s (remain 10m 47s) Loss: 0.0016(0.0632) Grad: 1155.8184  LR: 0.000019  \n",
      "Epoch: [1][2600/3575] Elapsed 26m 6s (remain 9m 46s) Loss: 0.0006(0.0612) Grad: 510.9552  LR: 0.000019  \n",
      "Epoch: [1][2700/3575] Elapsed 27m 4s (remain 8m 45s) Loss: 0.0030(0.0594) Grad: 8048.5884  LR: 0.000019  \n",
      "Epoch: [1][2800/3575] Elapsed 28m 4s (remain 7m 45s) Loss: 0.0040(0.0577) Grad: 6428.8369  LR: 0.000019  \n",
      "Epoch: [1][2900/3575] Elapsed 29m 4s (remain 6m 45s) Loss: 0.0222(0.0561) Grad: 34584.3672  LR: 0.000019  \n",
      "Epoch: [1][3000/3575] Elapsed 30m 5s (remain 5m 45s) Loss: 0.0000(0.0546) Grad: 52.5981  LR: 0.000018  \n",
      "Epoch: [1][3100/3575] Elapsed 31m 6s (remain 4m 45s) Loss: 0.0002(0.0532) Grad: 60.8339  LR: 0.000018  \n",
      "Epoch: [1][3200/3575] Elapsed 32m 5s (remain 3m 45s) Loss: 0.0092(0.0518) Grad: 5592.2842  LR: 0.000018  \n",
      "Epoch: [1][3300/3575] Elapsed 33m 4s (remain 2m 44s) Loss: 0.0047(0.0506) Grad: 3202.8247  LR: 0.000018  \n",
      "Epoch: [1][3400/3575] Elapsed 34m 4s (remain 1m 44s) Loss: 0.0233(0.0494) Grad: 8112.9277  LR: 0.000018  \n",
      "Epoch: [1][3500/3575] Elapsed 35m 2s (remain 0m 44s) Loss: 0.0077(0.0482) Grad: 28713.9219  LR: 0.000018  \n",
      "Epoch: [1][3574/3575] Elapsed 35m 46s (remain 0m 0s) Loss: 0.0000(0.0474) Grad: 38.3534  LR: 0.000018  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 9m 10s) Loss: 0.0111(0.0111) \n",
      "EVAL: [100/1192] Elapsed 0m 19s (remain 3m 26s) Loss: 0.0311(0.0091) \n",
      "EVAL: [200/1192] Elapsed 0m 37s (remain 3m 4s) Loss: 0.0079(0.0085) \n",
      "EVAL: [300/1192] Elapsed 0m 56s (remain 2m 46s) Loss: 0.0055(0.0103) \n",
      "EVAL: [400/1192] Elapsed 1m 15s (remain 2m 28s) Loss: 0.0004(0.0098) \n",
      "EVAL: [500/1192] Elapsed 1m 33s (remain 2m 9s) Loss: 0.0341(0.0088) \n",
      "EVAL: [600/1192] Elapsed 1m 53s (remain 1m 51s) Loss: 0.0117(0.0093) \n",
      "EVAL: [700/1192] Elapsed 2m 14s (remain 1m 34s) Loss: 0.0049(0.0097) \n",
      "EVAL: [800/1192] Elapsed 2m 33s (remain 1m 15s) Loss: 0.0114(0.0094) \n",
      "EVAL: [900/1192] Elapsed 2m 52s (remain 0m 55s) Loss: 0.0084(0.0099) \n",
      "EVAL: [1000/1192] Elapsed 3m 10s (remain 0m 36s) Loss: 0.0004(0.0095) \n",
      "EVAL: [1100/1192] Elapsed 3m 29s (remain 0m 17s) Loss: 0.0133(0.0091) \n",
      "EVAL: [1191/1192] Elapsed 3m 45s (remain 0m 0s) Loss: 0.0010(0.0088) \n",
      "Epoch 1 - avg_train_loss: 0.0474  avg_val_loss: 0.0088  time: 2376s\n",
      "Epoch 1 - Score: 0.8373\n",
      "Epoch 1 - Save Best Score: 0.8373 Model\n",
      "Epoch: [2][0/3575] Elapsed 0m 0s (remain 54m 3s) Loss: 0.0014(0.0014) Grad: 2061.9529  LR: 0.000018  \n",
      "Epoch: [2][100/3575] Elapsed 1m 2s (remain 35m 45s) Loss: 0.0096(0.0063) Grad: 3170.4995  LR: 0.000018  \n",
      "Epoch: [2][200/3575] Elapsed 2m 3s (remain 34m 30s) Loss: 0.0008(0.0075) Grad: 6768.2812  LR: 0.000018  \n",
      "Epoch: [2][300/3575] Elapsed 3m 1s (remain 32m 55s) Loss: 0.0014(0.0080) Grad: 40589.4883  LR: 0.000017  \n",
      "Epoch: [2][400/3575] Elapsed 3m 59s (remain 31m 38s) Loss: 0.0016(0.0079) Grad: 6664.3374  LR: 0.000017  \n",
      "Epoch: [2][500/3575] Elapsed 4m 59s (remain 30m 37s) Loss: 0.0067(0.0081) Grad: 19707.0547  LR: 0.000017  \n",
      "Epoch: [2][600/3575] Elapsed 5m 59s (remain 29m 37s) Loss: 0.0057(0.0081) Grad: 11118.7441  LR: 0.000017  \n",
      "Epoch: [2][700/3575] Elapsed 6m 57s (remain 28m 30s) Loss: 0.0221(0.0081) Grad: 17727.6504  LR: 0.000017  \n",
      "Epoch: [2][800/3575] Elapsed 7m 55s (remain 27m 25s) Loss: 0.0034(0.0082) Grad: 9893.6885  LR: 0.000017  \n",
      "Epoch: [2][900/3575] Elapsed 8m 53s (remain 26m 22s) Loss: 0.0079(0.0081) Grad: 23324.1426  LR: 0.000017  \n",
      "Epoch: [2][1000/3575] Elapsed 9m 51s (remain 25m 21s) Loss: 0.0075(0.0080) Grad: 7802.4199  LR: 0.000017  \n",
      "Epoch: [2][1100/3575] Elapsed 10m 51s (remain 24m 24s) Loss: 0.0123(0.0080) Grad: 3257.5525  LR: 0.000016  \n",
      "Epoch: [2][1200/3575] Elapsed 11m 53s (remain 23m 29s) Loss: 0.0002(0.0081) Grad: 51.9046  LR: 0.000016  \n",
      "Epoch: [2][1300/3575] Elapsed 12m 56s (remain 22m 36s) Loss: 0.0104(0.0082) Grad: 1373.5137  LR: 0.000016  \n",
      "Epoch: [2][1400/3575] Elapsed 13m 55s (remain 21m 36s) Loss: 0.0174(0.0082) Grad: 5146.7954  LR: 0.000016  \n",
      "Epoch: [2][1500/3575] Elapsed 14m 53s (remain 20m 35s) Loss: 0.0042(0.0082) Grad: 24878.1934  LR: 0.000016  \n",
      "Epoch: [2][1600/3575] Elapsed 15m 54s (remain 19m 36s) Loss: 0.0130(0.0081) Grad: 5604.7007  LR: 0.000016  \n",
      "Epoch: [2][1700/3575] Elapsed 16m 53s (remain 18m 36s) Loss: 0.0010(0.0080) Grad: 9278.8486  LR: 0.000016  \n",
      "Epoch: [2][1800/3575] Elapsed 17m 51s (remain 17m 35s) Loss: 0.0031(0.0080) Grad: 3782.0786  LR: 0.000016  \n",
      "Epoch: [2][1900/3575] Elapsed 18m 50s (remain 16m 35s) Loss: 0.0427(0.0080) Grad: 44942.9062  LR: 0.000015  \n",
      "Epoch: [2][2000/3575] Elapsed 19m 49s (remain 15m 35s) Loss: 0.0005(0.0080) Grad: 1719.1161  LR: 0.000015  \n",
      "Epoch: [2][2100/3575] Elapsed 20m 48s (remain 14m 36s) Loss: 0.0116(0.0080) Grad: 5162.4814  LR: 0.000015  \n",
      "Epoch: [2][2200/3575] Elapsed 21m 46s (remain 13m 35s) Loss: 0.0002(0.0079) Grad: 71.3426  LR: 0.000015  \n",
      "Epoch: [2][2300/3575] Elapsed 22m 45s (remain 12m 36s) Loss: 0.0186(0.0079) Grad: 8481.4365  LR: 0.000015  \n",
      "Epoch: [2][2400/3575] Elapsed 23m 45s (remain 11m 36s) Loss: 0.0383(0.0078) Grad: 4188.1514  LR: 0.000015  \n",
      "Epoch: [2][2500/3575] Elapsed 24m 45s (remain 10m 37s) Loss: 0.0107(0.0079) Grad: 7359.8892  LR: 0.000015  \n",
      "Epoch: [2][2600/3575] Elapsed 25m 45s (remain 9m 38s) Loss: 0.0011(0.0079) Grad: 1625.0573  LR: 0.000015  \n",
      "Epoch: [2][2700/3575] Elapsed 26m 43s (remain 8m 38s) Loss: 0.0005(0.0078) Grad: 172.0644  LR: 0.000014  \n",
      "Epoch: [2][2800/3575] Elapsed 27m 43s (remain 7m 39s) Loss: 0.0022(0.0077) Grad: 3512.6675  LR: 0.000014  \n",
      "Epoch: [2][2900/3575] Elapsed 28m 43s (remain 6m 40s) Loss: 0.0106(0.0077) Grad: 23474.8418  LR: 0.000014  \n",
      "Epoch: [2][3000/3575] Elapsed 29m 42s (remain 5m 40s) Loss: 0.0107(0.0077) Grad: 13416.3672  LR: 0.000014  \n",
      "Epoch: [2][3100/3575] Elapsed 30m 41s (remain 4m 41s) Loss: 0.0114(0.0077) Grad: 24737.0859  LR: 0.000014  \n",
      "Epoch: [2][3200/3575] Elapsed 31m 41s (remain 3m 42s) Loss: 0.0130(0.0077) Grad: 2315.1028  LR: 0.000014  \n",
      "Epoch: [2][3300/3575] Elapsed 32m 40s (remain 2m 42s) Loss: 0.0038(0.0077) Grad: 11028.1113  LR: 0.000014  \n",
      "Epoch: [2][3400/3575] Elapsed 33m 39s (remain 1m 43s) Loss: 0.0010(0.0077) Grad: 873.7170  LR: 0.000014  \n",
      "Epoch: [2][3500/3575] Elapsed 34m 38s (remain 0m 43s) Loss: 0.0000(0.0076) Grad: 13.5624  LR: 0.000013  \n",
      "Epoch: [2][3574/3575] Elapsed 35m 23s (remain 0m 0s) Loss: 0.0000(0.0076) Grad: 81.6973  LR: 0.000013  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 9m 42s) Loss: 0.0004(0.0004) \n",
      "EVAL: [100/1192] Elapsed 0m 19s (remain 3m 34s) Loss: 0.0320(0.0079) \n",
      "EVAL: [200/1192] Elapsed 0m 40s (remain 3m 19s) Loss: 0.0060(0.0067) \n",
      "EVAL: [300/1192] Elapsed 0m 58s (remain 2m 54s) Loss: 0.0071(0.0073) \n",
      "EVAL: [400/1192] Elapsed 1m 17s (remain 2m 32s) Loss: 0.0001(0.0070) \n",
      "EVAL: [500/1192] Elapsed 1m 36s (remain 2m 13s) Loss: 0.0418(0.0067) \n",
      "EVAL: [600/1192] Elapsed 1m 56s (remain 1m 54s) Loss: 0.0087(0.0069) \n",
      "EVAL: [700/1192] Elapsed 2m 15s (remain 1m 34s) Loss: 0.0047(0.0077) \n",
      "EVAL: [800/1192] Elapsed 2m 34s (remain 1m 15s) Loss: 0.0084(0.0076) \n",
      "EVAL: [900/1192] Elapsed 2m 52s (remain 0m 55s) Loss: 0.0076(0.0079) \n",
      "EVAL: [1000/1192] Elapsed 3m 10s (remain 0m 36s) Loss: 0.0001(0.0076) \n",
      "EVAL: [1100/1192] Elapsed 3m 29s (remain 0m 17s) Loss: 0.0189(0.0074) \n",
      "EVAL: [1191/1192] Elapsed 3m 47s (remain 0m 0s) Loss: 0.0004(0.0072) \n",
      "Epoch 2 - avg_train_loss: 0.0076  avg_val_loss: 0.0072  time: 2354s\n",
      "Epoch 2 - Score: 0.8687\n",
      "Epoch 2 - Save Best Score: 0.8687 Model\n",
      "Epoch: [3][0/3575] Elapsed 0m 0s (remain 55m 47s) Loss: 0.0015(0.0015) Grad: 5217.2271  LR: 0.000013  \n",
      "Epoch: [3][100/3575] Elapsed 1m 6s (remain 37m 54s) Loss: 0.0003(0.0066) Grad: 165.1578  LR: 0.000013  \n",
      "Epoch: [3][200/3575] Elapsed 2m 8s (remain 35m 59s) Loss: 0.0078(0.0066) Grad: 13337.7539  LR: 0.000013  \n",
      "Epoch: [3][300/3575] Elapsed 3m 9s (remain 34m 20s) Loss: 0.0002(0.0059) Grad: 2846.6680  LR: 0.000013  \n",
      "Epoch: [3][400/3575] Elapsed 4m 9s (remain 32m 58s) Loss: 0.0133(0.0057) Grad: 54123.5820  LR: 0.000013  \n",
      "Epoch: [3][500/3575] Elapsed 5m 11s (remain 31m 53s) Loss: 0.0002(0.0056) Grad: 190.3430  LR: 0.000013  \n",
      "Epoch: [3][600/3575] Elapsed 6m 16s (remain 31m 4s) Loss: 0.0039(0.0056) Grad: 7124.0454  LR: 0.000013  \n",
      "Epoch: [3][700/3575] Elapsed 7m 18s (remain 29m 55s) Loss: 0.0003(0.0057) Grad: 175.1812  LR: 0.000012  \n",
      "Epoch: [3][800/3575] Elapsed 8m 18s (remain 28m 47s) Loss: 0.0837(0.0057) Grad: 71590.2109  LR: 0.000012  \n",
      "Epoch: [3][900/3575] Elapsed 9m 19s (remain 27m 41s) Loss: 0.0079(0.0058) Grad: 19484.2285  LR: 0.000012  \n",
      "Epoch: [3][1000/3575] Elapsed 10m 21s (remain 26m 38s) Loss: 0.0002(0.0058) Grad: 95.2421  LR: 0.000012  \n",
      "Epoch: [3][1100/3575] Elapsed 11m 23s (remain 25m 35s) Loss: 0.0104(0.0059) Grad: 29264.1855  LR: 0.000012  \n",
      "Epoch: [3][1200/3575] Elapsed 12m 27s (remain 24m 36s) Loss: 0.0155(0.0059) Grad: 15199.6807  LR: 0.000012  \n",
      "Epoch: [3][1300/3575] Elapsed 13m 26s (remain 23m 29s) Loss: 0.0130(0.0059) Grad: 30966.3320  LR: 0.000012  \n",
      "Epoch: [3][1400/3575] Elapsed 14m 24s (remain 22m 21s) Loss: 0.0015(0.0058) Grad: 24183.0215  LR: 0.000012  \n",
      "Epoch: [3][1500/3575] Elapsed 15m 23s (remain 21m 16s) Loss: 0.0135(0.0058) Grad: 88971.1094  LR: 0.000011  \n",
      "Epoch: [3][1600/3575] Elapsed 16m 24s (remain 20m 13s) Loss: 0.0072(0.0058) Grad: 6194.8916  LR: 0.000011  \n",
      "Epoch: [3][1700/3575] Elapsed 17m 22s (remain 19m 8s) Loss: 0.0001(0.0058) Grad: 78.8019  LR: 0.000011  \n",
      "Epoch: [3][1800/3575] Elapsed 18m 21s (remain 18m 4s) Loss: 0.0214(0.0058) Grad: 33645.2969  LR: 0.000011  \n",
      "Epoch: [3][1900/3575] Elapsed 19m 19s (remain 17m 0s) Loss: 0.0001(0.0058) Grad: 53.0714  LR: 0.000011  \n",
      "Epoch: [3][2000/3575] Elapsed 20m 17s (remain 15m 57s) Loss: 0.0000(0.0057) Grad: 31.2857  LR: 0.000011  \n",
      "Epoch: [3][2100/3575] Elapsed 21m 16s (remain 14m 55s) Loss: 0.0223(0.0057) Grad: 22037.7637  LR: 0.000011  \n",
      "Epoch: [3][2200/3575] Elapsed 22m 17s (remain 13m 55s) Loss: 0.0044(0.0057) Grad: 6901.8037  LR: 0.000011  \n",
      "Epoch: [3][2300/3575] Elapsed 23m 16s (remain 12m 53s) Loss: 0.0001(0.0057) Grad: 101.6789  LR: 0.000010  \n",
      "Epoch: [3][2400/3575] Elapsed 24m 15s (remain 11m 51s) Loss: 0.0015(0.0057) Grad: 24111.9102  LR: 0.000010  \n",
      "Epoch: [3][2500/3575] Elapsed 25m 14s (remain 10m 50s) Loss: 0.0273(0.0057) Grad: 20441.3652  LR: 0.000010  \n",
      "Epoch: [3][2600/3575] Elapsed 26m 12s (remain 9m 48s) Loss: 0.0092(0.0057) Grad: 47586.4766  LR: 0.000010  \n",
      "Epoch: [3][2700/3575] Elapsed 27m 10s (remain 8m 47s) Loss: 0.0022(0.0057) Grad: 2862.2107  LR: 0.000010  \n",
      "Epoch: [3][2800/3575] Elapsed 28m 9s (remain 7m 46s) Loss: 0.0006(0.0057) Grad: 1507.0710  LR: 0.000010  \n",
      "Epoch: [3][2900/3575] Elapsed 29m 8s (remain 6m 46s) Loss: 0.0036(0.0056) Grad: 7680.6982  LR: 0.000010  \n",
      "Epoch: [3][3000/3575] Elapsed 30m 10s (remain 5m 46s) Loss: 0.0003(0.0056) Grad: 808.7715  LR: 0.000010  \n",
      "Epoch: [3][3100/3575] Elapsed 31m 8s (remain 4m 45s) Loss: 0.0314(0.0056) Grad: 57201.6680  LR: 0.000009  \n",
      "Epoch: [3][3200/3575] Elapsed 32m 7s (remain 3m 45s) Loss: 0.0001(0.0056) Grad: 189.4576  LR: 0.000009  \n",
      "Epoch: [3][3300/3575] Elapsed 33m 7s (remain 2m 44s) Loss: 0.0075(0.0055) Grad: 10974.0029  LR: 0.000009  \n",
      "Epoch: [3][3400/3575] Elapsed 34m 8s (remain 1m 44s) Loss: 0.0088(0.0056) Grad: 30993.2910  LR: 0.000009  \n",
      "Epoch: [3][3500/3575] Elapsed 35m 8s (remain 0m 44s) Loss: 0.0060(0.0056) Grad: 12873.4824  LR: 0.000009  \n",
      "Epoch: [3][3574/3575] Elapsed 35m 51s (remain 0m 0s) Loss: 0.0000(0.0055) Grad: 5.7919  LR: 0.000009  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 9m 15s) Loss: 0.0002(0.0002) \n",
      "EVAL: [100/1192] Elapsed 0m 19s (remain 3m 27s) Loss: 0.0445(0.0081) \n",
      "EVAL: [200/1192] Elapsed 0m 37s (remain 3m 6s) Loss: 0.0055(0.0069) \n",
      "EVAL: [300/1192] Elapsed 0m 56s (remain 2m 46s) Loss: 0.0085(0.0072) \n",
      "EVAL: [400/1192] Elapsed 1m 14s (remain 2m 27s) Loss: 0.0000(0.0072) \n",
      "EVAL: [500/1192] Elapsed 1m 33s (remain 2m 8s) Loss: 0.0607(0.0071) \n",
      "EVAL: [600/1192] Elapsed 1m 52s (remain 1m 50s) Loss: 0.0168(0.0077) \n",
      "EVAL: [700/1192] Elapsed 2m 11s (remain 1m 32s) Loss: 0.0045(0.0087) \n",
      "EVAL: [800/1192] Elapsed 2m 30s (remain 1m 13s) Loss: 0.0090(0.0087) \n",
      "EVAL: [900/1192] Elapsed 2m 49s (remain 0m 54s) Loss: 0.0066(0.0088) \n",
      "EVAL: [1000/1192] Elapsed 3m 8s (remain 0m 35s) Loss: 0.0000(0.0086) \n",
      "EVAL: [1100/1192] Elapsed 3m 27s (remain 0m 17s) Loss: 0.0213(0.0083) \n",
      "EVAL: [1191/1192] Elapsed 3m 44s (remain 0m 0s) Loss: 0.0001(0.0081) \n",
      "Epoch 3 - avg_train_loss: 0.0055  avg_val_loss: 0.0081  time: 2379s\n",
      "Epoch 3 - Score: 0.8822\n",
      "Epoch 3 - Save Best Score: 0.8822 Model\n",
      "Epoch: [4][0/3575] Elapsed 0m 0s (remain 53m 29s) Loss: 0.0001(0.0001) Grad: 64.0042  LR: 0.000009  \n",
      "Epoch: [4][100/3575] Elapsed 0m 59s (remain 34m 19s) Loss: 0.0044(0.0060) Grad: 7024.5034  LR: 0.000009  \n",
      "Epoch: [4][200/3575] Elapsed 1m 58s (remain 33m 5s) Loss: 0.0059(0.0053) Grad: 4703.3413  LR: 0.000009  \n",
      "Epoch: [4][300/3575] Elapsed 2m 57s (remain 32m 15s) Loss: 0.0117(0.0055) Grad: 24989.0898  LR: 0.000009  \n",
      "Epoch: [4][400/3575] Elapsed 3m 58s (remain 31m 27s) Loss: 0.0001(0.0054) Grad: 151.1015  LR: 0.000008  \n",
      "Epoch: [4][500/3575] Elapsed 4m 56s (remain 30m 22s) Loss: 0.0001(0.0051) Grad: 109.7217  LR: 0.000008  \n",
      "Epoch: [4][600/3575] Elapsed 5m 55s (remain 29m 19s) Loss: 0.0111(0.0049) Grad: 70249.0859  LR: 0.000008  \n",
      "Epoch: [4][700/3575] Elapsed 6m 56s (remain 28m 27s) Loss: 0.0019(0.0046) Grad: 12189.4619  LR: 0.000008  \n",
      "Epoch: [4][800/3575] Elapsed 7m 55s (remain 27m 27s) Loss: 0.0092(0.0047) Grad: 13346.8701  LR: 0.000008  \n",
      "Epoch: [4][900/3575] Elapsed 8m 54s (remain 26m 25s) Loss: 0.0001(0.0047) Grad: 82.1025  LR: 0.000008  \n",
      "Epoch: [4][1000/3575] Elapsed 9m 52s (remain 25m 24s) Loss: 0.0022(0.0047) Grad: 9153.0830  LR: 0.000008  \n",
      "Epoch: [4][1100/3575] Elapsed 10m 51s (remain 24m 23s) Loss: 0.0001(0.0046) Grad: 121.1873  LR: 0.000008  \n",
      "Epoch: [4][1200/3575] Elapsed 11m 49s (remain 23m 22s) Loss: 0.0000(0.0046) Grad: 15.8769  LR: 0.000007  \n",
      "Epoch: [4][1300/3575] Elapsed 12m 47s (remain 22m 22s) Loss: 0.0006(0.0045) Grad: 8982.9307  LR: 0.000007  \n",
      "Epoch: [4][1400/3575] Elapsed 13m 46s (remain 21m 22s) Loss: 0.0001(0.0044) Grad: 1285.5890  LR: 0.000007  \n",
      "Epoch: [4][1500/3575] Elapsed 14m 45s (remain 20m 22s) Loss: 0.0010(0.0043) Grad: 2133.5527  LR: 0.000007  \n",
      "Epoch: [4][1600/3575] Elapsed 15m 45s (remain 19m 25s) Loss: 0.0096(0.0043) Grad: 131639.3750  LR: 0.000007  \n",
      "Epoch: [4][1700/3575] Elapsed 16m 44s (remain 18m 27s) Loss: 0.0001(0.0043) Grad: 230.4920  LR: 0.000007  \n",
      "Epoch: [4][1800/3575] Elapsed 17m 43s (remain 17m 27s) Loss: 0.0001(0.0043) Grad: 46.4644  LR: 0.000007  \n",
      "Epoch: [4][1900/3575] Elapsed 18m 41s (remain 16m 27s) Loss: 0.0019(0.0043) Grad: 12214.8330  LR: 0.000007  \n",
      "Epoch: [4][2000/3575] Elapsed 19m 40s (remain 15m 28s) Loss: 0.0002(0.0044) Grad: 272.2280  LR: 0.000006  \n",
      "Epoch: [4][2100/3575] Elapsed 20m 41s (remain 14m 31s) Loss: 0.0000(0.0044) Grad: 20.0054  LR: 0.000006  \n",
      "Epoch: [4][2200/3575] Elapsed 21m 39s (remain 13m 31s) Loss: 0.0002(0.0044) Grad: 307.9703  LR: 0.000006  \n",
      "Epoch: [4][2300/3575] Elapsed 22m 39s (remain 12m 32s) Loss: 0.0006(0.0045) Grad: 2213.3271  LR: 0.000006  \n",
      "Epoch: [4][2400/3575] Elapsed 23m 37s (remain 11m 33s) Loss: 0.0003(0.0044) Grad: 2722.7144  LR: 0.000006  \n",
      "Epoch: [4][2500/3575] Elapsed 24m 36s (remain 10m 33s) Loss: 0.0114(0.0044) Grad: 9008.1504  LR: 0.000006  \n",
      "Epoch: [4][2600/3575] Elapsed 25m 35s (remain 9m 35s) Loss: 0.0062(0.0044) Grad: 16684.3066  LR: 0.000006  \n",
      "Epoch: [4][2700/3575] Elapsed 26m 34s (remain 8m 36s) Loss: 0.0042(0.0044) Grad: 28090.2910  LR: 0.000006  \n",
      "Epoch: [4][2800/3575] Elapsed 27m 34s (remain 7m 37s) Loss: 0.0010(0.0044) Grad: 715.8835  LR: 0.000005  \n",
      "Epoch: [4][2900/3575] Elapsed 28m 32s (remain 6m 37s) Loss: 0.0004(0.0044) Grad: 1674.9409  LR: 0.000005  \n",
      "Epoch: [4][3000/3575] Elapsed 29m 30s (remain 5m 38s) Loss: 0.0055(0.0045) Grad: 42617.9414  LR: 0.000005  \n",
      "Epoch: [4][3100/3575] Elapsed 30m 28s (remain 4m 39s) Loss: 0.0105(0.0045) Grad: 31993.0703  LR: 0.000005  \n",
      "Epoch: [4][3200/3575] Elapsed 31m 29s (remain 3m 40s) Loss: 0.0025(0.0045) Grad: 10963.6328  LR: 0.000005  \n",
      "Epoch: [4][3300/3575] Elapsed 32m 27s (remain 2m 41s) Loss: 0.0226(0.0045) Grad: 84686.2422  LR: 0.000005  \n",
      "Epoch: [4][3400/3575] Elapsed 33m 26s (remain 1m 42s) Loss: 0.0001(0.0045) Grad: 183.3417  LR: 0.000005  \n",
      "Epoch: [4][3500/3575] Elapsed 34m 24s (remain 0m 43s) Loss: 0.0001(0.0045) Grad: 794.4347  LR: 0.000005  \n",
      "Epoch: [4][3574/3575] Elapsed 35m 9s (remain 0m 0s) Loss: 0.0007(0.0044) Grad: 5523.5947  LR: 0.000004  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 9m 48s) Loss: 0.0001(0.0001) \n",
      "EVAL: [100/1192] Elapsed 0m 19s (remain 3m 25s) Loss: 0.0485(0.0089) \n",
      "EVAL: [200/1192] Elapsed 0m 37s (remain 3m 5s) Loss: 0.0220(0.0080) \n",
      "EVAL: [300/1192] Elapsed 0m 56s (remain 2m 46s) Loss: 0.0092(0.0086) \n",
      "EVAL: [400/1192] Elapsed 1m 14s (remain 2m 27s) Loss: 0.0000(0.0084) \n",
      "EVAL: [500/1192] Elapsed 1m 33s (remain 2m 8s) Loss: 0.0667(0.0081) \n",
      "EVAL: [600/1192] Elapsed 1m 51s (remain 1m 50s) Loss: 0.0190(0.0086) \n",
      "EVAL: [700/1192] Elapsed 2m 10s (remain 1m 31s) Loss: 0.0039(0.0097) \n",
      "EVAL: [800/1192] Elapsed 2m 29s (remain 1m 12s) Loss: 0.0164(0.0096) \n",
      "EVAL: [900/1192] Elapsed 2m 48s (remain 0m 54s) Loss: 0.0059(0.0097) \n",
      "EVAL: [1000/1192] Elapsed 3m 10s (remain 0m 36s) Loss: 0.0000(0.0095) \n",
      "EVAL: [1100/1192] Elapsed 3m 29s (remain 0m 17s) Loss: 0.0242(0.0092) \n",
      "EVAL: [1191/1192] Elapsed 3m 45s (remain 0m 0s) Loss: 0.0001(0.0089) \n",
      "Epoch 4 - avg_train_loss: 0.0044  avg_val_loss: 0.0089  time: 2339s\n",
      "Epoch 4 - Score: 0.8822\n",
      "Epoch: [5][0/3575] Elapsed 0m 0s (remain 51m 9s) Loss: 0.0051(0.0051) Grad: 12857.5342  LR: 0.000004  \n",
      "Epoch: [5][100/3575] Elapsed 0m 59s (remain 34m 8s) Loss: 0.0022(0.0024) Grad: 10404.1660  LR: 0.000004  \n",
      "Epoch: [5][200/3575] Elapsed 1m 58s (remain 33m 1s) Loss: 0.0000(0.0027) Grad: 28.8034  LR: 0.000004  \n",
      "Epoch: [5][300/3575] Elapsed 2m 56s (remain 31m 58s) Loss: 0.0123(0.0030) Grad: 17603.1680  LR: 0.000004  \n",
      "Epoch: [5][400/3575] Elapsed 3m 55s (remain 31m 6s) Loss: 0.0144(0.0034) Grad: 16671.9648  LR: 0.000004  \n",
      "Epoch: [5][500/3575] Elapsed 4m 54s (remain 30m 8s) Loss: 0.0127(0.0033) Grad: 61902.4883  LR: 0.000004  \n",
      "Epoch: [5][600/3575] Elapsed 5m 53s (remain 29m 8s) Loss: 0.0036(0.0033) Grad: 30041.0488  LR: 0.000004  \n",
      "Epoch: [5][700/3575] Elapsed 6m 52s (remain 28m 11s) Loss: 0.0063(0.0033) Grad: 21196.3848  LR: 0.000004  \n",
      "Epoch: [5][800/3575] Elapsed 7m 53s (remain 27m 19s) Loss: 0.0001(0.0033) Grad: 354.1987  LR: 0.000003  \n",
      "Epoch: [5][900/3575] Elapsed 8m 52s (remain 26m 19s) Loss: 0.0000(0.0032) Grad: 24.1066  LR: 0.000003  \n",
      "Epoch: [5][1000/3575] Elapsed 9m 50s (remain 25m 18s) Loss: 0.0001(0.0034) Grad: 78.1468  LR: 0.000003  \n",
      "Epoch: [5][1100/3575] Elapsed 10m 49s (remain 24m 18s) Loss: 0.0045(0.0035) Grad: 22935.6699  LR: 0.000003  \n",
      "Epoch: [5][1200/3575] Elapsed 11m 47s (remain 23m 18s) Loss: 0.0012(0.0035) Grad: 36233.6992  LR: 0.000003  \n",
      "Epoch: [5][1300/3575] Elapsed 12m 46s (remain 22m 18s) Loss: 0.0028(0.0034) Grad: 8150.3657  LR: 0.000003  \n",
      "Epoch: [5][1400/3575] Elapsed 13m 45s (remain 21m 20s) Loss: 0.0097(0.0035) Grad: 17737.2656  LR: 0.000003  \n",
      "Epoch: [5][1500/3575] Elapsed 14m 44s (remain 20m 21s) Loss: 0.0002(0.0035) Grad: 2177.9666  LR: 0.000003  \n",
      "Epoch: [5][1600/3575] Elapsed 15m 42s (remain 19m 22s) Loss: 0.0096(0.0035) Grad: 8942.7832  LR: 0.000002  \n",
      "Epoch: [5][1700/3575] Elapsed 16m 40s (remain 18m 22s) Loss: 0.0001(0.0035) Grad: 1520.5037  LR: 0.000002  \n",
      "Epoch: [5][1800/3575] Elapsed 17m 41s (remain 17m 25s) Loss: 0.0001(0.0034) Grad: 741.1533  LR: 0.000002  \n",
      "Epoch: [5][1900/3575] Elapsed 18m 42s (remain 16m 28s) Loss: 0.0008(0.0034) Grad: 426.1412  LR: 0.000002  \n",
      "Epoch: [5][2000/3575] Elapsed 19m 44s (remain 15m 31s) Loss: 0.0013(0.0034) Grad: 7284.7148  LR: 0.000002  \n",
      "Epoch: [5][2100/3575] Elapsed 20m 43s (remain 14m 32s) Loss: 0.0002(0.0034) Grad: 1286.9972  LR: 0.000002  \n",
      "Epoch: [5][2200/3575] Elapsed 21m 41s (remain 13m 32s) Loss: 0.0139(0.0034) Grad: 8317.1289  LR: 0.000002  \n",
      "Epoch: [5][2300/3575] Elapsed 22m 40s (remain 12m 33s) Loss: 0.0001(0.0034) Grad: 37.5527  LR: 0.000002  \n",
      "Epoch: [5][2400/3575] Elapsed 23m 40s (remain 11m 34s) Loss: 0.0000(0.0035) Grad: 22.2517  LR: 0.000001  \n",
      "Epoch: [5][2500/3575] Elapsed 24m 42s (remain 10m 36s) Loss: 0.0173(0.0036) Grad: 30772.9238  LR: 0.000001  \n",
      "Epoch: [5][2600/3575] Elapsed 25m 41s (remain 9m 37s) Loss: 0.0000(0.0036) Grad: 85.1272  LR: 0.000001  \n",
      "Epoch: [5][2700/3575] Elapsed 26m 39s (remain 8m 37s) Loss: 0.0000(0.0036) Grad: 37.2410  LR: 0.000001  \n",
      "Epoch: [5][2800/3575] Elapsed 27m 38s (remain 7m 38s) Loss: 0.0001(0.0035) Grad: 64.4494  LR: 0.000001  \n",
      "Epoch: [5][2900/3575] Elapsed 28m 36s (remain 6m 38s) Loss: 0.0008(0.0036) Grad: 2027.2224  LR: 0.000001  \n",
      "Epoch: [5][3000/3575] Elapsed 29m 35s (remain 5m 39s) Loss: 0.0000(0.0036) Grad: 17.6115  LR: 0.000001  \n",
      "Epoch: [5][3100/3575] Elapsed 30m 35s (remain 4m 40s) Loss: 0.0019(0.0036) Grad: 20201.1680  LR: 0.000001  \n",
      "Epoch: [5][3200/3575] Elapsed 31m 33s (remain 3m 41s) Loss: 0.0000(0.0036) Grad: 116.4767  LR: 0.000000  \n",
      "Epoch: [5][3300/3575] Elapsed 32m 32s (remain 2m 42s) Loss: 0.0010(0.0037) Grad: 8683.1064  LR: 0.000000  \n",
      "Epoch: [5][3400/3575] Elapsed 33m 32s (remain 1m 42s) Loss: 0.0001(0.0036) Grad: 78.2454  LR: 0.000000  \n",
      "Epoch: [5][3500/3575] Elapsed 34m 31s (remain 0m 43s) Loss: 0.0050(0.0036) Grad: 26275.8711  LR: 0.000000  \n",
      "Epoch: [5][3574/3575] Elapsed 35m 14s (remain 0m 0s) Loss: 0.0001(0.0036) Grad: 43.6121  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 11m 37s) Loss: 0.0001(0.0001) \n",
      "EVAL: [100/1192] Elapsed 0m 19s (remain 3m 34s) Loss: 0.0430(0.0079) \n",
      "EVAL: [200/1192] Elapsed 0m 38s (remain 3m 10s) Loss: 0.0134(0.0074) \n",
      "EVAL: [300/1192] Elapsed 0m 57s (remain 2m 49s) Loss: 0.0101(0.0080) \n",
      "EVAL: [400/1192] Elapsed 1m 15s (remain 2m 29s) Loss: 0.0000(0.0079) \n",
      "EVAL: [500/1192] Elapsed 1m 35s (remain 2m 12s) Loss: 0.0658(0.0077) \n",
      "EVAL: [600/1192] Elapsed 1m 54s (remain 1m 52s) Loss: 0.0155(0.0082) \n",
      "EVAL: [700/1192] Elapsed 2m 13s (remain 1m 33s) Loss: 0.0041(0.0093) \n",
      "EVAL: [800/1192] Elapsed 2m 31s (remain 1m 14s) Loss: 0.0190(0.0092) \n",
      "EVAL: [900/1192] Elapsed 2m 50s (remain 0m 55s) Loss: 0.0112(0.0094) \n",
      "EVAL: [1000/1192] Elapsed 3m 9s (remain 0m 36s) Loss: 0.0000(0.0091) \n",
      "EVAL: [1100/1192] Elapsed 3m 28s (remain 0m 17s) Loss: 0.0234(0.0088) \n",
      "EVAL: [1191/1192] Elapsed 3m 45s (remain 0m 0s) Loss: 0.0001(0.0086) \n",
      "Epoch 5 - avg_train_loss: 0.0036  avg_val_loss: 0.0086  time: 2343s\n",
      "Epoch 5 - Score: 0.8853\n",
      "Epoch 5 - Save Best Score: 0.8853 Model\n",
      "Best thres: 0.5, Score: 0.8813\n",
      "Best thres: 0.4859375, Score: 0.8812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca8d463a0b1499a9f4f3bbc4b730bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-23-8e89234e65ba>\", line 41, in __getitem__\n    mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n  File \"<ipython-input-23-8e89234e65ba>\", line 32, in _create_mapping_from_token_to_char\n    mapping_from_token_to_char = np.zeros(self.max_char_len)\nAttributeError: 'TestDataset' object has no attribute 'max_char_len'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-0f6b8dbff6ab>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mtest_token_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"fold{i_fold}_{i}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_token_probs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mtest_char_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_char_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pn_history\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_token_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-ef7a3b9ca97f>\u001b[0m in \u001b[0;36minference_fn\u001b[0;34m(test_dataloader, model, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtk0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappings_from_token_to_char\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtk0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-23-8e89234e65ba>\", line 41, in __getitem__\n    mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n  File \"<ipython-input-23-8e89234e65ba>\", line 32, in _create_mapping_from_token_to_char\n    mapping_from_token_to_char = np.zeros(self.max_char_len)\nAttributeError: 'TestDataset' object has no attribute 'max_char_len'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "name": "nbme-exp043.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 641.572862,
   "end_time": "2022-02-27T11:39:50.972497",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-27T11:29:09.399635",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0b503e832e57492291cbf6e9ae66e343": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0c70395dca6341349fc883dc6b95090a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a92171cb3d34fc8b1ed5119e32951ac",
      "max": 42146,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a202501b76a748fe80180604dff32c7b",
      "value": 42146
     }
    },
    "0dfe4d3aa0354d95bb5d019420b510a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3d9ac191d9b4772ae4bca323a34ddd7",
      "placeholder": "​",
      "style": "IPY_MODEL_9deae6afbd4740df8eb0289bc5f53ae7",
      "value": " 42146/42146 [00:35&lt;00:00, 2027.81it/s]"
     }
    },
    "14efac00edd349898e9fa95a63a2773d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f6c8df95c7845818253189f2e365ba9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33e4f48f7c8f40f48081c34bc992f215": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40ae5d1c9e9f4feaa4207599ef17a1ec",
      "placeholder": "​",
      "style": "IPY_MODEL_8cd3352e5e9342e4a814e9958ea6dc1d",
      "value": "100%"
     }
    },
    "40ae5d1c9e9f4feaa4207599ef17a1ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "47e8d82f628f4bd7b8b0ef0aa96852a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cc9a4ba21d664dafb8ae32a4e0a1c5e0",
       "IPY_MODEL_f4421d3b3a844dbcb003f11902ee1898",
       "IPY_MODEL_d02f186539954463873bb560b775894e"
      ],
      "layout": "IPY_MODEL_14efac00edd349898e9fa95a63a2773d"
     }
    },
    "5a92171cb3d34fc8b1ed5119e32951ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e29db6be6284caa978ee223047f23c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "625abc68d2fb4fd4b8556c7cc1ae514a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83d1b90076dc431893e0ab1c87e35f9c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c7eb508782d4253af8cbff0a39e5d19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_33e4f48f7c8f40f48081c34bc992f215",
       "IPY_MODEL_0c70395dca6341349fc883dc6b95090a",
       "IPY_MODEL_0dfe4d3aa0354d95bb5d019420b510a9"
      ],
      "layout": "IPY_MODEL_e6775e6fc2ad4b14b473b8a07e27426d"
     }
    },
    "8cd3352e5e9342e4a814e9958ea6dc1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9deae6afbd4740df8eb0289bc5f53ae7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a202501b76a748fe80180604dff32c7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c3d9ac191d9b4772ae4bca323a34ddd7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc9a4ba21d664dafb8ae32a4e0a1c5e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83d1b90076dc431893e0ab1c87e35f9c",
      "placeholder": "​",
      "style": "IPY_MODEL_0b503e832e57492291cbf6e9ae66e343",
      "value": "100%"
     }
    },
    "d02f186539954463873bb560b775894e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f6c8df95c7845818253189f2e365ba9",
      "placeholder": "​",
      "style": "IPY_MODEL_5e29db6be6284caa978ee223047f23c5",
      "value": " 143/143 [00:00&lt;00:00, 2361.05it/s]"
     }
    },
    "d2581946e9fd4f9a8dc56b3453cc70bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e6775e6fc2ad4b14b473b8a07e27426d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4421d3b3a844dbcb003f11902ee1898": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_625abc68d2fb4fd4b8556c7cc1ae514a",
      "max": 143,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d2581946e9fd4f9a8dc56b3453cc70bd",
      "value": 143
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
