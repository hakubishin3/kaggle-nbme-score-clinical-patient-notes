{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "technological-option",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-messaging",
   "metadata": {},
   "source": [
    "- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-sussex",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "terminal-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"nbme-exp001\"\n",
    "ENV = \"local\"\n",
    "DEBUG_MODE = False\n",
    "SUBMISSION_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "material-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    apex=True\n",
    "    input_dir=None\n",
    "    output_dir=None\n",
    "    #env=\"colab\"  # [\"kaggle\", \"colab\"]\n",
    "    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n",
    "    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n",
    "    competition_name=\"nbme-score-clinical-patient-notes\"\n",
    "    id_col=\"id\"\n",
    "    target_col=\"location\"\n",
    "    pretrained_model_name=\"microsoft/deberta-base\"\n",
    "    tokenizer=None\n",
    "    max_len=None\n",
    "    output_dim=1\n",
    "    dropout=0.2\n",
    "    num_workers=4\n",
    "    batch_size=16\n",
    "    lr=2e-5\n",
    "    weight_decay=0.1\n",
    "    num_warmup_steps_rate=0.1\n",
    "    batch_scheduler=True\n",
    "    epochs=5\n",
    "    n_fold=5\n",
    "    train_fold=[0, 1, 2, 3, 4]\n",
    "    seed=71\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=1000\n",
    "    print_freq=100\n",
    "    train=False\n",
    "    inference=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "distinct-edward",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG_MODE:\n",
    "    CFG.epochs = 2\n",
    "    CFG.train_fold = [0]\n",
    "\n",
    "if SUBMISSION_MODE:\n",
    "    CFG.train = False\n",
    "    CFG.inference = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-straight",
   "metadata": {},
   "source": [
    "## Directory Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "unavailable-documentation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "print(ENV)\n",
    "if ENV == \"colab\":\n",
    "    # colab環境\n",
    "    CFG.env = \"colab\"\n",
    "    from google.colab import drive\n",
    "    drive._mount('/content/drive')\n",
    "    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/petfinder2/input/petfinder-pawpularity-score/\")\n",
    "    CFG.train_image_dir = CFG.input_dir / \"train\"\n",
    "    CFG.test_image_dir = CFG.input_dir / \"test\"\n",
    "    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/petfinder2/output/\") / EXP_NAME\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "elif ENV == \"local\":\n",
    "    # ローカルサーバ\n",
    "    CFG.env = \"local\"\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / EXP_NAME\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "elif ENV == \"kaggle\":\n",
    "    # kaggle環境\n",
    "    CFG.env = \"kaggle\"\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.train_image_dir = CFG.input_dir / \"train\"\n",
    "    CFG.test_image_dir = CFG.input_dir / \"test\"\n",
    "    CFG.output_dir = Path(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "developing-latino",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import BartModel,BertModel,BertTokenizer\n",
    "from transformers import DebertaModel,DebertaTokenizer\n",
    "from transformers import RobertaModel,RobertaTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-relief",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "proved-capability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-particle",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "disabled-attempt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 6), (143, 3), (42146, 3), (5, 4))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(CFG.input_dir / \"train.csv\")\n",
    "features = pd.read_csv(CFG.input_dir / \"features.csv\")\n",
    "patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n",
    "test = pd.read_csv(CFG.input_dir / \"test.csv\")\n",
    "train.shape, features.shape, patient_notes.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "sitting-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG_MODE:\n",
    "    train = train.sample(n=1000, random_state=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-suicide",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "supposed-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(features):\n",
    "    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "\n",
    "\n",
    "features = preprocess_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "inappropriate-tobacco",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 8), (5, 6))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "laughing-strain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_anno(df, target_id, annotation, location):\n",
    "    idx = df[\"id\"] == target_id\n",
    "    df.loc[idx, \"annotation\"] = annotation\n",
    "    df.loc[idx, \"location\"] = location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "square-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_anno(train, \"00669_000\", \"['father heart attack']\", \"['764 783']\")\n",
    "fix_anno(train, \"01110_010\", \"['for the last 2-3 months', 'over the last 2 months']\", \"['77 100', '398 420']\")\n",
    "fix_anno(train, \"01146_005\", \"['no heat intolerance', 'no cold intolerance']\", \"['285 292;301 312', '285 287;296 312']\")\n",
    "fix_anno(train, \"02428_001\", \"['mother thyroid problem']\", \"['551 557;565 580']\")\n",
    "fix_anno(train, \"02428_004\", '[\\'felt like he was going to \"pass out\"\\']', \"['131 135;181 212']\")\n",
    "fix_anno(train, \"10047_105\", \"['stool , with no blood']\", \"['259 280']\")\n",
    "fix_anno(train, \"10196_105\", \"['diarrhoe non blooody']\", \"['176 184;201 212']\")\n",
    "fix_anno(train, \"10206_103\", \"['diarrhea for last 2-3 days']\", \"['249 257;271 288']\")\n",
    "fix_anno(train, \"10228_100\", \"['no vaginal discharge']\", \"['822 824;907 924']\")\n",
    "fix_anno(train, \"10268_111\", \"['started about 8-10 hours ago']\", \"['101 129']\")\n",
    "fix_anno(train, \"10459_105\", \"['no blood in the stool']\", \"['531 539;549 561']\")\n",
    "fix_anno(train, \"10620_102\", \"['last sexually active 9 months ago']\", \"['540 560;581 593']\")\n",
    "fix_anno(train, \"10646_107\", \"['right lower quadrant pain']\", \"['32 57']\")\n",
    "fix_anno(train, \"10968_105\", \"['diarrhoea no blood']\", \"['308 317;376 384']\")\n",
    "fix_anno(train, \"20747_214\", \"['sweating']\", \"['549 557']\")\n",
    "fix_anno(\n",
    "    train,\n",
    "    \"21686_200\",\n",
    "    \"['previously as regular', 'previously eveyr 28-29 days', 'previously lasting 5 days', 'previously regular flow']\",\n",
    "    \"['102 123', '102 112;125 141', '102 112;143 157', '102 112;159 171']\",\n",
    ")\n",
    "fix_anno(train, \"30437_309\", \"['for 2 months']\", \"['33 45']\")\n",
    "fix_anno(train, \"32657_315\", \"['35 year old']\", \"['5 16']\")\n",
    "fix_anno(train, \"32996_302\", \"['darker brown stools']\", \"['175 194']\")\n",
    "fix_anno(train, \"33531_300\", \"['uncle with peptic ulcer']\", \"['700 723']\")\n",
    "fix_anno(train, \"40974_406\", \"['difficulty falling asleep']\", \"['225 250']\")\n",
    "fix_anno(train, \"41825_402\", \"['helps to take care of aging mother and in-laws']\", \"['197 218;236 260']\")\n",
    "fix_anno(\n",
    "    train,\n",
    "    \"42625_400\",\n",
    "    \"['No hair changes', 'No skin changes', 'No GI changes', 'No palpitations', 'No excessive sweating']\",\n",
    "    \"['480 482;507 519', '480 482;499 503;512 519', '480 482;521 531', '480 482;533 545', '480 482;564 582']\",\n",
    ")\n",
    "fix_anno(\n",
    "    train,\n",
    "    \"43451_402\",\n",
    "    \"['stressed due to taking care of her mother', 'stressed due to taking care of husbands parents']\",\n",
    "    \"['290 320;327 337', '290 320;342 358']\",\n",
    ")\n",
    "fix_anno(train, \"44958_402\", \"['stressor taking care of many sick family members']\", \"['288 296;324 363']\")\n",
    "fix_anno(train, \"50574_514\", \"['heart started racing and felt numbness for the 1st time in her finger tips']\", \"['108 182']\")\n",
    "fix_anno(train, \"52512_500\", \"['first started 5 yrs']\", \"['102 121']\")\n",
    "fix_anno(train, \"60235_608\", \"['No shortness of breath']\", \"['481 483;533 552']\")\n",
    "fix_anno(train, \"60469_603\", \"['recent URI', 'nasal stuffines, rhinorrhea, for 3-4 days']\", \"['92 102', '123 164']\")\n",
    "fix_anno(\n",
    "    train,\n",
    "    \"70255_702\",\n",
    "    \"['irregularity with her cycles', 'heavier bleeding', 'changes her pad every couple hours']\",\n",
    "    \"['89 117', '122 138', '368 402']\",\n",
    ")\n",
    "fix_anno(train, \"70412_701\", \"['gaining 10-15 lbs']\", \"['344 361']\")\n",
    "fix_anno(train, \"72660_701\", \"['weight gain', 'gain of 10-16lbs']\", \"['600 611', '607 623']\")\n",
    "fix_anno(train, \"81856_813\", \"['seeing her son knows are not real']\", \"['386 400;443 461']\")\n",
    "fix_anno(train, \"81985_813\", \"['saw him once in the kitchen after he died']\", \"['160 201']\")\n",
    "fix_anno(train, \"83199_810\", \"['tried Ambien but it didnt work']\", \"['325 337;349 366']\")\n",
    "fix_anno(train, \"83757_803\", \"['heard what she described as a party later than evening these things did not actually happen']\", \"['405 459;488 524']\")\n",
    "fix_anno(train, \"83757_813\", \"['experienced seeing her son at the kitchen table these things did not actually happen']\", \"['353 400;488 524']\")\n",
    "fix_anno(train, \"92224_909\", \"['SCRACHY THROAT', 'RUNNY NOSE']\", \"['293 307', '321 331']\")\n",
    "fix_anno(train, \"92385_900\", \"['without improvement when taking tylenol', 'without improvement when taking ibuprofen']\", \"['182 221', '182 213;225 234']\")\n",
    "fix_anno(train, \"92385_902\", \"['yesterday', 'yesterday']\", \"['79 88', '409 418']\")\n",
    "fix_anno(train, \"93988_904\", \"['headache global', 'headache throughout her head']\", \"['86 94;230 236', '86 94;237 256']\")\n",
    "fix_anno(train, \"94656_904\", \"['headache generalized in her head']\", \"['56 64;156 179']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "equal-barbados",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n",
    "train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "binding-qatar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4399\n",
       "1    8184\n",
       "2    1293\n",
       "3     287\n",
       "4      99\n",
       "5      27\n",
       "6       9\n",
       "7       1\n",
       "8       1\n",
       "Name: annotation_length, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n",
    "display(train['annotation_length'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-stretch",
   "metadata": {},
   "source": [
    "## CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "unlike-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_groupkfold(df, group_name):\n",
    "    groups = df[group_name].unique()\n",
    "\n",
    "    kf = KFold(\n",
    "        n_splits=CFG.n_fold,\n",
    "        shuffle=True,\n",
    "        random_state=CFG.seed,\n",
    "    )\n",
    "    folds_ids = []\n",
    "    for i_fold, (_, val_group_idx) in enumerate(kf.split(groups)):\n",
    "        val_group = groups[val_group_idx]\n",
    "        is_val = df[group_name].isin(val_group)\n",
    "        val_idx = df[is_val].index\n",
    "        df.loc[val_idx, \"fold\"] = int(i_fold)\n",
    "\n",
    "    df[\"fold\"] = df[\"fold\"].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "composite-spencer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    2902\n",
       "1    2894\n",
       "2    2813\n",
       "3    2791\n",
       "4    2900\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = get_groupkfold(train, \"pn_num\")\n",
    "display(train.groupby(\"fold\").size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-guard",
   "metadata": {},
   "source": [
    "## Setup tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "raised-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SUBMISSION_MODE:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Path(\"../input/\") / EXP_NAME / \"tokenizer/\")\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG.pretrained_model_name)\n",
    "    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n",
    "\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-utilization",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "creative-springer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d547b0127b4d9e8c91474a98ac292f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 433\n"
     ]
    }
   ],
   "source": [
    "pn_history_lengths = []\n",
    "tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    pn_history_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(pn_history_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cognitive-scotland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620e4b7c781249bda6ebb06fe75d60cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 30\n"
     ]
    }
   ],
   "source": [
    "feature_text_lengths = []\n",
    "tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    feature_text_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(feature_text_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "active-intranet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 466\n"
     ]
    }
   ],
   "source": [
    "CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n",
    "print(\"max length:\", CFG.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "personal-vertical",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "        self.annotation_lengths = self.df[\"annotation_length\"].values\n",
    "        self.locations = self.df[\"location\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def _create_label(self, pn_history, annotation_length, location_list):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        offset_mapping = encoded[\"offset_mapping\"]\n",
    "        ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n",
    "        label = np.zeros(len(offset_mapping))\n",
    "        label[ignore_idxes] = -1\n",
    "\n",
    "        if annotation_length > 0:\n",
    "            for location in location_list:\n",
    "                for loc in [s.split() for s in location.split(\";\")]:\n",
    "                    start, end = int(loc[0]), int(loc[1])\n",
    "                    start_idx = -1\n",
    "                    end_idx = -1\n",
    "                    for idx in range(len(offset_mapping)):\n",
    "                        if (start_idx == -1) & (start < offset_mapping[idx][0]):\n",
    "                            start_idx = idx - 1\n",
    "                        if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n",
    "                            end_idx = idx + 1\n",
    "                    if start_idx == -1:\n",
    "                        start_idx = end_idx\n",
    "                    if (start_idx != -1) & (end_idx != -1):\n",
    "                        label[start_idx:end_idx] = 1\n",
    "\n",
    "        return torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n",
    "        return input_, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "illegal-environment",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        return input_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-investor",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "reliable-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, model_config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if model_config_path is None:\n",
    "            self.model_config = AutoConfig.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "        else:\n",
    "            self.model_config = torch.load(model_config_path)\n",
    "\n",
    "        if pretrained:\n",
    "            self.backbone = AutoModel.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                config=self.model_config,\n",
    "            )\n",
    "        else:\n",
    "            self.backbone = AutoModel.from_config(self.model_config)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(self.cfg.dropout),\n",
    "            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h = self.backbone(**inputs)[\"last_hidden_state\"]\n",
    "        output = self.fc(h)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-apartment",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bright-wheel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels_for_scoring(df):\n",
    "    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n",
    "    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, \"location\"]\n",
    "        if lst:\n",
    "            new_lst = \";\".join(lst)\n",
    "            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n",
    "\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df[\"location_for_create_labels\"].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, predictions, tokenizer):\n",
    "    res = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(\n",
    "            text=text,\n",
    "            max_length=CFG.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n",
    "            start, end = offset_mapping\n",
    "            res[i][start:end] = pred\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_results(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def get_result(df):\n",
    "    labels = create_labels_for_scoring(df)\n",
    "    predictions = df[[i for i in range(CFG.max_len)]].values\n",
    "    char_probs = get_char_probs(df[\"pn_history\"].values, predictions, CFG.tokenizer)\n",
    "    results = get_results(char_probs, th=0.5)\n",
    "    preds = get_predictions(results)\n",
    "    score = get_score(labels, preds)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "tight-caution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    train_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    epoch,\n",
    "    scheduler,\n",
    "    device,\n",
    "):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels) in enumerate(train_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            output = model(inputs)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if CFG.batch_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n",
    "            print(\n",
    "                \"Epoch: [{0}][{1}/{2}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                \"Grad: {grad_norm:.4f}  \"\n",
    "                \"LR: {lr:.6f}  \"\n",
    "                .format(\n",
    "                    epoch+1,\n",
    "                    step,\n",
    "                    len(train_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n",
    "                    loss=losses,\n",
    "                     grad_norm=grad_norm,\n",
    "                     lr=scheduler.get_lr()[0],\n",
    "                )\n",
    "            )\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "thrown-dollar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fn(\n",
    "    val_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    device,\n",
    "):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels) in enumerate(val_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(output.sigmoid().squeeze().detach().cpu().numpy())\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n",
    "            print(\n",
    "                \"EVAL: [{0}/{1}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                .format(\n",
    "                    step, len(val_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n",
    "                    loss=losses,\n",
    "                )\n",
    "            )\n",
    "    preds = np.concatenate(preds)\n",
    "    return losses.avg, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "forbidden-regulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_fn(test_dataloader, model, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preds = []\n",
    "    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs)\n",
    "        preds.append(output.sigmoid().squeeze().detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "charming-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(df, i_fold, device):\n",
    "    print(f\"========== fold: {i_fold} training ==========\")\n",
    "    train_idx = df[df[\"fold\"] != i_fold].index\n",
    "    val_idx = df[df[\"fold\"] == i_fold].index\n",
    "\n",
    "    train_folds = df.loc[train_idx].reset_index(drop=True)\n",
    "    val_folds = df.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = TrainingDataset(CFG, train_folds)\n",
    "    val_dataset = TrainingDataset(CFG, val_folds)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n",
    "    model.to(device)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in param_optimizer if not any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n",
    "        {\"params\": [p for n, p in param_optimizer if any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=CFG.lr,\n",
    "        betas=(0.9, 0.98),\n",
    "        weight_decay=CFG.weight_decay,\n",
    "    )\n",
    "    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n",
    "    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_train_optimization_steps,\n",
    "    )\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    best_score = -1 * np.inf\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "        avg_loss = train_fn(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            scheduler,\n",
    "            device,\n",
    "        )\n",
    "        avg_val_loss, preds = valid_fn(\n",
    "            val_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "\n",
    "        # scoring\n",
    "        val_folds[[i for i in range(CFG.max_len)]] = preds\n",
    "        score = get_result(val_folds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n",
    "        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"predictions\": preds,\n",
    "                },\n",
    "                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "            )\n",
    "\n",
    "    predictions = torch.load(\n",
    "        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )[\"predictions\"]\n",
    "    val_folds[[i for i in range(CFG.max_len)]] = preds\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return val_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-insight",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "young-married",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for i_fold in range(CFG.n_fold):\n",
    "            if i_fold in CFG.train_fold:\n",
    "                _oof_df = train_loop(train, i_fold, device)\n",
    "                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n",
    "        oof_df.to_csv(CFG.output_dir / \"oof_df.csv\", index=False)\n",
    "        score = get_result(oof_df)\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "\n",
    "    if CFG.inference:\n",
    "        test_dataset = TestDataset(CFG, test)\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=CFG.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=CFG.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        predictions = []\n",
    "        for i_fold in CFG.train_fold:\n",
    "            if SUBMISSION_MODE:\n",
    "                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / EXP_NAME / \"model_config.pth\", pretrained=False)\n",
    "                path = Path(\"../input/\") / EXP_NAME / f\"fold{i_fold}_best.pth\"\n",
    "            else:\n",
    "                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n",
    "            state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "            model.load_state_dict(state[\"model\"])\n",
    "            prediction = inference_fn(test_dataloader, model, device)\n",
    "            test_char_probs = get_char_probs(test[\"pn_history\"].values, prediction, CFG.tokenizer)\n",
    "            predictions.append(test_char_probs)\n",
    "            del state, prediction; gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        predictions = np.mean(predictions, axis=0)\n",
    "        best_th = 0.5\n",
    "        results = get_results(predictions, th=best_th)\n",
    "        test[CFG.target_col] = results\n",
    "        test[[CFG.id_col, CFG.target_col]].to_csv(\n",
    "            CFG.output_dir / \"submission.csv\", index=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "tribal-necessity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0ab062077746fca3859ae82b80609c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee090191a47e457dbe13d5edd90d54f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cca42d0430a4bba93b92938d668b17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e0acef868c4bb5ac459e27f1432b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6108f6bb7341d3a5536519b0d8b151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-ivory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-flash",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-cradle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-purpose",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "corrected-institute",
   "metadata": {},
   "source": [
    "========== fold: 0 training ==========\n",
    "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']\n",
    "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "Epoch: [1][0/712] Elapsed 0m 1s (remain 16m 12s) Loss: 0.7798(0.7798) Grad: inf  LR: 0.000000  \n",
    "Epoch: [1][100/712] Elapsed 0m 51s (remain 5m 14s) Loss: 0.0757(0.2923) Grad: 1366.5475  LR: 0.000006  \n",
    "Epoch: [1][200/712] Elapsed 1m 42s (remain 4m 21s) Loss: 0.0270(0.1687) Grad: 1315.1006  LR: 0.000011  \n",
    "Epoch: [1][300/712] Elapsed 2m 33s (remain 3m 29s) Loss: 0.0225(0.1229) Grad: 2152.5745  LR: 0.000017  \n",
    "Epoch: [1][400/712] Elapsed 3m 24s (remain 2m 38s) Loss: 0.0192(0.0984) Grad: 1430.9381  LR: 0.000020  \n",
    "Epoch: [1][500/712] Elapsed 4m 15s (remain 1m 47s) Loss: 0.0432(0.0832) Grad: 4269.2373  LR: 0.000019  \n",
    "Epoch: [1][600/712] Elapsed 5m 6s (remain 0m 56s) Loss: 0.0085(0.0726) Grad: 934.1167  LR: 0.000018  \n",
    "Epoch: [1][700/712] Elapsed 5m 58s (remain 0m 5s) Loss: 0.0137(0.0649) Grad: 1024.1807  LR: 0.000018  \n",
    "Epoch: [1][711/712] Elapsed 6m 3s (remain 0m 0s) Loss: 0.0100(0.0641) Grad: 1044.4680  LR: 0.000018  \n",
    "EVAL: [0/182] Elapsed 0m 0s (remain 1m 42s) Loss: 0.0053(0.0053) \n",
    "EVAL: [100/182] Elapsed 0m 26s (remain 0m 21s) Loss: 0.0186(0.0163) \n",
    "EVAL: [181/182] Elapsed 0m 46s (remain 0m 0s) Loss: 0.0031(0.0147) \n",
    "Epoch 1 - avg_train_loss: 0.0641  avg_val_loss: 0.0147  time: 416s\n",
    "Epoch 1 - Score: 0.8176\n",
    "Epoch 1 - Save Best Score: 0.8176 Model\n",
    "Epoch: [2][0/712] Elapsed 0m 0s (remain 9m 45s) Loss: 0.0081(0.0081) Grad: 13977.5127  LR: 0.000018  \n",
    "Epoch: [2][100/712] Elapsed 0m 51s (remain 5m 14s) Loss: 0.0102(0.0141) Grad: 16649.7344  LR: 0.000017  \n",
    "Epoch: [2][200/712] Elapsed 1m 43s (remain 4m 22s) Loss: 0.0124(0.0135) Grad: 17529.8496  LR: 0.000017  \n",
    "Epoch: [2][300/712] Elapsed 2m 34s (remain 3m 30s) Loss: 0.0140(0.0136) Grad: 23777.1445  LR: 0.000016  \n",
    "Epoch: [2][400/712] Elapsed 3m 26s (remain 2m 40s) Loss: 0.0186(0.0132) Grad: 37763.7578  LR: 0.000015  \n",
    "Epoch: [2][500/712] Elapsed 4m 18s (remain 1m 48s) Loss: 0.0090(0.0130) Grad: 34108.5156  LR: 0.000015  \n",
    "Epoch: [2][600/712] Elapsed 5m 9s (remain 0m 57s) Loss: 0.0150(0.0129) Grad: 42035.9922  LR: 0.000014  \n",
    "Epoch: [2][700/712] Elapsed 6m 1s (remain 0m 5s) Loss: 0.0100(0.0129) Grad: 29793.0078  LR: 0.000013  \n",
    "Epoch: [2][711/712] Elapsed 6m 7s (remain 0m 0s) Loss: 0.0115(0.0130) Grad: 24105.8145  LR: 0.000013  \n",
    "EVAL: [0/182] Elapsed 0m 0s (remain 1m 44s) Loss: 0.0080(0.0080) \n",
    "EVAL: [100/182] Elapsed 0m 26s (remain 0m 21s) Loss: 0.0124(0.0137) \n",
    "EVAL: [181/182] Elapsed 0m 46s (remain 0m 0s) Loss: 0.0013(0.0124) \n",
    "Epoch 2 - avg_train_loss: 0.0130  avg_val_loss: 0.0124  time: 419s\n",
    "Epoch 2 - Score: 0.8480\n",
    "Epoch 2 - Save Best Score: 0.8480 Model\n",
    "Epoch: [3][0/712] Elapsed 0m 0s (remain 10m 26s) Loss: 0.0111(0.0111) Grad: 52333.1719  LR: 0.000013  \n",
    "Epoch: [3][100/712] Elapsed 0m 52s (remain 5m 14s) Loss: 0.0068(0.0100) Grad: 20206.5234  LR: 0.000013  \n",
    "Epoch: [3][200/712] Elapsed 1m 43s (remain 4m 22s) Loss: 0.0013(0.0102) Grad: 8466.7520  LR: 0.000012  \n",
    "Epoch: [3][300/712] Elapsed 2m 34s (remain 3m 30s) Loss: 0.0148(0.0102) Grad: 29660.7559  LR: 0.000011  \n",
    "Epoch: [3][400/712] Elapsed 3m 25s (remain 2m 39s) Loss: 0.0051(0.0105) Grad: 7590.0947  LR: 0.000011  \n",
    "Epoch: [3][500/712] Elapsed 4m 16s (remain 1m 47s) Loss: 0.0083(0.0104) Grad: 15675.9365  LR: 0.000010  \n",
    "Epoch: [3][600/712] Elapsed 5m 7s (remain 0m 56s) Loss: 0.0055(0.0103) Grad: 16806.9258  LR: 0.000010  \n",
    "Epoch: [3][700/712] Elapsed 5m 58s (remain 0m 5s) Loss: 0.0081(0.0102) Grad: 19197.1680  LR: 0.000009  \n",
    "Epoch: [3][711/712] Elapsed 6m 3s (remain 0m 0s) Loss: 0.0050(0.0102) Grad: 21164.6172  LR: 0.000009  \n",
    "EVAL: [0/182] Elapsed 0m 0s (remain 1m 44s) Loss: 0.0036(0.0036) \n",
    "EVAL: [100/182] Elapsed 0m 26s (remain 0m 21s) Loss: 0.0133(0.0138) \n",
    "EVAL: [181/182] Elapsed 0m 47s (remain 0m 0s) Loss: 0.0008(0.0121) \n",
    "Epoch 3 - avg_train_loss: 0.0102  avg_val_loss: 0.0121  time: 416s\n",
    "Epoch 3 - Score: 0.8602\n",
    "Epoch 3 - Save Best Score: 0.8602 Model\n",
    "Epoch: [4][0/712] Elapsed 0m 0s (remain 10m 20s) Loss: 0.0200(0.0200) Grad: 34881.4219  LR: 0.000009  \n",
    "Epoch: [4][100/712] Elapsed 0m 52s (remain 5m 14s) Loss: 0.0047(0.0082) Grad: 11323.3809  LR: 0.000008  \n",
    "Epoch: [4][200/712] Elapsed 1m 43s (remain 4m 22s) Loss: 0.0061(0.0081) Grad: 15933.8809  LR: 0.000008  \n",
    "Epoch: [4][300/712] Elapsed 2m 34s (remain 3m 31s) Loss: 0.0027(0.0081) Grad: 7276.8257  LR: 0.000007  \n",
    "Epoch: [4][400/712] Elapsed 3m 26s (remain 2m 40s) Loss: 0.0016(0.0080) Grad: 8965.0859  LR: 0.000006  \n",
    "Epoch: [4][500/712] Elapsed 4m 18s (remain 1m 48s) Loss: 0.0024(0.0079) Grad: 16135.4619  LR: 0.000006  \n",
    "Epoch: [4][600/712] Elapsed 5m 9s (remain 0m 57s) Loss: 0.0061(0.0080) Grad: 10176.5928  LR: 0.000005  \n",
    "Epoch: [4][700/712] Elapsed 6m 1s (remain 0m 5s) Loss: 0.0058(0.0080) Grad: 16652.6504  LR: 0.000005  \n",
    "Epoch: [4][711/712] Elapsed 6m 7s (remain 0m 0s) Loss: 0.0101(0.0080) Grad: 18709.5957  LR: 0.000004  \n",
    "EVAL: [0/182] Elapsed 0m 0s (remain 1m 42s) Loss: 0.0032(0.0032) \n",
    "EVAL: [100/182] Elapsed 0m 26s (remain 0m 21s) Loss: 0.0121(0.0136) \n",
    "EVAL: [181/182] Elapsed 0m 46s (remain 0m 0s) Loss: 0.0012(0.0121) \n",
    "Epoch 4 - avg_train_loss: 0.0080  avg_val_loss: 0.0121  time: 419s\n",
    "Epoch 4 - Score: 0.8647\n",
    "Epoch 4 - Save Best Score: 0.8647 Model\n",
    "Epoch: [5][0/712] Elapsed 0m 0s (remain 10m 30s) Loss: 0.0029(0.0029) Grad: 5419.8066  LR: 0.000004  \n",
    "Epoch: [5][100/712] Elapsed 0m 52s (remain 5m 17s) Loss: 0.0143(0.0071) Grad: 31381.6836  LR: 0.000004  \n",
    "Epoch: [5][200/712] Elapsed 1m 44s (remain 4m 24s) Loss: 0.0041(0.0066) Grad: 22125.6816  LR: 0.000003  \n",
    "Epoch: [5][300/712] Elapsed 2m 35s (remain 3m 32s) Loss: 0.0035(0.0068) Grad: 11048.9355  LR: 0.000003  \n",
    "Epoch: [5][400/712] Elapsed 3m 26s (remain 2m 40s) Loss: 0.0025(0.0068) Grad: 11144.6133  LR: 0.000002  \n",
    "Epoch: [5][500/712] Elapsed 4m 17s (remain 1m 48s) Loss: 0.0170(0.0067) Grad: 44204.5703  LR: 0.000001  \n",
    "Epoch: [5][600/712] Elapsed 5m 9s (remain 0m 57s) Loss: 0.0080(0.0068) Grad: 24316.0000  LR: 0.000001  \n",
    "Epoch: [5][700/712] Elapsed 6m 0s (remain 0m 5s) Loss: 0.0157(0.0067) Grad: 25459.1484  LR: 0.000000  \n",
    "Epoch: [5][711/712] Elapsed 6m 6s (remain 0m 0s) Loss: 0.0063(0.0067) Grad: 13977.8398  LR: 0.000000  \n",
    "EVAL: [0/182] Elapsed 0m 0s (remain 1m 42s) Loss: 0.0026(0.0026) \n",
    "EVAL: [100/182] Elapsed 0m 26s (remain 0m 21s) Loss: 0.0124(0.0141) \n",
    "EVAL: [181/182] Elapsed 0m 47s (remain 0m 0s) Loss: 0.0008(0.0126) \n",
    "Epoch 5 - avg_train_loss: 0.0067  avg_val_loss: 0.0126  time: 418s\n",
    "Epoch 5 - Score: 0.8672\n",
    "Epoch 5 - Save Best Score: 0.8672 Model\n",
    "========== fold: 1 training ==========\n",
    "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']\n",
    "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "Epoch: [1][0/712] Elapsed 0m 0s (remain 9m 47s) Loss: 0.9500(0.9500) Grad: inf  LR: 0.000000  \n",
    "Epoch: [1][100/712] Elapsed 0m 52s (remain 5m 15s) Loss: 0.0802(0.3611) Grad: 952.4308  LR: 0.000006  \n",
    "Epoch: [1][200/712] Elapsed 1m 43s (remain 4m 23s) Loss: 0.0504(0.2055) Grad: 2900.6521  LR: 0.000011  \n",
    "Epoch: [1][300/712] Elapsed 2m 35s (remain 3m 31s) Loss: 0.0342(0.1472) Grad: 2159.0554  LR: 0.000017  \n",
    "Epoch: [1][400/712] Elapsed 3m 26s (remain 2m 39s) Loss: 0.0076(0.1164) Grad: 1385.1884  LR: 0.000020  \n",
    "Epoch: [1][500/712] Elapsed 4m 17s (remain 1m 48s) Loss: 0.0178(0.0976) Grad: 1936.5457  LR: 0.000019  \n",
    "Epoch: [1][600/712] Elapsed 5m 9s (remain 0m 57s) Loss: 0.0093(0.0846) Grad: 2250.4241  LR: 0.000018  \n",
    "Epoch: [1][700/712] Elapsed 6m 0s (remain 0m 5s) Loss: 0.0058(0.0750) Grad: 1090.1874  LR: 0.000018  \n",
    "Epoch: [1][711/712] Elapsed 6m 6s (remain 0m 0s) Loss: 0.0190(0.0741) Grad: 2345.7434  LR: 0.000018  \n",
    "EVAL: [0/181] Elapsed 0m 0s (remain 1m 49s) Loss: 0.0094(0.0094) \n",
    "EVAL: [100/181] Elapsed 0m 26s (remain 0m 20s) Loss: 0.0283(0.0169) \n",
    "EVAL: [180/181] Elapsed 0m 46s (remain 0m 0s) Loss: 0.0067(0.0158) \n",
    "Epoch 1 - avg_train_loss: 0.0741  avg_val_loss: 0.0158  time: 419s\n",
    "Epoch 1 - Score: 0.8033\n",
    "Epoch 1 - Save Best Score: 0.8033 Model\n",
    "Epoch: [2][0/712] Elapsed 0m 0s (remain 9m 44s) Loss: 0.0310(0.0310) Grad: 54322.7734  LR: 0.000018  \n",
    "Epoch: [2][100/712] Elapsed 0m 52s (remain 5m 15s) Loss: 0.0103(0.0140) Grad: 19509.5195  LR: 0.000017  \n",
    "Epoch: [2][200/712] Elapsed 1m 43s (remain 4m 23s) Loss: 0.0091(0.0138) Grad: 20611.0625  LR: 0.000017  \n",
    "Epoch: [2][300/712] Elapsed 2m 34s (remain 3m 31s) Loss: 0.0057(0.0137) Grad: 12656.3066  LR: 0.000016  \n",
    "Epoch: [2][400/712] Elapsed 3m 26s (remain 2m 39s) Loss: 0.0105(0.0134) Grad: 17143.4453  LR: 0.000015  \n",
    "Epoch: [2][500/712] Elapsed 4m 18s (remain 1m 48s) Loss: 0.0101(0.0135) Grad: 16158.7529  LR: 0.000015  \n",
    "Epoch: [2][600/712] Elapsed 5m 10s (remain 0m 57s) Loss: 0.0456(0.0133) Grad: 82531.8125  LR: 0.000014  \n",
    "Epoch: [2][700/712] Elapsed 6m 1s (remain 0m 5s) Loss: 0.0092(0.0131) Grad: 13869.7910  LR: 0.000013  \n",
    "Epoch: [2][711/712] Elapsed 6m 7s (remain 0m 0s) Loss: 0.0095(0.0131) Grad: 12595.5254  LR: 0.000013  \n",
    "EVAL: [0/181] Elapsed 0m 0s (remain 1m 39s) Loss: 0.0054(0.0054) \n",
    "EVAL: [100/181] Elapsed 0m 26s (remain 0m 20s) Loss: 0.0295(0.0148) \n",
    "EVAL: [180/181] Elapsed 0m 46s (remain 0m 0s) Loss: 0.0055(0.0133) \n",
    "Epoch 2 - avg_train_loss: 0.0131  avg_val_loss: 0.0133  time: 419s\n",
    "Epoch 2 - Score: 0.8393\n",
    "Epoch 2 - Save Best Score: 0.8393 Model\n",
    "Epoch: [3][0/712] Elapsed 0m 0s (remain 10m 49s) Loss: 0.0135(0.0135) Grad: 30406.5605  LR: 0.000013  \n",
    "Epoch: [3][100/712] Elapsed 0m 52s (remain 5m 15s) Loss: 0.0125(0.0109) Grad: 36558.4336  LR: 0.000013  \n",
    "Epoch: [3][200/712] Elapsed 1m 43s (remain 4m 22s) Loss: 0.0276(0.0107) Grad: 99852.3516  LR: 0.000012  \n",
    "Epoch: [3][300/712] Elapsed 2m 34s (remain 3m 31s) Loss: 0.0068(0.0105) Grad: 14221.4824  LR: 0.000011  \n",
    "Epoch: [3][400/712] Elapsed 3m 26s (remain 2m 40s) Loss: 0.0099(0.0107) Grad: 19987.7129  LR: 0.000011  \n",
    "Epoch: [3][500/712] Elapsed 4m 18s (remain 1m 48s) Loss: 0.0193(0.0107) Grad: 29633.6191  LR: 0.000010  \n",
    "Epoch: [3][600/712] Elapsed 5m 11s (remain 0m 57s) Loss: 0.0226(0.0106) Grad: 35131.6875  LR: 0.000010  \n",
    "Epoch: [3][700/712] Elapsed 6m 3s (remain 0m 5s) Loss: 0.0388(0.0104) Grad: 63908.1758  LR: 0.000009  \n",
    "Epoch: [3][711/712] Elapsed 6m 9s (remain 0m 0s) Loss: 0.0020(0.0103) Grad: 17033.3809  LR: 0.000009  \n",
    "EVAL: [0/181] Elapsed 0m 0s (remain 1m 37s) Loss: 0.0070(0.0070) \n",
    "EVAL: [100/181] Elapsed 0m 26s (remain 0m 20s) Loss: 0.0299(0.0143) \n",
    "EVAL: [180/181] Elapsed 0m 46s (remain 0m 0s) Loss: 0.0042(0.0131) \n",
    "Epoch 3 - avg_train_loss: 0.0103  avg_val_loss: 0.0131  time: 421s\n",
    "Epoch 3 - Score: 0.8527\n",
    "Epoch 3 - Save Best Score: 0.8527 Model\n",
    "Epoch: [4][0/712] Elapsed 0m 0s (remain 10m 13s) Loss: 0.0095(0.0095) Grad: 21045.0254  LR: 0.000009  \n",
    "Epoch: [4][100/712] Elapsed 0m 52s (remain 5m 16s) Loss: 0.0142(0.0076) Grad: 24434.9883  LR: 0.000008  \n",
    "Epoch: [4][200/712] Elapsed 1m 43s (remain 4m 23s) Loss: 0.0042(0.0077) Grad: 12008.2266  LR: 0.000008  \n",
    "Epoch: [4][300/712] Elapsed 2m 35s (remain 3m 31s) Loss: 0.0041(0.0083) Grad: 17314.1875  LR: 0.000007  \n",
    "Epoch: [4][400/712] Elapsed 3m 27s (remain 2m 40s) Loss: 0.0016(0.0086) Grad: 4729.5532  LR: 0.000006  \n",
    "Epoch: [4][500/712] Elapsed 4m 18s (remain 1m 48s) Loss: 0.0049(0.0083) Grad: 14056.5371  LR: 0.000006  \n",
    "Epoch: [4][600/712] Elapsed 5m 9s (remain 0m 57s) Loss: 0.0048(0.0084) Grad: 19522.2598  LR: 0.000005  \n",
    "Epoch: [4][700/712] Elapsed 6m 1s (remain 0m 5s) Loss: 0.0058(0.0084) Grad: 15232.4775  LR: 0.000005  \n",
    "Epoch: [4][711/712] Elapsed 6m 6s (remain 0m 0s) Loss: 0.0080(0.0083) Grad: 14370.6113  LR: 0.000004  \n",
    "EVAL: [0/181] Elapsed 0m 0s (remain 1m 40s) Loss: 0.0060(0.0060) \n",
    "EVAL: [100/181] Elapsed 0m 26s (remain 0m 20s) Loss: 0.0345(0.0142) \n",
    "EVAL: [180/181] Elapsed 0m 46s (remain 0m 0s) Loss: 0.0035(0.0133) \n",
    "Epoch 4 - avg_train_loss: 0.0083  avg_val_loss: 0.0133  time: 419s\n",
    "Epoch 4 - Score: 0.8568\n",
    "Epoch 4 - Save Best Score: 0.8568 Model\n",
    "Epoch: [5][0/712] Elapsed 0m 0s (remain 11m 5s) Loss: 0.0046(0.0046) Grad: 10348.6074  LR: 0.000004  \n",
    "Epoch: [5][100/712] Elapsed 0m 52s (remain 5m 17s) Loss: 0.0060(0.0072) Grad: 20586.1855  LR: 0.000004  \n",
    "Epoch: [5][200/712] Elapsed 1m 44s (remain 4m 25s) Loss: 0.0048(0.0071) Grad: 20680.5859  LR: 0.000003  \n",
    "Epoch: [5][300/712] Elapsed 2m 35s (remain 3m 32s) Loss: 0.0042(0.0073) Grad: 20825.9531  LR: 0.000003  \n",
    "Epoch: [5][400/712] Elapsed 3m 27s (remain 2m 40s) Loss: 0.0052(0.0072) Grad: 14706.0859  LR: 0.000002  \n",
    "Epoch: [5][500/712] Elapsed 4m 18s (remain 1m 48s) Loss: 0.0065(0.0072) Grad: 19743.6758  LR: 0.000001  \n",
    "Epoch: [5][600/712] Elapsed 5m 9s (remain 0m 57s) Loss: 0.0014(0.0072) Grad: 5298.4155  LR: 0.000001  \n",
    "Epoch: [5][700/712] Elapsed 6m 1s (remain 0m 5s) Loss: 0.0078(0.0071) Grad: 27145.3359  LR: 0.000000  \n",
    "Epoch: [5][711/712] Elapsed 6m 6s (remain 0m 0s) Loss: 0.0039(0.0070) Grad: 10878.4453  LR: 0.000000  \n",
    "EVAL: [0/181] Elapsed 0m 0s (remain 1m 40s) Loss: 0.0056(0.0056) \n",
    "EVAL: [100/181] Elapsed 0m 26s (remain 0m 20s) Loss: 0.0337(0.0142) \n",
    "EVAL: [180/181] Elapsed 0m 46s (remain 0m 0s) Loss: 0.0031(0.0132) \n",
    "Epoch 5 - avg_train_loss: 0.0070  avg_val_loss: 0.0132  time: 419s\n",
    "Epoch 5 - Score: 0.8603\n",
    "Epoch 5 - Save Best Score: 0.8603 Model\n",
    "========== fold: 2 training ==========\n",
    "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']\n",
    "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "Epoch: [1][0/717] Elapsed 0m 0s (remain 9m 48s) Loss: 0.5760(0.5760) Grad: inf  LR: 0.000000  \n",
    "Epoch: [1][100/717] Elapsed 0m 52s (remain 5m 19s) Loss: 0.0565(0.2336) Grad: 1776.0911  LR: 0.000006  \n",
    "Epoch: [1][200/717] Elapsed 1m 43s (remain 4m 26s) Loss: 0.0312(0.1384) Grad: 5241.5347  LR: 0.000011  \n",
    "Epoch: [1][300/717] Elapsed 2m 35s (remain 3m 35s) Loss: 0.0248(0.1016) Grad: 4024.2803  LR: 0.000017  \n",
    "Epoch: [1][400/717] Elapsed 3m 27s (remain 2m 43s) Loss: 0.0297(0.0828) Grad: 3945.1711  LR: 0.000020  \n",
    "Epoch: [1][500/717] Elapsed 4m 18s (remain 1m 51s) Loss: 0.0209(0.0709) Grad: 2584.2070  LR: 0.000019  \n",
    "Epoch: [1][600/717] Elapsed 5m 9s (remain 0m 59s) Loss: 0.0168(0.0621) Grad: 3280.2688  LR: 0.000018  \n",
    "Epoch: [1][700/717] Elapsed 6m 1s (remain 0m 8s) Loss: 0.0105(0.0558) Grad: 2134.2964  LR: 0.000018  \n",
    "Epoch: [1][716/717] Elapsed 6m 9s (remain 0m 0s) Loss: 0.0033(0.0550) Grad: 719.3870  LR: 0.000018  \n",
    "EVAL: [0/176] Elapsed 0m 0s (remain 1m 40s) Loss: 0.0256(0.0256) \n",
    "EVAL: [100/176] Elapsed 0m 26s (remain 0m 19s) Loss: 0.0259(0.0167) \n",
    "EVAL: [175/176] Elapsed 0m 45s (remain 0m 0s) Loss: 0.0043(0.0168) \n",
    "Epoch 1 - avg_train_loss: 0.0550  avg_val_loss: 0.0168  time: 420s\n",
    "Epoch 1 - Score: 0.7849\n",
    "Epoch 1 - Save Best Score: 0.7849 Model\n",
    "Epoch: [2][0/717] Elapsed 0m 0s (remain 10m 2s) Loss: 0.0073(0.0073) Grad: 12840.9932  LR: 0.000018  \n",
    "Epoch: [2][100/717] Elapsed 0m 52s (remain 5m 17s) Loss: 0.0141(0.0149) Grad: 20557.3613  LR: 0.000017  \n",
    "Epoch: [2][200/717] Elapsed 1m 43s (remain 4m 25s) Loss: 0.0054(0.0143) Grad: 22376.9180  LR: 0.000017  \n",
    "Epoch: [2][300/717] Elapsed 2m 35s (remain 3m 34s) Loss: 0.0070(0.0138) Grad: 33000.8711  LR: 0.000016  \n",
    "Epoch: [2][400/717] Elapsed 3m 27s (remain 2m 43s) Loss: 0.0263(0.0135) Grad: 36933.8438  LR: 0.000015  \n",
    "Epoch: [2][500/717] Elapsed 4m 19s (remain 1m 51s) Loss: 0.0252(0.0135) Grad: 39680.6719  LR: 0.000015  \n",
    "Epoch: [2][600/717] Elapsed 5m 10s (remain 0m 59s) Loss: 0.0142(0.0134) Grad: 29544.3164  LR: 0.000014  \n",
    "Epoch: [2][700/717] Elapsed 6m 2s (remain 0m 8s) Loss: 0.0115(0.0133) Grad: 24327.6602  LR: 0.000013  \n",
    "Epoch: [2][716/717] Elapsed 6m 10s (remain 0m 0s) Loss: 0.0129(0.0132) Grad: 28564.6660  LR: 0.000013  \n",
    "EVAL: [0/176] Elapsed 0m 0s (remain 1m 46s) Loss: 0.0141(0.0141) \n",
    "EVAL: [100/176] Elapsed 0m 26s (remain 0m 19s) Loss: 0.0204(0.0140) \n",
    "EVAL: [175/176] Elapsed 0m 45s (remain 0m 0s) Loss: 0.0019(0.0142) \n",
    "Epoch 2 - avg_train_loss: 0.0132  avg_val_loss: 0.0142  time: 421s\n",
    "Epoch 2 - Score: 0.8240\n",
    "Epoch 2 - Save Best Score: 0.8240 Model\n",
    "Epoch: [3][0/717] Elapsed 0m 0s (remain 11m 8s) Loss: 0.0182(0.0182) Grad: 27961.2891  LR: 0.000013  \n",
    "Epoch: [3][100/717] Elapsed 0m 52s (remain 5m 20s) Loss: 0.0097(0.0104) Grad: 16493.1973  LR: 0.000013  \n",
    "Epoch: [3][200/717] Elapsed 1m 44s (remain 4m 27s) Loss: 0.0091(0.0104) Grad: 17227.7773  LR: 0.000012  \n",
    "Epoch: [3][300/717] Elapsed 2m 36s (remain 3m 36s) Loss: 0.0105(0.0098) Grad: 23725.2188  LR: 0.000011  \n",
    "Epoch: [3][400/717] Elapsed 3m 27s (remain 2m 43s) Loss: 0.0150(0.0100) Grad: 31148.3223  LR: 0.000011  \n",
    "Epoch: [3][500/717] Elapsed 4m 18s (remain 1m 51s) Loss: 0.0077(0.0102) Grad: 36118.2305  LR: 0.000010  \n",
    "Epoch: [3][600/717] Elapsed 5m 10s (remain 0m 59s) Loss: 0.0165(0.0100) Grad: 30110.5820  LR: 0.000010  \n",
    "Epoch: [3][700/717] Elapsed 6m 1s (remain 0m 8s) Loss: 0.0117(0.0098) Grad: 29895.6621  LR: 0.000009  \n",
    "Epoch: [3][716/717] Elapsed 6m 9s (remain 0m 0s) Loss: 0.0064(0.0099) Grad: 19579.7871  LR: 0.000009  \n",
    "EVAL: [0/176] Elapsed 0m 0s (remain 1m 47s) Loss: 0.0058(0.0058) \n",
    "EVAL: [100/176] Elapsed 0m 26s (remain 0m 19s) Loss: 0.0123(0.0132) \n",
    "EVAL: [175/176] Elapsed 0m 45s (remain 0m 0s) Loss: 0.0016(0.0137) \n",
    "Epoch 3 - avg_train_loss: 0.0099  avg_val_loss: 0.0137  time: 420s\n",
    "Epoch 3 - Score: 0.8475\n",
    "Epoch 3 - Save Best Score: 0.8475 Model\n",
    "Epoch: [4][0/717] Elapsed 0m 0s (remain 10m 32s) Loss: 0.0031(0.0031) Grad: 7216.0815  LR: 0.000009  \n",
    "Epoch: [4][100/717] Elapsed 0m 53s (remain 5m 23s) Loss: 0.0066(0.0084) Grad: 16821.7930  LR: 0.000008  \n",
    "Epoch: [4][200/717] Elapsed 1m 44s (remain 4m 28s) Loss: 0.0133(0.0075) Grad: 51619.0000  LR: 0.000008  \n",
    "Epoch: [4][300/717] Elapsed 2m 35s (remain 3m 35s) Loss: 0.0133(0.0076) Grad: 42018.2344  LR: 0.000007  \n",
    "Epoch: [4][400/717] Elapsed 3m 27s (remain 2m 43s) Loss: 0.0067(0.0077) Grad: 25267.9785  LR: 0.000006  \n",
    "Epoch: [4][500/717] Elapsed 4m 18s (remain 1m 51s) Loss: 0.0039(0.0078) Grad: 9958.6562  LR: 0.000006  \n",
    "Epoch: [4][600/717] Elapsed 5m 11s (remain 1m 0s) Loss: 0.0036(0.0078) Grad: 12617.1387  LR: 0.000005  \n",
    "Epoch: [4][700/717] Elapsed 6m 4s (remain 0m 8s) Loss: 0.0113(0.0078) Grad: 16916.3613  LR: 0.000005  \n",
    "Epoch: [4][716/717] Elapsed 6m 12s (remain 0m 0s) Loss: 0.0052(0.0078) Grad: 13479.9639  LR: 0.000004  \n",
    "EVAL: [0/176] Elapsed 0m 0s (remain 1m 41s) Loss: 0.0033(0.0033) \n",
    "EVAL: [100/176] Elapsed 0m 26s (remain 0m 19s) Loss: 0.0123(0.0131) \n",
    "EVAL: [175/176] Elapsed 0m 45s (remain 0m 0s) Loss: 0.0007(0.0135) \n",
    "Epoch 4 - avg_train_loss: 0.0078  avg_val_loss: 0.0135  time: 423s\n",
    "Epoch 4 - Score: 0.8517\n",
    "Epoch 4 - Save Best Score: 0.8517 Model\n",
    "Epoch: [5][0/717] Elapsed 0m 0s (remain 11m 5s) Loss: 0.0044(0.0044) Grad: 15515.7500  LR: 0.000004  \n",
    "Epoch: [5][100/717] Elapsed 0m 52s (remain 5m 18s) Loss: 0.0015(0.0066) Grad: 12929.9893  LR: 0.000004  \n",
    "Epoch: [5][200/717] Elapsed 1m 43s (remain 4m 25s) Loss: 0.0063(0.0066) Grad: 12418.2285  LR: 0.000003  \n",
    "Epoch: [5][300/717] Elapsed 2m 34s (remain 3m 33s) Loss: 0.0048(0.0066) Grad: 11512.3496  LR: 0.000003  \n",
    "Epoch: [5][400/717] Elapsed 3m 26s (remain 2m 42s) Loss: 0.0120(0.0065) Grad: 20664.1543  LR: 0.000002  \n",
    "Epoch: [5][500/717] Elapsed 4m 17s (remain 1m 51s) Loss: 0.0030(0.0065) Grad: 7600.6919  LR: 0.000001  \n",
    "Epoch: [5][600/717] Elapsed 5m 9s (remain 0m 59s) Loss: 0.0054(0.0066) Grad: 12602.7676  LR: 0.000001  \n",
    "Epoch: [5][700/717] Elapsed 6m 0s (remain 0m 8s) Loss: 0.0033(0.0066) Grad: 7729.0146  LR: 0.000000  \n",
    "Epoch: [5][716/717] Elapsed 6m 8s (remain 0m 0s) Loss: 0.0017(0.0066) Grad: 8355.9502  LR: 0.000000  \n",
    "EVAL: [0/176] Elapsed 0m 0s (remain 1m 48s) Loss: 0.0024(0.0024) \n",
    "EVAL: [100/176] Elapsed 0m 26s (remain 0m 19s) Loss: 0.0135(0.0132) \n",
    "EVAL: [175/176] Elapsed 0m 45s (remain 0m 0s) Loss: 0.0008(0.0135) \n",
    "Epoch 5 - avg_train_loss: 0.0066  avg_val_loss: 0.0135  time: 420s\n",
    "Epoch 5 - Score: 0.8555\n",
    "Epoch 5 - Save Best Score: 0.8555 Model\n",
    "========== fold: 3 training ==========\n",
    "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']\n",
    "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "Epoch: [1][0/719] Elapsed 0m 0s (remain 9m 58s) Loss: 1.0619(1.0619) Grad: inf  LR: 0.000000  \n",
    "Epoch: [1][100/719] Elapsed 0m 52s (remain 5m 18s) Loss: 0.0374(0.4016) Grad: 975.7155  LR: 0.000006  \n",
    "Epoch: [1][200/719] Elapsed 1m 43s (remain 4m 27s) Loss: 0.0294(0.2240) Grad: 2603.8152  LR: 0.000011  \n",
    "Epoch: [1][300/719] Elapsed 2m 34s (remain 3m 35s) Loss: 0.0263(0.1594) Grad: 2475.5859  LR: 0.000017  \n",
    "Epoch: [1][400/719] Elapsed 3m 26s (remain 2m 43s) Loss: 0.0206(0.1260) Grad: 1416.5098  LR: 0.000020  \n",
    "Epoch: [1][500/719] Elapsed 4m 17s (remain 1m 52s) Loss: 0.0199(0.1052) Grad: 1542.1224  LR: 0.000019  \n",
    "Epoch: [1][600/719] Elapsed 5m 8s (remain 1m 0s) Loss: 0.0225(0.0907) Grad: 1038.6124  LR: 0.000019  \n",
    "Epoch: [1][700/719] Elapsed 5m 59s (remain 0m 9s) Loss: 0.0170(0.0801) Grad: 2482.5432  LR: 0.000018  \n",
    "Epoch: [1][718/719] Elapsed 6m 8s (remain 0m 0s) Loss: 0.0073(0.0785) Grad: 828.7495  LR: 0.000018  \n",
    "EVAL: [0/175] Elapsed 0m 0s (remain 1m 39s) Loss: 0.0074(0.0074) \n",
    "EVAL: [100/175] Elapsed 0m 26s (remain 0m 19s) Loss: 0.0119(0.0153) \n",
    "EVAL: [174/175] Elapsed 0m 45s (remain 0m 0s) Loss: 0.0015(0.0145) \n",
    "Epoch 1 - avg_train_loss: 0.0785  avg_val_loss: 0.0145  time: 419s\n",
    "Epoch 1 - Score: 0.8167\n",
    "Epoch 1 - Save Best Score: 0.8167 Model\n",
    "Epoch: [2][0/719] Elapsed 0m 0s (remain 10m 1s) Loss: 0.0189(0.0189) Grad: 44838.2969  LR: 0.000018  \n",
    "Epoch: [2][100/719] Elapsed 0m 52s (remain 5m 19s) Loss: 0.0197(0.0135) Grad: 31879.7852  LR: 0.000017  \n",
    "Epoch: [2][200/719] Elapsed 1m 43s (remain 4m 27s) Loss: 0.0124(0.0142) Grad: 17697.9863  LR: 0.000017  \n",
    "Epoch: [2][300/719] Elapsed 2m 36s (remain 3m 36s) Loss: 0.0183(0.0140) Grad: 27233.4473  LR: 0.000016  \n",
    "Epoch: [2][400/719] Elapsed 3m 28s (remain 2m 45s) Loss: 0.0154(0.0139) Grad: 17281.6445  LR: 0.000015  \n",
    "Epoch: [2][500/719] Elapsed 4m 20s (remain 1m 53s) Loss: 0.0014(0.0134) Grad: 3942.8113  LR: 0.000015  \n",
    "Epoch: [2][600/719] Elapsed 5m 12s (remain 1m 1s) Loss: 0.0063(0.0134) Grad: 20015.9297  LR: 0.000014  \n",
    "Epoch: [2][700/719] Elapsed 6m 4s (remain 0m 9s) Loss: 0.0176(0.0132) Grad: 30243.2109  LR: 0.000013  \n",
    "Epoch: [2][718/719] Elapsed 6m 13s (remain 0m 0s) Loss: 0.0080(0.0131) Grad: 8403.2920  LR: 0.000013  \n",
    "EVAL: [0/175] Elapsed 0m 0s (remain 1m 35s) Loss: 0.0040(0.0040) \n",
    "EVAL: [100/175] Elapsed 0m 26s (remain 0m 19s) Loss: 0.0120(0.0127) \n",
    "EVAL: [174/175] Elapsed 0m 44s (remain 0m 0s) Loss: 0.0006(0.0123) \n",
    "Epoch 2 - avg_train_loss: 0.0131  avg_val_loss: 0.0123  time: 423s\n",
    "Epoch 2 - Score: 0.8477\n",
    "Epoch 2 - Save Best Score: 0.8477 Model\n",
    "Epoch: [3][0/719] Elapsed 0m 0s (remain 11m 0s) Loss: 0.0085(0.0085) Grad: 13265.3516  LR: 0.000013  \n",
    "Epoch: [3][100/719] Elapsed 0m 52s (remain 5m 18s) Loss: 0.0102(0.0096) Grad: 29454.7188  LR: 0.000013  \n",
    "Epoch: [3][200/719] Elapsed 1m 43s (remain 4m 26s) Loss: 0.0093(0.0099) Grad: 23030.0078  LR: 0.000012  \n",
    "Epoch: [3][300/719] Elapsed 2m 35s (remain 3m 36s) Loss: 0.0083(0.0105) Grad: 17166.6699  LR: 0.000011  \n",
    "Epoch: [3][400/719] Elapsed 3m 26s (remain 2m 44s) Loss: 0.0031(0.0105) Grad: 10209.6865  LR: 0.000011  \n",
    "Epoch: [3][500/719] Elapsed 4m 18s (remain 1m 52s) Loss: 0.0006(0.0105) Grad: 3198.8638  LR: 0.000010  \n",
    "Epoch: [3][600/719] Elapsed 5m 9s (remain 1m 0s) Loss: 0.0047(0.0104) Grad: 9279.9111  LR: 0.000010  \n",
    "Epoch: [3][700/719] Elapsed 6m 0s (remain 0m 9s) Loss: 0.0268(0.0103) Grad: 42701.5664  LR: 0.000009  \n",
    "Epoch: [3][718/719] Elapsed 6m 9s (remain 0m 0s) Loss: 0.0025(0.0103) Grad: 8551.9297  LR: 0.000009  \n",
    "EVAL: [0/175] Elapsed 0m 0s (remain 1m 47s) Loss: 0.0023(0.0023) \n",
    "EVAL: [100/175] Elapsed 0m 26s (remain 0m 19s) Loss: 0.0158(0.0122) \n",
    "EVAL: [174/175] Elapsed 0m 45s (remain 0m 0s) Loss: 0.0006(0.0117) \n",
    "Epoch 3 - avg_train_loss: 0.0103  avg_val_loss: 0.0117  time: 420s\n",
    "Epoch 3 - Score: 0.8599\n",
    "Epoch 3 - Save Best Score: 0.8599 Model\n",
    "Epoch: [4][0/719] Elapsed 0m 0s (remain 11m 9s) Loss: 0.0033(0.0033) Grad: 13959.6514  LR: 0.000009  \n",
    "Epoch: [4][100/719] Elapsed 0m 52s (remain 5m 19s) Loss: 0.0027(0.0083) Grad: 6956.8550  LR: 0.000008  \n",
    "Epoch: [4][200/719] Elapsed 1m 43s (remain 4m 26s) Loss: 0.0032(0.0083) Grad: 15149.8184  LR: 0.000008  \n",
    "Epoch: [4][300/719] Elapsed 2m 34s (remain 3m 34s) Loss: 0.0044(0.0080) Grad: 16648.5879  LR: 0.000007  \n",
    "Epoch: [4][400/719] Elapsed 3m 26s (remain 2m 43s) Loss: 0.0028(0.0082) Grad: 13756.1797  LR: 0.000006  \n",
    "Epoch: [4][500/719] Elapsed 4m 18s (remain 1m 52s) Loss: 0.0066(0.0084) Grad: 11495.6768  LR: 0.000006  \n",
    "Epoch: [4][600/719] Elapsed 5m 9s (remain 1m 0s) Loss: 0.0113(0.0083) Grad: 23293.7441  LR: 0.000005  \n",
    "Epoch: [4][700/719] Elapsed 6m 0s (remain 0m 9s) Loss: 0.0010(0.0082) Grad: 4205.5894  LR: 0.000005  \n",
    "Epoch: [4][718/719] Elapsed 6m 10s (remain 0m 0s) Loss: 0.0027(0.0081) Grad: 17012.6465  LR: 0.000004  \n",
    "EVAL: [0/175] Elapsed 0m 0s (remain 1m 37s) Loss: 0.0026(0.0026) \n",
    "EVAL: [100/175] Elapsed 0m 26s (remain 0m 19s) Loss: 0.0111(0.0126) \n",
    "EVAL: [174/175] Elapsed 0m 44s (remain 0m 0s) Loss: 0.0003(0.0121) \n",
    "Epoch 4 - avg_train_loss: 0.0081  avg_val_loss: 0.0121  time: 420s\n",
    "Epoch 4 - Score: 0.8642\n",
    "Epoch 4 - Save Best Score: 0.8642 Model\n",
    "Epoch: [5][0/719] Elapsed 0m 0s (remain 11m 20s) Loss: 0.0034(0.0034) Grad: 9203.2881  LR: 0.000004  \n",
    "Epoch: [5][100/719] Elapsed 0m 52s (remain 5m 21s) Loss: 0.0066(0.0065) Grad: 15132.8213  LR: 0.000004  \n",
    "Epoch: [5][200/719] Elapsed 1m 43s (remain 4m 27s) Loss: 0.0092(0.0068) Grad: 45111.8320  LR: 0.000003  \n",
    "Epoch: [5][300/719] Elapsed 2m 34s (remain 3m 34s) Loss: 0.0085(0.0069) Grad: 24726.6562  LR: 0.000003  \n",
    "Epoch: [5][400/719] Elapsed 3m 25s (remain 2m 43s) Loss: 0.0077(0.0069) Grad: 23677.4824  LR: 0.000002  \n",
    "Epoch: [5][500/719] Elapsed 4m 16s (remain 1m 51s) Loss: 0.0008(0.0068) Grad: 4383.9536  LR: 0.000001  \n",
    "Epoch: [5][600/719] Elapsed 5m 8s (remain 1m 0s) Loss: 0.0071(0.0068) Grad: 11846.6787  LR: 0.000001  \n",
    "Epoch: [5][700/719] Elapsed 6m 1s (remain 0m 9s) Loss: 0.0043(0.0069) Grad: 11576.6797  LR: 0.000000  \n",
    "Epoch: [5][718/719] Elapsed 6m 10s (remain 0m 0s) Loss: 0.0027(0.0069) Grad: 13109.6387  LR: 0.000000  \n",
    "EVAL: [0/175] Elapsed 0m 0s (remain 1m 38s) Loss: 0.0023(0.0023) \n",
    "EVAL: [100/175] Elapsed 0m 26s (remain 0m 19s) Loss: 0.0107(0.0125) \n",
    "EVAL: [174/175] Elapsed 0m 44s (remain 0m 0s) Loss: 0.0004(0.0120) \n",
    "Epoch 5 - avg_train_loss: 0.0069  avg_val_loss: 0.0120  time: 421s\n",
    "Epoch 5 - Score: 0.8645\n",
    "Epoch 5 - Save Best Score: 0.8645 Model\n",
    "========== fold: 4 training ==========\n",
    "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']\n",
    "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "Epoch: [1][0/712] Elapsed 0m 0s (remain 9m 51s) Loss: 0.9575(0.9575) Grad: inf  LR: 0.000000  \n",
    "Epoch: [1][100/712] Elapsed 0m 52s (remain 5m 14s) Loss: 0.0472(0.3601) Grad: 897.9247  LR: 0.000006  \n",
    "Epoch: [1][200/712] Elapsed 1m 44s (remain 4m 24s) Loss: 0.0246(0.2014) Grad: 2885.2925  LR: 0.000011  \n",
    "Epoch: [1][300/712] Elapsed 2m 35s (remain 3m 32s) Loss: 0.0237(0.1439) Grad: 2856.0505  LR: 0.000017  \n",
    "Epoch: [1][400/712] Elapsed 3m 26s (remain 2m 40s) Loss: 0.0248(0.1141) Grad: 2242.8555  LR: 0.000020  \n",
    "Epoch: [1][500/712] Elapsed 4m 17s (remain 1m 48s) Loss: 0.0259(0.0956) Grad: 4059.1238  LR: 0.000019  \n",
    "Epoch: [1][600/712] Elapsed 5m 8s (remain 0m 57s) Loss: 0.0278(0.0833) Grad: 3173.1501  LR: 0.000018  \n",
    "Epoch: [1][700/712] Elapsed 5m 59s (remain 0m 5s) Loss: 0.0320(0.0739) Grad: 2918.3689  LR: 0.000018  \n",
    "Epoch: [1][711/712] Elapsed 6m 5s (remain 0m 0s) Loss: 0.0147(0.0729) Grad: 1188.6802  LR: 0.000018  \n",
    "EVAL: [0/182] Elapsed 0m 0s (remain 1m 42s) Loss: 0.0256(0.0256) \n",
    "EVAL: [100/182] Elapsed 0m 26s (remain 0m 20s) Loss: 0.0495(0.0172) \n",
    "EVAL: [181/182] Elapsed 0m 46s (remain 0m 0s) Loss: 0.0053(0.0153) \n",
    "Epoch 1 - avg_train_loss: 0.0729  avg_val_loss: 0.0153  time: 417s\n",
    "Epoch 1 - Score: 0.8157\n",
    "Epoch 1 - Save Best Score: 0.8157 Model\n",
    "Epoch: [2][0/712] Elapsed 0m 0s (remain 9m 55s) Loss: 0.0092(0.0092) Grad: 20895.6699  LR: 0.000018  \n",
    "Epoch: [2][100/712] Elapsed 0m 52s (remain 5m 14s) Loss: 0.0087(0.0143) Grad: 8895.1973  LR: 0.000017  \n",
    "Epoch: [2][200/712] Elapsed 1m 43s (remain 4m 22s) Loss: 0.0197(0.0132) Grad: 34384.1289  LR: 0.000017  \n",
    "Epoch: [2][300/712] Elapsed 2m 34s (remain 3m 31s) Loss: 0.0065(0.0130) Grad: 14027.5430  LR: 0.000016  \n",
    "Epoch: [2][400/712] Elapsed 3m 26s (remain 2m 39s) Loss: 0.0059(0.0129) Grad: 9931.1221  LR: 0.000015  \n",
    "Epoch: [2][500/712] Elapsed 4m 18s (remain 1m 48s) Loss: 0.0212(0.0131) Grad: 56407.7891  LR: 0.000015  \n",
    "Epoch: [2][600/712] Elapsed 5m 9s (remain 0m 57s) Loss: 0.0171(0.0129) Grad: 17855.0195  LR: 0.000014  \n",
    "Epoch: [2][700/712] Elapsed 6m 0s (remain 0m 5s) Loss: 0.0092(0.0128) Grad: 16696.6738  LR: 0.000013  \n",
    "Epoch: [2][711/712] Elapsed 6m 6s (remain 0m 0s) Loss: 0.0142(0.0128) Grad: 18871.1973  LR: 0.000013  \n",
    "EVAL: [0/182] Elapsed 0m 0s (remain 1m 44s) Loss: 0.0120(0.0120) \n",
    "EVAL: [100/182] Elapsed 0m 26s (remain 0m 21s) Loss: 0.0594(0.0160) \n",
    "EVAL: [181/182] Elapsed 0m 46s (remain 0m 0s) Loss: 0.0061(0.0141) \n",
    "Epoch 2 - avg_train_loss: 0.0128  avg_val_loss: 0.0141  time: 418s\n",
    "Epoch 2 - Score: 0.8426\n",
    "Epoch 2 - Save Best Score: 0.8426 Model\n",
    "Epoch: [3][0/712] Elapsed 0m 0s (remain 11m 4s) Loss: 0.0048(0.0048) Grad: 23358.0195  LR: 0.000013  \n",
    "Epoch: [3][100/712] Elapsed 0m 51s (remain 5m 14s) Loss: 0.0090(0.0101) Grad: 16714.3223  LR: 0.000013  \n",
    "Epoch: [3][200/712] Elapsed 1m 43s (remain 4m 22s) Loss: 0.0187(0.0103) Grad: 72955.9219  LR: 0.000012  \n",
    "Epoch: [3][300/712] Elapsed 2m 34s (remain 3m 31s) Loss: 0.0052(0.0102) Grad: 20347.4434  LR: 0.000011  \n",
    "Epoch: [3][400/712] Elapsed 3m 26s (remain 2m 40s) Loss: 0.0113(0.0102) Grad: 40586.5664  LR: 0.000011  \n",
    "Epoch: [3][500/712] Elapsed 4m 18s (remain 1m 48s) Loss: 0.0037(0.0101) Grad: 9424.6201  LR: 0.000010  \n",
    "Epoch: [3][600/712] Elapsed 5m 9s (remain 0m 57s) Loss: 0.0103(0.0101) Grad: 28416.9805  LR: 0.000010  \n",
    "Epoch: [3][700/712] Elapsed 6m 1s (remain 0m 5s) Loss: 0.0139(0.0101) Grad: 27063.6992  LR: 0.000009  \n",
    "Epoch: [3][711/712] Elapsed 6m 7s (remain 0m 0s) Loss: 0.0043(0.0101) Grad: 20963.6035  LR: 0.000009  \n",
    "EVAL: [0/182] Elapsed 0m 0s (remain 1m 55s) Loss: 0.0079(0.0079) \n",
    "EVAL: [100/182] Elapsed 0m 26s (remain 0m 21s) Loss: 0.0480(0.0146) \n",
    "EVAL: [181/182] Elapsed 0m 46s (remain 0m 0s) Loss: 0.0059(0.0127) \n",
    "Epoch 3 - avg_train_loss: 0.0101  avg_val_loss: 0.0127  time: 419s\n",
    "Epoch 3 - Score: 0.8535\n",
    "Epoch 3 - Save Best Score: 0.8535 Model\n",
    "Epoch: [4][0/712] Elapsed 0m 0s (remain 11m 5s) Loss: 0.0026(0.0026) Grad: 5051.4468  LR: 0.000009  \n",
    "Epoch: [4][100/712] Elapsed 0m 52s (remain 5m 16s) Loss: 0.0031(0.0079) Grad: 16139.7725  LR: 0.000008  \n",
    "Epoch: [4][200/712] Elapsed 1m 43s (remain 4m 23s) Loss: 0.0145(0.0077) Grad: 30466.1426  LR: 0.000008  \n",
    "Epoch: [4][300/712] Elapsed 2m 35s (remain 3m 31s) Loss: 0.0084(0.0079) Grad: 18274.2012  LR: 0.000007  \n",
    "Epoch: [4][400/712] Elapsed 3m 26s (remain 2m 40s) Loss: 0.0226(0.0078) Grad: 35175.9219  LR: 0.000006  \n",
    "Epoch: [4][500/712] Elapsed 4m 17s (remain 1m 48s) Loss: 0.0065(0.0079) Grad: 11477.7207  LR: 0.000006  \n",
    "Epoch: [4][600/712] Elapsed 5m 9s (remain 0m 57s) Loss: 0.0068(0.0080) Grad: 21257.8867  LR: 0.000005  \n",
    "Epoch: [4][700/712] Elapsed 6m 1s (remain 0m 5s) Loss: 0.0043(0.0080) Grad: 17176.4707  LR: 0.000005  \n",
    "Epoch: [4][711/712] Elapsed 6m 6s (remain 0m 0s) Loss: 0.0048(0.0080) Grad: 11005.6768  LR: 0.000004  \n",
    "EVAL: [0/182] Elapsed 0m 0s (remain 1m 56s) Loss: 0.0103(0.0103) \n",
    "EVAL: [100/182] Elapsed 0m 26s (remain 0m 21s) Loss: 0.0538(0.0157) \n",
    "EVAL: [181/182] Elapsed 0m 46s (remain 0m 0s) Loss: 0.0065(0.0133) \n",
    "Epoch 4 - avg_train_loss: 0.0080  avg_val_loss: 0.0133  time: 419s\n",
    "Epoch 4 - Score: 0.8580\n",
    "Epoch 4 - Save Best Score: 0.8580 Model\n",
    "Epoch: [5][0/712] Elapsed 0m 0s (remain 11m 8s) Loss: 0.0096(0.0096) Grad: 28264.8184  LR: 0.000004  \n",
    "Epoch: [5][100/712] Elapsed 0m 52s (remain 5m 15s) Loss: 0.0034(0.0066) Grad: 14201.5898  LR: 0.000004  \n",
    "Epoch: [5][200/712] Elapsed 1m 43s (remain 4m 22s) Loss: 0.0054(0.0068) Grad: 11046.1191  LR: 0.000003  \n",
    "Epoch: [5][300/712] Elapsed 2m 34s (remain 3m 30s) Loss: 0.0087(0.0069) Grad: 18392.0234  LR: 0.000003  \n",
    "Epoch: [5][400/712] Elapsed 3m 25s (remain 2m 39s) Loss: 0.0125(0.0071) Grad: 38556.3125  LR: 0.000002  \n",
    "Epoch: [5][500/712] Elapsed 4m 17s (remain 1m 48s) Loss: 0.0006(0.0070) Grad: 3282.4463  LR: 0.000001  \n",
    "Epoch: [5][600/712] Elapsed 5m 8s (remain 0m 57s) Loss: 0.0086(0.0069) Grad: 27053.7969  LR: 0.000001  \n",
    "Epoch: [5][700/712] Elapsed 5m 59s (remain 0m 5s) Loss: 0.0025(0.0069) Grad: 13092.8965  LR: 0.000000  \n",
    "Epoch: [5][711/712] Elapsed 6m 5s (remain 0m 0s) Loss: 0.0043(0.0069) Grad: 21682.1992  LR: 0.000000  \n",
    "EVAL: [0/182] Elapsed 0m 0s (remain 1m 51s) Loss: 0.0103(0.0103) \n",
    "EVAL: [100/182] Elapsed 0m 26s (remain 0m 21s) Loss: 0.0503(0.0157) \n",
    "EVAL: [181/182] Elapsed 0m 46s (remain 0m 0s) Loss: 0.0065(0.0133) \n",
    "Epoch 5 - avg_train_loss: 0.0069  avg_val_loss: 0.0133  time: 418s\n",
    "Epoch 5 - Score: 0.8607\n",
    "Epoch 5 - Save Best Score: 0.8607 Model\n",
    "Score: 0.8616"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
