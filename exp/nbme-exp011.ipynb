{"cells":[{"cell_type":"markdown","metadata":{"id":"aa1f8e80"},"source":["## References"],"id":"aa1f8e80"},{"cell_type":"markdown","metadata":{"id":"c0138fac"},"source":["- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"],"id":"c0138fac"},{"cell_type":"markdown","metadata":{"id":"cf1dfda9"},"source":["## Configurations"],"id":"cf1dfda9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a7a78d25"},"outputs":[],"source":["EXP_NAME = \"nbme-exp011\"\n","ENV = \"colab\"\n","DEBUG_MODE = False\n","SUBMISSION_MODE = False"],"id":"a7a78d25"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ecc4e4d"},"outputs":[],"source":["class CFG:\n","    env=ENV\n","    exp_name=EXP_NAME\n","    debug=DEBUG_MODE\n","    submission=SUBMISSION_MODE\n","    apex=True\n","    input_dir=None\n","    output_dir=None\n","    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n","    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n","    competition_name=\"nbme-score-clinical-patient-notes\"\n","    id_col=\"id\"\n","    target_col=\"location\"\n","    pretrained_model_name=\"microsoft/deberta-base\"\n","    tokenizer=None\n","    max_len=None\n","    output_dim=1\n","    dropout=0.2\n","    num_workers=4\n","    batch_size=8\n","    lr=2e-5\n","    betas=(0.9, 0.98)\n","    weight_decay=0.1\n","    num_warmup_steps_rate=0.1\n","    batch_scheduler=True\n","    epochs=5\n","    n_fold=5\n","    train_fold=[0, 1, 2, 3, 4]\n","    seed=71\n","    gradient_accumulation_steps=1\n","    max_grad_norm=1000\n","    print_freq=100\n","    train=True\n","    inference=True"],"id":"4ecc4e4d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"3894c88b"},"outputs":[],"source":["if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.train_fold = [0, 1]\n","\n","if CFG.submission:\n","    CFG.train = False\n","    CFG.inference = True"],"id":"3894c88b"},{"cell_type":"markdown","metadata":{"id":"31768c85"},"source":["## Directory Settings"],"id":"31768c85"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4693,"status":"ok","timestamp":1646023773081,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"00e7d967","outputId":"d56a483d-9171-44e6-856a-a90dfe8e0ac3"},"outputs":[{"name":"stdout","output_type":"stream","text":["colab\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"]}],"source":["import sys\n","from pathlib import Path\n","\n","\n","print(CFG.env)\n","if CFG.env == \"colab\":\n","    # colab環境\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","    # install packages\n","    !pip install transformers\n","\n","elif CFG.env == \"local\":\n","    # ローカルサーバ\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","\n","elif CFG.env == \"kaggle\":\n","    # kaggle環境\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./\")"],"id":"00e7d967"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d726b7d9"},"outputs":[],"source":["import gc\n","import os\n","import ast\n","import time\n","import math\n","import random\n","import itertools\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","from scipy.optimize import minimize\n","from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as T\n","from torchvision.io import read_image\n","from torch.utils.data import DataLoader, Dataset\n","\n","from transformers import AutoModelForMaskedLM\n","from transformers import BartModel,BertModel,BertTokenizer\n","from transformers import DebertaModel,DebertaTokenizer\n","from transformers import RobertaModel,RobertaTokenizer\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"id":"d726b7d9"},{"cell_type":"markdown","metadata":{"id":"b6d82f71"},"source":["## Utilities"],"id":"b6d82f71"},{"cell_type":"code","execution_count":null,"metadata":{"id":"95abbe2c"},"outputs":[],"source":["def micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on binary arrays.\n","\n","    Args:\n","        preds (list of lists of ints): Predictions.\n","        truths (list of lists of ints): Ground truths.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    # Micro : aggregating over all instances\n","    preds = np.concatenate(preds)\n","    truths = np.concatenate(truths)\n","    return f1_score(truths, preds)\n","\n","\n","def spans_to_binary(spans, length=None):\n","    \"\"\"\n","    Converts spans to a binary array indicating whether each character is in the span.\n","\n","    Args:\n","        spans (list of lists of two ints): Spans.\n","\n","    Returns:\n","        np array [length]: Binarized spans.\n","    \"\"\"\n","    length = np.max(spans) if length is None else length\n","    binary = np.zeros(length)\n","    for start, end in spans:\n","        binary[start:end] = 1\n","    return binary\n","\n","\n","def span_micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on spans.\n","\n","    Args:\n","        preds (list of lists of two ints): Prediction spans.\n","        truths (list of lists of two ints): Ground truth spans.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    bin_preds = []\n","    bin_truths = []\n","    for pred, truth in zip(preds, truths):\n","        if not len(pred) and not len(truth):\n","            continue\n","        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n","        bin_preds.append(spans_to_binary(pred, length))\n","        bin_truths.append(spans_to_binary(truth, length))\n","    return micro_f1(bin_preds, bin_truths)\n","\n","\n","def get_score(y_true, y_pred):\n","    score = span_micro_f1(y_true, y_pred)\n","    return score"],"id":"95abbe2c"},{"cell_type":"code","execution_count":null,"metadata":{"id":"832ee36d"},"outputs":[],"source":["def create_labels_for_scoring(df):\n","    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n","    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n","    for i in range(len(df)):\n","        lst = df.loc[i, \"location\"]\n","        if lst:\n","            new_lst = \";\".join(lst)\n","            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n","\n","    # create labels\n","    truths = []\n","    for location_list in df[\"location_for_create_labels\"].values:\n","        truth = []\n","        if len(location_list) > 0:\n","            location = location_list[0]\n","            for loc in [s.split() for s in location.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                truth.append([start, end])\n","        truths.append(truth)\n","\n","    return truths\n","\n","\n","def get_char_probs(texts, token_probs, tokenizer):\n","    res = [np.zeros(len(t)) for t in texts]\n","    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n","        encoded = tokenizer(\n","            text=text,\n","            max_length=CFG.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n","            start, end = offset_mapping\n","            res[i][start:end] = pred\n","    return res\n","\n","\n","def get_predicted_location_str(char_probs, th=0.5):\n","    results = []\n","    for char_prob in char_probs:\n","        result = np.where(char_prob >= th)[0] + 1\n","        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n","        result = [f\"{min(r)} {max(r)}\" for r in result]\n","        result = \";\".join(result)\n","        results.append(result)\n","    return results\n","\n","\n","def get_predictions(results):\n","    predictions = []\n","    for result in results:\n","        prediction = []\n","        if result != \"\":\n","            for loc in [s.split() for s in result.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                prediction.append([start, end])\n","        predictions.append(prediction)\n","    return predictions\n","\n","\n","def scoring(df, th=0.5):\n","    labels = create_labels_for_scoring(df)\n","\n","    token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n","    char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n","    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n","    preds = get_predictions(predicted_location_str)\n","\n","    score = get_score(labels, preds)\n","    return score\n","\n","\n","def get_best_thres(oof_df):\n","    def f1_opt(x):\n","        return -1 * scoring(oof_df, th=x)\n","\n","    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n","    return best_thres"],"id":"832ee36d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"918828a7"},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return \"%dm %ds\" % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"],"id":"918828a7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d02a78e1"},"outputs":[],"source":["seed_everything()"],"id":"d02a78e1"},{"cell_type":"markdown","metadata":{"id":"47266f39"},"source":["## Data Loading"],"id":"47266f39"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":815,"status":"ok","timestamp":1646023777557,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"20fed6da","outputId":"64d3e7ad-0986-4799-f9df-f0242c1977a2"},"outputs":[{"data":{"text/plain":["((14300, 6), (143, 3), (42146, 3), (5, 4))"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["train = pd.read_csv(CFG.input_dir / \"train.csv\")\n","features = pd.read_csv(CFG.input_dir / \"features.csv\")\n","patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n","test = pd.read_csv(CFG.input_dir / \"test.csv\")\n","\n","train.shape, features.shape, patient_notes.shape, test.shape"],"id":"20fed6da"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e67d0132"},"outputs":[],"source":["if CFG.debug:\n","    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n","    print(train.shape)"],"id":"e67d0132"},{"cell_type":"markdown","metadata":{"id":"47bca11a"},"source":["## Preprocessing"],"id":"47bca11a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d9c8e9ba"},"outputs":[],"source":["def preprocess_features(features):\n","    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n","    return features\n","\n","\n","features = preprocess_features(features)"],"id":"d9c8e9ba"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1646023777558,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"7ef41e18","outputId":"31edaa7d-c088-495a-95ee-f0d56f97074c"},"outputs":[{"data":{"text/plain":["((14300, 8), (5, 6))"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","\n","train.shape, test.shape"],"id":"7ef41e18"},{"cell_type":"code","execution_count":null,"metadata":{"id":"8233df16"},"outputs":[],"source":["train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n","train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"],"id":"8233df16"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1646023778018,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"e9143e61","outputId":"cf45e2d7-5f66-4d96-c6e2-da79c888bcc8"},"outputs":[{"data":{"text/plain":["0    4399\n","1    8181\n","2    1296\n","3     287\n","4      99\n","5      27\n","6       9\n","7       1\n","8       1\n","Name: annotation_length, dtype: int64"]},"metadata":{},"output_type":"display_data"}],"source":["train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n","display(train['annotation_length'].value_counts().sort_index())"],"id":"e9143e61"},{"cell_type":"markdown","metadata":{"id":"6bdc7949"},"source":["## CV split"],"id":"6bdc7949"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c4acf61d"},"outputs":[],"source":["def get_groupkfold(df, group_name):\n","    groups = df[group_name].unique()\n","\n","    kf = KFold(\n","        n_splits=CFG.n_fold,\n","        shuffle=True,\n","        random_state=CFG.seed,\n","    )\n","    folds_ids = []\n","    for i_fold, (_, val_group_idx) in enumerate(kf.split(groups)):\n","        val_group = groups[val_group_idx]\n","        is_val = df[group_name].isin(val_group)\n","        val_idx = df[is_val].index\n","        df.loc[val_idx, \"fold\"] = int(i_fold)\n","\n","    df[\"fold\"] = df[\"fold\"].astype(int)\n","    return df"],"id":"c4acf61d"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1646023778018,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"2ca0c08e","outputId":"cfc9c06e-e30c-4cb5-a072-d0cfcfa5fdc7"},"outputs":[{"data":{"text/plain":["fold\n","0    2902\n","1    2894\n","2    2813\n","3    2791\n","4    2900\n","dtype: int64"]},"metadata":{},"output_type":"display_data"}],"source":["train = get_groupkfold(train, \"pn_num\")\n","display(train.groupby(\"fold\").size())"],"id":"2ca0c08e"},{"cell_type":"markdown","metadata":{"id":"a8560070"},"source":["## Setup tokenizer"],"id":"a8560070"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c316b13f"},"outputs":[],"source":["if CFG.submission:\n","    tokenizer = AutoTokenizer.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n","else:\n","    tokenizer = AutoTokenizer.from_pretrained(CFG.pretrained_model_name)\n","    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n","\n","CFG.tokenizer = tokenizer"],"id":"c316b13f"},{"cell_type":"markdown","metadata":{"id":"e689a7fc"},"source":["## Create dataset"],"id":"e689a7fc"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["a31c60ff4dab48e08d2ef9293d85df6d","c40d970496ff447a8c0b80d787b07a4d","40e6583408c447199ff5b94d23601936","1141ae38bc6f473aab89db14fa4eeacf","47b8a7f3d0544d79b30ad02e4222082e","0260998578564385a0b5b9425a0a5ca1","428ca357bd284d199e2558b1f577d79a","2e32fee744ef42e0aaa89a7b03e82427","d51d3aa414db4aa8b0ccae896e671152","ad49cbf6b6e84ccaab873458182f22a1","5375de82ce3a41a8b5550e0a6b4316c1"]},"executionInfo":{"elapsed":37449,"status":"ok","timestamp":1646023819498,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"df31758e","outputId":"e3ee6910-2896-413b-9bb7-1e1dd630166c"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a31c60ff4dab48e08d2ef9293d85df6d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/42146 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["max length: 433\n"]}],"source":["pn_history_lengths = []\n","tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    pn_history_lengths.append(length)\n","\n","print(\"max length:\", np.max(pn_history_lengths))"],"id":"df31758e"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["8a503d1abd884514a1e23101e03c6781","e06e5e9eb0414b6fad63bdc99b44a313","6b04b019813e458080f02bc9111433a6","2e3818222bab4603a896be5976cb8409","eeb468dbb94943fcb30219d4dd98fcab","5e66444e9c714134bd2765cb3b6d1f15","220f78b6119042af8729543465e1234e","7f1d7796e2174485a0d1b1e9a71d7ade","e72cad76f875451a8e2479e2df237575","9754a5f1e61d49c8972df40ee9290375","25bf78e432e641e0a435dc3626c3ee8a"]},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1646023819500,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"3caff24a","outputId":"09841871-9f3a-4e70-a528-07122a0ebba2"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8a503d1abd884514a1e23101e03c6781","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/143 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["max length: 30\n"]}],"source":["feature_text_lengths = []\n","tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    feature_text_lengths.append(length)\n","\n","print(\"max length:\", np.max(feature_text_lengths))"],"id":"3caff24a"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1646023819500,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"756d83ff","outputId":"02d1e748-4ce8-4d68-f2a2-175f08316e9d"},"outputs":[{"name":"stdout","output_type":"stream","text":["max length: 466\n"]}],"source":["CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n","\n","print(\"max length:\", CFG.max_len)"],"id":"756d83ff"},{"cell_type":"code","execution_count":null,"metadata":{"id":"054b899a"},"outputs":[],"source":["class TrainingDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","        self.annotation_lengths = self.df[\"annotation_length\"].values\n","        self.locations = self.df[\"location\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def _create_label(self, pn_history, annotation_length, location_list):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        offset_mapping = encoded[\"offset_mapping\"]\n","        ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n","        label = np.zeros(len(offset_mapping))\n","        label[ignore_idxes] = -1\n","\n","        if annotation_length > 0:\n","            for location in location_list:\n","                for loc in [s.split() for s in location.split(\";\")]:\n","                    start, end = int(loc[0]), int(loc[1])\n","                    start_idx = -1\n","                    end_idx = -1\n","                    for idx in range(len(offset_mapping)):\n","                        if (start_idx == -1) & (start < offset_mapping[idx][0]):\n","                            start_idx = idx - 1\n","                        if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n","                            end_idx = idx + 1\n","                    if start_idx == -1:\n","                        start_idx = end_idx\n","                    if (start_idx != -1) & (end_idx != -1):\n","                        label[start_idx:end_idx] = 1\n","\n","        return torch.tensor(label, dtype=torch.float)\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n","        return input_, label"],"id":"054b899a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1d58367c"},"outputs":[],"source":["class TestDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        return input_"],"id":"1d58367c"},{"cell_type":"markdown","metadata":{"id":"8c57abef"},"source":["## Model"],"id":"8c57abef"},{"cell_type":"code","execution_count":null,"metadata":{"id":"54f92d89"},"outputs":[],"source":["class CustomModel(nn.Module):\n","    def __init__(self, cfg, model_config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        if model_config_path is None:\n","            self.model_config = AutoConfig.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                output_hidden_states=True,\n","            )\n","        else:\n","            self.model_config = torch.load(model_config_path)\n","\n","        if pretrained:\n","            self.backbone = AutoModel.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                config=self.model_config,\n","            )\n","            print(f\"Load weight from pretrained\")\n","        else:\n","            #self.backbone = AutoModel.from_config(self.model_config)\n","            itpt = AutoModelForMaskedLM.from_config(self.model_config)\n","            path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp009/checkpoint-129000/pytorch_model.bin\")\n","            state_dict = torch.load(path)\n","            itpt.load_state_dict(state_dict)\n","            self.backbone = itpt.deberta\n","            print(f\"Load weight from {path}\")\n","\n","        self.fc = nn.Sequential(\n","            nn.Dropout(self.cfg.dropout),\n","            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n","        )\n","\n","    def forward(self, inputs):\n","        h = self.backbone(**inputs)[\"last_hidden_state\"]\n","        output = self.fc(h)\n","        return output"],"id":"54f92d89"},{"cell_type":"markdown","metadata":{"id":"91401041"},"source":["## Training"],"id":"91401041"},{"cell_type":"code","execution_count":null,"metadata":{"id":"eda8175d"},"outputs":[],"source":["def train_fn(\n","    train_dataloader,\n","    model,\n","    criterion,\n","    optimizer,\n","    epoch,\n","    scheduler,\n","    device,\n","):\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(train_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","\n","        if CFG.batch_scheduler:\n","            scheduler.step()\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n","            print(\n","                \"Epoch: [{0}][{1}/{2}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                \"Grad: {grad_norm:.4f}  \"\n","                \"LR: {lr:.6f}  \"\n","                .format(\n","                    epoch+1,\n","                    step,\n","                    len(train_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n","                    loss=losses,\n","                     grad_norm=grad_norm,\n","                     lr=scheduler.get_lr()[0],\n","                )\n","            )\n","    return losses.avg"],"id":"eda8175d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c44b63a7"},"outputs":[],"source":["def valid_fn(\n","    val_dataloader,\n","    model,\n","    criterion,\n","    device,\n","):\n","    model.eval()\n","    preds = []\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(val_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.no_grad():\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(output.sigmoid().squeeze().detach().cpu().numpy())\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n","            print(\n","                \"EVAL: [{0}/{1}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                .format(\n","                    step, len(val_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n","                    loss=losses,\n","                )\n","            )\n","    preds = np.concatenate(preds)\n","    return losses.avg, preds"],"id":"c44b63a7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4219ac38"},"outputs":[],"source":["def inference_fn(test_dataloader, model, device):\n","    model.eval()\n","    model.to(device)\n","    preds = []\n","    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n","    for inputs in tk0:\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        with torch.no_grad():\n","            output = model(inputs)\n","        preds.append(output.sigmoid().squeeze().detach().cpu().numpy())\n","    preds = np.concatenate(preds)\n","    return preds"],"id":"4219ac38"},{"cell_type":"code","execution_count":null,"metadata":{"id":"014a76b7"},"outputs":[],"source":["def train_loop(df, i_fold, device):\n","    print(f\"========== fold: {i_fold} training ==========\")\n","    train_idx = df[df[\"fold\"] != i_fold].index\n","    val_idx = df[df[\"fold\"] == i_fold].index\n","\n","    train_folds = df.loc[train_idx].reset_index(drop=True)\n","    val_folds = df.loc[val_idx].reset_index(drop=True)\n","\n","    train_dataset = TrainingDataset(CFG, train_folds)\n","    val_dataset = TrainingDataset(CFG, val_folds)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=True,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=True,\n","    )\n","    val_dataloader = DataLoader(\n","        val_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=False,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=False,\n","    )\n","\n","    # model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","    model = CustomModel(CFG, model_config_path=None, pretrained=False)   # itptを使うため\n","    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n","    model.to(device)\n","\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\"params\": [p for n, p in param_optimizer if not any(\n","            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n","        {\"params\": [p for n, p in param_optimizer if any(\n","            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n","    ]\n","    optimizer = AdamW(\n","        optimizer_grouped_parameters,\n","        lr=CFG.lr,\n","        betas=CFG.betas,\n","        weight_decay=CFG.weight_decay,\n","    )\n","    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n","    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        num_training_steps=num_train_optimization_steps,\n","    )\n","\n","    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    best_score = -1 * np.inf\n","\n","    for epoch in range(CFG.epochs):\n","        start_time = time.time()\n","        avg_loss = train_fn(\n","            train_dataloader,\n","            model,\n","            criterion,\n","            optimizer,\n","            epoch,\n","            scheduler,\n","            device,\n","        )\n","        avg_val_loss, val_preds = valid_fn(\n","            val_dataloader,\n","            model,\n","            criterion,\n","            device,\n","        )\n","\n","        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n","            scheduler.step()\n","\n","        # scoring\n","        val_folds[[str(i) for i in range(CFG.max_len)]] = val_preds\n","        score = scoring(val_folds, th=0.5)\n","\n","        elapsed = time.time() - start_time\n","\n","        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n","        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n","        if score > best_score:\n","            best_score = score\n","            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n","            torch.save({\n","                \"model\": model.state_dict(),\n","                \"predictions\": val_preds,\n","                },\n","                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","            )\n","\n","    predictions = torch.load(\n","        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","        map_location=torch.device(\"cpu\"),\n","    )[\"predictions\"]\n","    val_folds[[str(i) for i in range(CFG.max_len)]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return val_folds"],"id":"014a76b7"},{"cell_type":"markdown","metadata":{"id":"c38fb834"},"source":["## Main"],"id":"c38fb834"},{"cell_type":"code","execution_count":null,"metadata":{"id":"62d677cd"},"outputs":[],"source":["def main():\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    if CFG.train:\n","        oof_df = pd.DataFrame()\n","        for i_fold in range(CFG.n_fold):\n","            if i_fold in CFG.train_fold:\n","                _oof_df = train_loop(train, i_fold, device)\n","                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n","        #oof_df.to_csv(CFG.output_dir / \"oof_df.csv\", index=False)\n","        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    if CFG.submission:\n","        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n","    else:\n","        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    score = scoring(oof_df, th=0.5)\n","    print(f\"Best thres: 0.5, Score: {score:.4f}\")\n","    best_thres = get_best_thres(oof_df)\n","    score = scoring(oof_df, th=best_thres)\n","    print(f\"Best thres: {best_thres}, Score: {score:.4f}\")\n","\n","    if CFG.inference:\n","        test_dataset = TestDataset(CFG, test)\n","        test_dataloader = DataLoader(\n","            test_dataset,\n","            batch_size=CFG.batch_size,\n","            shuffle=False,\n","            num_workers=CFG.num_workers,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","        predictions = []\n","        for i_fold in CFG.train_fold:\n","            if CFG.submission:\n","                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n","                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n","            else:\n","                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n","\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            model.load_state_dict(state[\"model\"])\n","            test_token_probs = inference_fn(test_dataloader, model, device)\n","            test[[f\"fold{i_fold}_{i}\" for i in range(CFG.max_len)]] = test_token_probs\n","            test_char_probs = get_char_probs(test[\"pn_history\"].values, test_token_probs, CFG.tokenizer)\n","            predictions.append(test_char_probs)\n","\n","            del state, test_token_probs, model; gc.collect()\n","            torch.cuda.empty_cache()\n","\n","        predictions = np.mean(predictions, axis=0)\n","        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n","        test[CFG.target_col] = predicted_location_str\n","        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n","        test[[CFG.id_col, CFG.target_col]].to_csv(\n","            CFG.output_dir / \"submission.csv\", index=False\n","        )"],"id":"62d677cd"},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5a00641beddc46eb8da430f2d9999490","04966868d2974cb8b3215a50572c2c94","5a5c41748cba4234a6a6f9aabddfa861","72044a7dbe7d4b5f839f38f6e827ec63","bed6a691643a46d5bd25e03cdc5b73f7","b4d0bd0dea5341a9b03f0092fe3cba39"]},"id":"1d4fcf7c","outputId":"1362d223-3d70-4ba7-daa5-14b7300eef5c","executionInfo":{"status":"ok","timestamp":1646034258180,"user_tz":-540,"elapsed":315,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["========== fold: 0 training ==========\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp009/checkpoint-129000/pytorch_model.bin\n","Epoch: [1][0/1424] Elapsed 0m 0s (remain 16m 35s) Loss: 0.7267(0.7267) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/1424] Elapsed 0m 22s (remain 4m 56s) Loss: 0.2028(0.5351) Grad: 26314.0898  LR: 0.000003  \n","Epoch: [1][200/1424] Elapsed 0m 44s (remain 4m 31s) Loss: 0.0781(0.3148) Grad: 1818.7069  LR: 0.000006  \n","Epoch: [1][300/1424] Elapsed 1m 6s (remain 4m 9s) Loss: 0.0953(0.2337) Grad: 3815.1091  LR: 0.000008  \n","Epoch: [1][400/1424] Elapsed 1m 29s (remain 3m 48s) Loss: 0.0181(0.1889) Grad: 2548.2700  LR: 0.000011  \n","Epoch: [1][500/1424] Elapsed 1m 51s (remain 3m 25s) Loss: 0.0667(0.1574) Grad: 5631.8652  LR: 0.000014  \n","Epoch: [1][600/1424] Elapsed 2m 13s (remain 3m 2s) Loss: 0.0185(0.1354) Grad: 5983.2803  LR: 0.000017  \n","Epoch: [1][700/1424] Elapsed 2m 35s (remain 2m 40s) Loss: 0.0072(0.1191) Grad: 1769.6704  LR: 0.000020  \n","Epoch: [1][800/1424] Elapsed 2m 57s (remain 2m 18s) Loss: 0.0063(0.1066) Grad: 2456.5833  LR: 0.000020  \n","Epoch: [1][900/1424] Elapsed 3m 19s (remain 1m 55s) Loss: 0.0145(0.0970) Grad: 3083.3716  LR: 0.000019  \n","Epoch: [1][1000/1424] Elapsed 3m 41s (remain 1m 33s) Loss: 0.0321(0.0889) Grad: 5658.8262  LR: 0.000019  \n","Epoch: [1][1100/1424] Elapsed 4m 3s (remain 1m 11s) Loss: 0.0064(0.0823) Grad: 7122.9814  LR: 0.000019  \n","Epoch: [1][1200/1424] Elapsed 4m 25s (remain 0m 49s) Loss: 0.0039(0.0768) Grad: 2331.2703  LR: 0.000018  \n","Epoch: [1][1300/1424] Elapsed 4m 47s (remain 0m 27s) Loss: 0.0108(0.0719) Grad: 2532.6074  LR: 0.000018  \n","Epoch: [1][1400/1424] Elapsed 5m 9s (remain 0m 5s) Loss: 0.0143(0.0679) Grad: 2379.6904  LR: 0.000018  \n","Epoch: [1][1423/1424] Elapsed 5m 14s (remain 0m 0s) Loss: 0.0153(0.0670) Grad: 5089.8013  LR: 0.000018  \n","EVAL: [0/363] Elapsed 0m 0s (remain 2m 33s) Loss: 0.0265(0.0265) \n","EVAL: [100/363] Elapsed 0m 11s (remain 0m 31s) Loss: 0.0133(0.0145) \n","EVAL: [200/363] Elapsed 0m 23s (remain 0m 19s) Loss: 0.0191(0.0154) \n","EVAL: [300/363] Elapsed 0m 35s (remain 0m 7s) Loss: 0.0062(0.0144) \n","EVAL: [362/363] Elapsed 0m 42s (remain 0m 0s) Loss: 0.0049(0.0132) \n","Epoch 1 - avg_train_loss: 0.0670  avg_val_loss: 0.0132  time: 362s\n","Epoch 1 - Score: 0.8424\n","Epoch 1 - Save Best Score: 0.8424 Model\n","Epoch: [2][0/1424] Elapsed 0m 0s (remain 12m 11s) Loss: 0.0026(0.0026) Grad: 4622.3286  LR: 0.000018  \n","Epoch: [2][100/1424] Elapsed 0m 22s (remain 4m 58s) Loss: 0.0151(0.0116) Grad: 73435.7266  LR: 0.000017  \n","Epoch: [2][200/1424] Elapsed 0m 44s (remain 4m 31s) Loss: 0.0131(0.0109) Grad: 16347.4316  LR: 0.000017  \n","Epoch: [2][300/1424] Elapsed 1m 6s (remain 4m 8s) Loss: 0.0071(0.0105) Grad: 69022.0234  LR: 0.000017  \n","Epoch: [2][400/1424] Elapsed 1m 28s (remain 3m 46s) Loss: 0.0093(0.0109) Grad: 20254.5605  LR: 0.000017  \n","Epoch: [2][500/1424] Elapsed 1m 50s (remain 3m 23s) Loss: 0.0033(0.0111) Grad: 5774.9067  LR: 0.000016  \n","Epoch: [2][600/1424] Elapsed 2m 12s (remain 3m 1s) Loss: 0.0027(0.0110) Grad: 10198.3516  LR: 0.000016  \n","Epoch: [2][700/1424] Elapsed 2m 34s (remain 2m 39s) Loss: 0.0075(0.0112) Grad: 15804.2178  LR: 0.000016  \n","Epoch: [2][800/1424] Elapsed 2m 56s (remain 2m 17s) Loss: 0.0175(0.0111) Grad: 23471.1621  LR: 0.000015  \n","Epoch: [2][900/1424] Elapsed 3m 18s (remain 1m 55s) Loss: 0.0094(0.0108) Grad: 19100.2949  LR: 0.000015  \n","Epoch: [2][1000/1424] Elapsed 3m 40s (remain 1m 33s) Loss: 0.0070(0.0109) Grad: 14867.0732  LR: 0.000015  \n","Epoch: [2][1100/1424] Elapsed 4m 2s (remain 1m 11s) Loss: 0.0101(0.0108) Grad: 10185.1641  LR: 0.000014  \n","Epoch: [2][1200/1424] Elapsed 4m 23s (remain 0m 49s) Loss: 0.0349(0.0109) Grad: 22532.5234  LR: 0.000014  \n","Epoch: [2][1300/1424] Elapsed 4m 45s (remain 0m 27s) Loss: 0.0004(0.0109) Grad: 1032.9568  LR: 0.000014  \n","Epoch: [2][1400/1424] Elapsed 5m 7s (remain 0m 5s) Loss: 0.0053(0.0110) Grad: 31330.9277  LR: 0.000013  \n","Epoch: [2][1423/1424] Elapsed 5m 12s (remain 0m 0s) Loss: 0.0136(0.0110) Grad: 70457.2266  LR: 0.000013  \n","EVAL: [0/363] Elapsed 0m 0s (remain 2m 16s) Loss: 0.0091(0.0091) \n","EVAL: [100/363] Elapsed 0m 11s (remain 0m 30s) Loss: 0.0094(0.0127) \n","EVAL: [200/363] Elapsed 0m 23s (remain 0m 18s) Loss: 0.0141(0.0150) \n","EVAL: [300/363] Elapsed 0m 35s (remain 0m 7s) Loss: 0.0041(0.0135) \n","EVAL: [362/363] Elapsed 0m 42s (remain 0m 0s) Loss: 0.0046(0.0123) \n","Epoch 2 - avg_train_loss: 0.0110  avg_val_loss: 0.0123  time: 359s\n","Epoch 2 - Score: 0.8700\n","Epoch 2 - Save Best Score: 0.8700 Model\n","Epoch: [3][0/1424] Elapsed 0m 0s (remain 14m 7s) Loss: 0.0082(0.0082) Grad: 10939.4561  LR: 0.000013  \n","Epoch: [3][100/1424] Elapsed 0m 22s (remain 4m 58s) Loss: 0.0130(0.0069) Grad: 18279.8066  LR: 0.000013  \n","Epoch: [3][200/1424] Elapsed 0m 44s (remain 4m 32s) Loss: 0.0062(0.0081) Grad: 12231.7744  LR: 0.000013  \n","Epoch: [3][300/1424] Elapsed 1m 6s (remain 4m 8s) Loss: 0.0160(0.0084) Grad: 49107.9180  LR: 0.000012  \n","Epoch: [3][400/1424] Elapsed 1m 28s (remain 3m 46s) Loss: 0.0017(0.0084) Grad: 5742.6538  LR: 0.000012  \n","Epoch: [3][500/1424] Elapsed 1m 50s (remain 3m 23s) Loss: 0.0029(0.0086) Grad: 7064.4062  LR: 0.000012  \n","Epoch: [3][600/1424] Elapsed 2m 12s (remain 3m 1s) Loss: 0.0106(0.0086) Grad: 8264.6768  LR: 0.000011  \n","Epoch: [3][700/1424] Elapsed 2m 34s (remain 2m 39s) Loss: 0.0022(0.0087) Grad: 7896.4077  LR: 0.000011  \n","Epoch: [3][800/1424] Elapsed 2m 56s (remain 2m 17s) Loss: 0.0070(0.0086) Grad: 26031.6309  LR: 0.000011  \n","Epoch: [3][900/1424] Elapsed 3m 18s (remain 1m 55s) Loss: 0.0068(0.0086) Grad: 18378.9434  LR: 0.000011  \n","Epoch: [3][1000/1424] Elapsed 3m 40s (remain 1m 32s) Loss: 0.0022(0.0086) Grad: 9067.4580  LR: 0.000010  \n","Epoch: [3][1100/1424] Elapsed 4m 1s (remain 1m 10s) Loss: 0.0305(0.0086) Grad: 36800.5586  LR: 0.000010  \n","Epoch: [3][1200/1424] Elapsed 4m 23s (remain 0m 48s) Loss: 0.0015(0.0085) Grad: 7609.1406  LR: 0.000010  \n","Epoch: [3][1300/1424] Elapsed 4m 45s (remain 0m 27s) Loss: 0.0298(0.0085) Grad: 46907.0352  LR: 0.000009  \n","Epoch: [3][1400/1424] Elapsed 5m 7s (remain 0m 5s) Loss: 0.0037(0.0085) Grad: 7521.9482  LR: 0.000009  \n","Epoch: [3][1423/1424] Elapsed 5m 12s (remain 0m 0s) Loss: 0.0030(0.0085) Grad: 11075.1094  LR: 0.000009  \n","EVAL: [0/363] Elapsed 0m 0s (remain 2m 11s) Loss: 0.0093(0.0093) \n","EVAL: [100/363] Elapsed 0m 11s (remain 0m 30s) Loss: 0.0125(0.0117) \n","EVAL: [200/363] Elapsed 0m 23s (remain 0m 18s) Loss: 0.0059(0.0138) \n","EVAL: [300/363] Elapsed 0m 35s (remain 0m 7s) Loss: 0.0031(0.0125) \n","EVAL: [362/363] Elapsed 0m 42s (remain 0m 0s) Loss: 0.0040(0.0115) \n","Epoch 3 - avg_train_loss: 0.0085  avg_val_loss: 0.0115  time: 360s\n","Epoch 3 - Score: 0.8755\n","Epoch 3 - Save Best Score: 0.8755 Model\n","Epoch: [4][0/1424] Elapsed 0m 0s (remain 13m 1s) Loss: 0.0024(0.0024) Grad: 5737.6309  LR: 0.000009  \n","Epoch: [4][100/1424] Elapsed 0m 22s (remain 4m 59s) Loss: 0.0101(0.0079) Grad: 24614.8398  LR: 0.000009  \n","Epoch: [4][200/1424] Elapsed 0m 44s (remain 4m 32s) Loss: 0.0001(0.0071) Grad: 564.3566  LR: 0.000008  \n","Epoch: [4][300/1424] Elapsed 1m 6s (remain 4m 8s) Loss: 0.0056(0.0074) Grad: 24187.8438  LR: 0.000008  \n","Epoch: [4][400/1424] Elapsed 1m 28s (remain 3m 45s) Loss: 0.0029(0.0077) Grad: 5091.1440  LR: 0.000008  \n","Epoch: [4][500/1424] Elapsed 1m 50s (remain 3m 23s) Loss: 0.0051(0.0074) Grad: 13162.9131  LR: 0.000007  \n","Epoch: [4][600/1424] Elapsed 2m 12s (remain 3m 1s) Loss: 0.0003(0.0075) Grad: 810.3823  LR: 0.000007  \n","Epoch: [4][700/1424] Elapsed 2m 34s (remain 2m 39s) Loss: 0.0216(0.0075) Grad: 51258.2227  LR: 0.000007  \n","Epoch: [4][800/1424] Elapsed 2m 56s (remain 2m 17s) Loss: 0.0100(0.0073) Grad: 27195.0742  LR: 0.000006  \n","Epoch: [4][900/1424] Elapsed 3m 18s (remain 1m 55s) Loss: 0.0998(0.0073) Grad: 128388.0703  LR: 0.000006  \n","Epoch: [4][1000/1424] Elapsed 3m 40s (remain 1m 33s) Loss: 0.0092(0.0072) Grad: 60709.5625  LR: 0.000006  \n","Epoch: [4][1100/1424] Elapsed 4m 2s (remain 1m 11s) Loss: 0.0017(0.0071) Grad: 6547.2271  LR: 0.000005  \n","Epoch: [4][1200/1424] Elapsed 4m 24s (remain 0m 49s) Loss: 0.0068(0.0069) Grad: 21531.7148  LR: 0.000005  \n","Epoch: [4][1300/1424] Elapsed 4m 45s (remain 0m 27s) Loss: 0.0024(0.0070) Grad: 6296.9849  LR: 0.000005  \n","Epoch: [4][1400/1424] Elapsed 5m 7s (remain 0m 5s) Loss: 0.0001(0.0070) Grad: 526.5366  LR: 0.000005  \n","Epoch: [4][1423/1424] Elapsed 5m 12s (remain 0m 0s) Loss: 0.0022(0.0069) Grad: 14864.1299  LR: 0.000004  \n","EVAL: [0/363] Elapsed 0m 0s (remain 2m 6s) Loss: 0.0070(0.0070) \n","EVAL: [100/363] Elapsed 0m 11s (remain 0m 30s) Loss: 0.0142(0.0133) \n","EVAL: [200/363] Elapsed 0m 23s (remain 0m 18s) Loss: 0.0171(0.0152) \n","EVAL: [300/363] Elapsed 0m 35s (remain 0m 7s) Loss: 0.0025(0.0137) \n","EVAL: [362/363] Elapsed 0m 42s (remain 0m 0s) Loss: 0.0055(0.0126) \n","Epoch 4 - avg_train_loss: 0.0069  avg_val_loss: 0.0126  time: 360s\n","Epoch 4 - Score: 0.8754\n","Epoch: [5][0/1424] Elapsed 0m 0s (remain 12m 46s) Loss: 0.0093(0.0093) Grad: 37203.5664  LR: 0.000004  \n","Epoch: [5][100/1424] Elapsed 0m 22s (remain 4m 54s) Loss: 0.0012(0.0048) Grad: 4607.7808  LR: 0.000004  \n","Epoch: [5][200/1424] Elapsed 0m 44s (remain 4m 29s) Loss: 0.0009(0.0053) Grad: 5897.1260  LR: 0.000004  \n","Epoch: [5][300/1424] Elapsed 1m 6s (remain 4m 7s) Loss: 0.0034(0.0057) Grad: 9151.5400  LR: 0.000004  \n","Epoch: [5][400/1424] Elapsed 1m 28s (remain 3m 44s) Loss: 0.0100(0.0058) Grad: 22254.1660  LR: 0.000003  \n","Epoch: [5][500/1424] Elapsed 1m 50s (remain 3m 22s) Loss: 0.0002(0.0058) Grad: 2024.1362  LR: 0.000003  \n","Epoch: [5][600/1424] Elapsed 2m 12s (remain 3m 0s) Loss: 0.0032(0.0059) Grad: 6495.1309  LR: 0.000003  \n","Epoch: [5][700/1424] Elapsed 2m 33s (remain 2m 38s) Loss: 0.0011(0.0060) Grad: 15319.5186  LR: 0.000002  \n","Epoch: [5][800/1424] Elapsed 2m 55s (remain 2m 16s) Loss: 0.0001(0.0058) Grad: 509.3255  LR: 0.000002  \n","Epoch: [5][900/1424] Elapsed 3m 17s (remain 1m 54s) Loss: 0.0093(0.0056) Grad: 28835.4629  LR: 0.000002  \n","Epoch: [5][1000/1424] Elapsed 3m 39s (remain 1m 32s) Loss: 0.0219(0.0058) Grad: 36695.4375  LR: 0.000001  \n","Epoch: [5][1100/1424] Elapsed 4m 1s (remain 1m 10s) Loss: 0.0037(0.0058) Grad: 30130.0820  LR: 0.000001  \n","Epoch: [5][1200/1424] Elapsed 4m 23s (remain 0m 48s) Loss: 0.0101(0.0059) Grad: 52390.7578  LR: 0.000001  \n","Epoch: [5][1300/1424] Elapsed 4m 45s (remain 0m 26s) Loss: 0.0001(0.0059) Grad: 278.0380  LR: 0.000000  \n","Epoch: [5][1400/1424] Elapsed 5m 7s (remain 0m 5s) Loss: 0.0039(0.0059) Grad: 28040.1152  LR: 0.000000  \n","Epoch: [5][1423/1424] Elapsed 5m 12s (remain 0m 0s) Loss: 0.0012(0.0059) Grad: 10687.9219  LR: 0.000000  \n","EVAL: [0/363] Elapsed 0m 0s (remain 2m 10s) Loss: 0.0048(0.0048) \n","EVAL: [100/363] Elapsed 0m 11s (remain 0m 30s) Loss: 0.0154(0.0137) \n","EVAL: [200/363] Elapsed 0m 23s (remain 0m 18s) Loss: 0.0206(0.0155) \n","EVAL: [300/363] Elapsed 0m 34s (remain 0m 7s) Loss: 0.0044(0.0140) \n","EVAL: [362/363] Elapsed 0m 42s (remain 0m 0s) Loss: 0.0053(0.0129) \n","Epoch 5 - avg_train_loss: 0.0059  avg_val_loss: 0.0129  time: 359s\n","Epoch 5 - Score: 0.8760\n","Epoch 5 - Save Best Score: 0.8760 Model\n","========== fold: 1 training ==========\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp009/checkpoint-129000/pytorch_model.bin\n","Epoch: [1][0/1425] Elapsed 0m 0s (remain 12m 18s) Loss: 0.6656(0.6656) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/1425] Elapsed 0m 22s (remain 4m 57s) Loss: 0.1858(0.4736) Grad: 22255.9941  LR: 0.000003  \n","Epoch: [1][200/1425] Elapsed 0m 44s (remain 4m 31s) Loss: 0.0546(0.2828) Grad: 1036.3805  LR: 0.000006  \n","Epoch: [1][300/1425] Elapsed 1m 6s (remain 4m 8s) Loss: 0.0665(0.2119) Grad: 1984.7518  LR: 0.000008  \n","Epoch: [1][400/1425] Elapsed 1m 28s (remain 3m 45s) Loss: 0.0360(0.1720) Grad: 5404.6313  LR: 0.000011  \n","Epoch: [1][500/1425] Elapsed 1m 50s (remain 3m 23s) Loss: 0.0241(0.1444) Grad: 5320.8945  LR: 0.000014  \n","Epoch: [1][600/1425] Elapsed 2m 12s (remain 3m 1s) Loss: 0.0144(0.1246) Grad: 2119.2827  LR: 0.000017  \n","Epoch: [1][700/1425] Elapsed 2m 34s (remain 2m 39s) Loss: 0.0215(0.1095) Grad: 8585.7656  LR: 0.000020  \n","Epoch: [1][800/1425] Elapsed 2m 56s (remain 2m 17s) Loss: 0.0364(0.0987) Grad: 14041.4736  LR: 0.000020  \n","Epoch: [1][900/1425] Elapsed 3m 17s (remain 1m 55s) Loss: 0.0203(0.0897) Grad: 5042.4683  LR: 0.000019  \n","Epoch: [1][1000/1425] Elapsed 3m 39s (remain 1m 33s) Loss: 0.0234(0.0825) Grad: 4159.4243  LR: 0.000019  \n","Epoch: [1][1100/1425] Elapsed 4m 1s (remain 1m 11s) Loss: 0.0153(0.0765) Grad: 4188.4800  LR: 0.000019  \n","Epoch: [1][1200/1425] Elapsed 4m 23s (remain 0m 49s) Loss: 0.0063(0.0715) Grad: 1647.4734  LR: 0.000018  \n","Epoch: [1][1300/1425] Elapsed 4m 45s (remain 0m 27s) Loss: 0.0044(0.0671) Grad: 1240.4224  LR: 0.000018  \n","Epoch: [1][1400/1425] Elapsed 5m 7s (remain 0m 5s) Loss: 0.0008(0.0633) Grad: 243.4217  LR: 0.000018  \n","Epoch: [1][1424/1425] Elapsed 5m 12s (remain 0m 0s) Loss: 0.0061(0.0624) Grad: 1886.9121  LR: 0.000018  \n","EVAL: [0/362] Elapsed 0m 0s (remain 2m 11s) Loss: 0.0107(0.0107) \n","EVAL: [100/362] Elapsed 0m 11s (remain 0m 30s) Loss: 0.0074(0.0144) \n","EVAL: [200/362] Elapsed 0m 23s (remain 0m 18s) Loss: 0.0132(0.0134) \n","EVAL: [300/362] Elapsed 0m 35s (remain 0m 7s) Loss: 0.0122(0.0143) \n","EVAL: [361/362] Elapsed 0m 42s (remain 0m 0s) Loss: 0.0056(0.0133) \n","Epoch 1 - avg_train_loss: 0.0624  avg_val_loss: 0.0133  time: 359s\n","Epoch 1 - Score: 0.8445\n","Epoch 1 - Save Best Score: 0.8445 Model\n","Epoch: [2][0/1425] Elapsed 0m 0s (remain 13m 54s) Loss: 0.0036(0.0036) Grad: 10134.6094  LR: 0.000018  \n","Epoch: [2][100/1425] Elapsed 0m 23s (remain 5m 2s) Loss: 0.0036(0.0119) Grad: 9233.4395  LR: 0.000017  \n","Epoch: [2][200/1425] Elapsed 0m 44s (remain 4m 34s) Loss: 0.0155(0.0112) Grad: 24947.0410  LR: 0.000017  \n","Epoch: [2][300/1425] Elapsed 1m 6s (remain 4m 9s) Loss: 0.0063(0.0105) Grad: 13372.6084  LR: 0.000017  \n","Epoch: [2][400/1425] Elapsed 1m 28s (remain 3m 46s) Loss: 0.0227(0.0106) Grad: 63599.7891  LR: 0.000017  \n","Epoch: [2][500/1425] Elapsed 1m 50s (remain 3m 24s) Loss: 0.0155(0.0110) Grad: 23618.3184  LR: 0.000016  \n","Epoch: [2][600/1425] Elapsed 2m 12s (remain 3m 1s) Loss: 0.0341(0.0111) Grad: 38370.6328  LR: 0.000016  \n","Epoch: [2][700/1425] Elapsed 2m 34s (remain 2m 39s) Loss: 0.0106(0.0109) Grad: 17003.3105  LR: 0.000016  \n","Epoch: [2][800/1425] Elapsed 2m 56s (remain 2m 17s) Loss: 0.0202(0.0109) Grad: 32012.9824  LR: 0.000015  \n","Epoch: [2][900/1425] Elapsed 3m 18s (remain 1m 55s) Loss: 0.0105(0.0110) Grad: 26404.4434  LR: 0.000015  \n","Epoch: [2][1000/1425] Elapsed 3m 40s (remain 1m 33s) Loss: 0.0006(0.0109) Grad: 4236.9268  LR: 0.000015  \n","Epoch: [2][1100/1425] Elapsed 4m 2s (remain 1m 11s) Loss: 0.0249(0.0109) Grad: 28624.1172  LR: 0.000014  \n","Epoch: [2][1200/1425] Elapsed 4m 23s (remain 0m 49s) Loss: 0.0026(0.0110) Grad: 4077.5310  LR: 0.000014  \n","Epoch: [2][1300/1425] Elapsed 4m 45s (remain 0m 27s) Loss: 0.0290(0.0110) Grad: 47763.2227  LR: 0.000014  \n","Epoch: [2][1400/1425] Elapsed 5m 7s (remain 0m 5s) Loss: 0.0077(0.0110) Grad: 11694.1855  LR: 0.000013  \n","Epoch: [2][1424/1425] Elapsed 5m 13s (remain 0m 0s) Loss: 0.0096(0.0110) Grad: 43576.6328  LR: 0.000013  \n","EVAL: [0/362] Elapsed 0m 0s (remain 2m 19s) Loss: 0.0014(0.0014) \n","EVAL: [100/362] Elapsed 0m 11s (remain 0m 30s) Loss: 0.0018(0.0118) \n","EVAL: [200/362] Elapsed 0m 23s (remain 0m 18s) Loss: 0.0093(0.0112) \n","EVAL: [300/362] Elapsed 0m 35s (remain 0m 7s) Loss: 0.0037(0.0119) \n","EVAL: [361/362] Elapsed 0m 42s (remain 0m 0s) Loss: 0.0042(0.0108) \n","Epoch 2 - avg_train_loss: 0.0110  avg_val_loss: 0.0108  time: 360s\n","Epoch 2 - Score: 0.8694\n","Epoch 2 - Save Best Score: 0.8694 Model\n","Epoch: [3][0/1425] Elapsed 0m 0s (remain 13m 47s) Loss: 0.0009(0.0009) Grad: 4324.7075  LR: 0.000013  \n","Epoch: [3][100/1425] Elapsed 0m 22s (remain 4m 58s) Loss: 0.0048(0.0090) Grad: 5267.2241  LR: 0.000013  \n","Epoch: [3][200/1425] Elapsed 0m 44s (remain 4m 31s) Loss: 0.0011(0.0080) Grad: 3952.5049  LR: 0.000013  \n","Epoch: [3][300/1425] Elapsed 1m 6s (remain 4m 8s) Loss: 0.0005(0.0084) Grad: 2406.0579  LR: 0.000012  \n","Epoch: [3][400/1425] Elapsed 1m 28s (remain 3m 45s) Loss: 0.0183(0.0088) Grad: 33911.9102  LR: 0.000012  \n","Epoch: [3][500/1425] Elapsed 1m 50s (remain 3m 23s) Loss: 0.0048(0.0091) Grad: 19265.1426  LR: 0.000012  \n","Epoch: [3][600/1425] Elapsed 2m 12s (remain 3m 1s) Loss: 0.0054(0.0090) Grad: 11120.7686  LR: 0.000011  \n","Epoch: [3][700/1425] Elapsed 2m 34s (remain 2m 39s) Loss: 0.0043(0.0090) Grad: 26752.3984  LR: 0.000011  \n","Epoch: [3][800/1425] Elapsed 2m 56s (remain 2m 17s) Loss: 0.0203(0.0091) Grad: 39748.3516  LR: 0.000011  \n","Epoch: [3][900/1425] Elapsed 3m 18s (remain 1m 55s) Loss: 0.0062(0.0090) Grad: 13003.7002  LR: 0.000011  \n","Epoch: [3][1000/1425] Elapsed 3m 39s (remain 1m 33s) Loss: 0.0014(0.0090) Grad: 4222.9199  LR: 0.000010  \n","Epoch: [3][1100/1425] Elapsed 4m 1s (remain 1m 11s) Loss: 0.0082(0.0089) Grad: 14801.9160  LR: 0.000010  \n","Epoch: [3][1200/1425] Elapsed 4m 23s (remain 0m 49s) Loss: 0.0031(0.0089) Grad: 7497.4907  LR: 0.000010  \n","Epoch: [3][1300/1425] Elapsed 4m 45s (remain 0m 27s) Loss: 0.0117(0.0087) Grad: 17000.2852  LR: 0.000009  \n","Epoch: [3][1400/1425] Elapsed 5m 7s (remain 0m 5s) Loss: 0.0045(0.0086) Grad: 26930.1895  LR: 0.000009  \n","Epoch: [3][1424/1425] Elapsed 5m 12s (remain 0m 0s) Loss: 0.0126(0.0086) Grad: 46202.8281  LR: 0.000009  \n","EVAL: [0/362] Elapsed 0m 0s (remain 2m 13s) Loss: 0.0011(0.0011) \n","EVAL: [100/362] Elapsed 0m 11s (remain 0m 30s) Loss: 0.0017(0.0127) \n","EVAL: [200/362] Elapsed 0m 23s (remain 0m 18s) Loss: 0.0028(0.0116) \n","EVAL: [300/362] Elapsed 0m 35s (remain 0m 7s) Loss: 0.0017(0.0124) \n","EVAL: [361/362] Elapsed 0m 42s (remain 0m 0s) Loss: 0.0047(0.0113) \n","Epoch 3 - avg_train_loss: 0.0086  avg_val_loss: 0.0113  time: 359s\n","Epoch 3 - Score: 0.8785\n","Epoch 3 - Save Best Score: 0.8785 Model\n","Epoch: [4][0/1425] Elapsed 0m 0s (remain 13m 3s) Loss: 0.0060(0.0060) Grad: 21116.1699  LR: 0.000009  \n","Epoch: [4][100/1425] Elapsed 0m 22s (remain 4m 59s) Loss: 0.0034(0.0057) Grad: 10895.2686  LR: 0.000009  \n","Epoch: [4][200/1425] Elapsed 0m 44s (remain 4m 32s) Loss: 0.0127(0.0060) Grad: 51702.4414  LR: 0.000008  \n","Epoch: [4][300/1425] Elapsed 1m 6s (remain 4m 9s) Loss: 0.0849(0.0062) Grad: 123976.9141  LR: 0.000008  \n","Epoch: [4][400/1425] Elapsed 1m 28s (remain 3m 46s) Loss: 0.0132(0.0068) Grad: 98027.1094  LR: 0.000008  \n","Epoch: [4][500/1425] Elapsed 1m 50s (remain 3m 23s) Loss: 0.0086(0.0067) Grad: 17689.8555  LR: 0.000007  \n","Epoch: [4][600/1425] Elapsed 2m 12s (remain 3m 1s) Loss: 0.0008(0.0068) Grad: 11752.8945  LR: 0.000007  \n","Epoch: [4][700/1425] Elapsed 2m 34s (remain 2m 39s) Loss: 0.0104(0.0069) Grad: 36155.5742  LR: 0.000007  \n","Epoch: [4][800/1425] Elapsed 2m 56s (remain 2m 17s) Loss: 0.0005(0.0070) Grad: 1841.4435  LR: 0.000006  \n","Epoch: [4][900/1425] Elapsed 3m 18s (remain 1m 55s) Loss: 0.0032(0.0070) Grad: 9031.1631  LR: 0.000006  \n","Epoch: [4][1000/1425] Elapsed 3m 39s (remain 1m 33s) Loss: 0.0002(0.0070) Grad: 1248.5415  LR: 0.000006  \n","Epoch: [4][1100/1425] Elapsed 4m 1s (remain 1m 11s) Loss: 0.0030(0.0070) Grad: 5410.0469  LR: 0.000005  \n","Epoch: [4][1200/1425] Elapsed 4m 23s (remain 0m 49s) Loss: 0.0028(0.0069) Grad: 9002.4346  LR: 0.000005  \n","Epoch: [4][1300/1425] Elapsed 4m 45s (remain 0m 27s) Loss: 0.0025(0.0070) Grad: 8362.9053  LR: 0.000005  \n","Epoch: [4][1400/1425] Elapsed 5m 7s (remain 0m 5s) Loss: 0.0023(0.0069) Grad: 5817.5796  LR: 0.000005  \n","Epoch: [4][1424/1425] Elapsed 5m 12s (remain 0m 0s) Loss: 0.0020(0.0069) Grad: 8300.2285  LR: 0.000004  \n","EVAL: [0/362] Elapsed 0m 0s (remain 2m 24s) Loss: 0.0016(0.0016) \n","EVAL: [100/362] Elapsed 0m 11s (remain 0m 30s) Loss: 0.0013(0.0127) \n","EVAL: [200/362] Elapsed 0m 23s (remain 0m 18s) Loss: 0.0040(0.0119) \n","EVAL: [300/362] Elapsed 0m 35s (remain 0m 7s) Loss: 0.0009(0.0127) \n","EVAL: [361/362] Elapsed 0m 42s (remain 0m 0s) Loss: 0.0035(0.0116) \n","Epoch 4 - avg_train_loss: 0.0069  avg_val_loss: 0.0116  time: 359s\n","Epoch 4 - Score: 0.8803\n","Epoch 4 - Save Best Score: 0.8803 Model\n","Epoch: [5][0/1425] Elapsed 0m 0s (remain 13m 26s) Loss: 0.0097(0.0097) Grad: 28250.1816  LR: 0.000004  \n","Epoch: [5][100/1425] Elapsed 0m 22s (remain 4m 58s) Loss: 0.0022(0.0065) Grad: 6262.1343  LR: 0.000004  \n","Epoch: [5][200/1425] Elapsed 0m 44s (remain 4m 32s) Loss: 0.0045(0.0066) Grad: 13212.7031  LR: 0.000004  \n","Epoch: [5][300/1425] Elapsed 1m 6s (remain 4m 8s) Loss: 0.0035(0.0065) Grad: 24119.5352  LR: 0.000004  \n","Epoch: [5][400/1425] Elapsed 1m 28s (remain 3m 46s) Loss: 0.0132(0.0063) Grad: 43304.3008  LR: 0.000003  \n","Epoch: [5][500/1425] Elapsed 1m 50s (remain 3m 23s) Loss: 0.0268(0.0062) Grad: 55236.6797  LR: 0.000003  \n","Epoch: [5][600/1425] Elapsed 2m 12s (remain 3m 1s) Loss: 0.0028(0.0062) Grad: 11499.0381  LR: 0.000003  \n","Epoch: [5][700/1425] Elapsed 2m 34s (remain 2m 39s) Loss: 0.0013(0.0062) Grad: 3641.2896  LR: 0.000002  \n","Epoch: [5][800/1425] Elapsed 2m 56s (remain 2m 17s) Loss: 0.0026(0.0062) Grad: 8532.6445  LR: 0.000002  \n","Epoch: [5][900/1425] Elapsed 3m 18s (remain 1m 55s) Loss: 0.0014(0.0061) Grad: 3971.0559  LR: 0.000002  \n","Epoch: [5][1000/1425] Elapsed 3m 39s (remain 1m 33s) Loss: 0.0014(0.0060) Grad: 4982.7764  LR: 0.000001  \n","Epoch: [5][1100/1425] Elapsed 4m 1s (remain 1m 11s) Loss: 0.0085(0.0060) Grad: 13578.5332  LR: 0.000001  \n","Epoch: [5][1200/1425] Elapsed 4m 23s (remain 0m 49s) Loss: 0.0090(0.0059) Grad: 51727.6875  LR: 0.000001  \n","Epoch: [5][1300/1425] Elapsed 4m 45s (remain 0m 27s) Loss: 0.0358(0.0059) Grad: 45966.7344  LR: 0.000000  \n","Epoch: [5][1400/1425] Elapsed 5m 7s (remain 0m 5s) Loss: 0.0022(0.0059) Grad: 8604.8193  LR: 0.000000  \n","Epoch: [5][1424/1425] Elapsed 5m 12s (remain 0m 0s) Loss: 0.0048(0.0058) Grad: 17369.4824  LR: 0.000000  \n","EVAL: [0/362] Elapsed 0m 0s (remain 2m 13s) Loss: 0.0013(0.0013) \n","EVAL: [100/362] Elapsed 0m 11s (remain 0m 30s) Loss: 0.0013(0.0136) \n","EVAL: [200/362] Elapsed 0m 23s (remain 0m 18s) Loss: 0.0027(0.0126) \n","EVAL: [300/362] Elapsed 0m 35s (remain 0m 7s) Loss: 0.0012(0.0135) \n","EVAL: [361/362] Elapsed 0m 42s (remain 0m 0s) Loss: 0.0034(0.0123) \n","Epoch 5 - avg_train_loss: 0.0058  avg_val_loss: 0.0123  time: 360s\n","Epoch 5 - Score: 0.8811\n","Epoch 5 - Save Best Score: 0.8811 Model\n","========== fold: 2 training ==========\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp009/checkpoint-129000/pytorch_model.bin\n","Epoch: [1][0/1435] Elapsed 0m 0s (remain 12m 56s) Loss: 0.8774(0.8774) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/1435] Elapsed 0m 22s (remain 5m 3s) Loss: 0.2358(0.6372) Grad: 34104.9805  LR: 0.000003  \n","Epoch: [1][200/1435] Elapsed 0m 44s (remain 4m 35s) Loss: 0.1112(0.3693) Grad: 3031.4597  LR: 0.000006  \n","Epoch: [1][300/1435] Elapsed 1m 6s (remain 4m 12s) Loss: 0.0505(0.2707) Grad: 1351.0232  LR: 0.000008  \n","Epoch: [1][400/1435] Elapsed 1m 28s (remain 3m 48s) Loss: 0.0366(0.2164) Grad: 4047.4438  LR: 0.000011  \n","Epoch: [1][500/1435] Elapsed 1m 50s (remain 3m 26s) Loss: 0.0468(0.1805) Grad: 10221.4258  LR: 0.000014  \n","Epoch: [1][600/1435] Elapsed 2m 12s (remain 3m 4s) Loss: 0.0322(0.1551) Grad: 4931.7168  LR: 0.000017  \n","Epoch: [1][700/1435] Elapsed 2m 34s (remain 2m 41s) Loss: 0.0161(0.1363) Grad: 2427.0818  LR: 0.000020  \n","Epoch: [1][800/1435] Elapsed 2m 56s (remain 2m 19s) Loss: 0.0190(0.1214) Grad: 2257.7959  LR: 0.000020  \n","Epoch: [1][900/1435] Elapsed 3m 18s (remain 1m 57s) Loss: 0.0084(0.1098) Grad: 2097.3701  LR: 0.000019  \n","Epoch: [1][1000/1435] Elapsed 3m 40s (remain 1m 35s) Loss: 0.0068(0.1004) Grad: 1890.6226  LR: 0.000019  \n","Epoch: [1][1100/1435] Elapsed 4m 2s (remain 1m 13s) Loss: 0.0110(0.0928) Grad: 2626.2263  LR: 0.000019  \n","Epoch: [1][1200/1435] Elapsed 4m 24s (remain 0m 51s) Loss: 0.0238(0.0862) Grad: 3386.3286  LR: 0.000019  \n","Epoch: [1][1300/1435] Elapsed 4m 46s (remain 0m 29s) Loss: 0.0262(0.0807) Grad: 5552.9771  LR: 0.000018  \n","Epoch: [1][1400/1435] Elapsed 5m 7s (remain 0m 7s) Loss: 0.0048(0.0760) Grad: 1636.7480  LR: 0.000018  \n","Epoch: [1][1434/1435] Elapsed 5m 15s (remain 0m 0s) Loss: 0.0326(0.0745) Grad: 2804.0955  LR: 0.000018  \n","EVAL: [0/352] Elapsed 0m 0s (remain 2m 25s) Loss: 0.0165(0.0165) \n","EVAL: [100/352] Elapsed 0m 11s (remain 0m 29s) Loss: 0.0148(0.0140) \n","EVAL: [200/352] Elapsed 0m 23s (remain 0m 17s) Loss: 0.0131(0.0142) \n","EVAL: [300/352] Elapsed 0m 35s (remain 0m 5s) Loss: 0.0088(0.0153) \n","EVAL: [351/352] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0020(0.0145) \n","Epoch 1 - avg_train_loss: 0.0745  avg_val_loss: 0.0145  time: 361s\n","Epoch 1 - Score: 0.8254\n","Epoch 1 - Save Best Score: 0.8254 Model\n","Epoch: [2][0/1435] Elapsed 0m 0s (remain 12m 50s) Loss: 0.0148(0.0148) Grad: 22104.9023  LR: 0.000018  \n","Epoch: [2][100/1435] Elapsed 0m 22s (remain 5m 0s) Loss: 0.0684(0.0121) Grad: 89335.6250  LR: 0.000017  \n","Epoch: [2][200/1435] Elapsed 0m 44s (remain 4m 34s) Loss: 0.0043(0.0105) Grad: 7897.8003  LR: 0.000017  \n","Epoch: [2][300/1435] Elapsed 1m 6s (remain 4m 10s) Loss: 0.0068(0.0107) Grad: 15026.7188  LR: 0.000017  \n","Epoch: [2][400/1435] Elapsed 1m 28s (remain 3m 48s) Loss: 0.0062(0.0107) Grad: 8108.3149  LR: 0.000017  \n","Epoch: [2][500/1435] Elapsed 1m 50s (remain 3m 25s) Loss: 0.0083(0.0106) Grad: 21769.6309  LR: 0.000016  \n","Epoch: [2][600/1435] Elapsed 2m 12s (remain 3m 3s) Loss: 0.0041(0.0106) Grad: 13923.2617  LR: 0.000016  \n","Epoch: [2][700/1435] Elapsed 2m 34s (remain 2m 41s) Loss: 0.0082(0.0108) Grad: 7568.5532  LR: 0.000016  \n","Epoch: [2][800/1435] Elapsed 2m 56s (remain 2m 19s) Loss: 0.0141(0.0107) Grad: 46811.3672  LR: 0.000015  \n","Epoch: [2][900/1435] Elapsed 3m 18s (remain 1m 57s) Loss: 0.0021(0.0106) Grad: 13657.0664  LR: 0.000015  \n","Epoch: [2][1000/1435] Elapsed 3m 40s (remain 1m 35s) Loss: 0.0048(0.0110) Grad: 7562.7617  LR: 0.000015  \n","Epoch: [2][1100/1435] Elapsed 4m 1s (remain 1m 13s) Loss: 0.0295(0.0109) Grad: 28446.0098  LR: 0.000014  \n","Epoch: [2][1200/1435] Elapsed 4m 23s (remain 0m 51s) Loss: 0.0073(0.0110) Grad: 23418.6641  LR: 0.000014  \n","Epoch: [2][1300/1435] Elapsed 4m 45s (remain 0m 29s) Loss: 0.0025(0.0109) Grad: 3986.6780  LR: 0.000014  \n","Epoch: [2][1400/1435] Elapsed 5m 7s (remain 0m 7s) Loss: 0.0165(0.0109) Grad: 57393.7617  LR: 0.000013  \n","Epoch: [2][1434/1435] Elapsed 5m 15s (remain 0m 0s) Loss: 0.0167(0.0109) Grad: 24034.5156  LR: 0.000013  \n","EVAL: [0/352] Elapsed 0m 0s (remain 2m 23s) Loss: 0.0112(0.0112) \n","EVAL: [100/352] Elapsed 0m 11s (remain 0m 29s) Loss: 0.0149(0.0112) \n","EVAL: [200/352] Elapsed 0m 23s (remain 0m 17s) Loss: 0.0182(0.0113) \n","EVAL: [300/352] Elapsed 0m 35s (remain 0m 5s) Loss: 0.0017(0.0125) \n","EVAL: [351/352] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0012(0.0117) \n","Epoch 2 - avg_train_loss: 0.0109  avg_val_loss: 0.0117  time: 360s\n","Epoch 2 - Score: 0.8589\n","Epoch 2 - Save Best Score: 0.8589 Model\n","Epoch: [3][0/1435] Elapsed 0m 0s (remain 13m 41s) Loss: 0.0097(0.0097) Grad: 34889.7930  LR: 0.000013  \n","Epoch: [3][100/1435] Elapsed 0m 22s (remain 5m 1s) Loss: 0.0308(0.0070) Grad: 24428.1836  LR: 0.000013  \n","Epoch: [3][200/1435] Elapsed 0m 44s (remain 4m 34s) Loss: 0.0115(0.0075) Grad: 28612.0820  LR: 0.000013  \n","Epoch: [3][300/1435] Elapsed 1m 6s (remain 4m 11s) Loss: 0.0154(0.0075) Grad: 27214.7188  LR: 0.000012  \n","Epoch: [3][400/1435] Elapsed 1m 28s (remain 3m 48s) Loss: 0.0074(0.0080) Grad: 19177.7051  LR: 0.000012  \n","Epoch: [3][500/1435] Elapsed 1m 50s (remain 3m 25s) Loss: 0.0057(0.0082) Grad: 8057.4165  LR: 0.000012  \n","Epoch: [3][600/1435] Elapsed 2m 12s (remain 3m 3s) Loss: 0.0013(0.0084) Grad: 5939.7231  LR: 0.000011  \n","Epoch: [3][700/1435] Elapsed 2m 34s (remain 2m 41s) Loss: 0.0053(0.0084) Grad: 11538.4043  LR: 0.000011  \n","Epoch: [3][800/1435] Elapsed 2m 56s (remain 2m 19s) Loss: 0.0006(0.0083) Grad: 2201.4111  LR: 0.000011  \n","Epoch: [3][900/1435] Elapsed 3m 18s (remain 1m 57s) Loss: 0.0126(0.0083) Grad: 19494.6758  LR: 0.000011  \n","Epoch: [3][1000/1435] Elapsed 3m 40s (remain 1m 35s) Loss: 0.0132(0.0082) Grad: 22196.1094  LR: 0.000010  \n","Epoch: [3][1100/1435] Elapsed 4m 1s (remain 1m 13s) Loss: 0.0124(0.0081) Grad: 27521.2266  LR: 0.000010  \n","Epoch: [3][1200/1435] Elapsed 4m 23s (remain 0m 51s) Loss: 0.0083(0.0082) Grad: 8845.9072  LR: 0.000010  \n","Epoch: [3][1300/1435] Elapsed 4m 45s (remain 0m 29s) Loss: 0.0015(0.0084) Grad: 15118.0195  LR: 0.000009  \n","Epoch: [3][1400/1435] Elapsed 5m 7s (remain 0m 7s) Loss: 0.0117(0.0085) Grad: 14001.6768  LR: 0.000009  \n","Epoch: [3][1434/1435] Elapsed 5m 15s (remain 0m 0s) Loss: 0.0091(0.0085) Grad: 35508.3359  LR: 0.000009  \n","EVAL: [0/352] Elapsed 0m 0s (remain 2m 17s) Loss: 0.0136(0.0136) \n","EVAL: [100/352] Elapsed 0m 11s (remain 0m 29s) Loss: 0.0097(0.0113) \n","EVAL: [200/352] Elapsed 0m 23s (remain 0m 17s) Loss: 0.0140(0.0114) \n","EVAL: [300/352] Elapsed 0m 35s (remain 0m 5s) Loss: 0.0008(0.0127) \n","EVAL: [351/352] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0002(0.0120) \n","Epoch 3 - avg_train_loss: 0.0085  avg_val_loss: 0.0120  time: 360s\n","Epoch 3 - Score: 0.8667\n","Epoch 3 - Save Best Score: 0.8667 Model\n","Epoch: [4][0/1435] Elapsed 0m 0s (remain 13m 32s) Loss: 0.0025(0.0025) Grad: 4752.4741  LR: 0.000009  \n","Epoch: [4][100/1435] Elapsed 0m 22s (remain 5m 0s) Loss: 0.0035(0.0051) Grad: 12404.1543  LR: 0.000009  \n","Epoch: [4][200/1435] Elapsed 0m 44s (remain 4m 33s) Loss: 0.0158(0.0059) Grad: 21253.4355  LR: 0.000008  \n","Epoch: [4][300/1435] Elapsed 1m 6s (remain 4m 10s) Loss: 0.0010(0.0061) Grad: 4515.0640  LR: 0.000008  \n","Epoch: [4][400/1435] Elapsed 1m 28s (remain 3m 48s) Loss: 0.0098(0.0065) Grad: 38109.7305  LR: 0.000008  \n","Epoch: [4][500/1435] Elapsed 1m 50s (remain 3m 25s) Loss: 0.0051(0.0064) Grad: 13605.9795  LR: 0.000007  \n","Epoch: [4][600/1435] Elapsed 2m 12s (remain 3m 3s) Loss: 0.0017(0.0066) Grad: 6742.8403  LR: 0.000007  \n","Epoch: [4][700/1435] Elapsed 2m 34s (remain 2m 41s) Loss: 0.0014(0.0065) Grad: 8202.1045  LR: 0.000007  \n","Epoch: [4][800/1435] Elapsed 2m 56s (remain 2m 19s) Loss: 0.0069(0.0067) Grad: 20517.8906  LR: 0.000006  \n","Epoch: [4][900/1435] Elapsed 3m 18s (remain 1m 57s) Loss: 0.0001(0.0066) Grad: 368.9551  LR: 0.000006  \n","Epoch: [4][1000/1435] Elapsed 3m 40s (remain 1m 35s) Loss: 0.0033(0.0066) Grad: 24745.6738  LR: 0.000006  \n","Epoch: [4][1100/1435] Elapsed 4m 2s (remain 1m 13s) Loss: 0.0013(0.0068) Grad: 18876.3672  LR: 0.000005  \n","Epoch: [4][1200/1435] Elapsed 4m 24s (remain 0m 51s) Loss: 0.0044(0.0069) Grad: 16475.8555  LR: 0.000005  \n","Epoch: [4][1300/1435] Elapsed 4m 45s (remain 0m 29s) Loss: 0.0049(0.0068) Grad: 10385.8418  LR: 0.000005  \n","Epoch: [4][1400/1435] Elapsed 5m 7s (remain 0m 7s) Loss: 0.0040(0.0068) Grad: 8531.5811  LR: 0.000005  \n","Epoch: [4][1434/1435] Elapsed 5m 15s (remain 0m 0s) Loss: 0.0001(0.0068) Grad: 477.5257  LR: 0.000004  \n","EVAL: [0/352] Elapsed 0m 0s (remain 2m 4s) Loss: 0.0168(0.0168) \n","EVAL: [100/352] Elapsed 0m 11s (remain 0m 29s) Loss: 0.0125(0.0131) \n","EVAL: [200/352] Elapsed 0m 23s (remain 0m 17s) Loss: 0.0207(0.0131) \n","EVAL: [300/352] Elapsed 0m 34s (remain 0m 5s) Loss: 0.0012(0.0149) \n","EVAL: [351/352] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0003(0.0140) \n","Epoch 4 - avg_train_loss: 0.0068  avg_val_loss: 0.0140  time: 360s\n","Epoch 4 - Score: 0.8653\n","Epoch: [5][0/1435] Elapsed 0m 0s (remain 13m 1s) Loss: 0.0196(0.0196) Grad: 26119.4062  LR: 0.000004  \n","Epoch: [5][100/1435] Elapsed 0m 22s (remain 4m 55s) Loss: 0.0033(0.0048) Grad: 5704.8154  LR: 0.000004  \n","Epoch: [5][200/1435] Elapsed 0m 44s (remain 4m 31s) Loss: 0.0009(0.0049) Grad: 4117.3257  LR: 0.000004  \n","Epoch: [5][300/1435] Elapsed 1m 6s (remain 4m 9s) Loss: 0.0142(0.0050) Grad: 10113.3857  LR: 0.000004  \n","Epoch: [5][400/1435] Elapsed 1m 28s (remain 3m 47s) Loss: 0.0045(0.0053) Grad: 17110.8477  LR: 0.000003  \n","Epoch: [5][500/1435] Elapsed 1m 50s (remain 3m 25s) Loss: 0.0025(0.0053) Grad: 9440.4463  LR: 0.000003  \n","Epoch: [5][600/1435] Elapsed 2m 11s (remain 3m 3s) Loss: 0.0001(0.0056) Grad: 741.6326  LR: 0.000003  \n","Epoch: [5][700/1435] Elapsed 2m 33s (remain 2m 41s) Loss: 0.0006(0.0057) Grad: 2783.9236  LR: 0.000002  \n","Epoch: [5][800/1435] Elapsed 2m 55s (remain 2m 19s) Loss: 0.0054(0.0056) Grad: 15454.2764  LR: 0.000002  \n","Epoch: [5][900/1435] Elapsed 3m 17s (remain 1m 57s) Loss: 0.0038(0.0055) Grad: 17162.5293  LR: 0.000002  \n","Epoch: [5][1000/1435] Elapsed 3m 39s (remain 1m 35s) Loss: 0.0033(0.0055) Grad: 26921.0684  LR: 0.000001  \n","Epoch: [5][1100/1435] Elapsed 4m 1s (remain 1m 13s) Loss: 0.0262(0.0056) Grad: 41810.8672  LR: 0.000001  \n","Epoch: [5][1200/1435] Elapsed 4m 23s (remain 0m 51s) Loss: 0.0159(0.0056) Grad: 12229.7471  LR: 0.000001  \n","Epoch: [5][1300/1435] Elapsed 4m 45s (remain 0m 29s) Loss: 0.0033(0.0056) Grad: 11100.6367  LR: 0.000000  \n","Epoch: [5][1400/1435] Elapsed 5m 7s (remain 0m 7s) Loss: 0.0044(0.0057) Grad: 15027.8203  LR: 0.000000  \n","Epoch: [5][1434/1435] Elapsed 5m 14s (remain 0m 0s) Loss: 0.0017(0.0058) Grad: 6311.3354  LR: 0.000000  \n","EVAL: [0/352] Elapsed 0m 0s (remain 2m 15s) Loss: 0.0189(0.0189) \n","EVAL: [100/352] Elapsed 0m 11s (remain 0m 29s) Loss: 0.0166(0.0136) \n","EVAL: [200/352] Elapsed 0m 23s (remain 0m 17s) Loss: 0.0267(0.0136) \n","EVAL: [300/352] Elapsed 0m 35s (remain 0m 5s) Loss: 0.0007(0.0155) \n","EVAL: [351/352] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0003(0.0146) \n","Epoch 5 - avg_train_loss: 0.0058  avg_val_loss: 0.0146  time: 360s\n","Epoch 5 - Score: 0.8670\n","Epoch 5 - Save Best Score: 0.8670 Model\n","========== fold: 3 training ==========\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp009/checkpoint-129000/pytorch_model.bin\n","Epoch: [1][0/1438] Elapsed 0m 0s (remain 14m 17s) Loss: 0.6511(0.6511) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/1438] Elapsed 0m 22s (remain 5m 2s) Loss: 0.1649(0.4706) Grad: 24823.6504  LR: 0.000003  \n","Epoch: [1][200/1438] Elapsed 0m 44s (remain 4m 35s) Loss: 0.1448(0.2790) Grad: 4653.2314  LR: 0.000006  \n","Epoch: [1][300/1438] Elapsed 1m 6s (remain 4m 12s) Loss: 0.0349(0.2097) Grad: 1038.1847  LR: 0.000008  \n","Epoch: [1][400/1438] Elapsed 1m 28s (remain 3m 49s) Loss: 0.0283(0.1722) Grad: 2727.2800  LR: 0.000011  \n","Epoch: [1][500/1438] Elapsed 1m 50s (remain 3m 26s) Loss: 0.0172(0.1445) Grad: 3932.5996  LR: 0.000014  \n","Epoch: [1][600/1438] Elapsed 2m 12s (remain 3m 4s) Loss: 0.0079(0.1249) Grad: 2506.8884  LR: 0.000017  \n","Epoch: [1][700/1438] Elapsed 2m 34s (remain 2m 42s) Loss: 0.0288(0.1104) Grad: 5569.9785  LR: 0.000019  \n","Epoch: [1][800/1438] Elapsed 2m 56s (remain 2m 20s) Loss: 0.0182(0.0990) Grad: 2246.0562  LR: 0.000020  \n","Epoch: [1][900/1438] Elapsed 3m 18s (remain 1m 58s) Loss: 0.0042(0.0900) Grad: 986.9644  LR: 0.000019  \n","Epoch: [1][1000/1438] Elapsed 3m 40s (remain 1m 36s) Loss: 0.0128(0.0827) Grad: 2568.0090  LR: 0.000019  \n","Epoch: [1][1100/1438] Elapsed 4m 2s (remain 1m 14s) Loss: 0.0097(0.0765) Grad: 1657.8346  LR: 0.000019  \n","Epoch: [1][1200/1438] Elapsed 4m 23s (remain 0m 52s) Loss: 0.0221(0.0713) Grad: 2459.3823  LR: 0.000019  \n","Epoch: [1][1300/1438] Elapsed 4m 45s (remain 0m 30s) Loss: 0.0217(0.0669) Grad: 2897.0769  LR: 0.000018  \n","Epoch: [1][1400/1438] Elapsed 5m 7s (remain 0m 8s) Loss: 0.0092(0.0632) Grad: 1784.5156  LR: 0.000018  \n","Epoch: [1][1437/1438] Elapsed 5m 15s (remain 0m 0s) Loss: 0.0056(0.0619) Grad: 1571.3781  LR: 0.000018  \n","EVAL: [0/349] Elapsed 0m 0s (remain 2m 28s) Loss: 0.0048(0.0048) \n","EVAL: [100/349] Elapsed 0m 12s (remain 0m 29s) Loss: 0.0246(0.0118) \n","EVAL: [200/349] Elapsed 0m 23s (remain 0m 17s) Loss: 0.0148(0.0129) \n","EVAL: [300/349] Elapsed 0m 35s (remain 0m 5s) Loss: 0.0102(0.0139) \n","EVAL: [348/349] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0013(0.0133) \n","Epoch 1 - avg_train_loss: 0.0619  avg_val_loss: 0.0133  time: 361s\n","Epoch 1 - Score: 0.8398\n","Epoch 1 - Save Best Score: 0.8398 Model\n","Epoch: [2][0/1438] Elapsed 0m 0s (remain 14m 7s) Loss: 0.0095(0.0095) Grad: 20600.0410  LR: 0.000018  \n","Epoch: [2][100/1438] Elapsed 0m 22s (remain 5m 3s) Loss: 0.0023(0.0116) Grad: 7065.1787  LR: 0.000017  \n","Epoch: [2][200/1438] Elapsed 0m 44s (remain 4m 36s) Loss: 0.0148(0.0119) Grad: 38618.4102  LR: 0.000017  \n","Epoch: [2][300/1438] Elapsed 1m 6s (remain 4m 12s) Loss: 0.0049(0.0115) Grad: 12813.5166  LR: 0.000017  \n","Epoch: [2][400/1438] Elapsed 1m 28s (remain 3m 49s) Loss: 0.0100(0.0111) Grad: 16798.6582  LR: 0.000017  \n","Epoch: [2][500/1438] Elapsed 1m 50s (remain 3m 27s) Loss: 0.0002(0.0109) Grad: 1178.5417  LR: 0.000016  \n","Epoch: [2][600/1438] Elapsed 2m 12s (remain 3m 4s) Loss: 0.0048(0.0107) Grad: 15801.1631  LR: 0.000016  \n","Epoch: [2][700/1438] Elapsed 2m 34s (remain 2m 42s) Loss: 0.0063(0.0110) Grad: 9800.7539  LR: 0.000016  \n","Epoch: [2][800/1438] Elapsed 2m 56s (remain 2m 20s) Loss: 0.0105(0.0111) Grad: 25708.7891  LR: 0.000015  \n","Epoch: [2][900/1438] Elapsed 3m 18s (remain 1m 58s) Loss: 0.0129(0.0110) Grad: 28972.1191  LR: 0.000015  \n","Epoch: [2][1000/1438] Elapsed 3m 40s (remain 1m 36s) Loss: 0.0083(0.0110) Grad: 28430.6855  LR: 0.000015  \n","Epoch: [2][1100/1438] Elapsed 4m 2s (remain 1m 14s) Loss: 0.0087(0.0109) Grad: 14554.9727  LR: 0.000014  \n","Epoch: [2][1200/1438] Elapsed 4m 24s (remain 0m 52s) Loss: 0.0226(0.0109) Grad: 40238.2031  LR: 0.000014  \n","Epoch: [2][1300/1438] Elapsed 4m 46s (remain 0m 30s) Loss: 0.0089(0.0108) Grad: 10844.7422  LR: 0.000014  \n","Epoch: [2][1400/1438] Elapsed 5m 8s (remain 0m 8s) Loss: 0.0049(0.0108) Grad: 11734.9053  LR: 0.000013  \n","Epoch: [2][1437/1438] Elapsed 5m 16s (remain 0m 0s) Loss: 0.0103(0.0108) Grad: 18998.0000  LR: 0.000013  \n","EVAL: [0/349] Elapsed 0m 0s (remain 2m 23s) Loss: 0.0083(0.0083) \n","EVAL: [100/349] Elapsed 0m 12s (remain 0m 29s) Loss: 0.0308(0.0111) \n","EVAL: [200/349] Elapsed 0m 23s (remain 0m 17s) Loss: 0.0216(0.0113) \n","EVAL: [300/349] Elapsed 0m 35s (remain 0m 5s) Loss: 0.0072(0.0116) \n","EVAL: [348/349] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0003(0.0110) \n","Epoch 2 - avg_train_loss: 0.0108  avg_val_loss: 0.0110  time: 361s\n","Epoch 2 - Score: 0.8700\n","Epoch 2 - Save Best Score: 0.8700 Model\n","Epoch: [3][0/1438] Elapsed 0m 0s (remain 13m 20s) Loss: 0.0165(0.0165) Grad: 20167.4258  LR: 0.000013  \n","Epoch: [3][100/1438] Elapsed 0m 22s (remain 5m 3s) Loss: 0.0031(0.0100) Grad: 5211.1372  LR: 0.000013  \n","Epoch: [3][200/1438] Elapsed 0m 44s (remain 4m 35s) Loss: 0.0011(0.0088) Grad: 3906.0884  LR: 0.000013  \n","Epoch: [3][300/1438] Elapsed 1m 6s (remain 4m 11s) Loss: 0.0015(0.0085) Grad: 11062.8926  LR: 0.000012  \n","Epoch: [3][400/1438] Elapsed 1m 28s (remain 3m 48s) Loss: 0.0056(0.0090) Grad: 8741.4375  LR: 0.000012  \n","Epoch: [3][500/1438] Elapsed 1m 50s (remain 3m 26s) Loss: 0.0128(0.0089) Grad: 37639.3086  LR: 0.000012  \n","Epoch: [3][600/1438] Elapsed 2m 12s (remain 3m 4s) Loss: 0.0113(0.0088) Grad: 20746.2969  LR: 0.000011  \n","Epoch: [3][700/1438] Elapsed 2m 34s (remain 2m 42s) Loss: 0.0071(0.0087) Grad: 9324.5312  LR: 0.000011  \n","Epoch: [3][800/1438] Elapsed 2m 56s (remain 2m 20s) Loss: 0.0009(0.0088) Grad: 1944.1283  LR: 0.000011  \n","Epoch: [3][900/1438] Elapsed 3m 18s (remain 1m 58s) Loss: 0.0084(0.0088) Grad: 25742.9355  LR: 0.000011  \n","Epoch: [3][1000/1438] Elapsed 3m 40s (remain 1m 36s) Loss: 0.0095(0.0086) Grad: 20867.1016  LR: 0.000010  \n","Epoch: [3][1100/1438] Elapsed 4m 2s (remain 1m 14s) Loss: 0.0003(0.0086) Grad: 874.5590  LR: 0.000010  \n","Epoch: [3][1200/1438] Elapsed 4m 23s (remain 0m 52s) Loss: 0.0119(0.0086) Grad: 31115.6270  LR: 0.000010  \n","Epoch: [3][1300/1438] Elapsed 4m 45s (remain 0m 30s) Loss: 0.0102(0.0085) Grad: 13219.9873  LR: 0.000009  \n","Epoch: [3][1400/1438] Elapsed 5m 7s (remain 0m 8s) Loss: 0.0023(0.0085) Grad: 11425.6084  LR: 0.000009  \n","Epoch: [3][1437/1438] Elapsed 5m 15s (remain 0m 0s) Loss: 0.0174(0.0085) Grad: 15382.4229  LR: 0.000009  \n","EVAL: [0/349] Elapsed 0m 0s (remain 2m 18s) Loss: 0.0100(0.0100) \n","EVAL: [100/349] Elapsed 0m 12s (remain 0m 29s) Loss: 0.0112(0.0112) \n","EVAL: [200/349] Elapsed 0m 23s (remain 0m 17s) Loss: 0.0218(0.0119) \n","EVAL: [300/349] Elapsed 0m 35s (remain 0m 5s) Loss: 0.0047(0.0120) \n","EVAL: [348/349] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0002(0.0115) \n","Epoch 3 - avg_train_loss: 0.0085  avg_val_loss: 0.0115  time: 361s\n","Epoch 3 - Score: 0.8720\n","Epoch 3 - Save Best Score: 0.8720 Model\n","Epoch: [4][0/1438] Elapsed 0m 0s (remain 14m 13s) Loss: 0.0007(0.0007) Grad: 2952.2266  LR: 0.000009  \n","Epoch: [4][100/1438] Elapsed 0m 22s (remain 5m 3s) Loss: 0.0037(0.0059) Grad: 10403.2480  LR: 0.000009  \n","Epoch: [4][200/1438] Elapsed 0m 44s (remain 4m 35s) Loss: 0.0024(0.0068) Grad: 10101.3262  LR: 0.000008  \n","Epoch: [4][300/1438] Elapsed 1m 6s (remain 4m 12s) Loss: 0.0026(0.0067) Grad: 7510.8115  LR: 0.000008  \n","Epoch: [4][400/1438] Elapsed 1m 28s (remain 3m 49s) Loss: 0.0037(0.0067) Grad: 7706.2759  LR: 0.000008  \n","Epoch: [4][500/1438] Elapsed 1m 50s (remain 3m 26s) Loss: 0.0057(0.0065) Grad: 8829.5957  LR: 0.000007  \n","Epoch: [4][600/1438] Elapsed 2m 12s (remain 3m 4s) Loss: 0.0200(0.0066) Grad: 97630.7969  LR: 0.000007  \n","Epoch: [4][700/1438] Elapsed 2m 34s (remain 2m 42s) Loss: 0.0010(0.0067) Grad: 5313.3325  LR: 0.000007  \n","Epoch: [4][800/1438] Elapsed 2m 56s (remain 2m 20s) Loss: 0.0010(0.0067) Grad: 4743.1787  LR: 0.000006  \n","Epoch: [4][900/1438] Elapsed 3m 18s (remain 1m 58s) Loss: 0.0044(0.0069) Grad: 12486.7998  LR: 0.000006  \n","Epoch: [4][1000/1438] Elapsed 3m 40s (remain 1m 36s) Loss: 0.0024(0.0069) Grad: 7988.8374  LR: 0.000006  \n","Epoch: [4][1100/1438] Elapsed 4m 2s (remain 1m 14s) Loss: 0.0064(0.0068) Grad: 15952.4912  LR: 0.000005  \n","Epoch: [4][1200/1438] Elapsed 4m 24s (remain 0m 52s) Loss: 0.0043(0.0069) Grad: 11207.8115  LR: 0.000005  \n","Epoch: [4][1300/1438] Elapsed 4m 45s (remain 0m 30s) Loss: 0.0030(0.0069) Grad: 13954.2686  LR: 0.000005  \n","Epoch: [4][1400/1438] Elapsed 5m 7s (remain 0m 8s) Loss: 0.0049(0.0069) Grad: 31679.2520  LR: 0.000005  \n","Epoch: [4][1437/1438] Elapsed 5m 15s (remain 0m 0s) Loss: 0.0091(0.0069) Grad: 10768.6201  LR: 0.000004  \n","EVAL: [0/349] Elapsed 0m 0s (remain 2m 20s) Loss: 0.0089(0.0089) \n","EVAL: [100/349] Elapsed 0m 12s (remain 0m 29s) Loss: 0.0117(0.0118) \n","EVAL: [200/349] Elapsed 0m 23s (remain 0m 17s) Loss: 0.0308(0.0126) \n","EVAL: [300/349] Elapsed 0m 35s (remain 0m 5s) Loss: 0.0049(0.0127) \n","EVAL: [348/349] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0001(0.0122) \n","Epoch 4 - avg_train_loss: 0.0069  avg_val_loss: 0.0122  time: 361s\n","Epoch 4 - Score: 0.8778\n","Epoch 4 - Save Best Score: 0.8778 Model\n","Epoch: [5][0/1438] Elapsed 0m 0s (remain 13m 35s) Loss: 0.0010(0.0010) Grad: 2958.6011  LR: 0.000004  \n","Epoch: [5][100/1438] Elapsed 0m 22s (remain 5m 3s) Loss: 0.0050(0.0063) Grad: 19494.3105  LR: 0.000004  \n","Epoch: [5][200/1438] Elapsed 0m 44s (remain 4m 35s) Loss: 0.0031(0.0062) Grad: 10859.8223  LR: 0.000004  \n","Epoch: [5][300/1438] Elapsed 1m 6s (remain 4m 12s) Loss: 0.0024(0.0059) Grad: 10656.4453  LR: 0.000004  \n","Epoch: [5][400/1438] Elapsed 1m 28s (remain 3m 49s) Loss: 0.0168(0.0058) Grad: 33398.1289  LR: 0.000003  \n","Epoch: [5][500/1438] Elapsed 1m 50s (remain 3m 26s) Loss: 0.0003(0.0057) Grad: 3013.9470  LR: 0.000003  \n","Epoch: [5][600/1438] Elapsed 2m 12s (remain 3m 4s) Loss: 0.0169(0.0056) Grad: 14684.1631  LR: 0.000003  \n","Epoch: [5][700/1438] Elapsed 2m 34s (remain 2m 42s) Loss: 0.0035(0.0056) Grad: 7225.0015  LR: 0.000002  \n","Epoch: [5][800/1438] Elapsed 2m 56s (remain 2m 20s) Loss: 0.0017(0.0057) Grad: 7656.3462  LR: 0.000002  \n","Epoch: [5][900/1438] Elapsed 3m 18s (remain 1m 58s) Loss: 0.0001(0.0057) Grad: 807.9932  LR: 0.000002  \n","Epoch: [5][1000/1438] Elapsed 3m 40s (remain 1m 36s) Loss: 0.0002(0.0058) Grad: 1116.0165  LR: 0.000001  \n","Epoch: [5][1100/1438] Elapsed 4m 2s (remain 1m 14s) Loss: 0.0007(0.0058) Grad: 5855.2827  LR: 0.000001  \n","Epoch: [5][1200/1438] Elapsed 4m 23s (remain 0m 52s) Loss: 0.0029(0.0057) Grad: 6913.2612  LR: 0.000001  \n","Epoch: [5][1300/1438] Elapsed 4m 45s (remain 0m 30s) Loss: 0.0111(0.0057) Grad: 65885.3047  LR: 0.000000  \n","Epoch: [5][1400/1438] Elapsed 5m 7s (remain 0m 8s) Loss: 0.0102(0.0058) Grad: 15411.7451  LR: 0.000000  \n","Epoch: [5][1437/1438] Elapsed 5m 15s (remain 0m 0s) Loss: 0.0029(0.0058) Grad: 9091.0088  LR: 0.000000  \n","EVAL: [0/349] Elapsed 0m 0s (remain 2m 8s) Loss: 0.0101(0.0101) \n","EVAL: [100/349] Elapsed 0m 11s (remain 0m 29s) Loss: 0.0146(0.0127) \n","EVAL: [200/349] Elapsed 0m 23s (remain 0m 17s) Loss: 0.0275(0.0134) \n","EVAL: [300/349] Elapsed 0m 35s (remain 0m 5s) Loss: 0.0062(0.0138) \n","EVAL: [348/349] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0000(0.0132) \n","Epoch 5 - avg_train_loss: 0.0058  avg_val_loss: 0.0132  time: 361s\n","Epoch 5 - Score: 0.8777\n","========== fold: 4 training ==========\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp009/checkpoint-129000/pytorch_model.bin\n","Epoch: [1][0/1425] Elapsed 0m 0s (remain 13m 43s) Loss: 0.9009(0.9009) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/1425] Elapsed 0m 22s (remain 4m 54s) Loss: 0.2025(0.6365) Grad: 11771.8184  LR: 0.000003  \n","Epoch: [1][200/1425] Elapsed 0m 44s (remain 4m 30s) Loss: 0.0519(0.3626) Grad: 1161.6427  LR: 0.000006  \n","Epoch: [1][300/1425] Elapsed 1m 6s (remain 4m 7s) Loss: 0.0458(0.2649) Grad: 823.7053  LR: 0.000008  \n","Epoch: [1][400/1425] Elapsed 1m 28s (remain 3m 45s) Loss: 0.0562(0.2115) Grad: 2368.1433  LR: 0.000011  \n","Epoch: [1][500/1425] Elapsed 1m 50s (remain 3m 23s) Loss: 0.0225(0.1754) Grad: 1430.4214  LR: 0.000014  \n","Epoch: [1][600/1425] Elapsed 2m 12s (remain 3m 1s) Loss: 0.0172(0.1503) Grad: 2534.4067  LR: 0.000017  \n","Epoch: [1][700/1425] Elapsed 2m 34s (remain 2m 39s) Loss: 0.0099(0.1317) Grad: 753.7834  LR: 0.000020  \n","Epoch: [1][800/1425] Elapsed 2m 56s (remain 2m 17s) Loss: 0.0174(0.1177) Grad: 1401.0167  LR: 0.000020  \n","Epoch: [1][900/1425] Elapsed 3m 18s (remain 1m 55s) Loss: 0.0167(0.1068) Grad: 1772.1083  LR: 0.000019  \n","Epoch: [1][1000/1425] Elapsed 3m 39s (remain 1m 33s) Loss: 0.0111(0.0978) Grad: 880.6352  LR: 0.000019  \n","Epoch: [1][1100/1425] Elapsed 4m 1s (remain 1m 11s) Loss: 0.0070(0.0904) Grad: 966.9084  LR: 0.000019  \n","Epoch: [1][1200/1425] Elapsed 4m 23s (remain 0m 49s) Loss: 0.0170(0.0840) Grad: 1195.6122  LR: 0.000018  \n","Epoch: [1][1300/1425] Elapsed 4m 45s (remain 0m 27s) Loss: 0.0234(0.0788) Grad: 3067.6821  LR: 0.000018  \n","Epoch: [1][1400/1425] Elapsed 5m 7s (remain 0m 5s) Loss: 0.0153(0.0742) Grad: 1195.9197  LR: 0.000018  \n","Epoch: [1][1424/1425] Elapsed 5m 12s (remain 0m 0s) Loss: 0.0090(0.0731) Grad: 897.2462  LR: 0.000018  \n","EVAL: [0/363] Elapsed 0m 0s (remain 2m 16s) Loss: 0.0216(0.0216) \n","EVAL: [100/363] Elapsed 0m 12s (remain 0m 31s) Loss: 0.0111(0.0135) \n","EVAL: [200/363] Elapsed 0m 23s (remain 0m 19s) Loss: 0.0928(0.0137) \n","EVAL: [300/363] Elapsed 0m 35s (remain 0m 7s) Loss: 0.0074(0.0139) \n","EVAL: [362/363] Elapsed 0m 42s (remain 0m 0s) Loss: 0.0071(0.0126) \n","Epoch 1 - avg_train_loss: 0.0731  avg_val_loss: 0.0126  time: 360s\n","Epoch 1 - Score: 0.8397\n","Epoch 1 - Save Best Score: 0.8397 Model\n","Epoch: [2][0/1425] Elapsed 0m 0s (remain 13m 11s) Loss: 0.0059(0.0059) Grad: 8634.7334  LR: 0.000018  \n","Epoch: [2][100/1425] Elapsed 0m 22s (remain 4m 59s) Loss: 0.0039(0.0125) Grad: 11424.5264  LR: 0.000017  \n","Epoch: [2][200/1425] Elapsed 0m 44s (remain 4m 32s) Loss: 0.0022(0.0114) Grad: 10375.2529  LR: 0.000017  \n","Epoch: [2][300/1425] Elapsed 1m 6s (remain 4m 9s) Loss: 0.0061(0.0110) Grad: 12842.6670  LR: 0.000017  \n","Epoch: [2][400/1425] Elapsed 1m 28s (remain 3m 46s) Loss: 0.0062(0.0109) Grad: 7111.0347  LR: 0.000017  \n","Epoch: [2][500/1425] Elapsed 1m 50s (remain 3m 24s) Loss: 0.0014(0.0111) Grad: 8517.7070  LR: 0.000016  \n","Epoch: [2][600/1425] Elapsed 2m 12s (remain 3m 2s) Loss: 0.0033(0.0108) Grad: 6251.1689  LR: 0.000016  \n","Epoch: [2][700/1425] Elapsed 2m 34s (remain 2m 39s) Loss: 0.0048(0.0108) Grad: 10461.8760  LR: 0.000016  \n","Epoch: [2][800/1425] Elapsed 2m 56s (remain 2m 17s) Loss: 0.0024(0.0109) Grad: 7038.6758  LR: 0.000015  \n","Epoch: [2][900/1425] Elapsed 3m 18s (remain 1m 55s) Loss: 0.0068(0.0109) Grad: 22307.5293  LR: 0.000015  \n","Epoch: [2][1000/1425] Elapsed 3m 40s (remain 1m 33s) Loss: 0.0326(0.0108) Grad: 55476.9180  LR: 0.000015  \n","Epoch: [2][1100/1425] Elapsed 4m 2s (remain 1m 11s) Loss: 0.0033(0.0108) Grad: 13400.3154  LR: 0.000014  \n","Epoch: [2][1200/1425] Elapsed 4m 24s (remain 0m 49s) Loss: 0.0110(0.0109) Grad: 23297.0703  LR: 0.000014  \n","Epoch: [2][1300/1425] Elapsed 4m 46s (remain 0m 27s) Loss: 0.0289(0.0108) Grad: 30882.9473  LR: 0.000014  \n","Epoch: [2][1400/1425] Elapsed 5m 8s (remain 0m 5s) Loss: 0.0035(0.0107) Grad: 7844.2036  LR: 0.000013  \n","Epoch: [2][1424/1425] Elapsed 5m 13s (remain 0m 0s) Loss: 0.0004(0.0107) Grad: 1140.7207  LR: 0.000013  \n","EVAL: [0/363] Elapsed 0m 0s (remain 2m 22s) Loss: 0.0118(0.0118) \n","EVAL: [100/363] Elapsed 0m 12s (remain 0m 31s) Loss: 0.0096(0.0130) \n","EVAL: [200/363] Elapsed 0m 23s (remain 0m 19s) Loss: 0.0987(0.0133) \n","EVAL: [300/363] Elapsed 0m 35s (remain 0m 7s) Loss: 0.0039(0.0131) \n","EVAL: [362/363] Elapsed 0m 42s (remain 0m 0s) Loss: 0.0090(0.0118) \n","Epoch 2 - avg_train_loss: 0.0107  avg_val_loss: 0.0118  time: 360s\n","Epoch 2 - Score: 0.8727\n","Epoch 2 - Save Best Score: 0.8727 Model\n","Epoch: [3][0/1425] Elapsed 0m 0s (remain 13m 56s) Loss: 0.0051(0.0051) Grad: 15838.0156  LR: 0.000013  \n","Epoch: [3][100/1425] Elapsed 0m 22s (remain 4m 58s) Loss: 0.0073(0.0081) Grad: 27076.1836  LR: 0.000013  \n","Epoch: [3][200/1425] Elapsed 0m 44s (remain 4m 32s) Loss: 0.0015(0.0077) Grad: 4879.6318  LR: 0.000013  \n","Epoch: [3][300/1425] Elapsed 1m 6s (remain 4m 8s) Loss: 0.0091(0.0083) Grad: 21975.5938  LR: 0.000012  \n","Epoch: [3][400/1425] Elapsed 1m 28s (remain 3m 46s) Loss: 0.0108(0.0081) Grad: 39213.7422  LR: 0.000012  \n","Epoch: [3][500/1425] Elapsed 1m 50s (remain 3m 23s) Loss: 0.0149(0.0080) Grad: 23429.5996  LR: 0.000012  \n","Epoch: [3][600/1425] Elapsed 2m 12s (remain 3m 1s) Loss: 0.0073(0.0081) Grad: 15725.6064  LR: 0.000011  \n","Epoch: [3][700/1425] Elapsed 2m 34s (remain 2m 39s) Loss: 0.0035(0.0081) Grad: 7916.1206  LR: 0.000011  \n","Epoch: [3][800/1425] Elapsed 2m 56s (remain 2m 17s) Loss: 0.0071(0.0082) Grad: 13508.8750  LR: 0.000011  \n","Epoch: [3][900/1425] Elapsed 3m 18s (remain 1m 55s) Loss: 0.0069(0.0082) Grad: 10092.7744  LR: 0.000011  \n","Epoch: [3][1000/1425] Elapsed 3m 40s (remain 1m 33s) Loss: 0.0102(0.0083) Grad: 17538.4844  LR: 0.000010  \n","Epoch: [3][1100/1425] Elapsed 4m 2s (remain 1m 11s) Loss: 0.0017(0.0084) Grad: 3819.4402  LR: 0.000010  \n","Epoch: [3][1200/1425] Elapsed 4m 24s (remain 0m 49s) Loss: 0.0080(0.0083) Grad: 43552.3906  LR: 0.000010  \n","Epoch: [3][1300/1425] Elapsed 4m 46s (remain 0m 27s) Loss: 0.0001(0.0083) Grad: 270.6902  LR: 0.000009  \n","Epoch: [3][1400/1425] Elapsed 5m 8s (remain 0m 5s) Loss: 0.0005(0.0084) Grad: 4966.0146  LR: 0.000009  \n","Epoch: [3][1424/1425] Elapsed 5m 13s (remain 0m 0s) Loss: 0.0024(0.0084) Grad: 6992.4375  LR: 0.000009  \n","EVAL: [0/363] Elapsed 0m 0s (remain 2m 8s) Loss: 0.0165(0.0165) \n","EVAL: [100/363] Elapsed 0m 11s (remain 0m 31s) Loss: 0.0097(0.0122) \n","EVAL: [200/363] Elapsed 0m 23s (remain 0m 19s) Loss: 0.1006(0.0129) \n","EVAL: [300/363] Elapsed 0m 35s (remain 0m 7s) Loss: 0.0082(0.0128) \n","EVAL: [362/363] Elapsed 0m 42s (remain 0m 0s) Loss: 0.0082(0.0115) \n","Epoch 3 - avg_train_loss: 0.0084  avg_val_loss: 0.0115  time: 360s\n","Epoch 3 - Score: 0.8705\n","Epoch: [4][0/1425] Elapsed 0m 0s (remain 14m 2s) Loss: 0.0006(0.0006) Grad: 1727.3307  LR: 0.000009  \n","Epoch: [4][100/1425] Elapsed 0m 22s (remain 4m 55s) Loss: 0.0048(0.0063) Grad: 12255.9307  LR: 0.000009  \n","Epoch: [4][200/1425] Elapsed 0m 44s (remain 4m 31s) Loss: 0.0048(0.0063) Grad: 6782.9658  LR: 0.000008  \n","Epoch: [4][300/1425] Elapsed 1m 6s (remain 4m 8s) Loss: 0.0109(0.0065) Grad: 31533.4492  LR: 0.000008  \n","Epoch: [4][400/1425] Elapsed 1m 28s (remain 3m 45s) Loss: 0.0075(0.0064) Grad: 31674.7031  LR: 0.000008  \n","Epoch: [4][500/1425] Elapsed 1m 50s (remain 3m 23s) Loss: 0.0030(0.0066) Grad: 19478.0723  LR: 0.000007  \n","Epoch: [4][600/1425] Elapsed 2m 12s (remain 3m 1s) Loss: 0.0015(0.0066) Grad: 8302.0498  LR: 0.000007  \n","Epoch: [4][700/1425] Elapsed 2m 34s (remain 2m 39s) Loss: 0.0333(0.0066) Grad: 58272.8516  LR: 0.000007  \n","Epoch: [4][800/1425] Elapsed 2m 56s (remain 2m 17s) Loss: 0.0064(0.0066) Grad: 13523.5273  LR: 0.000006  \n","Epoch: [4][900/1425] Elapsed 3m 17s (remain 1m 55s) Loss: 0.0056(0.0066) Grad: 20714.2910  LR: 0.000006  \n","Epoch: [4][1000/1425] Elapsed 3m 39s (remain 1m 33s) Loss: 0.0001(0.0066) Grad: 675.4759  LR: 0.000006  \n","Epoch: [4][1100/1425] Elapsed 4m 1s (remain 1m 11s) Loss: 0.0036(0.0067) Grad: 26609.0273  LR: 0.000005  \n","Epoch: [4][1200/1425] Elapsed 4m 23s (remain 0m 49s) Loss: 0.0004(0.0066) Grad: 1758.7902  LR: 0.000005  \n","Epoch: [4][1300/1425] Elapsed 4m 45s (remain 0m 27s) Loss: 0.0077(0.0067) Grad: 22830.3926  LR: 0.000005  \n","Epoch: [4][1400/1425] Elapsed 5m 7s (remain 0m 5s) Loss: 0.0040(0.0067) Grad: 13757.7305  LR: 0.000005  \n","Epoch: [4][1424/1425] Elapsed 5m 12s (remain 0m 0s) Loss: 0.0269(0.0067) Grad: 58934.2812  LR: 0.000004  \n","EVAL: [0/363] Elapsed 0m 0s (remain 2m 27s) Loss: 0.0189(0.0189) \n","EVAL: [100/363] Elapsed 0m 12s (remain 0m 31s) Loss: 0.0093(0.0131) \n","EVAL: [200/363] Elapsed 0m 23s (remain 0m 19s) Loss: 0.0784(0.0132) \n","EVAL: [300/363] Elapsed 0m 35s (remain 0m 7s) Loss: 0.0111(0.0132) \n","EVAL: [362/363] Elapsed 0m 42s (remain 0m 0s) Loss: 0.0092(0.0118) \n","Epoch 4 - avg_train_loss: 0.0067  avg_val_loss: 0.0118  time: 360s\n","Epoch 4 - Score: 0.8769\n","Epoch 4 - Save Best Score: 0.8769 Model\n","Epoch: [5][0/1425] Elapsed 0m 0s (remain 13m 57s) Loss: 0.0008(0.0008) Grad: 5397.7607  LR: 0.000004  \n","Epoch: [5][100/1425] Elapsed 0m 22s (remain 4m 59s) Loss: 0.0067(0.0059) Grad: 17198.3066  LR: 0.000004  \n","Epoch: [5][200/1425] Elapsed 0m 44s (remain 4m 32s) Loss: 0.0050(0.0055) Grad: 48713.4531  LR: 0.000004  \n","Epoch: [5][300/1425] Elapsed 1m 6s (remain 4m 9s) Loss: 0.0020(0.0058) Grad: 4784.1455  LR: 0.000004  \n","Epoch: [5][400/1425] Elapsed 1m 28s (remain 3m 46s) Loss: 0.0010(0.0056) Grad: 6021.0249  LR: 0.000003  \n","Epoch: [5][500/1425] Elapsed 1m 50s (remain 3m 24s) Loss: 0.0001(0.0055) Grad: 1164.1278  LR: 0.000003  \n","Epoch: [5][600/1425] Elapsed 2m 12s (remain 3m 1s) Loss: 0.0005(0.0055) Grad: 18494.9551  LR: 0.000003  \n","Epoch: [5][700/1425] Elapsed 2m 34s (remain 2m 39s) Loss: 0.0039(0.0055) Grad: 7100.3975  LR: 0.000002  \n","Epoch: [5][800/1425] Elapsed 2m 56s (remain 2m 17s) Loss: 0.0019(0.0055) Grad: 6501.0142  LR: 0.000002  \n","Epoch: [5][900/1425] Elapsed 3m 18s (remain 1m 55s) Loss: 0.0021(0.0054) Grad: 10926.6436  LR: 0.000002  \n","Epoch: [5][1000/1425] Elapsed 3m 40s (remain 1m 33s) Loss: 0.0098(0.0054) Grad: 20271.3574  LR: 0.000001  \n","Epoch: [5][1100/1425] Elapsed 4m 2s (remain 1m 11s) Loss: 0.0113(0.0054) Grad: 52882.9297  LR: 0.000001  \n","Epoch: [5][1200/1425] Elapsed 4m 24s (remain 0m 49s) Loss: 0.0001(0.0055) Grad: 616.2673  LR: 0.000001  \n","Epoch: [5][1300/1425] Elapsed 4m 46s (remain 0m 27s) Loss: 0.0047(0.0055) Grad: 11347.0430  LR: 0.000000  \n","Epoch: [5][1400/1425] Elapsed 5m 8s (remain 0m 5s) Loss: 0.0005(0.0057) Grad: 9597.1943  LR: 0.000000  \n","Epoch: [5][1424/1425] Elapsed 5m 13s (remain 0m 0s) Loss: 0.0017(0.0057) Grad: 6100.1050  LR: 0.000000  \n","EVAL: [0/363] Elapsed 0m 0s (remain 2m 22s) Loss: 0.0159(0.0159) \n","EVAL: [100/363] Elapsed 0m 12s (remain 0m 31s) Loss: 0.0082(0.0142) \n","EVAL: [200/363] Elapsed 0m 23s (remain 0m 19s) Loss: 0.1470(0.0148) \n","EVAL: [300/363] Elapsed 0m 35s (remain 0m 7s) Loss: 0.0134(0.0148) \n","EVAL: [362/363] Elapsed 0m 42s (remain 0m 0s) Loss: 0.0093(0.0132) \n","Epoch 5 - avg_train_loss: 0.0057  avg_val_loss: 0.0132  time: 360s\n","Epoch 5 - Score: 0.8782\n","Epoch 5 - Save Best Score: 0.8782 Model\n","Best thres: 0.5, Score: 0.8760\n","Best thres: 0.5127929687499999, Score: 0.8761\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5a00641beddc46eb8da430f2d9999490","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/533M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Load weight from pretrained\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"04966868d2974cb8b3215a50572c2c94","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Exception ignored in: <function _ConnectionBase.__del__ at 0x7ff6161258c0>\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 132, in __del__\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Load weight from pretrained\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5a5c41748cba4234a6a6f9aabddfa861","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Load weight from pretrained\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"72044a7dbe7d4b5f839f38f6e827ec63","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Load weight from pretrained\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bed6a691643a46d5bd25e03cdc5b73f7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Exception ignored in: <function _ConnectionBase.__del__ at 0x7ff6161258c0>\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 132, in __del__\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Load weight from pretrained\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b4d0bd0dea5341a9b03f0092fe3cba39","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n"]}],"source":["if __name__ == \"__main__\":\n","    main()"],"id":"1d4fcf7c"}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","name":"nbme-exp011.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":641.572862,"end_time":"2022-02-27T11:39:50.972497","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-02-27T11:29:09.399635","version":"2.3.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0260998578564385a0b5b9425a0a5ca1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1141ae38bc6f473aab89db14fa4eeacf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad49cbf6b6e84ccaab873458182f22a1","placeholder":"​","style":"IPY_MODEL_5375de82ce3a41a8b5550e0a6b4316c1","value":" 42146/42146 [00:36&lt;00:00, 2019.05it/s]"}},"220f78b6119042af8729543465e1234e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25bf78e432e641e0a435dc3626c3ee8a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e32fee744ef42e0aaa89a7b03e82427":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e3818222bab4603a896be5976cb8409":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9754a5f1e61d49c8972df40ee9290375","placeholder":"​","style":"IPY_MODEL_25bf78e432e641e0a435dc3626c3ee8a","value":" 143/143 [00:00&lt;00:00, 2166.88it/s]"}},"40e6583408c447199ff5b94d23601936":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e32fee744ef42e0aaa89a7b03e82427","max":42146,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d51d3aa414db4aa8b0ccae896e671152","value":42146}},"428ca357bd284d199e2558b1f577d79a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47b8a7f3d0544d79b30ad02e4222082e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5375de82ce3a41a8b5550e0a6b4316c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e66444e9c714134bd2765cb3b6d1f15":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b04b019813e458080f02bc9111433a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f1d7796e2174485a0d1b1e9a71d7ade","max":143,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e72cad76f875451a8e2479e2df237575","value":143}},"7f1d7796e2174485a0d1b1e9a71d7ade":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a503d1abd884514a1e23101e03c6781":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e06e5e9eb0414b6fad63bdc99b44a313","IPY_MODEL_6b04b019813e458080f02bc9111433a6","IPY_MODEL_2e3818222bab4603a896be5976cb8409"],"layout":"IPY_MODEL_eeb468dbb94943fcb30219d4dd98fcab"}},"9754a5f1e61d49c8972df40ee9290375":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a31c60ff4dab48e08d2ef9293d85df6d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c40d970496ff447a8c0b80d787b07a4d","IPY_MODEL_40e6583408c447199ff5b94d23601936","IPY_MODEL_1141ae38bc6f473aab89db14fa4eeacf"],"layout":"IPY_MODEL_47b8a7f3d0544d79b30ad02e4222082e"}},"ad49cbf6b6e84ccaab873458182f22a1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c40d970496ff447a8c0b80d787b07a4d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0260998578564385a0b5b9425a0a5ca1","placeholder":"​","style":"IPY_MODEL_428ca357bd284d199e2558b1f577d79a","value":"100%"}},"d51d3aa414db4aa8b0ccae896e671152":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e06e5e9eb0414b6fad63bdc99b44a313":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e66444e9c714134bd2765cb3b6d1f15","placeholder":"​","style":"IPY_MODEL_220f78b6119042af8729543465e1234e","value":"100%"}},"e72cad76f875451a8e2479e2df237575":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eeb468dbb94943fcb30219d4dd98fcab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":5}