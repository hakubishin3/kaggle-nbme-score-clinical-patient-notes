{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "careful-fields",
   "metadata": {
    "id": "national-fancy"
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-sustainability",
   "metadata": {
    "id": "copyrighted-centre"
   },
   "source": [
    "- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-baltimore",
   "metadata": {
    "id": "imported-offset"
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "representative-chinese",
   "metadata": {
    "id": "complimentary-wyoming"
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"nbme-exp088\"\n",
    "ENV = \"local\"\n",
    "DEBUG_MODE = False\n",
    "SUBMISSION_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "educational-trailer",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y03AHjwJAlGL",
    "outputId": "c33caf3f-c530-4628-bcbf-b9af87b252a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "forced-blues",
   "metadata": {
    "id": "allied-circuit"
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    env=ENV\n",
    "    exp_name=EXP_NAME\n",
    "    debug=DEBUG_MODE\n",
    "    submission=SUBMISSION_MODE\n",
    "    apex=True\n",
    "    input_dir=None\n",
    "    output_dir=None\n",
    "    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n",
    "    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n",
    "    competition_name=\"nbme-score-clinical-patient-notes\"\n",
    "    id_col=\"id\"\n",
    "    target_col=\"location\"\n",
    "    pretrained_model_name=\"microsoft/deberta-v3-large\"\n",
    "    tokenizer=None\n",
    "    max_len=None\n",
    "    pseudo_plain_path='../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl'\n",
    "    #pseudo_plain_path=\"./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\"\n",
    "    n_pseudo_labels=100000\n",
    "    output_dim=1\n",
    "    dropout=0.2\n",
    "    num_workers=4\n",
    "    batch_size=3\n",
    "    lr=2e-5\n",
    "    betas=(0.9, 0.98)\n",
    "    weight_decay=0.1\n",
    "    alpha=1\n",
    "    gamma=2\n",
    "    smoothing=0.0001\n",
    "    num_warmup_steps_rate=0.1\n",
    "    batch_scheduler=True\n",
    "    epochs=1\n",
    "    n_fold=4\n",
    "    train_fold=[0, 1, 2, 3]\n",
    "    seed=71\n",
    "    gradient_accumulation_steps=2\n",
    "    max_grad_norm=1000\n",
    "    print_freq=100\n",
    "    train=True\n",
    "    inference=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aquatic-portland",
   "metadata": {
    "id": "geographic-hindu"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.train_fold = [0, 1]\n",
    "\n",
    "if CFG.submission:\n",
    "    CFG.train = False\n",
    "    CFG.inference = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-washer",
   "metadata": {
    "id": "confident-fifth"
   },
   "source": [
    "## Directory Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "anonymous-liberia",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "miniature-greeting",
    "outputId": "6a439b4d-636c-4026-8cfd-f86093855807"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "print(CFG.env)\n",
    "if CFG.env == \"colab\":\n",
    "    # colab環境\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "    # install packages\n",
    "    !pip install transformers==4.16.2\n",
    "    !pip install -q sentencepiece==0.1.96\n",
    "\n",
    "elif CFG.env == \"local\":\n",
    "    # ローカルサーバ\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "\n",
    "elif CFG.env == \"kaggle\":\n",
    "    # kaggle環境\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "molecular-prayer",
   "metadata": {
    "id": "nMFg9zv8YGcx"
   },
   "outputs": [],
   "source": [
    "# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n",
    "# This must be done before importing transformers\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "if CFG.env == \"colab\":\n",
    "    input_dir = Path(\"./drive/MyDrive/00.kaggle/input/deberta-v2-3-fast-tokenizer\")\n",
    "    transformers_path = Path(\"/usr/local/lib/python3.7/dist-packages/transformers\")\n",
    "else:\n",
    "    input_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n",
    "    transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n",
    "\n",
    "convert_file = input_dir / \"convert_slow_tokenizer.py\"\n",
    "conversion_path = transformers_path/convert_file.name\n",
    "\n",
    "if conversion_path.exists():\n",
    "    conversion_path.unlink()\n",
    "\n",
    "shutil.copy(convert_file, transformers_path)\n",
    "deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n",
    "\n",
    "for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n",
    "    filepath = deberta_v2_path/filename\n",
    "    if filepath.exists():\n",
    "        filepath.unlink()\n",
    "\n",
    "    shutil.copy(input_dir/filename, filepath)\n",
    "    \n",
    "    \n",
    "from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "central-federal",
   "metadata": {
    "id": "guilty-filename"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import BartModel,BertModel,BertTokenizer\n",
    "from transformers import DebertaModel,DebertaTokenizer\n",
    "from transformers import RobertaModel,RobertaTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-domestic",
   "metadata": {
    "id": "cubic-designation"
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "stupid-limit",
   "metadata": {
    "id": "opposite-plasma"
   },
   "outputs": [],
   "source": [
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "included-environment",
   "metadata": {
    "id": "multiple-poland"
   },
   "outputs": [],
   "source": [
    "def create_labels_for_scoring(df):\n",
    "    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n",
    "    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, \"location\"]\n",
    "        if lst:\n",
    "            new_lst = \";\".join(lst)\n",
    "            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n",
    "\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df[\"location_for_create_labels\"].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, token_probs, tokenizer):\n",
    "    res = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n",
    "        encoded = tokenizer(\n",
    "            text=text,\n",
    "            max_length=CFG.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n",
    "            start, end = offset_mapping\n",
    "            res[i][start:end] = pred\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_predicted_location_str(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        # result = np.where(char_prob >= th)[0] + 1\n",
    "        result = np.where(char_prob >= th)[0]\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        # result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = [f\"{min(r)} {max(r) + 1}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def scoring(df, th=0.5, use_token_prob=True):\n",
    "    labels = create_labels_for_scoring(df)\n",
    "\n",
    "    if use_token_prob:\n",
    "        token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n",
    "        char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n",
    "    else:\n",
    "        char_probs = df[[str(i) for i in range(CFG.max_char_len)]].values\n",
    "        char_probs = [char_probs[i] for i in range(len(char_probs))]\n",
    "\n",
    "    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n",
    "    preds = get_predictions(predicted_location_str)\n",
    "\n",
    "    score = get_score(labels, preds)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_best_thres(oof_df):\n",
    "    def f1_opt(x):\n",
    "        return -1 * scoring(oof_df, th=x)\n",
    "\n",
    "    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n",
    "    return best_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "preliminary-cache",
   "metadata": {
    "id": "seventh-fighter"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "amino-wallpaper",
   "metadata": {
    "id": "fifty-boundary"
   },
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "vocational-briefing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(texts, preds):\n",
    "    fix_tokenize_dict = {\n",
    "        'heart': ['h', 'eart'],\n",
    "        'hair': ['h', 'air'],\n",
    "        'adderal': ['a', 'dderal'],\n",
    "        'mother': ['m', 'other'],\n",
    "        'intermittent': ['i', 'ntermittent'],\n",
    "        'temperature': ['t', 'emperature'],\n",
    "        'episodes': ['e', 'pisodes'],\n",
    "        'no': ['n', 'o'],\n",
    "        'has': ['h', 'as'],\n",
    "        'LMP': ['L', 'MP'],\n",
    "        '10': ['1', '0'],\n",
    "        'blood': ['b', 'lood'],\n",
    "        'recurrent': ['r', 'ecurrent'],\n",
    "        'denies': ['d', 'enies'],\n",
    "        'sudden': ['s', 'udden'],\n",
    "        'Sexually': ['S', 'exually'],\n",
    "        'up': ['u', 'p'],\n",
    "        'wakes': ['w', 'akes'],\n",
    "        'sweats': ['s', 'weats'],\n",
    "        'hot': ['h', 'ot'],\n",
    "        'drenched': ['d', 'renched'],\n",
    "        'gnawing': ['g', 'nawing'],\n",
    "        'Uses': ['U', 'ses'],\n",
    "        'Begin': ['B', 'egin'],\n",
    "        'Nausea': ['N', 'ausea'],\n",
    "        'Burning': ['B', 'urning'],\n",
    "        'Started': ['S', 'tarted'],\n",
    "        'neurvousness': ['n', 'eurvousness'],\n",
    "        'constipation': ['c', 'onstipation'],\n",
    "        'nervousness': ['n', 'ervousness'],\n",
    "        'cold': ['c', 'old'],\n",
    "        'loss': ['l', 'oss'],\n",
    "        'CBC': ['C', 'BC'],\n",
    "        'Hx': ['H', 'x'],\n",
    "        'tingling': ['t', 'ingling'],\n",
    "        'feels': ['f', 'eels'],\n",
    "        'Lost': ['L', 'ost'],\n",
    "        'she': ['s', 'he'],\n",
    "        'racing': ['r', 'acing'],\n",
    "        'throat': ['t', 'hroat'],\n",
    "        'PATIENT': ['P', 'ATIENT'],\n",
    "        'recreational': ['r', 'ecreational'],\n",
    "        'clammy': ['c', 'lammy'],\n",
    "        'numbness': ['n', 'umbness'],\n",
    "        'like': ['l', 'ike'],\n",
    "        'reports': ['r', 'eports'],\n",
    "        'exercise': ['e', 'xercise'],\n",
    "        'started': ['s', 'tarted'],\n",
    "        'brough': ['b', 'rough'],\n",
    "        'Associated': ['A', 'ssociated'],\n",
    "        'exacerbated': ['e', 'xacerbated'],\n",
    "        'sharp': ['s', 'harp'],\n",
    "        'cannot': ['c', 'annot'],\n",
    "        'heavy': ['h', 'eavy'],\n",
    "        'fatigue': ['f', 'atigue'],\n",
    "        'trouble': ['t', 'rouble'],\n",
    "        'hearing': ['h', 'earing'],\n",
    "        'reduced': ['r', 'educed'],\n",
    "        'lack': ['l', 'ack'],\n",
    "        'vomiting': ['v', 'omiting'],\n",
    "        'generalized': ['g', 'eneralized'],\n",
    "        'body': ['b', 'ody'],\n",
    "        'all': ['a', 'll'],\n",
    "        'scratchy': ['s', 'cratchy'],\n",
    "        'mom': ['m', 'om'],\n",
    "        'discomfort': ['d', 'iscomfort'],\n",
    "        'CAD': ['C', 'AD'],\n",
    "        'Thyroid': ['T', 'hyroid'],\n",
    "        'BLADDER': ['B', 'LADDER'],\n",
    "        'diarrhea': ['d', 'iarrhea'],\n",
    "        'Started': ['S', 'tarted'],\n",
    "        'Vaginal': ['V', 'aginal'],\n",
    "        'sleeping': ['s', 'leeping'],\n",
    "        'UNCLE': ['U', 'NCLE'],\n",
    "        'USING': ['U', 'SING'],\n",
    "        'BURNING': ['B', 'URNING'],\n",
    "        'GETTING': ['G', 'ETTING'],\n",
    "        'ETOH': ['E', 'TOH'],\n",
    "        'ON': ['O', 'N'],\n",
    "        'INITIALLY': ['I', 'NITIALLY'],\n",
    "        'epigastric': ['e', 'pigastric'],\n",
    "        'occurs': ['o', 'ccurs'],\n",
    "        'began': ['b', 'egan'],\n",
    "        'alleviated': ['a', 'lleviated'],\n",
    "        'overwhelmed': ['o', 'verwhelmed'],\n",
    "        'clamminess': ['c', 'lamminess'],\n",
    "        'strongly': ['s', 'trongly'],\n",
    "        'lump': ['l', 'ump'],\n",
    "        'drugs': ['d', 'rugs'],\n",
    "        'chest': ['c', 'hest'],\n",
    "        'stuffy': ['s', 'tuffy'],\n",
    "        'changes': ['c', 'hanges'],\n",
    "        'trouble': ['t', 'rouble'],\n",
    "        'takes': ['t', 'akes'],\n",
    "        'tossing': ['t', 'ossing'],\n",
    "        'Fam': ['F', 'am'],\n",
    "        'sweating': ['s', 'weating'],\n",
    "        'dyspareunia': ['d', 'yspareunia'],\n",
    "        'irregular': ['i', 'rregular'],\n",
    "        'time': ['t', 'ime'],\n",
    "        'unpredictable': ['u', 'npredictable'],\n",
    "        'darkened': ['d', 'arkened'],\n",
    "        'anxiety': ['a', 'nxiety'],\n",
    "        'nervous': ['n', 'ervous'],\n",
    "        'TAKING': ['T', 'AKING'],\n",
    "        'losing': ['l', 'osing'],\n",
    "        'Difficulyt': ['D', 'ifficulyt'],\n",
    "        'Appetite': ['A', 'ppetite'],\n",
    "        'increased': ['i', 'ncreased'],\n",
    "        'fingers': ['f', 'ingers'],\n",
    "        'illicit': ['i', 'llicit'],\n",
    "        'claminess': ['c', 'laminess'],\n",
    "        'clamy': ['c', 'lamy'],\n",
    "        'Recently': ['R', 'ecently'],\n",
    "        'feeling': ['f', 'eeling'],\n",
    "        'aggrav': ['a', 'ggrav'],\n",
    "        'changing': ['c', 'hanging'],\n",
    "        'unable': ['u', 'nable'],\n",
    "        'SEEING': ['S', 'EEING'],\n",
    "        'staying': ['s', 'taying'],\n",
    "        'lightheadedness': ['l', 'ightheadedness'],\n",
    "        'lighheadeness': ['l', 'ighheadeness'],\n",
    "        'nail': ['n', 'ail'],\n",
    "        'pounding': ['p', 'ounding'],\n",
    "        'My': ['M', 'y'],\n",
    "        'Father': ['F', 'ather'],\n",
    "        'urinary': ['u', 'rinary'],\n",
    "        'pain': ['p', 'ain'],\n",
    "        'not': ['n', 'ot'],\n",
    "        'lower': ['l', 'ower'],\n",
    "        'menses': ['m', 'enses'],\n",
    "        'at': ['a', 't'],\n",
    "        'takes': ['t', 'akes'],\n",
    "        'initally': ['i', 'nitally'],\n",
    "        'melena': ['m', 'elena'],\n",
    "        'BOWEL': ['B', 'OWEL'],\n",
    "        'WEIGHT': ['W', 'EIGHT'],\n",
    "        'difficulty': ['d', 'ifficulty'],\n",
    "        'condo': ['c', 'ondo'],\n",
    "        'experiences': ['e', 'xperiences'],\n",
    "        'stuffy': ['s', 'tuffy'],\n",
    "        'rhinorrhea': ['r', 'hinorrhea'],\n",
    "        'felt': ['f', 'elt'],\n",
    "        'feverish': ['f', 'everish'],\n",
    "        'CYCLE': ['C', 'YCLE'],\n",
    "        'tampon': ['t', 'ampon'],\n",
    "        'Last': ['L', 'ast'],\n",
    "        'Son': ['S', 'on'],\n",
    "        'saw': ['s', 'aw'],\n",
    "        'tightness': ['t', 'ightness'],\n",
    "        'rash': ['r', 'ash'],\n",
    "        'ibuprofen': ['i', 'buprofen'],\n",
    "        'SCRATHY': ['S', 'CRATHY'],\n",
    "        'PHOTOPHOBIA': ['P', 'HOTOPHOBIA'],\n",
    "    }\n",
    "    preds_pp = preds.copy()\n",
    "    tk0 = tqdm(range(len(preds_pp)), total=len(preds_pp))\n",
    "    for raw_idx in tk0:\n",
    "        pred = preds[raw_idx]\n",
    "        text = texts[raw_idx]\n",
    "        if len(pred) != 0:\n",
    "            # pp1: indexが1から始まる予測値は0から始まるように修正 ## 0.88579 -> 0.88702\n",
    "            if pred[0][0] == 1:\n",
    "                preds_pp[raw_idx][0][0] = 0\n",
    "            for p_index, pp in enumerate(pred):\n",
    "                start, end = pred[p_index]\n",
    "                # pp2: startとendが同じ予測値はstartを前に１ずらす ## 0.88702 -> 0.88714\n",
    "                if start == end:\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                # pp3: 始点が改行の場合始点を1つ後ろにずらす ## 0.88714 -> 0.88746\n",
    "                if text[start] == '\\n':\n",
    "                    preds_pp[raw_idx][p_index][0] = start + 1\n",
    "                    start = start + 1\n",
    "                # pp4: 1-2などは-2で予測されることがあるので修正 ## 0.88746 -> 0.88747\n",
    "                if text[start-1].isdigit() and text[start] == '-' and text[start+1].isdigit():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                if text[start-1].isdigit() and text[start] == '/' and text[start+1].isdigit():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                # pp5: 67などは7で予測されることがあるので修正 ## 0.88747 -> 0.88748\n",
    "                if text[start-1].isdigit() and text[start].isdigit():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                # pp6: 文頭が大文字で始まるものは大文字部分が除かれて予測されることがあるので修正 ## 0.88748 -> 0.88761\n",
    "                if text[start-2] == '.' and text[start-1].isupper():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                if text[start-2] == ',' and text[start-1].isupper():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                if text[start-2] == ':' and text[start-1].isupper():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                if text[start-2] == '-' and text[start-1].isupper():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                # pp7: heart -> h + eart となっているようなものを修正する ## 0.88761 -> 0.88806\n",
    "                for key, fix_tokenize in fix_tokenize_dict.items():\n",
    "                    _s, s = fix_tokenize[0], fix_tokenize[1]\n",
    "                    if text[start-1].lower() == _s.lower() and text[start:start+len(s)].lower() == s.lower():\n",
    "                        preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                        start = start - 1\n",
    "    return preds_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "warming-mention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_from_preds_list(preds):\n",
    "    results = []\n",
    "    for pred in preds:\n",
    "        s = []\n",
    "        for p in pred:\n",
    "            s.append(' '.join(list(map(str, p))))\n",
    "        s = ';'.join(s)\n",
    "        results.append(s)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sonic-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trunc_pred(texts, preds):\n",
    "    preds_pp = preds.copy()\n",
    "    tk0 = tqdm(range(len(preds_pp)), total=len(preds_pp))\n",
    "    for raw_idx in tk0:\n",
    "        text = texts[raw_idx]\n",
    "        num_text = len(text)\n",
    "        preds_pp[raw_idx, num_text:] = 0\n",
    "    return preds_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "altered-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label(pn_history, location_list, max_char_len):\n",
    "    label = np.zeros(max_char_len)\n",
    "    label[len(pn_history):] = -1\n",
    "    if len(location_list) > 0:\n",
    "        for location in location_list:\n",
    "            start, end = int(location[0]), int(location[1])\n",
    "            label[start:end] = 1\n",
    "    return label\n",
    "\n",
    "def get_preds_from_results(results, texts, max_char_len):\n",
    "    labels = []\n",
    "    for idx, result in enumerate(results):\n",
    "        label = create_label(texts[idx], result, max_char_len)\n",
    "        labels.append(label)\n",
    "    labels = np.stack(labels)\n",
    "    print(labels.shape)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-catholic",
   "metadata": {
    "id": "unlimited-hotel"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "living-blair",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "classical-machine",
    "outputId": "918ddc66-5d2d-44dc-c637-0145350ded5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 6), (143, 3), (42146, 3), (5, 4))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(CFG.input_dir / \"train.csv\")\n",
    "features = pd.read_csv(CFG.input_dir / \"features.csv\")\n",
    "patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n",
    "test = pd.read_csv(CFG.input_dir / \"test.csv\")\n",
    "\n",
    "train.shape, features.shape, patient_notes.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eight-information",
   "metadata": {
    "id": "vanilla-iceland"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n",
    "    print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-michael",
   "metadata": {
    "id": "convenient-plant"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "shared-smart",
   "metadata": {
    "id": "convertible-thunder"
   },
   "outputs": [],
   "source": [
    "def preprocess_features(features):\n",
    "    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "\n",
    "\n",
    "features = preprocess_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "minus-bicycle",
   "metadata": {
    "id": "a7YBS_idYKtL"
   },
   "outputs": [],
   "source": [
    "features['feature_text'] = features['feature_text'].str.lower()\n",
    "patient_notes['pn_history'] = patient_notes['pn_history'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "still-washington",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "charitable-memphis",
    "outputId": "fca9323a-7ebb-469c-e494-f3ea3aa5db24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 8), (5, 6))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aware-hollow",
   "metadata": {
    "id": "governing-election"
   },
   "outputs": [],
   "source": [
    "train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n",
    "train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "electronic-diabetes",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "negative-provincial",
    "outputId": "a14cf34d-27c0-41c1-a97f-a02cb70b1482"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4399\n",
       "1    8181\n",
       "2    1296\n",
       "3     287\n",
       "4      99\n",
       "5      27\n",
       "6       9\n",
       "7       1\n",
       "8       1\n",
       "Name: annotation_length, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n",
    "display(train['annotation_length'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-reader",
   "metadata": {
    "id": "arbitrary-beatles"
   },
   "source": [
    "## CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "short-decade",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "important-murray",
    "outputId": "0f77c675-4cc4-4be5-9d99-346e40c7135a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    3575\n",
       "1    3575\n",
       "2    3575\n",
       "3    3575\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Fold = GroupKFold(n_splits=CFG.n_fold)\n",
    "groups = train['pn_num'].values\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-swimming",
   "metadata": {
    "id": "configured-chemistry"
   },
   "source": [
    "## Setup tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "suburban-boost",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hindu-contest",
    "outputId": "93f3e77c-2a10-4a0e-8e67-65805e82e164"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "if CFG.submission:\n",
    "    tokenizer = DebertaV2TokenizerFast.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n",
    "else:\n",
    "    tokenizer = DebertaV2TokenizerFast.from_pretrained(CFG.pretrained_model_name)\n",
    "    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n",
    "\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-sewing",
   "metadata": {
    "id": "alleged-protein"
   },
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "crazy-transcript",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "18b47303dd5b48bfb118053a80c44a40",
      "971945a52a6d4f06ba35e5363d264037",
      "2d7cfbb1d1a54c0590e59de0b280ab22",
      "054630edadaa453fb86d66a20e49030f",
      "8fce27d68c1c41ec9a3c58d439c2a5e7",
      "e9a3708f7e4c486da3a09e066b6936fb",
      "cd1144e40332427fa3e8d5bc7f57b924",
      "28c710503f154bdea731991801a85a47",
      "8c57bf78a80247db9521bd5b163502ef",
      "747b8a73ce544c569f4d063fc9d6d18a",
      "c7aa62f5eaca401dbbfbfd5f843d6a59"
     ]
    },
    "id": "composed-stroke",
    "outputId": "71ab3663-99ff-4577-ac6f-28425ca4c488"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0247145a1dc4d23a3b28ca8cd0d2992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 284\n"
     ]
    }
   ],
   "source": [
    "pn_history_lengths = []\n",
    "tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    pn_history_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(pn_history_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "certified-passion",
   "metadata": {
    "id": "emotional-region"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3eff6c47a9344dbaf3dc8b17512a582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 28\n"
     ]
    }
   ],
   "source": [
    "feature_text_lengths = []\n",
    "tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    feature_text_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(feature_text_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cheap-edgar",
   "metadata": {
    "id": "wrong-leisure"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 315\n"
     ]
    }
   ],
   "source": [
    "CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n",
    "\n",
    "print(\"max length:\", CFG.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dietary-somerset",
   "metadata": {
    "id": "convenient-gospel"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38eb1b6cb5aa46c0880ada0f2e775904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 950\n"
     ]
    }
   ],
   "source": [
    "pn_history_lengths = []\n",
    "tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n",
    "for text in tk0:\n",
    "    length = len(text)\n",
    "    pn_history_lengths.append(length)\n",
    "\n",
    "CFG.max_char_len = max(pn_history_lengths)\n",
    "\n",
    "print(\"max length:\", CFG.max_char_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "complimentary-trauma",
   "metadata": {
    "id": "representative-contributor"
   },
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, cfg, df, pseudo_label=None):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.max_char_len = self.cfg.max_char_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "        self.annotation_lengths = self.df[\"annotation_length\"].values\n",
    "        self.locations = self.df[\"location\"].values\n",
    "        if \"pseudo_idx\" in df.columns:\n",
    "            self.pseudo_idx = self.df[\"pseudo_idx\"].values\n",
    "            self.pseudo_label = pseudo_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def _create_mapping_from_token_to_char(self, pn_history):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        mapping_from_token_to_char = np.zeros(self.max_char_len)\n",
    "        offset_mapping = encoded[\"offset_mapping\"]\n",
    "        for i, offset in enumerate(offset_mapping):\n",
    "            start_idx, end_idx = offset\n",
    "            mapping_from_token_to_char[start_idx:end_idx] = i\n",
    "        return torch.tensor(mapping_from_token_to_char, dtype=torch.long)\n",
    "\n",
    "    def _create_label(self, pn_history, annotation_length, location_list):\n",
    "        label = np.zeros(self.max_char_len)\n",
    "        label[len(pn_history):] = -1\n",
    "        if annotation_length > 0:\n",
    "            for location in location_list:\n",
    "                for loc in [s.split() for s in location.split(\";\")]:\n",
    "                    start, end = int(loc[0]), int(loc[1])\n",
    "                    label[start:end] = 1\n",
    "        return torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        if not np.isnan(self.annotation_lengths[idx]):\n",
    "            label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n",
    "        else:\n",
    "            p_idx = int(self.pseudo_idx[idx])\n",
    "            label = torch.tensor(self.pseudo_label[p_idx], dtype=torch.float)\n",
    "        mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n",
    "        return input_, label, mapping_from_token_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "prostate-romance",
   "metadata": {
    "id": "decent-johnson"
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.max_char_len = self.cfg.max_char_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def _create_mapping_from_token_to_char(self, pn_history):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        mapping_from_token_to_char = np.zeros(self.max_char_len)\n",
    "        offset_mapping = encoded[\"offset_mapping\"]\n",
    "        for i, offset in enumerate(offset_mapping):\n",
    "            start_idx, end_idx = offset\n",
    "            mapping_from_token_to_char[start_idx:end_idx] = i\n",
    "        return torch.tensor(mapping_from_token_to_char, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n",
    "        return input_, mapping_from_token_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-mirror",
   "metadata": {
    "id": "arctic-joint"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "challenging-ranking",
   "metadata": {
    "id": "qTRu8eKOTlcX"
   },
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import MaskedLMOutput\n",
    "\n",
    "class MaskedModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(\n",
    "                cfg.pretrained_model_name,\n",
    "                output_hidden_states=False\n",
    "                )\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        \n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.pretrained_model_name, config=self.config)\n",
    "            self.lm_head = AutoModelForMaskedLM.from_pretrained(cfg.pretrained_model_name, config=self.config).cls # [cls, lm_head]\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "            self.lm_head = AutoModelForMaskedLM(self.config).cls # [cls, lm_head]\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            #position_ids=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None):\n",
    "        \n",
    "        outputs = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            #position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,)\n",
    "        \n",
    "        sequence_output = outputs[0]\n",
    "        prediction_scores = self.lm_head(sequence_output)\n",
    "\n",
    "        masked_lm_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        return MaskedLMOutput(loss=masked_lm_loss,\n",
    "                              logits=prediction_scores,\n",
    "                              hidden_states=outputs.hidden_states,\n",
    "                              attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "quarterly-sodium",
   "metadata": {
    "id": "OJt_cHeyTmDS"
   },
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, model_config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if model_config_path is None:\n",
    "            self.model_config = AutoConfig.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "        else:\n",
    "            self.model_config = torch.load(model_config_path)\n",
    "\n",
    "        if pretrained:\n",
    "            self.backbone = AutoModel.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                config=self.model_config,\n",
    "            )\n",
    "            print(f\"Load weight from pretrained\")\n",
    "        else:\n",
    "            self.backbone = AutoModel.from_config(self.model_config)\n",
    "            # itpt = AutoModelForMaskedLM.from_config(self.model_config)\n",
    "            #path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp010/checkpoint-130170/pytorch_model.bin\")\n",
    "            # path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n",
    "            # state_dict = torch.load(path)\n",
    "            # itpt.load_state_dict(state_dict)\n",
    "            #path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\")\n",
    "            #path = str(Path(\"../output\") / CFG.competition_name /  \"nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\")\n",
    "            #masked_model = MaskedModel(CFG, config_path=None, pretrained=True)\n",
    "            #state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "            #masked_model.load_state_dict(state)\n",
    "            #self.backbone = masked_model.model\n",
    "            #print(f\"Load weight from {path}\")\n",
    "            #del state, masked_model; gc.collect()\n",
    "\n",
    "        self.lstm = nn.GRU(\n",
    "            input_size=self.model_config.hidden_size,\n",
    "            bidirectional=True,\n",
    "            hidden_size=self.model_config.hidden_size // 2,\n",
    "            num_layers=4,\n",
    "            dropout=self.cfg.dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(self.cfg.dropout),\n",
    "            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, mappings_from_token_to_char):\n",
    "        h = self.backbone(**inputs)[\"last_hidden_state\"]  # [batch, seq_len, d_model]\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.unsqueeze(2).expand(-1, -1, self.model_config.hidden_size)\n",
    "        h = torch.gather(h, 1, mappings_from_token_to_char)    # [batch, seq_len, d_model]\n",
    "        h, _ = self.lstm(h)\n",
    "        output = self.fc(h)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-louisville",
   "metadata": {
    "id": "therapeutic-assembly"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dying-ambassador",
   "metadata": {
    "id": "going-conversion"
   },
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    train_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    epoch,\n",
    "    scheduler,\n",
    "    device,\n",
    "):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels, mappings_from_token_to_char) in enumerate(train_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device) \n",
    "        batch_size = labels.size(0)\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            output = model(inputs, mappings_from_token_to_char)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if CFG.batch_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n",
    "            print(\n",
    "                \"Epoch: [{0}][{1}/{2}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                \"Grad: {grad_norm:.4f}  \"\n",
    "                \"LR: {lr:.6f}  \"\n",
    "                .format(\n",
    "                    epoch+1,\n",
    "                    step,\n",
    "                    len(train_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n",
    "                    loss=losses,\n",
    "                     grad_norm=grad_norm,\n",
    "                     lr=scheduler.get_lr()[0],\n",
    "                )\n",
    "            )\n",
    "    del output, loss, inputs, labels, mappings_from_token_to_char, scaler, grad_norm; gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "valid-character",
   "metadata": {
    "id": "alleged-commonwealth"
   },
   "outputs": [],
   "source": [
    "def valid_fn(\n",
    "    val_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    device,\n",
    "):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels, mappings_from_token_to_char) in enumerate(val_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device) \n",
    "        batch_size = labels.size(0)\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            output = model(inputs, mappings_from_token_to_char)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "        loss = loss.mean()\n",
    "    \n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n",
    "            print(\n",
    "                \"EVAL: [{0}/{1}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                .format(\n",
    "                    step, len(val_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n",
    "                    loss=losses,\n",
    "                )\n",
    "            )\n",
    "    preds = np.concatenate(preds)\n",
    "    return losses.avg, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "public-calibration",
   "metadata": {
    "id": "middle-determination"
   },
   "outputs": [],
   "source": [
    "def inference_fn(test_dataloader, model, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preds = []\n",
    "    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n",
    "    for (inputs, mappings_from_token_to_char) in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs, mappings_from_token_to_char)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "residential-daughter",
   "metadata": {
    "id": "familiar-participation"
   },
   "outputs": [],
   "source": [
    "def train_loop(df, i_fold, device):\n",
    "    print(f\"========== fold: {i_fold} training ==========\")\n",
    "    train_idx = df[df[\"fold\"] != i_fold].index\n",
    "    val_idx = df[df[\"fold\"] == i_fold].index\n",
    "\n",
    "    train_folds = df.loc[train_idx].reset_index(drop=True)\n",
    "    val_folds = df.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    if CFG.pseudo_plain_path is not None:\n",
    "        pseudo_plain = pd.read_pickle(CFG.pseudo_plain_path)\n",
    "        print(f\"get pseudo plain from {CFG.pseudo_plain_path}\")\n",
    "        pseudo_label_list = []\n",
    "        weights = [0.4433659049657008, 0.20859987143371844, 0.3480342236005807]\n",
    "        for exp_name in [\"nbme-exp060\", \"nbme-exp067\", \"nbme-exp083\"]:\n",
    "            #pseudo_label_path = f'./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/{exp_name}/pseudo_labels_{i_fold}.npy'\n",
    "            pseudo_label_path = f'../output/nbme-score-clinical-patient-notes/{exp_name}/pseudo_labels_{i_fold}.npy'\n",
    "            pseudo_label = np.load(pseudo_label_path)\n",
    "            print(f\"get pseudo labels from {pseudo_label_path}\")\n",
    "            pseudo_label_list.append(pseudo_label)\n",
    "\n",
    "        pseudo_label = weights[0] * pseudo_label_list[0] + weights[1] * pseudo_label_list[1] + weights[2] * pseudo_label_list[2]\n",
    "        pseudo_label = trunc_pred(pseudo_plain[\"pn_history\"].values, pseudo_label)\n",
    "        predicted_location_str = get_predicted_location_str(pseudo_label, th=0.5)\n",
    "        preds = get_predictions(predicted_location_str)\n",
    "        results_postprocess = postprocess(pseudo_plain[\"pn_history\"].values, preds)\n",
    "        #results_postprocess = get_results_from_preds_list(results_postprocess)\n",
    "        pseudo_label = get_preds_from_results(results_postprocess, pseudo_plain[\"pn_history\"].values, pseudo_label.shape[1])\n",
    "        print(pseudo_plain.shape, pseudo_label.shape)\n",
    "\n",
    "        pseudo_plain['feature_text'] = pseudo_plain['feature_text'].str.lower()\n",
    "        pseudo_plain['pn_history'] = pseudo_plain['pn_history'].str.lower()\n",
    "\n",
    "        pseudo_plain[\"pseudo_idx\"] = np.arange(len(pseudo_plain))\n",
    "        pseudo_plain = pseudo_plain.sample(n=CFG.n_pseudo_labels)\n",
    "        print(pseudo_plain.shape)\n",
    "        train_folds = pd.concat([train_folds, pseudo_plain], axis=0, ignore_index=True)\n",
    "        print(train_folds.shape)\n",
    "\n",
    "    train_dataset = TrainingDataset(CFG, train_folds, pseudo_label)\n",
    "    val_dataset = TrainingDataset(CFG, val_folds)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "    #model = CustomModel(CFG, model_config_path=None, pretrained=False)   # itptを使うため\n",
    "    model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "    path = f\"../output/nbme-score-clinical-patient-notes/nbme-exp087/fold{i_fold}_best.pth\"\n",
    "    state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    print(f\"load weights from {path}\")\n",
    "    del state; gc.collect()\n",
    "    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n",
    "    model.to(device)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in param_optimizer if not any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n",
    "        {\"params\": [p for n, p in param_optimizer if any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=CFG.lr,\n",
    "        betas=CFG.betas,\n",
    "        weight_decay=CFG.weight_decay,\n",
    "    )\n",
    "    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n",
    "    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_train_optimization_steps,\n",
    "    )\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    best_score = -1 * np.inf\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "        avg_loss = train_fn(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            scheduler,\n",
    "            device,\n",
    "        )\n",
    "        avg_val_loss, val_preds = valid_fn(\n",
    "            val_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "\n",
    "        # scoring\n",
    "        val_folds[[str(i) for i in range(CFG.max_char_len)]] = val_preds\n",
    "        score = scoring(val_folds, th=0.5, use_token_prob=False)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n",
    "        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"predictions\": val_preds,\n",
    "                },\n",
    "                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "            )\n",
    "\n",
    "    predictions = torch.load(\n",
    "        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )[\"predictions\"]\n",
    "    val_folds[[str(i) for i in range(CFG.max_char_len)]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return val_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-argument",
   "metadata": {
    "id": "coated-cameroon"
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "scenic-animal",
   "metadata": {
    "id": "quality-expansion"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for i_fold in range(CFG.n_fold):\n",
    "            if i_fold in CFG.train_fold:\n",
    "                _oof_df = train_loop(train, i_fold, device)\n",
    "                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n",
    "        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    if CFG.submission:\n",
    "        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n",
    "    else:\n",
    "        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    best_thres = 0.5\n",
    "    best_score = 0.\n",
    "    for th in np.arange(0.45, 0.55, 0.01):\n",
    "        th = np.round(th, 2)\n",
    "        score = scoring(oof_df, th=th, use_token_prob=False)\n",
    "        if best_score < score:\n",
    "            best_thres = th\n",
    "            best_score = score\n",
    "    print(f\"best_thres: {best_thres}  score: {best_score:.5f}\")\n",
    "\n",
    "    if CFG.inference:\n",
    "        test_dataset = TestDataset(CFG, test)\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=CFG.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=CFG.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        predictions = []\n",
    "        for i_fold in CFG.train_fold:\n",
    "            if CFG.submission:\n",
    "                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n",
    "                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n",
    "            else:\n",
    "                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n",
    "\n",
    "            state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "            model.load_state_dict(state[\"model\"])\n",
    "            print(f\"load weights from {path}\")\n",
    "            test_char_probs = inference_fn(test_dataloader, model, device)\n",
    "            predictions.append(test_char_probs)\n",
    "\n",
    "            del state, test_char_probs, model; gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        predictions = np.mean(predictions, axis=0)\n",
    "        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n",
    "        test[CFG.target_col] = predicted_location_str\n",
    "        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n",
    "        test[[CFG.id_col, CFG.target_col]].to_csv(\n",
    "            CFG.output_dir / \"submission.csv\", index=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "lasting-triple",
   "metadata": {
    "id": "proprietary-civilian"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "get pseudo plain from ../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_0.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_0.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp083/pseudo_labels_0.npy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d42b228604a4989bd163c7ec8716826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12317bc3384448d1b58ca8ecf97aacdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(612602, 950)\n",
      "(612602, 6) (612602, 950)\n",
      "(100000, 7)\n",
      "(110725, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n",
      "load weights from ../output/nbme-score-clinical-patient-notes/nbme-exp087/fold0_best.pth\n",
      "Epoch: [1][0/36908] Elapsed 0m 1s (remain 899m 6s) Loss: 0.0004(0.0004) Grad: 3458.6907  LR: 0.000000  \n",
      "Epoch: [1][100/36908] Elapsed 1m 15s (remain 459m 21s) Loss: 0.0000(0.0024) Grad: 5.6068  LR: 0.000001  \n",
      "Epoch: [1][200/36908] Elapsed 2m 31s (remain 460m 29s) Loss: 0.0003(0.0018) Grad: 492.1881  LR: 0.000001  \n",
      "Epoch: [1][300/36908] Elapsed 3m 46s (remain 459m 41s) Loss: 0.0000(0.0018) Grad: 12.0818  LR: 0.000002  \n",
      "Epoch: [1][400/36908] Elapsed 5m 1s (remain 457m 48s) Loss: 0.0289(0.0018) Grad: 37991.3125  LR: 0.000002  \n",
      "Epoch: [1][500/36908] Elapsed 6m 17s (remain 457m 20s) Loss: 0.0021(0.0018) Grad: 3401.4019  LR: 0.000003  \n",
      "Epoch: [1][600/36908] Elapsed 7m 34s (remain 457m 18s) Loss: 0.0000(0.0019) Grad: 5.2664  LR: 0.000003  \n",
      "Epoch: [1][700/36908] Elapsed 8m 50s (remain 456m 35s) Loss: 0.0000(0.0018) Grad: 20.4231  LR: 0.000004  \n",
      "Epoch: [1][800/36908] Elapsed 10m 6s (remain 455m 42s) Loss: 0.0001(0.0020) Grad: 396.9222  LR: 0.000004  \n",
      "Epoch: [1][900/36908] Elapsed 11m 24s (remain 455m 44s) Loss: 0.0003(0.0021) Grad: 1791.9794  LR: 0.000005  \n",
      "Epoch: [1][1000/36908] Elapsed 12m 40s (remain 454m 28s) Loss: 0.0065(0.0020) Grad: 64243.3633  LR: 0.000005  \n",
      "Epoch: [1][1100/36908] Elapsed 13m 54s (remain 452m 34s) Loss: 0.0010(0.0023) Grad: 2558.2346  LR: 0.000006  \n",
      "Epoch: [1][1200/36908] Elapsed 15m 11s (remain 451m 35s) Loss: 0.0000(0.0023) Grad: 25.8110  LR: 0.000007  \n",
      "Epoch: [1][1300/36908] Elapsed 16m 27s (remain 450m 33s) Loss: 0.0000(0.0022) Grad: 91.3191  LR: 0.000007  \n",
      "Epoch: [1][1400/36908] Elapsed 17m 45s (remain 449m 53s) Loss: 0.0003(0.0022) Grad: 792.4384  LR: 0.000008  \n",
      "Epoch: [1][1500/36908] Elapsed 19m 1s (remain 448m 49s) Loss: 0.0074(0.0022) Grad: 16621.3594  LR: 0.000008  \n",
      "Epoch: [1][1600/36908] Elapsed 20m 16s (remain 447m 12s) Loss: 0.0080(0.0022) Grad: 34517.5898  LR: 0.000009  \n",
      "Epoch: [1][1700/36908] Elapsed 21m 33s (remain 446m 4s) Loss: 0.0000(0.0022) Grad: 146.4987  LR: 0.000009  \n",
      "Epoch: [1][1800/36908] Elapsed 22m 49s (remain 444m 55s) Loss: 0.0000(0.0022) Grad: 49.5062  LR: 0.000010  \n",
      "Epoch: [1][1900/36908] Elapsed 24m 6s (remain 443m 51s) Loss: 0.0001(0.0022) Grad: 1248.7797  LR: 0.000010  \n",
      "Epoch: [1][2000/36908] Elapsed 25m 22s (remain 442m 41s) Loss: 0.0000(0.0022) Grad: 26.9513  LR: 0.000011  \n",
      "Epoch: [1][2100/36908] Elapsed 26m 39s (remain 441m 44s) Loss: 0.0000(0.0022) Grad: 16.6105  LR: 0.000011  \n",
      "Epoch: [1][2200/36908] Elapsed 27m 56s (remain 440m 33s) Loss: 0.0453(0.0022) Grad: 124642.4688  LR: 0.000012  \n",
      "Epoch: [1][2300/36908] Elapsed 29m 12s (remain 439m 14s) Loss: 0.0000(0.0022) Grad: 11.1996  LR: 0.000012  \n",
      "Epoch: [1][2400/36908] Elapsed 30m 29s (remain 438m 13s) Loss: 0.0006(0.0023) Grad: 3891.8252  LR: 0.000013  \n",
      "Epoch: [1][2500/36908] Elapsed 31m 47s (remain 437m 16s) Loss: 0.0025(0.0023) Grad: 21107.3262  LR: 0.000014  \n",
      "Epoch: [1][2600/36908] Elapsed 33m 3s (remain 436m 6s) Loss: 0.0014(0.0023) Grad: 8348.5791  LR: 0.000014  \n",
      "Epoch: [1][2700/36908] Elapsed 34m 18s (remain 434m 33s) Loss: 0.0007(0.0023) Grad: 3579.4009  LR: 0.000015  \n",
      "Epoch: [1][2800/36908] Elapsed 35m 35s (remain 433m 20s) Loss: 0.0006(0.0024) Grad: 8666.0166  LR: 0.000015  \n",
      "Epoch: [1][2900/36908] Elapsed 36m 52s (remain 432m 10s) Loss: 0.0024(0.0024) Grad: 7024.6904  LR: 0.000016  \n",
      "Epoch: [1][3000/36908] Elapsed 38m 9s (remain 431m 2s) Loss: 0.0068(0.0024) Grad: 25604.8652  LR: 0.000016  \n",
      "Epoch: [1][3100/36908] Elapsed 39m 25s (remain 429m 52s) Loss: 0.0001(0.0024) Grad: 974.4891  LR: 0.000017  \n",
      "Epoch: [1][3200/36908] Elapsed 40m 42s (remain 428m 45s) Loss: 0.0021(0.0024) Grad: 9162.7783  LR: 0.000017  \n",
      "Epoch: [1][3300/36908] Elapsed 41m 59s (remain 427m 34s) Loss: 0.0000(0.0024) Grad: 4.3803  LR: 0.000018  \n",
      "Epoch: [1][3400/36908] Elapsed 43m 15s (remain 426m 8s) Loss: 0.0000(0.0024) Grad: 175.6457  LR: 0.000018  \n",
      "Epoch: [1][3500/36908] Elapsed 44m 31s (remain 424m 55s) Loss: 0.0000(0.0024) Grad: 244.8456  LR: 0.000019  \n",
      "Epoch: [1][3600/36908] Elapsed 45m 48s (remain 423m 44s) Loss: 0.0081(0.0024) Grad: 2796.3401  LR: 0.000020  \n",
      "Epoch: [1][3700/36908] Elapsed 47m 4s (remain 422m 21s) Loss: 0.0000(0.0024) Grad: 2.0650  LR: 0.000020  \n",
      "Epoch: [1][3800/36908] Elapsed 48m 19s (remain 420m 56s) Loss: 0.0077(0.0025) Grad: 34873.6758  LR: 0.000020  \n",
      "Epoch: [1][3900/36908] Elapsed 49m 34s (remain 419m 30s) Loss: 0.0002(0.0025) Grad: 845.4100  LR: 0.000020  \n",
      "Epoch: [1][4000/36908] Elapsed 50m 51s (remain 418m 14s) Loss: 0.0032(0.0025) Grad: 75747.6875  LR: 0.000020  \n",
      "Epoch: [1][4100/36908] Elapsed 52m 6s (remain 416m 49s) Loss: 0.0189(0.0025) Grad: 70043.7812  LR: 0.000020  \n",
      "Epoch: [1][4200/36908] Elapsed 53m 22s (remain 415m 34s) Loss: 0.0002(0.0025) Grad: 4144.3989  LR: 0.000020  \n",
      "Epoch: [1][4300/36908] Elapsed 54m 39s (remain 414m 19s) Loss: 0.0000(0.0026) Grad: 31.7093  LR: 0.000020  \n",
      "Epoch: [1][4400/36908] Elapsed 55m 56s (remain 413m 11s) Loss: 0.0000(0.0026) Grad: 151.0511  LR: 0.000020  \n",
      "Epoch: [1][4500/36908] Elapsed 57m 12s (remain 411m 55s) Loss: 0.0000(0.0026) Grad: 172.7506  LR: 0.000020  \n",
      "Epoch: [1][4600/36908] Elapsed 58m 28s (remain 410m 38s) Loss: 0.0000(0.0026) Grad: 1.3022  LR: 0.000019  \n",
      "Epoch: [1][4700/36908] Elapsed 59m 45s (remain 409m 21s) Loss: 0.0001(0.0026) Grad: 1056.7460  LR: 0.000019  \n",
      "Epoch: [1][4800/36908] Elapsed 61m 1s (remain 408m 9s) Loss: 0.0014(0.0026) Grad: 70753.3828  LR: 0.000019  \n",
      "Epoch: [1][4900/36908] Elapsed 62m 18s (remain 406m 53s) Loss: 0.0055(0.0026) Grad: 45315.5156  LR: 0.000019  \n",
      "Epoch: [1][5000/36908] Elapsed 63m 34s (remain 405m 35s) Loss: 0.0000(0.0026) Grad: 89.4598  LR: 0.000019  \n",
      "Epoch: [1][5100/36908] Elapsed 64m 49s (remain 404m 15s) Loss: 0.0005(0.0026) Grad: 3810.4434  LR: 0.000019  \n",
      "Epoch: [1][5200/36908] Elapsed 66m 7s (remain 403m 6s) Loss: 0.0021(0.0026) Grad: 7623.3071  LR: 0.000019  \n",
      "Epoch: [1][5300/36908] Elapsed 67m 24s (remain 401m 55s) Loss: 0.0002(0.0026) Grad: 9228.9883  LR: 0.000019  \n",
      "Epoch: [1][5400/36908] Elapsed 68m 41s (remain 400m 41s) Loss: 0.0000(0.0026) Grad: 53.2417  LR: 0.000019  \n",
      "Epoch: [1][5500/36908] Elapsed 69m 58s (remain 399m 32s) Loss: 0.0059(0.0027) Grad: 71599.9375  LR: 0.000019  \n",
      "Epoch: [1][5600/36908] Elapsed 71m 15s (remain 398m 16s) Loss: 0.0003(0.0027) Grad: 5005.1055  LR: 0.000019  \n",
      "Epoch: [1][5700/36908] Elapsed 72m 30s (remain 396m 55s) Loss: 0.0000(0.0027) Grad: 177.6592  LR: 0.000019  \n",
      "Epoch: [1][5800/36908] Elapsed 73m 47s (remain 395m 39s) Loss: 0.0000(0.0027) Grad: 119.2435  LR: 0.000019  \n",
      "Epoch: [1][5900/36908] Elapsed 75m 5s (remain 394m 32s) Loss: 0.0267(0.0027) Grad: 143057.3125  LR: 0.000019  \n",
      "Epoch: [1][6000/36908] Elapsed 76m 22s (remain 393m 23s) Loss: 0.0022(0.0027) Grad: 23326.5645  LR: 0.000019  \n",
      "Epoch: [1][6100/36908] Elapsed 77m 38s (remain 392m 5s) Loss: 0.0235(0.0027) Grad: 152753.8125  LR: 0.000019  \n",
      "Epoch: [1][6200/36908] Elapsed 78m 55s (remain 390m 51s) Loss: 0.0000(0.0027) Grad: 37.3619  LR: 0.000018  \n",
      "Epoch: [1][6300/36908] Elapsed 80m 11s (remain 389m 32s) Loss: 0.0018(0.0027) Grad: 24343.2891  LR: 0.000018  \n",
      "Epoch: [1][6400/36908] Elapsed 81m 28s (remain 388m 17s) Loss: 0.0000(0.0027) Grad: 12.7160  LR: 0.000018  \n",
      "Epoch: [1][6500/36908] Elapsed 82m 44s (remain 387m 1s) Loss: 0.0010(0.0027) Grad: 3927.7654  LR: 0.000018  \n",
      "Epoch: [1][6600/36908] Elapsed 84m 1s (remain 385m 49s) Loss: 0.0000(0.0027) Grad: 18.5524  LR: 0.000018  \n",
      "Epoch: [1][6700/36908] Elapsed 85m 20s (remain 384m 41s) Loss: 0.0000(0.0027) Grad: 241.1025  LR: 0.000018  \n",
      "Epoch: [1][6800/36908] Elapsed 86m 37s (remain 383m 28s) Loss: 0.0000(0.0027) Grad: 7.9353  LR: 0.000018  \n",
      "Epoch: [1][6900/36908] Elapsed 87m 55s (remain 382m 16s) Loss: 0.0000(0.0027) Grad: 26.2900  LR: 0.000018  \n",
      "Epoch: [1][7000/36908] Elapsed 89m 12s (remain 381m 3s) Loss: 0.0009(0.0027) Grad: 2402.5876  LR: 0.000018  \n",
      "Epoch: [1][7100/36908] Elapsed 90m 28s (remain 379m 47s) Loss: 0.0000(0.0027) Grad: 8.5096  LR: 0.000018  \n",
      "Epoch: [1][7200/36908] Elapsed 91m 45s (remain 378m 33s) Loss: 0.0000(0.0027) Grad: 220.5989  LR: 0.000018  \n",
      "Epoch: [1][7300/36908] Elapsed 93m 2s (remain 377m 19s) Loss: 0.0000(0.0027) Grad: 471.6808  LR: 0.000018  \n",
      "Epoch: [1][7400/36908] Elapsed 94m 19s (remain 376m 4s) Loss: 0.0001(0.0028) Grad: 1440.3688  LR: 0.000018  \n",
      "Epoch: [1][7500/36908] Elapsed 95m 36s (remain 374m 49s) Loss: 0.0000(0.0028) Grad: 21.7522  LR: 0.000018  \n",
      "Epoch: [1][7600/36908] Elapsed 96m 54s (remain 373m 37s) Loss: 0.0004(0.0028) Grad: 947.3207  LR: 0.000018  \n",
      "Epoch: [1][7700/36908] Elapsed 98m 11s (remain 372m 24s) Loss: 0.0018(0.0028) Grad: 28956.9766  LR: 0.000018  \n",
      "Epoch: [1][7800/36908] Elapsed 99m 28s (remain 371m 10s) Loss: 0.0243(0.0028) Grad: 79132.9297  LR: 0.000018  \n",
      "Epoch: [1][7900/36908] Elapsed 100m 44s (remain 369m 52s) Loss: 0.0000(0.0028) Grad: 166.3904  LR: 0.000017  \n",
      "Epoch: [1][8000/36908] Elapsed 102m 1s (remain 368m 36s) Loss: 0.0000(0.0028) Grad: 871.3862  LR: 0.000017  \n",
      "Epoch: [1][8100/36908] Elapsed 103m 18s (remain 367m 20s) Loss: 0.0001(0.0028) Grad: 1314.0648  LR: 0.000017  \n",
      "Epoch: [1][8200/36908] Elapsed 104m 34s (remain 366m 2s) Loss: 0.0125(0.0028) Grad: 39384.8984  LR: 0.000017  \n",
      "Epoch: [1][8300/36908] Elapsed 105m 50s (remain 364m 45s) Loss: 0.0049(0.0028) Grad: 12850.7480  LR: 0.000017  \n",
      "Epoch: [1][8400/36908] Elapsed 107m 6s (remain 363m 27s) Loss: 0.0000(0.0028) Grad: 273.4679  LR: 0.000017  \n",
      "Epoch: [1][8500/36908] Elapsed 108m 23s (remain 362m 13s) Loss: 0.0000(0.0028) Grad: 439.7105  LR: 0.000017  \n",
      "Epoch: [1][8600/36908] Elapsed 109m 41s (remain 360m 59s) Loss: 0.0001(0.0028) Grad: 347.7040  LR: 0.000017  \n",
      "Epoch: [1][8700/36908] Elapsed 110m 59s (remain 359m 48s) Loss: 0.0143(0.0028) Grad: 14420.1914  LR: 0.000017  \n",
      "Epoch: [1][8800/36908] Elapsed 112m 15s (remain 358m 31s) Loss: 0.0000(0.0028) Grad: 65.1823  LR: 0.000017  \n",
      "Epoch: [1][8900/36908] Elapsed 113m 32s (remain 357m 15s) Loss: 0.0000(0.0028) Grad: 13.4763  LR: 0.000017  \n",
      "Epoch: [1][9000/36908] Elapsed 114m 49s (remain 356m 0s) Loss: 0.0000(0.0028) Grad: 62.5526  LR: 0.000017  \n",
      "Epoch: [1][9100/36908] Elapsed 116m 6s (remain 354m 44s) Loss: 0.0000(0.0028) Grad: 32.7370  LR: 0.000017  \n",
      "Epoch: [1][9200/36908] Elapsed 117m 22s (remain 353m 27s) Loss: 0.0027(0.0028) Grad: 8006.9238  LR: 0.000017  \n",
      "Epoch: [1][9300/36908] Elapsed 118m 39s (remain 352m 11s) Loss: 0.0005(0.0028) Grad: 6872.2881  LR: 0.000017  \n",
      "Epoch: [1][9400/36908] Elapsed 119m 55s (remain 350m 54s) Loss: 0.0008(0.0028) Grad: 6449.1143  LR: 0.000017  \n",
      "Epoch: [1][9500/36908] Elapsed 121m 12s (remain 349m 39s) Loss: 0.0000(0.0028) Grad: 56.9830  LR: 0.000017  \n",
      "Epoch: [1][9600/36908] Elapsed 122m 29s (remain 348m 23s) Loss: 0.0006(0.0028) Grad: 747.6318  LR: 0.000016  \n",
      "Epoch: [1][9700/36908] Elapsed 123m 45s (remain 347m 4s) Loss: 0.0002(0.0028) Grad: 1559.1022  LR: 0.000016  \n",
      "Epoch: [1][9800/36908] Elapsed 125m 1s (remain 345m 48s) Loss: 0.0000(0.0028) Grad: 34.7722  LR: 0.000016  \n",
      "Epoch: [1][9900/36908] Elapsed 126m 18s (remain 344m 32s) Loss: 0.0001(0.0028) Grad: 241.8719  LR: 0.000016  \n",
      "Epoch: [1][10000/36908] Elapsed 127m 35s (remain 343m 16s) Loss: 0.0031(0.0028) Grad: 42020.2461  LR: 0.000016  \n",
      "Epoch: [1][10100/36908] Elapsed 128m 51s (remain 341m 57s) Loss: 0.0003(0.0029) Grad: 679.1722  LR: 0.000016  \n",
      "Epoch: [1][10200/36908] Elapsed 130m 7s (remain 340m 39s) Loss: 0.0000(0.0029) Grad: 51.3098  LR: 0.000016  \n",
      "Epoch: [1][10300/36908] Elapsed 131m 23s (remain 339m 22s) Loss: 0.0005(0.0029) Grad: 1090.9352  LR: 0.000016  \n",
      "Epoch: [1][10400/36908] Elapsed 132m 39s (remain 338m 3s) Loss: 0.0001(0.0029) Grad: 890.2051  LR: 0.000016  \n",
      "Epoch: [1][10500/36908] Elapsed 133m 56s (remain 336m 50s) Loss: 0.0004(0.0028) Grad: 287.7024  LR: 0.000016  \n",
      "Epoch: [1][10600/36908] Elapsed 135m 13s (remain 335m 34s) Loss: 0.0022(0.0028) Grad: 15057.2656  LR: 0.000016  \n",
      "Epoch: [1][10700/36908] Elapsed 136m 31s (remain 334m 21s) Loss: 0.0034(0.0028) Grad: 17427.7168  LR: 0.000016  \n",
      "Epoch: [1][10800/36908] Elapsed 137m 48s (remain 333m 5s) Loss: 0.0000(0.0028) Grad: 77.9585  LR: 0.000016  \n",
      "Epoch: [1][10900/36908] Elapsed 139m 3s (remain 331m 46s) Loss: 0.0000(0.0028) Grad: 4.9239  LR: 0.000016  \n",
      "Epoch: [1][11000/36908] Elapsed 140m 20s (remain 330m 30s) Loss: 0.0000(0.0029) Grad: 102.1486  LR: 0.000016  \n",
      "Epoch: [1][11100/36908] Elapsed 141m 37s (remain 329m 14s) Loss: 0.0000(0.0028) Grad: 35.6938  LR: 0.000016  \n",
      "Epoch: [1][11200/36908] Elapsed 142m 54s (remain 327m 59s) Loss: 0.0000(0.0028) Grad: 869.3294  LR: 0.000015  \n",
      "Epoch: [1][11300/36908] Elapsed 144m 10s (remain 326m 41s) Loss: 0.0083(0.0028) Grad: 88971.4453  LR: 0.000015  \n",
      "Epoch: [1][11400/36908] Elapsed 145m 27s (remain 325m 26s) Loss: 0.0000(0.0028) Grad: 22.5309  LR: 0.000015  \n",
      "Epoch: [1][11500/36908] Elapsed 146m 43s (remain 324m 8s) Loss: 0.0000(0.0028) Grad: 4.7734  LR: 0.000015  \n",
      "Epoch: [1][11600/36908] Elapsed 147m 59s (remain 322m 49s) Loss: 0.0044(0.0028) Grad: 35056.9258  LR: 0.000015  \n",
      "Epoch: [1][11700/36908] Elapsed 149m 15s (remain 321m 33s) Loss: 0.0009(0.0028) Grad: 20402.2871  LR: 0.000015  \n",
      "Epoch: [1][11800/36908] Elapsed 150m 31s (remain 320m 15s) Loss: 0.0130(0.0028) Grad: 48524.6562  LR: 0.000015  \n",
      "Epoch: [1][11900/36908] Elapsed 151m 48s (remain 318m 58s) Loss: 0.0001(0.0028) Grad: 383.1028  LR: 0.000015  \n",
      "Epoch: [1][12000/36908] Elapsed 153m 3s (remain 317m 40s) Loss: 0.0079(0.0028) Grad: 3403.8052  LR: 0.000015  \n",
      "Epoch: [1][12100/36908] Elapsed 154m 20s (remain 316m 24s) Loss: 0.0069(0.0028) Grad: 17401.8105  LR: 0.000015  \n",
      "Epoch: [1][12200/36908] Elapsed 155m 39s (remain 315m 11s) Loss: 0.0054(0.0028) Grad: 11079.8945  LR: 0.000015  \n",
      "Epoch: [1][12300/36908] Elapsed 156m 55s (remain 313m 55s) Loss: 0.0000(0.0028) Grad: 167.2845  LR: 0.000015  \n",
      "Epoch: [1][12400/36908] Elapsed 158m 13s (remain 312m 40s) Loss: 0.0014(0.0028) Grad: 7579.7134  LR: 0.000015  \n",
      "Epoch: [1][12500/36908] Elapsed 159m 30s (remain 311m 25s) Loss: 0.0000(0.0028) Grad: 59.5683  LR: 0.000015  \n",
      "Epoch: [1][12600/36908] Elapsed 160m 47s (remain 310m 9s) Loss: 0.0000(0.0028) Grad: 593.2241  LR: 0.000015  \n",
      "Epoch: [1][12700/36908] Elapsed 162m 4s (remain 308m 53s) Loss: 0.0008(0.0028) Grad: 1747.8353  LR: 0.000015  \n",
      "Epoch: [1][12800/36908] Elapsed 163m 19s (remain 307m 35s) Loss: 0.0022(0.0028) Grad: 6926.9976  LR: 0.000015  \n",
      "Epoch: [1][12900/36908] Elapsed 164m 36s (remain 306m 19s) Loss: 0.0059(0.0028) Grad: 28048.4492  LR: 0.000014  \n",
      "Epoch: [1][13000/36908] Elapsed 165m 54s (remain 305m 4s) Loss: 0.0015(0.0028) Grad: 31941.5488  LR: 0.000014  \n",
      "Epoch: [1][13100/36908] Elapsed 167m 10s (remain 303m 46s) Loss: 0.0000(0.0029) Grad: 72.6804  LR: 0.000014  \n",
      "Epoch: [1][13200/36908] Elapsed 168m 25s (remain 302m 28s) Loss: 0.0002(0.0028) Grad: 3732.4490  LR: 0.000014  \n",
      "Epoch: [1][13300/36908] Elapsed 169m 41s (remain 301m 11s) Loss: 0.0115(0.0029) Grad: 98823.1641  LR: 0.000014  \n",
      "Epoch: [1][13400/36908] Elapsed 170m 58s (remain 299m 55s) Loss: 0.0069(0.0029) Grad: 35171.4219  LR: 0.000014  \n",
      "Epoch: [1][13500/36908] Elapsed 172m 15s (remain 298m 38s) Loss: 0.0000(0.0029) Grad: 138.8770  LR: 0.000014  \n",
      "Epoch: [1][13600/36908] Elapsed 173m 31s (remain 297m 22s) Loss: 0.0000(0.0029) Grad: 84.2843  LR: 0.000014  \n",
      "Epoch: [1][13700/36908] Elapsed 174m 47s (remain 296m 4s) Loss: 0.0036(0.0029) Grad: 14917.9951  LR: 0.000014  \n",
      "Epoch: [1][13800/36908] Elapsed 176m 4s (remain 294m 47s) Loss: 0.0000(0.0029) Grad: 9.0378  LR: 0.000014  \n",
      "Epoch: [1][13900/36908] Elapsed 177m 20s (remain 293m 30s) Loss: 0.0011(0.0029) Grad: 5802.4180  LR: 0.000014  \n",
      "Epoch: [1][14000/36908] Elapsed 178m 38s (remain 292m 16s) Loss: 0.0000(0.0029) Grad: 1430.0940  LR: 0.000014  \n",
      "Epoch: [1][14100/36908] Elapsed 179m 55s (remain 291m 0s) Loss: 0.0002(0.0029) Grad: 781.6994  LR: 0.000014  \n",
      "Epoch: [1][14200/36908] Elapsed 181m 12s (remain 289m 44s) Loss: 0.0007(0.0029) Grad: 19989.0605  LR: 0.000014  \n",
      "Epoch: [1][14300/36908] Elapsed 182m 28s (remain 288m 28s) Loss: 0.0000(0.0029) Grad: 33.7451  LR: 0.000014  \n",
      "Epoch: [1][14400/36908] Elapsed 183m 46s (remain 287m 12s) Loss: 0.0024(0.0029) Grad: 24732.5957  LR: 0.000014  \n",
      "Epoch: [1][14500/36908] Elapsed 185m 2s (remain 285m 55s) Loss: 0.0032(0.0029) Grad: 13810.5518  LR: 0.000013  \n",
      "Epoch: [1][14600/36908] Elapsed 186m 18s (remain 284m 38s) Loss: 0.0003(0.0029) Grad: 2577.4031  LR: 0.000013  \n",
      "Epoch: [1][14700/36908] Elapsed 187m 34s (remain 283m 21s) Loss: 0.0000(0.0029) Grad: 17.6804  LR: 0.000013  \n",
      "Epoch: [1][14800/36908] Elapsed 188m 51s (remain 282m 5s) Loss: 0.0000(0.0029) Grad: 81.2825  LR: 0.000013  \n",
      "Epoch: [1][14900/36908] Elapsed 190m 8s (remain 280m 48s) Loss: 0.0164(0.0029) Grad: 225474.4375  LR: 0.000013  \n",
      "Epoch: [1][15000/36908] Elapsed 191m 26s (remain 279m 34s) Loss: 0.0129(0.0029) Grad: 110702.7969  LR: 0.000013  \n",
      "Epoch: [1][15100/36908] Elapsed 192m 43s (remain 278m 19s) Loss: 0.0038(0.0029) Grad: 23556.9688  LR: 0.000013  \n",
      "Epoch: [1][15200/36908] Elapsed 194m 1s (remain 277m 4s) Loss: 0.0095(0.0029) Grad: 46845.0547  LR: 0.000013  \n",
      "Epoch: [1][15300/36908] Elapsed 195m 18s (remain 275m 48s) Loss: 0.0142(0.0029) Grad: 162359.9375  LR: 0.000013  \n",
      "Epoch: [1][15400/36908] Elapsed 196m 34s (remain 274m 31s) Loss: 0.0000(0.0028) Grad: 3.8128  LR: 0.000013  \n",
      "Epoch: [1][15500/36908] Elapsed 197m 52s (remain 273m 15s) Loss: 0.0000(0.0028) Grad: 306.5410  LR: 0.000013  \n",
      "Epoch: [1][15600/36908] Elapsed 199m 9s (remain 271m 59s) Loss: 0.0000(0.0028) Grad: 116.0198  LR: 0.000013  \n",
      "Epoch: [1][15700/36908] Elapsed 200m 26s (remain 270m 43s) Loss: 0.0014(0.0028) Grad: 26416.7793  LR: 0.000013  \n",
      "Epoch: [1][15800/36908] Elapsed 201m 43s (remain 269m 28s) Loss: 0.0078(0.0028) Grad: 46129.9141  LR: 0.000013  \n",
      "Epoch: [1][15900/36908] Elapsed 203m 1s (remain 268m 13s) Loss: 0.0009(0.0028) Grad: 1605.3502  LR: 0.000013  \n",
      "Epoch: [1][16000/36908] Elapsed 204m 18s (remain 266m 56s) Loss: 0.0000(0.0028) Grad: 8.5596  LR: 0.000013  \n",
      "Epoch: [1][16100/36908] Elapsed 205m 35s (remain 265m 40s) Loss: 0.0030(0.0028) Grad: 14561.8936  LR: 0.000013  \n",
      "Epoch: [1][16200/36908] Elapsed 206m 52s (remain 264m 24s) Loss: 0.0000(0.0028) Grad: 67.5282  LR: 0.000012  \n",
      "Epoch: [1][16300/36908] Elapsed 208m 8s (remain 263m 7s) Loss: 0.0057(0.0028) Grad: 40420.9492  LR: 0.000012  \n",
      "Epoch: [1][16400/36908] Elapsed 209m 24s (remain 261m 50s) Loss: 0.0000(0.0028) Grad: 542.0757  LR: 0.000012  \n",
      "Epoch: [1][16500/36908] Elapsed 210m 41s (remain 260m 33s) Loss: 0.0004(0.0028) Grad: 3354.0347  LR: 0.000012  \n",
      "Epoch: [1][16600/36908] Elapsed 211m 57s (remain 259m 16s) Loss: 0.0196(0.0028) Grad: 183710.4531  LR: 0.000012  \n",
      "Epoch: [1][16700/36908] Elapsed 213m 14s (remain 257m 59s) Loss: 0.0001(0.0028) Grad: 3364.8254  LR: 0.000012  \n",
      "Epoch: [1][16800/36908] Elapsed 214m 29s (remain 256m 41s) Loss: 0.0000(0.0028) Grad: 156.0480  LR: 0.000012  \n",
      "Epoch: [1][16900/36908] Elapsed 215m 46s (remain 255m 25s) Loss: 0.0000(0.0028) Grad: 12.8479  LR: 0.000012  \n",
      "Epoch: [1][17000/36908] Elapsed 217m 2s (remain 254m 7s) Loss: 0.0000(0.0028) Grad: 8.5605  LR: 0.000012  \n",
      "Epoch: [1][17100/36908] Elapsed 218m 19s (remain 252m 52s) Loss: 0.0001(0.0028) Grad: 6549.6377  LR: 0.000012  \n",
      "Epoch: [1][17200/36908] Elapsed 219m 36s (remain 251m 36s) Loss: 0.0000(0.0028) Grad: 5.7018  LR: 0.000012  \n",
      "Epoch: [1][17300/36908] Elapsed 220m 53s (remain 250m 20s) Loss: 0.0000(0.0028) Grad: 266.9490  LR: 0.000012  \n",
      "Epoch: [1][17400/36908] Elapsed 222m 10s (remain 249m 3s) Loss: 0.0022(0.0028) Grad: 47448.8555  LR: 0.000012  \n",
      "Epoch: [1][17500/36908] Elapsed 223m 26s (remain 247m 46s) Loss: 0.0150(0.0028) Grad: 243617.9844  LR: 0.000012  \n",
      "Epoch: [1][17600/36908] Elapsed 224m 42s (remain 246m 28s) Loss: 0.0000(0.0028) Grad: 10.1884  LR: 0.000012  \n",
      "Epoch: [1][17700/36908] Elapsed 225m 58s (remain 245m 12s) Loss: 0.0000(0.0028) Grad: 9.6385  LR: 0.000012  \n",
      "Epoch: [1][17800/36908] Elapsed 227m 14s (remain 243m 54s) Loss: 0.0021(0.0028) Grad: 48484.5352  LR: 0.000012  \n",
      "Epoch: [1][17900/36908] Elapsed 228m 30s (remain 242m 38s) Loss: 0.0000(0.0028) Grad: 5.4186  LR: 0.000011  \n",
      "Epoch: [1][18000/36908] Elapsed 229m 47s (remain 241m 21s) Loss: 0.0000(0.0028) Grad: 254.0956  LR: 0.000011  \n",
      "Epoch: [1][18100/36908] Elapsed 231m 3s (remain 240m 4s) Loss: 0.0000(0.0028) Grad: 6.3611  LR: 0.000011  \n",
      "Epoch: [1][18200/36908] Elapsed 232m 19s (remain 238m 46s) Loss: 0.0001(0.0028) Grad: 720.9823  LR: 0.000011  \n",
      "Epoch: [1][18300/36908] Elapsed 233m 35s (remain 237m 29s) Loss: 0.0096(0.0028) Grad: 34594.9102  LR: 0.000011  \n",
      "Epoch: [1][18400/36908] Elapsed 234m 51s (remain 236m 12s) Loss: 0.0000(0.0028) Grad: 6.1762  LR: 0.000011  \n",
      "Epoch: [1][18500/36908] Elapsed 236m 6s (remain 234m 54s) Loss: 0.0044(0.0028) Grad: 32256.5020  LR: 0.000011  \n",
      "Epoch: [1][18600/36908] Elapsed 237m 23s (remain 233m 38s) Loss: 0.0000(0.0028) Grad: 14.3235  LR: 0.000011  \n",
      "Epoch: [1][18700/36908] Elapsed 238m 40s (remain 232m 22s) Loss: 0.0025(0.0028) Grad: 56909.2344  LR: 0.000011  \n",
      "Epoch: [1][18800/36908] Elapsed 239m 58s (remain 231m 6s) Loss: 0.0140(0.0028) Grad: 184434.9688  LR: 0.000011  \n",
      "Epoch: [1][18900/36908] Elapsed 241m 15s (remain 229m 50s) Loss: 0.0007(0.0028) Grad: 8878.7041  LR: 0.000011  \n",
      "Epoch: [1][19000/36908] Elapsed 242m 33s (remain 228m 35s) Loss: 0.0005(0.0028) Grad: 20317.3516  LR: 0.000011  \n",
      "Epoch: [1][19100/36908] Elapsed 243m 50s (remain 227m 19s) Loss: 0.0007(0.0028) Grad: 3132.5505  LR: 0.000011  \n",
      "Epoch: [1][19200/36908] Elapsed 245m 7s (remain 226m 3s) Loss: 0.0012(0.0028) Grad: 21489.6387  LR: 0.000011  \n",
      "Epoch: [1][19300/36908] Elapsed 246m 24s (remain 224m 46s) Loss: 0.0000(0.0028) Grad: 60.1725  LR: 0.000011  \n",
      "Epoch: [1][19400/36908] Elapsed 247m 41s (remain 223m 30s) Loss: 0.0000(0.0028) Grad: 168.5282  LR: 0.000011  \n",
      "Epoch: [1][19500/36908] Elapsed 248m 59s (remain 222m 15s) Loss: 0.0002(0.0028) Grad: 2718.5364  LR: 0.000010  \n",
      "Epoch: [1][19600/36908] Elapsed 250m 17s (remain 220m 59s) Loss: 0.0024(0.0028) Grad: 20218.6621  LR: 0.000010  \n",
      "Epoch: [1][19700/36908] Elapsed 251m 34s (remain 219m 43s) Loss: 0.0000(0.0028) Grad: 28.5674  LR: 0.000010  \n",
      "Epoch: [1][19800/36908] Elapsed 252m 51s (remain 218m 27s) Loss: 0.0000(0.0028) Grad: 236.9546  LR: 0.000010  \n",
      "Epoch: [1][19900/36908] Elapsed 254m 8s (remain 217m 11s) Loss: 0.0000(0.0028) Grad: 932.6685  LR: 0.000010  \n",
      "Epoch: [1][20000/36908] Elapsed 255m 25s (remain 215m 54s) Loss: 0.0000(0.0028) Grad: 67.1786  LR: 0.000010  \n",
      "Epoch: [1][20100/36908] Elapsed 256m 42s (remain 214m 38s) Loss: 0.0000(0.0028) Grad: 140.8701  LR: 0.000010  \n",
      "Epoch: [1][20200/36908] Elapsed 257m 59s (remain 213m 21s) Loss: 0.0000(0.0028) Grad: 7.3233  LR: 0.000010  \n",
      "Epoch: [1][20300/36908] Elapsed 259m 16s (remain 212m 5s) Loss: 0.0001(0.0028) Grad: 879.9780  LR: 0.000010  \n",
      "Epoch: [1][20400/36908] Elapsed 260m 33s (remain 210m 49s) Loss: 0.0000(0.0028) Grad: 19.6982  LR: 0.000010  \n",
      "Epoch: [1][20500/36908] Elapsed 261m 50s (remain 209m 33s) Loss: 0.0003(0.0028) Grad: 5159.8857  LR: 0.000010  \n",
      "Epoch: [1][20600/36908] Elapsed 263m 7s (remain 208m 16s) Loss: 0.0043(0.0028) Grad: 15304.2852  LR: 0.000010  \n",
      "Epoch: [1][20700/36908] Elapsed 264m 22s (remain 206m 59s) Loss: 0.0000(0.0028) Grad: 4.7458  LR: 0.000010  \n",
      "Epoch: [1][20800/36908] Elapsed 265m 40s (remain 205m 43s) Loss: 0.0017(0.0028) Grad: 46222.6602  LR: 0.000010  \n",
      "Epoch: [1][20900/36908] Elapsed 266m 57s (remain 204m 27s) Loss: 0.0072(0.0028) Grad: 63958.4023  LR: 0.000010  \n",
      "Epoch: [1][21000/36908] Elapsed 268m 14s (remain 203m 10s) Loss: 0.0000(0.0028) Grad: 5.2718  LR: 0.000010  \n",
      "Epoch: [1][21100/36908] Elapsed 269m 32s (remain 201m 54s) Loss: 0.0040(0.0028) Grad: 60366.5039  LR: 0.000010  \n",
      "Epoch: [1][21200/36908] Elapsed 270m 49s (remain 200m 38s) Loss: 0.0003(0.0028) Grad: 8309.3047  LR: 0.000009  \n",
      "Epoch: [1][21300/36908] Elapsed 272m 6s (remain 199m 22s) Loss: 0.0000(0.0028) Grad: 4.2994  LR: 0.000009  \n",
      "Epoch: [1][21400/36908] Elapsed 273m 24s (remain 198m 6s) Loss: 0.0000(0.0028) Grad: 262.4803  LR: 0.000009  \n",
      "Epoch: [1][21500/36908] Elapsed 274m 40s (remain 196m 49s) Loss: 0.0000(0.0028) Grad: 55.9476  LR: 0.000009  \n",
      "Epoch: [1][21600/36908] Elapsed 275m 58s (remain 195m 33s) Loss: 0.0000(0.0028) Grad: 4.6591  LR: 0.000009  \n",
      "Epoch: [1][21700/36908] Elapsed 277m 14s (remain 194m 16s) Loss: 0.0000(0.0028) Grad: 13.9369  LR: 0.000009  \n",
      "Epoch: [1][21800/36908] Elapsed 278m 31s (remain 193m 0s) Loss: 0.0000(0.0028) Grad: 4.4686  LR: 0.000009  \n",
      "Epoch: [1][21900/36908] Elapsed 279m 47s (remain 191m 43s) Loss: 0.0012(0.0028) Grad: 2784.7876  LR: 0.000009  \n",
      "Epoch: [1][22000/36908] Elapsed 281m 3s (remain 190m 25s) Loss: 0.0000(0.0028) Grad: 23.0084  LR: 0.000009  \n",
      "Epoch: [1][22100/36908] Elapsed 282m 19s (remain 189m 8s) Loss: 0.0000(0.0028) Grad: 5.6290  LR: 0.000009  \n",
      "Epoch: [1][22200/36908] Elapsed 283m 34s (remain 187m 51s) Loss: 0.0079(0.0028) Grad: 122781.3281  LR: 0.000009  \n",
      "Epoch: [1][22300/36908] Elapsed 284m 52s (remain 186m 35s) Loss: 0.0579(0.0028) Grad: 804943.0000  LR: 0.000009  \n",
      "Epoch: [1][22400/36908] Elapsed 286m 8s (remain 185m 18s) Loss: 0.0000(0.0028) Grad: 5.1554  LR: 0.000009  \n",
      "Epoch: [1][22500/36908] Elapsed 287m 24s (remain 184m 1s) Loss: 0.0080(0.0028) Grad: 463059.5312  LR: 0.000009  \n",
      "Epoch: [1][22600/36908] Elapsed 288m 40s (remain 182m 44s) Loss: 0.0000(0.0028) Grad: 7.6380  LR: 0.000009  \n",
      "Epoch: [1][22700/36908] Elapsed 289m 55s (remain 181m 26s) Loss: 0.0130(0.0028) Grad: 93054.1328  LR: 0.000009  \n",
      "Epoch: [1][22800/36908] Elapsed 291m 13s (remain 180m 10s) Loss: 0.0010(0.0028) Grad: 12755.2832  LR: 0.000008  \n",
      "Epoch: [1][22900/36908] Elapsed 292m 29s (remain 178m 53s) Loss: 0.0014(0.0028) Grad: 37162.2852  LR: 0.000008  \n",
      "Epoch: [1][23000/36908] Elapsed 293m 45s (remain 177m 36s) Loss: 0.0006(0.0028) Grad: 4851.5488  LR: 0.000008  \n",
      "Epoch: [1][23100/36908] Elapsed 295m 1s (remain 176m 20s) Loss: 0.0000(0.0028) Grad: 73.2023  LR: 0.000008  \n",
      "Epoch: [1][23200/36908] Elapsed 296m 17s (remain 175m 2s) Loss: 0.0000(0.0028) Grad: 1.8844  LR: 0.000008  \n",
      "Epoch: [1][23300/36908] Elapsed 297m 35s (remain 173m 46s) Loss: 0.0001(0.0028) Grad: 255.8954  LR: 0.000008  \n",
      "Epoch: [1][23400/36908] Elapsed 298m 52s (remain 172m 30s) Loss: 0.0002(0.0028) Grad: 2578.7498  LR: 0.000008  \n",
      "Epoch: [1][23500/36908] Elapsed 300m 8s (remain 171m 13s) Loss: 0.0001(0.0028) Grad: 2341.9365  LR: 0.000008  \n",
      "Epoch: [1][23600/36908] Elapsed 301m 24s (remain 169m 56s) Loss: 0.0001(0.0028) Grad: 9048.4521  LR: 0.000008  \n",
      "Epoch: [1][23700/36908] Elapsed 302m 42s (remain 168m 40s) Loss: 0.0034(0.0027) Grad: 53490.9805  LR: 0.000008  \n",
      "Epoch: [1][23800/36908] Elapsed 303m 59s (remain 167m 24s) Loss: 0.0000(0.0027) Grad: 99.3103  LR: 0.000008  \n",
      "Epoch: [1][23900/36908] Elapsed 305m 16s (remain 166m 8s) Loss: 0.0084(0.0027) Grad: 22457.0176  LR: 0.000008  \n",
      "Epoch: [1][24000/36908] Elapsed 306m 34s (remain 164m 51s) Loss: 0.0000(0.0027) Grad: 225.7705  LR: 0.000008  \n",
      "Epoch: [1][24100/36908] Elapsed 307m 51s (remain 163m 35s) Loss: 0.0056(0.0028) Grad: 14379.3320  LR: 0.000008  \n",
      "Epoch: [1][24200/36908] Elapsed 309m 9s (remain 162m 19s) Loss: 0.0000(0.0028) Grad: 15.7710  LR: 0.000008  \n",
      "Epoch: [1][24300/36908] Elapsed 310m 25s (remain 161m 2s) Loss: 0.0008(0.0028) Grad: 78197.8203  LR: 0.000008  \n",
      "Epoch: [1][24400/36908] Elapsed 311m 42s (remain 159m 46s) Loss: 0.0000(0.0027) Grad: 50.5437  LR: 0.000008  \n",
      "Epoch: [1][24500/36908] Elapsed 312m 59s (remain 158m 29s) Loss: 0.0100(0.0027) Grad: 43709.9375  LR: 0.000007  \n",
      "Epoch: [1][24600/36908] Elapsed 314m 16s (remain 157m 12s) Loss: 0.0002(0.0027) Grad: 3829.0017  LR: 0.000007  \n",
      "Epoch: [1][24700/36908] Elapsed 315m 33s (remain 155m 56s) Loss: 0.0000(0.0027) Grad: 18.3494  LR: 0.000007  \n",
      "Epoch: [1][24800/36908] Elapsed 316m 50s (remain 154m 40s) Loss: 0.0000(0.0027) Grad: 36.5014  LR: 0.000007  \n",
      "Epoch: [1][24900/36908] Elapsed 318m 6s (remain 153m 23s) Loss: 0.0000(0.0027) Grad: 112.8240  LR: 0.000007  \n",
      "Epoch: [1][25000/36908] Elapsed 319m 21s (remain 152m 6s) Loss: 0.0000(0.0027) Grad: 10.2009  LR: 0.000007  \n",
      "Epoch: [1][25100/36908] Elapsed 320m 38s (remain 150m 49s) Loss: 0.0000(0.0027) Grad: 20.3127  LR: 0.000007  \n",
      "Epoch: [1][25200/36908] Elapsed 321m 55s (remain 149m 32s) Loss: 0.0000(0.0027) Grad: 105.9685  LR: 0.000007  \n",
      "Epoch: [1][25300/36908] Elapsed 323m 12s (remain 148m 16s) Loss: 0.0000(0.0027) Grad: 25.6332  LR: 0.000007  \n",
      "Epoch: [1][25400/36908] Elapsed 324m 27s (remain 146m 59s) Loss: 0.0000(0.0027) Grad: 71.9254  LR: 0.000007  \n",
      "Epoch: [1][25500/36908] Elapsed 325m 44s (remain 145m 42s) Loss: 0.0031(0.0027) Grad: 21855.2969  LR: 0.000007  \n",
      "Epoch: [1][25600/36908] Elapsed 326m 59s (remain 144m 25s) Loss: 0.0000(0.0027) Grad: 75.0953  LR: 0.000007  \n",
      "Epoch: [1][25700/36908] Elapsed 328m 15s (remain 143m 8s) Loss: 0.0012(0.0027) Grad: 31737.8574  LR: 0.000007  \n",
      "Epoch: [1][25800/36908] Elapsed 329m 33s (remain 141m 52s) Loss: 0.0000(0.0027) Grad: 265.0725  LR: 0.000007  \n",
      "Epoch: [1][25900/36908] Elapsed 330m 50s (remain 140m 35s) Loss: 0.0001(0.0027) Grad: 274.1416  LR: 0.000007  \n",
      "Epoch: [1][26000/36908] Elapsed 332m 7s (remain 139m 19s) Loss: 0.0000(0.0027) Grad: 10.3956  LR: 0.000007  \n",
      "Epoch: [1][26100/36908] Elapsed 333m 25s (remain 138m 2s) Loss: 0.0019(0.0027) Grad: 29051.2129  LR: 0.000007  \n",
      "Epoch: [1][26200/36908] Elapsed 334m 41s (remain 136m 46s) Loss: 0.0046(0.0027) Grad: 146870.9062  LR: 0.000006  \n",
      "Epoch: [1][26300/36908] Elapsed 335m 58s (remain 135m 29s) Loss: 0.0000(0.0027) Grad: 7.0743  LR: 0.000006  \n",
      "Epoch: [1][26400/36908] Elapsed 337m 16s (remain 134m 13s) Loss: 0.0000(0.0027) Grad: 69.3338  LR: 0.000006  \n",
      "Epoch: [1][26500/36908] Elapsed 338m 33s (remain 132m 56s) Loss: 0.0000(0.0027) Grad: 170.9111  LR: 0.000006  \n",
      "Epoch: [1][26600/36908] Elapsed 339m 51s (remain 131m 40s) Loss: 0.0000(0.0027) Grad: 30.7057  LR: 0.000006  \n",
      "Epoch: [1][26700/36908] Elapsed 341m 7s (remain 130m 24s) Loss: 0.0001(0.0027) Grad: 2300.4478  LR: 0.000006  \n",
      "Epoch: [1][26800/36908] Elapsed 342m 23s (remain 129m 7s) Loss: 0.0000(0.0027) Grad: 1363.3741  LR: 0.000006  \n",
      "Epoch: [1][26900/36908] Elapsed 343m 39s (remain 127m 50s) Loss: 0.0000(0.0027) Grad: 7.1170  LR: 0.000006  \n",
      "Epoch: [1][27000/36908] Elapsed 344m 55s (remain 126m 33s) Loss: 0.0000(0.0027) Grad: 34.1355  LR: 0.000006  \n",
      "Epoch: [1][27100/36908] Elapsed 346m 12s (remain 125m 16s) Loss: 0.0000(0.0027) Grad: 335.6093  LR: 0.000006  \n",
      "Epoch: [1][27200/36908] Elapsed 347m 27s (remain 123m 59s) Loss: 0.0000(0.0027) Grad: 14.9991  LR: 0.000006  \n",
      "Epoch: [1][27300/36908] Elapsed 348m 44s (remain 122m 43s) Loss: 0.0076(0.0027) Grad: 58108.3555  LR: 0.000006  \n",
      "Epoch: [1][27400/36908] Elapsed 350m 0s (remain 121m 26s) Loss: 0.0039(0.0027) Grad: 10710.9873  LR: 0.000006  \n",
      "Epoch: [1][27500/36908] Elapsed 351m 18s (remain 120m 9s) Loss: 0.0066(0.0027) Grad: 85080.4219  LR: 0.000006  \n",
      "Epoch: [1][27600/36908] Elapsed 352m 35s (remain 118m 53s) Loss: 0.0000(0.0027) Grad: 75.3546  LR: 0.000006  \n",
      "Epoch: [1][27700/36908] Elapsed 353m 52s (remain 117m 37s) Loss: 0.0001(0.0027) Grad: 5455.0776  LR: 0.000006  \n",
      "Epoch: [1][27800/36908] Elapsed 355m 10s (remain 116m 20s) Loss: 0.0000(0.0027) Grad: 43.5913  LR: 0.000005  \n",
      "Epoch: [1][27900/36908] Elapsed 356m 27s (remain 115m 4s) Loss: 0.0003(0.0027) Grad: 8823.8994  LR: 0.000005  \n",
      "Epoch: [1][28000/36908] Elapsed 357m 44s (remain 113m 47s) Loss: 0.0043(0.0027) Grad: 58814.5938  LR: 0.000005  \n",
      "Epoch: [1][28100/36908] Elapsed 359m 2s (remain 112m 31s) Loss: 0.0000(0.0027) Grad: 16.6889  LR: 0.000005  \n",
      "Epoch: [1][28200/36908] Elapsed 360m 20s (remain 111m 15s) Loss: 0.0000(0.0027) Grad: 1992.2527  LR: 0.000005  \n",
      "Epoch: [1][28300/36908] Elapsed 361m 37s (remain 109m 58s) Loss: 0.0000(0.0027) Grad: 17.4151  LR: 0.000005  \n",
      "Epoch: [1][28400/36908] Elapsed 362m 54s (remain 108m 42s) Loss: 0.0072(0.0027) Grad: 166185.6250  LR: 0.000005  \n",
      "Epoch: [1][28500/36908] Elapsed 364m 11s (remain 107m 25s) Loss: 0.0000(0.0027) Grad: 12.2331  LR: 0.000005  \n",
      "Epoch: [1][28600/36908] Elapsed 365m 27s (remain 106m 8s) Loss: 0.0000(0.0027) Grad: 25.1646  LR: 0.000005  \n",
      "Epoch: [1][28700/36908] Elapsed 366m 45s (remain 104m 52s) Loss: 0.0000(0.0027) Grad: 68.9364  LR: 0.000005  \n",
      "Epoch: [1][28800/36908] Elapsed 368m 2s (remain 103m 35s) Loss: 0.0004(0.0027) Grad: 3314.1545  LR: 0.000005  \n",
      "Epoch: [1][28900/36908] Elapsed 369m 19s (remain 102m 19s) Loss: 0.0000(0.0027) Grad: 257.8862  LR: 0.000005  \n",
      "Epoch: [1][29000/36908] Elapsed 370m 37s (remain 101m 2s) Loss: 0.0000(0.0027) Grad: 4.4932  LR: 0.000005  \n",
      "Epoch: [1][29100/36908] Elapsed 371m 53s (remain 99m 46s) Loss: 0.0105(0.0027) Grad: 122800.9766  LR: 0.000005  \n",
      "Epoch: [1][29200/36908] Elapsed 373m 9s (remain 98m 29s) Loss: 0.0000(0.0027) Grad: 222.9332  LR: 0.000005  \n",
      "Epoch: [1][29300/36908] Elapsed 374m 24s (remain 97m 12s) Loss: 0.0001(0.0027) Grad: 7245.7744  LR: 0.000005  \n",
      "Epoch: [1][29400/36908] Elapsed 375m 41s (remain 95m 55s) Loss: 0.0000(0.0027) Grad: 10.8934  LR: 0.000005  \n",
      "Epoch: [1][29500/36908] Elapsed 376m 58s (remain 94m 38s) Loss: 0.0000(0.0027) Grad: 402.8232  LR: 0.000004  \n",
      "Epoch: [1][29600/36908] Elapsed 378m 15s (remain 93m 22s) Loss: 0.0000(0.0027) Grad: 14.6026  LR: 0.000004  \n",
      "Epoch: [1][29700/36908] Elapsed 379m 30s (remain 92m 5s) Loss: 0.0000(0.0027) Grad: 104.1692  LR: 0.000004  \n",
      "Epoch: [1][29800/36908] Elapsed 380m 46s (remain 90m 48s) Loss: 0.0128(0.0027) Grad: 570101.3125  LR: 0.000004  \n",
      "Epoch: [1][29900/36908] Elapsed 382m 4s (remain 89m 32s) Loss: 0.0005(0.0027) Grad: 5226.3726  LR: 0.000004  \n",
      "Epoch: [1][30000/36908] Elapsed 383m 20s (remain 88m 15s) Loss: 0.0006(0.0027) Grad: 2881.1470  LR: 0.000004  \n",
      "Epoch: [1][30100/36908] Elapsed 384m 37s (remain 86m 58s) Loss: 0.0003(0.0027) Grad: 4820.3105  LR: 0.000004  \n",
      "Epoch: [1][30200/36908] Elapsed 385m 54s (remain 85m 42s) Loss: 0.0006(0.0027) Grad: 5295.7266  LR: 0.000004  \n",
      "Epoch: [1][30300/36908] Elapsed 387m 11s (remain 84m 25s) Loss: 0.0000(0.0027) Grad: 63.0345  LR: 0.000004  \n",
      "Epoch: [1][30400/36908] Elapsed 388m 27s (remain 83m 8s) Loss: 0.0056(0.0027) Grad: 53805.8477  LR: 0.000004  \n",
      "Epoch: [1][30500/36908] Elapsed 389m 44s (remain 81m 52s) Loss: 0.0000(0.0027) Grad: 51.2193  LR: 0.000004  \n",
      "Epoch: [1][30600/36908] Elapsed 391m 1s (remain 80m 35s) Loss: 0.0000(0.0027) Grad: 81.0272  LR: 0.000004  \n",
      "Epoch: [1][30700/36908] Elapsed 392m 17s (remain 79m 18s) Loss: 0.0013(0.0027) Grad: 24054.2500  LR: 0.000004  \n",
      "Epoch: [1][30800/36908] Elapsed 393m 34s (remain 78m 2s) Loss: 0.0000(0.0027) Grad: 44.2751  LR: 0.000004  \n",
      "Epoch: [1][30900/36908] Elapsed 394m 50s (remain 76m 45s) Loss: 0.0000(0.0027) Grad: 30.7395  LR: 0.000004  \n",
      "Epoch: [1][31000/36908] Elapsed 396m 5s (remain 75m 28s) Loss: 0.0000(0.0027) Grad: 25.9317  LR: 0.000004  \n",
      "Epoch: [1][31100/36908] Elapsed 397m 21s (remain 74m 11s) Loss: 0.0000(0.0027) Grad: 3.3077  LR: 0.000003  \n",
      "Epoch: [1][31200/36908] Elapsed 398m 38s (remain 72m 54s) Loss: 0.0000(0.0027) Grad: 7.9575  LR: 0.000003  \n",
      "Epoch: [1][31300/36908] Elapsed 399m 53s (remain 71m 38s) Loss: 0.0130(0.0027) Grad: 40318.3477  LR: 0.000003  \n",
      "Epoch: [1][31400/36908] Elapsed 401m 9s (remain 70m 21s) Loss: 0.0000(0.0027) Grad: 2432.4602  LR: 0.000003  \n",
      "Epoch: [1][31500/36908] Elapsed 402m 25s (remain 69m 4s) Loss: 0.0000(0.0027) Grad: 25.0427  LR: 0.000003  \n",
      "Epoch: [1][31600/36908] Elapsed 403m 41s (remain 67m 47s) Loss: 0.0000(0.0027) Grad: 21.3969  LR: 0.000003  \n",
      "Epoch: [1][31700/36908] Elapsed 404m 58s (remain 66m 31s) Loss: 0.0033(0.0027) Grad: 34736.0039  LR: 0.000003  \n",
      "Epoch: [1][31800/36908] Elapsed 406m 14s (remain 65m 14s) Loss: 0.0049(0.0027) Grad: 54858.1211  LR: 0.000003  \n",
      "Epoch: [1][31900/36908] Elapsed 407m 29s (remain 63m 57s) Loss: 0.0011(0.0027) Grad: 2274.6907  LR: 0.000003  \n",
      "Epoch: [1][32000/36908] Elapsed 408m 44s (remain 62m 40s) Loss: 0.0337(0.0027) Grad: 65290.9102  LR: 0.000003  \n",
      "Epoch: [1][32100/36908] Elapsed 410m 0s (remain 61m 23s) Loss: 0.0000(0.0027) Grad: 14.0995  LR: 0.000003  \n",
      "Epoch: [1][32200/36908] Elapsed 411m 15s (remain 60m 7s) Loss: 0.0001(0.0027) Grad: 830.0294  LR: 0.000003  \n",
      "Epoch: [1][32300/36908] Elapsed 412m 29s (remain 58m 49s) Loss: 0.0000(0.0027) Grad: 1.3073  LR: 0.000003  \n",
      "Epoch: [1][32400/36908] Elapsed 413m 45s (remain 57m 33s) Loss: 0.0000(0.0027) Grad: 13.2450  LR: 0.000003  \n",
      "Epoch: [1][32500/36908] Elapsed 415m 1s (remain 56m 16s) Loss: 0.0000(0.0027) Grad: 108.9638  LR: 0.000003  \n",
      "Epoch: [1][32600/36908] Elapsed 416m 17s (remain 54m 59s) Loss: 0.0000(0.0027) Grad: 83.3955  LR: 0.000003  \n",
      "Epoch: [1][32700/36908] Elapsed 417m 33s (remain 53m 43s) Loss: 0.0010(0.0027) Grad: 14069.0938  LR: 0.000003  \n",
      "Epoch: [1][32800/36908] Elapsed 418m 48s (remain 52m 26s) Loss: 0.0061(0.0027) Grad: 33312.5469  LR: 0.000002  \n",
      "Epoch: [1][32900/36908] Elapsed 420m 3s (remain 51m 9s) Loss: 0.0000(0.0027) Grad: 43.8162  LR: 0.000002  \n",
      "Epoch: [1][33000/36908] Elapsed 421m 19s (remain 49m 52s) Loss: 0.0000(0.0027) Grad: 10.8678  LR: 0.000002  \n",
      "Epoch: [1][33100/36908] Elapsed 422m 36s (remain 48m 36s) Loss: 0.0000(0.0027) Grad: 7.9102  LR: 0.000002  \n",
      "Epoch: [1][33200/36908] Elapsed 423m 53s (remain 47m 19s) Loss: 0.0111(0.0027) Grad: 44363.9922  LR: 0.000002  \n",
      "Epoch: [1][33300/36908] Elapsed 425m 10s (remain 46m 3s) Loss: 0.0000(0.0027) Grad: 2.3951  LR: 0.000002  \n",
      "Epoch: [1][33400/36908] Elapsed 426m 28s (remain 44m 46s) Loss: 0.0002(0.0027) Grad: 1081.7386  LR: 0.000002  \n",
      "Epoch: [1][33500/36908] Elapsed 427m 46s (remain 43m 30s) Loss: 0.0031(0.0027) Grad: 68122.1406  LR: 0.000002  \n",
      "Epoch: [1][33600/36908] Elapsed 429m 4s (remain 42m 13s) Loss: 0.0000(0.0027) Grad: 17.0263  LR: 0.000002  \n",
      "Epoch: [1][33700/36908] Elapsed 430m 21s (remain 40m 57s) Loss: 0.0000(0.0027) Grad: 111.2333  LR: 0.000002  \n",
      "Epoch: [1][33800/36908] Elapsed 431m 38s (remain 39m 40s) Loss: 0.0000(0.0027) Grad: 33.0640  LR: 0.000002  \n",
      "Epoch: [1][33900/36908] Elapsed 432m 56s (remain 38m 24s) Loss: 0.0000(0.0027) Grad: 627.2335  LR: 0.000002  \n",
      "Epoch: [1][34000/36908] Elapsed 434m 12s (remain 37m 7s) Loss: 0.0000(0.0027) Grad: 2.7181  LR: 0.000002  \n",
      "Epoch: [1][34100/36908] Elapsed 435m 28s (remain 35m 50s) Loss: 0.0000(0.0027) Grad: 12.6113  LR: 0.000002  \n",
      "Epoch: [1][34200/36908] Elapsed 436m 44s (remain 34m 34s) Loss: 0.0000(0.0027) Grad: 249.2795  LR: 0.000002  \n",
      "Epoch: [1][34300/36908] Elapsed 437m 59s (remain 33m 17s) Loss: 0.0000(0.0027) Grad: 12.9915  LR: 0.000002  \n",
      "Epoch: [1][34400/36908] Elapsed 439m 15s (remain 32m 0s) Loss: 0.0010(0.0027) Grad: 96241.4453  LR: 0.000002  \n",
      "Epoch: [1][34500/36908] Elapsed 440m 31s (remain 30m 44s) Loss: 0.0011(0.0027) Grad: 40966.7344  LR: 0.000001  \n",
      "Epoch: [1][34600/36908] Elapsed 441m 48s (remain 29m 27s) Loss: 0.0001(0.0027) Grad: 2017.7753  LR: 0.000001  \n",
      "Epoch: [1][34700/36908] Elapsed 443m 4s (remain 28m 10s) Loss: 0.0001(0.0027) Grad: 6018.8730  LR: 0.000001  \n",
      "Epoch: [1][34800/36908] Elapsed 444m 20s (remain 26m 54s) Loss: 0.0044(0.0027) Grad: 113942.5859  LR: 0.000001  \n",
      "Epoch: [1][34900/36908] Elapsed 445m 37s (remain 25m 37s) Loss: 0.0000(0.0027) Grad: 6.0066  LR: 0.000001  \n",
      "Epoch: [1][35000/36908] Elapsed 446m 55s (remain 24m 21s) Loss: 0.0012(0.0027) Grad: 29715.7949  LR: 0.000001  \n",
      "Epoch: [1][35100/36908] Elapsed 448m 12s (remain 23m 4s) Loss: 0.0000(0.0027) Grad: 3.6455  LR: 0.000001  \n",
      "Epoch: [1][35200/36908] Elapsed 449m 28s (remain 21m 47s) Loss: 0.0000(0.0027) Grad: 4.3265  LR: 0.000001  \n",
      "Epoch: [1][35300/36908] Elapsed 450m 46s (remain 20m 31s) Loss: 0.0001(0.0027) Grad: 22611.7812  LR: 0.000001  \n",
      "Epoch: [1][35400/36908] Elapsed 452m 3s (remain 19m 14s) Loss: 0.0000(0.0027) Grad: 100.5744  LR: 0.000001  \n",
      "Epoch: [1][35500/36908] Elapsed 453m 20s (remain 17m 58s) Loss: 0.0000(0.0027) Grad: 40.6418  LR: 0.000001  \n",
      "Epoch: [1][35600/36908] Elapsed 454m 37s (remain 16m 41s) Loss: 0.0000(0.0027) Grad: 10.2259  LR: 0.000001  \n",
      "Epoch: [1][35700/36908] Elapsed 455m 54s (remain 15m 24s) Loss: 0.0000(0.0027) Grad: 111.4022  LR: 0.000001  \n",
      "Epoch: [1][35800/36908] Elapsed 457m 12s (remain 14m 8s) Loss: 0.0000(0.0027) Grad: 226.4946  LR: 0.000001  \n",
      "Epoch: [1][35900/36908] Elapsed 458m 29s (remain 12m 51s) Loss: 0.0000(0.0027) Grad: 330.9076  LR: 0.000001  \n",
      "Epoch: [1][36000/36908] Elapsed 459m 45s (remain 11m 34s) Loss: 0.0056(0.0027) Grad: 70923.7578  LR: 0.000001  \n",
      "Epoch: [1][36100/36908] Elapsed 461m 3s (remain 10m 18s) Loss: 0.0000(0.0027) Grad: 39.0361  LR: 0.000000  \n",
      "Epoch: [1][36200/36908] Elapsed 462m 19s (remain 9m 1s) Loss: 0.0029(0.0027) Grad: 12639.9736  LR: 0.000000  \n",
      "Epoch: [1][36300/36908] Elapsed 463m 35s (remain 7m 45s) Loss: 0.0000(0.0027) Grad: 18.9024  LR: 0.000000  \n",
      "Epoch: [1][36400/36908] Elapsed 464m 51s (remain 6m 28s) Loss: 0.0000(0.0027) Grad: 334.2203  LR: 0.000000  \n",
      "Epoch: [1][36500/36908] Elapsed 466m 6s (remain 5m 11s) Loss: 0.0014(0.0027) Grad: 54267.5039  LR: 0.000000  \n",
      "Epoch: [1][36600/36908] Elapsed 467m 23s (remain 3m 55s) Loss: 0.0000(0.0027) Grad: 41.3574  LR: 0.000000  \n",
      "Epoch: [1][36700/36908] Elapsed 468m 38s (remain 2m 38s) Loss: 0.0000(0.0027) Grad: 10.4166  LR: 0.000000  \n",
      "Epoch: [1][36800/36908] Elapsed 469m 54s (remain 1m 21s) Loss: 0.0000(0.0027) Grad: 114.9708  LR: 0.000000  \n",
      "Epoch: [1][36900/36908] Elapsed 471m 10s (remain 0m 5s) Loss: 0.0009(0.0027) Grad: 8825.1006  LR: 0.000000  \n",
      "Epoch: [1][36907/36908] Elapsed 471m 15s (remain 0m 0s) Loss: 0.0028(0.0027) Grad: 38842.5742  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 19m 46s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 30s (remain 5m 32s) Loss: 0.0255(0.0092) \n",
      "EVAL: [200/1192] Elapsed 1m 1s (remain 5m 0s) Loss: 0.0293(0.0102) \n",
      "EVAL: [300/1192] Elapsed 1m 30s (remain 4m 27s) Loss: 0.0024(0.0107) \n",
      "EVAL: [400/1192] Elapsed 2m 1s (remain 3m 58s) Loss: 0.0121(0.0112) \n",
      "EVAL: [500/1192] Elapsed 2m 31s (remain 3m 28s) Loss: 0.0161(0.0106) \n",
      "EVAL: [600/1192] Elapsed 3m 0s (remain 2m 57s) Loss: 0.0000(0.0111) \n",
      "EVAL: [700/1192] Elapsed 3m 30s (remain 2m 27s) Loss: 0.1363(0.0131) \n",
      "EVAL: [800/1192] Elapsed 4m 1s (remain 1m 57s) Loss: 0.0017(0.0134) \n",
      "EVAL: [900/1192] Elapsed 4m 30s (remain 1m 27s) Loss: 0.0012(0.0133) \n",
      "EVAL: [1000/1192] Elapsed 5m 0s (remain 0m 57s) Loss: 0.0000(0.0130) \n",
      "EVAL: [1100/1192] Elapsed 5m 30s (remain 0m 27s) Loss: 0.0000(0.0125) \n",
      "EVAL: [1191/1192] Elapsed 5m 57s (remain 0m 0s) Loss: 0.0000(0.0121) \n",
      "Epoch 1 - avg_train_loss: 0.0027  avg_val_loss: 0.0121  time: 28637s\n",
      "Epoch 1 - Score: 0.8928\n",
      "Epoch 1 - Save Best Score: 0.8928 Model\n",
      "========== fold: 1 training ==========\n",
      "get pseudo plain from ../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_1.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_1.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp083/pseudo_labels_1.npy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687948af59444402a5f94b416dbe5a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288b452d80dc4878989a15465866e29a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(612602, 950)\n",
      "(612602, 6) (612602, 950)\n",
      "(100000, 7)\n",
      "(110725, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n",
      "load weights from ../output/nbme-score-clinical-patient-notes/nbme-exp087/fold1_best.pth\n",
      "Epoch: [1][0/36908] Elapsed 0m 1s (remain 1024m 4s) Loss: 0.0000(0.0000) Grad: 2.4621  LR: 0.000000  \n",
      "Epoch: [1][100/36908] Elapsed 1m 18s (remain 477m 55s) Loss: 0.0017(0.0025) Grad: 11173.5088  LR: 0.000001  \n",
      "Epoch: [1][200/36908] Elapsed 2m 35s (remain 473m 13s) Loss: 0.0000(0.0027) Grad: 50.3054  LR: 0.000001  \n",
      "Epoch: [1][300/36908] Elapsed 3m 53s (remain 472m 26s) Loss: 0.0000(0.0024) Grad: 1216.7847  LR: 0.000002  \n",
      "Epoch: [1][400/36908] Elapsed 5m 10s (remain 471m 11s) Loss: 0.0001(0.0024) Grad: 2226.5032  LR: 0.000002  \n",
      "Epoch: [1][500/36908] Elapsed 6m 28s (remain 470m 16s) Loss: 0.0000(0.0024) Grad: 13.6046  LR: 0.000003  \n",
      "Epoch: [1][600/36908] Elapsed 7m 44s (remain 467m 55s) Loss: 0.0000(0.0027) Grad: 39.5923  LR: 0.000003  \n",
      "Epoch: [1][700/36908] Elapsed 9m 2s (remain 466m 52s) Loss: 0.0000(0.0028) Grad: 50.9001  LR: 0.000004  \n",
      "Epoch: [1][800/36908] Elapsed 10m 19s (remain 465m 7s) Loss: 0.0000(0.0026) Grad: 15.0980  LR: 0.000004  \n",
      "Epoch: [1][900/36908] Elapsed 11m 36s (remain 463m 52s) Loss: 0.0000(0.0025) Grad: 16.6895  LR: 0.000005  \n",
      "Epoch: [1][1000/36908] Elapsed 12m 55s (remain 463m 37s) Loss: 0.0045(0.0024) Grad: 27132.6465  LR: 0.000005  \n",
      "Epoch: [1][1100/36908] Elapsed 14m 13s (remain 462m 44s) Loss: 0.0009(0.0024) Grad: 11546.2988  LR: 0.000006  \n",
      "Epoch: [1][1200/36908] Elapsed 15m 32s (remain 461m 59s) Loss: 0.0000(0.0023) Grad: 52.3899  LR: 0.000007  \n",
      "Epoch: [1][1300/36908] Elapsed 16m 50s (remain 460m 51s) Loss: 0.0099(0.0023) Grad: 31935.7617  LR: 0.000007  \n",
      "Epoch: [1][1400/36908] Elapsed 18m 8s (remain 459m 34s) Loss: 0.0000(0.0022) Grad: 16.1886  LR: 0.000008  \n",
      "Epoch: [1][1500/36908] Elapsed 19m 26s (remain 458m 46s) Loss: 0.0000(0.0022) Grad: 0.8370  LR: 0.000008  \n",
      "Epoch: [1][1600/36908] Elapsed 20m 44s (remain 457m 30s) Loss: 0.0042(0.0022) Grad: 15802.5547  LR: 0.000009  \n",
      "Epoch: [1][1700/36908] Elapsed 22m 2s (remain 456m 11s) Loss: 0.0163(0.0022) Grad: 50502.1719  LR: 0.000009  \n",
      "Epoch: [1][1800/36908] Elapsed 23m 20s (remain 454m 54s) Loss: 0.0000(0.0023) Grad: 12.4112  LR: 0.000010  \n",
      "Epoch: [1][1900/36908] Elapsed 24m 39s (remain 454m 8s) Loss: 0.0008(0.0023) Grad: 16752.3867  LR: 0.000010  \n",
      "Epoch: [1][2000/36908] Elapsed 25m 57s (remain 452m 45s) Loss: 0.0004(0.0022) Grad: 1824.8392  LR: 0.000011  \n",
      "Epoch: [1][2100/36908] Elapsed 27m 15s (remain 451m 30s) Loss: 0.0000(0.0023) Grad: 165.5667  LR: 0.000011  \n",
      "Epoch: [1][2200/36908] Elapsed 28m 34s (remain 450m 29s) Loss: 0.0001(0.0023) Grad: 874.4135  LR: 0.000012  \n",
      "Epoch: [1][2300/36908] Elapsed 29m 52s (remain 449m 11s) Loss: 0.0037(0.0023) Grad: 19259.5430  LR: 0.000012  \n",
      "Epoch: [1][2400/36908] Elapsed 31m 8s (remain 447m 31s) Loss: 0.0006(0.0023) Grad: 2970.4026  LR: 0.000013  \n",
      "Epoch: [1][2500/36908] Elapsed 32m 25s (remain 446m 9s) Loss: 0.0000(0.0023) Grad: 390.4772  LR: 0.000014  \n",
      "Epoch: [1][2600/36908] Elapsed 33m 42s (remain 444m 31s) Loss: 0.0000(0.0023) Grad: 7.7737  LR: 0.000014  \n",
      "Epoch: [1][2700/36908] Elapsed 34m 58s (remain 442m 56s) Loss: 0.0000(0.0024) Grad: 45.2698  LR: 0.000015  \n",
      "Epoch: [1][2800/36908] Elapsed 36m 16s (remain 441m 41s) Loss: 0.0000(0.0024) Grad: 65.8967  LR: 0.000015  \n",
      "Epoch: [1][2900/36908] Elapsed 37m 32s (remain 440m 9s) Loss: 0.0052(0.0024) Grad: 73913.8359  LR: 0.000016  \n",
      "Epoch: [1][3000/36908] Elapsed 38m 50s (remain 438m 52s) Loss: 0.0055(0.0024) Grad: 12268.0244  LR: 0.000016  \n",
      "Epoch: [1][3100/36908] Elapsed 40m 6s (remain 437m 18s) Loss: 0.0001(0.0024) Grad: 525.6838  LR: 0.000017  \n",
      "Epoch: [1][3200/36908] Elapsed 41m 25s (remain 436m 10s) Loss: 0.0001(0.0024) Grad: 253.6739  LR: 0.000017  \n",
      "Epoch: [1][3300/36908] Elapsed 42m 43s (remain 435m 0s) Loss: 0.0001(0.0024) Grad: 225.1723  LR: 0.000018  \n",
      "Epoch: [1][3400/36908] Elapsed 44m 1s (remain 433m 48s) Loss: 0.0012(0.0025) Grad: 1330.1432  LR: 0.000018  \n",
      "Epoch: [1][3500/36908] Elapsed 45m 19s (remain 432m 32s) Loss: 0.0000(0.0025) Grad: 27.8415  LR: 0.000019  \n",
      "Epoch: [1][3600/36908] Elapsed 46m 37s (remain 431m 14s) Loss: 0.0028(0.0025) Grad: 37206.1289  LR: 0.000020  \n",
      "Epoch: [1][3700/36908] Elapsed 47m 54s (remain 429m 54s) Loss: 0.0088(0.0025) Grad: 34740.6289  LR: 0.000020  \n",
      "Epoch: [1][3800/36908] Elapsed 49m 12s (remain 428m 38s) Loss: 0.0000(0.0025) Grad: 57.4396  LR: 0.000020  \n",
      "Epoch: [1][3900/36908] Elapsed 50m 31s (remain 427m 29s) Loss: 0.0000(0.0026) Grad: 10.7580  LR: 0.000020  \n",
      "Epoch: [1][4000/36908] Elapsed 51m 49s (remain 426m 12s) Loss: 0.0002(0.0026) Grad: 4467.7964  LR: 0.000020  \n",
      "Epoch: [1][4100/36908] Elapsed 53m 7s (remain 424m 59s) Loss: 0.0000(0.0026) Grad: 1346.3806  LR: 0.000020  \n",
      "Epoch: [1][4200/36908] Elapsed 54m 25s (remain 423m 45s) Loss: 0.0000(0.0026) Grad: 114.0813  LR: 0.000020  \n",
      "Epoch: [1][4300/36908] Elapsed 55m 44s (remain 422m 32s) Loss: 0.0000(0.0026) Grad: 11.3511  LR: 0.000020  \n",
      "Epoch: [1][4400/36908] Elapsed 57m 1s (remain 421m 11s) Loss: 0.0215(0.0026) Grad: 36836.9570  LR: 0.000020  \n",
      "Epoch: [1][4500/36908] Elapsed 58m 20s (remain 420m 0s) Loss: 0.0000(0.0027) Grad: 363.0126  LR: 0.000020  \n",
      "Epoch: [1][4600/36908] Elapsed 59m 37s (remain 418m 42s) Loss: 0.0000(0.0027) Grad: 5.1362  LR: 0.000019  \n",
      "Epoch: [1][4700/36908] Elapsed 60m 55s (remain 417m 22s) Loss: 0.0037(0.0027) Grad: 12487.5322  LR: 0.000019  \n",
      "Epoch: [1][4800/36908] Elapsed 62m 13s (remain 416m 7s) Loss: 0.0000(0.0027) Grad: 60.0068  LR: 0.000019  \n",
      "Epoch: [1][4900/36908] Elapsed 63m 30s (remain 414m 43s) Loss: 0.0004(0.0028) Grad: 2343.2925  LR: 0.000019  \n",
      "Epoch: [1][5000/36908] Elapsed 64m 47s (remain 413m 21s) Loss: 0.0000(0.0028) Grad: 181.7488  LR: 0.000019  \n",
      "Epoch: [1][5100/36908] Elapsed 66m 4s (remain 412m 1s) Loss: 0.0000(0.0028) Grad: 13.5979  LR: 0.000019  \n",
      "Epoch: [1][5200/36908] Elapsed 67m 21s (remain 410m 37s) Loss: 0.0001(0.0028) Grad: 633.9611  LR: 0.000019  \n",
      "Epoch: [1][5300/36908] Elapsed 68m 39s (remain 409m 23s) Loss: 0.0039(0.0028) Grad: 68117.5156  LR: 0.000019  \n",
      "Epoch: [1][5400/36908] Elapsed 69m 57s (remain 408m 7s) Loss: 0.0000(0.0028) Grad: 35.6927  LR: 0.000019  \n",
      "Epoch: [1][5500/36908] Elapsed 71m 14s (remain 406m 45s) Loss: 0.0000(0.0028) Grad: 72.2985  LR: 0.000019  \n",
      "Epoch: [1][5600/36908] Elapsed 72m 31s (remain 405m 23s) Loss: 0.0001(0.0029) Grad: 873.9202  LR: 0.000019  \n",
      "Epoch: [1][5700/36908] Elapsed 73m 48s (remain 404m 0s) Loss: 0.0000(0.0029) Grad: 78.3473  LR: 0.000019  \n",
      "Epoch: [1][5800/36908] Elapsed 75m 6s (remain 402m 45s) Loss: 0.0000(0.0029) Grad: 18.4621  LR: 0.000019  \n",
      "Epoch: [1][5900/36908] Elapsed 76m 23s (remain 401m 24s) Loss: 0.0005(0.0029) Grad: 2161.6531  LR: 0.000019  \n",
      "Epoch: [1][6000/36908] Elapsed 77m 40s (remain 400m 3s) Loss: 0.0016(0.0029) Grad: 6545.1528  LR: 0.000019  \n",
      "Epoch: [1][6100/36908] Elapsed 78m 58s (remain 398m 48s) Loss: 0.0047(0.0029) Grad: 52556.2188  LR: 0.000019  \n",
      "Epoch: [1][6200/36908] Elapsed 80m 16s (remain 397m 29s) Loss: 0.0000(0.0029) Grad: 6.0184  LR: 0.000018  \n",
      "Epoch: [1][6300/36908] Elapsed 81m 34s (remain 396m 13s) Loss: 0.0682(0.0029) Grad: 245416.2188  LR: 0.000018  \n",
      "Epoch: [1][6400/36908] Elapsed 82m 52s (remain 394m 56s) Loss: 0.0000(0.0029) Grad: 36.2876  LR: 0.000018  \n",
      "Epoch: [1][6500/36908] Elapsed 84m 9s (remain 393m 38s) Loss: 0.0000(0.0029) Grad: 347.6061  LR: 0.000018  \n",
      "Epoch: [1][6600/36908] Elapsed 85m 26s (remain 392m 18s) Loss: 0.0000(0.0029) Grad: 61.5666  LR: 0.000018  \n",
      "Epoch: [1][6700/36908] Elapsed 86m 44s (remain 391m 3s) Loss: 0.0000(0.0029) Grad: 20.0650  LR: 0.000018  \n",
      "Epoch: [1][6800/36908] Elapsed 88m 2s (remain 389m 46s) Loss: 0.0024(0.0029) Grad: 37211.6523  LR: 0.000018  \n",
      "Epoch: [1][6900/36908] Elapsed 89m 22s (remain 388m 37s) Loss: 0.0007(0.0029) Grad: 13464.7178  LR: 0.000018  \n",
      "Epoch: [1][7000/36908] Elapsed 90m 41s (remain 387m 25s) Loss: 0.0000(0.0029) Grad: 16.7978  LR: 0.000018  \n",
      "Epoch: [1][7100/36908] Elapsed 91m 59s (remain 386m 9s) Loss: 0.0000(0.0029) Grad: 991.6329  LR: 0.000018  \n",
      "Epoch: [1][7200/36908] Elapsed 93m 18s (remain 384m 56s) Loss: 0.0001(0.0029) Grad: 934.6191  LR: 0.000018  \n",
      "Epoch: [1][7300/36908] Elapsed 94m 36s (remain 383m 39s) Loss: 0.0000(0.0029) Grad: 99.9140  LR: 0.000018  \n",
      "Epoch: [1][7400/36908] Elapsed 95m 54s (remain 382m 22s) Loss: 0.0000(0.0029) Grad: 11.8103  LR: 0.000018  \n",
      "Epoch: [1][7500/36908] Elapsed 97m 12s (remain 381m 6s) Loss: 0.0038(0.0029) Grad: 11665.8076  LR: 0.000018  \n",
      "Epoch: [1][7600/36908] Elapsed 98m 32s (remain 379m 55s) Loss: 0.0154(0.0029) Grad: 69049.8750  LR: 0.000018  \n",
      "Epoch: [1][7700/36908] Elapsed 99m 50s (remain 378m 38s) Loss: 0.0001(0.0029) Grad: 1748.0770  LR: 0.000018  \n",
      "Epoch: [1][7800/36908] Elapsed 101m 8s (remain 377m 22s) Loss: 0.0061(0.0029) Grad: 68569.0938  LR: 0.000018  \n",
      "Epoch: [1][7900/36908] Elapsed 102m 25s (remain 376m 1s) Loss: 0.0006(0.0029) Grad: 31362.4062  LR: 0.000017  \n",
      "Epoch: [1][8000/36908] Elapsed 103m 42s (remain 374m 39s) Loss: 0.0177(0.0029) Grad: 71620.9766  LR: 0.000017  \n",
      "Epoch: [1][8100/36908] Elapsed 105m 0s (remain 373m 25s) Loss: 0.0000(0.0029) Grad: 387.5426  LR: 0.000017  \n",
      "Epoch: [1][8200/36908] Elapsed 106m 18s (remain 372m 8s) Loss: 0.0009(0.0029) Grad: 47819.0195  LR: 0.000017  \n",
      "Epoch: [1][8300/36908] Elapsed 107m 36s (remain 370m 50s) Loss: 0.0002(0.0029) Grad: 3938.6938  LR: 0.000017  \n",
      "Epoch: [1][8400/36908] Elapsed 108m 53s (remain 369m 30s) Loss: 0.0019(0.0029) Grad: 219421.2500  LR: 0.000017  \n",
      "Epoch: [1][8500/36908] Elapsed 110m 9s (remain 368m 7s) Loss: 0.0114(0.0029) Grad: 314007.1875  LR: 0.000017  \n",
      "Epoch: [1][8600/36908] Elapsed 111m 27s (remain 366m 50s) Loss: 0.0001(0.0029) Grad: 2930.3223  LR: 0.000017  \n",
      "Epoch: [1][8700/36908] Elapsed 112m 45s (remain 365m 33s) Loss: 0.0000(0.0029) Grad: 44.7359  LR: 0.000017  \n",
      "Epoch: [1][8800/36908] Elapsed 114m 4s (remain 364m 17s) Loss: 0.0000(0.0029) Grad: 18.5644  LR: 0.000017  \n",
      "Epoch: [1][8900/36908] Elapsed 115m 21s (remain 362m 57s) Loss: 0.0002(0.0029) Grad: 9303.3223  LR: 0.000017  \n",
      "Epoch: [1][9000/36908] Elapsed 116m 37s (remain 361m 36s) Loss: 0.0000(0.0029) Grad: 27.9100  LR: 0.000017  \n",
      "Epoch: [1][9100/36908] Elapsed 117m 55s (remain 360m 18s) Loss: 0.0000(0.0029) Grad: 12.3826  LR: 0.000017  \n",
      "Epoch: [1][9200/36908] Elapsed 119m 14s (remain 359m 2s) Loss: 0.0000(0.0029) Grad: 101.0263  LR: 0.000017  \n",
      "Epoch: [1][9300/36908] Elapsed 120m 32s (remain 357m 47s) Loss: 0.0029(0.0029) Grad: 64016.1367  LR: 0.000017  \n",
      "Epoch: [1][9400/36908] Elapsed 121m 51s (remain 356m 33s) Loss: 0.0025(0.0029) Grad: 5062.3970  LR: 0.000017  \n",
      "Epoch: [1][9500/36908] Elapsed 123m 8s (remain 355m 12s) Loss: 0.0000(0.0029) Grad: 45.5895  LR: 0.000017  \n",
      "Epoch: [1][9600/36908] Elapsed 124m 26s (remain 353m 55s) Loss: 0.0019(0.0029) Grad: 10730.5137  LR: 0.000016  \n",
      "Epoch: [1][9700/36908] Elapsed 125m 43s (remain 352m 35s) Loss: 0.0000(0.0029) Grad: 199.0329  LR: 0.000016  \n",
      "Epoch: [1][9800/36908] Elapsed 127m 1s (remain 351m 17s) Loss: 0.0000(0.0029) Grad: 8.4862  LR: 0.000016  \n",
      "Epoch: [1][9900/36908] Elapsed 128m 18s (remain 349m 59s) Loss: 0.0002(0.0029) Grad: 2887.7805  LR: 0.000016  \n",
      "Epoch: [1][10000/36908] Elapsed 129m 34s (remain 348m 37s) Loss: 0.0000(0.0029) Grad: 4.5109  LR: 0.000016  \n",
      "Epoch: [1][10100/36908] Elapsed 130m 52s (remain 347m 18s) Loss: 0.0000(0.0029) Grad: 14.0822  LR: 0.000016  \n",
      "Epoch: [1][10200/36908] Elapsed 132m 9s (remain 345m 59s) Loss: 0.0009(0.0029) Grad: 2445.6633  LR: 0.000016  \n",
      "Epoch: [1][10300/36908] Elapsed 133m 27s (remain 344m 42s) Loss: 0.0009(0.0029) Grad: 1209.0204  LR: 0.000016  \n",
      "Epoch: [1][10400/36908] Elapsed 134m 45s (remain 343m 25s) Loss: 0.0000(0.0029) Grad: 84.6020  LR: 0.000016  \n",
      "Epoch: [1][10500/36908] Elapsed 136m 1s (remain 342m 4s) Loss: 0.0000(0.0029) Grad: 72.2886  LR: 0.000016  \n",
      "Epoch: [1][10600/36908] Elapsed 137m 19s (remain 340m 45s) Loss: 0.0000(0.0029) Grad: 130.9942  LR: 0.000016  \n",
      "Epoch: [1][10700/36908] Elapsed 138m 35s (remain 339m 25s) Loss: 0.0000(0.0029) Grad: 13.3163  LR: 0.000016  \n",
      "Epoch: [1][10800/36908] Elapsed 139m 53s (remain 338m 7s) Loss: 0.0002(0.0029) Grad: 744.9444  LR: 0.000016  \n",
      "Epoch: [1][10900/36908] Elapsed 141m 10s (remain 336m 47s) Loss: 0.0000(0.0029) Grad: 25.9117  LR: 0.000016  \n",
      "Epoch: [1][11000/36908] Elapsed 142m 27s (remain 335m 29s) Loss: 0.0008(0.0029) Grad: 6616.5200  LR: 0.000016  \n",
      "Epoch: [1][11100/36908] Elapsed 143m 45s (remain 334m 12s) Loss: 0.0026(0.0029) Grad: 3899.7112  LR: 0.000016  \n",
      "Epoch: [1][11200/36908] Elapsed 145m 2s (remain 332m 53s) Loss: 0.0002(0.0029) Grad: 2283.2488  LR: 0.000015  \n",
      "Epoch: [1][11300/36908] Elapsed 146m 21s (remain 331m 38s) Loss: 0.0048(0.0029) Grad: 28299.0938  LR: 0.000015  \n",
      "Epoch: [1][11400/36908] Elapsed 147m 40s (remain 330m 22s) Loss: 0.0001(0.0029) Grad: 239.1322  LR: 0.000015  \n",
      "Epoch: [1][11500/36908] Elapsed 148m 59s (remain 329m 7s) Loss: 0.0030(0.0029) Grad: 11533.4971  LR: 0.000015  \n",
      "Epoch: [1][11600/36908] Elapsed 150m 17s (remain 327m 50s) Loss: 0.0007(0.0029) Grad: 7483.5659  LR: 0.000015  \n",
      "Epoch: [1][11700/36908] Elapsed 151m 35s (remain 326m 34s) Loss: 0.0002(0.0029) Grad: 643.2494  LR: 0.000015  \n",
      "Epoch: [1][11800/36908] Elapsed 152m 53s (remain 325m 17s) Loss: 0.0000(0.0029) Grad: 91.3514  LR: 0.000015  \n",
      "Epoch: [1][11900/36908] Elapsed 154m 12s (remain 324m 1s) Loss: 0.0079(0.0029) Grad: 47705.5117  LR: 0.000015  \n",
      "Epoch: [1][12000/36908] Elapsed 155m 29s (remain 322m 43s) Loss: 0.0004(0.0029) Grad: 13207.8584  LR: 0.000015  \n",
      "Epoch: [1][12100/36908] Elapsed 156m 49s (remain 321m 28s) Loss: 0.0000(0.0029) Grad: 18.1248  LR: 0.000015  \n",
      "Epoch: [1][12200/36908] Elapsed 158m 6s (remain 320m 9s) Loss: 0.0010(0.0029) Grad: 40062.2188  LR: 0.000015  \n",
      "Epoch: [1][12300/36908] Elapsed 159m 23s (remain 318m 51s) Loss: 0.0000(0.0029) Grad: 22.6314  LR: 0.000015  \n",
      "Epoch: [1][12400/36908] Elapsed 160m 40s (remain 317m 31s) Loss: 0.0025(0.0029) Grad: 19686.1348  LR: 0.000015  \n",
      "Epoch: [1][12500/36908] Elapsed 161m 56s (remain 316m 11s) Loss: 0.0087(0.0028) Grad: 14406.3955  LR: 0.000015  \n",
      "Epoch: [1][12600/36908] Elapsed 163m 14s (remain 314m 53s) Loss: 0.0017(0.0028) Grad: 7024.9414  LR: 0.000015  \n",
      "Epoch: [1][12700/36908] Elapsed 164m 31s (remain 313m 35s) Loss: 0.0000(0.0028) Grad: 21.5336  LR: 0.000015  \n",
      "Epoch: [1][12800/36908] Elapsed 165m 48s (remain 312m 16s) Loss: 0.0009(0.0028) Grad: 1819.6257  LR: 0.000015  \n",
      "Epoch: [1][12900/36908] Elapsed 167m 6s (remain 310m 57s) Loss: 0.0002(0.0028) Grad: 891.6588  LR: 0.000014  \n",
      "Epoch: [1][13000/36908] Elapsed 168m 22s (remain 309m 37s) Loss: 0.0150(0.0028) Grad: 27937.6133  LR: 0.000014  \n",
      "Epoch: [1][13100/36908] Elapsed 169m 40s (remain 308m 20s) Loss: 0.0002(0.0028) Grad: 515.8860  LR: 0.000014  \n",
      "Epoch: [1][13200/36908] Elapsed 170m 58s (remain 307m 3s) Loss: 0.0092(0.0028) Grad: 14523.8242  LR: 0.000014  \n",
      "Epoch: [1][13300/36908] Elapsed 172m 16s (remain 305m 45s) Loss: 0.0000(0.0028) Grad: 489.3582  LR: 0.000014  \n",
      "Epoch: [1][13400/36908] Elapsed 173m 35s (remain 304m 30s) Loss: 0.0002(0.0028) Grad: 634.5906  LR: 0.000014  \n",
      "Epoch: [1][13500/36908] Elapsed 174m 53s (remain 303m 13s) Loss: 0.0000(0.0028) Grad: 6.0911  LR: 0.000014  \n",
      "Epoch: [1][13600/36908] Elapsed 176m 12s (remain 301m 57s) Loss: 0.0022(0.0028) Grad: 10870.5947  LR: 0.000014  \n",
      "Epoch: [1][13700/36908] Elapsed 177m 29s (remain 300m 38s) Loss: 0.0003(0.0028) Grad: 896.8281  LR: 0.000014  \n",
      "Epoch: [1][13800/36908] Elapsed 178m 47s (remain 299m 21s) Loss: 0.0000(0.0028) Grad: 17.0329  LR: 0.000014  \n",
      "Epoch: [1][13900/36908] Elapsed 180m 4s (remain 298m 1s) Loss: 0.0000(0.0028) Grad: 44.5979  LR: 0.000014  \n",
      "Epoch: [1][14000/36908] Elapsed 181m 22s (remain 296m 44s) Loss: 0.0000(0.0028) Grad: 48.4802  LR: 0.000014  \n",
      "Epoch: [1][14100/36908] Elapsed 182m 39s (remain 295m 25s) Loss: 0.0000(0.0028) Grad: 29.5687  LR: 0.000014  \n",
      "Epoch: [1][14200/36908] Elapsed 183m 56s (remain 294m 7s) Loss: 0.0008(0.0028) Grad: 9363.9824  LR: 0.000014  \n",
      "Epoch: [1][14300/36908] Elapsed 185m 14s (remain 292m 49s) Loss: 0.0045(0.0028) Grad: 23193.3652  LR: 0.000014  \n",
      "Epoch: [1][14400/36908] Elapsed 186m 31s (remain 291m 30s) Loss: 0.0025(0.0028) Grad: 221310.4062  LR: 0.000014  \n",
      "Epoch: [1][14500/36908] Elapsed 187m 49s (remain 290m 14s) Loss: 0.0161(0.0028) Grad: 47942.3984  LR: 0.000013  \n",
      "Epoch: [1][14600/36908] Elapsed 189m 6s (remain 288m 54s) Loss: 0.0000(0.0028) Grad: 13.2326  LR: 0.000013  \n",
      "Epoch: [1][14700/36908] Elapsed 190m 22s (remain 287m 34s) Loss: 0.0069(0.0028) Grad: 40499.3164  LR: 0.000013  \n",
      "Epoch: [1][14800/36908] Elapsed 191m 40s (remain 286m 16s) Loss: 0.0017(0.0028) Grad: 12628.2910  LR: 0.000013  \n",
      "Epoch: [1][14900/36908] Elapsed 192m 57s (remain 284m 59s) Loss: 0.0000(0.0028) Grad: 88.9512  LR: 0.000013  \n",
      "Epoch: [1][15000/36908] Elapsed 194m 15s (remain 283m 42s) Loss: 0.0000(0.0028) Grad: 328.3046  LR: 0.000013  \n",
      "Epoch: [1][15100/36908] Elapsed 195m 32s (remain 282m 22s) Loss: 0.0000(0.0028) Grad: 17.2037  LR: 0.000013  \n",
      "Epoch: [1][15200/36908] Elapsed 196m 48s (remain 281m 2s) Loss: 0.0039(0.0028) Grad: 36835.6992  LR: 0.000013  \n",
      "Epoch: [1][15300/36908] Elapsed 198m 7s (remain 279m 47s) Loss: 0.0009(0.0028) Grad: 13769.9414  LR: 0.000013  \n",
      "Epoch: [1][15400/36908] Elapsed 199m 26s (remain 278m 31s) Loss: 0.0000(0.0028) Grad: 23.9953  LR: 0.000013  \n",
      "Epoch: [1][15500/36908] Elapsed 200m 45s (remain 277m 14s) Loss: 0.0000(0.0028) Grad: 72.9362  LR: 0.000013  \n",
      "Epoch: [1][15600/36908] Elapsed 202m 3s (remain 275m 57s) Loss: 0.0000(0.0028) Grad: 351.1947  LR: 0.000013  \n",
      "Epoch: [1][15700/36908] Elapsed 203m 21s (remain 274m 40s) Loss: 0.0000(0.0028) Grad: 3.7477  LR: 0.000013  \n",
      "Epoch: [1][15800/36908] Elapsed 204m 40s (remain 273m 23s) Loss: 0.0005(0.0028) Grad: 12905.9971  LR: 0.000013  \n",
      "Epoch: [1][15900/36908] Elapsed 205m 57s (remain 272m 6s) Loss: 0.0058(0.0028) Grad: 11875.6865  LR: 0.000013  \n",
      "Epoch: [1][16000/36908] Elapsed 207m 15s (remain 270m 48s) Loss: 0.0000(0.0028) Grad: 23.6393  LR: 0.000013  \n",
      "Epoch: [1][16100/36908] Elapsed 208m 33s (remain 269m 30s) Loss: 0.0000(0.0028) Grad: 34.5233  LR: 0.000013  \n",
      "Epoch: [1][16200/36908] Elapsed 209m 51s (remain 268m 13s) Loss: 0.0000(0.0028) Grad: 300.9744  LR: 0.000012  \n",
      "Epoch: [1][16300/36908] Elapsed 211m 9s (remain 266m 56s) Loss: 0.0000(0.0028) Grad: 839.1353  LR: 0.000012  \n",
      "Epoch: [1][16400/36908] Elapsed 212m 27s (remain 265m 38s) Loss: 0.0000(0.0028) Grad: 702.9022  LR: 0.000012  \n",
      "Epoch: [1][16500/36908] Elapsed 213m 45s (remain 264m 21s) Loss: 0.0039(0.0028) Grad: 30939.1660  LR: 0.000012  \n",
      "Epoch: [1][16600/36908] Elapsed 215m 2s (remain 263m 2s) Loss: 0.0000(0.0028) Grad: 208.6814  LR: 0.000012  \n",
      "Epoch: [1][16700/36908] Elapsed 216m 19s (remain 261m 44s) Loss: 0.0095(0.0028) Grad: 26014.8262  LR: 0.000012  \n",
      "Epoch: [1][16800/36908] Elapsed 217m 37s (remain 260m 26s) Loss: 0.0038(0.0028) Grad: 138449.0156  LR: 0.000012  \n",
      "Epoch: [1][16900/36908] Elapsed 218m 54s (remain 259m 7s) Loss: 0.0000(0.0028) Grad: 35.3109  LR: 0.000012  \n",
      "Epoch: [1][17000/36908] Elapsed 220m 11s (remain 257m 49s) Loss: 0.0282(0.0028) Grad: 47659.6055  LR: 0.000012  \n",
      "Epoch: [1][17100/36908] Elapsed 221m 29s (remain 256m 32s) Loss: 0.0077(0.0028) Grad: 190926.5781  LR: 0.000012  \n",
      "Epoch: [1][17200/36908] Elapsed 222m 47s (remain 255m 14s) Loss: 0.0049(0.0028) Grad: 116346.8594  LR: 0.000012  \n",
      "Epoch: [1][17300/36908] Elapsed 224m 3s (remain 253m 55s) Loss: 0.0000(0.0028) Grad: 31.1662  LR: 0.000012  \n",
      "Epoch: [1][17400/36908] Elapsed 225m 20s (remain 252m 37s) Loss: 0.0179(0.0028) Grad: 228786.9531  LR: 0.000012  \n",
      "Epoch: [1][17500/36908] Elapsed 226m 37s (remain 251m 18s) Loss: 0.0129(0.0028) Grad: 66626.5859  LR: 0.000012  \n",
      "Epoch: [1][17600/36908] Elapsed 227m 56s (remain 250m 2s) Loss: 0.0022(0.0028) Grad: 19157.6777  LR: 0.000012  \n",
      "Epoch: [1][17700/36908] Elapsed 229m 15s (remain 248m 45s) Loss: 0.0009(0.0028) Grad: 9870.2832  LR: 0.000012  \n",
      "Epoch: [1][17800/36908] Elapsed 230m 33s (remain 247m 28s) Loss: 0.0000(0.0028) Grad: 19.7636  LR: 0.000012  \n",
      "Epoch: [1][17900/36908] Elapsed 231m 52s (remain 246m 11s) Loss: 0.0000(0.0028) Grad: 10.5704  LR: 0.000011  \n",
      "Epoch: [1][18000/36908] Elapsed 233m 10s (remain 244m 54s) Loss: 0.0025(0.0028) Grad: 18573.4512  LR: 0.000011  \n",
      "Epoch: [1][18100/36908] Elapsed 234m 29s (remain 243m 38s) Loss: 0.0015(0.0028) Grad: 56665.8672  LR: 0.000011  \n",
      "Epoch: [1][18200/36908] Elapsed 235m 47s (remain 242m 21s) Loss: 0.0000(0.0028) Grad: 26.6078  LR: 0.000011  \n",
      "Epoch: [1][18300/36908] Elapsed 237m 4s (remain 241m 2s) Loss: 0.0000(0.0028) Grad: 2.4280  LR: 0.000011  \n",
      "Epoch: [1][18400/36908] Elapsed 238m 23s (remain 239m 45s) Loss: 0.0010(0.0028) Grad: 11204.4844  LR: 0.000011  \n",
      "Epoch: [1][18500/36908] Elapsed 239m 41s (remain 238m 28s) Loss: 0.0000(0.0028) Grad: 129.3021  LR: 0.000011  \n",
      "Epoch: [1][18600/36908] Elapsed 241m 0s (remain 237m 11s) Loss: 0.0003(0.0028) Grad: 6883.7725  LR: 0.000011  \n",
      "Epoch: [1][18700/36908] Elapsed 242m 18s (remain 235m 54s) Loss: 0.0000(0.0028) Grad: 6.5211  LR: 0.000011  \n",
      "Epoch: [1][18800/36908] Elapsed 243m 36s (remain 234m 36s) Loss: 0.0000(0.0028) Grad: 288.7404  LR: 0.000011  \n",
      "Epoch: [1][18900/36908] Elapsed 244m 53s (remain 233m 18s) Loss: 0.0002(0.0028) Grad: 14030.9453  LR: 0.000011  \n",
      "Epoch: [1][19000/36908] Elapsed 246m 10s (remain 231m 59s) Loss: 0.0002(0.0028) Grad: 4544.4780  LR: 0.000011  \n",
      "Epoch: [1][19100/36908] Elapsed 247m 28s (remain 230m 42s) Loss: 0.0049(0.0028) Grad: 49426.3945  LR: 0.000011  \n",
      "Epoch: [1][19200/36908] Elapsed 248m 47s (remain 229m 25s) Loss: 0.0008(0.0028) Grad: 47543.2773  LR: 0.000011  \n",
      "Epoch: [1][19300/36908] Elapsed 250m 5s (remain 228m 8s) Loss: 0.0000(0.0028) Grad: 23.9657  LR: 0.000011  \n",
      "Epoch: [1][19400/36908] Elapsed 251m 24s (remain 226m 51s) Loss: 0.0074(0.0028) Grad: 188770.8125  LR: 0.000011  \n",
      "Epoch: [1][19500/36908] Elapsed 252m 42s (remain 225m 34s) Loss: 0.0000(0.0028) Grad: 10.8130  LR: 0.000010  \n",
      "Epoch: [1][19600/36908] Elapsed 254m 0s (remain 224m 16s) Loss: 0.0000(0.0028) Grad: 37.7798  LR: 0.000010  \n",
      "Epoch: [1][19700/36908] Elapsed 255m 18s (remain 222m 59s) Loss: 0.0062(0.0028) Grad: 276082.3125  LR: 0.000010  \n",
      "Epoch: [1][19800/36908] Elapsed 256m 36s (remain 221m 41s) Loss: 0.0253(0.0028) Grad: 290941.0625  LR: 0.000010  \n",
      "Epoch: [1][19900/36908] Elapsed 257m 54s (remain 220m 23s) Loss: 0.0000(0.0028) Grad: 1.6815  LR: 0.000010  \n",
      "Epoch: [1][20000/36908] Elapsed 259m 12s (remain 219m 6s) Loss: 0.0001(0.0028) Grad: 2799.2615  LR: 0.000010  \n",
      "Epoch: [1][20100/36908] Elapsed 260m 30s (remain 217m 49s) Loss: 0.0004(0.0028) Grad: 7496.6748  LR: 0.000010  \n",
      "Epoch: [1][20200/36908] Elapsed 261m 49s (remain 216m 32s) Loss: 0.0030(0.0028) Grad: 86015.4219  LR: 0.000010  \n",
      "Epoch: [1][20300/36908] Elapsed 263m 7s (remain 215m 14s) Loss: 0.0001(0.0028) Grad: 2207.0295  LR: 0.000010  \n",
      "Epoch: [1][20400/36908] Elapsed 264m 25s (remain 213m 56s) Loss: 0.0000(0.0028) Grad: 56.4970  LR: 0.000010  \n",
      "Epoch: [1][20500/36908] Elapsed 265m 43s (remain 212m 39s) Loss: 0.0000(0.0028) Grad: 77.2846  LR: 0.000010  \n",
      "Epoch: [1][20600/36908] Elapsed 267m 0s (remain 211m 21s) Loss: 0.0000(0.0028) Grad: 4957.5908  LR: 0.000010  \n",
      "Epoch: [1][20700/36908] Elapsed 268m 18s (remain 210m 3s) Loss: 0.0000(0.0028) Grad: 579.7532  LR: 0.000010  \n",
      "Epoch: [1][20800/36908] Elapsed 269m 35s (remain 208m 45s) Loss: 0.0030(0.0028) Grad: 32863.7734  LR: 0.000010  \n",
      "Epoch: [1][20900/36908] Elapsed 270m 52s (remain 207m 27s) Loss: 0.0026(0.0028) Grad: 123641.2969  LR: 0.000010  \n",
      "Epoch: [1][21000/36908] Elapsed 272m 9s (remain 206m 8s) Loss: 0.0000(0.0028) Grad: 490.6739  LR: 0.000010  \n",
      "Epoch: [1][21100/36908] Elapsed 273m 26s (remain 204m 50s) Loss: 0.0160(0.0028) Grad: 63706.3438  LR: 0.000010  \n",
      "Epoch: [1][21200/36908] Elapsed 274m 44s (remain 203m 32s) Loss: 0.0000(0.0028) Grad: 33.0956  LR: 0.000009  \n",
      "Epoch: [1][21300/36908] Elapsed 276m 0s (remain 202m 14s) Loss: 0.0000(0.0028) Grad: 40.0415  LR: 0.000009  \n",
      "Epoch: [1][21400/36908] Elapsed 277m 18s (remain 200m 56s) Loss: 0.0025(0.0028) Grad: 71846.5625  LR: 0.000009  \n",
      "Epoch: [1][21500/36908] Elapsed 278m 35s (remain 199m 37s) Loss: 0.0000(0.0028) Grad: 51.7126  LR: 0.000009  \n",
      "Epoch: [1][21600/36908] Elapsed 279m 53s (remain 198m 20s) Loss: 0.0006(0.0027) Grad: 18639.0859  LR: 0.000009  \n",
      "Epoch: [1][21700/36908] Elapsed 281m 9s (remain 197m 1s) Loss: 0.0000(0.0027) Grad: 13.3903  LR: 0.000009  \n",
      "Epoch: [1][21800/36908] Elapsed 282m 27s (remain 195m 43s) Loss: 0.0000(0.0027) Grad: 41.5221  LR: 0.000009  \n",
      "Epoch: [1][21900/36908] Elapsed 283m 43s (remain 194m 24s) Loss: 0.0000(0.0027) Grad: 131.8029  LR: 0.000009  \n",
      "Epoch: [1][22000/36908] Elapsed 285m 1s (remain 193m 7s) Loss: 0.0111(0.0027) Grad: 332659.4375  LR: 0.000009  \n",
      "Epoch: [1][22100/36908] Elapsed 286m 18s (remain 191m 48s) Loss: 0.0076(0.0027) Grad: 288032.7500  LR: 0.000009  \n",
      "Epoch: [1][22200/36908] Elapsed 287m 36s (remain 190m 31s) Loss: 0.0000(0.0027) Grad: 59.1346  LR: 0.000009  \n",
      "Epoch: [1][22300/36908] Elapsed 288m 54s (remain 189m 13s) Loss: 0.0006(0.0027) Grad: 21108.1211  LR: 0.000009  \n",
      "Epoch: [1][22400/36908] Elapsed 290m 12s (remain 187m 56s) Loss: 0.0000(0.0027) Grad: 425.9063  LR: 0.000009  \n",
      "Epoch: [1][22500/36908] Elapsed 291m 30s (remain 186m 38s) Loss: 0.0001(0.0027) Grad: 3777.3796  LR: 0.000009  \n",
      "Epoch: [1][22600/36908] Elapsed 292m 47s (remain 185m 20s) Loss: 0.0000(0.0027) Grad: 12.9396  LR: 0.000009  \n",
      "Epoch: [1][22700/36908] Elapsed 294m 5s (remain 184m 3s) Loss: 0.0000(0.0027) Grad: 32.7793  LR: 0.000009  \n",
      "Epoch: [1][22800/36908] Elapsed 295m 22s (remain 182m 44s) Loss: 0.0006(0.0027) Grad: 3592.5244  LR: 0.000008  \n",
      "Epoch: [1][22900/36908] Elapsed 296m 40s (remain 181m 27s) Loss: 0.0000(0.0027) Grad: 4.7164  LR: 0.000008  \n",
      "Epoch: [1][23000/36908] Elapsed 297m 56s (remain 180m 8s) Loss: 0.0000(0.0027) Grad: 7.7985  LR: 0.000008  \n",
      "Epoch: [1][23100/36908] Elapsed 299m 14s (remain 178m 50s) Loss: 0.0104(0.0027) Grad: 7598.2080  LR: 0.000008  \n",
      "Epoch: [1][23200/36908] Elapsed 300m 31s (remain 177m 32s) Loss: 0.0000(0.0027) Grad: 43.3988  LR: 0.000008  \n",
      "Epoch: [1][23300/36908] Elapsed 301m 48s (remain 176m 14s) Loss: 0.0114(0.0027) Grad: 99407.4844  LR: 0.000008  \n",
      "Epoch: [1][23400/36908] Elapsed 303m 5s (remain 174m 56s) Loss: 0.0008(0.0027) Grad: 97031.9375  LR: 0.000008  \n",
      "Epoch: [1][23500/36908] Elapsed 304m 22s (remain 173m 38s) Loss: 0.0065(0.0027) Grad: 42305.1211  LR: 0.000008  \n",
      "Epoch: [1][23600/36908] Elapsed 305m 40s (remain 172m 20s) Loss: 0.0007(0.0027) Grad: 8678.5957  LR: 0.000008  \n",
      "Epoch: [1][23700/36908] Elapsed 306m 57s (remain 171m 2s) Loss: 0.0004(0.0027) Grad: 10060.1992  LR: 0.000008  \n",
      "Epoch: [1][23800/36908] Elapsed 308m 14s (remain 169m 44s) Loss: 0.0000(0.0027) Grad: 69.2264  LR: 0.000008  \n",
      "Epoch: [1][23900/36908] Elapsed 309m 31s (remain 168m 26s) Loss: 0.0000(0.0027) Grad: 35.9656  LR: 0.000008  \n",
      "Epoch: [1][24000/36908] Elapsed 310m 48s (remain 167m 8s) Loss: 0.0000(0.0027) Grad: 3.5135  LR: 0.000008  \n",
      "Epoch: [1][24100/36908] Elapsed 312m 6s (remain 165m 50s) Loss: 0.0000(0.0027) Grad: 30.9422  LR: 0.000008  \n",
      "Epoch: [1][24200/36908] Elapsed 313m 23s (remain 164m 33s) Loss: 0.0072(0.0027) Grad: 416929.4375  LR: 0.000008  \n",
      "Epoch: [1][24300/36908] Elapsed 314m 42s (remain 163m 15s) Loss: 0.0000(0.0027) Grad: 1311.4285  LR: 0.000008  \n",
      "Epoch: [1][24400/36908] Elapsed 316m 0s (remain 161m 58s) Loss: 0.0000(0.0027) Grad: 515.3672  LR: 0.000008  \n",
      "Epoch: [1][24500/36908] Elapsed 317m 17s (remain 160m 40s) Loss: 0.0000(0.0027) Grad: 67.3183  LR: 0.000007  \n",
      "Epoch: [1][24600/36908] Elapsed 318m 34s (remain 159m 22s) Loss: 0.0101(0.0027) Grad: 48360.9844  LR: 0.000007  \n",
      "Epoch: [1][24700/36908] Elapsed 319m 52s (remain 158m 4s) Loss: 0.0004(0.0027) Grad: 7447.7363  LR: 0.000007  \n",
      "Epoch: [1][24800/36908] Elapsed 321m 9s (remain 156m 46s) Loss: 0.0000(0.0027) Grad: 39.2465  LR: 0.000007  \n",
      "Epoch: [1][24900/36908] Elapsed 322m 26s (remain 155m 28s) Loss: 0.0000(0.0027) Grad: 5.7647  LR: 0.000007  \n",
      "Epoch: [1][25000/36908] Elapsed 323m 45s (remain 154m 11s) Loss: 0.0000(0.0027) Grad: 43.7130  LR: 0.000007  \n",
      "Epoch: [1][25100/36908] Elapsed 325m 2s (remain 152m 53s) Loss: 0.0010(0.0027) Grad: 13795.0742  LR: 0.000007  \n",
      "Epoch: [1][25200/36908] Elapsed 326m 19s (remain 151m 35s) Loss: 0.0000(0.0027) Grad: 5.9829  LR: 0.000007  \n",
      "Epoch: [1][25300/36908] Elapsed 327m 37s (remain 150m 18s) Loss: 0.0000(0.0027) Grad: 907.4979  LR: 0.000007  \n",
      "Epoch: [1][25400/36908] Elapsed 328m 54s (remain 148m 59s) Loss: 0.0010(0.0027) Grad: 19947.0527  LR: 0.000007  \n",
      "Epoch: [1][25500/36908] Elapsed 330m 11s (remain 147m 41s) Loss: 0.0000(0.0027) Grad: 4.6409  LR: 0.000007  \n",
      "Epoch: [1][25600/36908] Elapsed 331m 27s (remain 146m 23s) Loss: 0.0000(0.0027) Grad: 10.0088  LR: 0.000007  \n",
      "Epoch: [1][25700/36908] Elapsed 332m 44s (remain 145m 5s) Loss: 0.0000(0.0027) Grad: 97.8781  LR: 0.000007  \n",
      "Epoch: [1][25800/36908] Elapsed 334m 1s (remain 143m 47s) Loss: 0.0168(0.0027) Grad: 48719.4023  LR: 0.000007  \n",
      "Epoch: [1][25900/36908] Elapsed 335m 17s (remain 142m 29s) Loss: 0.0001(0.0027) Grad: 1904.7174  LR: 0.000007  \n",
      "Epoch: [1][26000/36908] Elapsed 336m 34s (remain 141m 11s) Loss: 0.0000(0.0027) Grad: 23.9387  LR: 0.000007  \n",
      "Epoch: [1][26100/36908] Elapsed 337m 51s (remain 139m 53s) Loss: 0.0032(0.0027) Grad: 30846.5039  LR: 0.000007  \n",
      "Epoch: [1][26200/36908] Elapsed 339m 8s (remain 138m 35s) Loss: 0.0000(0.0027) Grad: 39.1393  LR: 0.000006  \n",
      "Epoch: [1][26300/36908] Elapsed 340m 25s (remain 137m 17s) Loss: 0.0018(0.0027) Grad: 12812.5176  LR: 0.000006  \n",
      "Epoch: [1][26400/36908] Elapsed 341m 43s (remain 136m 0s) Loss: 0.0000(0.0027) Grad: 119.6895  LR: 0.000006  \n",
      "Epoch: [1][26500/36908] Elapsed 343m 0s (remain 134m 42s) Loss: 0.0398(0.0027) Grad: 69543.6094  LR: 0.000006  \n",
      "Epoch: [1][26600/36908] Elapsed 344m 17s (remain 133m 24s) Loss: 0.0000(0.0027) Grad: 190.0015  LR: 0.000006  \n",
      "Epoch: [1][26700/36908] Elapsed 345m 34s (remain 132m 6s) Loss: 0.0000(0.0027) Grad: 34.9390  LR: 0.000006  \n",
      "Epoch: [1][26800/36908] Elapsed 346m 54s (remain 130m 49s) Loss: 0.0000(0.0027) Grad: 33.3042  LR: 0.000006  \n",
      "Epoch: [1][26900/36908] Elapsed 348m 12s (remain 129m 31s) Loss: 0.0000(0.0027) Grad: 19.3629  LR: 0.000006  \n",
      "Epoch: [1][27000/36908] Elapsed 349m 31s (remain 128m 14s) Loss: 0.0000(0.0027) Grad: 13.1572  LR: 0.000006  \n",
      "Epoch: [1][27100/36908] Elapsed 350m 49s (remain 126m 57s) Loss: 0.0000(0.0027) Grad: 14.3668  LR: 0.000006  \n",
      "Epoch: [1][27200/36908] Elapsed 352m 8s (remain 125m 39s) Loss: 0.0001(0.0027) Grad: 2489.7380  LR: 0.000006  \n",
      "Epoch: [1][27300/36908] Elapsed 353m 27s (remain 124m 22s) Loss: 0.0000(0.0027) Grad: 15.1514  LR: 0.000006  \n",
      "Epoch: [1][27400/36908] Elapsed 354m 44s (remain 123m 4s) Loss: 0.0000(0.0027) Grad: 111.6412  LR: 0.000006  \n",
      "Epoch: [1][27500/36908] Elapsed 356m 2s (remain 121m 47s) Loss: 0.0069(0.0027) Grad: 35129.6172  LR: 0.000006  \n",
      "Epoch: [1][27600/36908] Elapsed 357m 18s (remain 120m 28s) Loss: 0.0000(0.0027) Grad: 503.6436  LR: 0.000006  \n",
      "Epoch: [1][27700/36908] Elapsed 358m 35s (remain 119m 11s) Loss: 0.0003(0.0027) Grad: 2081.2046  LR: 0.000006  \n",
      "Epoch: [1][27800/36908] Elapsed 359m 53s (remain 117m 53s) Loss: 0.0001(0.0027) Grad: 1338.0157  LR: 0.000005  \n",
      "Epoch: [1][27900/36908] Elapsed 361m 10s (remain 116m 35s) Loss: 0.0023(0.0027) Grad: 20288.8965  LR: 0.000005  \n",
      "Epoch: [1][28000/36908] Elapsed 362m 25s (remain 115m 17s) Loss: 0.0000(0.0027) Grad: 256.1802  LR: 0.000005  \n",
      "Epoch: [1][28100/36908] Elapsed 363m 44s (remain 113m 59s) Loss: 0.0001(0.0027) Grad: 776.2299  LR: 0.000005  \n",
      "Epoch: [1][28200/36908] Elapsed 365m 1s (remain 112m 42s) Loss: 0.0099(0.0027) Grad: 35137.8594  LR: 0.000005  \n",
      "Epoch: [1][28300/36908] Elapsed 366m 17s (remain 111m 23s) Loss: 0.0000(0.0027) Grad: 5.9428  LR: 0.000005  \n",
      "Epoch: [1][28400/36908] Elapsed 367m 35s (remain 110m 6s) Loss: 0.0000(0.0027) Grad: 24.4571  LR: 0.000005  \n",
      "Epoch: [1][28500/36908] Elapsed 368m 53s (remain 108m 48s) Loss: 0.0020(0.0027) Grad: 41791.5547  LR: 0.000005  \n",
      "Epoch: [1][28600/36908] Elapsed 370m 11s (remain 107m 31s) Loss: 0.0083(0.0027) Grad: 46913.1367  LR: 0.000005  \n",
      "Epoch: [1][28700/36908] Elapsed 371m 27s (remain 106m 13s) Loss: 0.0000(0.0027) Grad: 226.0967  LR: 0.000005  \n",
      "Epoch: [1][28800/36908] Elapsed 372m 45s (remain 104m 55s) Loss: 0.0000(0.0027) Grad: 10.1885  LR: 0.000005  \n",
      "Epoch: [1][28900/36908] Elapsed 374m 3s (remain 103m 38s) Loss: 0.0000(0.0027) Grad: 288.6180  LR: 0.000005  \n",
      "Epoch: [1][29000/36908] Elapsed 375m 21s (remain 102m 20s) Loss: 0.0003(0.0027) Grad: 8280.1787  LR: 0.000005  \n",
      "Epoch: [1][29100/36908] Elapsed 376m 40s (remain 101m 3s) Loss: 0.0002(0.0027) Grad: 6075.0117  LR: 0.000005  \n",
      "Epoch: [1][29200/36908] Elapsed 377m 59s (remain 99m 45s) Loss: 0.0000(0.0027) Grad: 18.5234  LR: 0.000005  \n",
      "Epoch: [1][29300/36908] Elapsed 379m 18s (remain 98m 28s) Loss: 0.0005(0.0027) Grad: 13136.2656  LR: 0.000005  \n",
      "Epoch: [1][29400/36908] Elapsed 380m 36s (remain 97m 10s) Loss: 0.0003(0.0027) Grad: 3917.6052  LR: 0.000005  \n",
      "Epoch: [1][29500/36908] Elapsed 381m 55s (remain 95m 53s) Loss: 0.0000(0.0026) Grad: 28.8413  LR: 0.000004  \n",
      "Epoch: [1][29600/36908] Elapsed 383m 13s (remain 94m 35s) Loss: 0.0000(0.0026) Grad: 27.8388  LR: 0.000004  \n",
      "Epoch: [1][29700/36908] Elapsed 384m 32s (remain 93m 18s) Loss: 0.0003(0.0026) Grad: 23289.6094  LR: 0.000004  \n",
      "Epoch: [1][29800/36908] Elapsed 385m 48s (remain 92m 0s) Loss: 0.0063(0.0026) Grad: 69385.6328  LR: 0.000004  \n",
      "Epoch: [1][29900/36908] Elapsed 387m 5s (remain 90m 42s) Loss: 0.0013(0.0026) Grad: 13345.6396  LR: 0.000004  \n",
      "Epoch: [1][30000/36908] Elapsed 388m 24s (remain 89m 25s) Loss: 0.0238(0.0026) Grad: 1303567.5000  LR: 0.000004  \n",
      "Epoch: [1][30100/36908] Elapsed 389m 41s (remain 88m 7s) Loss: 0.0232(0.0026) Grad: 417809.7188  LR: 0.000004  \n",
      "Epoch: [1][30200/36908] Elapsed 390m 58s (remain 86m 49s) Loss: 0.0000(0.0026) Grad: 6.8111  LR: 0.000004  \n",
      "Epoch: [1][30300/36908] Elapsed 392m 16s (remain 85m 31s) Loss: 0.0007(0.0026) Grad: 7048.4541  LR: 0.000004  \n",
      "Epoch: [1][30400/36908] Elapsed 393m 32s (remain 84m 14s) Loss: 0.0000(0.0026) Grad: 42.8581  LR: 0.000004  \n",
      "Epoch: [1][30500/36908] Elapsed 394m 49s (remain 82m 56s) Loss: 0.0000(0.0026) Grad: 216.9161  LR: 0.000004  \n",
      "Epoch: [1][30600/36908] Elapsed 396m 6s (remain 81m 38s) Loss: 0.0001(0.0026) Grad: 2337.2246  LR: 0.000004  \n",
      "Epoch: [1][30700/36908] Elapsed 397m 23s (remain 80m 20s) Loss: 0.0016(0.0026) Grad: 25085.3984  LR: 0.000004  \n",
      "Epoch: [1][30800/36908] Elapsed 398m 40s (remain 79m 2s) Loss: 0.0000(0.0026) Grad: 26.9442  LR: 0.000004  \n",
      "Epoch: [1][30900/36908] Elapsed 399m 57s (remain 77m 44s) Loss: 0.0000(0.0026) Grad: 28.8326  LR: 0.000004  \n",
      "Epoch: [1][31000/36908] Elapsed 401m 14s (remain 76m 27s) Loss: 0.0000(0.0026) Grad: 12.3127  LR: 0.000004  \n",
      "Epoch: [1][31100/36908] Elapsed 402m 32s (remain 75m 9s) Loss: 0.0000(0.0026) Grad: 374.2564  LR: 0.000003  \n",
      "Epoch: [1][31200/36908] Elapsed 403m 51s (remain 73m 52s) Loss: 0.0008(0.0026) Grad: 52514.2695  LR: 0.000003  \n",
      "Epoch: [1][31300/36908] Elapsed 405m 9s (remain 72m 34s) Loss: 0.0036(0.0026) Grad: 53229.3242  LR: 0.000003  \n",
      "Epoch: [1][31400/36908] Elapsed 406m 26s (remain 71m 16s) Loss: 0.0000(0.0026) Grad: 1133.2896  LR: 0.000003  \n",
      "Epoch: [1][31500/36908] Elapsed 407m 44s (remain 69m 59s) Loss: 0.0001(0.0026) Grad: 10782.3506  LR: 0.000003  \n",
      "Epoch: [1][31600/36908] Elapsed 409m 1s (remain 68m 41s) Loss: 0.0005(0.0026) Grad: 29051.0898  LR: 0.000003  \n",
      "Epoch: [1][31700/36908] Elapsed 410m 18s (remain 67m 23s) Loss: 0.0000(0.0026) Grad: 511.9859  LR: 0.000003  \n",
      "Epoch: [1][31800/36908] Elapsed 411m 35s (remain 66m 5s) Loss: 0.0000(0.0026) Grad: 4.2765  LR: 0.000003  \n",
      "Epoch: [1][31900/36908] Elapsed 412m 53s (remain 64m 48s) Loss: 0.0008(0.0026) Grad: 6490.8984  LR: 0.000003  \n",
      "Epoch: [1][32000/36908] Elapsed 414m 9s (remain 63m 30s) Loss: 0.0001(0.0026) Grad: 2512.0154  LR: 0.000003  \n",
      "Epoch: [1][32100/36908] Elapsed 415m 26s (remain 62m 12s) Loss: 0.0023(0.0026) Grad: 98522.7344  LR: 0.000003  \n",
      "Epoch: [1][32200/36908] Elapsed 416m 44s (remain 60m 55s) Loss: 0.0000(0.0026) Grad: 24.9537  LR: 0.000003  \n",
      "Epoch: [1][32300/36908] Elapsed 418m 3s (remain 59m 37s) Loss: 0.0021(0.0026) Grad: 15285.4795  LR: 0.000003  \n",
      "Epoch: [1][32400/36908] Elapsed 419m 21s (remain 58m 19s) Loss: 0.0028(0.0026) Grad: 55635.6797  LR: 0.000003  \n",
      "Epoch: [1][32500/36908] Elapsed 420m 39s (remain 57m 2s) Loss: 0.0000(0.0026) Grad: 17.5773  LR: 0.000003  \n",
      "Epoch: [1][32600/36908] Elapsed 421m 58s (remain 55m 44s) Loss: 0.0001(0.0026) Grad: 1570.4893  LR: 0.000003  \n",
      "Epoch: [1][32700/36908] Elapsed 423m 17s (remain 54m 27s) Loss: 0.0076(0.0026) Grad: 226139.6094  LR: 0.000003  \n",
      "Epoch: [1][32800/36908] Elapsed 424m 34s (remain 53m 9s) Loss: 0.0000(0.0026) Grad: 116.0424  LR: 0.000002  \n",
      "Epoch: [1][32900/36908] Elapsed 425m 50s (remain 51m 51s) Loss: 0.0003(0.0026) Grad: 32075.8535  LR: 0.000002  \n",
      "Epoch: [1][33000/36908] Elapsed 427m 8s (remain 50m 34s) Loss: 0.0008(0.0026) Grad: 50184.6836  LR: 0.000002  \n",
      "Epoch: [1][33100/36908] Elapsed 428m 26s (remain 49m 16s) Loss: 0.0010(0.0026) Grad: 8866.0801  LR: 0.000002  \n",
      "Epoch: [1][33200/36908] Elapsed 429m 44s (remain 47m 58s) Loss: 0.0046(0.0026) Grad: 86893.7344  LR: 0.000002  \n",
      "Epoch: [1][33300/36908] Elapsed 431m 2s (remain 46m 41s) Loss: 0.0010(0.0026) Grad: 12804.1367  LR: 0.000002  \n",
      "Epoch: [1][33400/36908] Elapsed 432m 20s (remain 45m 23s) Loss: 0.0000(0.0026) Grad: 61.4815  LR: 0.000002  \n",
      "Epoch: [1][33500/36908] Elapsed 433m 37s (remain 44m 5s) Loss: 0.0000(0.0026) Grad: 563.6481  LR: 0.000002  \n",
      "Epoch: [1][33600/36908] Elapsed 434m 56s (remain 42m 48s) Loss: 0.0000(0.0026) Grad: 66.1068  LR: 0.000002  \n",
      "Epoch: [1][33700/36908] Elapsed 436m 14s (remain 41m 30s) Loss: 0.0011(0.0026) Grad: 112647.8281  LR: 0.000002  \n",
      "Epoch: [1][33800/36908] Elapsed 437m 31s (remain 40m 13s) Loss: 0.0000(0.0026) Grad: 52.6558  LR: 0.000002  \n",
      "Epoch: [1][33900/36908] Elapsed 438m 49s (remain 38m 55s) Loss: 0.0001(0.0026) Grad: 4405.2012  LR: 0.000002  \n",
      "Epoch: [1][34000/36908] Elapsed 440m 7s (remain 37m 37s) Loss: 0.0000(0.0026) Grad: 43.4393  LR: 0.000002  \n",
      "Epoch: [1][34100/36908] Elapsed 441m 24s (remain 36m 20s) Loss: 0.0000(0.0026) Grad: 23.3184  LR: 0.000002  \n",
      "Epoch: [1][34200/36908] Elapsed 442m 43s (remain 35m 2s) Loss: 0.0000(0.0026) Grad: 71.0956  LR: 0.000002  \n",
      "Epoch: [1][34300/36908] Elapsed 443m 59s (remain 33m 44s) Loss: 0.0367(0.0026) Grad: 827826.8750  LR: 0.000002  \n",
      "Epoch: [1][34400/36908] Elapsed 445m 16s (remain 32m 27s) Loss: 0.0000(0.0026) Grad: 159.8592  LR: 0.000002  \n",
      "Epoch: [1][34500/36908] Elapsed 446m 34s (remain 31m 9s) Loss: 0.0000(0.0026) Grad: 39.9811  LR: 0.000001  \n",
      "Epoch: [1][34600/36908] Elapsed 447m 52s (remain 29m 51s) Loss: 0.0000(0.0026) Grad: 91.2971  LR: 0.000001  \n",
      "Epoch: [1][34700/36908] Elapsed 449m 9s (remain 28m 34s) Loss: 0.0000(0.0026) Grad: 10.2986  LR: 0.000001  \n",
      "Epoch: [1][34800/36908] Elapsed 450m 27s (remain 27m 16s) Loss: 0.0072(0.0026) Grad: 53206.4102  LR: 0.000001  \n",
      "Epoch: [1][34900/36908] Elapsed 451m 44s (remain 25m 58s) Loss: 0.0144(0.0026) Grad: 44017.7812  LR: 0.000001  \n",
      "Epoch: [1][35000/36908] Elapsed 453m 1s (remain 24m 40s) Loss: 0.0000(0.0026) Grad: 12.4939  LR: 0.000001  \n",
      "Epoch: [1][35100/36908] Elapsed 454m 20s (remain 23m 23s) Loss: 0.0000(0.0026) Grad: 352.6318  LR: 0.000001  \n",
      "Epoch: [1][35200/36908] Elapsed 455m 38s (remain 22m 5s) Loss: 0.0000(0.0026) Grad: 4.4280  LR: 0.000001  \n",
      "Epoch: [1][35300/36908] Elapsed 456m 56s (remain 20m 48s) Loss: 0.0056(0.0026) Grad: 5858.9297  LR: 0.000001  \n",
      "Epoch: [1][35400/36908] Elapsed 458m 15s (remain 19m 30s) Loss: 0.0000(0.0026) Grad: 36.0907  LR: 0.000001  \n",
      "Epoch: [1][35500/36908] Elapsed 459m 33s (remain 18m 12s) Loss: 0.0005(0.0026) Grad: 12079.2510  LR: 0.000001  \n",
      "Epoch: [1][35600/36908] Elapsed 460m 51s (remain 16m 55s) Loss: 0.0000(0.0026) Grad: 30.5623  LR: 0.000001  \n",
      "Epoch: [1][35700/36908] Elapsed 462m 9s (remain 15m 37s) Loss: 0.0001(0.0026) Grad: 1327.8157  LR: 0.000001  \n",
      "Epoch: [1][35800/36908] Elapsed 463m 26s (remain 14m 19s) Loss: 0.0001(0.0026) Grad: 3829.4084  LR: 0.000001  \n",
      "Epoch: [1][35900/36908] Elapsed 464m 44s (remain 13m 2s) Loss: 0.0000(0.0026) Grad: 2.0340  LR: 0.000001  \n",
      "Epoch: [1][36000/36908] Elapsed 466m 2s (remain 11m 44s) Loss: 0.0067(0.0026) Grad: 87357.3359  LR: 0.000001  \n",
      "Epoch: [1][36100/36908] Elapsed 467m 19s (remain 10m 26s) Loss: 0.0101(0.0026) Grad: 111790.9531  LR: 0.000000  \n",
      "Epoch: [1][36200/36908] Elapsed 468m 37s (remain 9m 9s) Loss: 0.0005(0.0026) Grad: 946.7361  LR: 0.000000  \n",
      "Epoch: [1][36300/36908] Elapsed 469m 53s (remain 7m 51s) Loss: 0.0003(0.0026) Grad: 12540.3555  LR: 0.000000  \n",
      "Epoch: [1][36400/36908] Elapsed 471m 9s (remain 6m 33s) Loss: 0.0022(0.0026) Grad: 25792.5996  LR: 0.000000  \n",
      "Epoch: [1][36500/36908] Elapsed 472m 25s (remain 5m 16s) Loss: 0.0000(0.0026) Grad: 4.4165  LR: 0.000000  \n",
      "Epoch: [1][36600/36908] Elapsed 473m 42s (remain 3m 58s) Loss: 0.0000(0.0026) Grad: 1.8321  LR: 0.000000  \n",
      "Epoch: [1][36700/36908] Elapsed 474m 59s (remain 2m 40s) Loss: 0.0014(0.0026) Grad: 16466.0371  LR: 0.000000  \n",
      "Epoch: [1][36800/36908] Elapsed 476m 16s (remain 1m 23s) Loss: 0.0010(0.0026) Grad: 17459.1738  LR: 0.000000  \n",
      "Epoch: [1][36900/36908] Elapsed 477m 34s (remain 0m 5s) Loss: 0.0000(0.0026) Grad: 21.1129  LR: 0.000000  \n",
      "Epoch: [1][36907/36908] Elapsed 477m 39s (remain 0m 0s) Loss: 0.0020(0.0026) Grad: 36715.3359  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 1s (remain 22m 18s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 31s (remain 5m 41s) Loss: 0.0005(0.0079) \n",
      "EVAL: [200/1192] Elapsed 1m 2s (remain 5m 8s) Loss: 0.0000(0.0094) \n",
      "EVAL: [300/1192] Elapsed 1m 33s (remain 4m 35s) Loss: 0.0013(0.0150) \n",
      "EVAL: [400/1192] Elapsed 2m 3s (remain 4m 2s) Loss: 0.0454(0.0152) \n",
      "EVAL: [500/1192] Elapsed 2m 33s (remain 3m 31s) Loss: 0.0401(0.0139) \n",
      "EVAL: [600/1192] Elapsed 3m 3s (remain 3m 0s) Loss: 0.1837(0.0141) \n",
      "EVAL: [700/1192] Elapsed 3m 33s (remain 2m 29s) Loss: 0.0101(0.0164) \n",
      "EVAL: [800/1192] Elapsed 4m 3s (remain 1m 59s) Loss: 0.0161(0.0160) \n",
      "EVAL: [900/1192] Elapsed 4m 34s (remain 1m 28s) Loss: 0.0017(0.0154) \n",
      "EVAL: [1000/1192] Elapsed 5m 4s (remain 0m 58s) Loss: 0.0000(0.0149) \n",
      "EVAL: [1100/1192] Elapsed 5m 35s (remain 0m 27s) Loss: 0.0081(0.0142) \n",
      "EVAL: [1191/1192] Elapsed 6m 2s (remain 0m 0s) Loss: 0.0132(0.0133) \n",
      "Epoch 1 - avg_train_loss: 0.0026  avg_val_loss: 0.0133  time: 29026s\n",
      "Epoch 1 - Score: 0.8843\n",
      "Epoch 1 - Save Best Score: 0.8843 Model\n",
      "========== fold: 2 training ==========\n",
      "get pseudo plain from ../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_2.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_2.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp083/pseudo_labels_2.npy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11118c5cddf74fc184b45ed09ffb3b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b7f10a77194ce2b9ec4a26b39b6a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(612602, 950)\n",
      "(612602, 6) (612602, 950)\n",
      "(100000, 7)\n",
      "(110725, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n",
      "load weights from ../output/nbme-score-clinical-patient-notes/nbme-exp087/fold2_best.pth\n",
      "Epoch: [1][0/36908] Elapsed 0m 1s (remain 923m 52s) Loss: 0.0000(0.0000) Grad: 462.0971  LR: 0.000000  \n",
      "Epoch: [1][100/36908] Elapsed 1m 18s (remain 477m 28s) Loss: 0.0000(0.0018) Grad: 229.0694  LR: 0.000001  \n",
      "Epoch: [1][200/36908] Elapsed 2m 36s (remain 475m 1s) Loss: 0.0006(0.0019) Grad: 9277.7256  LR: 0.000001  \n",
      "Epoch: [1][300/36908] Elapsed 3m 52s (remain 471m 32s) Loss: 0.0000(0.0017) Grad: 7.3089  LR: 0.000002  \n",
      "Epoch: [1][400/36908] Elapsed 5m 9s (remain 470m 10s) Loss: 0.0004(0.0020) Grad: 1990.6484  LR: 0.000002  \n",
      "Epoch: [1][500/36908] Elapsed 6m 27s (remain 469m 43s) Loss: 0.0000(0.0020) Grad: 20.5692  LR: 0.000003  \n",
      "Epoch: [1][600/36908] Elapsed 7m 44s (remain 468m 10s) Loss: 0.0000(0.0020) Grad: 79.4753  LR: 0.000003  \n",
      "Epoch: [1][700/36908] Elapsed 9m 2s (remain 466m 43s) Loss: 0.0000(0.0019) Grad: 37.3476  LR: 0.000004  \n",
      "Epoch: [1][800/36908] Elapsed 10m 18s (remain 464m 55s) Loss: 0.0000(0.0020) Grad: 6.4792  LR: 0.000004  \n",
      "Epoch: [1][900/36908] Elapsed 11m 35s (remain 463m 4s) Loss: 0.0012(0.0021) Grad: 18638.4297  LR: 0.000005  \n",
      "Epoch: [1][1000/36908] Elapsed 12m 52s (remain 461m 59s) Loss: 0.0000(0.0021) Grad: 47.2537  LR: 0.000005  \n",
      "Epoch: [1][1100/36908] Elapsed 14m 8s (remain 460m 11s) Loss: 0.0000(0.0022) Grad: 6.6399  LR: 0.000006  \n",
      "Epoch: [1][1200/36908] Elapsed 15m 27s (remain 459m 33s) Loss: 0.0000(0.0022) Grad: 161.4108  LR: 0.000007  \n",
      "Epoch: [1][1300/36908] Elapsed 16m 46s (remain 458m 57s) Loss: 0.0000(0.0023) Grad: 207.8578  LR: 0.000007  \n",
      "Epoch: [1][1400/36908] Elapsed 18m 4s (remain 458m 3s) Loss: 0.0056(0.0024) Grad: 41304.3047  LR: 0.000008  \n",
      "Epoch: [1][1500/36908] Elapsed 19m 22s (remain 457m 8s) Loss: 0.0131(0.0024) Grad: 51980.5391  LR: 0.000008  \n",
      "Epoch: [1][1600/36908] Elapsed 20m 41s (remain 456m 8s) Loss: 0.0036(0.0024) Grad: 16726.7871  LR: 0.000009  \n",
      "Epoch: [1][1700/36908] Elapsed 21m 58s (remain 454m 57s) Loss: 0.0000(0.0024) Grad: 13.4098  LR: 0.000009  \n",
      "Epoch: [1][1800/36908] Elapsed 23m 17s (remain 454m 4s) Loss: 0.0000(0.0025) Grad: 235.1850  LR: 0.000010  \n",
      "Epoch: [1][1900/36908] Elapsed 24m 35s (remain 452m 47s) Loss: 0.0065(0.0025) Grad: 178431.5938  LR: 0.000010  \n",
      "Epoch: [1][2000/36908] Elapsed 25m 51s (remain 451m 11s) Loss: 0.0037(0.0025) Grad: 147474.5156  LR: 0.000011  \n",
      "Epoch: [1][2100/36908] Elapsed 27m 10s (remain 450m 5s) Loss: 0.0001(0.0025) Grad: 897.3543  LR: 0.000011  \n",
      "Epoch: [1][2200/36908] Elapsed 28m 28s (remain 449m 0s) Loss: 0.0005(0.0025) Grad: 4519.7354  LR: 0.000012  \n",
      "Epoch: [1][2300/36908] Elapsed 29m 45s (remain 447m 41s) Loss: 0.0000(0.0025) Grad: 16.9462  LR: 0.000012  \n",
      "Epoch: [1][2400/36908] Elapsed 31m 3s (remain 446m 21s) Loss: 0.0028(0.0025) Grad: 31753.1543  LR: 0.000013  \n",
      "Epoch: [1][2500/36908] Elapsed 32m 22s (remain 445m 21s) Loss: 0.0000(0.0025) Grad: 113.5070  LR: 0.000014  \n",
      "Epoch: [1][2600/36908] Elapsed 33m 39s (remain 443m 56s) Loss: 0.0000(0.0025) Grad: 17.7060  LR: 0.000014  \n",
      "Epoch: [1][2700/36908] Elapsed 34m 57s (remain 442m 41s) Loss: 0.0013(0.0025) Grad: 25719.8027  LR: 0.000015  \n",
      "Epoch: [1][2800/36908] Elapsed 36m 13s (remain 441m 9s) Loss: 0.0000(0.0025) Grad: 4.2295  LR: 0.000015  \n",
      "Epoch: [1][2900/36908] Elapsed 37m 30s (remain 439m 42s) Loss: 0.0002(0.0025) Grad: 1163.9513  LR: 0.000016  \n",
      "Epoch: [1][3000/36908] Elapsed 38m 46s (remain 438m 7s) Loss: 0.0000(0.0025) Grad: 135.1437  LR: 0.000016  \n",
      "Epoch: [1][3100/36908] Elapsed 40m 2s (remain 436m 35s) Loss: 0.0005(0.0026) Grad: 17837.7129  LR: 0.000017  \n",
      "Epoch: [1][3200/36908] Elapsed 41m 21s (remain 435m 28s) Loss: 0.0001(0.0026) Grad: 830.6336  LR: 0.000017  \n",
      "Epoch: [1][3300/36908] Elapsed 42m 39s (remain 434m 15s) Loss: 0.0000(0.0026) Grad: 19.2176  LR: 0.000018  \n",
      "Epoch: [1][3400/36908] Elapsed 43m 56s (remain 432m 59s) Loss: 0.0003(0.0027) Grad: 2068.5183  LR: 0.000018  \n",
      "Epoch: [1][3500/36908] Elapsed 45m 15s (remain 431m 50s) Loss: 0.0001(0.0027) Grad: 1502.3015  LR: 0.000019  \n",
      "Epoch: [1][3600/36908] Elapsed 46m 33s (remain 430m 39s) Loss: 0.0000(0.0027) Grad: 35.8474  LR: 0.000020  \n",
      "Epoch: [1][3700/36908] Elapsed 47m 53s (remain 429m 38s) Loss: 0.0001(0.0028) Grad: 681.5569  LR: 0.000020  \n",
      "Epoch: [1][3800/36908] Elapsed 49m 11s (remain 428m 30s) Loss: 0.0156(0.0028) Grad: 79676.3828  LR: 0.000020  \n",
      "Epoch: [1][3900/36908] Elapsed 50m 30s (remain 427m 20s) Loss: 0.0000(0.0028) Grad: 70.6295  LR: 0.000020  \n",
      "Epoch: [1][4000/36908] Elapsed 51m 47s (remain 425m 59s) Loss: 0.0001(0.0029) Grad: 2734.3000  LR: 0.000020  \n",
      "Epoch: [1][4100/36908] Elapsed 53m 5s (remain 424m 41s) Loss: 0.0080(0.0029) Grad: 24103.9023  LR: 0.000020  \n",
      "Epoch: [1][4200/36908] Elapsed 54m 21s (remain 423m 12s) Loss: 0.0005(0.0029) Grad: 9996.2354  LR: 0.000020  \n",
      "Epoch: [1][4300/36908] Elapsed 55m 39s (remain 421m 59s) Loss: 0.0013(0.0029) Grad: 51803.9688  LR: 0.000020  \n",
      "Epoch: [1][4400/36908] Elapsed 56m 55s (remain 420m 30s) Loss: 0.0024(0.0029) Grad: 5788.6362  LR: 0.000020  \n",
      "Epoch: [1][4500/36908] Elapsed 58m 13s (remain 419m 14s) Loss: 0.0000(0.0030) Grad: 122.9048  LR: 0.000020  \n",
      "Epoch: [1][4600/36908] Elapsed 59m 30s (remain 417m 54s) Loss: 0.0029(0.0030) Grad: 36537.2266  LR: 0.000019  \n",
      "Epoch: [1][4700/36908] Elapsed 60m 48s (remain 416m 36s) Loss: 0.0000(0.0030) Grad: 59.1162  LR: 0.000019  \n",
      "Epoch: [1][4800/36908] Elapsed 62m 7s (remain 415m 24s) Loss: 0.0005(0.0030) Grad: 3611.4150  LR: 0.000019  \n",
      "Epoch: [1][4900/36908] Elapsed 63m 25s (remain 414m 15s) Loss: 0.0000(0.0030) Grad: 14.8158  LR: 0.000019  \n",
      "Epoch: [1][5000/36908] Elapsed 64m 43s (remain 412m 57s) Loss: 0.0003(0.0030) Grad: 4659.7158  LR: 0.000019  \n",
      "Epoch: [1][5100/36908] Elapsed 66m 1s (remain 411m 40s) Loss: 0.0011(0.0030) Grad: 24576.6250  LR: 0.000019  \n",
      "Epoch: [1][5200/36908] Elapsed 67m 20s (remain 410m 31s) Loss: 0.0014(0.0030) Grad: 5848.6187  LR: 0.000019  \n",
      "Epoch: [1][5300/36908] Elapsed 68m 37s (remain 409m 12s) Loss: 0.0000(0.0030) Grad: 545.0626  LR: 0.000019  \n",
      "Epoch: [1][5400/36908] Elapsed 69m 55s (remain 407m 53s) Loss: 0.0001(0.0030) Grad: 241.0097  LR: 0.000019  \n",
      "Epoch: [1][5500/36908] Elapsed 71m 13s (remain 406m 41s) Loss: 0.0027(0.0030) Grad: 6847.0366  LR: 0.000019  \n",
      "Epoch: [1][5600/36908] Elapsed 72m 31s (remain 405m 24s) Loss: 0.0008(0.0030) Grad: 7682.2065  LR: 0.000019  \n",
      "Epoch: [1][5700/36908] Elapsed 73m 50s (remain 404m 13s) Loss: 0.0013(0.0030) Grad: 11986.3418  LR: 0.000019  \n",
      "Epoch: [1][5800/36908] Elapsed 75m 9s (remain 403m 1s) Loss: 0.0000(0.0031) Grad: 122.0593  LR: 0.000019  \n",
      "Epoch: [1][5900/36908] Elapsed 76m 27s (remain 401m 43s) Loss: 0.0109(0.0031) Grad: 30800.5391  LR: 0.000019  \n",
      "Epoch: [1][6000/36908] Elapsed 77m 44s (remain 400m 23s) Loss: 0.0057(0.0031) Grad: 46012.3281  LR: 0.000019  \n",
      "Epoch: [1][6100/36908] Elapsed 79m 3s (remain 399m 11s) Loss: 0.0011(0.0031) Grad: 56491.1406  LR: 0.000019  \n",
      "Epoch: [1][6200/36908] Elapsed 80m 21s (remain 397m 54s) Loss: 0.0001(0.0031) Grad: 191.6282  LR: 0.000018  \n",
      "Epoch: [1][6300/36908] Elapsed 81m 40s (remain 396m 41s) Loss: 0.0094(0.0031) Grad: 53414.6484  LR: 0.000018  \n",
      "Epoch: [1][6400/36908] Elapsed 82m 57s (remain 395m 22s) Loss: 0.0006(0.0031) Grad: 1768.3130  LR: 0.000018  \n",
      "Epoch: [1][6500/36908] Elapsed 84m 15s (remain 394m 6s) Loss: 0.0002(0.0031) Grad: 776.7883  LR: 0.000018  \n",
      "Epoch: [1][6600/36908] Elapsed 85m 33s (remain 392m 47s) Loss: 0.0005(0.0031) Grad: 38651.7695  LR: 0.000018  \n",
      "Epoch: [1][6700/36908] Elapsed 86m 50s (remain 391m 28s) Loss: 0.0000(0.0031) Grad: 193.5970  LR: 0.000018  \n",
      "Epoch: [1][6800/36908] Elapsed 88m 7s (remain 390m 8s) Loss: 0.0001(0.0031) Grad: 402.4549  LR: 0.000018  \n",
      "Epoch: [1][6900/36908] Elapsed 89m 24s (remain 388m 44s) Loss: 0.0000(0.0031) Grad: 65.9376  LR: 0.000018  \n",
      "Epoch: [1][7000/36908] Elapsed 90m 42s (remain 387m 27s) Loss: 0.0008(0.0031) Grad: 656.3785  LR: 0.000018  \n",
      "Epoch: [1][7100/36908] Elapsed 91m 59s (remain 386m 7s) Loss: 0.0000(0.0031) Grad: 22.7419  LR: 0.000018  \n",
      "Epoch: [1][7200/36908] Elapsed 93m 17s (remain 384m 52s) Loss: 0.0181(0.0031) Grad: 64952.7070  LR: 0.000018  \n",
      "Epoch: [1][7300/36908] Elapsed 94m 35s (remain 383m 36s) Loss: 0.0000(0.0032) Grad: 100.3656  LR: 0.000018  \n",
      "Epoch: [1][7400/36908] Elapsed 95m 53s (remain 382m 19s) Loss: 0.0000(0.0032) Grad: 35.8673  LR: 0.000018  \n",
      "Epoch: [1][7500/36908] Elapsed 97m 12s (remain 381m 4s) Loss: 0.0010(0.0032) Grad: 9937.5107  LR: 0.000018  \n",
      "Epoch: [1][7600/36908] Elapsed 98m 29s (remain 379m 45s) Loss: 0.0000(0.0032) Grad: 5703.1680  LR: 0.000018  \n",
      "Epoch: [1][7700/36908] Elapsed 99m 46s (remain 378m 24s) Loss: 0.0033(0.0032) Grad: 10680.9336  LR: 0.000018  \n",
      "Epoch: [1][7800/36908] Elapsed 101m 5s (remain 377m 9s) Loss: 0.0002(0.0032) Grad: 714.1212  LR: 0.000018  \n",
      "Epoch: [1][7900/36908] Elapsed 102m 22s (remain 375m 49s) Loss: 0.0036(0.0032) Grad: 6630.5820  LR: 0.000017  \n",
      "Epoch: [1][8000/36908] Elapsed 103m 37s (remain 374m 24s) Loss: 0.0000(0.0032) Grad: 192.9487  LR: 0.000017  \n",
      "Epoch: [1][8100/36908] Elapsed 104m 55s (remain 373m 4s) Loss: 0.0001(0.0032) Grad: 170.1940  LR: 0.000017  \n",
      "Epoch: [1][8200/36908] Elapsed 106m 11s (remain 371m 44s) Loss: 0.0000(0.0032) Grad: 17.7341  LR: 0.000017  \n",
      "Epoch: [1][8300/36908] Elapsed 107m 28s (remain 370m 23s) Loss: 0.0000(0.0032) Grad: 47.9952  LR: 0.000017  \n",
      "Epoch: [1][8400/36908] Elapsed 108m 46s (remain 369m 7s) Loss: 0.0069(0.0032) Grad: 62399.6680  LR: 0.000017  \n",
      "Epoch: [1][8500/36908] Elapsed 110m 5s (remain 367m 52s) Loss: 0.0535(0.0032) Grad: 132165.6875  LR: 0.000017  \n",
      "Epoch: [1][8600/36908] Elapsed 111m 23s (remain 366m 37s) Loss: 0.0016(0.0032) Grad: 6721.7954  LR: 0.000017  \n",
      "Epoch: [1][8700/36908] Elapsed 112m 40s (remain 365m 15s) Loss: 0.0000(0.0032) Grad: 105.6190  LR: 0.000017  \n",
      "Epoch: [1][8800/36908] Elapsed 113m 56s (remain 363m 53s) Loss: 0.0026(0.0032) Grad: 20116.0488  LR: 0.000017  \n",
      "Epoch: [1][8900/36908] Elapsed 115m 13s (remain 362m 34s) Loss: 0.0054(0.0032) Grad: 9065.9912  LR: 0.000017  \n",
      "Epoch: [1][9000/36908] Elapsed 116m 31s (remain 361m 17s) Loss: 0.0001(0.0032) Grad: 236.4673  LR: 0.000017  \n",
      "Epoch: [1][9100/36908] Elapsed 117m 50s (remain 360m 4s) Loss: 0.0095(0.0032) Grad: 17842.8223  LR: 0.000017  \n",
      "Epoch: [1][9200/36908] Elapsed 119m 9s (remain 358m 48s) Loss: 0.0000(0.0032) Grad: 63.0304  LR: 0.000017  \n",
      "Epoch: [1][9300/36908] Elapsed 120m 28s (remain 357m 34s) Loss: 0.0000(0.0032) Grad: 380.7202  LR: 0.000017  \n",
      "Epoch: [1][9400/36908] Elapsed 121m 46s (remain 356m 17s) Loss: 0.0000(0.0032) Grad: 37.1515  LR: 0.000017  \n",
      "Epoch: [1][9500/36908] Elapsed 123m 4s (remain 355m 2s) Loss: 0.0055(0.0032) Grad: 62529.2539  LR: 0.000017  \n",
      "Epoch: [1][9600/36908] Elapsed 124m 22s (remain 353m 43s) Loss: 0.0011(0.0032) Grad: 2937.3784  LR: 0.000016  \n",
      "Epoch: [1][9700/36908] Elapsed 125m 40s (remain 352m 27s) Loss: 0.0002(0.0032) Grad: 628.1733  LR: 0.000016  \n",
      "Epoch: [1][9800/36908] Elapsed 126m 58s (remain 351m 10s) Loss: 0.0000(0.0032) Grad: 30.0260  LR: 0.000016  \n",
      "Epoch: [1][9900/36908] Elapsed 128m 16s (remain 349m 54s) Loss: 0.0000(0.0032) Grad: 17.0095  LR: 0.000016  \n",
      "Epoch: [1][10000/36908] Elapsed 129m 34s (remain 348m 37s) Loss: 0.0001(0.0032) Grad: 567.8314  LR: 0.000016  \n",
      "Epoch: [1][10100/36908] Elapsed 130m 52s (remain 347m 19s) Loss: 0.0003(0.0032) Grad: 1277.8522  LR: 0.000016  \n",
      "Epoch: [1][10200/36908] Elapsed 132m 10s (remain 346m 3s) Loss: 0.0000(0.0032) Grad: 13.7462  LR: 0.000016  \n",
      "Epoch: [1][10300/36908] Elapsed 133m 29s (remain 344m 48s) Loss: 0.0000(0.0032) Grad: 13.6364  LR: 0.000016  \n",
      "Epoch: [1][10400/36908] Elapsed 134m 46s (remain 343m 28s) Loss: 0.0007(0.0032) Grad: 1719.9918  LR: 0.000016  \n",
      "Epoch: [1][10500/36908] Elapsed 136m 4s (remain 342m 11s) Loss: 0.0001(0.0032) Grad: 370.0363  LR: 0.000016  \n",
      "Epoch: [1][10600/36908] Elapsed 137m 22s (remain 340m 54s) Loss: 0.0001(0.0032) Grad: 676.2402  LR: 0.000016  \n",
      "Epoch: [1][10700/36908] Elapsed 138m 40s (remain 339m 37s) Loss: 0.0001(0.0032) Grad: 569.1218  LR: 0.000016  \n",
      "Epoch: [1][10800/36908] Elapsed 139m 58s (remain 338m 19s) Loss: 0.0000(0.0032) Grad: 37.6802  LR: 0.000016  \n",
      "Epoch: [1][10900/36908] Elapsed 141m 17s (remain 337m 5s) Loss: 0.0000(0.0032) Grad: 12.2034  LR: 0.000016  \n",
      "Epoch: [1][11000/36908] Elapsed 142m 34s (remain 335m 46s) Loss: 0.0002(0.0032) Grad: 4200.9766  LR: 0.000016  \n",
      "Epoch: [1][11100/36908] Elapsed 143m 53s (remain 334m 29s) Loss: 0.0023(0.0032) Grad: 32994.3672  LR: 0.000016  \n",
      "Epoch: [1][11200/36908] Elapsed 145m 10s (remain 333m 10s) Loss: 0.0000(0.0032) Grad: 123.5184  LR: 0.000015  \n",
      "Epoch: [1][11300/36908] Elapsed 146m 29s (remain 331m 56s) Loss: 0.0000(0.0032) Grad: 106.6930  LR: 0.000015  \n",
      "Epoch: [1][11400/36908] Elapsed 147m 47s (remain 330m 38s) Loss: 0.0150(0.0032) Grad: 162218.3594  LR: 0.000015  \n",
      "Epoch: [1][11500/36908] Elapsed 149m 4s (remain 329m 18s) Loss: 0.0016(0.0032) Grad: 8026.7251  LR: 0.000015  \n",
      "Epoch: [1][11600/36908] Elapsed 150m 22s (remain 328m 1s) Loss: 0.0113(0.0032) Grad: 17765.2402  LR: 0.000015  \n",
      "Epoch: [1][11700/36908] Elapsed 151m 38s (remain 326m 40s) Loss: 0.0002(0.0032) Grad: 2474.4648  LR: 0.000015  \n",
      "Epoch: [1][11800/36908] Elapsed 152m 56s (remain 325m 22s) Loss: 0.0004(0.0032) Grad: 8053.2310  LR: 0.000015  \n",
      "Epoch: [1][11900/36908] Elapsed 154m 13s (remain 324m 3s) Loss: 0.0034(0.0032) Grad: 31416.9961  LR: 0.000015  \n",
      "Epoch: [1][12000/36908] Elapsed 155m 29s (remain 322m 42s) Loss: 0.0000(0.0032) Grad: 3.6456  LR: 0.000015  \n",
      "Epoch: [1][12100/36908] Elapsed 156m 46s (remain 321m 23s) Loss: 0.0008(0.0032) Grad: 3115.8523  LR: 0.000015  \n",
      "Epoch: [1][12200/36908] Elapsed 158m 4s (remain 320m 6s) Loss: 0.0066(0.0032) Grad: 56333.2227  LR: 0.000015  \n",
      "Epoch: [1][12300/36908] Elapsed 159m 22s (remain 318m 48s) Loss: 0.0069(0.0032) Grad: 89488.8906  LR: 0.000015  \n",
      "Epoch: [1][12400/36908] Elapsed 160m 40s (remain 317m 31s) Loss: 0.0000(0.0032) Grad: 77.9252  LR: 0.000015  \n",
      "Epoch: [1][12500/36908] Elapsed 161m 57s (remain 316m 12s) Loss: 0.0016(0.0032) Grad: 7279.2212  LR: 0.000015  \n",
      "Epoch: [1][12600/36908] Elapsed 163m 15s (remain 314m 56s) Loss: 0.0001(0.0032) Grad: 238.2682  LR: 0.000015  \n",
      "Epoch: [1][12700/36908] Elapsed 164m 33s (remain 313m 38s) Loss: 0.0044(0.0032) Grad: 25572.5762  LR: 0.000015  \n",
      "Epoch: [1][12800/36908] Elapsed 165m 51s (remain 312m 20s) Loss: 0.0055(0.0032) Grad: 59735.6016  LR: 0.000015  \n",
      "Epoch: [1][12900/36908] Elapsed 167m 10s (remain 311m 4s) Loss: 0.0196(0.0032) Grad: 21720.5605  LR: 0.000014  \n",
      "Epoch: [1][13000/36908] Elapsed 168m 29s (remain 309m 49s) Loss: 0.0020(0.0032) Grad: 27997.7578  LR: 0.000014  \n",
      "Epoch: [1][13100/36908] Elapsed 169m 47s (remain 308m 31s) Loss: 0.0000(0.0032) Grad: 106.0018  LR: 0.000014  \n",
      "Epoch: [1][13200/36908] Elapsed 171m 4s (remain 307m 13s) Loss: 0.0000(0.0032) Grad: 26.2979  LR: 0.000014  \n",
      "Epoch: [1][13300/36908] Elapsed 172m 21s (remain 305m 54s) Loss: 0.0000(0.0032) Grad: 59.3323  LR: 0.000014  \n",
      "Epoch: [1][13400/36908] Elapsed 173m 38s (remain 304m 35s) Loss: 0.0000(0.0032) Grad: 18.1338  LR: 0.000014  \n",
      "Epoch: [1][13500/36908] Elapsed 174m 54s (remain 303m 14s) Loss: 0.0001(0.0032) Grad: 382.9156  LR: 0.000014  \n",
      "Epoch: [1][13600/36908] Elapsed 176m 11s (remain 301m 56s) Loss: 0.0000(0.0032) Grad: 98.1669  LR: 0.000014  \n",
      "Epoch: [1][13700/36908] Elapsed 177m 29s (remain 300m 37s) Loss: 0.0001(0.0032) Grad: 1077.7480  LR: 0.000014  \n",
      "Epoch: [1][13800/36908] Elapsed 178m 46s (remain 299m 18s) Loss: 0.0063(0.0032) Grad: 102049.7188  LR: 0.000014  \n",
      "Epoch: [1][13900/36908] Elapsed 180m 4s (remain 298m 1s) Loss: 0.0398(0.0032) Grad: 633398.3750  LR: 0.000014  \n",
      "Epoch: [1][14000/36908] Elapsed 181m 21s (remain 296m 43s) Loss: 0.0000(0.0032) Grad: 20.1110  LR: 0.000014  \n",
      "Epoch: [1][14100/36908] Elapsed 182m 38s (remain 295m 24s) Loss: 0.0000(0.0032) Grad: 16.5373  LR: 0.000014  \n",
      "Epoch: [1][14200/36908] Elapsed 183m 55s (remain 294m 4s) Loss: 0.0000(0.0032) Grad: 33.5379  LR: 0.000014  \n",
      "Epoch: [1][14300/36908] Elapsed 185m 12s (remain 292m 47s) Loss: 0.0000(0.0032) Grad: 771.0131  LR: 0.000014  \n",
      "Epoch: [1][14400/36908] Elapsed 186m 30s (remain 291m 29s) Loss: 0.0000(0.0032) Grad: 112.0796  LR: 0.000014  \n",
      "Epoch: [1][14500/36908] Elapsed 187m 47s (remain 290m 9s) Loss: 0.0001(0.0032) Grad: 3450.1760  LR: 0.000013  \n",
      "Epoch: [1][14600/36908] Elapsed 189m 4s (remain 288m 51s) Loss: 0.0119(0.0032) Grad: 23816.3379  LR: 0.000013  \n",
      "Epoch: [1][14700/36908] Elapsed 190m 21s (remain 287m 32s) Loss: 0.0001(0.0032) Grad: 839.4649  LR: 0.000013  \n",
      "Epoch: [1][14800/36908] Elapsed 191m 38s (remain 286m 14s) Loss: 0.0022(0.0032) Grad: 37436.7305  LR: 0.000013  \n",
      "Epoch: [1][14900/36908] Elapsed 192m 56s (remain 284m 57s) Loss: 0.0003(0.0032) Grad: 13503.5547  LR: 0.000013  \n",
      "Epoch: [1][15000/36908] Elapsed 194m 13s (remain 283m 38s) Loss: 0.0000(0.0032) Grad: 3.3765  LR: 0.000013  \n",
      "Epoch: [1][15100/36908] Elapsed 195m 31s (remain 282m 20s) Loss: 0.0001(0.0032) Grad: 457.5739  LR: 0.000013  \n",
      "Epoch: [1][15200/36908] Elapsed 196m 47s (remain 281m 1s) Loss: 0.0001(0.0032) Grad: 1109.3868  LR: 0.000013  \n",
      "Epoch: [1][15300/36908] Elapsed 198m 4s (remain 279m 42s) Loss: 0.0126(0.0032) Grad: 112375.8516  LR: 0.000013  \n",
      "Epoch: [1][15400/36908] Elapsed 199m 22s (remain 278m 25s) Loss: 0.0000(0.0032) Grad: 36.9217  LR: 0.000013  \n",
      "Epoch: [1][15500/36908] Elapsed 200m 40s (remain 277m 7s) Loss: 0.0000(0.0032) Grad: 568.6934  LR: 0.000013  \n",
      "Epoch: [1][15600/36908] Elapsed 201m 58s (remain 275m 50s) Loss: 0.0059(0.0032) Grad: 83814.4375  LR: 0.000013  \n",
      "Epoch: [1][15700/36908] Elapsed 203m 16s (remain 274m 33s) Loss: 0.0004(0.0032) Grad: 17882.0918  LR: 0.000013  \n",
      "Epoch: [1][15800/36908] Elapsed 204m 33s (remain 273m 15s) Loss: 0.0000(0.0032) Grad: 4.1781  LR: 0.000013  \n",
      "Epoch: [1][15900/36908] Elapsed 205m 51s (remain 271m 57s) Loss: 0.0017(0.0032) Grad: 50549.5391  LR: 0.000013  \n",
      "Epoch: [1][16000/36908] Elapsed 207m 8s (remain 270m 39s) Loss: 0.0001(0.0032) Grad: 2696.3892  LR: 0.000013  \n",
      "Epoch: [1][16100/36908] Elapsed 208m 25s (remain 269m 20s) Loss: 0.0002(0.0032) Grad: 6275.8389  LR: 0.000013  \n",
      "Epoch: [1][16200/36908] Elapsed 209m 42s (remain 268m 1s) Loss: 0.0000(0.0032) Grad: 72.3288  LR: 0.000012  \n",
      "Epoch: [1][16300/36908] Elapsed 211m 0s (remain 266m 44s) Loss: 0.0001(0.0032) Grad: 622.5273  LR: 0.000012  \n",
      "Epoch: [1][16400/36908] Elapsed 212m 17s (remain 265m 25s) Loss: 0.0000(0.0032) Grad: 17.6403  LR: 0.000012  \n",
      "Epoch: [1][16500/36908] Elapsed 213m 33s (remain 264m 6s) Loss: 0.0001(0.0032) Grad: 1348.8364  LR: 0.000012  \n",
      "Epoch: [1][16600/36908] Elapsed 214m 49s (remain 262m 46s) Loss: 0.0001(0.0032) Grad: 627.3775  LR: 0.000012  \n",
      "Epoch: [1][16700/36908] Elapsed 216m 7s (remain 261m 29s) Loss: 0.0039(0.0032) Grad: 35753.0820  LR: 0.000012  \n",
      "Epoch: [1][16800/36908] Elapsed 217m 24s (remain 260m 11s) Loss: 0.0000(0.0032) Grad: 10.3876  LR: 0.000012  \n",
      "Epoch: [1][16900/36908] Elapsed 218m 40s (remain 258m 52s) Loss: 0.0234(0.0032) Grad: 40099.1211  LR: 0.000012  \n",
      "Epoch: [1][17000/36908] Elapsed 219m 58s (remain 257m 34s) Loss: 0.0041(0.0032) Grad: 60213.0625  LR: 0.000012  \n",
      "Epoch: [1][17100/36908] Elapsed 221m 15s (remain 256m 16s) Loss: 0.0106(0.0031) Grad: 65775.9766  LR: 0.000012  \n",
      "Epoch: [1][17200/36908] Elapsed 222m 33s (remain 254m 58s) Loss: 0.0052(0.0031) Grad: 54157.8750  LR: 0.000012  \n",
      "Epoch: [1][17300/36908] Elapsed 223m 51s (remain 253m 41s) Loss: 0.0047(0.0031) Grad: 50216.6680  LR: 0.000012  \n",
      "Epoch: [1][17400/36908] Elapsed 225m 8s (remain 252m 23s) Loss: 0.0218(0.0031) Grad: 226138.0625  LR: 0.000012  \n",
      "Epoch: [1][17500/36908] Elapsed 226m 25s (remain 251m 5s) Loss: 0.0076(0.0031) Grad: 136308.5781  LR: 0.000012  \n",
      "Epoch: [1][17600/36908] Elapsed 227m 42s (remain 249m 47s) Loss: 0.0000(0.0031) Grad: 25.7296  LR: 0.000012  \n",
      "Epoch: [1][17700/36908] Elapsed 228m 59s (remain 248m 28s) Loss: 0.0003(0.0031) Grad: 5723.2192  LR: 0.000012  \n",
      "Epoch: [1][17800/36908] Elapsed 230m 17s (remain 247m 11s) Loss: 0.0061(0.0031) Grad: 59360.6602  LR: 0.000012  \n",
      "Epoch: [1][17900/36908] Elapsed 231m 34s (remain 245m 53s) Loss: 0.0000(0.0031) Grad: 1014.3171  LR: 0.000011  \n",
      "Epoch: [1][18000/36908] Elapsed 232m 51s (remain 244m 34s) Loss: 0.0054(0.0031) Grad: 49363.2656  LR: 0.000011  \n",
      "Epoch: [1][18100/36908] Elapsed 234m 8s (remain 243m 16s) Loss: 0.0021(0.0031) Grad: 14828.8916  LR: 0.000011  \n",
      "Epoch: [1][18200/36908] Elapsed 235m 25s (remain 241m 57s) Loss: 0.0006(0.0031) Grad: 36679.9531  LR: 0.000011  \n",
      "Epoch: [1][18300/36908] Elapsed 236m 42s (remain 240m 40s) Loss: 0.0000(0.0031) Grad: 825.1987  LR: 0.000011  \n",
      "Epoch: [1][18400/36908] Elapsed 237m 59s (remain 239m 22s) Loss: 0.0598(0.0031) Grad: 364179.2500  LR: 0.000011  \n",
      "Epoch: [1][18500/36908] Elapsed 239m 17s (remain 238m 4s) Loss: 0.0000(0.0031) Grad: 13.1019  LR: 0.000011  \n",
      "Epoch: [1][18600/36908] Elapsed 240m 35s (remain 236m 47s) Loss: 0.0000(0.0031) Grad: 25.5707  LR: 0.000011  \n",
      "Epoch: [1][18700/36908] Elapsed 241m 51s (remain 235m 28s) Loss: 0.0000(0.0031) Grad: 80.8563  LR: 0.000011  \n",
      "Epoch: [1][18800/36908] Elapsed 243m 9s (remain 234m 11s) Loss: 0.0007(0.0031) Grad: 5073.7910  LR: 0.000011  \n",
      "Epoch: [1][18900/36908] Elapsed 244m 28s (remain 232m 54s) Loss: 0.0008(0.0031) Grad: 5365.8330  LR: 0.000011  \n",
      "Epoch: [1][19000/36908] Elapsed 245m 47s (remain 231m 38s) Loss: 0.0016(0.0031) Grad: 85579.8516  LR: 0.000011  \n",
      "Epoch: [1][19100/36908] Elapsed 247m 5s (remain 230m 21s) Loss: 0.0123(0.0031) Grad: 58766.8008  LR: 0.000011  \n",
      "Epoch: [1][19200/36908] Elapsed 248m 24s (remain 229m 5s) Loss: 0.0000(0.0031) Grad: 49.6663  LR: 0.000011  \n",
      "Epoch: [1][19300/36908] Elapsed 249m 42s (remain 227m 47s) Loss: 0.0000(0.0031) Grad: 73.1902  LR: 0.000011  \n",
      "Epoch: [1][19400/36908] Elapsed 251m 1s (remain 226m 30s) Loss: 0.0000(0.0031) Grad: 86.5367  LR: 0.000011  \n",
      "Epoch: [1][19500/36908] Elapsed 252m 17s (remain 225m 12s) Loss: 0.0000(0.0031) Grad: 9.3933  LR: 0.000010  \n",
      "Epoch: [1][19600/36908] Elapsed 253m 35s (remain 223m 54s) Loss: 0.0000(0.0031) Grad: 12.9523  LR: 0.000010  \n",
      "Epoch: [1][19700/36908] Elapsed 254m 52s (remain 222m 36s) Loss: 0.0000(0.0031) Grad: 263.0516  LR: 0.000010  \n",
      "Epoch: [1][19800/36908] Elapsed 256m 11s (remain 221m 20s) Loss: 0.0001(0.0031) Grad: 2119.6819  LR: 0.000010  \n",
      "Epoch: [1][19900/36908] Elapsed 257m 28s (remain 220m 2s) Loss: 0.0000(0.0031) Grad: 1.9789  LR: 0.000010  \n",
      "Epoch: [1][20000/36908] Elapsed 258m 44s (remain 218m 43s) Loss: 0.0001(0.0031) Grad: 4041.2251  LR: 0.000010  \n",
      "Epoch: [1][20100/36908] Elapsed 260m 1s (remain 217m 25s) Loss: 0.0079(0.0031) Grad: 45026.6797  LR: 0.000010  \n",
      "Epoch: [1][20200/36908] Elapsed 261m 18s (remain 216m 6s) Loss: 0.0000(0.0031) Grad: 4.4421  LR: 0.000010  \n",
      "Epoch: [1][20300/36908] Elapsed 262m 35s (remain 214m 48s) Loss: 0.0000(0.0031) Grad: 53.5436  LR: 0.000010  \n",
      "Epoch: [1][20400/36908] Elapsed 263m 51s (remain 213m 29s) Loss: 0.0000(0.0031) Grad: 37.0721  LR: 0.000010  \n",
      "Epoch: [1][20500/36908] Elapsed 265m 9s (remain 212m 12s) Loss: 0.0058(0.0031) Grad: 25564.2871  LR: 0.000010  \n",
      "Epoch: [1][20600/36908] Elapsed 266m 26s (remain 210m 54s) Loss: 0.0010(0.0031) Grad: 16260.2139  LR: 0.000010  \n",
      "Epoch: [1][20700/36908] Elapsed 267m 44s (remain 209m 36s) Loss: 0.0000(0.0031) Grad: 218.6140  LR: 0.000010  \n",
      "Epoch: [1][20800/36908] Elapsed 269m 3s (remain 208m 20s) Loss: 0.0009(0.0031) Grad: 48874.4531  LR: 0.000010  \n",
      "Epoch: [1][20900/36908] Elapsed 270m 21s (remain 207m 3s) Loss: 0.0001(0.0031) Grad: 3050.7654  LR: 0.000010  \n",
      "Epoch: [1][21000/36908] Elapsed 271m 40s (remain 205m 46s) Loss: 0.0004(0.0031) Grad: 11813.9014  LR: 0.000010  \n",
      "Epoch: [1][21100/36908] Elapsed 272m 58s (remain 204m 29s) Loss: 0.0010(0.0031) Grad: 1818.1973  LR: 0.000010  \n",
      "Epoch: [1][21200/36908] Elapsed 274m 17s (remain 203m 12s) Loss: 0.0001(0.0031) Grad: 2613.6040  LR: 0.000009  \n",
      "Epoch: [1][21300/36908] Elapsed 275m 36s (remain 201m 55s) Loss: 0.0082(0.0031) Grad: 16855.5195  LR: 0.000009  \n",
      "Epoch: [1][21400/36908] Elapsed 276m 53s (remain 200m 38s) Loss: 0.0008(0.0031) Grad: 11620.6426  LR: 0.000009  \n",
      "Epoch: [1][21500/36908] Elapsed 278m 11s (remain 199m 20s) Loss: 0.0000(0.0031) Grad: 55.2850  LR: 0.000009  \n",
      "Epoch: [1][21600/36908] Elapsed 279m 28s (remain 198m 2s) Loss: 0.0002(0.0031) Grad: 7367.0439  LR: 0.000009  \n",
      "Epoch: [1][21700/36908] Elapsed 280m 45s (remain 196m 44s) Loss: 0.0000(0.0031) Grad: 135.4151  LR: 0.000009  \n",
      "Epoch: [1][21800/36908] Elapsed 282m 3s (remain 195m 27s) Loss: 0.0048(0.0031) Grad: 14705.3379  LR: 0.000009  \n",
      "Epoch: [1][21900/36908] Elapsed 283m 22s (remain 194m 10s) Loss: 0.0000(0.0031) Grad: 17.3804  LR: 0.000009  \n",
      "Epoch: [1][22000/36908] Elapsed 284m 40s (remain 192m 53s) Loss: 0.0017(0.0031) Grad: 28610.4961  LR: 0.000009  \n",
      "Epoch: [1][22100/36908] Elapsed 285m 58s (remain 191m 35s) Loss: 0.0020(0.0031) Grad: 34690.3867  LR: 0.000009  \n",
      "Epoch: [1][22200/36908] Elapsed 287m 15s (remain 190m 17s) Loss: 0.0000(0.0031) Grad: 28.1300  LR: 0.000009  \n",
      "Epoch: [1][22300/36908] Elapsed 288m 33s (remain 188m 59s) Loss: 0.0000(0.0031) Grad: 19.0266  LR: 0.000009  \n",
      "Epoch: [1][22400/36908] Elapsed 289m 51s (remain 187m 43s) Loss: 0.0022(0.0031) Grad: 15114.0215  LR: 0.000009  \n",
      "Epoch: [1][22500/36908] Elapsed 291m 9s (remain 186m 25s) Loss: 0.0000(0.0031) Grad: 185.3326  LR: 0.000009  \n",
      "Epoch: [1][22600/36908] Elapsed 292m 28s (remain 185m 8s) Loss: 0.0149(0.0031) Grad: 92784.7734  LR: 0.000009  \n",
      "Epoch: [1][22700/36908] Elapsed 293m 45s (remain 183m 50s) Loss: 0.0017(0.0030) Grad: 10703.6084  LR: 0.000009  \n",
      "Epoch: [1][22800/36908] Elapsed 295m 2s (remain 182m 32s) Loss: 0.0112(0.0030) Grad: 56036.0312  LR: 0.000008  \n",
      "Epoch: [1][22900/36908] Elapsed 296m 20s (remain 181m 15s) Loss: 0.0000(0.0030) Grad: 231.8075  LR: 0.000008  \n",
      "Epoch: [1][23000/36908] Elapsed 297m 37s (remain 179m 57s) Loss: 0.0000(0.0030) Grad: 9.3509  LR: 0.000008  \n",
      "Epoch: [1][23100/36908] Elapsed 298m 53s (remain 178m 38s) Loss: 0.0026(0.0030) Grad: 16916.9473  LR: 0.000008  \n",
      "Epoch: [1][23200/36908] Elapsed 300m 10s (remain 177m 20s) Loss: 0.0002(0.0030) Grad: 4405.9546  LR: 0.000008  \n",
      "Epoch: [1][23300/36908] Elapsed 301m 29s (remain 176m 3s) Loss: 0.0000(0.0030) Grad: 24.1294  LR: 0.000008  \n",
      "Epoch: [1][23400/36908] Elapsed 302m 47s (remain 174m 46s) Loss: 0.0225(0.0030) Grad: 24901.8789  LR: 0.000008  \n",
      "Epoch: [1][23500/36908] Elapsed 304m 5s (remain 173m 28s) Loss: 0.0030(0.0030) Grad: 7082.2402  LR: 0.000008  \n",
      "Epoch: [1][23600/36908] Elapsed 305m 24s (remain 172m 11s) Loss: 0.0064(0.0030) Grad: 53970.9219  LR: 0.000008  \n",
      "Epoch: [1][23700/36908] Elapsed 306m 42s (remain 170m 54s) Loss: 0.0000(0.0030) Grad: 44.9269  LR: 0.000008  \n",
      "Epoch: [1][23800/36908] Elapsed 307m 59s (remain 169m 36s) Loss: 0.0000(0.0030) Grad: 306.7636  LR: 0.000008  \n",
      "Epoch: [1][23900/36908] Elapsed 309m 14s (remain 168m 17s) Loss: 0.0000(0.0030) Grad: 23.2087  LR: 0.000008  \n",
      "Epoch: [1][24000/36908] Elapsed 310m 32s (remain 166m 59s) Loss: 0.0000(0.0030) Grad: 739.1430  LR: 0.000008  \n",
      "Epoch: [1][24100/36908] Elapsed 311m 49s (remain 165m 41s) Loss: 0.0000(0.0030) Grad: 206.3633  LR: 0.000008  \n",
      "Epoch: [1][24200/36908] Elapsed 313m 6s (remain 164m 24s) Loss: 0.0006(0.0030) Grad: 6491.0898  LR: 0.000008  \n",
      "Epoch: [1][24300/36908] Elapsed 314m 24s (remain 163m 6s) Loss: 0.0000(0.0030) Grad: 45.8931  LR: 0.000008  \n",
      "Epoch: [1][24400/36908] Elapsed 315m 42s (remain 161m 49s) Loss: 0.0000(0.0030) Grad: 709.1393  LR: 0.000008  \n",
      "Epoch: [1][24500/36908] Elapsed 317m 0s (remain 160m 31s) Loss: 0.0002(0.0030) Grad: 3742.5483  LR: 0.000007  \n",
      "Epoch: [1][24600/36908] Elapsed 318m 18s (remain 159m 14s) Loss: 0.0000(0.0030) Grad: 194.5602  LR: 0.000007  \n",
      "Epoch: [1][24700/36908] Elapsed 319m 37s (remain 157m 57s) Loss: 0.0007(0.0030) Grad: 22756.6016  LR: 0.000007  \n",
      "Epoch: [1][24800/36908] Elapsed 320m 55s (remain 156m 39s) Loss: 0.0018(0.0030) Grad: 21194.8379  LR: 0.000007  \n",
      "Epoch: [1][24900/36908] Elapsed 322m 12s (remain 155m 21s) Loss: 0.0000(0.0030) Grad: 17.2695  LR: 0.000007  \n",
      "Epoch: [1][25000/36908] Elapsed 323m 28s (remain 154m 3s) Loss: 0.0000(0.0030) Grad: 17.9132  LR: 0.000007  \n",
      "Epoch: [1][25100/36908] Elapsed 324m 45s (remain 152m 45s) Loss: 0.0000(0.0030) Grad: 232.5544  LR: 0.000007  \n",
      "Epoch: [1][25200/36908] Elapsed 326m 3s (remain 151m 28s) Loss: 0.0123(0.0030) Grad: 334772.1250  LR: 0.000007  \n",
      "Epoch: [1][25300/36908] Elapsed 327m 21s (remain 150m 10s) Loss: 0.0000(0.0030) Grad: 85.7542  LR: 0.000007  \n",
      "Epoch: [1][25400/36908] Elapsed 328m 39s (remain 148m 53s) Loss: 0.0000(0.0030) Grad: 6.6269  LR: 0.000007  \n",
      "Epoch: [1][25500/36908] Elapsed 329m 57s (remain 147m 35s) Loss: 0.0010(0.0030) Grad: 59859.8867  LR: 0.000007  \n",
      "Epoch: [1][25600/36908] Elapsed 331m 16s (remain 146m 18s) Loss: 0.0015(0.0030) Grad: 117495.0391  LR: 0.000007  \n",
      "Epoch: [1][25700/36908] Elapsed 332m 33s (remain 145m 0s) Loss: 0.0000(0.0030) Grad: 212.7903  LR: 0.000007  \n",
      "Epoch: [1][25800/36908] Elapsed 333m 51s (remain 143m 43s) Loss: 0.0020(0.0030) Grad: 55347.9531  LR: 0.000007  \n",
      "Epoch: [1][25900/36908] Elapsed 335m 10s (remain 142m 26s) Loss: 0.0000(0.0030) Grad: 157.6289  LR: 0.000007  \n",
      "Epoch: [1][26000/36908] Elapsed 336m 28s (remain 141m 8s) Loss: 0.0000(0.0030) Grad: 55.1095  LR: 0.000007  \n",
      "Epoch: [1][26100/36908] Elapsed 337m 45s (remain 139m 51s) Loss: 0.0007(0.0030) Grad: 7955.2993  LR: 0.000007  \n",
      "Epoch: [1][26200/36908] Elapsed 339m 4s (remain 138m 33s) Loss: 0.0000(0.0030) Grad: 29.8598  LR: 0.000006  \n",
      "Epoch: [1][26300/36908] Elapsed 340m 23s (remain 137m 16s) Loss: 0.0000(0.0030) Grad: 83.5357  LR: 0.000006  \n",
      "Epoch: [1][26400/36908] Elapsed 341m 41s (remain 135m 59s) Loss: 0.0033(0.0030) Grad: 405292.8438  LR: 0.000006  \n",
      "Epoch: [1][26500/36908] Elapsed 342m 59s (remain 134m 41s) Loss: 0.0069(0.0030) Grad: 35744.0430  LR: 0.000006  \n",
      "Epoch: [1][26600/36908] Elapsed 344m 17s (remain 133m 24s) Loss: 0.0017(0.0030) Grad: 57352.6211  LR: 0.000006  \n",
      "Epoch: [1][26700/36908] Elapsed 345m 35s (remain 132m 6s) Loss: 0.0000(0.0030) Grad: 13.0711  LR: 0.000006  \n",
      "Epoch: [1][26800/36908] Elapsed 346m 53s (remain 130m 49s) Loss: 0.0001(0.0030) Grad: 4349.7529  LR: 0.000006  \n",
      "Epoch: [1][26900/36908] Elapsed 348m 12s (remain 129m 32s) Loss: 0.0000(0.0030) Grad: 2659.7041  LR: 0.000006  \n",
      "Epoch: [1][27000/36908] Elapsed 349m 30s (remain 128m 14s) Loss: 0.0001(0.0030) Grad: 8246.1016  LR: 0.000006  \n",
      "Epoch: [1][27100/36908] Elapsed 350m 49s (remain 126m 57s) Loss: 0.0000(0.0030) Grad: 282.0706  LR: 0.000006  \n",
      "Epoch: [1][27200/36908] Elapsed 352m 6s (remain 125m 39s) Loss: 0.0000(0.0030) Grad: 403.0889  LR: 0.000006  \n",
      "Epoch: [1][27300/36908] Elapsed 353m 25s (remain 124m 22s) Loss: 0.0011(0.0030) Grad: 34313.6562  LR: 0.000006  \n",
      "Epoch: [1][27400/36908] Elapsed 354m 43s (remain 123m 4s) Loss: 0.0001(0.0030) Grad: 1387.9447  LR: 0.000006  \n",
      "Epoch: [1][27500/36908] Elapsed 356m 0s (remain 121m 46s) Loss: 0.0000(0.0030) Grad: 10.9147  LR: 0.000006  \n",
      "Epoch: [1][27600/36908] Elapsed 357m 17s (remain 120m 28s) Loss: 0.0000(0.0030) Grad: 11.8476  LR: 0.000006  \n",
      "Epoch: [1][27700/36908] Elapsed 358m 33s (remain 119m 10s) Loss: 0.0052(0.0030) Grad: 164538.5312  LR: 0.000006  \n",
      "Epoch: [1][27800/36908] Elapsed 359m 52s (remain 117m 53s) Loss: 0.0078(0.0030) Grad: 429633.3125  LR: 0.000005  \n",
      "Epoch: [1][27900/36908] Elapsed 361m 10s (remain 116m 35s) Loss: 0.0009(0.0030) Grad: 35630.3672  LR: 0.000005  \n",
      "Epoch: [1][28000/36908] Elapsed 362m 27s (remain 115m 17s) Loss: 0.0021(0.0030) Grad: 44714.1641  LR: 0.000005  \n",
      "Epoch: [1][28100/36908] Elapsed 363m 46s (remain 114m 0s) Loss: 0.0000(0.0030) Grad: 10.5081  LR: 0.000005  \n",
      "Epoch: [1][28200/36908] Elapsed 365m 4s (remain 112m 42s) Loss: 0.0019(0.0030) Grad: 61554.9648  LR: 0.000005  \n",
      "Epoch: [1][28300/36908] Elapsed 366m 21s (remain 111m 25s) Loss: 0.0003(0.0030) Grad: 9835.8711  LR: 0.000005  \n",
      "Epoch: [1][28400/36908] Elapsed 367m 40s (remain 110m 7s) Loss: 0.0000(0.0030) Grad: 11.1376  LR: 0.000005  \n",
      "Epoch: [1][28500/36908] Elapsed 368m 59s (remain 108m 50s) Loss: 0.0000(0.0030) Grad: 7.6427  LR: 0.000005  \n",
      "Epoch: [1][28600/36908] Elapsed 370m 16s (remain 107m 32s) Loss: 0.0003(0.0030) Grad: 15558.8711  LR: 0.000005  \n",
      "Epoch: [1][28700/36908] Elapsed 371m 35s (remain 106m 15s) Loss: 0.0000(0.0029) Grad: 2193.4978  LR: 0.000005  \n",
      "Epoch: [1][28800/36908] Elapsed 372m 53s (remain 104m 57s) Loss: 0.0003(0.0030) Grad: 8155.5439  LR: 0.000005  \n",
      "Epoch: [1][28900/36908] Elapsed 374m 10s (remain 103m 39s) Loss: 0.0000(0.0030) Grad: 5.7105  LR: 0.000005  \n",
      "Epoch: [1][29000/36908] Elapsed 375m 28s (remain 102m 22s) Loss: 0.0029(0.0029) Grad: 49350.8984  LR: 0.000005  \n",
      "Epoch: [1][29100/36908] Elapsed 376m 45s (remain 101m 4s) Loss: 0.0000(0.0029) Grad: 85.9388  LR: 0.000005  \n",
      "Epoch: [1][29200/36908] Elapsed 378m 3s (remain 99m 46s) Loss: 0.0025(0.0029) Grad: 207129.0000  LR: 0.000005  \n",
      "Epoch: [1][29300/36908] Elapsed 379m 20s (remain 98m 28s) Loss: 0.0014(0.0029) Grad: 49839.6484  LR: 0.000005  \n",
      "Epoch: [1][29400/36908] Elapsed 380m 38s (remain 97m 11s) Loss: 0.0000(0.0029) Grad: 155.3525  LR: 0.000005  \n",
      "Epoch: [1][29500/36908] Elapsed 381m 54s (remain 95m 53s) Loss: 0.0000(0.0029) Grad: 13.2769  LR: 0.000004  \n",
      "Epoch: [1][29600/36908] Elapsed 383m 11s (remain 94m 35s) Loss: 0.0000(0.0029) Grad: 7.3529  LR: 0.000004  \n",
      "Epoch: [1][29700/36908] Elapsed 384m 28s (remain 93m 17s) Loss: 0.0008(0.0030) Grad: 11751.2393  LR: 0.000004  \n",
      "Epoch: [1][29800/36908] Elapsed 385m 45s (remain 91m 59s) Loss: 0.0000(0.0029) Grad: 80.1862  LR: 0.000004  \n",
      "Epoch: [1][29900/36908] Elapsed 387m 2s (remain 90m 41s) Loss: 0.0000(0.0029) Grad: 6.7102  LR: 0.000004  \n",
      "Epoch: [1][30000/36908] Elapsed 388m 19s (remain 89m 24s) Loss: 0.0000(0.0029) Grad: 440.9610  LR: 0.000004  \n",
      "Epoch: [1][30100/36908] Elapsed 389m 35s (remain 88m 6s) Loss: 0.0078(0.0029) Grad: 148310.4531  LR: 0.000004  \n",
      "Epoch: [1][30200/36908] Elapsed 390m 51s (remain 86m 48s) Loss: 0.0009(0.0029) Grad: 20057.8477  LR: 0.000004  \n",
      "Epoch: [1][30300/36908] Elapsed 392m 9s (remain 85m 30s) Loss: 0.0000(0.0029) Grad: 212.3535  LR: 0.000004  \n",
      "Epoch: [1][30400/36908] Elapsed 393m 27s (remain 84m 12s) Loss: 0.0048(0.0029) Grad: 38560.7305  LR: 0.000004  \n",
      "Epoch: [1][30500/36908] Elapsed 394m 43s (remain 82m 54s) Loss: 0.0052(0.0029) Grad: 42661.8125  LR: 0.000004  \n",
      "Epoch: [1][30600/36908] Elapsed 396m 1s (remain 81m 37s) Loss: 0.0028(0.0029) Grad: 22723.5098  LR: 0.000004  \n",
      "Epoch: [1][30700/36908] Elapsed 397m 19s (remain 80m 19s) Loss: 0.0000(0.0029) Grad: 9.6231  LR: 0.000004  \n",
      "Epoch: [1][30800/36908] Elapsed 398m 36s (remain 79m 1s) Loss: 0.0009(0.0029) Grad: 3505.8425  LR: 0.000004  \n",
      "Epoch: [1][30900/36908] Elapsed 399m 52s (remain 77m 44s) Loss: 0.0000(0.0029) Grad: 9.2103  LR: 0.000004  \n",
      "Epoch: [1][31000/36908] Elapsed 401m 9s (remain 76m 26s) Loss: 0.0001(0.0029) Grad: 8228.3486  LR: 0.000004  \n",
      "Epoch: [1][31100/36908] Elapsed 402m 26s (remain 75m 8s) Loss: 0.0005(0.0029) Grad: 24149.8691  LR: 0.000003  \n",
      "Epoch: [1][31200/36908] Elapsed 403m 43s (remain 73m 50s) Loss: 0.0009(0.0029) Grad: 23410.3750  LR: 0.000003  \n",
      "Epoch: [1][31300/36908] Elapsed 405m 0s (remain 72m 32s) Loss: 0.0000(0.0029) Grad: 145.9782  LR: 0.000003  \n",
      "Epoch: [1][31400/36908] Elapsed 406m 16s (remain 71m 15s) Loss: 0.0000(0.0029) Grad: 72.6048  LR: 0.000003  \n",
      "Epoch: [1][31500/36908] Elapsed 407m 35s (remain 69m 57s) Loss: 0.0001(0.0029) Grad: 20064.3418  LR: 0.000003  \n",
      "Epoch: [1][31600/36908] Elapsed 408m 51s (remain 68m 39s) Loss: 0.0000(0.0029) Grad: 6.7280  LR: 0.000003  \n",
      "Epoch: [1][31700/36908] Elapsed 410m 9s (remain 67m 22s) Loss: 0.0388(0.0029) Grad: 464015.2812  LR: 0.000003  \n",
      "Epoch: [1][31800/36908] Elapsed 411m 26s (remain 66m 4s) Loss: 0.0022(0.0029) Grad: 30450.5527  LR: 0.000003  \n",
      "Epoch: [1][31900/36908] Elapsed 412m 44s (remain 64m 46s) Loss: 0.0000(0.0029) Grad: 13.4670  LR: 0.000003  \n",
      "Epoch: [1][32000/36908] Elapsed 414m 3s (remain 63m 29s) Loss: 0.0000(0.0029) Grad: 6.6301  LR: 0.000003  \n",
      "Epoch: [1][32100/36908] Elapsed 415m 20s (remain 62m 11s) Loss: 0.0000(0.0029) Grad: 5.1798  LR: 0.000003  \n",
      "Epoch: [1][32200/36908] Elapsed 416m 38s (remain 60m 54s) Loss: 0.0000(0.0029) Grad: 8.5314  LR: 0.000003  \n",
      "Epoch: [1][32300/36908] Elapsed 417m 56s (remain 59m 36s) Loss: 0.0000(0.0029) Grad: 9.2527  LR: 0.000003  \n",
      "Epoch: [1][32400/36908] Elapsed 419m 14s (remain 58m 19s) Loss: 0.0000(0.0029) Grad: 3.8351  LR: 0.000003  \n",
      "Epoch: [1][32500/36908] Elapsed 420m 31s (remain 57m 1s) Loss: 0.0004(0.0029) Grad: 2466.0554  LR: 0.000003  \n",
      "Epoch: [1][32600/36908] Elapsed 421m 49s (remain 55m 43s) Loss: 0.0008(0.0029) Grad: 7343.1636  LR: 0.000003  \n",
      "Epoch: [1][32700/36908] Elapsed 423m 5s (remain 54m 25s) Loss: 0.0000(0.0029) Grad: 220.2390  LR: 0.000003  \n",
      "Epoch: [1][32800/36908] Elapsed 424m 22s (remain 53m 8s) Loss: 0.0000(0.0029) Grad: 2.1061  LR: 0.000002  \n",
      "Epoch: [1][32900/36908] Elapsed 425m 39s (remain 51m 50s) Loss: 0.0000(0.0029) Grad: 21.1594  LR: 0.000002  \n",
      "Epoch: [1][33000/36908] Elapsed 426m 56s (remain 50m 32s) Loss: 0.0000(0.0029) Grad: 13.2112  LR: 0.000002  \n",
      "Epoch: [1][33100/36908] Elapsed 428m 13s (remain 49m 15s) Loss: 0.0001(0.0029) Grad: 1308.6516  LR: 0.000002  \n",
      "Epoch: [1][33200/36908] Elapsed 429m 31s (remain 47m 57s) Loss: 0.0008(0.0029) Grad: 5552.4238  LR: 0.000002  \n",
      "Epoch: [1][33300/36908] Elapsed 430m 50s (remain 46m 40s) Loss: 0.0000(0.0029) Grad: 210.5753  LR: 0.000002  \n",
      "Epoch: [1][33400/36908] Elapsed 432m 9s (remain 45m 22s) Loss: 0.0000(0.0029) Grad: 53.4871  LR: 0.000002  \n",
      "Epoch: [1][33500/36908] Elapsed 433m 27s (remain 44m 4s) Loss: 0.0000(0.0029) Grad: 17.8015  LR: 0.000002  \n",
      "Epoch: [1][33600/36908] Elapsed 434m 44s (remain 42m 47s) Loss: 0.0011(0.0029) Grad: 5197.9712  LR: 0.000002  \n",
      "Epoch: [1][33700/36908] Elapsed 436m 2s (remain 41m 29s) Loss: 0.0000(0.0029) Grad: 60.6750  LR: 0.000002  \n",
      "Epoch: [1][33800/36908] Elapsed 437m 20s (remain 40m 12s) Loss: 0.0004(0.0029) Grad: 2705.3018  LR: 0.000002  \n",
      "Epoch: [1][33900/36908] Elapsed 438m 39s (remain 38m 54s) Loss: 0.0000(0.0029) Grad: 248.6700  LR: 0.000002  \n",
      "Epoch: [1][34000/36908] Elapsed 439m 55s (remain 37m 36s) Loss: 0.0022(0.0029) Grad: 29588.2441  LR: 0.000002  \n",
      "Epoch: [1][34100/36908] Elapsed 441m 13s (remain 36m 19s) Loss: 0.0000(0.0029) Grad: 17.3738  LR: 0.000002  \n",
      "Epoch: [1][34200/36908] Elapsed 442m 29s (remain 35m 1s) Loss: 0.0000(0.0029) Grad: 9.4669  LR: 0.000002  \n",
      "Epoch: [1][34300/36908] Elapsed 443m 47s (remain 33m 43s) Loss: 0.0000(0.0029) Grad: 8.9096  LR: 0.000002  \n",
      "Epoch: [1][34400/36908] Elapsed 445m 4s (remain 32m 26s) Loss: 0.0000(0.0029) Grad: 251.0579  LR: 0.000002  \n",
      "Epoch: [1][34500/36908] Elapsed 446m 21s (remain 31m 8s) Loss: 0.0025(0.0029) Grad: 36436.4844  LR: 0.000001  \n",
      "Epoch: [1][34600/36908] Elapsed 447m 38s (remain 29m 50s) Loss: 0.0000(0.0029) Grad: 220.3639  LR: 0.000001  \n",
      "Epoch: [1][34700/36908] Elapsed 448m 54s (remain 28m 33s) Loss: 0.0000(0.0029) Grad: 12.0669  LR: 0.000001  \n",
      "Epoch: [1][34800/36908] Elapsed 450m 12s (remain 27m 15s) Loss: 0.0000(0.0029) Grad: 92.3325  LR: 0.000001  \n",
      "Epoch: [1][34900/36908] Elapsed 451m 28s (remain 25m 57s) Loss: 0.0061(0.0029) Grad: 181835.9531  LR: 0.000001  \n",
      "Epoch: [1][35000/36908] Elapsed 452m 46s (remain 24m 40s) Loss: 0.0006(0.0029) Grad: 10085.1445  LR: 0.000001  \n",
      "Epoch: [1][35100/36908] Elapsed 454m 3s (remain 23m 22s) Loss: 0.0051(0.0029) Grad: 24055.9004  LR: 0.000001  \n",
      "Epoch: [1][35200/36908] Elapsed 455m 19s (remain 22m 4s) Loss: 0.0000(0.0029) Grad: 15.8028  LR: 0.000001  \n",
      "Epoch: [1][35300/36908] Elapsed 456m 37s (remain 20m 47s) Loss: 0.0019(0.0029) Grad: 76578.8125  LR: 0.000001  \n",
      "Epoch: [1][35400/36908] Elapsed 457m 57s (remain 19m 29s) Loss: 0.0000(0.0029) Grad: 31.8662  LR: 0.000001  \n",
      "Epoch: [1][35500/36908] Elapsed 459m 14s (remain 18m 12s) Loss: 0.0000(0.0029) Grad: 303.6560  LR: 0.000001  \n",
      "Epoch: [1][35600/36908] Elapsed 460m 32s (remain 16m 54s) Loss: 0.0000(0.0029) Grad: 5.2827  LR: 0.000001  \n",
      "Epoch: [1][35700/36908] Elapsed 461m 49s (remain 15m 36s) Loss: 0.0005(0.0029) Grad: 16726.1113  LR: 0.000001  \n",
      "Epoch: [1][35800/36908] Elapsed 463m 6s (remain 14m 19s) Loss: 0.0031(0.0029) Grad: 38290.2812  LR: 0.000001  \n",
      "Epoch: [1][35900/36908] Elapsed 464m 22s (remain 13m 1s) Loss: 0.0004(0.0029) Grad: 1933.4780  LR: 0.000001  \n",
      "Epoch: [1][36000/36908] Elapsed 465m 40s (remain 11m 43s) Loss: 0.0053(0.0029) Grad: 54414.4414  LR: 0.000001  \n",
      "Epoch: [1][36100/36908] Elapsed 466m 58s (remain 10m 26s) Loss: 0.0000(0.0029) Grad: 3.1689  LR: 0.000000  \n",
      "Epoch: [1][36200/36908] Elapsed 468m 15s (remain 9m 8s) Loss: 0.0150(0.0028) Grad: 109290.3359  LR: 0.000000  \n",
      "Epoch: [1][36300/36908] Elapsed 469m 33s (remain 7m 51s) Loss: 0.0012(0.0028) Grad: 17889.7695  LR: 0.000000  \n",
      "Epoch: [1][36400/36908] Elapsed 470m 51s (remain 6m 33s) Loss: 0.0001(0.0028) Grad: 5832.1377  LR: 0.000000  \n",
      "Epoch: [1][36500/36908] Elapsed 472m 7s (remain 5m 15s) Loss: 0.0011(0.0028) Grad: 110239.8203  LR: 0.000000  \n",
      "Epoch: [1][36600/36908] Elapsed 473m 24s (remain 3m 58s) Loss: 0.0030(0.0028) Grad: 50994.8242  LR: 0.000000  \n",
      "Epoch: [1][36700/36908] Elapsed 474m 41s (remain 2m 40s) Loss: 0.0004(0.0028) Grad: 25308.9863  LR: 0.000000  \n",
      "Epoch: [1][36800/36908] Elapsed 475m 57s (remain 1m 23s) Loss: 0.0000(0.0028) Grad: 353.4818  LR: 0.000000  \n",
      "Epoch: [1][36900/36908] Elapsed 477m 14s (remain 0m 5s) Loss: 0.0000(0.0028) Grad: 153.6842  LR: 0.000000  \n",
      "Epoch: [1][36907/36908] Elapsed 477m 19s (remain 0m 0s) Loss: 0.0000(0.0028) Grad: 276.2877  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 1s (remain 21m 30s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 31s (remain 5m 37s) Loss: 0.0725(0.0119) \n",
      "EVAL: [200/1192] Elapsed 1m 1s (remain 5m 5s) Loss: 0.0210(0.0119) \n",
      "EVAL: [300/1192] Elapsed 1m 32s (remain 4m 32s) Loss: 0.0125(0.0110) \n",
      "EVAL: [400/1192] Elapsed 2m 2s (remain 4m 2s) Loss: 0.0000(0.0117) \n",
      "EVAL: [500/1192] Elapsed 2m 33s (remain 3m 31s) Loss: 0.0000(0.0108) \n",
      "EVAL: [600/1192] Elapsed 3m 3s (remain 3m 0s) Loss: 0.0162(0.0110) \n",
      "EVAL: [700/1192] Elapsed 3m 34s (remain 2m 29s) Loss: 0.0157(0.0125) \n",
      "EVAL: [800/1192] Elapsed 4m 4s (remain 1m 59s) Loss: 0.0000(0.0123) \n",
      "EVAL: [900/1192] Elapsed 4m 34s (remain 1m 28s) Loss: 0.0191(0.0124) \n",
      "EVAL: [1000/1192] Elapsed 5m 4s (remain 0m 58s) Loss: 0.0000(0.0121) \n",
      "EVAL: [1100/1192] Elapsed 5m 35s (remain 0m 27s) Loss: 0.0389(0.0117) \n",
      "EVAL: [1191/1192] Elapsed 6m 3s (remain 0m 0s) Loss: 0.0000(0.0111) \n",
      "Epoch 1 - avg_train_loss: 0.0028  avg_val_loss: 0.0111  time: 29008s\n",
      "Epoch 1 - Score: 0.8959\n",
      "Epoch 1 - Save Best Score: 0.8959 Model\n",
      "========== fold: 3 training ==========\n",
      "get pseudo plain from ../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_3.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_3.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp083/pseudo_labels_3.npy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ef84244fc048a69b067a5f27acec62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf9cb0279154d20ba4789bd72976e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(612602, 950)\n",
      "(612602, 6) (612602, 950)\n",
      "(100000, 7)\n",
      "(110725, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n",
      "load weights from ../output/nbme-score-clinical-patient-notes/nbme-exp087/fold3_best.pth\n",
      "Epoch: [1][0/36908] Elapsed 0m 1s (remain 966m 47s) Loss: 0.0009(0.0009) Grad: 3397.6973  LR: 0.000000  \n",
      "Epoch: [1][100/36908] Elapsed 1m 18s (remain 475m 30s) Loss: 0.0200(0.0017) Grad: 17870.0977  LR: 0.000001  \n",
      "Epoch: [1][200/36908] Elapsed 2m 36s (remain 475m 19s) Loss: 0.0000(0.0019) Grad: 5.5631  LR: 0.000001  \n",
      "Epoch: [1][300/36908] Elapsed 3m 53s (remain 473m 0s) Loss: 0.0113(0.0022) Grad: 37953.2695  LR: 0.000002  \n",
      "Epoch: [1][400/36908] Elapsed 5m 10s (remain 471m 37s) Loss: 0.0130(0.0024) Grad: 30039.5840  LR: 0.000002  \n",
      "Epoch: [1][500/36908] Elapsed 6m 29s (remain 471m 21s) Loss: 0.0024(0.0024) Grad: 6314.7222  LR: 0.000003  \n",
      "Epoch: [1][600/36908] Elapsed 7m 48s (remain 471m 21s) Loss: 0.0008(0.0024) Grad: 6060.8145  LR: 0.000003  \n",
      "Epoch: [1][700/36908] Elapsed 9m 6s (remain 470m 27s) Loss: 0.0087(0.0023) Grad: 43092.6797  LR: 0.000004  \n",
      "Epoch: [1][800/36908] Elapsed 10m 24s (remain 468m 55s) Loss: 0.0065(0.0024) Grad: 17224.8809  LR: 0.000004  \n",
      "Epoch: [1][900/36908] Elapsed 11m 43s (remain 468m 22s) Loss: 0.0000(0.0025) Grad: 62.0125  LR: 0.000005  \n",
      "Epoch: [1][1000/36908] Elapsed 13m 1s (remain 466m 58s) Loss: 0.0041(0.0024) Grad: 5182.5229  LR: 0.000005  \n",
      "Epoch: [1][1100/36908] Elapsed 14m 19s (remain 466m 0s) Loss: 0.0003(0.0023) Grad: 712.5157  LR: 0.000006  \n",
      "Epoch: [1][1200/36908] Elapsed 15m 37s (remain 464m 44s) Loss: 0.0000(0.0024) Grad: 17.4454  LR: 0.000007  \n",
      "Epoch: [1][1300/36908] Elapsed 16m 57s (remain 463m 55s) Loss: 0.0000(0.0023) Grad: 167.3742  LR: 0.000007  \n",
      "Epoch: [1][1400/36908] Elapsed 18m 15s (remain 462m 46s) Loss: 0.0000(0.0023) Grad: 15.4136  LR: 0.000008  \n",
      "Epoch: [1][1500/36908] Elapsed 19m 33s (remain 461m 16s) Loss: 0.0000(0.0022) Grad: 6.6975  LR: 0.000008  \n",
      "Epoch: [1][1600/36908] Elapsed 20m 50s (remain 459m 45s) Loss: 0.0000(0.0023) Grad: 162.4931  LR: 0.000009  \n",
      "Epoch: [1][1700/36908] Elapsed 22m 9s (remain 458m 33s) Loss: 0.0000(0.0023) Grad: 32.7540  LR: 0.000009  \n",
      "Epoch: [1][1800/36908] Elapsed 23m 26s (remain 457m 0s) Loss: 0.0006(0.0023) Grad: 10729.3291  LR: 0.000010  \n",
      "Epoch: [1][1900/36908] Elapsed 24m 44s (remain 455m 34s) Loss: 0.0008(0.0023) Grad: 27210.6152  LR: 0.000010  \n",
      "Epoch: [1][2000/36908] Elapsed 26m 3s (remain 454m 26s) Loss: 0.0027(0.0023) Grad: 4602.4644  LR: 0.000011  \n",
      "Epoch: [1][2100/36908] Elapsed 27m 20s (remain 452m 55s) Loss: 0.0001(0.0023) Grad: 486.8757  LR: 0.000011  \n",
      "Epoch: [1][2200/36908] Elapsed 28m 37s (remain 451m 27s) Loss: 0.0000(0.0024) Grad: 197.7673  LR: 0.000012  \n",
      "Epoch: [1][2300/36908] Elapsed 29m 54s (remain 449m 52s) Loss: 0.0000(0.0023) Grad: 2.8858  LR: 0.000012  \n",
      "Epoch: [1][2400/36908] Elapsed 31m 11s (remain 448m 16s) Loss: 0.0092(0.0023) Grad: 23380.9336  LR: 0.000013  \n",
      "Epoch: [1][2500/36908] Elapsed 32m 29s (remain 447m 1s) Loss: 0.0015(0.0024) Grad: 4908.1406  LR: 0.000014  \n",
      "Epoch: [1][2600/36908] Elapsed 33m 46s (remain 445m 28s) Loss: 0.0002(0.0024) Grad: 926.2933  LR: 0.000014  \n",
      "Epoch: [1][2700/36908] Elapsed 35m 4s (remain 444m 17s) Loss: 0.0000(0.0024) Grad: 402.8616  LR: 0.000015  \n",
      "Epoch: [1][2800/36908] Elapsed 36m 22s (remain 442m 54s) Loss: 0.0019(0.0024) Grad: 10401.6318  LR: 0.000015  \n",
      "Epoch: [1][2900/36908] Elapsed 37m 38s (remain 441m 16s) Loss: 0.0001(0.0024) Grad: 2577.3013  LR: 0.000016  \n",
      "Epoch: [1][3000/36908] Elapsed 38m 55s (remain 439m 52s) Loss: 0.0001(0.0024) Grad: 457.9572  LR: 0.000016  \n",
      "Epoch: [1][3100/36908] Elapsed 40m 12s (remain 438m 16s) Loss: 0.0009(0.0024) Grad: 8056.9141  LR: 0.000017  \n",
      "Epoch: [1][3200/36908] Elapsed 41m 31s (remain 437m 13s) Loss: 0.0000(0.0025) Grad: 30.3440  LR: 0.000017  \n",
      "Epoch: [1][3300/36908] Elapsed 42m 50s (remain 436m 10s) Loss: 0.0002(0.0025) Grad: 932.9982  LR: 0.000018  \n",
      "Epoch: [1][3400/36908] Elapsed 44m 8s (remain 434m 54s) Loss: 0.0000(0.0025) Grad: 5.6880  LR: 0.000018  \n",
      "Epoch: [1][3500/36908] Elapsed 45m 27s (remain 433m 48s) Loss: 0.0569(0.0025) Grad: 231743.5000  LR: 0.000019  \n",
      "Epoch: [1][3600/36908] Elapsed 46m 44s (remain 432m 24s) Loss: 0.0052(0.0025) Grad: 44487.8945  LR: 0.000020  \n",
      "Epoch: [1][3700/36908] Elapsed 48m 3s (remain 431m 14s) Loss: 0.0003(0.0025) Grad: 2090.1787  LR: 0.000020  \n",
      "Epoch: [1][3800/36908] Elapsed 49m 21s (remain 429m 52s) Loss: 0.0000(0.0025) Grad: 221.4123  LR: 0.000020  \n",
      "Epoch: [1][3900/36908] Elapsed 50m 39s (remain 428m 34s) Loss: 0.0000(0.0025) Grad: 17.1239  LR: 0.000020  \n",
      "Epoch: [1][4000/36908] Elapsed 51m 56s (remain 427m 16s) Loss: 0.0001(0.0025) Grad: 1979.6790  LR: 0.000020  \n",
      "Epoch: [1][4100/36908] Elapsed 53m 14s (remain 425m 56s) Loss: 0.0000(0.0025) Grad: 69.8131  LR: 0.000020  \n",
      "Epoch: [1][4200/36908] Elapsed 54m 34s (remain 424m 50s) Loss: 0.0000(0.0025) Grad: 14.2100  LR: 0.000020  \n",
      "Epoch: [1][4300/36908] Elapsed 55m 53s (remain 423m 41s) Loss: 0.0025(0.0025) Grad: 14641.9697  LR: 0.000020  \n",
      "Epoch: [1][4400/36908] Elapsed 57m 11s (remain 422m 22s) Loss: 0.0000(0.0025) Grad: 11.1417  LR: 0.000020  \n",
      "Epoch: [1][4500/36908] Elapsed 58m 30s (remain 421m 18s) Loss: 0.0000(0.0025) Grad: 3.3098  LR: 0.000020  \n",
      "Epoch: [1][4600/36908] Elapsed 59m 48s (remain 419m 59s) Loss: 0.0000(0.0025) Grad: 481.4914  LR: 0.000019  \n",
      "Epoch: [1][4700/36908] Elapsed 61m 6s (remain 418m 41s) Loss: 0.0000(0.0025) Grad: 139.0233  LR: 0.000019  \n",
      "Epoch: [1][4800/36908] Elapsed 62m 25s (remain 417m 30s) Loss: 0.0000(0.0025) Grad: 583.1606  LR: 0.000019  \n",
      "Epoch: [1][4900/36908] Elapsed 63m 43s (remain 416m 9s) Loss: 0.0001(0.0025) Grad: 568.4392  LR: 0.000019  \n",
      "Epoch: [1][5000/36908] Elapsed 65m 1s (remain 414m 52s) Loss: 0.0002(0.0025) Grad: 1344.9562  LR: 0.000019  \n",
      "Epoch: [1][5100/36908] Elapsed 66m 21s (remain 413m 43s) Loss: 0.0000(0.0026) Grad: 76.2348  LR: 0.000019  \n",
      "Epoch: [1][5200/36908] Elapsed 67m 38s (remain 412m 20s) Loss: 0.0073(0.0026) Grad: 14673.3154  LR: 0.000019  \n",
      "Epoch: [1][5300/36908] Elapsed 68m 56s (remain 411m 3s) Loss: 0.0000(0.0026) Grad: 12.0304  LR: 0.000019  \n",
      "Epoch: [1][5400/36908] Elapsed 70m 13s (remain 409m 37s) Loss: 0.0001(0.0026) Grad: 507.5907  LR: 0.000019  \n",
      "Epoch: [1][5500/36908] Elapsed 71m 30s (remain 408m 13s) Loss: 0.0000(0.0026) Grad: 15.0017  LR: 0.000019  \n",
      "Epoch: [1][5600/36908] Elapsed 72m 48s (remain 406m 59s) Loss: 0.0001(0.0026) Grad: 2483.9084  LR: 0.000019  \n",
      "Epoch: [1][5700/36908] Elapsed 74m 7s (remain 405m 43s) Loss: 0.0078(0.0026) Grad: 35096.4492  LR: 0.000019  \n",
      "Epoch: [1][5800/36908] Elapsed 75m 25s (remain 404m 27s) Loss: 0.0000(0.0026) Grad: 101.4266  LR: 0.000019  \n",
      "Epoch: [1][5900/36908] Elapsed 76m 42s (remain 403m 5s) Loss: 0.0000(0.0026) Grad: 17.6599  LR: 0.000019  \n",
      "Epoch: [1][6000/36908] Elapsed 77m 59s (remain 401m 41s) Loss: 0.0000(0.0026) Grad: 270.4281  LR: 0.000019  \n",
      "Epoch: [1][6100/36908] Elapsed 79m 17s (remain 400m 21s) Loss: 0.0005(0.0026) Grad: 2455.2825  LR: 0.000019  \n",
      "Epoch: [1][6200/36908] Elapsed 80m 33s (remain 398m 57s) Loss: 0.0000(0.0026) Grad: 31.6292  LR: 0.000018  \n",
      "Epoch: [1][6300/36908] Elapsed 81m 51s (remain 397m 37s) Loss: 0.0010(0.0026) Grad: 20289.1699  LR: 0.000018  \n",
      "Epoch: [1][6400/36908] Elapsed 83m 9s (remain 396m 21s) Loss: 0.0017(0.0026) Grad: 15923.8359  LR: 0.000018  \n",
      "Epoch: [1][6500/36908] Elapsed 84m 27s (remain 395m 0s) Loss: 0.0079(0.0026) Grad: 118276.0938  LR: 0.000018  \n",
      "Epoch: [1][6600/36908] Elapsed 85m 46s (remain 393m 46s) Loss: 0.0016(0.0026) Grad: 17976.0195  LR: 0.000018  \n",
      "Epoch: [1][6700/36908] Elapsed 87m 3s (remain 392m 28s) Loss: 0.0000(0.0026) Grad: 16.9637  LR: 0.000018  \n",
      "Epoch: [1][6800/36908] Elapsed 88m 20s (remain 391m 6s) Loss: 0.0000(0.0026) Grad: 19.1009  LR: 0.000018  \n",
      "Epoch: [1][6900/36908] Elapsed 89m 39s (remain 389m 49s) Loss: 0.0012(0.0026) Grad: 6685.4541  LR: 0.000018  \n",
      "Epoch: [1][7000/36908] Elapsed 90m 57s (remain 388m 32s) Loss: 0.0000(0.0026) Grad: 14.1676  LR: 0.000018  \n",
      "Epoch: [1][7100/36908] Elapsed 92m 14s (remain 387m 11s) Loss: 0.0000(0.0026) Grad: 27.7242  LR: 0.000018  \n",
      "Epoch: [1][7200/36908] Elapsed 93m 30s (remain 385m 47s) Loss: 0.0000(0.0026) Grad: 51.0398  LR: 0.000018  \n",
      "Epoch: [1][7300/36908] Elapsed 94m 47s (remain 384m 24s) Loss: 0.0047(0.0026) Grad: 132362.6250  LR: 0.000018  \n",
      "Epoch: [1][7400/36908] Elapsed 96m 5s (remain 383m 7s) Loss: 0.0000(0.0026) Grad: 182.0618  LR: 0.000018  \n",
      "Epoch: [1][7500/36908] Elapsed 97m 23s (remain 381m 48s) Loss: 0.0000(0.0026) Grad: 66.7625  LR: 0.000018  \n",
      "Epoch: [1][7600/36908] Elapsed 98m 41s (remain 380m 32s) Loss: 0.0000(0.0027) Grad: 17.3862  LR: 0.000018  \n",
      "Epoch: [1][7700/36908] Elapsed 99m 59s (remain 379m 15s) Loss: 0.0003(0.0027) Grad: 5765.3076  LR: 0.000018  \n",
      "Epoch: [1][7800/36908] Elapsed 101m 17s (remain 377m 56s) Loss: 0.0419(0.0027) Grad: 131130.8906  LR: 0.000018  \n",
      "Epoch: [1][7900/36908] Elapsed 102m 34s (remain 376m 36s) Loss: 0.0101(0.0027) Grad: 52161.6133  LR: 0.000017  \n",
      "Epoch: [1][8000/36908] Elapsed 103m 53s (remain 375m 22s) Loss: 0.0023(0.0027) Grad: 340103.5625  LR: 0.000017  \n",
      "Epoch: [1][8100/36908] Elapsed 105m 11s (remain 374m 2s) Loss: 0.0005(0.0027) Grad: 7112.6880  LR: 0.000017  \n",
      "Epoch: [1][8200/36908] Elapsed 106m 29s (remain 372m 46s) Loss: 0.0001(0.0027) Grad: 1228.4105  LR: 0.000017  \n",
      "Epoch: [1][8300/36908] Elapsed 107m 46s (remain 371m 26s) Loss: 0.0100(0.0027) Grad: 257261.5312  LR: 0.000017  \n",
      "Epoch: [1][8400/36908] Elapsed 109m 4s (remain 370m 5s) Loss: 0.0000(0.0027) Grad: 136.0242  LR: 0.000017  \n",
      "Epoch: [1][8500/36908] Elapsed 110m 20s (remain 368m 43s) Loss: 0.0091(0.0027) Grad: 105246.7109  LR: 0.000017  \n",
      "Epoch: [1][8600/36908] Elapsed 111m 38s (remain 367m 26s) Loss: 0.0000(0.0027) Grad: 27.5480  LR: 0.000017  \n",
      "Epoch: [1][8700/36908] Elapsed 112m 55s (remain 366m 6s) Loss: 0.0001(0.0027) Grad: 7785.8364  LR: 0.000017  \n",
      "Epoch: [1][8800/36908] Elapsed 114m 13s (remain 364m 47s) Loss: 0.0008(0.0027) Grad: 10637.0020  LR: 0.000017  \n",
      "Epoch: [1][8900/36908] Elapsed 115m 31s (remain 363m 29s) Loss: 0.0007(0.0027) Grad: 32240.3477  LR: 0.000017  \n",
      "Epoch: [1][9000/36908] Elapsed 116m 48s (remain 362m 10s) Loss: 0.0000(0.0027) Grad: 39.9114  LR: 0.000017  \n",
      "Epoch: [1][9100/36908] Elapsed 118m 8s (remain 360m 56s) Loss: 0.0009(0.0028) Grad: 9241.0576  LR: 0.000017  \n",
      "Epoch: [1][9200/36908] Elapsed 119m 26s (remain 359m 40s) Loss: 0.0000(0.0027) Grad: 686.8056  LR: 0.000017  \n",
      "Epoch: [1][9300/36908] Elapsed 120m 45s (remain 358m 26s) Loss: 0.0188(0.0028) Grad: 266816.1562  LR: 0.000017  \n",
      "Epoch: [1][9400/36908] Elapsed 122m 4s (remain 357m 9s) Loss: 0.0000(0.0028) Grad: 27.3669  LR: 0.000017  \n",
      "Epoch: [1][9500/36908] Elapsed 123m 22s (remain 355m 54s) Loss: 0.0000(0.0028) Grad: 348.7115  LR: 0.000017  \n",
      "Epoch: [1][9600/36908] Elapsed 124m 41s (remain 354m 37s) Loss: 0.0000(0.0028) Grad: 9.8547  LR: 0.000016  \n",
      "Epoch: [1][9700/36908] Elapsed 125m 58s (remain 353m 18s) Loss: 0.0073(0.0028) Grad: 45662.2773  LR: 0.000016  \n",
      "Epoch: [1][9800/36908] Elapsed 127m 14s (remain 351m 55s) Loss: 0.0001(0.0028) Grad: 5232.0850  LR: 0.000016  \n",
      "Epoch: [1][9900/36908] Elapsed 128m 33s (remain 350m 38s) Loss: 0.0000(0.0028) Grad: 13.0159  LR: 0.000016  \n",
      "Epoch: [1][10000/36908] Elapsed 129m 51s (remain 349m 21s) Loss: 0.0001(0.0028) Grad: 4367.2446  LR: 0.000016  \n",
      "Epoch: [1][10100/36908] Elapsed 131m 9s (remain 348m 4s) Loss: 0.0000(0.0028) Grad: 772.4274  LR: 0.000016  \n",
      "Epoch: [1][10200/36908] Elapsed 132m 28s (remain 346m 49s) Loss: 0.0063(0.0028) Grad: 131348.9062  LR: 0.000016  \n",
      "Epoch: [1][10300/36908] Elapsed 133m 46s (remain 345m 33s) Loss: 0.0001(0.0028) Grad: 5539.2197  LR: 0.000016  \n",
      "Epoch: [1][10400/36908] Elapsed 135m 5s (remain 344m 16s) Loss: 0.0000(0.0028) Grad: 23.1310  LR: 0.000016  \n",
      "Epoch: [1][10500/36908] Elapsed 136m 23s (remain 342m 59s) Loss: 0.0000(0.0028) Grad: 696.6461  LR: 0.000016  \n",
      "Epoch: [1][10600/36908] Elapsed 137m 42s (remain 341m 42s) Loss: 0.0010(0.0028) Grad: 44799.6406  LR: 0.000016  \n",
      "Epoch: [1][10700/36908] Elapsed 139m 0s (remain 340m 25s) Loss: 0.0457(0.0028) Grad: 150552.9531  LR: 0.000016  \n",
      "Epoch: [1][10800/36908] Elapsed 140m 17s (remain 339m 5s) Loss: 0.0016(0.0028) Grad: 50722.6719  LR: 0.000016  \n",
      "Epoch: [1][10900/36908] Elapsed 141m 36s (remain 337m 49s) Loss: 0.0000(0.0028) Grad: 68.3435  LR: 0.000016  \n",
      "Epoch: [1][11000/36908] Elapsed 142m 53s (remain 336m 30s) Loss: 0.0000(0.0028) Grad: 11.7623  LR: 0.000016  \n",
      "Epoch: [1][11100/36908] Elapsed 144m 11s (remain 335m 13s) Loss: 0.0002(0.0028) Grad: 4528.7085  LR: 0.000016  \n",
      "Epoch: [1][11200/36908] Elapsed 145m 31s (remain 333m 58s) Loss: 0.0000(0.0028) Grad: 12.0037  LR: 0.000015  \n",
      "Epoch: [1][11300/36908] Elapsed 146m 49s (remain 332m 40s) Loss: 0.0000(0.0028) Grad: 11.9194  LR: 0.000015  \n",
      "Epoch: [1][11400/36908] Elapsed 148m 6s (remain 331m 22s) Loss: 0.0000(0.0028) Grad: 91.8511  LR: 0.000015  \n",
      "Epoch: [1][11500/36908] Elapsed 149m 25s (remain 330m 6s) Loss: 0.0000(0.0028) Grad: 615.9269  LR: 0.000015  \n",
      "Epoch: [1][11600/36908] Elapsed 150m 42s (remain 328m 44s) Loss: 0.0000(0.0028) Grad: 33.8256  LR: 0.000015  \n",
      "Epoch: [1][11700/36908] Elapsed 152m 0s (remain 327m 27s) Loss: 0.0000(0.0028) Grad: 730.8149  LR: 0.000015  \n",
      "Epoch: [1][11800/36908] Elapsed 153m 17s (remain 326m 9s) Loss: 0.0024(0.0028) Grad: 123964.0859  LR: 0.000015  \n",
      "Epoch: [1][11900/36908] Elapsed 154m 34s (remain 324m 48s) Loss: 0.0000(0.0028) Grad: 42.2506  LR: 0.000015  \n",
      "Epoch: [1][12000/36908] Elapsed 155m 53s (remain 323m 32s) Loss: 0.0097(0.0028) Grad: 658599.7500  LR: 0.000015  \n",
      "Epoch: [1][12100/36908] Elapsed 157m 11s (remain 322m 14s) Loss: 0.0000(0.0028) Grad: 20.3729  LR: 0.000015  \n",
      "Epoch: [1][12200/36908] Elapsed 158m 29s (remain 320m 57s) Loss: 0.0002(0.0028) Grad: 59493.8281  LR: 0.000015  \n",
      "Epoch: [1][12300/36908] Elapsed 159m 47s (remain 319m 38s) Loss: 0.0000(0.0028) Grad: 1991.6903  LR: 0.000015  \n",
      "Epoch: [1][12400/36908] Elapsed 161m 6s (remain 318m 22s) Loss: 0.0000(0.0028) Grad: 712.3269  LR: 0.000015  \n",
      "Epoch: [1][12500/36908] Elapsed 162m 24s (remain 317m 5s) Loss: 0.0000(0.0028) Grad: 6.7967  LR: 0.000015  \n",
      "Epoch: [1][12600/36908] Elapsed 163m 43s (remain 315m 48s) Loss: 0.0000(0.0028) Grad: 25.3664  LR: 0.000015  \n",
      "Epoch: [1][12700/36908] Elapsed 165m 0s (remain 314m 29s) Loss: 0.0000(0.0028) Grad: 6.9618  LR: 0.000015  \n",
      "Epoch: [1][12800/36908] Elapsed 166m 18s (remain 313m 11s) Loss: 0.0190(0.0028) Grad: 179129.8281  LR: 0.000015  \n",
      "Epoch: [1][12900/36908] Elapsed 167m 36s (remain 311m 54s) Loss: 0.0031(0.0028) Grad: 78236.5156  LR: 0.000014  \n",
      "Epoch: [1][13000/36908] Elapsed 168m 54s (remain 310m 36s) Loss: 0.0000(0.0028) Grad: 3.2810  LR: 0.000014  \n",
      "Epoch: [1][13100/36908] Elapsed 170m 12s (remain 309m 18s) Loss: 0.0000(0.0028) Grad: 2924.3848  LR: 0.000014  \n",
      "Epoch: [1][13200/36908] Elapsed 171m 31s (remain 308m 2s) Loss: 0.0000(0.0028) Grad: 9.8390  LR: 0.000014  \n",
      "Epoch: [1][13300/36908] Elapsed 172m 49s (remain 306m 43s) Loss: 0.0000(0.0028) Grad: 450.2740  LR: 0.000014  \n",
      "Epoch: [1][13400/36908] Elapsed 174m 7s (remain 305m 26s) Loss: 0.0000(0.0028) Grad: 10.4530  LR: 0.000014  \n",
      "Epoch: [1][13500/36908] Elapsed 175m 26s (remain 304m 9s) Loss: 0.0000(0.0028) Grad: 34.8914  LR: 0.000014  \n",
      "Epoch: [1][13600/36908] Elapsed 176m 44s (remain 302m 52s) Loss: 0.0000(0.0028) Grad: 248.2224  LR: 0.000014  \n",
      "Epoch: [1][13700/36908] Elapsed 178m 3s (remain 301m 35s) Loss: 0.0000(0.0028) Grad: 30.3553  LR: 0.000014  \n",
      "Epoch: [1][13800/36908] Elapsed 179m 19s (remain 300m 15s) Loss: 0.0167(0.0028) Grad: 114073.2891  LR: 0.000014  \n",
      "Epoch: [1][13900/36908] Elapsed 180m 36s (remain 298m 54s) Loss: 0.0000(0.0028) Grad: 6.0999  LR: 0.000014  \n",
      "Epoch: [1][14000/36908] Elapsed 181m 55s (remain 297m 38s) Loss: 0.0042(0.0028) Grad: 57502.7539  LR: 0.000014  \n",
      "Epoch: [1][14100/36908] Elapsed 183m 14s (remain 296m 23s) Loss: 0.0000(0.0028) Grad: 15.7245  LR: 0.000014  \n",
      "Epoch: [1][14200/36908] Elapsed 184m 33s (remain 295m 5s) Loss: 0.0002(0.0028) Grad: 43552.7695  LR: 0.000014  \n",
      "Epoch: [1][14300/36908] Elapsed 185m 52s (remain 293m 49s) Loss: 0.0000(0.0028) Grad: 99.5149  LR: 0.000014  \n",
      "Epoch: [1][14400/36908] Elapsed 187m 11s (remain 292m 33s) Loss: 0.0000(0.0028) Grad: 5.9737  LR: 0.000014  \n",
      "Epoch: [1][14500/36908] Elapsed 188m 30s (remain 291m 16s) Loss: 0.0000(0.0028) Grad: 136.5403  LR: 0.000013  \n",
      "Epoch: [1][14600/36908] Elapsed 189m 48s (remain 289m 59s) Loss: 0.0000(0.0028) Grad: 46.0307  LR: 0.000013  \n",
      "Epoch: [1][14700/36908] Elapsed 191m 7s (remain 288m 42s) Loss: 0.0000(0.0028) Grad: 7.5620  LR: 0.000013  \n",
      "Epoch: [1][14800/36908] Elapsed 192m 23s (remain 287m 22s) Loss: 0.0000(0.0028) Grad: 592.5598  LR: 0.000013  \n",
      "Epoch: [1][14900/36908] Elapsed 193m 41s (remain 286m 3s) Loss: 0.0017(0.0028) Grad: 6486.3584  LR: 0.000013  \n",
      "Epoch: [1][15000/36908] Elapsed 194m 58s (remain 284m 43s) Loss: 0.0066(0.0028) Grad: 25483.8184  LR: 0.000013  \n",
      "Epoch: [1][15100/36908] Elapsed 196m 15s (remain 283m 24s) Loss: 0.0000(0.0028) Grad: 10.2473  LR: 0.000013  \n",
      "Epoch: [1][15200/36908] Elapsed 197m 32s (remain 282m 5s) Loss: 0.0492(0.0028) Grad: 96898.3125  LR: 0.000013  \n",
      "Epoch: [1][15300/36908] Elapsed 198m 49s (remain 280m 46s) Loss: 0.0000(0.0028) Grad: 54.7754  LR: 0.000013  \n",
      "Epoch: [1][15400/36908] Elapsed 200m 6s (remain 279m 26s) Loss: 0.0005(0.0028) Grad: 5209.0132  LR: 0.000013  \n",
      "Epoch: [1][15500/36908] Elapsed 201m 23s (remain 278m 8s) Loss: 0.0078(0.0028) Grad: 48264.1055  LR: 0.000013  \n",
      "Epoch: [1][15600/36908] Elapsed 202m 42s (remain 276m 51s) Loss: 0.0065(0.0028) Grad: 41483.5039  LR: 0.000013  \n",
      "Epoch: [1][15700/36908] Elapsed 204m 1s (remain 275m 34s) Loss: 0.0000(0.0027) Grad: 166.2401  LR: 0.000013  \n",
      "Epoch: [1][15800/36908] Elapsed 205m 19s (remain 274m 17s) Loss: 0.0000(0.0027) Grad: 23.6175  LR: 0.000013  \n",
      "Epoch: [1][15900/36908] Elapsed 206m 38s (remain 273m 0s) Loss: 0.0000(0.0027) Grad: 182.8644  LR: 0.000013  \n",
      "Epoch: [1][16000/36908] Elapsed 207m 58s (remain 271m 43s) Loss: 0.0032(0.0027) Grad: 39889.4727  LR: 0.000013  \n",
      "Epoch: [1][16100/36908] Elapsed 209m 16s (remain 270m 25s) Loss: 0.0001(0.0028) Grad: 2144.8999  LR: 0.000013  \n",
      "Epoch: [1][16200/36908] Elapsed 210m 34s (remain 269m 9s) Loss: 0.0000(0.0028) Grad: 17.3079  LR: 0.000012  \n",
      "Epoch: [1][16300/36908] Elapsed 211m 53s (remain 267m 52s) Loss: 0.0000(0.0028) Grad: 292.0610  LR: 0.000012  \n",
      "Epoch: [1][16400/36908] Elapsed 213m 12s (remain 266m 34s) Loss: 0.0001(0.0028) Grad: 849.7004  LR: 0.000012  \n",
      "Epoch: [1][16500/36908] Elapsed 214m 30s (remain 265m 17s) Loss: 0.0000(0.0028) Grad: 20.9454  LR: 0.000012  \n",
      "Epoch: [1][16600/36908] Elapsed 215m 48s (remain 263m 59s) Loss: 0.0000(0.0028) Grad: 19.4809  LR: 0.000012  \n",
      "Epoch: [1][16700/36908] Elapsed 217m 6s (remain 262m 41s) Loss: 0.0021(0.0028) Grad: 3269.3379  LR: 0.000012  \n",
      "Epoch: [1][16800/36908] Elapsed 218m 24s (remain 261m 23s) Loss: 0.0000(0.0028) Grad: 347.1738  LR: 0.000012  \n",
      "Epoch: [1][16900/36908] Elapsed 219m 43s (remain 260m 5s) Loss: 0.0018(0.0028) Grad: 6726.0073  LR: 0.000012  \n",
      "Epoch: [1][17000/36908] Elapsed 221m 1s (remain 258m 48s) Loss: 0.0000(0.0028) Grad: 76.7143  LR: 0.000012  \n",
      "Epoch: [1][17100/36908] Elapsed 222m 20s (remain 257m 30s) Loss: 0.0021(0.0027) Grad: 2944.8860  LR: 0.000012  \n",
      "Epoch: [1][17200/36908] Elapsed 223m 39s (remain 256m 14s) Loss: 0.0002(0.0027) Grad: 856.2690  LR: 0.000012  \n",
      "Epoch: [1][17300/36908] Elapsed 224m 56s (remain 254m 54s) Loss: 0.0011(0.0027) Grad: 6983.4707  LR: 0.000012  \n",
      "Epoch: [1][17400/36908] Elapsed 226m 13s (remain 253m 36s) Loss: 0.0000(0.0027) Grad: 116.1108  LR: 0.000012  \n",
      "Epoch: [1][17500/36908] Elapsed 227m 30s (remain 252m 17s) Loss: 0.0000(0.0027) Grad: 52.9369  LR: 0.000012  \n",
      "Epoch: [1][17600/36908] Elapsed 228m 46s (remain 250m 57s) Loss: 0.0016(0.0028) Grad: 3450.9626  LR: 0.000012  \n",
      "Epoch: [1][17700/36908] Elapsed 230m 2s (remain 249m 36s) Loss: 0.0000(0.0027) Grad: 13.9097  LR: 0.000012  \n",
      "Epoch: [1][17800/36908] Elapsed 231m 20s (remain 248m 18s) Loss: 0.0018(0.0027) Grad: 15581.4375  LR: 0.000012  \n",
      "Epoch: [1][17900/36908] Elapsed 232m 37s (remain 246m 59s) Loss: 0.0051(0.0027) Grad: 57847.2070  LR: 0.000011  \n",
      "Epoch: [1][18000/36908] Elapsed 233m 54s (remain 245m 41s) Loss: 0.0002(0.0027) Grad: 1775.1477  LR: 0.000011  \n",
      "Epoch: [1][18100/36908] Elapsed 235m 12s (remain 244m 22s) Loss: 0.0001(0.0027) Grad: 377.8390  LR: 0.000011  \n",
      "Epoch: [1][18200/36908] Elapsed 236m 30s (remain 243m 4s) Loss: 0.0015(0.0027) Grad: 15367.7158  LR: 0.000011  \n",
      "Epoch: [1][18300/36908] Elapsed 237m 47s (remain 241m 46s) Loss: 0.0001(0.0027) Grad: 335.1382  LR: 0.000011  \n",
      "Epoch: [1][18400/36908] Elapsed 239m 4s (remain 240m 27s) Loss: 0.0006(0.0027) Grad: 10482.2051  LR: 0.000011  \n",
      "Epoch: [1][18500/36908] Elapsed 240m 23s (remain 239m 10s) Loss: 0.0172(0.0027) Grad: 28805.2812  LR: 0.000011  \n",
      "Epoch: [1][18600/36908] Elapsed 241m 41s (remain 237m 52s) Loss: 0.0000(0.0027) Grad: 51.2085  LR: 0.000011  \n",
      "Epoch: [1][18700/36908] Elapsed 243m 0s (remain 236m 35s) Loss: 0.0001(0.0027) Grad: 656.6058  LR: 0.000011  \n",
      "Epoch: [1][18800/36908] Elapsed 244m 18s (remain 235m 16s) Loss: 0.0000(0.0027) Grad: 6.4353  LR: 0.000011  \n",
      "Epoch: [1][18900/36908] Elapsed 245m 37s (remain 234m 0s) Loss: 0.0048(0.0027) Grad: 24912.7930  LR: 0.000011  \n",
      "Epoch: [1][19000/36908] Elapsed 246m 54s (remain 232m 41s) Loss: 0.0095(0.0027) Grad: 18273.2988  LR: 0.000011  \n",
      "Epoch: [1][19100/36908] Elapsed 248m 11s (remain 231m 22s) Loss: 0.0000(0.0027) Grad: 11.1515  LR: 0.000011  \n",
      "Epoch: [1][19200/36908] Elapsed 249m 30s (remain 230m 5s) Loss: 0.0010(0.0027) Grad: 2024.2244  LR: 0.000011  \n",
      "Epoch: [1][19300/36908] Elapsed 250m 47s (remain 228m 46s) Loss: 0.0003(0.0027) Grad: 2263.0867  LR: 0.000011  \n",
      "Epoch: [1][19400/36908] Elapsed 252m 6s (remain 227m 29s) Loss: 0.0000(0.0027) Grad: 88.0735  LR: 0.000011  \n",
      "Epoch: [1][19500/36908] Elapsed 253m 22s (remain 226m 10s) Loss: 0.0116(0.0027) Grad: 29003.9902  LR: 0.000010  \n",
      "Epoch: [1][19600/36908] Elapsed 254m 40s (remain 224m 51s) Loss: 0.0000(0.0027) Grad: 69.5709  LR: 0.000010  \n",
      "Epoch: [1][19700/36908] Elapsed 255m 57s (remain 223m 33s) Loss: 0.0060(0.0027) Grad: 37732.5078  LR: 0.000010  \n",
      "Epoch: [1][19800/36908] Elapsed 257m 14s (remain 222m 14s) Loss: 0.0013(0.0027) Grad: 20850.1680  LR: 0.000010  \n",
      "Epoch: [1][19900/36908] Elapsed 258m 32s (remain 220m 57s) Loss: 0.0000(0.0027) Grad: 20.6453  LR: 0.000010  \n",
      "Epoch: [1][20000/36908] Elapsed 259m 51s (remain 219m 39s) Loss: 0.0006(0.0027) Grad: 2319.6584  LR: 0.000010  \n",
      "Epoch: [1][20100/36908] Elapsed 261m 10s (remain 218m 22s) Loss: 0.0018(0.0027) Grad: 7377.3867  LR: 0.000010  \n",
      "Epoch: [1][20200/36908] Elapsed 262m 28s (remain 217m 4s) Loss: 0.0057(0.0027) Grad: 15416.3623  LR: 0.000010  \n",
      "Epoch: [1][20300/36908] Elapsed 263m 46s (remain 215m 46s) Loss: 0.0017(0.0027) Grad: 10224.7979  LR: 0.000010  \n",
      "Epoch: [1][20400/36908] Elapsed 265m 4s (remain 214m 28s) Loss: 0.0061(0.0027) Grad: 27665.7676  LR: 0.000010  \n",
      "Epoch: [1][20500/36908] Elapsed 266m 22s (remain 213m 11s) Loss: 0.0001(0.0027) Grad: 375.0777  LR: 0.000010  \n",
      "Epoch: [1][20600/36908] Elapsed 267m 40s (remain 211m 53s) Loss: 0.0027(0.0027) Grad: 20717.7500  LR: 0.000010  \n",
      "Epoch: [1][20700/36908] Elapsed 269m 0s (remain 210m 36s) Loss: 0.0001(0.0027) Grad: 312.4651  LR: 0.000010  \n",
      "Epoch: [1][20800/36908] Elapsed 270m 16s (remain 209m 17s) Loss: 0.0017(0.0027) Grad: 14470.5029  LR: 0.000010  \n",
      "Epoch: [1][20900/36908] Elapsed 271m 34s (remain 207m 58s) Loss: 0.0000(0.0027) Grad: 14.7021  LR: 0.000010  \n",
      "Epoch: [1][21000/36908] Elapsed 272m 51s (remain 206m 40s) Loss: 0.0000(0.0027) Grad: 202.7855  LR: 0.000010  \n",
      "Epoch: [1][21100/36908] Elapsed 274m 7s (remain 205m 21s) Loss: 0.0002(0.0027) Grad: 842.0976  LR: 0.000010  \n",
      "Epoch: [1][21200/36908] Elapsed 275m 25s (remain 204m 3s) Loss: 0.0010(0.0027) Grad: 21961.8457  LR: 0.000009  \n",
      "Epoch: [1][21300/36908] Elapsed 276m 42s (remain 202m 44s) Loss: 0.0000(0.0027) Grad: 288.3753  LR: 0.000009  \n",
      "Epoch: [1][21400/36908] Elapsed 278m 1s (remain 201m 27s) Loss: 0.0001(0.0027) Grad: 2313.4436  LR: 0.000009  \n",
      "Epoch: [1][21500/36908] Elapsed 279m 18s (remain 200m 8s) Loss: 0.0102(0.0027) Grad: 13164.9521  LR: 0.000009  \n",
      "Epoch: [1][21600/36908] Elapsed 280m 35s (remain 198m 49s) Loss: 0.0003(0.0027) Grad: 6966.0029  LR: 0.000009  \n",
      "Epoch: [1][21700/36908] Elapsed 281m 53s (remain 197m 31s) Loss: 0.0000(0.0027) Grad: 355.6039  LR: 0.000009  \n",
      "Epoch: [1][21800/36908] Elapsed 283m 10s (remain 196m 13s) Loss: 0.0001(0.0027) Grad: 1497.7147  LR: 0.000009  \n",
      "Epoch: [1][21900/36908] Elapsed 284m 27s (remain 194m 54s) Loss: 0.0126(0.0027) Grad: 21110.2285  LR: 0.000009  \n",
      "Epoch: [1][22000/36908] Elapsed 285m 44s (remain 193m 36s) Loss: 0.0000(0.0027) Grad: 843.3385  LR: 0.000009  \n",
      "Epoch: [1][22100/36908] Elapsed 287m 4s (remain 192m 19s) Loss: 0.0015(0.0027) Grad: 54517.9062  LR: 0.000009  \n",
      "Epoch: [1][22200/36908] Elapsed 288m 22s (remain 191m 2s) Loss: 0.0050(0.0027) Grad: 42271.5742  LR: 0.000009  \n",
      "Epoch: [1][22300/36908] Elapsed 289m 41s (remain 189m 44s) Loss: 0.0141(0.0027) Grad: 28237.6602  LR: 0.000009  \n",
      "Epoch: [1][22400/36908] Elapsed 290m 59s (remain 188m 26s) Loss: 0.0000(0.0027) Grad: 6.7218  LR: 0.000009  \n",
      "Epoch: [1][22500/36908] Elapsed 292m 17s (remain 187m 8s) Loss: 0.0000(0.0027) Grad: 162.4404  LR: 0.000009  \n",
      "Epoch: [1][22600/36908] Elapsed 293m 35s (remain 185m 51s) Loss: 0.0000(0.0027) Grad: 12.0395  LR: 0.000009  \n",
      "Epoch: [1][22700/36908] Elapsed 294m 54s (remain 184m 33s) Loss: 0.0000(0.0027) Grad: 25.1292  LR: 0.000009  \n",
      "Epoch: [1][22800/36908] Elapsed 296m 12s (remain 183m 15s) Loss: 0.0010(0.0027) Grad: 16330.2148  LR: 0.000008  \n",
      "Epoch: [1][22900/36908] Elapsed 297m 29s (remain 181m 57s) Loss: 0.0005(0.0027) Grad: 8801.2822  LR: 0.000008  \n",
      "Epoch: [1][23000/36908] Elapsed 298m 46s (remain 180m 38s) Loss: 0.0004(0.0027) Grad: 12207.5537  LR: 0.000008  \n",
      "Epoch: [1][23100/36908] Elapsed 300m 4s (remain 179m 21s) Loss: 0.0000(0.0027) Grad: 67.7913  LR: 0.000008  \n",
      "Epoch: [1][23200/36908] Elapsed 301m 21s (remain 178m 2s) Loss: 0.0036(0.0027) Grad: 117808.6406  LR: 0.000008  \n",
      "Epoch: [1][23300/36908] Elapsed 302m 38s (remain 176m 44s) Loss: 0.0038(0.0027) Grad: 128417.7422  LR: 0.000008  \n",
      "Epoch: [1][23400/36908] Elapsed 303m 55s (remain 175m 25s) Loss: 0.0000(0.0027) Grad: 7.1395  LR: 0.000008  \n",
      "Epoch: [1][23500/36908] Elapsed 305m 14s (remain 174m 7s) Loss: 0.0000(0.0027) Grad: 239.8007  LR: 0.000008  \n",
      "Epoch: [1][23600/36908] Elapsed 306m 32s (remain 172m 50s) Loss: 0.0111(0.0027) Grad: 78255.4844  LR: 0.000008  \n",
      "Epoch: [1][23700/36908] Elapsed 307m 50s (remain 171m 32s) Loss: 0.0061(0.0027) Grad: 7131.5674  LR: 0.000008  \n",
      "Epoch: [1][23800/36908] Elapsed 309m 8s (remain 170m 14s) Loss: 0.0164(0.0027) Grad: 45057.9453  LR: 0.000008  \n",
      "Epoch: [1][23900/36908] Elapsed 310m 26s (remain 168m 56s) Loss: 0.0287(0.0027) Grad: 231799.0312  LR: 0.000008  \n",
      "Epoch: [1][24000/36908] Elapsed 311m 43s (remain 167m 37s) Loss: 0.0666(0.0027) Grad: 447013.8750  LR: 0.000008  \n",
      "Epoch: [1][24100/36908] Elapsed 313m 0s (remain 166m 19s) Loss: 0.0017(0.0027) Grad: 4705.3037  LR: 0.000008  \n",
      "Epoch: [1][24200/36908] Elapsed 314m 18s (remain 165m 1s) Loss: 0.0000(0.0027) Grad: 72.3777  LR: 0.000008  \n",
      "Epoch: [1][24300/36908] Elapsed 315m 34s (remain 163m 43s) Loss: 0.0002(0.0027) Grad: 5386.9277  LR: 0.000008  \n",
      "Epoch: [1][24400/36908] Elapsed 316m 53s (remain 162m 25s) Loss: 0.0001(0.0027) Grad: 1003.3737  LR: 0.000008  \n",
      "Epoch: [1][24500/36908] Elapsed 318m 13s (remain 161m 8s) Loss: 0.0001(0.0027) Grad: 1061.0237  LR: 0.000007  \n",
      "Epoch: [1][24600/36908] Elapsed 319m 31s (remain 159m 50s) Loss: 0.0006(0.0027) Grad: 15728.6064  LR: 0.000007  \n",
      "Epoch: [1][24700/36908] Elapsed 320m 49s (remain 158m 32s) Loss: 0.0013(0.0027) Grad: 129433.5547  LR: 0.000007  \n",
      "Epoch: [1][24800/36908] Elapsed 322m 7s (remain 157m 14s) Loss: 0.0001(0.0027) Grad: 6779.8682  LR: 0.000007  \n",
      "Epoch: [1][24900/36908] Elapsed 323m 24s (remain 155m 56s) Loss: 0.0010(0.0027) Grad: 7910.2266  LR: 0.000007  \n",
      "Epoch: [1][25000/36908] Elapsed 324m 41s (remain 154m 38s) Loss: 0.0000(0.0027) Grad: 12.7415  LR: 0.000007  \n",
      "Epoch: [1][25100/36908] Elapsed 325m 58s (remain 153m 19s) Loss: 0.0131(0.0027) Grad: 36523.2266  LR: 0.000007  \n",
      "Epoch: [1][25200/36908] Elapsed 327m 17s (remain 152m 2s) Loss: 0.0000(0.0027) Grad: 7.3366  LR: 0.000007  \n",
      "Epoch: [1][25300/36908] Elapsed 328m 35s (remain 150m 44s) Loss: 0.0113(0.0027) Grad: 118566.7188  LR: 0.000007  \n",
      "Epoch: [1][25400/36908] Elapsed 329m 54s (remain 149m 27s) Loss: 0.0000(0.0027) Grad: 926.2309  LR: 0.000007  \n",
      "Epoch: [1][25500/36908] Elapsed 331m 13s (remain 148m 9s) Loss: 0.0000(0.0027) Grad: 3.4087  LR: 0.000007  \n",
      "Epoch: [1][25600/36908] Elapsed 332m 31s (remain 146m 51s) Loss: 0.0022(0.0027) Grad: 6793.2700  LR: 0.000007  \n",
      "Epoch: [1][25700/36908] Elapsed 333m 48s (remain 145m 33s) Loss: 0.0000(0.0027) Grad: 4.8127  LR: 0.000007  \n",
      "Epoch: [1][25800/36908] Elapsed 335m 6s (remain 144m 15s) Loss: 0.0000(0.0027) Grad: 5.5029  LR: 0.000007  \n",
      "Epoch: [1][25900/36908] Elapsed 336m 24s (remain 142m 57s) Loss: 0.0047(0.0027) Grad: 69163.7344  LR: 0.000007  \n",
      "Epoch: [1][26000/36908] Elapsed 337m 42s (remain 141m 39s) Loss: 0.0000(0.0027) Grad: 453.8686  LR: 0.000007  \n",
      "Epoch: [1][26100/36908] Elapsed 339m 0s (remain 140m 22s) Loss: 0.0000(0.0027) Grad: 32.3814  LR: 0.000007  \n",
      "Epoch: [1][26200/36908] Elapsed 340m 17s (remain 139m 3s) Loss: 0.0000(0.0027) Grad: 58.9875  LR: 0.000006  \n",
      "Epoch: [1][26300/36908] Elapsed 341m 37s (remain 137m 46s) Loss: 0.0013(0.0027) Grad: 43863.3750  LR: 0.000006  \n",
      "Epoch: [1][26400/36908] Elapsed 342m 55s (remain 136m 28s) Loss: 0.0000(0.0027) Grad: 11.8898  LR: 0.000006  \n",
      "Epoch: [1][26500/36908] Elapsed 344m 13s (remain 135m 10s) Loss: 0.0010(0.0027) Grad: 25407.3320  LR: 0.000006  \n",
      "Epoch: [1][26600/36908] Elapsed 345m 31s (remain 133m 52s) Loss: 0.0000(0.0027) Grad: 22.0520  LR: 0.000006  \n",
      "Epoch: [1][26700/36908] Elapsed 346m 49s (remain 132m 34s) Loss: 0.0008(0.0027) Grad: 54165.5898  LR: 0.000006  \n",
      "Epoch: [1][26800/36908] Elapsed 348m 7s (remain 131m 16s) Loss: 0.0000(0.0027) Grad: 6.3978  LR: 0.000006  \n",
      "Epoch: [1][26900/36908] Elapsed 349m 24s (remain 129m 58s) Loss: 0.0000(0.0027) Grad: 8.2852  LR: 0.000006  \n",
      "Epoch: [1][27000/36908] Elapsed 350m 40s (remain 128m 40s) Loss: 0.0029(0.0027) Grad: 37014.9258  LR: 0.000006  \n",
      "Epoch: [1][27100/36908] Elapsed 351m 58s (remain 127m 21s) Loss: 0.0000(0.0027) Grad: 256.3486  LR: 0.000006  \n",
      "Epoch: [1][27200/36908] Elapsed 353m 14s (remain 126m 3s) Loss: 0.0000(0.0027) Grad: 217.2722  LR: 0.000006  \n",
      "Epoch: [1][27300/36908] Elapsed 354m 33s (remain 124m 45s) Loss: 0.0000(0.0027) Grad: 225.8476  LR: 0.000006  \n",
      "Epoch: [1][27400/36908] Elapsed 355m 52s (remain 123m 28s) Loss: 0.0000(0.0027) Grad: 35.6373  LR: 0.000006  \n",
      "Epoch: [1][27500/36908] Elapsed 357m 9s (remain 122m 10s) Loss: 0.0081(0.0027) Grad: 85418.5078  LR: 0.000006  \n",
      "Epoch: [1][27600/36908] Elapsed 358m 26s (remain 120m 51s) Loss: 0.0000(0.0027) Grad: 12.2900  LR: 0.000006  \n",
      "Epoch: [1][27700/36908] Elapsed 359m 43s (remain 119m 33s) Loss: 0.0000(0.0027) Grad: 75.8553  LR: 0.000006  \n",
      "Epoch: [1][27800/36908] Elapsed 361m 0s (remain 118m 15s) Loss: 0.0041(0.0027) Grad: 36302.2422  LR: 0.000005  \n",
      "Epoch: [1][27900/36908] Elapsed 362m 18s (remain 116m 57s) Loss: 0.0000(0.0027) Grad: 28.5902  LR: 0.000005  \n",
      "Epoch: [1][28000/36908] Elapsed 363m 35s (remain 115m 39s) Loss: 0.0000(0.0027) Grad: 64.2664  LR: 0.000005  \n",
      "Epoch: [1][28100/36908] Elapsed 364m 52s (remain 114m 21s) Loss: 0.0010(0.0027) Grad: 37034.1836  LR: 0.000005  \n",
      "Epoch: [1][28200/36908] Elapsed 366m 9s (remain 113m 3s) Loss: 0.0002(0.0027) Grad: 3396.9424  LR: 0.000005  \n",
      "Epoch: [1][28300/36908] Elapsed 367m 28s (remain 111m 45s) Loss: 0.0000(0.0026) Grad: 1568.1453  LR: 0.000005  \n",
      "Epoch: [1][28400/36908] Elapsed 368m 46s (remain 110m 27s) Loss: 0.0000(0.0026) Grad: 3.5800  LR: 0.000005  \n",
      "Epoch: [1][28500/36908] Elapsed 370m 5s (remain 109m 10s) Loss: 0.0039(0.0026) Grad: 270134.7188  LR: 0.000005  \n",
      "Epoch: [1][28600/36908] Elapsed 371m 23s (remain 107m 52s) Loss: 0.0000(0.0026) Grad: 158.7301  LR: 0.000005  \n",
      "Epoch: [1][28700/36908] Elapsed 372m 42s (remain 106m 34s) Loss: 0.0000(0.0026) Grad: 185.4247  LR: 0.000005  \n",
      "Epoch: [1][28800/36908] Elapsed 373m 59s (remain 105m 16s) Loss: 0.0035(0.0026) Grad: 93517.4141  LR: 0.000005  \n",
      "Epoch: [1][28900/36908] Elapsed 375m 17s (remain 103m 58s) Loss: 0.0001(0.0026) Grad: 4793.2900  LR: 0.000005  \n",
      "Epoch: [1][29000/36908] Elapsed 376m 34s (remain 102m 40s) Loss: 0.0000(0.0026) Grad: 3.2221  LR: 0.000005  \n",
      "Epoch: [1][29100/36908] Elapsed 377m 52s (remain 101m 22s) Loss: 0.0000(0.0026) Grad: 35.0509  LR: 0.000005  \n",
      "Epoch: [1][29200/36908] Elapsed 379m 10s (remain 100m 4s) Loss: 0.0214(0.0026) Grad: 285218.6875  LR: 0.000005  \n",
      "Epoch: [1][29300/36908] Elapsed 380m 26s (remain 98m 46s) Loss: 0.0010(0.0026) Grad: 7935.5806  LR: 0.000005  \n",
      "Epoch: [1][29400/36908] Elapsed 381m 43s (remain 97m 28s) Loss: 0.0000(0.0026) Grad: 7.7660  LR: 0.000005  \n",
      "Epoch: [1][29500/36908] Elapsed 383m 1s (remain 96m 10s) Loss: 0.0003(0.0026) Grad: 5397.8643  LR: 0.000004  \n",
      "Epoch: [1][29600/36908] Elapsed 384m 18s (remain 94m 52s) Loss: 0.0000(0.0026) Grad: 3.7000  LR: 0.000004  \n",
      "Epoch: [1][29700/36908] Elapsed 385m 38s (remain 93m 34s) Loss: 0.0000(0.0026) Grad: 20.6317  LR: 0.000004  \n",
      "Epoch: [1][29800/36908] Elapsed 386m 55s (remain 92m 16s) Loss: 0.0000(0.0026) Grad: 6.3255  LR: 0.000004  \n",
      "Epoch: [1][29900/36908] Elapsed 388m 13s (remain 90m 58s) Loss: 0.0000(0.0026) Grad: 21.4000  LR: 0.000004  \n",
      "Epoch: [1][30000/36908] Elapsed 389m 32s (remain 89m 41s) Loss: 0.0014(0.0026) Grad: 12809.5762  LR: 0.000004  \n",
      "Epoch: [1][30100/36908] Elapsed 390m 50s (remain 88m 23s) Loss: 0.0000(0.0026) Grad: 10.1086  LR: 0.000004  \n",
      "Epoch: [1][30200/36908] Elapsed 392m 8s (remain 87m 5s) Loss: 0.0063(0.0026) Grad: 58915.6875  LR: 0.000004  \n",
      "Epoch: [1][30300/36908] Elapsed 393m 26s (remain 85m 47s) Loss: 0.0095(0.0026) Grad: 73160.7422  LR: 0.000004  \n",
      "Epoch: [1][30400/36908] Elapsed 394m 43s (remain 84m 29s) Loss: 0.0009(0.0026) Grad: 35579.8906  LR: 0.000004  \n",
      "Epoch: [1][30500/36908] Elapsed 396m 1s (remain 83m 11s) Loss: 0.0000(0.0026) Grad: 22.3454  LR: 0.000004  \n",
      "Epoch: [1][30600/36908] Elapsed 397m 18s (remain 81m 53s) Loss: 0.0164(0.0026) Grad: 56498.0156  LR: 0.000004  \n",
      "Epoch: [1][30700/36908] Elapsed 398m 36s (remain 80m 35s) Loss: 0.0000(0.0026) Grad: 1.5244  LR: 0.000004  \n",
      "Epoch: [1][30800/36908] Elapsed 399m 54s (remain 79m 17s) Loss: 0.0000(0.0026) Grad: 10.7408  LR: 0.000004  \n",
      "Epoch: [1][30900/36908] Elapsed 401m 11s (remain 77m 59s) Loss: 0.0035(0.0026) Grad: 37801.8477  LR: 0.000004  \n",
      "Epoch: [1][31000/36908] Elapsed 402m 29s (remain 76m 41s) Loss: 0.0000(0.0026) Grad: 29.7619  LR: 0.000004  \n",
      "Epoch: [1][31100/36908] Elapsed 403m 48s (remain 75m 23s) Loss: 0.0193(0.0026) Grad: 54436.8477  LR: 0.000003  \n",
      "Epoch: [1][31200/36908] Elapsed 405m 5s (remain 74m 5s) Loss: 0.0072(0.0026) Grad: 67036.9375  LR: 0.000003  \n",
      "Epoch: [1][31300/36908] Elapsed 406m 22s (remain 72m 47s) Loss: 0.0009(0.0026) Grad: 51330.8906  LR: 0.000003  \n",
      "Epoch: [1][31400/36908] Elapsed 407m 39s (remain 71m 29s) Loss: 0.0096(0.0026) Grad: 96592.9453  LR: 0.000003  \n",
      "Epoch: [1][31500/36908] Elapsed 408m 59s (remain 70m 12s) Loss: 0.0192(0.0026) Grad: 51891.4453  LR: 0.000003  \n",
      "Epoch: [1][31600/36908] Elapsed 410m 16s (remain 68m 54s) Loss: 0.0005(0.0026) Grad: 2032.9487  LR: 0.000003  \n",
      "Epoch: [1][31700/36908] Elapsed 411m 34s (remain 67m 36s) Loss: 0.0001(0.0026) Grad: 5429.2920  LR: 0.000003  \n",
      "Epoch: [1][31800/36908] Elapsed 412m 52s (remain 66m 18s) Loss: 0.0000(0.0026) Grad: 27.8243  LR: 0.000003  \n",
      "Epoch: [1][31900/36908] Elapsed 414m 9s (remain 65m 0s) Loss: 0.0000(0.0026) Grad: 40.2485  LR: 0.000003  \n",
      "Epoch: [1][32000/36908] Elapsed 415m 27s (remain 63m 42s) Loss: 0.0000(0.0026) Grad: 6.1562  LR: 0.000003  \n",
      "Epoch: [1][32100/36908] Elapsed 416m 44s (remain 62m 24s) Loss: 0.0000(0.0026) Grad: 61.1357  LR: 0.000003  \n",
      "Epoch: [1][32200/36908] Elapsed 418m 0s (remain 61m 6s) Loss: 0.0000(0.0026) Grad: 139.9200  LR: 0.000003  \n",
      "Epoch: [1][32300/36908] Elapsed 419m 18s (remain 59m 48s) Loss: 0.0000(0.0026) Grad: 31.5718  LR: 0.000003  \n",
      "Epoch: [1][32400/36908] Elapsed 420m 35s (remain 58m 30s) Loss: 0.0039(0.0026) Grad: 24607.9609  LR: 0.000003  \n",
      "Epoch: [1][32500/36908] Elapsed 421m 53s (remain 57m 12s) Loss: 0.0001(0.0026) Grad: 3713.6201  LR: 0.000003  \n",
      "Epoch: [1][32600/36908] Elapsed 423m 11s (remain 55m 54s) Loss: 0.0000(0.0026) Grad: 143.6953  LR: 0.000003  \n",
      "Epoch: [1][32700/36908] Elapsed 424m 30s (remain 54m 36s) Loss: 0.0000(0.0026) Grad: 7.3982  LR: 0.000003  \n",
      "Epoch: [1][32800/36908] Elapsed 425m 48s (remain 53m 18s) Loss: 0.0002(0.0026) Grad: 3644.8000  LR: 0.000002  \n",
      "Epoch: [1][32900/36908] Elapsed 427m 6s (remain 52m 1s) Loss: 0.0004(0.0026) Grad: 3038.0347  LR: 0.000002  \n",
      "Epoch: [1][33000/36908] Elapsed 428m 25s (remain 50m 43s) Loss: 0.0011(0.0026) Grad: 31653.9492  LR: 0.000002  \n",
      "Epoch: [1][33100/36908] Elapsed 429m 44s (remain 49m 25s) Loss: 0.0011(0.0026) Grad: 12536.4492  LR: 0.000002  \n",
      "Epoch: [1][33200/36908] Elapsed 431m 3s (remain 48m 7s) Loss: 0.0002(0.0026) Grad: 7183.1475  LR: 0.000002  \n",
      "Epoch: [1][33300/36908] Elapsed 432m 21s (remain 46m 49s) Loss: 0.0003(0.0026) Grad: 7821.1694  LR: 0.000002  \n",
      "Epoch: [1][33400/36908] Elapsed 433m 39s (remain 45m 31s) Loss: 0.0038(0.0026) Grad: 123017.2969  LR: 0.000002  \n",
      "Epoch: [1][33500/36908] Elapsed 434m 57s (remain 44m 14s) Loss: 0.0009(0.0026) Grad: 17690.8594  LR: 0.000002  \n",
      "Epoch: [1][33600/36908] Elapsed 436m 16s (remain 42m 56s) Loss: 0.0003(0.0026) Grad: 5956.3022  LR: 0.000002  \n",
      "Epoch: [1][33700/36908] Elapsed 437m 34s (remain 41m 38s) Loss: 0.0000(0.0026) Grad: 9.0765  LR: 0.000002  \n",
      "Epoch: [1][33800/36908] Elapsed 438m 53s (remain 40m 20s) Loss: 0.0000(0.0026) Grad: 7.1823  LR: 0.000002  \n",
      "Epoch: [1][33900/36908] Elapsed 440m 11s (remain 39m 2s) Loss: 0.0008(0.0026) Grad: 10196.3535  LR: 0.000002  \n",
      "Epoch: [1][34000/36908] Elapsed 441m 28s (remain 37m 44s) Loss: 0.0030(0.0026) Grad: 29298.8613  LR: 0.000002  \n",
      "Epoch: [1][34100/36908] Elapsed 442m 45s (remain 36m 26s) Loss: 0.0002(0.0026) Grad: 11489.1436  LR: 0.000002  \n",
      "Epoch: [1][34200/36908] Elapsed 444m 1s (remain 35m 8s) Loss: 0.0000(0.0026) Grad: 61.3370  LR: 0.000002  \n",
      "Epoch: [1][34300/36908] Elapsed 445m 18s (remain 33m 50s) Loss: 0.0000(0.0026) Grad: 4.3770  LR: 0.000002  \n",
      "Epoch: [1][34400/36908] Elapsed 446m 36s (remain 32m 32s) Loss: 0.0000(0.0026) Grad: 9.8500  LR: 0.000002  \n",
      "Epoch: [1][34500/36908] Elapsed 447m 53s (remain 31m 14s) Loss: 0.0000(0.0026) Grad: 1793.7344  LR: 0.000001  \n",
      "Epoch: [1][34600/36908] Elapsed 449m 11s (remain 29m 56s) Loss: 0.0000(0.0026) Grad: 37.0229  LR: 0.000001  \n",
      "Epoch: [1][34700/36908] Elapsed 450m 27s (remain 28m 38s) Loss: 0.0000(0.0026) Grad: 84.3281  LR: 0.000001  \n",
      "Epoch: [1][34800/36908] Elapsed 451m 44s (remain 27m 21s) Loss: 0.0054(0.0026) Grad: 140262.6719  LR: 0.000001  \n",
      "Epoch: [1][34900/36908] Elapsed 453m 1s (remain 26m 3s) Loss: 0.0000(0.0026) Grad: 36.0526  LR: 0.000001  \n",
      "Epoch: [1][35000/36908] Elapsed 454m 17s (remain 24m 45s) Loss: 0.0058(0.0026) Grad: 128152.3359  LR: 0.000001  \n",
      "Epoch: [1][35100/36908] Elapsed 455m 34s (remain 23m 27s) Loss: 0.0005(0.0026) Grad: 8269.2275  LR: 0.000001  \n",
      "Epoch: [1][35200/36908] Elapsed 456m 52s (remain 22m 9s) Loss: 0.0000(0.0026) Grad: 10.0391  LR: 0.000001  \n",
      "Epoch: [1][35300/36908] Elapsed 458m 9s (remain 20m 51s) Loss: 0.0018(0.0026) Grad: 42898.1680  LR: 0.000001  \n",
      "Epoch: [1][35400/36908] Elapsed 459m 24s (remain 19m 33s) Loss: 0.0001(0.0026) Grad: 3801.7466  LR: 0.000001  \n",
      "Epoch: [1][35500/36908] Elapsed 460m 41s (remain 18m 15s) Loss: 0.0000(0.0026) Grad: 9.3613  LR: 0.000001  \n",
      "Epoch: [1][35600/36908] Elapsed 461m 58s (remain 16m 57s) Loss: 0.0010(0.0026) Grad: 12524.9014  LR: 0.000001  \n",
      "Epoch: [1][35700/36908] Elapsed 463m 15s (remain 15m 39s) Loss: 0.0000(0.0026) Grad: 65.3148  LR: 0.000001  \n",
      "Epoch: [1][35800/36908] Elapsed 464m 32s (remain 14m 21s) Loss: 0.0000(0.0026) Grad: 1.4725  LR: 0.000001  \n",
      "Epoch: [1][35900/36908] Elapsed 465m 50s (remain 13m 3s) Loss: 0.0000(0.0026) Grad: 564.7006  LR: 0.000001  \n",
      "Epoch: [1][36000/36908] Elapsed 467m 9s (remain 11m 46s) Loss: 0.0000(0.0026) Grad: 66.1868  LR: 0.000001  \n",
      "Epoch: [1][36100/36908] Elapsed 468m 27s (remain 10m 28s) Loss: 0.0000(0.0026) Grad: 751.9280  LR: 0.000000  \n",
      "Epoch: [1][36200/36908] Elapsed 469m 46s (remain 9m 10s) Loss: 0.0000(0.0026) Grad: 383.4116  LR: 0.000000  \n",
      "Epoch: [1][36300/36908] Elapsed 471m 4s (remain 7m 52s) Loss: 0.0010(0.0026) Grad: 13048.0098  LR: 0.000000  \n",
      "Epoch: [1][36400/36908] Elapsed 472m 21s (remain 6m 34s) Loss: 0.0031(0.0026) Grad: 111817.8828  LR: 0.000000  \n",
      "Epoch: [1][36500/36908] Elapsed 473m 38s (remain 5m 16s) Loss: 0.0000(0.0026) Grad: 58.7110  LR: 0.000000  \n",
      "Epoch: [1][36600/36908] Elapsed 474m 54s (remain 3m 59s) Loss: 0.0000(0.0026) Grad: 28.0859  LR: 0.000000  \n",
      "Epoch: [1][36700/36908] Elapsed 476m 12s (remain 2m 41s) Loss: 0.0028(0.0026) Grad: 65516.7227  LR: 0.000000  \n",
      "Epoch: [1][36800/36908] Elapsed 477m 29s (remain 1m 23s) Loss: 0.0003(0.0026) Grad: 5767.6392  LR: 0.000000  \n",
      "Epoch: [1][36900/36908] Elapsed 478m 45s (remain 0m 5s) Loss: 0.0000(0.0026) Grad: 15.1831  LR: 0.000000  \n",
      "Epoch: [1][36907/36908] Elapsed 478m 51s (remain 0m 0s) Loss: 0.0010(0.0026) Grad: 15287.9551  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 1s (remain 24m 35s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 32s (remain 5m 48s) Loss: 0.0704(0.0102) \n",
      "EVAL: [200/1192] Elapsed 1m 2s (remain 5m 7s) Loss: 0.0333(0.0098) \n",
      "EVAL: [300/1192] Elapsed 1m 32s (remain 4m 35s) Loss: 0.0144(0.0112) \n",
      "EVAL: [400/1192] Elapsed 2m 3s (remain 4m 3s) Loss: 0.0000(0.0107) \n",
      "EVAL: [500/1192] Elapsed 2m 34s (remain 3m 32s) Loss: 0.0560(0.0104) \n",
      "EVAL: [600/1192] Elapsed 3m 4s (remain 3m 1s) Loss: 0.0246(0.0106) \n",
      "EVAL: [700/1192] Elapsed 3m 35s (remain 2m 30s) Loss: 0.0070(0.0121) \n",
      "EVAL: [800/1192] Elapsed 4m 5s (remain 1m 59s) Loss: 0.0374(0.0123) \n",
      "EVAL: [900/1192] Elapsed 4m 35s (remain 1m 28s) Loss: 0.0107(0.0127) \n",
      "EVAL: [1000/1192] Elapsed 5m 6s (remain 0m 58s) Loss: 0.0000(0.0123) \n",
      "EVAL: [1100/1192] Elapsed 5m 36s (remain 0m 27s) Loss: 0.0258(0.0119) \n",
      "EVAL: [1191/1192] Elapsed 6m 3s (remain 0m 0s) Loss: 0.0000(0.0116) \n",
      "Epoch 1 - avg_train_loss: 0.0026  avg_val_loss: 0.0116  time: 29100s\n",
      "Epoch 1 - Score: 0.8947\n",
      "Epoch 1 - Save Best Score: 0.8947 Model\n",
      "best_thres: 0.53  score: 0.89198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n",
      "load weights from ../output/nbme-score-clinical-patient-notes/nbme-exp088/fold0_best.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5dcf51afcc4c3da5b663328ad6b81c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x7fc8fbe3c170>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 132, in __del__\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n",
      "load weights from ../output/nbme-score-clinical-patient-notes/nbme-exp088/fold1_best.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f33814129543b89ee166d9db442569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "<function _ConnectionBase.__del__ at 0x7fc8fbe3c170>\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 132, in __del__\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n",
      "load weights from ../output/nbme-score-clinical-patient-notes/nbme-exp088/fold2_best.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11581c40df91439d82b0a8dfa1b1eb8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n",
      "load weights from ../output/nbme-score-clinical-patient-notes/nbme-exp088/fold3_best.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb1e4bcf2db460f9991fe76d885ac8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "name": "nbme-exp085.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 641.572862,
   "end_time": "2022-02-27T11:39:50.972497",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-27T11:29:09.399635",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "054630edadaa453fb86d66a20e49030f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_747b8a73ce544c569f4d063fc9d6d18a",
      "placeholder": "​",
      "style": "IPY_MODEL_c7aa62f5eaca401dbbfbfd5f843d6a59",
      "value": " 28720/42146 [00:27&lt;00:07, 1839.83it/s]"
     }
    },
    "18b47303dd5b48bfb118053a80c44a40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_971945a52a6d4f06ba35e5363d264037",
       "IPY_MODEL_2d7cfbb1d1a54c0590e59de0b280ab22",
       "IPY_MODEL_054630edadaa453fb86d66a20e49030f"
      ],
      "layout": "IPY_MODEL_8fce27d68c1c41ec9a3c58d439c2a5e7"
     }
    },
    "28c710503f154bdea731991801a85a47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d7cfbb1d1a54c0590e59de0b280ab22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28c710503f154bdea731991801a85a47",
      "max": 42146,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8c57bf78a80247db9521bd5b163502ef",
      "value": 28905
     }
    },
    "747b8a73ce544c569f4d063fc9d6d18a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c57bf78a80247db9521bd5b163502ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8fce27d68c1c41ec9a3c58d439c2a5e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "971945a52a6d4f06ba35e5363d264037": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9a3708f7e4c486da3a09e066b6936fb",
      "placeholder": "​",
      "style": "IPY_MODEL_cd1144e40332427fa3e8d5bc7f57b924",
      "value": " 69%"
     }
    },
    "c7aa62f5eaca401dbbfbfd5f843d6a59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd1144e40332427fa3e8d5bc7f57b924": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e9a3708f7e4c486da3a09e066b6936fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
