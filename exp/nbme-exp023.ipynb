{"cells":[{"cell_type":"markdown","id":"blind-kingdom","metadata":{"id":"blind-kingdom"},"source":["## References"]},{"cell_type":"markdown","id":"antique-glenn","metadata":{"id":"antique-glenn"},"source":["- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"]},{"cell_type":"markdown","id":"bored-ministry","metadata":{"id":"bored-ministry"},"source":["## Configurations"]},{"cell_type":"code","execution_count":1,"id":"deadly-confidence","metadata":{"id":"deadly-confidence","executionInfo":{"status":"ok","timestamp":1646540154037,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["EXP_NAME = \"nbme-exp023\"\n","ENV = \"colab\"\n","DEBUG_MODE = False\n","SUBMISSION_MODE = False"]},{"cell_type":"code","execution_count":2,"id":"aware-worcester","metadata":{"id":"aware-worcester","executionInfo":{"status":"ok","timestamp":1646540154038,"user_tz":-540,"elapsed":10,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class CFG:\n","    env=ENV\n","    exp_name=EXP_NAME\n","    debug=DEBUG_MODE\n","    submission=SUBMISSION_MODE\n","    apex=True\n","    input_dir=None\n","    output_dir=None\n","    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n","    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n","    competition_name=\"nbme-score-clinical-patient-notes\"\n","    id_col=\"id\"\n","    target_col=\"location\"\n","    pretrained_model_name=\"microsoft/deberta-v3-large\"\n","    tokenizer=None\n","    max_len=None\n","    output_dim=1\n","    dropout=0.2\n","    num_workers=4\n","    batch_size=4\n","    lr=2e-5\n","    betas=(0.9, 0.98)\n","    weight_decay=0.1\n","    num_warmup_steps_rate=0.1\n","    batch_scheduler=True\n","    epochs=5\n","    n_fold=5\n","    train_fold=[0, 1, 2, 3, 4]\n","    seed=71\n","    gradient_accumulation_steps=2\n","    max_grad_norm=1000\n","    print_freq=100\n","    train=True\n","    inference=True"]},{"cell_type":"code","execution_count":3,"id":"personalized-death","metadata":{"id":"personalized-death","executionInfo":{"status":"ok","timestamp":1646540154038,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.train_fold = [0, 1]\n","\n","if CFG.submission:\n","    CFG.train = False\n","    CFG.inference = True"]},{"cell_type":"markdown","id":"cardiovascular-neutral","metadata":{"id":"cardiovascular-neutral"},"source":["## Directory Settings"]},{"cell_type":"code","execution_count":4,"id":"checked-boards","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7987,"status":"ok","timestamp":1646540162016,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"checked-boards","outputId":"87364705-3e3f-4866-a538-14dfdc9c7e95"},"outputs":[{"output_type":"stream","name":"stdout","text":["colab\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"]}],"source":["import sys\n","from pathlib import Path\n","\n","\n","print(CFG.env)\n","if CFG.env == \"colab\":\n","    # colab環境\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","    # install packages\n","    !pip install transformers\n","    !pip install sentencepiece\n","\n","elif CFG.env == \"local\":\n","    # ローカルサーバ\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","\n","elif CFG.env == \"kaggle\":\n","    # kaggle環境\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./\")"]},{"cell_type":"code","source":["# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n","# This must be done before importing transformers\n","import shutil\n","from pathlib import Path\n","\n","if CFG.env == \"colab\":\n","    input_dir = Path(\"./drive/MyDrive/00.kaggle/input/deberta-v2-3-fast-tokenizer\")\n","    transformers_path = Path(\"/usr/local/lib/python3.7/dist-packages/transformers\")\n","else:\n","    input_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n","    transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n","\n","convert_file = input_dir / \"convert_slow_tokenizer.py\"\n","conversion_path = transformers_path/convert_file.name\n","\n","if conversion_path.exists():\n","    conversion_path.unlink()\n","\n","shutil.copy(convert_file, transformers_path)\n","deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n","\n","for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n","    filepath = deberta_v2_path/filename\n","    if filepath.exists():\n","        filepath.unlink()\n","\n","    shutil.copy(input_dir/filename, filepath)\n","    \n","    \n","from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast"],"metadata":{"id":"iGai035Rvu1Z","executionInfo":{"status":"ok","timestamp":1646540163184,"user_tz":-540,"elapsed":1178,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"id":"iGai035Rvu1Z","execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"id":"vital-mexico","metadata":{"id":"vital-mexico","executionInfo":{"status":"ok","timestamp":1646540165840,"user_tz":-540,"elapsed":2659,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["import gc\n","import os\n","import ast\n","import time\n","import math\n","import random\n","import itertools\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","from scipy.optimize import minimize\n","from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as T\n","from torchvision.io import read_image\n","from torch.utils.data import DataLoader, Dataset\n","\n","from transformers import AutoModelForMaskedLM\n","from transformers import BartModel,BertModel,BertTokenizer\n","from transformers import DebertaModel,DebertaTokenizer\n","from transformers import RobertaModel,RobertaTokenizer\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","id":"economic-ladder","metadata":{"id":"economic-ladder"},"source":["## Utilities"]},{"cell_type":"code","execution_count":7,"id":"desperate-keyboard","metadata":{"id":"desperate-keyboard","executionInfo":{"status":"ok","timestamp":1646540165841,"user_tz":-540,"elapsed":18,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on binary arrays.\n","\n","    Args:\n","        preds (list of lists of ints): Predictions.\n","        truths (list of lists of ints): Ground truths.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    # Micro : aggregating over all instances\n","    preds = np.concatenate(preds)\n","    truths = np.concatenate(truths)\n","    return f1_score(truths, preds)\n","\n","\n","def spans_to_binary(spans, length=None):\n","    \"\"\"\n","    Converts spans to a binary array indicating whether each character is in the span.\n","\n","    Args:\n","        spans (list of lists of two ints): Spans.\n","\n","    Returns:\n","        np array [length]: Binarized spans.\n","    \"\"\"\n","    length = np.max(spans) if length is None else length\n","    binary = np.zeros(length)\n","    for start, end in spans:\n","        binary[start:end] = 1\n","    return binary\n","\n","\n","def span_micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on spans.\n","\n","    Args:\n","        preds (list of lists of two ints): Prediction spans.\n","        truths (list of lists of two ints): Ground truth spans.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    bin_preds = []\n","    bin_truths = []\n","    for pred, truth in zip(preds, truths):\n","        if not len(pred) and not len(truth):\n","            continue\n","        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n","        bin_preds.append(spans_to_binary(pred, length))\n","        bin_truths.append(spans_to_binary(truth, length))\n","    return micro_f1(bin_preds, bin_truths)\n","\n","\n","def get_score(y_true, y_pred):\n","    score = span_micro_f1(y_true, y_pred)\n","    return score"]},{"cell_type":"code","execution_count":8,"id":"flexible-wednesday","metadata":{"id":"flexible-wednesday","executionInfo":{"status":"ok","timestamp":1646540165843,"user_tz":-540,"elapsed":19,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def create_labels_for_scoring(df):\n","    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n","    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n","    for i in range(len(df)):\n","        lst = df.loc[i, \"location\"]\n","        if lst:\n","            new_lst = \";\".join(lst)\n","            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n","\n","    # create labels\n","    truths = []\n","    for location_list in df[\"location_for_create_labels\"].values:\n","        truth = []\n","        if len(location_list) > 0:\n","            location = location_list[0]\n","            for loc in [s.split() for s in location.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                truth.append([start, end])\n","        truths.append(truth)\n","\n","    return truths\n","\n","\n","def get_char_probs(texts, token_probs, tokenizer):\n","    res = [np.zeros(len(t)) for t in texts]\n","    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n","        encoded = tokenizer(\n","            text=text,\n","            max_length=CFG.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n","            start, end = offset_mapping\n","            res[i][start:end] = pred\n","    return res\n","\n","\n","def get_predicted_location_str(char_probs, th=0.5):\n","    results = []\n","    for char_prob in char_probs:\n","        result = np.where(char_prob >= th)[0] + 1\n","        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n","        result = [f\"{min(r)} {max(r)}\" for r in result]\n","        result = \";\".join(result)\n","        results.append(result)\n","    return results\n","\n","\n","def get_predictions(results):\n","    predictions = []\n","    for result in results:\n","        prediction = []\n","        if result != \"\":\n","            for loc in [s.split() for s in result.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                prediction.append([start, end])\n","        predictions.append(prediction)\n","    return predictions\n","\n","\n","def scoring(df, th=0.5):\n","    labels = create_labels_for_scoring(df)\n","\n","    token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n","    char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n","    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n","    preds = get_predictions(predicted_location_str)\n","\n","    score = get_score(labels, preds)\n","    return score\n","\n","\n","def get_best_thres(oof_df):\n","    def f1_opt(x):\n","        return -1 * scoring(oof_df, th=x)\n","\n","    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n","    return best_thres"]},{"cell_type":"code","execution_count":9,"id":"logical-chemistry","metadata":{"id":"logical-chemistry","executionInfo":{"status":"ok","timestamp":1646540165844,"user_tz":-540,"elapsed":19,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return \"%dm %ds\" % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":10,"id":"gorgeous-record","metadata":{"id":"gorgeous-record","executionInfo":{"status":"ok","timestamp":1646540165845,"user_tz":-540,"elapsed":20,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["seed_everything()"]},{"cell_type":"markdown","id":"frozen-africa","metadata":{"id":"frozen-africa"},"source":["## Data Loading"]},{"cell_type":"code","execution_count":11,"id":"shaped-metallic","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":403,"status":"ok","timestamp":1646540166228,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"shaped-metallic","outputId":"11a7cabc-1b4b-4b13-c8cc-95ce538868f5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14300, 6), (143, 3), (42146, 3), (5, 4))"]},"metadata":{},"execution_count":11}],"source":["train = pd.read_csv(CFG.input_dir / \"train.csv\")\n","features = pd.read_csv(CFG.input_dir / \"features.csv\")\n","patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n","test = pd.read_csv(CFG.input_dir / \"test.csv\")\n","\n","train.shape, features.shape, patient_notes.shape, test.shape"]},{"cell_type":"code","execution_count":12,"id":"visible-australia","metadata":{"id":"visible-australia","executionInfo":{"status":"ok","timestamp":1646540166228,"user_tz":-540,"elapsed":5,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["if CFG.debug:\n","    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n","    print(train.shape)"]},{"cell_type":"markdown","id":"hydraulic-gibson","metadata":{"id":"hydraulic-gibson"},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":13,"id":"interpreted-northeast","metadata":{"id":"interpreted-northeast","executionInfo":{"status":"ok","timestamp":1646540166229,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def preprocess_features(features):\n","    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n","    return features\n","\n","\n","features = preprocess_features(features)"]},{"cell_type":"code","execution_count":14,"id":"martial-blind","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1646540166496,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"martial-blind","outputId":"42a83988-2022-4dc2-a38e-7f156b490c5a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14300, 8), (5, 6))"]},"metadata":{},"execution_count":14}],"source":["train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","\n","train.shape, test.shape"]},{"cell_type":"code","execution_count":15,"id":"electoral-favor","metadata":{"id":"electoral-favor","executionInfo":{"status":"ok","timestamp":1646540166497,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n","train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"]},{"cell_type":"code","execution_count":16,"id":"reported-parade","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1646540166498,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"reported-parade","outputId":"60a64543-0419-4942-9673-73f0921c1d34"},"outputs":[{"output_type":"display_data","data":{"text/plain":["0    4399\n","1    8181\n","2    1296\n","3     287\n","4      99\n","5      27\n","6       9\n","7       1\n","8       1\n","Name: annotation_length, dtype: int64"]},"metadata":{}}],"source":["train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n","display(train['annotation_length'].value_counts().sort_index())"]},{"cell_type":"markdown","id":"enabling-relevance","metadata":{"id":"enabling-relevance"},"source":["## CV split"]},{"cell_type":"code","execution_count":17,"id":"mature-coalition","metadata":{"id":"mature-coalition","executionInfo":{"status":"ok","timestamp":1646540166499,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def get_groupkfold(df, group_name):\n","    groups = df[group_name].unique()\n","\n","    kf = KFold(\n","        n_splits=CFG.n_fold,\n","        shuffle=True,\n","        random_state=CFG.seed,\n","    )\n","    folds_ids = []\n","    for i_fold, (_, val_group_idx) in enumerate(kf.split(groups)):\n","        val_group = groups[val_group_idx]\n","        is_val = df[group_name].isin(val_group)\n","        val_idx = df[is_val].index\n","        df.loc[val_idx, \"fold\"] = int(i_fold)\n","\n","    df[\"fold\"] = df[\"fold\"].astype(int)\n","    return df"]},{"cell_type":"code","execution_count":18,"id":"every-minutes","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"elapsed":410,"status":"ok","timestamp":1646540166901,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"every-minutes","outputId":"b1991a8c-a884-4b5a-8ad9-58990576bef7"},"outputs":[{"output_type":"display_data","data":{"text/plain":["fold\n","0    2902\n","1    2894\n","2    2813\n","3    2791\n","4    2900\n","dtype: int64"]},"metadata":{}}],"source":["train = get_groupkfold(train, \"pn_num\")\n","display(train.groupby(\"fold\").size())"]},{"cell_type":"markdown","id":"subjective-entrance","metadata":{"id":"subjective-entrance"},"source":["## Setup tokenizer"]},{"cell_type":"code","execution_count":19,"id":"dramatic-afghanistan","metadata":{"id":"dramatic-afghanistan","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646540168429,"user_tz":-540,"elapsed":1538,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}},"outputId":"f56c7a0a-6282-4575-c71e-5e908bc30b95"},"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["if CFG.submission:\n","    tokenizer = DebertaV2TokenizerFast.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n","else:\n","    tokenizer = DebertaV2TokenizerFast.from_pretrained(CFG.pretrained_model_name)\n","    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n","\n","CFG.tokenizer = tokenizer"]},{"cell_type":"markdown","id":"divided-arrow","metadata":{"id":"divided-arrow"},"source":["## Create dataset"]},{"cell_type":"code","execution_count":20,"id":"immune-campbell","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["661a9a315f8646a49162891ae47c69e7","ef004a834af944abbd512fa3218642a1","74fa3a6b51ad46958e58de5580cf5333","810a830f3b6743b9b074867dd8e4e179","7316ae87cfb849898eb022e100730ba2","c7cb034c107247cba318475c9952b4ac","7d891639f26644e8a05d7fe38d178245","c8a7a19edb074139baefe21f1901d4f4","7ed0ca5ee62d45d89050f3caf3d528c9","0b48bd338cc94fe396aa1b736b9a2507","2ddd9fc857b549a4ba446dd64a1dd1d4"]},"executionInfo":{"elapsed":23075,"status":"ok","timestamp":1646540191498,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"immune-campbell","outputId":"6b41528b-6e83-4946-cbd5-62621fd1ad43"},"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"661a9a315f8646a49162891ae47c69e7","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/42146 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 323\n"]}],"source":["pn_history_lengths = []\n","tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    pn_history_lengths.append(length)\n","\n","print(\"max length:\", np.max(pn_history_lengths))"]},{"cell_type":"code","execution_count":21,"id":"northern-branch","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["f319feca977544738ff2400ab23a9276","26b1a86ee1ff4ce2862c13d47be2b2d6","9815ec90f12a4696a85db6dc629ec62a","e032f2bf0bb241c2911087a6efe1ce0b","19783f5141cb47f8aaa057fb01dda913","953c495e9f64430cbdd9184bb0bd35cb","7e210db5a5fe41f696351dc87d525ee4","1ad701d95f084c98bd1bf0e9d7d498a9","6d74dcf5002c4752af12a65c3aca2113","1faca6dc4b0e43988d2f81cd209297be","2c55e9e0223548fbbbe29b3e11e59d50"]},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1646540191499,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"northern-branch","outputId":"82a35c47-ca3a-441a-ff12-3dc32603677d"},"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f319feca977544738ff2400ab23a9276","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/143 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 28\n"]}],"source":["feature_text_lengths = []\n","tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    feature_text_lengths.append(length)\n","\n","print(\"max length:\", np.max(feature_text_lengths))"]},{"cell_type":"code","execution_count":22,"id":"oriental-jacksonville","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1646540191500,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"oriental-jacksonville","outputId":"0ccfd10a-251f-49de-ce35-c4db182768ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["max length: 354\n"]}],"source":["CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n","\n","print(\"max length:\", CFG.max_len)"]},{"cell_type":"code","execution_count":23,"id":"flexible-trainer","metadata":{"id":"flexible-trainer","executionInfo":{"status":"ok","timestamp":1646540191867,"user_tz":-540,"elapsed":25,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class TrainingDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","        self.annotation_lengths = self.df[\"annotation_length\"].values\n","        self.locations = self.df[\"location\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def _create_label(self, pn_history, annotation_length, location_list):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        offset_mapping = encoded[\"offset_mapping\"]\n","        ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n","        label = np.zeros(len(offset_mapping))\n","        label[ignore_idxes] = -1\n","\n","        if annotation_length > 0:\n","            for location in location_list:\n","                for loc in [s.split() for s in location.split(\";\")]:\n","                    start, end = int(loc[0]), int(loc[1])\n","                    start_idx = -1\n","                    end_idx = -1\n","                    for idx in range(len(offset_mapping)):\n","                        if (start_idx == -1) & (start < offset_mapping[idx][0]):\n","                            start_idx = idx - 1\n","                        if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n","                            end_idx = idx + 1\n","                    if start_idx == -1:\n","                        start_idx = end_idx\n","                    if (start_idx != -1) & (end_idx != -1):\n","                        label[start_idx:end_idx] = 1\n","\n","        return torch.tensor(label, dtype=torch.float)\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n","        return input_, label"]},{"cell_type":"code","execution_count":24,"id":"stock-robertson","metadata":{"id":"stock-robertson","executionInfo":{"status":"ok","timestamp":1646540191867,"user_tz":-540,"elapsed":25,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class TestDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        return input_"]},{"cell_type":"markdown","id":"chemical-lucas","metadata":{"id":"chemical-lucas"},"source":["## Model"]},{"cell_type":"code","execution_count":25,"id":"animated-array","metadata":{"id":"animated-array","executionInfo":{"status":"ok","timestamp":1646540191868,"user_tz":-540,"elapsed":25,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class CustomModel(nn.Module):\n","    def __init__(self, cfg, model_config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        if model_config_path is None:\n","            self.model_config = AutoConfig.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                output_hidden_states=True,\n","            )\n","        else:\n","            self.model_config = torch.load(model_config_path)\n","\n","        if pretrained:\n","            self.backbone = AutoModel.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                config=self.model_config,\n","            )\n","            print(f\"Load weight from pretrained\")\n","        else:\n","            self.backbone = AutoModel.from_config(self.model_config)\n","\n","        \"\"\"\n","        itpt = AutoModelForMaskedLM.from_config(self.model_config)\n","        #path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp009/checkpoint-129000/pytorch_model.bin\")\n","        path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n","        state_dict = torch.load(path)\n","        itpt.load_state_dict(state_dict)\n","        self.backbone = itpt.deberta\n","        print(f\"Load weight from {path}\")\n","        \"\"\"\n","\n","        self.fc = nn.Sequential(\n","            nn.Dropout(self.cfg.dropout),\n","            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n","        )\n","\n","    def forward(self, inputs):\n","        h = self.backbone(**inputs)[\"last_hidden_state\"]\n","        output = self.fc(h)\n","        return output"]},{"cell_type":"markdown","id":"thorough-bristol","metadata":{"id":"thorough-bristol"},"source":["## Training"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class FocalLoss(nn.Module):\n","    \"\"\"https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/65938\n","    \"\"\"\n","    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n","        super(FocalLoss, self).__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.logits = logits\n","        self.reduce = reduce\n","\n","    def forward(self, inputs, targets):\n","        if self.logits:\n","            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n","        else:\n","            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n","        pt = torch.exp(-BCE_loss)\n","        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n","\n","        if self.reduce:\n","            return torch.mean(F_loss)\n","        else:\n","            return F_loss"],"metadata":{"id":"n8Z5UnO9cCxW","executionInfo":{"status":"ok","timestamp":1646540357097,"user_tz":-540,"elapsed":2,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"id":"n8Z5UnO9cCxW","execution_count":33,"outputs":[]},{"cell_type":"code","execution_count":27,"id":"talented-quantity","metadata":{"id":"talented-quantity","executionInfo":{"status":"ok","timestamp":1646540191869,"user_tz":-540,"elapsed":26,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def train_fn(\n","    train_dataloader,\n","    model,\n","    criterion,\n","    optimizer,\n","    epoch,\n","    scheduler,\n","    device,\n","):\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(train_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","\n","        pos_nums = (labels == 1).sum(axis=1)\n","        pos_nums = torch.tensor([[pos_num] * CFG.max_len for pos_num in pos_nums]).view(-1, 1).to(device)\n","        pos_nums = torch.masked_select(pos_nums, labels.view(-1, 1) != -1)\n","        weight = []\n","        for pos_num in pos_nums:\n","            if pos_num == 0:\n","                weight.append(3.0)\n","            else:\n","                weight.append(1.0)\n","        weight = torch.tensor(weight).to(device)\n","        loss = loss * weight\n","\n","        loss = loss.mean()\n","\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","\n","        if CFG.batch_scheduler:\n","            scheduler.step()\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n","            print(\n","                \"Epoch: [{0}][{1}/{2}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                \"Grad: {grad_norm:.4f}  \"\n","                \"LR: {lr:.6f}  \"\n","                .format(\n","                    epoch+1,\n","                    step,\n","                    len(train_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n","                    loss=losses,\n","                     grad_norm=grad_norm,\n","                     lr=scheduler.get_lr()[0],\n","                )\n","            )\n","    return losses.avg"]},{"cell_type":"code","execution_count":28,"id":"figured-cooperative","metadata":{"id":"figured-cooperative","executionInfo":{"status":"ok","timestamp":1646540191870,"user_tz":-540,"elapsed":26,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def valid_fn(\n","    val_dataloader,\n","    model,\n","    criterion,\n","    device,\n","):\n","    model.eval()\n","    preds = []\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(val_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.no_grad():\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","\n","        pos_nums = (labels == 1).sum(axis=1)\n","        pos_nums = torch.tensor([[pos_num] * CFG.max_len for pos_num in pos_nums]).view(-1, 1).to(device)\n","        pos_nums = torch.masked_select(pos_nums, labels.view(-1, 1) != -1)\n","        weight = []\n","        for pos_num in pos_nums:\n","            if pos_num == 0:\n","                weight.append(3.0)\n","            else:\n","                weight.append(1.0)\n","        weight = torch.tensor(weight).to(device)\n","        loss = loss * weight\n","\n","        loss = loss.mean()\n","\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n","            print(\n","                \"EVAL: [{0}/{1}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                .format(\n","                    step, len(val_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n","                    loss=losses,\n","                )\n","            )\n","    preds = np.concatenate(preds)\n","    return losses.avg, preds"]},{"cell_type":"code","execution_count":29,"id":"played-pointer","metadata":{"id":"played-pointer","executionInfo":{"status":"ok","timestamp":1646540191870,"user_tz":-540,"elapsed":26,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def inference_fn(test_dataloader, model, device):\n","    model.eval()\n","    model.to(device)\n","    preds = []\n","    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n","    for inputs in tk0:\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        with torch.no_grad():\n","            output = model(inputs)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","    preds = np.concatenate(preds)\n","    return preds"]},{"cell_type":"code","execution_count":30,"id":"brazilian-nigeria","metadata":{"id":"brazilian-nigeria","executionInfo":{"status":"ok","timestamp":1646540191871,"user_tz":-540,"elapsed":27,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def train_loop(df, i_fold, device):\n","    print(f\"========== fold: {i_fold} training ==========\")\n","    train_idx = df[df[\"fold\"] != i_fold].index\n","    val_idx = df[df[\"fold\"] == i_fold].index\n","\n","    train_folds = df.loc[train_idx].reset_index(drop=True)\n","    val_folds = df.loc[val_idx].reset_index(drop=True)\n","\n","    train_dataset = TrainingDataset(CFG, train_folds)\n","    val_dataset = TrainingDataset(CFG, val_folds)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=True,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=True,\n","    )\n","    val_dataloader = DataLoader(\n","        val_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=False,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=False,\n","    )\n","\n","    model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n","    model.to(device)\n","\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\"params\": [p for n, p in param_optimizer if not any(\n","            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n","        {\"params\": [p for n, p in param_optimizer if any(\n","            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n","    ]\n","    optimizer = AdamW(\n","        optimizer_grouped_parameters,\n","        lr=CFG.lr,\n","        betas=CFG.betas,\n","        weight_decay=CFG.weight_decay,\n","    )\n","    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n","    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        num_training_steps=num_train_optimization_steps,\n","    )\n","\n","    #criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    criterion = FocalLoss(reduce=False)\n","    best_score = -1 * np.inf\n","\n","    for epoch in range(CFG.epochs):\n","        start_time = time.time()\n","        avg_loss = train_fn(\n","            train_dataloader,\n","            model,\n","            criterion,\n","            optimizer,\n","            epoch,\n","            scheduler,\n","            device,\n","        )\n","        avg_val_loss, val_preds = valid_fn(\n","            val_dataloader,\n","            model,\n","            criterion,\n","            device,\n","        )\n","\n","        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n","            scheduler.step()\n","\n","        # scoring\n","        val_folds[[str(i) for i in range(CFG.max_len)]] = val_preds\n","        score = scoring(val_folds, th=0.5)\n","\n","        elapsed = time.time() - start_time\n","\n","        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n","        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n","        if score > best_score:\n","            best_score = score\n","            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n","            torch.save({\n","                \"model\": model.state_dict(),\n","                \"predictions\": val_preds,\n","                },\n","                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","            )\n","\n","    predictions = torch.load(\n","        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","        map_location=torch.device(\"cpu\"),\n","    )[\"predictions\"]\n","    val_folds[[str(i) for i in range(CFG.max_len)]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return val_folds"]},{"cell_type":"markdown","id":"bearing-switch","metadata":{"id":"bearing-switch"},"source":["## Main"]},{"cell_type":"code","execution_count":31,"id":"desperate-crime","metadata":{"id":"desperate-crime","executionInfo":{"status":"ok","timestamp":1646540191871,"user_tz":-540,"elapsed":26,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def main():\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    if CFG.train:\n","        oof_df = pd.DataFrame()\n","        for i_fold in range(CFG.n_fold):\n","            if i_fold in CFG.train_fold:\n","                _oof_df = train_loop(train, i_fold, device)\n","                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n","        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    if CFG.submission:\n","        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n","    else:\n","        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    score = scoring(oof_df, th=0.5)\n","    print(f\"Best thres: 0.5, Score: {score:.4f}\")\n","    best_thres = get_best_thres(oof_df)\n","    score = scoring(oof_df, th=best_thres)\n","    print(f\"Best thres: {best_thres}, Score: {score:.4f}\")\n","\n","    if CFG.inference:\n","        test_dataset = TestDataset(CFG, test)\n","        test_dataloader = DataLoader(\n","            test_dataset,\n","            batch_size=CFG.batch_size,\n","            shuffle=False,\n","            num_workers=CFG.num_workers,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","        predictions = []\n","        for i_fold in CFG.train_fold:\n","            if CFG.submission:\n","                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n","                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n","            else:\n","                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n","\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            model.load_state_dict(state[\"model\"])\n","            test_token_probs = inference_fn(test_dataloader, model, device)\n","            test[[f\"fold{i_fold}_{i}\" for i in range(CFG.max_len)]] = test_token_probs\n","            test_char_probs = get_char_probs(test[\"pn_history\"].values, test_token_probs, CFG.tokenizer)\n","            predictions.append(test_char_probs)\n","\n","            del state, test_token_probs, model; gc.collect()\n","            torch.cuda.empty_cache()\n","\n","        predictions = np.mean(predictions, axis=0)\n","        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n","        test[CFG.target_col] = predicted_location_str\n","        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n","        test[[CFG.id_col, CFG.target_col]].to_csv(\n","            CFG.output_dir / \"submission.csv\", index=False\n","        )"]},{"cell_type":"code","execution_count":34,"id":"graduate-vision","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["f39640d290374992aa246753125a91de","410c3733ee43430eb55278748d07bc45","5292a911912d43c2b80919e486b99de9","ce26114873ed4c96b5ea391a41b18f68","bec237aed5184115b697ea257f7b0c9b","3ca14e3fd6b84312b0af50a89d5ac7c1","8234c7d9369644dea7e5c7e8fe436771","5a3f361a320f480aa8a4115366073d32","0f856d468c8a4c4c9c83b5b263745508","490bfe688fe1419996b69f7de1cfee23","789324d1692d4f478d5b95491b03fc22","1193874a74974cc59982c8d5e3ced585","84ea1506dcad4e01ad1cc35b76c0339a","8e243ea82e59492fb5b845e51a56347a","55ac42bee2ce4f00841b8bd49a7c552d","2281dd4891c640a0b31c23976223f2ba","6919ba0239084b04988e1de02316c76e","548835fe547d4114bfd39e5fac680635","5068fb514bf143ba812fe202c3e7a83d","1e0d277fe44242e19e3bec17a1cb7280","ab8e5c4cef00426fa0cf2fc25c51381a","2b311e42f1294339a07248b31db0c26c","fc3c6209df394eefa2df9ce8dbb56830","3fb5b968d9ab4e88964b6b126c6023d9","d240d13622c14726a5639d44ef2421ec","9e66574c8c0343ffb0477891bfe5e892","219090e2dd934c1296f12660ea69b161","e7be4ec44f2a4183b295f486e250b414","44650208feba4c118904c7efc9887532","d1cd0285cfe34f188e9c779617d48448","331b7288a5024ce3a5036af53eb75cec","00419a15e9834e98b8a3459b62d01f8b","05fdce5a55c1483a937d07a50bd9465e","5536b7aaba7c41f28197e318b362ec75","5ecd28892bb84432935145e27ac71de7","3b50983bfae8445fa305d1edadd651af","cdbf6aefee644006826f76e2f6722b07","7c62f6a2b08c41a8bc3ecf2efa58c325","8478e8bf8b6146b48e717b84c021e7ab","b09413b459c8406882a16db62e8df9c0","72a8b4ea52534d4e9feab6c6fdd72a77","65d16c05424c4df0b79b5786be8bd5d1","63aa4d26409d437fa76e5a156bb04791","e48e5c946462499ab018748ccf80c5b5","160e78a145894001b2a1295627d80df9","008f77fefdba425ab2c755f515693e6f","605151b49d7641a28ebd0ca083770c69","8f8c8632070c4fa0a3182521f41e9c40","dfb4641da88e47d3bafabbaa56bc6916","537dee640701470c8fc3cc29e7940bee","6dc964a8934744608f92a6af0f0f923f","925a3ae98bd6488eb7cffdec89d768da","d76610cad4f645f187b26f1b82733569","b28fa99d1b1b4e4da668bcc50373e4cc","b8554928c5f141de8dfb94c04c2dda03"]},"executionInfo":{"elapsed":19859048,"status":"ok","timestamp":1646560224051,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"graduate-vision","outputId":"2a3a96e3-9421-4bcd-d191-898f1c27a819"},"outputs":[{"output_type":"stream","name":"stdout","text":["========== fold: 0 training ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n","Epoch: [1][0/2849] Elapsed 0m 0s (remain 29m 12s) Loss: 0.0564(0.0564) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/2849] Elapsed 0m 24s (remain 11m 11s) Loss: 0.0199(0.0526) Grad: 15805.7119  LR: 0.000001  \n","Epoch: [1][200/2849] Elapsed 0m 48s (remain 10m 44s) Loss: 0.0202(0.0343) Grad: 6194.0234  LR: 0.000003  \n","Epoch: [1][300/2849] Elapsed 1m 12s (remain 10m 17s) Loss: 0.0105(0.0269) Grad: 3086.9182  LR: 0.000004  \n","Epoch: [1][400/2849] Elapsed 1m 37s (remain 9m 52s) Loss: 0.0075(0.0223) Grad: 6688.9136  LR: 0.000006  \n","Epoch: [1][500/2849] Elapsed 2m 1s (remain 9m 27s) Loss: 0.0028(0.0192) Grad: 2457.4470  LR: 0.000007  \n","Epoch: [1][600/2849] Elapsed 2m 25s (remain 9m 2s) Loss: 0.0028(0.0171) Grad: 3114.5430  LR: 0.000008  \n","Epoch: [1][700/2849] Elapsed 2m 49s (remain 8m 38s) Loss: 0.0030(0.0154) Grad: 2088.9631  LR: 0.000010  \n","Epoch: [1][800/2849] Elapsed 3m 13s (remain 8m 14s) Loss: 0.0004(0.0139) Grad: 843.8429  LR: 0.000011  \n","Epoch: [1][900/2849] Elapsed 3m 37s (remain 7m 49s) Loss: 0.0066(0.0128) Grad: 5167.6304  LR: 0.000013  \n","Epoch: [1][1000/2849] Elapsed 4m 1s (remain 7m 25s) Loss: 0.0007(0.0119) Grad: 861.8798  LR: 0.000014  \n","Epoch: [1][1100/2849] Elapsed 4m 25s (remain 7m 1s) Loss: 0.0010(0.0112) Grad: 773.0765  LR: 0.000015  \n","Epoch: [1][1200/2849] Elapsed 4m 49s (remain 6m 37s) Loss: 0.0042(0.0106) Grad: 5444.1245  LR: 0.000017  \n","Epoch: [1][1300/2849] Elapsed 5m 13s (remain 6m 13s) Loss: 0.0323(0.0101) Grad: 16547.4082  LR: 0.000018  \n","Epoch: [1][1400/2849] Elapsed 5m 37s (remain 5m 48s) Loss: 0.0009(0.0097) Grad: 821.6253  LR: 0.000020  \n","Epoch: [1][1500/2849] Elapsed 6m 1s (remain 5m 24s) Loss: 0.0008(0.0093) Grad: 1313.4556  LR: 0.000020  \n","Epoch: [1][1600/2849] Elapsed 6m 25s (remain 5m 0s) Loss: 0.0064(0.0089) Grad: 6511.4248  LR: 0.000020  \n","Epoch: [1][1700/2849] Elapsed 6m 49s (remain 4m 36s) Loss: 0.0143(0.0086) Grad: 6964.6655  LR: 0.000020  \n","Epoch: [1][1800/2849] Elapsed 7m 13s (remain 4m 12s) Loss: 0.0021(0.0083) Grad: 1513.9843  LR: 0.000019  \n","Epoch: [1][1900/2849] Elapsed 7m 38s (remain 3m 48s) Loss: 0.0040(0.0080) Grad: 7817.6118  LR: 0.000019  \n","Epoch: [1][2000/2849] Elapsed 8m 2s (remain 3m 24s) Loss: 0.0004(0.0078) Grad: 255.3816  LR: 0.000019  \n","Epoch: [1][2100/2849] Elapsed 8m 26s (remain 3m 0s) Loss: 0.0007(0.0076) Grad: 526.5635  LR: 0.000019  \n","Epoch: [1][2200/2849] Elapsed 8m 50s (remain 2m 36s) Loss: 0.0058(0.0074) Grad: 4939.1035  LR: 0.000019  \n","Epoch: [1][2300/2849] Elapsed 9m 14s (remain 2m 12s) Loss: 0.0009(0.0072) Grad: 483.2671  LR: 0.000019  \n","Epoch: [1][2400/2849] Elapsed 9m 38s (remain 1m 47s) Loss: 0.0060(0.0070) Grad: 2307.7449  LR: 0.000018  \n","Epoch: [1][2500/2849] Elapsed 10m 2s (remain 1m 23s) Loss: 0.0002(0.0068) Grad: 442.9607  LR: 0.000018  \n","Epoch: [1][2600/2849] Elapsed 10m 26s (remain 0m 59s) Loss: 0.0014(0.0067) Grad: 2880.1995  LR: 0.000018  \n","Epoch: [1][2700/2849] Elapsed 10m 50s (remain 0m 35s) Loss: 0.0003(0.0065) Grad: 813.3593  LR: 0.000018  \n","Epoch: [1][2800/2849] Elapsed 11m 15s (remain 0m 11s) Loss: 0.0004(0.0064) Grad: 375.0701  LR: 0.000018  \n","Epoch: [1][2848/2849] Elapsed 11m 26s (remain 0m 0s) Loss: 0.0021(0.0063) Grad: 5341.7842  LR: 0.000018  \n","EVAL: [0/726] Elapsed 0m 0s (remain 4m 50s) Loss: 0.0011(0.0011) \n","EVAL: [100/726] Elapsed 0m 10s (remain 1m 4s) Loss: 0.0009(0.0027) \n","EVAL: [200/726] Elapsed 0m 20s (remain 0m 53s) Loss: 0.0002(0.0031) \n","EVAL: [300/726] Elapsed 0m 30s (remain 0m 42s) Loss: 0.0002(0.0030) \n","EVAL: [400/726] Elapsed 0m 40s (remain 0m 32s) Loss: 0.0008(0.0037) \n","EVAL: [500/726] Elapsed 0m 50s (remain 0m 22s) Loss: 0.0081(0.0036) \n","EVAL: [600/726] Elapsed 1m 0s (remain 0m 12s) Loss: 0.0002(0.0034) \n","EVAL: [700/726] Elapsed 1m 10s (remain 0m 2s) Loss: 0.0003(0.0031) \n","EVAL: [725/726] Elapsed 1m 12s (remain 0m 0s) Loss: 0.0000(0.0031) \n","Epoch 1 - avg_train_loss: 0.0063  avg_val_loss: 0.0031  time: 763s\n","Epoch 1 - Score: 0.8322\n","Epoch 1 - Save Best Score: 0.8322 Model\n","Epoch: [2][0/2849] Elapsed 0m 0s (remain 25m 35s) Loss: 0.0003(0.0003) Grad: 1299.5465  LR: 0.000018  \n","Epoch: [2][100/2849] Elapsed 0m 25s (remain 11m 26s) Loss: 0.0005(0.0020) Grad: 1996.3761  LR: 0.000018  \n","Epoch: [2][200/2849] Elapsed 0m 49s (remain 10m 50s) Loss: 0.0004(0.0019) Grad: 2619.9817  LR: 0.000017  \n","Epoch: [2][300/2849] Elapsed 1m 13s (remain 10m 22s) Loss: 0.0003(0.0019) Grad: 3191.5884  LR: 0.000017  \n","Epoch: [2][400/2849] Elapsed 1m 37s (remain 9m 56s) Loss: 0.0023(0.0022) Grad: 13056.1836  LR: 0.000017  \n","Epoch: [2][500/2849] Elapsed 2m 2s (remain 9m 32s) Loss: 0.0000(0.0022) Grad: 87.4317  LR: 0.000017  \n","Epoch: [2][600/2849] Elapsed 2m 26s (remain 9m 7s) Loss: 0.0039(0.0023) Grad: 9249.1182  LR: 0.000017  \n","Epoch: [2][700/2849] Elapsed 2m 50s (remain 8m 42s) Loss: 0.0001(0.0023) Grad: 613.7449  LR: 0.000017  \n","Epoch: [2][800/2849] Elapsed 3m 14s (remain 8m 17s) Loss: 0.0002(0.0023) Grad: 1320.9270  LR: 0.000017  \n","Epoch: [2][900/2849] Elapsed 3m 38s (remain 7m 53s) Loss: 0.0001(0.0023) Grad: 324.5984  LR: 0.000016  \n","Epoch: [2][1000/2849] Elapsed 4m 3s (remain 7m 28s) Loss: 0.0013(0.0022) Grad: 5952.5400  LR: 0.000016  \n","Epoch: [2][1100/2849] Elapsed 4m 27s (remain 7m 4s) Loss: 0.0086(0.0022) Grad: 27933.3984  LR: 0.000016  \n","Epoch: [2][1200/2849] Elapsed 4m 51s (remain 6m 39s) Loss: 0.0001(0.0022) Grad: 511.1632  LR: 0.000016  \n","Epoch: [2][1300/2849] Elapsed 5m 15s (remain 6m 15s) Loss: 0.0001(0.0022) Grad: 468.7723  LR: 0.000016  \n","Epoch: [2][1400/2849] Elapsed 5m 39s (remain 5m 51s) Loss: 0.0006(0.0022) Grad: 27395.9082  LR: 0.000016  \n","Epoch: [2][1500/2849] Elapsed 6m 3s (remain 5m 26s) Loss: 0.0019(0.0022) Grad: 6613.5811  LR: 0.000015  \n","Epoch: [2][1600/2849] Elapsed 6m 28s (remain 5m 2s) Loss: 0.0016(0.0022) Grad: 14678.5986  LR: 0.000015  \n","Epoch: [2][1700/2849] Elapsed 6m 52s (remain 4m 38s) Loss: 0.0048(0.0022) Grad: 9967.6074  LR: 0.000015  \n","Epoch: [2][1800/2849] Elapsed 7m 16s (remain 4m 14s) Loss: 0.0106(0.0022) Grad: 21161.3496  LR: 0.000015  \n","Epoch: [2][1900/2849] Elapsed 7m 40s (remain 3m 49s) Loss: 0.0046(0.0022) Grad: 12840.0791  LR: 0.000015  \n","Epoch: [2][2000/2849] Elapsed 8m 4s (remain 3m 25s) Loss: 0.0007(0.0022) Grad: 4897.0225  LR: 0.000015  \n","Epoch: [2][2100/2849] Elapsed 8m 29s (remain 3m 1s) Loss: 0.0040(0.0022) Grad: 59726.8516  LR: 0.000014  \n","Epoch: [2][2200/2849] Elapsed 8m 53s (remain 2m 37s) Loss: 0.0000(0.0022) Grad: 52.9830  LR: 0.000014  \n","Epoch: [2][2300/2849] Elapsed 9m 17s (remain 2m 12s) Loss: 0.0043(0.0022) Grad: 13511.2520  LR: 0.000014  \n","Epoch: [2][2400/2849] Elapsed 9m 41s (remain 1m 48s) Loss: 0.0013(0.0022) Grad: 3093.3657  LR: 0.000014  \n","Epoch: [2][2500/2849] Elapsed 10m 6s (remain 1m 24s) Loss: 0.0027(0.0022) Grad: 7246.3887  LR: 0.000014  \n","Epoch: [2][2600/2849] Elapsed 10m 30s (remain 1m 0s) Loss: 0.0013(0.0022) Grad: 5923.8818  LR: 0.000014  \n","Epoch: [2][2700/2849] Elapsed 10m 54s (remain 0m 35s) Loss: 0.0059(0.0022) Grad: 53968.9219  LR: 0.000014  \n","Epoch: [2][2800/2849] Elapsed 11m 19s (remain 0m 11s) Loss: 0.0005(0.0022) Grad: 1939.1736  LR: 0.000013  \n","Epoch: [2][2848/2849] Elapsed 11m 30s (remain 0m 0s) Loss: 0.0000(0.0022) Grad: 218.2398  LR: 0.000013  \n","EVAL: [0/726] Elapsed 0m 0s (remain 4m 42s) Loss: 0.0002(0.0002) \n","EVAL: [100/726] Elapsed 0m 10s (remain 1m 3s) Loss: 0.0015(0.0022) \n","EVAL: [200/726] Elapsed 0m 20s (remain 0m 52s) Loss: 0.0001(0.0025) \n","EVAL: [300/726] Elapsed 0m 30s (remain 0m 42s) Loss: 0.0000(0.0025) \n","EVAL: [400/726] Elapsed 0m 40s (remain 0m 32s) Loss: 0.0004(0.0031) \n","EVAL: [500/726] Elapsed 0m 50s (remain 0m 22s) Loss: 0.0062(0.0030) \n","EVAL: [600/726] Elapsed 1m 0s (remain 0m 12s) Loss: 0.0031(0.0028) \n","EVAL: [700/726] Elapsed 1m 9s (remain 0m 2s) Loss: 0.0003(0.0026) \n","EVAL: [725/726] Elapsed 1m 12s (remain 0m 0s) Loss: 0.0000(0.0026) \n","Epoch 2 - avg_train_loss: 0.0022  avg_val_loss: 0.0026  time: 767s\n","Epoch 2 - Score: 0.8725\n","Epoch 2 - Save Best Score: 0.8725 Model\n","Epoch: [3][0/2849] Elapsed 0m 0s (remain 26m 8s) Loss: 0.0003(0.0003) Grad: 1223.5513  LR: 0.000013  \n","Epoch: [3][100/2849] Elapsed 0m 25s (remain 11m 41s) Loss: 0.0009(0.0019) Grad: 3133.4390  LR: 0.000013  \n","Epoch: [3][200/2849] Elapsed 0m 50s (remain 11m 0s) Loss: 0.0024(0.0020) Grad: 4238.6694  LR: 0.000013  \n","Epoch: [3][300/2849] Elapsed 1m 14s (remain 10m 28s) Loss: 0.0005(0.0017) Grad: 3978.6694  LR: 0.000013  \n","Epoch: [3][400/2849] Elapsed 1m 38s (remain 10m 0s) Loss: 0.0010(0.0018) Grad: 3755.9729  LR: 0.000013  \n","Epoch: [3][500/2849] Elapsed 2m 2s (remain 9m 34s) Loss: 0.0007(0.0018) Grad: 5453.3872  LR: 0.000013  \n","Epoch: [3][600/2849] Elapsed 2m 26s (remain 9m 8s) Loss: 0.0029(0.0017) Grad: 5429.8018  LR: 0.000012  \n","Epoch: [3][700/2849] Elapsed 2m 51s (remain 8m 44s) Loss: 0.0024(0.0017) Grad: 5592.1362  LR: 0.000012  \n","Epoch: [3][800/2849] Elapsed 3m 15s (remain 8m 18s) Loss: 0.0082(0.0017) Grad: 31988.0742  LR: 0.000012  \n","Epoch: [3][900/2849] Elapsed 3m 39s (remain 7m 54s) Loss: 0.0000(0.0016) Grad: 50.3539  LR: 0.000012  \n","Epoch: [3][1000/2849] Elapsed 4m 3s (remain 7m 29s) Loss: 0.0043(0.0016) Grad: 6524.9351  LR: 0.000012  \n","Epoch: [3][1100/2849] Elapsed 4m 27s (remain 7m 4s) Loss: 0.0030(0.0016) Grad: 14534.8125  LR: 0.000012  \n","Epoch: [3][1200/2849] Elapsed 4m 51s (remain 6m 40s) Loss: 0.0001(0.0016) Grad: 707.2166  LR: 0.000011  \n","Epoch: [3][1300/2849] Elapsed 5m 16s (remain 6m 16s) Loss: 0.0008(0.0016) Grad: 3929.8982  LR: 0.000011  \n","Epoch: [3][1400/2849] Elapsed 5m 40s (remain 5m 51s) Loss: 0.0000(0.0016) Grad: 9.5138  LR: 0.000011  \n","Epoch: [3][1500/2849] Elapsed 6m 4s (remain 5m 27s) Loss: 0.0008(0.0016) Grad: 3670.7422  LR: 0.000011  \n","Epoch: [3][1600/2849] Elapsed 6m 28s (remain 5m 2s) Loss: 0.0010(0.0016) Grad: 5009.2358  LR: 0.000011  \n","Epoch: [3][1700/2849] Elapsed 6m 52s (remain 4m 38s) Loss: 0.0047(0.0016) Grad: 16367.5449  LR: 0.000011  \n","Epoch: [3][1800/2849] Elapsed 7m 16s (remain 4m 14s) Loss: 0.0002(0.0016) Grad: 1483.6000  LR: 0.000011  \n","Epoch: [3][1900/2849] Elapsed 7m 41s (remain 3m 49s) Loss: 0.0003(0.0016) Grad: 1696.6705  LR: 0.000010  \n","Epoch: [3][2000/2849] Elapsed 8m 5s (remain 3m 25s) Loss: 0.0006(0.0017) Grad: 3269.1340  LR: 0.000010  \n","Epoch: [3][2100/2849] Elapsed 8m 29s (remain 3m 1s) Loss: 0.0004(0.0017) Grad: 1749.1030  LR: 0.000010  \n","Epoch: [3][2200/2849] Elapsed 8m 53s (remain 2m 37s) Loss: 0.0000(0.0016) Grad: 61.3168  LR: 0.000010  \n","Epoch: [3][2300/2849] Elapsed 9m 17s (remain 2m 12s) Loss: 0.0000(0.0016) Grad: 9.1457  LR: 0.000010  \n","Epoch: [3][2400/2849] Elapsed 9m 41s (remain 1m 48s) Loss: 0.0003(0.0016) Grad: 1676.9304  LR: 0.000010  \n","Epoch: [3][2500/2849] Elapsed 10m 6s (remain 1m 24s) Loss: 0.0000(0.0017) Grad: 200.6717  LR: 0.000009  \n","Epoch: [3][2600/2849] Elapsed 10m 30s (remain 1m 0s) Loss: 0.0009(0.0016) Grad: 8834.1455  LR: 0.000009  \n","Epoch: [3][2700/2849] Elapsed 10m 54s (remain 0m 35s) Loss: 0.0006(0.0017) Grad: 4051.0464  LR: 0.000009  \n","Epoch: [3][2800/2849] Elapsed 11m 18s (remain 0m 11s) Loss: 0.0000(0.0017) Grad: 36.5028  LR: 0.000009  \n","Epoch: [3][2848/2849] Elapsed 11m 30s (remain 0m 0s) Loss: 0.0066(0.0017) Grad: 21306.9004  LR: 0.000009  \n","EVAL: [0/726] Elapsed 0m 0s (remain 4m 42s) Loss: 0.0002(0.0002) \n","EVAL: [100/726] Elapsed 0m 10s (remain 1m 3s) Loss: 0.0014(0.0028) \n","EVAL: [200/726] Elapsed 0m 20s (remain 0m 53s) Loss: 0.0001(0.0026) \n","EVAL: [300/726] Elapsed 0m 30s (remain 0m 42s) Loss: 0.0000(0.0027) \n","EVAL: [400/726] Elapsed 0m 40s (remain 0m 32s) Loss: 0.0003(0.0034) \n","EVAL: [500/726] Elapsed 0m 50s (remain 0m 22s) Loss: 0.0127(0.0034) \n","EVAL: [600/726] Elapsed 1m 0s (remain 0m 12s) Loss: 0.0044(0.0032) \n","EVAL: [700/726] Elapsed 1m 10s (remain 0m 2s) Loss: 0.0004(0.0030) \n","EVAL: [725/726] Elapsed 1m 12s (remain 0m 0s) Loss: 0.0000(0.0029) \n","Epoch 3 - avg_train_loss: 0.0017  avg_val_loss: 0.0029  time: 767s\n","Epoch 3 - Score: 0.8830\n","Epoch 3 - Save Best Score: 0.8830 Model\n","Epoch: [4][0/2849] Elapsed 0m 0s (remain 26m 25s) Loss: 0.0002(0.0002) Grad: 2142.2981  LR: 0.000009  \n","Epoch: [4][100/2849] Elapsed 0m 25s (remain 11m 37s) Loss: 0.0035(0.0013) Grad: 9375.5303  LR: 0.000009  \n","Epoch: [4][200/2849] Elapsed 0m 49s (remain 10m 56s) Loss: 0.0001(0.0012) Grad: 518.5944  LR: 0.000009  \n","Epoch: [4][300/2849] Elapsed 1m 14s (remain 10m 26s) Loss: 0.0006(0.0013) Grad: 1478.4323  LR: 0.000008  \n","Epoch: [4][400/2849] Elapsed 1m 38s (remain 9m 59s) Loss: 0.0025(0.0013) Grad: 9277.5674  LR: 0.000008  \n","Epoch: [4][500/2849] Elapsed 2m 2s (remain 9m 35s) Loss: 0.0073(0.0013) Grad: 14007.2998  LR: 0.000008  \n","Epoch: [4][600/2849] Elapsed 2m 26s (remain 9m 9s) Loss: 0.0018(0.0013) Grad: 6684.1118  LR: 0.000008  \n","Epoch: [4][700/2849] Elapsed 2m 51s (remain 8m 44s) Loss: 0.0000(0.0013) Grad: 22.5935  LR: 0.000008  \n","Epoch: [4][800/2849] Elapsed 3m 15s (remain 8m 19s) Loss: 0.0000(0.0012) Grad: 12.2394  LR: 0.000008  \n","Epoch: [4][900/2849] Elapsed 3m 39s (remain 7m 54s) Loss: 0.0008(0.0012) Grad: 3414.6782  LR: 0.000007  \n","Epoch: [4][1000/2849] Elapsed 4m 3s (remain 7m 30s) Loss: 0.0033(0.0012) Grad: 27356.8184  LR: 0.000007  \n","Epoch: [4][1100/2849] Elapsed 4m 28s (remain 7m 6s) Loss: 0.0000(0.0012) Grad: 21.3649  LR: 0.000007  \n","Epoch: [4][1200/2849] Elapsed 4m 52s (remain 6m 41s) Loss: 0.0002(0.0012) Grad: 841.1691  LR: 0.000007  \n","Epoch: [4][1300/2849] Elapsed 5m 16s (remain 6m 17s) Loss: 0.0003(0.0012) Grad: 2003.5558  LR: 0.000007  \n","Epoch: [4][1400/2849] Elapsed 5m 41s (remain 5m 52s) Loss: 0.0022(0.0012) Grad: 4684.5986  LR: 0.000007  \n","Epoch: [4][1500/2849] Elapsed 6m 5s (remain 5m 27s) Loss: 0.0001(0.0013) Grad: 572.9575  LR: 0.000007  \n","Epoch: [4][1600/2849] Elapsed 6m 29s (remain 5m 3s) Loss: 0.0010(0.0013) Grad: 5487.2056  LR: 0.000006  \n","Epoch: [4][1700/2849] Elapsed 6m 53s (remain 4m 39s) Loss: 0.0024(0.0013) Grad: 20992.4277  LR: 0.000006  \n","Epoch: [4][1800/2849] Elapsed 7m 17s (remain 4m 14s) Loss: 0.0078(0.0013) Grad: 9977.5488  LR: 0.000006  \n","Epoch: [4][1900/2849] Elapsed 7m 42s (remain 3m 50s) Loss: 0.0003(0.0014) Grad: 1773.2103  LR: 0.000006  \n","Epoch: [4][2000/2849] Elapsed 8m 6s (remain 3m 26s) Loss: 0.0003(0.0014) Grad: 1643.5508  LR: 0.000006  \n","Epoch: [4][2100/2849] Elapsed 8m 30s (remain 3m 1s) Loss: 0.0001(0.0014) Grad: 347.8923  LR: 0.000006  \n","Epoch: [4][2200/2849] Elapsed 8m 54s (remain 2m 37s) Loss: 0.0000(0.0013) Grad: 26.0824  LR: 0.000005  \n","Epoch: [4][2300/2849] Elapsed 9m 19s (remain 2m 13s) Loss: 0.0001(0.0013) Grad: 645.6558  LR: 0.000005  \n","Epoch: [4][2400/2849] Elapsed 9m 43s (remain 1m 48s) Loss: 0.0000(0.0013) Grad: 110.0095  LR: 0.000005  \n","Epoch: [4][2500/2849] Elapsed 10m 7s (remain 1m 24s) Loss: 0.0019(0.0013) Grad: 40953.7734  LR: 0.000005  \n","Epoch: [4][2600/2849] Elapsed 10m 31s (remain 1m 0s) Loss: 0.0002(0.0013) Grad: 2047.1909  LR: 0.000005  \n","Epoch: [4][2700/2849] Elapsed 10m 56s (remain 0m 35s) Loss: 0.0041(0.0013) Grad: 10304.7256  LR: 0.000005  \n","Epoch: [4][2800/2849] Elapsed 11m 20s (remain 0m 11s) Loss: 0.0000(0.0013) Grad: 86.7371  LR: 0.000005  \n","Epoch: [4][2848/2849] Elapsed 11m 32s (remain 0m 0s) Loss: 0.0011(0.0013) Grad: 6391.1826  LR: 0.000004  \n","EVAL: [0/726] Elapsed 0m 0s (remain 4m 47s) Loss: 0.0001(0.0001) \n","EVAL: [100/726] Elapsed 0m 10s (remain 1m 3s) Loss: 0.0008(0.0031) \n","EVAL: [200/726] Elapsed 0m 20s (remain 0m 52s) Loss: 0.0001(0.0030) \n","EVAL: [300/726] Elapsed 0m 30s (remain 0m 42s) Loss: 0.0000(0.0030) \n","EVAL: [400/726] Elapsed 0m 40s (remain 0m 32s) Loss: 0.0002(0.0037) \n","EVAL: [500/726] Elapsed 0m 49s (remain 0m 22s) Loss: 0.0101(0.0037) \n","EVAL: [600/726] Elapsed 0m 59s (remain 0m 12s) Loss: 0.0038(0.0034) \n","EVAL: [700/726] Elapsed 1m 9s (remain 0m 2s) Loss: 0.0010(0.0032) \n","EVAL: [725/726] Elapsed 1m 12s (remain 0m 0s) Loss: 0.0000(0.0031) \n","Epoch 4 - avg_train_loss: 0.0013  avg_val_loss: 0.0031  time: 768s\n","Epoch 4 - Score: 0.8844\n","Epoch 4 - Save Best Score: 0.8844 Model\n","Epoch: [5][0/2849] Elapsed 0m 0s (remain 26m 40s) Loss: 0.0001(0.0001) Grad: 1076.6130  LR: 0.000004  \n","Epoch: [5][100/2849] Elapsed 0m 25s (remain 11m 36s) Loss: 0.0036(0.0010) Grad: 10884.6025  LR: 0.000004  \n","Epoch: [5][200/2849] Elapsed 0m 49s (remain 10m 55s) Loss: 0.0001(0.0010) Grad: 420.4210  LR: 0.000004  \n","Epoch: [5][300/2849] Elapsed 1m 14s (remain 10m 27s) Loss: 0.0207(0.0011) Grad: 163447.3594  LR: 0.000004  \n","Epoch: [5][400/2849] Elapsed 1m 38s (remain 10m 0s) Loss: 0.0001(0.0011) Grad: 887.5995  LR: 0.000004  \n","Epoch: [5][500/2849] Elapsed 2m 2s (remain 9m 34s) Loss: 0.0034(0.0011) Grad: 21595.4688  LR: 0.000004  \n","Epoch: [5][600/2849] Elapsed 2m 26s (remain 9m 8s) Loss: 0.0009(0.0011) Grad: 8588.5771  LR: 0.000004  \n","Epoch: [5][700/2849] Elapsed 2m 50s (remain 8m 43s) Loss: 0.0001(0.0011) Grad: 480.7000  LR: 0.000003  \n","Epoch: [5][800/2849] Elapsed 3m 15s (remain 8m 19s) Loss: 0.0000(0.0010) Grad: 164.8376  LR: 0.000003  \n","Epoch: [5][900/2849] Elapsed 3m 39s (remain 7m 55s) Loss: 0.0000(0.0011) Grad: 216.1906  LR: 0.000003  \n","Epoch: [5][1000/2849] Elapsed 4m 3s (remain 7m 30s) Loss: 0.0000(0.0010) Grad: 22.0458  LR: 0.000003  \n","Epoch: [5][1100/2849] Elapsed 4m 28s (remain 7m 5s) Loss: 0.0020(0.0010) Grad: 5100.9678  LR: 0.000003  \n","Epoch: [5][1200/2849] Elapsed 4m 52s (remain 6m 41s) Loss: 0.0001(0.0010) Grad: 1938.9464  LR: 0.000003  \n","Epoch: [5][1300/2849] Elapsed 5m 16s (remain 6m 16s) Loss: 0.0059(0.0010) Grad: 25252.2598  LR: 0.000002  \n","Epoch: [5][1400/2849] Elapsed 5m 40s (remain 5m 52s) Loss: 0.0016(0.0010) Grad: 9800.8916  LR: 0.000002  \n","Epoch: [5][1500/2849] Elapsed 6m 5s (remain 5m 28s) Loss: 0.0259(0.0010) Grad: 83247.6797  LR: 0.000002  \n","Epoch: [5][1600/2849] Elapsed 6m 29s (remain 5m 3s) Loss: 0.0001(0.0011) Grad: 1192.1781  LR: 0.000002  \n","Epoch: [5][1700/2849] Elapsed 6m 53s (remain 4m 39s) Loss: 0.0001(0.0011) Grad: 1021.4694  LR: 0.000002  \n","Epoch: [5][1800/2849] Elapsed 7m 18s (remain 4m 14s) Loss: 0.0006(0.0011) Grad: 3764.0779  LR: 0.000002  \n","Epoch: [5][1900/2849] Elapsed 7m 42s (remain 3m 50s) Loss: 0.0027(0.0011) Grad: 51873.0820  LR: 0.000001  \n","Epoch: [5][2000/2849] Elapsed 8m 6s (remain 3m 26s) Loss: 0.0000(0.0011) Grad: 26.9794  LR: 0.000001  \n","Epoch: [5][2100/2849] Elapsed 8m 31s (remain 3m 1s) Loss: 0.0026(0.0010) Grad: 11468.3467  LR: 0.000001  \n","Epoch: [5][2200/2849] Elapsed 8m 55s (remain 2m 37s) Loss: 0.0024(0.0010) Grad: 6289.5391  LR: 0.000001  \n","Epoch: [5][2300/2849] Elapsed 9m 19s (remain 2m 13s) Loss: 0.0001(0.0010) Grad: 646.8334  LR: 0.000001  \n","Epoch: [5][2400/2849] Elapsed 9m 43s (remain 1m 48s) Loss: 0.0006(0.0010) Grad: 4313.0308  LR: 0.000001  \n","Epoch: [5][2500/2849] Elapsed 10m 7s (remain 1m 24s) Loss: 0.0001(0.0010) Grad: 400.1193  LR: 0.000001  \n","Epoch: [5][2600/2849] Elapsed 10m 32s (remain 1m 0s) Loss: 0.0001(0.0010) Grad: 750.5881  LR: 0.000000  \n","Epoch: [5][2700/2849] Elapsed 10m 56s (remain 0m 35s) Loss: 0.0004(0.0010) Grad: 3034.3208  LR: 0.000000  \n","Epoch: [5][2800/2849] Elapsed 11m 20s (remain 0m 11s) Loss: 0.0005(0.0010) Grad: 2352.1504  LR: 0.000000  \n","Epoch: [5][2848/2849] Elapsed 11m 32s (remain 0m 0s) Loss: 0.0001(0.0010) Grad: 528.1292  LR: 0.000000  \n","EVAL: [0/726] Elapsed 0m 0s (remain 4m 41s) Loss: 0.0001(0.0001) \n","EVAL: [100/726] Elapsed 0m 10s (remain 1m 4s) Loss: 0.0017(0.0037) \n","EVAL: [200/726] Elapsed 0m 20s (remain 0m 53s) Loss: 0.0000(0.0034) \n","EVAL: [300/726] Elapsed 0m 30s (remain 0m 42s) Loss: 0.0000(0.0035) \n","EVAL: [400/726] Elapsed 0m 40s (remain 0m 32s) Loss: 0.0002(0.0041) \n","EVAL: [500/726] Elapsed 0m 50s (remain 0m 22s) Loss: 0.0136(0.0041) \n","EVAL: [600/726] Elapsed 1m 0s (remain 0m 12s) Loss: 0.0147(0.0039) \n","EVAL: [700/726] Elapsed 1m 10s (remain 0m 2s) Loss: 0.0004(0.0036) \n","EVAL: [725/726] Elapsed 1m 12s (remain 0m 0s) Loss: 0.0000(0.0035) \n","Epoch 5 - avg_train_loss: 0.0010  avg_val_loss: 0.0035  time: 769s\n","Epoch 5 - Score: 0.8831\n","========== fold: 1 training ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n","Epoch: [1][0/2851] Elapsed 0m 0s (remain 26m 20s) Loss: 0.1554(0.1554) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/2851] Elapsed 0m 25s (remain 11m 23s) Loss: 0.0377(0.0924) Grad: 15116.9180  LR: 0.000001  \n","Epoch: [1][200/2851] Elapsed 0m 49s (remain 10m 55s) Loss: 0.0144(0.0543) Grad: 1724.7361  LR: 0.000003  \n","Epoch: [1][300/2851] Elapsed 1m 14s (remain 10m 29s) Loss: 0.0103(0.0401) Grad: 1544.4493  LR: 0.000004  \n","Epoch: [1][400/2851] Elapsed 1m 38s (remain 10m 3s) Loss: 0.0037(0.0318) Grad: 2647.0930  LR: 0.000006  \n","Epoch: [1][500/2851] Elapsed 2m 3s (remain 9m 38s) Loss: 0.0035(0.0267) Grad: 2356.5972  LR: 0.000007  \n","Epoch: [1][600/2851] Elapsed 2m 27s (remain 9m 13s) Loss: 0.0024(0.0231) Grad: 1995.4442  LR: 0.000008  \n","Epoch: [1][700/2851] Elapsed 2m 52s (remain 8m 47s) Loss: 0.0049(0.0204) Grad: 1450.8163  LR: 0.000010  \n","Epoch: [1][800/2851] Elapsed 3m 16s (remain 8m 22s) Loss: 0.0007(0.0183) Grad: 288.7253  LR: 0.000011  \n","Epoch: [1][900/2851] Elapsed 3m 40s (remain 7m 57s) Loss: 0.0013(0.0167) Grad: 393.1173  LR: 0.000013  \n","Epoch: [1][1000/2851] Elapsed 4m 4s (remain 7m 32s) Loss: 0.0007(0.0153) Grad: 310.6751  LR: 0.000014  \n","Epoch: [1][1100/2851] Elapsed 4m 29s (remain 7m 7s) Loss: 0.0001(0.0143) Grad: 32.5396  LR: 0.000015  \n","Epoch: [1][1200/2851] Elapsed 4m 53s (remain 6m 42s) Loss: 0.0006(0.0134) Grad: 718.4497  LR: 0.000017  \n","Epoch: [1][1300/2851] Elapsed 5m 17s (remain 6m 18s) Loss: 0.0015(0.0127) Grad: 636.5229  LR: 0.000018  \n","Epoch: [1][1400/2851] Elapsed 5m 41s (remain 5m 53s) Loss: 0.0003(0.0120) Grad: 105.5891  LR: 0.000020  \n","Epoch: [1][1500/2851] Elapsed 6m 5s (remain 5m 28s) Loss: 0.0045(0.0114) Grad: 1392.6453  LR: 0.000020  \n","Epoch: [1][1600/2851] Elapsed 6m 29s (remain 5m 4s) Loss: 0.0030(0.0109) Grad: 1343.5216  LR: 0.000020  \n","Epoch: [1][1700/2851] Elapsed 6m 54s (remain 4m 40s) Loss: 0.0016(0.0105) Grad: 789.6987  LR: 0.000020  \n","Epoch: [1][1800/2851] Elapsed 7m 18s (remain 4m 15s) Loss: 0.0023(0.0101) Grad: 408.8320  LR: 0.000019  \n","Epoch: [1][1900/2851] Elapsed 7m 42s (remain 3m 51s) Loss: 0.0002(0.0097) Grad: 91.6810  LR: 0.000019  \n","Epoch: [1][2000/2851] Elapsed 8m 6s (remain 3m 26s) Loss: 0.0027(0.0093) Grad: 1641.1342  LR: 0.000019  \n","Epoch: [1][2100/2851] Elapsed 8m 30s (remain 3m 2s) Loss: 0.0014(0.0090) Grad: 363.7196  LR: 0.000019  \n","Epoch: [1][2200/2851] Elapsed 8m 54s (remain 2m 37s) Loss: 0.0007(0.0087) Grad: 335.3809  LR: 0.000019  \n","Epoch: [1][2300/2851] Elapsed 9m 18s (remain 2m 13s) Loss: 0.0005(0.0085) Grad: 204.7604  LR: 0.000019  \n","Epoch: [1][2400/2851] Elapsed 9m 42s (remain 1m 49s) Loss: 0.0006(0.0082) Grad: 415.3192  LR: 0.000018  \n","Epoch: [1][2500/2851] Elapsed 10m 7s (remain 1m 24s) Loss: 0.0065(0.0080) Grad: 3435.6353  LR: 0.000018  \n","Epoch: [1][2600/2851] Elapsed 10m 31s (remain 1m 0s) Loss: 0.0066(0.0078) Grad: 1688.4899  LR: 0.000018  \n","Epoch: [1][2700/2851] Elapsed 10m 55s (remain 0m 36s) Loss: 0.0011(0.0076) Grad: 362.2856  LR: 0.000018  \n","Epoch: [1][2800/2851] Elapsed 11m 19s (remain 0m 12s) Loss: 0.0094(0.0074) Grad: 4712.9146  LR: 0.000018  \n","Epoch: [1][2850/2851] Elapsed 11m 31s (remain 0m 0s) Loss: 0.0036(0.0073) Grad: 864.1764  LR: 0.000018  \n","EVAL: [0/724] Elapsed 0m 0s (remain 4m 45s) Loss: 0.0006(0.0006) \n","EVAL: [100/724] Elapsed 0m 10s (remain 1m 3s) Loss: 0.0010(0.0022) \n","EVAL: [200/724] Elapsed 0m 20s (remain 0m 52s) Loss: 0.0001(0.0031) \n","EVAL: [300/724] Elapsed 0m 30s (remain 0m 42s) Loss: 0.0002(0.0030) \n","EVAL: [400/724] Elapsed 0m 40s (remain 0m 32s) Loss: 0.0000(0.0030) \n","EVAL: [500/724] Elapsed 0m 50s (remain 0m 22s) Loss: 0.0021(0.0032) \n","EVAL: [600/724] Elapsed 1m 0s (remain 0m 12s) Loss: 0.0002(0.0030) \n","EVAL: [700/724] Elapsed 1m 9s (remain 0m 2s) Loss: 0.0000(0.0027) \n","EVAL: [723/724] Elapsed 1m 11s (remain 0m 0s) Loss: 0.0001(0.0027) \n","Epoch 1 - avg_train_loss: 0.0073  avg_val_loss: 0.0027  time: 767s\n","Epoch 1 - Score: 0.8568\n","Epoch 1 - Save Best Score: 0.8568 Model\n","Epoch: [2][0/2851] Elapsed 0m 0s (remain 26m 24s) Loss: 0.0015(0.0015) Grad: 3675.2144  LR: 0.000018  \n","Epoch: [2][100/2851] Elapsed 0m 25s (remain 11m 46s) Loss: 0.0020(0.0017) Grad: 5794.0830  LR: 0.000018  \n","Epoch: [2][200/2851] Elapsed 0m 50s (remain 11m 4s) Loss: 0.0006(0.0018) Grad: 2720.7542  LR: 0.000017  \n","Epoch: [2][300/2851] Elapsed 1m 14s (remain 10m 34s) Loss: 0.0018(0.0019) Grad: 24433.2949  LR: 0.000017  \n","Epoch: [2][400/2851] Elapsed 1m 39s (remain 10m 7s) Loss: 0.0015(0.0019) Grad: 14624.6885  LR: 0.000017  \n","Epoch: [2][500/2851] Elapsed 2m 3s (remain 9m 40s) Loss: 0.0011(0.0020) Grad: 3137.4604  LR: 0.000017  \n","Epoch: [2][600/2851] Elapsed 2m 28s (remain 9m 14s) Loss: 0.0027(0.0020) Grad: 16742.6348  LR: 0.000017  \n","Epoch: [2][700/2851] Elapsed 2m 52s (remain 8m 48s) Loss: 0.0002(0.0020) Grad: 788.2679  LR: 0.000017  \n","Epoch: [2][800/2851] Elapsed 3m 16s (remain 8m 23s) Loss: 0.0019(0.0021) Grad: 13668.0449  LR: 0.000017  \n","Epoch: [2][900/2851] Elapsed 3m 41s (remain 7m 58s) Loss: 0.0005(0.0021) Grad: 2141.7683  LR: 0.000016  \n","Epoch: [2][1000/2851] Elapsed 4m 5s (remain 7m 33s) Loss: 0.0001(0.0021) Grad: 249.8412  LR: 0.000016  \n","Epoch: [2][1100/2851] Elapsed 4m 29s (remain 7m 8s) Loss: 0.0069(0.0021) Grad: 22279.4824  LR: 0.000016  \n","Epoch: [2][1200/2851] Elapsed 4m 54s (remain 6m 44s) Loss: 0.0008(0.0021) Grad: 2361.0232  LR: 0.000016  \n","Epoch: [2][1300/2851] Elapsed 5m 18s (remain 6m 19s) Loss: 0.0000(0.0021) Grad: 184.3909  LR: 0.000016  \n","Epoch: [2][1400/2851] Elapsed 5m 42s (remain 5m 54s) Loss: 0.0008(0.0021) Grad: 2654.6140  LR: 0.000016  \n","Epoch: [2][1500/2851] Elapsed 6m 7s (remain 5m 30s) Loss: 0.0003(0.0021) Grad: 895.4015  LR: 0.000015  \n","Epoch: [2][1600/2851] Elapsed 6m 31s (remain 5m 5s) Loss: 0.0009(0.0020) Grad: 2937.8955  LR: 0.000015  \n","Epoch: [2][1700/2851] Elapsed 6m 56s (remain 4m 41s) Loss: 0.0013(0.0020) Grad: 8152.1157  LR: 0.000015  \n","Epoch: [2][1800/2851] Elapsed 7m 20s (remain 4m 17s) Loss: 0.0001(0.0020) Grad: 797.9363  LR: 0.000015  \n","Epoch: [2][1900/2851] Elapsed 7m 45s (remain 3m 52s) Loss: 0.0001(0.0020) Grad: 748.1267  LR: 0.000015  \n","Epoch: [2][2000/2851] Elapsed 8m 9s (remain 3m 28s) Loss: 0.0001(0.0020) Grad: 300.4834  LR: 0.000015  \n","Epoch: [2][2100/2851] Elapsed 8m 34s (remain 3m 3s) Loss: 0.0004(0.0020) Grad: 3341.5598  LR: 0.000015  \n","Epoch: [2][2200/2851] Elapsed 8m 58s (remain 2m 39s) Loss: 0.0081(0.0020) Grad: 26825.1270  LR: 0.000014  \n","Epoch: [2][2300/2851] Elapsed 9m 23s (remain 2m 14s) Loss: 0.0004(0.0020) Grad: 2186.7976  LR: 0.000014  \n","Epoch: [2][2400/2851] Elapsed 9m 47s (remain 1m 50s) Loss: 0.0000(0.0020) Grad: 191.3048  LR: 0.000014  \n","Epoch: [2][2500/2851] Elapsed 10m 11s (remain 1m 25s) Loss: 0.0000(0.0020) Grad: 37.2026  LR: 0.000014  \n","Epoch: [2][2600/2851] Elapsed 10m 36s (remain 1m 1s) Loss: 0.0114(0.0020) Grad: 9353.1338  LR: 0.000014  \n","Epoch: [2][2700/2851] Elapsed 11m 0s (remain 0m 36s) Loss: 0.0023(0.0020) Grad: 4729.3706  LR: 0.000014  \n","Epoch: [2][2800/2851] Elapsed 11m 25s (remain 0m 12s) Loss: 0.0006(0.0020) Grad: 2228.9934  LR: 0.000013  \n","Epoch: [2][2850/2851] Elapsed 11m 37s (remain 0m 0s) Loss: 0.0002(0.0020) Grad: 857.5073  LR: 0.000013  \n","EVAL: [0/724] Elapsed 0m 0s (remain 4m 46s) Loss: 0.0003(0.0003) \n","EVAL: [100/724] Elapsed 0m 10s (remain 1m 4s) Loss: 0.0016(0.0021) \n","EVAL: [200/724] Elapsed 0m 20s (remain 0m 53s) Loss: 0.0000(0.0028) \n","EVAL: [300/724] Elapsed 0m 30s (remain 0m 42s) Loss: 0.0027(0.0029) \n","EVAL: [400/724] Elapsed 0m 40s (remain 0m 32s) Loss: 0.0000(0.0029) \n","EVAL: [500/724] Elapsed 0m 50s (remain 0m 22s) Loss: 0.0032(0.0031) \n","EVAL: [600/724] Elapsed 1m 0s (remain 0m 12s) Loss: 0.0002(0.0029) \n","EVAL: [700/724] Elapsed 1m 10s (remain 0m 2s) Loss: 0.0000(0.0027) \n","EVAL: [723/724] Elapsed 1m 12s (remain 0m 0s) Loss: 0.0000(0.0026) \n","Epoch 2 - avg_train_loss: 0.0020  avg_val_loss: 0.0026  time: 773s\n","Epoch 2 - Score: 0.8721\n","Epoch 2 - Save Best Score: 0.8721 Model\n","Epoch: [3][0/2851] Elapsed 0m 0s (remain 25m 37s) Loss: 0.0005(0.0005) Grad: 1533.5402  LR: 0.000013  \n","Epoch: [3][100/2851] Elapsed 0m 25s (remain 11m 27s) Loss: 0.0004(0.0017) Grad: 2803.2249  LR: 0.000013  \n","Epoch: [3][200/2851] Elapsed 0m 49s (remain 10m 56s) Loss: 0.0003(0.0019) Grad: 1240.4160  LR: 0.000013  \n","Epoch: [3][300/2851] Elapsed 1m 14s (remain 10m 28s) Loss: 0.0006(0.0017) Grad: 1446.5706  LR: 0.000013  \n","Epoch: [3][400/2851] Elapsed 1m 38s (remain 10m 1s) Loss: 0.0000(0.0015) Grad: 39.3820  LR: 0.000013  \n","Epoch: [3][500/2851] Elapsed 2m 2s (remain 9m 35s) Loss: 0.0014(0.0017) Grad: 2012.8793  LR: 0.000013  \n","Epoch: [3][600/2851] Elapsed 2m 26s (remain 9m 9s) Loss: 0.0135(0.0017) Grad: 12923.4512  LR: 0.000012  \n","Epoch: [3][700/2851] Elapsed 2m 51s (remain 8m 44s) Loss: 0.0002(0.0017) Grad: 2731.6616  LR: 0.000012  \n","Epoch: [3][800/2851] Elapsed 3m 15s (remain 8m 19s) Loss: 0.0092(0.0017) Grad: 33967.4102  LR: 0.000012  \n","Epoch: [3][900/2851] Elapsed 3m 39s (remain 7m 55s) Loss: 0.0005(0.0017) Grad: 1475.1279  LR: 0.000012  \n","Epoch: [3][1000/2851] Elapsed 4m 3s (remain 7m 30s) Loss: 0.0001(0.0017) Grad: 1008.6626  LR: 0.000012  \n","Epoch: [3][1100/2851] Elapsed 4m 28s (remain 7m 6s) Loss: 0.0000(0.0016) Grad: 255.8513  LR: 0.000012  \n","Epoch: [3][1200/2851] Elapsed 4m 52s (remain 6m 41s) Loss: 0.0003(0.0017) Grad: 1475.2648  LR: 0.000011  \n","Epoch: [3][1300/2851] Elapsed 5m 16s (remain 6m 17s) Loss: 0.0000(0.0017) Grad: 8.9787  LR: 0.000011  \n","Epoch: [3][1400/2851] Elapsed 5m 40s (remain 5m 52s) Loss: 0.0006(0.0016) Grad: 1687.0917  LR: 0.000011  \n","Epoch: [3][1500/2851] Elapsed 6m 5s (remain 5m 28s) Loss: 0.0019(0.0016) Grad: 17609.0684  LR: 0.000011  \n","Epoch: [3][1600/2851] Elapsed 6m 29s (remain 5m 3s) Loss: 0.0030(0.0016) Grad: 8369.3594  LR: 0.000011  \n","Epoch: [3][1700/2851] Elapsed 6m 53s (remain 4m 39s) Loss: 0.0001(0.0016) Grad: 629.4385  LR: 0.000011  \n","Epoch: [3][1800/2851] Elapsed 7m 17s (remain 4m 15s) Loss: 0.0005(0.0016) Grad: 4168.0210  LR: 0.000011  \n","Epoch: [3][1900/2851] Elapsed 7m 42s (remain 3m 50s) Loss: 0.0001(0.0016) Grad: 488.0820  LR: 0.000010  \n","Epoch: [3][2000/2851] Elapsed 8m 6s (remain 3m 26s) Loss: 0.0133(0.0016) Grad: 17878.9902  LR: 0.000010  \n","Epoch: [3][2100/2851] Elapsed 8m 30s (remain 3m 2s) Loss: 0.0090(0.0016) Grad: 13210.1807  LR: 0.000010  \n","Epoch: [3][2200/2851] Elapsed 8m 54s (remain 2m 37s) Loss: 0.0007(0.0016) Grad: 3041.2522  LR: 0.000010  \n","Epoch: [3][2300/2851] Elapsed 9m 19s (remain 2m 13s) Loss: 0.0066(0.0016) Grad: 25969.0430  LR: 0.000010  \n","Epoch: [3][2400/2851] Elapsed 9m 43s (remain 1m 49s) Loss: 0.0002(0.0016) Grad: 772.9730  LR: 0.000010  \n","Epoch: [3][2500/2851] Elapsed 10m 7s (remain 1m 25s) Loss: 0.0000(0.0016) Grad: 67.0077  LR: 0.000009  \n","Epoch: [3][2600/2851] Elapsed 10m 32s (remain 1m 0s) Loss: 0.0000(0.0016) Grad: 221.6997  LR: 0.000009  \n","Epoch: [3][2700/2851] Elapsed 10m 56s (remain 0m 36s) Loss: 0.0015(0.0016) Grad: 14645.4941  LR: 0.000009  \n","Epoch: [3][2800/2851] Elapsed 11m 20s (remain 0m 12s) Loss: 0.0000(0.0016) Grad: 59.4482  LR: 0.000009  \n","Epoch: [3][2850/2851] Elapsed 11m 32s (remain 0m 0s) Loss: 0.0035(0.0016) Grad: 5571.1660  LR: 0.000009  \n","EVAL: [0/724] Elapsed 0m 0s (remain 4m 52s) Loss: 0.0003(0.0003) \n","EVAL: [100/724] Elapsed 0m 10s (remain 1m 3s) Loss: 0.0012(0.0028) \n","EVAL: [200/724] Elapsed 0m 20s (remain 0m 53s) Loss: 0.0000(0.0035) \n","EVAL: [300/724] Elapsed 0m 30s (remain 0m 43s) Loss: 0.0038(0.0035) \n","EVAL: [400/724] Elapsed 0m 40s (remain 0m 32s) Loss: 0.0000(0.0035) \n","EVAL: [500/724] Elapsed 0m 50s (remain 0m 22s) Loss: 0.0062(0.0041) \n","EVAL: [600/724] Elapsed 1m 0s (remain 0m 12s) Loss: 0.0002(0.0038) \n","EVAL: [700/724] Elapsed 1m 10s (remain 0m 2s) Loss: 0.0000(0.0034) \n","EVAL: [723/724] Elapsed 1m 12s (remain 0m 0s) Loss: 0.0000(0.0034) \n","Epoch 3 - avg_train_loss: 0.0016  avg_val_loss: 0.0034  time: 769s\n","Epoch 3 - Score: 0.8746\n","Epoch 3 - Save Best Score: 0.8746 Model\n","Epoch: [4][0/2851] Elapsed 0m 0s (remain 26m 2s) Loss: 0.0000(0.0000) Grad: 984.0914  LR: 0.000009  \n","Epoch: [4][100/2851] Elapsed 0m 25s (remain 11m 27s) Loss: 0.0000(0.0013) Grad: 177.5638  LR: 0.000009  \n","Epoch: [4][200/2851] Elapsed 0m 49s (remain 10m 54s) Loss: 0.0019(0.0012) Grad: 4840.4912  LR: 0.000009  \n","Epoch: [4][300/2851] Elapsed 1m 13s (remain 10m 26s) Loss: 0.0000(0.0012) Grad: 14.5852  LR: 0.000008  \n","Epoch: [4][400/2851] Elapsed 1m 38s (remain 10m 2s) Loss: 0.0001(0.0013) Grad: 775.8649  LR: 0.000008  \n","Epoch: [4][500/2851] Elapsed 2m 2s (remain 9m 35s) Loss: 0.0005(0.0012) Grad: 3376.3596  LR: 0.000008  \n","Epoch: [4][600/2851] Elapsed 2m 26s (remain 9m 10s) Loss: 0.0014(0.0012) Grad: 8710.9131  LR: 0.000008  \n","Epoch: [4][700/2851] Elapsed 2m 51s (remain 8m 45s) Loss: 0.0001(0.0012) Grad: 5371.4849  LR: 0.000008  \n","Epoch: [4][800/2851] Elapsed 3m 15s (remain 8m 20s) Loss: 0.0032(0.0011) Grad: 14029.7578  LR: 0.000008  \n","Epoch: [4][900/2851] Elapsed 3m 39s (remain 7m 55s) Loss: 0.0001(0.0011) Grad: 812.6856  LR: 0.000007  \n","Epoch: [4][1000/2851] Elapsed 4m 4s (remain 7m 31s) Loss: 0.0005(0.0011) Grad: 1815.0845  LR: 0.000007  \n","Epoch: [4][1100/2851] Elapsed 4m 28s (remain 7m 6s) Loss: 0.0000(0.0011) Grad: 189.5546  LR: 0.000007  \n","Epoch: [4][1200/2851] Elapsed 4m 52s (remain 6m 42s) Loss: 0.0023(0.0011) Grad: 4350.1460  LR: 0.000007  \n","Epoch: [4][1300/2851] Elapsed 5m 16s (remain 6m 17s) Loss: 0.0000(0.0011) Grad: 123.2369  LR: 0.000007  \n","Epoch: [4][1400/2851] Elapsed 5m 41s (remain 5m 53s) Loss: 0.0034(0.0011) Grad: 11804.4375  LR: 0.000007  \n","Epoch: [4][1500/2851] Elapsed 6m 5s (remain 5m 28s) Loss: 0.0215(0.0012) Grad: 48887.4922  LR: 0.000007  \n","Epoch: [4][1600/2851] Elapsed 6m 29s (remain 5m 4s) Loss: 0.0000(0.0012) Grad: 233.6107  LR: 0.000006  \n","Epoch: [4][1700/2851] Elapsed 6m 53s (remain 4m 39s) Loss: 0.0009(0.0012) Grad: 5148.8921  LR: 0.000006  \n","Epoch: [4][1800/2851] Elapsed 7m 18s (remain 4m 15s) Loss: 0.0008(0.0012) Grad: 5070.3306  LR: 0.000006  \n","Epoch: [4][1900/2851] Elapsed 7m 42s (remain 3m 51s) Loss: 0.0000(0.0012) Grad: 53.2920  LR: 0.000006  \n","Epoch: [4][2000/2851] Elapsed 8m 6s (remain 3m 26s) Loss: 0.0000(0.0012) Grad: 245.7946  LR: 0.000006  \n","Epoch: [4][2100/2851] Elapsed 8m 30s (remain 3m 2s) Loss: 0.0006(0.0012) Grad: 2252.3730  LR: 0.000006  \n","Epoch: [4][2200/2851] Elapsed 8m 55s (remain 2m 38s) Loss: 0.0002(0.0012) Grad: 1219.7281  LR: 0.000005  \n","Epoch: [4][2300/2851] Elapsed 9m 19s (remain 2m 13s) Loss: 0.0002(0.0012) Grad: 1303.8369  LR: 0.000005  \n","Epoch: [4][2400/2851] Elapsed 9m 43s (remain 1m 49s) Loss: 0.0005(0.0012) Grad: 2815.6926  LR: 0.000005  \n","Epoch: [4][2500/2851] Elapsed 10m 8s (remain 1m 25s) Loss: 0.0001(0.0012) Grad: 296.5448  LR: 0.000005  \n","Epoch: [4][2600/2851] Elapsed 10m 32s (remain 1m 0s) Loss: 0.0005(0.0012) Grad: 11331.7061  LR: 0.000005  \n","Epoch: [4][2700/2851] Elapsed 10m 56s (remain 0m 36s) Loss: 0.0017(0.0012) Grad: 4086.2432  LR: 0.000005  \n","Epoch: [4][2800/2851] Elapsed 11m 20s (remain 0m 12s) Loss: 0.0004(0.0012) Grad: 2582.0725  LR: 0.000005  \n","Epoch: [4][2850/2851] Elapsed 11m 33s (remain 0m 0s) Loss: 0.0001(0.0012) Grad: 1368.2961  LR: 0.000004  \n","EVAL: [0/724] Elapsed 0m 0s (remain 4m 44s) Loss: 0.0004(0.0004) \n","EVAL: [100/724] Elapsed 0m 10s (remain 1m 4s) Loss: 0.0017(0.0027) \n","EVAL: [200/724] Elapsed 0m 20s (remain 0m 53s) Loss: 0.0000(0.0032) \n","EVAL: [300/724] Elapsed 0m 30s (remain 0m 42s) Loss: 0.0037(0.0032) \n","EVAL: [400/724] Elapsed 0m 40s (remain 0m 32s) Loss: 0.0000(0.0033) \n","EVAL: [500/724] Elapsed 0m 50s (remain 0m 22s) Loss: 0.0059(0.0039) \n","EVAL: [600/724] Elapsed 1m 0s (remain 0m 12s) Loss: 0.0008(0.0036) \n","EVAL: [700/724] Elapsed 1m 10s (remain 0m 2s) Loss: 0.0000(0.0033) \n","EVAL: [723/724] Elapsed 1m 12s (remain 0m 0s) Loss: 0.0000(0.0033) \n","Epoch 4 - avg_train_loss: 0.0012  avg_val_loss: 0.0033  time: 769s\n","Epoch 4 - Score: 0.8823\n","Epoch 4 - Save Best Score: 0.8823 Model\n","Epoch: [5][0/2851] Elapsed 0m 0s (remain 25m 57s) Loss: 0.0001(0.0001) Grad: 1584.2070  LR: 0.000004  \n","Epoch: [5][100/2851] Elapsed 0m 26s (remain 11m 48s) Loss: 0.0000(0.0007) Grad: 59.8385  LR: 0.000004  \n","Epoch: [5][200/2851] Elapsed 0m 50s (remain 11m 5s) Loss: 0.0009(0.0006) Grad: 7064.2729  LR: 0.000004  \n","Epoch: [5][300/2851] Elapsed 1m 14s (remain 10m 32s) Loss: 0.0090(0.0006) Grad: 11551.2754  LR: 0.000004  \n","Epoch: [5][400/2851] Elapsed 1m 38s (remain 10m 4s) Loss: 0.0002(0.0008) Grad: 2862.6709  LR: 0.000004  \n","Epoch: [5][500/2851] Elapsed 2m 3s (remain 9m 37s) Loss: 0.0019(0.0008) Grad: 14591.4199  LR: 0.000004  \n","Epoch: [5][600/2851] Elapsed 2m 27s (remain 9m 11s) Loss: 0.0006(0.0008) Grad: 3203.0916  LR: 0.000004  \n","Epoch: [5][700/2851] Elapsed 2m 51s (remain 8m 46s) Loss: 0.0000(0.0009) Grad: 684.9135  LR: 0.000003  \n","Epoch: [5][800/2851] Elapsed 3m 16s (remain 8m 21s) Loss: 0.0002(0.0009) Grad: 917.2115  LR: 0.000003  \n","Epoch: [5][900/2851] Elapsed 3m 40s (remain 7m 56s) Loss: 0.0000(0.0009) Grad: 92.3801  LR: 0.000003  \n","Epoch: [5][1000/2851] Elapsed 4m 4s (remain 7m 31s) Loss: 0.0000(0.0009) Grad: 608.6570  LR: 0.000003  \n","Epoch: [5][1100/2851] Elapsed 4m 28s (remain 7m 7s) Loss: 0.0003(0.0009) Grad: 2217.0361  LR: 0.000003  \n","Epoch: [5][1200/2851] Elapsed 4m 52s (remain 6m 42s) Loss: 0.0011(0.0010) Grad: 5484.8730  LR: 0.000003  \n","Epoch: [5][1300/2851] Elapsed 5m 17s (remain 6m 17s) Loss: 0.0000(0.0010) Grad: 83.4632  LR: 0.000002  \n","Epoch: [5][1400/2851] Elapsed 5m 41s (remain 5m 53s) Loss: 0.0000(0.0010) Grad: 44.4508  LR: 0.000002  \n","Epoch: [5][1500/2851] Elapsed 6m 5s (remain 5m 29s) Loss: 0.0065(0.0010) Grad: 5989.7329  LR: 0.000002  \n","Epoch: [5][1600/2851] Elapsed 6m 30s (remain 5m 4s) Loss: 0.0002(0.0010) Grad: 1238.6486  LR: 0.000002  \n","Epoch: [5][1700/2851] Elapsed 6m 54s (remain 4m 40s) Loss: 0.0000(0.0010) Grad: 33.7233  LR: 0.000002  \n","Epoch: [5][1800/2851] Elapsed 7m 18s (remain 4m 15s) Loss: 0.0019(0.0010) Grad: 25117.6562  LR: 0.000002  \n","Epoch: [5][1900/2851] Elapsed 7m 42s (remain 3m 51s) Loss: 0.0000(0.0010) Grad: 7.7598  LR: 0.000001  \n","Epoch: [5][2000/2851] Elapsed 8m 7s (remain 3m 26s) Loss: 0.0000(0.0010) Grad: 1262.9867  LR: 0.000001  \n","Epoch: [5][2100/2851] Elapsed 8m 31s (remain 3m 2s) Loss: 0.0005(0.0010) Grad: 3578.2844  LR: 0.000001  \n","Epoch: [5][2200/2851] Elapsed 8m 55s (remain 2m 38s) Loss: 0.0000(0.0010) Grad: 406.1403  LR: 0.000001  \n","Epoch: [5][2300/2851] Elapsed 9m 19s (remain 2m 13s) Loss: 0.0000(0.0010) Grad: 10.8866  LR: 0.000001  \n","Epoch: [5][2400/2851] Elapsed 9m 44s (remain 1m 49s) Loss: 0.0011(0.0010) Grad: 2883.5251  LR: 0.000001  \n","Epoch: [5][2500/2851] Elapsed 10m 8s (remain 1m 25s) Loss: 0.0000(0.0010) Grad: 17.4660  LR: 0.000001  \n","Epoch: [5][2600/2851] Elapsed 10m 32s (remain 1m 0s) Loss: 0.0003(0.0010) Grad: 1329.5540  LR: 0.000000  \n","Epoch: [5][2700/2851] Elapsed 10m 56s (remain 0m 36s) Loss: 0.0007(0.0010) Grad: 7393.2515  LR: 0.000000  \n","Epoch: [5][2800/2851] Elapsed 11m 21s (remain 0m 12s) Loss: 0.0000(0.0010) Grad: 5.8517  LR: 0.000000  \n","Epoch: [5][2850/2851] Elapsed 11m 33s (remain 0m 0s) Loss: 0.0061(0.0010) Grad: 46586.3516  LR: 0.000000  \n","EVAL: [0/724] Elapsed 0m 0s (remain 4m 47s) Loss: 0.0004(0.0004) \n","EVAL: [100/724] Elapsed 0m 10s (remain 1m 3s) Loss: 0.0011(0.0030) \n","EVAL: [200/724] Elapsed 0m 20s (remain 0m 53s) Loss: 0.0000(0.0036) \n","EVAL: [300/724] Elapsed 0m 30s (remain 0m 42s) Loss: 0.0057(0.0035) \n","EVAL: [400/724] Elapsed 0m 40s (remain 0m 32s) Loss: 0.0000(0.0037) \n","EVAL: [500/724] Elapsed 0m 50s (remain 0m 22s) Loss: 0.0074(0.0043) \n","EVAL: [600/724] Elapsed 1m 0s (remain 0m 12s) Loss: 0.0005(0.0041) \n","EVAL: [700/724] Elapsed 1m 10s (remain 0m 2s) Loss: 0.0000(0.0037) \n","EVAL: [723/724] Elapsed 1m 12s (remain 0m 0s) Loss: 0.0000(0.0037) \n","Epoch 5 - avg_train_loss: 0.0010  avg_val_loss: 0.0037  time: 769s\n","Epoch 5 - Score: 0.8793\n","========== fold: 2 training ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n","Epoch: [1][0/2871] Elapsed 0m 0s (remain 26m 2s) Loss: 0.0739(0.0739) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/2871] Elapsed 0m 24s (remain 11m 19s) Loss: 0.0262(0.0770) Grad: 4952.2852  LR: 0.000001  \n","Epoch: [1][200/2871] Elapsed 0m 49s (remain 10m 51s) Loss: 0.0144(0.0467) Grad: 1148.5426  LR: 0.000003  \n","Epoch: [1][300/2871] Elapsed 1m 13s (remain 10m 26s) Loss: 0.0066(0.0350) Grad: 2932.6274  LR: 0.000004  \n","Epoch: [1][400/2871] Elapsed 1m 37s (remain 10m 1s) Loss: 0.0053(0.0280) Grad: 2093.7815  LR: 0.000006  \n","Epoch: [1][500/2871] Elapsed 2m 1s (remain 9m 36s) Loss: 0.0015(0.0234) Grad: 1045.5540  LR: 0.000007  \n","Epoch: [1][600/2871] Elapsed 2m 26s (remain 9m 13s) Loss: 0.0009(0.0203) Grad: 453.5620  LR: 0.000008  \n","Epoch: [1][700/2871] Elapsed 2m 50s (remain 8m 48s) Loss: 0.0017(0.0181) Grad: 443.6257  LR: 0.000010  \n","Epoch: [1][800/2871] Elapsed 3m 15s (remain 8m 24s) Loss: 0.0052(0.0163) Grad: 3564.8088  LR: 0.000011  \n","Epoch: [1][900/2871] Elapsed 3m 39s (remain 7m 59s) Loss: 0.0035(0.0149) Grad: 1204.6033  LR: 0.000013  \n","Epoch: [1][1000/2871] Elapsed 4m 3s (remain 7m 35s) Loss: 0.0010(0.0138) Grad: 261.5430  LR: 0.000014  \n","Epoch: [1][1100/2871] Elapsed 4m 27s (remain 7m 10s) Loss: 0.0008(0.0129) Grad: 399.3849  LR: 0.000015  \n","Epoch: [1][1200/2871] Elapsed 4m 52s (remain 6m 46s) Loss: 0.0028(0.0120) Grad: 1220.1344  LR: 0.000017  \n","Epoch: [1][1300/2871] Elapsed 5m 16s (remain 6m 22s) Loss: 0.0009(0.0113) Grad: 370.3891  LR: 0.000018  \n","Epoch: [1][1400/2871] Elapsed 5m 40s (remain 5m 57s) Loss: 0.0028(0.0108) Grad: 1647.6488  LR: 0.000020  \n","Epoch: [1][1500/2871] Elapsed 6m 5s (remain 5m 33s) Loss: 0.0043(0.0103) Grad: 1568.3751  LR: 0.000020  \n","Epoch: [1][1600/2871] Elapsed 6m 29s (remain 5m 8s) Loss: 0.0044(0.0099) Grad: 1065.0620  LR: 0.000020  \n","Epoch: [1][1700/2871] Elapsed 6m 53s (remain 4m 44s) Loss: 0.0006(0.0095) Grad: 838.6427  LR: 0.000020  \n","Epoch: [1][1800/2871] Elapsed 7m 18s (remain 4m 20s) Loss: 0.0081(0.0092) Grad: 6250.9761  LR: 0.000019  \n","Epoch: [1][1900/2871] Elapsed 7m 42s (remain 3m 55s) Loss: 0.0011(0.0088) Grad: 389.7084  LR: 0.000019  \n","Epoch: [1][2000/2871] Elapsed 8m 6s (remain 3m 31s) Loss: 0.0005(0.0085) Grad: 188.3942  LR: 0.000019  \n","Epoch: [1][2100/2871] Elapsed 8m 31s (remain 3m 7s) Loss: 0.0001(0.0083) Grad: 61.2742  LR: 0.000019  \n","Epoch: [1][2200/2871] Elapsed 8m 55s (remain 2m 42s) Loss: 0.0025(0.0081) Grad: 905.3271  LR: 0.000019  \n","Epoch: [1][2300/2871] Elapsed 9m 19s (remain 2m 18s) Loss: 0.0040(0.0079) Grad: 985.5296  LR: 0.000019  \n","Epoch: [1][2400/2871] Elapsed 9m 44s (remain 1m 54s) Loss: 0.0029(0.0076) Grad: 625.0800  LR: 0.000019  \n","Epoch: [1][2500/2871] Elapsed 10m 8s (remain 1m 30s) Loss: 0.0004(0.0075) Grad: 154.2308  LR: 0.000018  \n","Epoch: [1][2600/2871] Elapsed 10m 32s (remain 1m 5s) Loss: 0.0002(0.0072) Grad: 673.7864  LR: 0.000018  \n","Epoch: [1][2700/2871] Elapsed 10m 57s (remain 0m 41s) Loss: 0.0012(0.0071) Grad: 419.2995  LR: 0.000018  \n","Epoch: [1][2800/2871] Elapsed 11m 21s (remain 0m 17s) Loss: 0.0060(0.0069) Grad: 1263.8900  LR: 0.000018  \n","Epoch: [1][2870/2871] Elapsed 11m 38s (remain 0m 0s) Loss: 0.0004(0.0068) Grad: 104.7432  LR: 0.000018  \n","EVAL: [0/704] Elapsed 0m 0s (remain 4m 37s) Loss: 0.0019(0.0019) \n","EVAL: [100/704] Elapsed 0m 10s (remain 1m 2s) Loss: 0.0012(0.0027) \n","EVAL: [200/704] Elapsed 0m 20s (remain 0m 51s) Loss: 0.0000(0.0026) \n","EVAL: [300/704] Elapsed 0m 30s (remain 0m 40s) Loss: 0.0004(0.0024) \n","EVAL: [400/704] Elapsed 0m 40s (remain 0m 30s) Loss: 0.0019(0.0027) \n","EVAL: [500/704] Elapsed 0m 50s (remain 0m 20s) Loss: 0.0031(0.0028) \n","EVAL: [600/704] Elapsed 1m 0s (remain 0m 10s) Loss: 0.0001(0.0028) \n","EVAL: [700/704] Elapsed 1m 10s (remain 0m 0s) Loss: 0.0000(0.0026) \n","EVAL: [703/704] Elapsed 1m 10s (remain 0m 0s) Loss: 0.0000(0.0026) \n","Epoch 1 - avg_train_loss: 0.0068  avg_val_loss: 0.0026  time: 773s\n","Epoch 1 - Score: 0.8310\n","Epoch 1 - Save Best Score: 0.8310 Model\n","Epoch: [2][0/2871] Elapsed 0m 0s (remain 25m 58s) Loss: 0.0012(0.0012) Grad: 3376.9473  LR: 0.000018  \n","Epoch: [2][100/2871] Elapsed 0m 25s (remain 11m 46s) Loss: 0.0011(0.0018) Grad: 3443.4871  LR: 0.000018  \n","Epoch: [2][200/2871] Elapsed 0m 50s (remain 11m 10s) Loss: 0.0001(0.0017) Grad: 599.6555  LR: 0.000017  \n","Epoch: [2][300/2871] Elapsed 1m 14s (remain 10m 39s) Loss: 0.0006(0.0020) Grad: 1963.1967  LR: 0.000017  \n","Epoch: [2][400/2871] Elapsed 1m 39s (remain 10m 13s) Loss: 0.0005(0.0020) Grad: 1369.4242  LR: 0.000017  \n","Epoch: [2][500/2871] Elapsed 2m 4s (remain 9m 46s) Loss: 0.0006(0.0020) Grad: 2131.9751  LR: 0.000017  \n","Epoch: [2][600/2871] Elapsed 2m 28s (remain 9m 20s) Loss: 0.0101(0.0020) Grad: 26931.1172  LR: 0.000017  \n","Epoch: [2][700/2871] Elapsed 2m 52s (remain 8m 54s) Loss: 0.0022(0.0020) Grad: 5519.8037  LR: 0.000017  \n","Epoch: [2][800/2871] Elapsed 3m 17s (remain 8m 29s) Loss: 0.0015(0.0021) Grad: 3983.7817  LR: 0.000017  \n","Epoch: [2][900/2871] Elapsed 3m 41s (remain 8m 4s) Loss: 0.0050(0.0021) Grad: 34555.1016  LR: 0.000016  \n","Epoch: [2][1000/2871] Elapsed 4m 6s (remain 7m 39s) Loss: 0.0007(0.0020) Grad: 2832.8506  LR: 0.000016  \n","Epoch: [2][1100/2871] Elapsed 4m 30s (remain 7m 15s) Loss: 0.0002(0.0020) Grad: 707.6669  LR: 0.000016  \n","Epoch: [2][1200/2871] Elapsed 4m 55s (remain 6m 50s) Loss: 0.0000(0.0020) Grad: 146.3446  LR: 0.000016  \n","Epoch: [2][1300/2871] Elapsed 5m 19s (remain 6m 25s) Loss: 0.0000(0.0020) Grad: 38.3141  LR: 0.000016  \n","Epoch: [2][1400/2871] Elapsed 5m 44s (remain 6m 1s) Loss: 0.0006(0.0020) Grad: 2139.8943  LR: 0.000016  \n","Epoch: [2][1500/2871] Elapsed 6m 8s (remain 5m 36s) Loss: 0.0020(0.0020) Grad: 4772.1270  LR: 0.000015  \n","Epoch: [2][1600/2871] Elapsed 6m 33s (remain 5m 11s) Loss: 0.0037(0.0020) Grad: 6782.8281  LR: 0.000015  \n","Epoch: [2][1700/2871] Elapsed 6m 57s (remain 4m 47s) Loss: 0.0009(0.0020) Grad: 9195.1621  LR: 0.000015  \n","Epoch: [2][1800/2871] Elapsed 7m 22s (remain 4m 22s) Loss: 0.0125(0.0020) Grad: 24996.1758  LR: 0.000015  \n","Epoch: [2][1900/2871] Elapsed 7m 46s (remain 3m 58s) Loss: 0.0017(0.0020) Grad: 3338.9968  LR: 0.000015  \n","Epoch: [2][2000/2871] Elapsed 8m 11s (remain 3m 33s) Loss: 0.0001(0.0021) Grad: 520.9916  LR: 0.000015  \n","Epoch: [2][2100/2871] Elapsed 8m 35s (remain 3m 8s) Loss: 0.0026(0.0020) Grad: 7890.2617  LR: 0.000015  \n","Epoch: [2][2200/2871] Elapsed 9m 0s (remain 2m 44s) Loss: 0.0009(0.0020) Grad: 3094.3716  LR: 0.000014  \n","Epoch: [2][2300/2871] Elapsed 9m 24s (remain 2m 19s) Loss: 0.0013(0.0020) Grad: 2616.1267  LR: 0.000014  \n","Epoch: [2][2400/2871] Elapsed 9m 49s (remain 1m 55s) Loss: 0.0020(0.0020) Grad: 4521.8896  LR: 0.000014  \n","Epoch: [2][2500/2871] Elapsed 10m 13s (remain 1m 30s) Loss: 0.0007(0.0020) Grad: 2252.5605  LR: 0.000014  \n","Epoch: [2][2600/2871] Elapsed 10m 38s (remain 1m 6s) Loss: 0.0006(0.0020) Grad: 2480.0481  LR: 0.000014  \n","Epoch: [2][2700/2871] Elapsed 11m 2s (remain 0m 41s) Loss: 0.0001(0.0020) Grad: 717.1251  LR: 0.000014  \n","Epoch: [2][2800/2871] Elapsed 11m 27s (remain 0m 17s) Loss: 0.0006(0.0020) Grad: 1988.9084  LR: 0.000013  \n","Epoch: [2][2870/2871] Elapsed 11m 44s (remain 0m 0s) Loss: 0.0049(0.0020) Grad: 9060.4980  LR: 0.000013  \n","EVAL: [0/704] Elapsed 0m 0s (remain 4m 39s) Loss: 0.0002(0.0002) \n","EVAL: [100/704] Elapsed 0m 10s (remain 1m 2s) Loss: 0.0002(0.0027) \n","EVAL: [200/704] Elapsed 0m 20s (remain 0m 51s) Loss: 0.0000(0.0026) \n","EVAL: [300/704] Elapsed 0m 30s (remain 0m 40s) Loss: 0.0001(0.0024) \n","EVAL: [400/704] Elapsed 0m 40s (remain 0m 30s) Loss: 0.0022(0.0027) \n","EVAL: [500/704] Elapsed 0m 50s (remain 0m 20s) Loss: 0.0018(0.0029) \n","EVAL: [600/704] Elapsed 1m 0s (remain 0m 10s) Loss: 0.0001(0.0030) \n","EVAL: [700/704] Elapsed 1m 10s (remain 0m 0s) Loss: 0.0000(0.0028) \n","EVAL: [703/704] Elapsed 1m 10s (remain 0m 0s) Loss: 0.0000(0.0028) \n","Epoch 2 - avg_train_loss: 0.0020  avg_val_loss: 0.0028  time: 778s\n","Epoch 2 - Score: 0.8625\n","Epoch 2 - Save Best Score: 0.8625 Model\n","Epoch: [3][0/2871] Elapsed 0m 0s (remain 25m 46s) Loss: 0.0000(0.0000) Grad: 1006.6307  LR: 0.000013  \n","Epoch: [3][100/2871] Elapsed 0m 25s (remain 11m 47s) Loss: 0.0006(0.0019) Grad: 1875.2965  LR: 0.000013  \n","Epoch: [3][200/2871] Elapsed 0m 49s (remain 11m 3s) Loss: 0.0047(0.0017) Grad: 25558.5547  LR: 0.000013  \n","Epoch: [3][300/2871] Elapsed 1m 14s (remain 10m 33s) Loss: 0.0002(0.0015) Grad: 1019.5562  LR: 0.000013  \n","Epoch: [3][400/2871] Elapsed 1m 38s (remain 10m 6s) Loss: 0.0009(0.0015) Grad: 8405.0664  LR: 0.000013  \n","Epoch: [3][500/2871] Elapsed 2m 2s (remain 9m 40s) Loss: 0.0001(0.0015) Grad: 415.7165  LR: 0.000013  \n","Epoch: [3][600/2871] Elapsed 2m 27s (remain 9m 15s) Loss: 0.0005(0.0015) Grad: 2742.9832  LR: 0.000012  \n","Epoch: [3][700/2871] Elapsed 2m 51s (remain 8m 50s) Loss: 0.0000(0.0015) Grad: 188.2104  LR: 0.000012  \n","Epoch: [3][800/2871] Elapsed 3m 15s (remain 8m 25s) Loss: 0.0000(0.0015) Grad: 101.4264  LR: 0.000012  \n","Epoch: [3][900/2871] Elapsed 3m 39s (remain 8m 0s) Loss: 0.0018(0.0015) Grad: 25238.9414  LR: 0.000012  \n","Epoch: [3][1000/2871] Elapsed 4m 4s (remain 7m 36s) Loss: 0.0000(0.0015) Grad: 202.6241  LR: 0.000012  \n","Epoch: [3][1100/2871] Elapsed 4m 28s (remain 7m 11s) Loss: 0.0007(0.0015) Grad: 2104.4578  LR: 0.000012  \n","Epoch: [3][1200/2871] Elapsed 4m 52s (remain 6m 47s) Loss: 0.0005(0.0015) Grad: 2116.6377  LR: 0.000011  \n","Epoch: [3][1300/2871] Elapsed 5m 17s (remain 6m 22s) Loss: 0.0008(0.0015) Grad: 5044.9004  LR: 0.000011  \n","Epoch: [3][1400/2871] Elapsed 5m 41s (remain 5m 58s) Loss: 0.0000(0.0015) Grad: 27.5483  LR: 0.000011  \n","Epoch: [3][1500/2871] Elapsed 6m 5s (remain 5m 33s) Loss: 0.0025(0.0015) Grad: 2802.7698  LR: 0.000011  \n","Epoch: [3][1600/2871] Elapsed 6m 30s (remain 5m 9s) Loss: 0.0002(0.0015) Grad: 566.1764  LR: 0.000011  \n","Epoch: [3][1700/2871] Elapsed 6m 54s (remain 4m 45s) Loss: 0.0001(0.0015) Grad: 439.2096  LR: 0.000011  \n","Epoch: [3][1800/2871] Elapsed 7m 18s (remain 4m 20s) Loss: 0.0000(0.0015) Grad: 31.2120  LR: 0.000011  \n","Epoch: [3][1900/2871] Elapsed 7m 43s (remain 3m 56s) Loss: 0.0002(0.0015) Grad: 1212.7952  LR: 0.000010  \n","Epoch: [3][2000/2871] Elapsed 8m 7s (remain 3m 31s) Loss: 0.0000(0.0015) Grad: 164.4447  LR: 0.000010  \n","Epoch: [3][2100/2871] Elapsed 8m 31s (remain 3m 7s) Loss: 0.0000(0.0015) Grad: 182.9309  LR: 0.000010  \n","Epoch: [3][2200/2871] Elapsed 8m 55s (remain 2m 43s) Loss: 0.0005(0.0015) Grad: 3429.8164  LR: 0.000010  \n","Epoch: [3][2300/2871] Elapsed 9m 20s (remain 2m 18s) Loss: 0.0019(0.0015) Grad: 5935.6831  LR: 0.000010  \n","Epoch: [3][2400/2871] Elapsed 9m 44s (remain 1m 54s) Loss: 0.0001(0.0016) Grad: 288.7003  LR: 0.000010  \n","Epoch: [3][2500/2871] Elapsed 10m 8s (remain 1m 30s) Loss: 0.0005(0.0015) Grad: 1830.6301  LR: 0.000009  \n","Epoch: [3][2600/2871] Elapsed 10m 33s (remain 1m 5s) Loss: 0.0001(0.0016) Grad: 544.5503  LR: 0.000009  \n","Epoch: [3][2700/2871] Elapsed 10m 57s (remain 0m 41s) Loss: 0.0001(0.0016) Grad: 762.7075  LR: 0.000009  \n","Epoch: [3][2800/2871] Elapsed 11m 21s (remain 0m 17s) Loss: 0.0000(0.0016) Grad: 167.6775  LR: 0.000009  \n","Epoch: [3][2870/2871] Elapsed 11m 38s (remain 0m 0s) Loss: 0.0004(0.0016) Grad: 1977.9144  LR: 0.000009  \n","EVAL: [0/704] Elapsed 0m 0s (remain 4m 43s) Loss: 0.0002(0.0002) \n","EVAL: [100/704] Elapsed 0m 10s (remain 1m 2s) Loss: 0.0003(0.0026) \n","EVAL: [200/704] Elapsed 0m 20s (remain 0m 51s) Loss: 0.0000(0.0027) \n","EVAL: [300/704] Elapsed 0m 30s (remain 0m 40s) Loss: 0.0000(0.0024) \n","EVAL: [400/704] Elapsed 0m 40s (remain 0m 30s) Loss: 0.0032(0.0028) \n","EVAL: [500/704] Elapsed 0m 50s (remain 0m 20s) Loss: 0.0033(0.0031) \n","EVAL: [600/704] Elapsed 1m 0s (remain 0m 10s) Loss: 0.0000(0.0032) \n","EVAL: [700/704] Elapsed 1m 10s (remain 0m 0s) Loss: 0.0000(0.0029) \n","EVAL: [703/704] Elapsed 1m 10s (remain 0m 0s) Loss: 0.0000(0.0029) \n","Epoch 3 - avg_train_loss: 0.0016  avg_val_loss: 0.0029  time: 773s\n","Epoch 3 - Score: 0.8725\n","Epoch 3 - Save Best Score: 0.8725 Model\n","Epoch: [4][0/2871] Elapsed 0m 0s (remain 26m 44s) Loss: 0.0002(0.0002) Grad: 2584.1731  LR: 0.000009  \n","Epoch: [4][100/2871] Elapsed 0m 26s (remain 11m 53s) Loss: 0.0005(0.0019) Grad: 2544.0571  LR: 0.000009  \n","Epoch: [4][200/2871] Elapsed 0m 50s (remain 11m 7s) Loss: 0.0000(0.0014) Grad: 240.1617  LR: 0.000009  \n","Epoch: [4][300/2871] Elapsed 1m 14s (remain 10m 36s) Loss: 0.0016(0.0013) Grad: 5703.0718  LR: 0.000008  \n","Epoch: [4][400/2871] Elapsed 1m 38s (remain 10m 8s) Loss: 0.0085(0.0014) Grad: 27102.4727  LR: 0.000008  \n","Epoch: [4][500/2871] Elapsed 2m 3s (remain 9m 43s) Loss: 0.0003(0.0014) Grad: 1272.1630  LR: 0.000008  \n","Epoch: [4][600/2871] Elapsed 2m 27s (remain 9m 17s) Loss: 0.0007(0.0013) Grad: 4069.1963  LR: 0.000008  \n","Epoch: [4][700/2871] Elapsed 2m 51s (remain 8m 52s) Loss: 0.0000(0.0013) Grad: 49.1546  LR: 0.000008  \n","Epoch: [4][800/2871] Elapsed 3m 16s (remain 8m 26s) Loss: 0.0017(0.0013) Grad: 6393.2119  LR: 0.000008  \n","Epoch: [4][900/2871] Elapsed 3m 40s (remain 8m 1s) Loss: 0.0000(0.0013) Grad: 342.1586  LR: 0.000007  \n","Epoch: [4][1000/2871] Elapsed 4m 4s (remain 7m 37s) Loss: 0.0029(0.0013) Grad: 24775.0293  LR: 0.000007  \n","Epoch: [4][1100/2871] Elapsed 4m 29s (remain 7m 12s) Loss: 0.0000(0.0013) Grad: 158.9767  LR: 0.000007  \n","Epoch: [4][1200/2871] Elapsed 4m 53s (remain 6m 47s) Loss: 0.0002(0.0013) Grad: 1078.2000  LR: 0.000007  \n","Epoch: [4][1300/2871] Elapsed 5m 17s (remain 6m 23s) Loss: 0.0009(0.0013) Grad: 10026.6865  LR: 0.000007  \n","Epoch: [4][1400/2871] Elapsed 5m 41s (remain 5m 58s) Loss: 0.0000(0.0014) Grad: 72.4607  LR: 0.000007  \n","Epoch: [4][1500/2871] Elapsed 6m 6s (remain 5m 34s) Loss: 0.0004(0.0013) Grad: 5200.4404  LR: 0.000007  \n","Epoch: [4][1600/2871] Elapsed 6m 30s (remain 5m 9s) Loss: 0.0003(0.0013) Grad: 1773.3436  LR: 0.000006  \n","Epoch: [4][1700/2871] Elapsed 6m 55s (remain 4m 45s) Loss: 0.0001(0.0013) Grad: 587.2454  LR: 0.000006  \n","Epoch: [4][1800/2871] Elapsed 7m 19s (remain 4m 21s) Loss: 0.0006(0.0013) Grad: 2197.8662  LR: 0.000006  \n","Epoch: [4][1900/2871] Elapsed 7m 43s (remain 3m 56s) Loss: 0.0000(0.0013) Grad: 386.0967  LR: 0.000006  \n","Epoch: [4][2000/2871] Elapsed 8m 7s (remain 3m 32s) Loss: 0.0004(0.0013) Grad: 4220.3652  LR: 0.000006  \n","Epoch: [4][2100/2871] Elapsed 8m 32s (remain 3m 7s) Loss: 0.0000(0.0013) Grad: 551.6745  LR: 0.000006  \n","Epoch: [4][2200/2871] Elapsed 8m 56s (remain 2m 43s) Loss: 0.0004(0.0013) Grad: 1616.4644  LR: 0.000005  \n","Epoch: [4][2300/2871] Elapsed 9m 20s (remain 2m 18s) Loss: 0.0085(0.0013) Grad: 14814.3691  LR: 0.000005  \n","Epoch: [4][2400/2871] Elapsed 9m 44s (remain 1m 54s) Loss: 0.0000(0.0013) Grad: 293.2539  LR: 0.000005  \n","Epoch: [4][2500/2871] Elapsed 10m 8s (remain 1m 30s) Loss: 0.0000(0.0013) Grad: 73.3135  LR: 0.000005  \n","Epoch: [4][2600/2871] Elapsed 10m 32s (remain 1m 5s) Loss: 0.0017(0.0012) Grad: 6274.7476  LR: 0.000005  \n","Epoch: [4][2700/2871] Elapsed 10m 56s (remain 0m 41s) Loss: 0.0001(0.0013) Grad: 678.1739  LR: 0.000005  \n","Epoch: [4][2800/2871] Elapsed 11m 20s (remain 0m 17s) Loss: 0.0054(0.0013) Grad: 29711.8613  LR: 0.000005  \n","Epoch: [4][2870/2871] Elapsed 11m 37s (remain 0m 0s) Loss: 0.0001(0.0013) Grad: 839.9382  LR: 0.000004  \n","EVAL: [0/704] Elapsed 0m 0s (remain 4m 35s) Loss: 0.0004(0.0004) \n","EVAL: [100/704] Elapsed 0m 10s (remain 1m 1s) Loss: 0.0003(0.0029) \n","EVAL: [200/704] Elapsed 0m 20s (remain 0m 50s) Loss: 0.0000(0.0029) \n","EVAL: [300/704] Elapsed 0m 30s (remain 0m 40s) Loss: 0.0001(0.0026) \n","EVAL: [400/704] Elapsed 0m 39s (remain 0m 30s) Loss: 0.0017(0.0030) \n","EVAL: [500/704] Elapsed 0m 49s (remain 0m 20s) Loss: 0.0038(0.0034) \n","EVAL: [600/704] Elapsed 0m 59s (remain 0m 10s) Loss: 0.0000(0.0036) \n","EVAL: [700/704] Elapsed 1m 9s (remain 0m 0s) Loss: 0.0000(0.0033) \n","EVAL: [703/704] Elapsed 1m 9s (remain 0m 0s) Loss: 0.0000(0.0033) \n","Epoch 4 - avg_train_loss: 0.0013  avg_val_loss: 0.0033  time: 771s\n","Epoch 4 - Score: 0.8722\n","Epoch: [5][0/2871] Elapsed 0m 0s (remain 25m 59s) Loss: 0.0011(0.0011) Grad: 7574.2812  LR: 0.000004  \n","Epoch: [5][100/2871] Elapsed 0m 24s (remain 11m 20s) Loss: 0.0381(0.0011) Grad: 44994.2148  LR: 0.000004  \n","Epoch: [5][200/2871] Elapsed 0m 49s (remain 10m 52s) Loss: 0.0004(0.0010) Grad: 2935.7039  LR: 0.000004  \n","Epoch: [5][300/2871] Elapsed 1m 13s (remain 10m 28s) Loss: 0.0000(0.0011) Grad: 40.6507  LR: 0.000004  \n","Epoch: [5][400/2871] Elapsed 1m 37s (remain 10m 2s) Loss: 0.0003(0.0011) Grad: 2076.5144  LR: 0.000004  \n","Epoch: [5][500/2871] Elapsed 2m 2s (remain 9m 37s) Loss: 0.0000(0.0010) Grad: 60.3700  LR: 0.000004  \n","Epoch: [5][600/2871] Elapsed 2m 26s (remain 9m 12s) Loss: 0.0017(0.0010) Grad: 10408.2139  LR: 0.000004  \n","Epoch: [5][700/2871] Elapsed 2m 50s (remain 8m 48s) Loss: 0.0011(0.0010) Grad: 2983.0046  LR: 0.000003  \n","Epoch: [5][800/2871] Elapsed 3m 15s (remain 8m 23s) Loss: 0.0000(0.0010) Grad: 26.3623  LR: 0.000003  \n","Epoch: [5][900/2871] Elapsed 3m 39s (remain 7m 59s) Loss: 0.0022(0.0011) Grad: 44105.4766  LR: 0.000003  \n","Epoch: [5][1000/2871] Elapsed 4m 3s (remain 7m 35s) Loss: 0.0002(0.0011) Grad: 1803.5508  LR: 0.000003  \n","Epoch: [5][1100/2871] Elapsed 4m 27s (remain 7m 10s) Loss: 0.0001(0.0011) Grad: 1222.9185  LR: 0.000003  \n","Epoch: [5][1200/2871] Elapsed 4m 52s (remain 6m 46s) Loss: 0.0001(0.0011) Grad: 343.0169  LR: 0.000003  \n","Epoch: [5][1300/2871] Elapsed 5m 16s (remain 6m 21s) Loss: 0.0000(0.0010) Grad: 16.3867  LR: 0.000002  \n","Epoch: [5][1400/2871] Elapsed 5m 40s (remain 5m 57s) Loss: 0.0000(0.0011) Grad: 275.7569  LR: 0.000002  \n","Epoch: [5][1500/2871] Elapsed 6m 4s (remain 5m 33s) Loss: 0.0004(0.0011) Grad: 2448.6873  LR: 0.000002  \n","Epoch: [5][1600/2871] Elapsed 6m 29s (remain 5m 8s) Loss: 0.0000(0.0010) Grad: 74.6630  LR: 0.000002  \n","Epoch: [5][1700/2871] Elapsed 6m 53s (remain 4m 44s) Loss: 0.0001(0.0010) Grad: 660.9669  LR: 0.000002  \n","Epoch: [5][1800/2871] Elapsed 7m 17s (remain 4m 20s) Loss: 0.0000(0.0010) Grad: 55.8300  LR: 0.000002  \n","Epoch: [5][1900/2871] Elapsed 7m 41s (remain 3m 55s) Loss: 0.0003(0.0010) Grad: 2770.2732  LR: 0.000002  \n","Epoch: [5][2000/2871] Elapsed 8m 6s (remain 3m 31s) Loss: 0.0000(0.0010) Grad: 6.8055  LR: 0.000001  \n","Epoch: [5][2100/2871] Elapsed 8m 30s (remain 3m 7s) Loss: 0.0001(0.0010) Grad: 475.4055  LR: 0.000001  \n","Epoch: [5][2200/2871] Elapsed 8m 54s (remain 2m 42s) Loss: 0.0016(0.0010) Grad: 6907.9448  LR: 0.000001  \n","Epoch: [5][2300/2871] Elapsed 9m 19s (remain 2m 18s) Loss: 0.0000(0.0010) Grad: 192.4421  LR: 0.000001  \n","Epoch: [5][2400/2871] Elapsed 9m 43s (remain 1m 54s) Loss: 0.0005(0.0010) Grad: 4180.5171  LR: 0.000001  \n","Epoch: [5][2500/2871] Elapsed 10m 7s (remain 1m 29s) Loss: 0.0000(0.0010) Grad: 141.6908  LR: 0.000001  \n","Epoch: [5][2600/2871] Elapsed 10m 31s (remain 1m 5s) Loss: 0.0002(0.0010) Grad: 1032.1323  LR: 0.000000  \n","Epoch: [5][2700/2871] Elapsed 10m 55s (remain 0m 41s) Loss: 0.0002(0.0010) Grad: 799.6765  LR: 0.000000  \n","Epoch: [5][2800/2871] Elapsed 11m 19s (remain 0m 16s) Loss: 0.0000(0.0010) Grad: 134.6420  LR: 0.000000  \n","Epoch: [5][2870/2871] Elapsed 11m 36s (remain 0m 0s) Loss: 0.0001(0.0010) Grad: 751.7803  LR: 0.000000  \n","EVAL: [0/704] Elapsed 0m 0s (remain 4m 40s) Loss: 0.0002(0.0002) \n","EVAL: [100/704] Elapsed 0m 10s (remain 1m 1s) Loss: 0.0002(0.0030) \n","EVAL: [200/704] Elapsed 0m 20s (remain 0m 50s) Loss: 0.0000(0.0030) \n","EVAL: [300/704] Elapsed 0m 30s (remain 0m 40s) Loss: 0.0001(0.0028) \n","EVAL: [400/704] Elapsed 0m 39s (remain 0m 30s) Loss: 0.0021(0.0033) \n","EVAL: [500/704] Elapsed 0m 49s (remain 0m 20s) Loss: 0.0039(0.0038) \n","EVAL: [600/704] Elapsed 0m 59s (remain 0m 10s) Loss: 0.0000(0.0039) \n","EVAL: [700/704] Elapsed 1m 9s (remain 0m 0s) Loss: 0.0000(0.0036) \n","EVAL: [703/704] Elapsed 1m 9s (remain 0m 0s) Loss: 0.0000(0.0036) \n","Epoch 5 - avg_train_loss: 0.0010  avg_val_loss: 0.0036  time: 771s\n","Epoch 5 - Score: 0.8747\n","Epoch 5 - Save Best Score: 0.8747 Model\n","========== fold: 3 training ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n","Epoch: [1][0/2877] Elapsed 0m 0s (remain 28m 21s) Loss: 0.1030(0.1030) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/2877] Elapsed 0m 25s (remain 11m 44s) Loss: 0.0243(0.1203) Grad: 4238.3071  LR: 0.000001  \n","Epoch: [1][200/2877] Elapsed 0m 49s (remain 11m 4s) Loss: 0.0116(0.0683) Grad: 797.4820  LR: 0.000003  \n","Epoch: [1][300/2877] Elapsed 1m 14s (remain 10m 35s) Loss: 0.0038(0.0492) Grad: 538.6315  LR: 0.000004  \n","Epoch: [1][400/2877] Elapsed 1m 38s (remain 10m 9s) Loss: 0.0125(0.0386) Grad: 2734.5032  LR: 0.000006  \n","Epoch: [1][500/2877] Elapsed 2m 3s (remain 9m 43s) Loss: 0.0008(0.0321) Grad: 235.7016  LR: 0.000007  \n","Epoch: [1][600/2877] Elapsed 2m 27s (remain 9m 18s) Loss: 0.0021(0.0274) Grad: 504.1712  LR: 0.000008  \n","Epoch: [1][700/2877] Elapsed 2m 51s (remain 8m 52s) Loss: 0.0005(0.0241) Grad: 116.7544  LR: 0.000010  \n","Epoch: [1][800/2877] Elapsed 3m 15s (remain 8m 27s) Loss: 0.0023(0.0216) Grad: 441.4161  LR: 0.000011  \n","Epoch: [1][900/2877] Elapsed 3m 39s (remain 8m 1s) Loss: 0.0014(0.0196) Grad: 256.4124  LR: 0.000013  \n","Epoch: [1][1000/2877] Elapsed 4m 3s (remain 7m 36s) Loss: 0.0013(0.0180) Grad: 307.0243  LR: 0.000014  \n","Epoch: [1][1100/2877] Elapsed 4m 27s (remain 7m 11s) Loss: 0.0026(0.0167) Grad: 500.6504  LR: 0.000015  \n","Epoch: [1][1200/2877] Elapsed 4m 51s (remain 6m 47s) Loss: 0.0149(0.0156) Grad: 2416.3445  LR: 0.000017  \n","Epoch: [1][1300/2877] Elapsed 5m 15s (remain 6m 22s) Loss: 0.0036(0.0147) Grad: 1265.7936  LR: 0.000018  \n","Epoch: [1][1400/2877] Elapsed 5m 40s (remain 5m 58s) Loss: 0.0006(0.0139) Grad: 122.4105  LR: 0.000019  \n","Epoch: [1][1500/2877] Elapsed 6m 4s (remain 5m 33s) Loss: 0.0158(0.0132) Grad: 1857.5121  LR: 0.000020  \n","Epoch: [1][1600/2877] Elapsed 6m 28s (remain 5m 9s) Loss: 0.0010(0.0126) Grad: 141.1221  LR: 0.000020  \n","Epoch: [1][1700/2877] Elapsed 6m 52s (remain 4m 45s) Loss: 0.0055(0.0120) Grad: 1532.0940  LR: 0.000020  \n","Epoch: [1][1800/2877] Elapsed 7m 16s (remain 4m 20s) Loss: 0.0003(0.0115) Grad: 76.2405  LR: 0.000019  \n","Epoch: [1][1900/2877] Elapsed 7m 40s (remain 3m 56s) Loss: 0.0009(0.0111) Grad: 111.0540  LR: 0.000019  \n","Epoch: [1][2000/2877] Elapsed 8m 4s (remain 3m 32s) Loss: 0.0013(0.0107) Grad: 126.1976  LR: 0.000019  \n","Epoch: [1][2100/2877] Elapsed 8m 28s (remain 3m 7s) Loss: 0.0012(0.0103) Grad: 183.0367  LR: 0.000019  \n","Epoch: [1][2200/2877] Elapsed 8m 52s (remain 2m 43s) Loss: 0.0010(0.0100) Grad: 231.3114  LR: 0.000019  \n","Epoch: [1][2300/2877] Elapsed 9m 16s (remain 2m 19s) Loss: 0.0038(0.0097) Grad: 615.5599  LR: 0.000019  \n","Epoch: [1][2400/2877] Elapsed 9m 40s (remain 1m 55s) Loss: 0.0021(0.0094) Grad: 225.4518  LR: 0.000019  \n","Epoch: [1][2500/2877] Elapsed 10m 4s (remain 1m 30s) Loss: 0.0026(0.0092) Grad: 688.3120  LR: 0.000018  \n","Epoch: [1][2600/2877] Elapsed 10m 28s (remain 1m 6s) Loss: 0.0035(0.0089) Grad: 268.1917  LR: 0.000018  \n","Epoch: [1][2700/2877] Elapsed 10m 52s (remain 0m 42s) Loss: 0.0001(0.0087) Grad: 15.2521  LR: 0.000018  \n","Epoch: [1][2800/2877] Elapsed 11m 16s (remain 0m 18s) Loss: 0.0004(0.0084) Grad: 94.1571  LR: 0.000018  \n","Epoch: [1][2876/2877] Elapsed 11m 35s (remain 0m 0s) Loss: 0.0069(0.0083) Grad: 1130.0887  LR: 0.000018  \n","EVAL: [0/698] Elapsed 0m 0s (remain 4m 29s) Loss: 0.0010(0.0010) \n","EVAL: [100/698] Elapsed 0m 10s (remain 1m 1s) Loss: 0.0014(0.0017) \n","EVAL: [200/698] Elapsed 0m 20s (remain 0m 50s) Loss: 0.0006(0.0020) \n","EVAL: [300/698] Elapsed 0m 30s (remain 0m 39s) Loss: 0.0004(0.0020) \n","EVAL: [400/698] Elapsed 0m 40s (remain 0m 29s) Loss: 0.0099(0.0022) \n","EVAL: [500/698] Elapsed 0m 49s (remain 0m 19s) Loss: 0.0053(0.0023) \n","EVAL: [600/698] Elapsed 0m 59s (remain 0m 9s) Loss: 0.0022(0.0022) \n","EVAL: [697/698] Elapsed 1m 9s (remain 0m 0s) Loss: 0.0000(0.0021) \n","Epoch 1 - avg_train_loss: 0.0083  avg_val_loss: 0.0021  time: 768s\n","Epoch 1 - Score: 0.8476\n","Epoch 1 - Save Best Score: 0.8476 Model\n","Epoch: [2][0/2877] Elapsed 0m 0s (remain 26m 15s) Loss: 0.0039(0.0039) Grad: 7249.0215  LR: 0.000018  \n","Epoch: [2][100/2877] Elapsed 0m 25s (remain 11m 48s) Loss: 0.0003(0.0021) Grad: 1389.9474  LR: 0.000018  \n","Epoch: [2][200/2877] Elapsed 0m 49s (remain 11m 3s) Loss: 0.0019(0.0024) Grad: 10034.8477  LR: 0.000017  \n","Epoch: [2][300/2877] Elapsed 1m 14s (remain 10m 33s) Loss: 0.0012(0.0023) Grad: 7338.4272  LR: 0.000017  \n","Epoch: [2][400/2877] Elapsed 1m 38s (remain 10m 5s) Loss: 0.0129(0.0021) Grad: 21874.9180  LR: 0.000017  \n","Epoch: [2][500/2877] Elapsed 2m 2s (remain 9m 38s) Loss: 0.0029(0.0021) Grad: 30096.7266  LR: 0.000017  \n","Epoch: [2][600/2877] Elapsed 2m 26s (remain 9m 13s) Loss: 0.0044(0.0021) Grad: 10556.3369  LR: 0.000017  \n","Epoch: [2][700/2877] Elapsed 2m 50s (remain 8m 47s) Loss: 0.0003(0.0022) Grad: 771.5239  LR: 0.000017  \n","Epoch: [2][800/2877] Elapsed 3m 13s (remain 8m 22s) Loss: 0.0003(0.0022) Grad: 1416.5229  LR: 0.000017  \n","Epoch: [2][900/2877] Elapsed 3m 38s (remain 7m 58s) Loss: 0.0001(0.0022) Grad: 4881.6577  LR: 0.000016  \n","Epoch: [2][1000/2877] Elapsed 4m 2s (remain 7m 33s) Loss: 0.0016(0.0021) Grad: 12495.9961  LR: 0.000016  \n","Epoch: [2][1100/2877] Elapsed 4m 26s (remain 7m 9s) Loss: 0.0005(0.0021) Grad: 2376.7109  LR: 0.000016  \n","Epoch: [2][1200/2877] Elapsed 4m 50s (remain 6m 44s) Loss: 0.0000(0.0020) Grad: 32.7575  LR: 0.000016  \n","Epoch: [2][1300/2877] Elapsed 5m 14s (remain 6m 20s) Loss: 0.0012(0.0021) Grad: 4471.8667  LR: 0.000016  \n","Epoch: [2][1400/2877] Elapsed 5m 38s (remain 5m 56s) Loss: 0.0013(0.0020) Grad: 4510.1357  LR: 0.000016  \n","Epoch: [2][1500/2877] Elapsed 6m 2s (remain 5m 32s) Loss: 0.0053(0.0020) Grad: 16414.4727  LR: 0.000015  \n","Epoch: [2][1600/2877] Elapsed 6m 26s (remain 5m 7s) Loss: 0.0019(0.0020) Grad: 4617.6704  LR: 0.000015  \n","Epoch: [2][1700/2877] Elapsed 6m 50s (remain 4m 43s) Loss: 0.0007(0.0020) Grad: 2671.8525  LR: 0.000015  \n","Epoch: [2][1800/2877] Elapsed 7m 14s (remain 4m 19s) Loss: 0.0009(0.0020) Grad: 2042.6364  LR: 0.000015  \n","Epoch: [2][1900/2877] Elapsed 7m 38s (remain 3m 55s) Loss: 0.0004(0.0020) Grad: 9892.8398  LR: 0.000015  \n","Epoch: [2][2000/2877] Elapsed 8m 2s (remain 3m 31s) Loss: 0.0000(0.0020) Grad: 133.7760  LR: 0.000015  \n","Epoch: [2][2100/2877] Elapsed 8m 27s (remain 3m 7s) Loss: 0.0004(0.0020) Grad: 1342.2446  LR: 0.000015  \n","Epoch: [2][2200/2877] Elapsed 8m 51s (remain 2m 43s) Loss: 0.0004(0.0020) Grad: 2059.9568  LR: 0.000014  \n","Epoch: [2][2300/2877] Elapsed 9m 15s (remain 2m 19s) Loss: 0.0000(0.0020) Grad: 68.9342  LR: 0.000014  \n","Epoch: [2][2400/2877] Elapsed 9m 39s (remain 1m 54s) Loss: 0.0052(0.0020) Grad: 14001.7969  LR: 0.000014  \n","Epoch: [2][2500/2877] Elapsed 10m 3s (remain 1m 30s) Loss: 0.0109(0.0020) Grad: 29410.5117  LR: 0.000014  \n","Epoch: [2][2600/2877] Elapsed 10m 27s (remain 1m 6s) Loss: 0.0030(0.0020) Grad: 5077.0796  LR: 0.000014  \n","Epoch: [2][2700/2877] Elapsed 10m 51s (remain 0m 42s) Loss: 0.0101(0.0020) Grad: 25518.1133  LR: 0.000014  \n","Epoch: [2][2800/2877] Elapsed 11m 15s (remain 0m 18s) Loss: 0.0003(0.0020) Grad: 1194.7686  LR: 0.000013  \n","Epoch: [2][2876/2877] Elapsed 11m 33s (remain 0m 0s) Loss: 0.0002(0.0020) Grad: 882.0160  LR: 0.000013  \n","EVAL: [0/698] Elapsed 0m 0s (remain 4m 31s) Loss: 0.0013(0.0013) \n","EVAL: [100/698] Elapsed 0m 10s (remain 1m 1s) Loss: 0.0008(0.0015) \n","EVAL: [200/698] Elapsed 0m 20s (remain 0m 49s) Loss: 0.0003(0.0022) \n","EVAL: [300/698] Elapsed 0m 29s (remain 0m 39s) Loss: 0.0005(0.0021) \n","EVAL: [400/698] Elapsed 0m 39s (remain 0m 29s) Loss: 0.0062(0.0023) \n","EVAL: [500/698] Elapsed 0m 49s (remain 0m 19s) Loss: 0.0018(0.0023) \n","EVAL: [600/698] Elapsed 0m 59s (remain 0m 9s) Loss: 0.0024(0.0022) \n","EVAL: [697/698] Elapsed 1m 9s (remain 0m 0s) Loss: 0.0000(0.0021) \n","Epoch 2 - avg_train_loss: 0.0020  avg_val_loss: 0.0021  time: 766s\n","Epoch 2 - Score: 0.8793\n","Epoch 2 - Save Best Score: 0.8793 Model\n","Epoch: [3][0/2877] Elapsed 0m 0s (remain 26m 5s) Loss: 0.0003(0.0003) Grad: 1288.5780  LR: 0.000013  \n","Epoch: [3][100/2877] Elapsed 0m 25s (remain 11m 36s) Loss: 0.0088(0.0021) Grad: 25868.7988  LR: 0.000013  \n","Epoch: [3][200/2877] Elapsed 0m 49s (remain 11m 0s) Loss: 0.0012(0.0016) Grad: 4048.4619  LR: 0.000013  \n","Epoch: [3][300/2877] Elapsed 1m 13s (remain 10m 30s) Loss: 0.0000(0.0017) Grad: 11.8046  LR: 0.000013  \n","Epoch: [3][400/2877] Elapsed 1m 37s (remain 10m 3s) Loss: 0.0006(0.0016) Grad: 2065.4060  LR: 0.000013  \n","Epoch: [3][500/2877] Elapsed 2m 1s (remain 9m 37s) Loss: 0.0004(0.0016) Grad: 1386.7428  LR: 0.000013  \n","Epoch: [3][600/2877] Elapsed 2m 25s (remain 9m 12s) Loss: 0.0002(0.0015) Grad: 845.6518  LR: 0.000012  \n","Epoch: [3][700/2877] Elapsed 2m 50s (remain 8m 47s) Loss: 0.0024(0.0015) Grad: 7676.9434  LR: 0.000012  \n","Epoch: [3][800/2877] Elapsed 3m 14s (remain 8m 22s) Loss: 0.0004(0.0015) Grad: 1531.9156  LR: 0.000012  \n","Epoch: [3][900/2877] Elapsed 3m 37s (remain 7m 57s) Loss: 0.0061(0.0016) Grad: 33103.3945  LR: 0.000012  \n","Epoch: [3][1000/2877] Elapsed 4m 1s (remain 7m 33s) Loss: 0.0002(0.0016) Grad: 592.4052  LR: 0.000012  \n","Epoch: [3][1100/2877] Elapsed 4m 25s (remain 7m 8s) Loss: 0.0006(0.0017) Grad: 3043.1987  LR: 0.000012  \n","Epoch: [3][1200/2877] Elapsed 4m 49s (remain 6m 44s) Loss: 0.0006(0.0016) Grad: 2671.3940  LR: 0.000011  \n","Epoch: [3][1300/2877] Elapsed 5m 14s (remain 6m 20s) Loss: 0.0003(0.0016) Grad: 1569.9796  LR: 0.000011  \n","Epoch: [3][1400/2877] Elapsed 5m 38s (remain 5m 56s) Loss: 0.0000(0.0016) Grad: 220.1893  LR: 0.000011  \n","Epoch: [3][1500/2877] Elapsed 6m 2s (remain 5m 32s) Loss: 0.0021(0.0016) Grad: 7588.0049  LR: 0.000011  \n","Epoch: [3][1600/2877] Elapsed 6m 26s (remain 5m 7s) Loss: 0.0006(0.0016) Grad: 2147.3826  LR: 0.000011  \n","Epoch: [3][1700/2877] Elapsed 6m 50s (remain 4m 43s) Loss: 0.0011(0.0016) Grad: 3072.8774  LR: 0.000011  \n","Epoch: [3][1800/2877] Elapsed 7m 14s (remain 4m 19s) Loss: 0.0000(0.0016) Grad: 214.1689  LR: 0.000011  \n","Epoch: [3][1900/2877] Elapsed 7m 38s (remain 3m 55s) Loss: 0.0017(0.0016) Grad: 6841.9604  LR: 0.000010  \n","Epoch: [3][2000/2877] Elapsed 8m 2s (remain 3m 31s) Loss: 0.0014(0.0016) Grad: 11666.6240  LR: 0.000010  \n","Epoch: [3][2100/2877] Elapsed 8m 26s (remain 3m 7s) Loss: 0.0000(0.0016) Grad: 45.2161  LR: 0.000010  \n","Epoch: [3][2200/2877] Elapsed 8m 50s (remain 2m 42s) Loss: 0.0002(0.0016) Grad: 1001.1951  LR: 0.000010  \n","Epoch: [3][2300/2877] Elapsed 9m 14s (remain 2m 18s) Loss: 0.0124(0.0016) Grad: 29380.9395  LR: 0.000010  \n","Epoch: [3][2400/2877] Elapsed 9m 38s (remain 1m 54s) Loss: 0.0002(0.0016) Grad: 624.7505  LR: 0.000010  \n","Epoch: [3][2500/2877] Elapsed 10m 2s (remain 1m 30s) Loss: 0.0126(0.0016) Grad: 26939.4395  LR: 0.000009  \n","Epoch: [3][2600/2877] Elapsed 10m 26s (remain 1m 6s) Loss: 0.0005(0.0016) Grad: 2001.8510  LR: 0.000009  \n","Epoch: [3][2700/2877] Elapsed 10m 50s (remain 0m 42s) Loss: 0.0001(0.0016) Grad: 716.7330  LR: 0.000009  \n","Epoch: [3][2800/2877] Elapsed 11m 14s (remain 0m 18s) Loss: 0.0001(0.0016) Grad: 665.4764  LR: 0.000009  \n","Epoch: [3][2876/2877] Elapsed 11m 32s (remain 0m 0s) Loss: 0.0000(0.0016) Grad: 18.1532  LR: 0.000009  \n","EVAL: [0/698] Elapsed 0m 0s (remain 4m 36s) Loss: 0.0003(0.0003) \n","EVAL: [100/698] Elapsed 0m 10s (remain 1m 0s) Loss: 0.0002(0.0017) \n","EVAL: [200/698] Elapsed 0m 20s (remain 0m 49s) Loss: 0.0001(0.0024) \n","EVAL: [300/698] Elapsed 0m 30s (remain 0m 39s) Loss: 0.0005(0.0023) \n","EVAL: [400/698] Elapsed 0m 39s (remain 0m 29s) Loss: 0.0019(0.0025) \n","EVAL: [500/698] Elapsed 0m 50s (remain 0m 19s) Loss: 0.0005(0.0025) \n","EVAL: [600/698] Elapsed 1m 0s (remain 0m 9s) Loss: 0.0014(0.0024) \n","EVAL: [697/698] Elapsed 1m 9s (remain 0m 0s) Loss: 0.0000(0.0024) \n","Epoch 3 - avg_train_loss: 0.0016  avg_val_loss: 0.0024  time: 766s\n","Epoch 3 - Score: 0.8808\n","Epoch 3 - Save Best Score: 0.8808 Model\n","Epoch: [4][0/2877] Elapsed 0m 0s (remain 25m 54s) Loss: 0.0003(0.0003) Grad: 1298.2639  LR: 0.000009  \n","Epoch: [4][100/2877] Elapsed 0m 25s (remain 11m 45s) Loss: 0.0000(0.0015) Grad: 221.9147  LR: 0.000009  \n","Epoch: [4][200/2877] Elapsed 0m 49s (remain 11m 1s) Loss: 0.0003(0.0012) Grad: 1354.9712  LR: 0.000009  \n","Epoch: [4][300/2877] Elapsed 1m 13s (remain 10m 30s) Loss: 0.0026(0.0012) Grad: 7886.3057  LR: 0.000008  \n","Epoch: [4][400/2877] Elapsed 1m 37s (remain 10m 3s) Loss: 0.0000(0.0011) Grad: 16.7921  LR: 0.000008  \n","Epoch: [4][500/2877] Elapsed 2m 1s (remain 9m 38s) Loss: 0.0001(0.0013) Grad: 420.9362  LR: 0.000008  \n","Epoch: [4][600/2877] Elapsed 2m 26s (remain 9m 13s) Loss: 0.0003(0.0012) Grad: 1921.4487  LR: 0.000008  \n","Epoch: [4][700/2877] Elapsed 2m 50s (remain 8m 48s) Loss: 0.0016(0.0012) Grad: 17436.6387  LR: 0.000008  \n","Epoch: [4][800/2877] Elapsed 3m 14s (remain 8m 23s) Loss: 0.0061(0.0013) Grad: 15445.2109  LR: 0.000008  \n","Epoch: [4][900/2877] Elapsed 3m 38s (remain 7m 58s) Loss: 0.0000(0.0012) Grad: 18.1043  LR: 0.000007  \n","Epoch: [4][1000/2877] Elapsed 4m 2s (remain 7m 33s) Loss: 0.0002(0.0012) Grad: 1248.8250  LR: 0.000007  \n","Epoch: [4][1100/2877] Elapsed 4m 26s (remain 7m 9s) Loss: 0.0000(0.0013) Grad: 49.7492  LR: 0.000007  \n","Epoch: [4][1200/2877] Elapsed 4m 50s (remain 6m 45s) Loss: 0.0001(0.0012) Grad: 449.8674  LR: 0.000007  \n","Epoch: [4][1300/2877] Elapsed 5m 14s (remain 6m 20s) Loss: 0.0000(0.0013) Grad: 78.7807  LR: 0.000007  \n","Epoch: [4][1400/2877] Elapsed 5m 38s (remain 5m 56s) Loss: 0.0000(0.0013) Grad: 149.0645  LR: 0.000007  \n","Epoch: [4][1500/2877] Elapsed 6m 2s (remain 5m 32s) Loss: 0.0001(0.0012) Grad: 606.7788  LR: 0.000007  \n","Epoch: [4][1600/2877] Elapsed 6m 26s (remain 5m 7s) Loss: 0.0000(0.0012) Grad: 321.2901  LR: 0.000006  \n","Epoch: [4][1700/2877] Elapsed 6m 50s (remain 4m 43s) Loss: 0.0005(0.0012) Grad: 7199.8574  LR: 0.000006  \n","Epoch: [4][1800/2877] Elapsed 7m 14s (remain 4m 19s) Loss: 0.0001(0.0012) Grad: 3890.5034  LR: 0.000006  \n","Epoch: [4][1900/2877] Elapsed 7m 38s (remain 3m 55s) Loss: 0.0002(0.0012) Grad: 1130.5916  LR: 0.000006  \n","Epoch: [4][2000/2877] Elapsed 8m 2s (remain 3m 31s) Loss: 0.0009(0.0012) Grad: 2819.1802  LR: 0.000006  \n","Epoch: [4][2100/2877] Elapsed 8m 26s (remain 3m 7s) Loss: 0.0000(0.0012) Grad: 12.4948  LR: 0.000006  \n","Epoch: [4][2200/2877] Elapsed 8m 50s (remain 2m 42s) Loss: 0.0008(0.0012) Grad: 4354.8696  LR: 0.000005  \n","Epoch: [4][2300/2877] Elapsed 9m 14s (remain 2m 18s) Loss: 0.0003(0.0013) Grad: 3195.9558  LR: 0.000005  \n","Epoch: [4][2400/2877] Elapsed 9m 38s (remain 1m 54s) Loss: 0.0000(0.0013) Grad: 95.0892  LR: 0.000005  \n","Epoch: [4][2500/2877] Elapsed 10m 2s (remain 1m 30s) Loss: 0.0000(0.0013) Grad: 16.0580  LR: 0.000005  \n","Epoch: [4][2600/2877] Elapsed 10m 26s (remain 1m 6s) Loss: 0.0010(0.0012) Grad: 6950.3906  LR: 0.000005  \n","Epoch: [4][2700/2877] Elapsed 10m 50s (remain 0m 42s) Loss: 0.0001(0.0012) Grad: 547.9901  LR: 0.000005  \n","Epoch: [4][2800/2877] Elapsed 11m 14s (remain 0m 18s) Loss: 0.0001(0.0012) Grad: 274.0318  LR: 0.000005  \n","Epoch: [4][2876/2877] Elapsed 11m 33s (remain 0m 0s) Loss: 0.0001(0.0012) Grad: 582.4088  LR: 0.000004  \n","EVAL: [0/698] Elapsed 0m 0s (remain 4m 32s) Loss: 0.0003(0.0003) \n","EVAL: [100/698] Elapsed 0m 10s (remain 1m 0s) Loss: 0.0003(0.0021) \n","EVAL: [200/698] Elapsed 0m 20s (remain 0m 49s) Loss: 0.0001(0.0030) \n","EVAL: [300/698] Elapsed 0m 29s (remain 0m 39s) Loss: 0.0007(0.0030) \n","EVAL: [400/698] Elapsed 0m 39s (remain 0m 29s) Loss: 0.0038(0.0031) \n","EVAL: [500/698] Elapsed 0m 49s (remain 0m 19s) Loss: 0.0003(0.0031) \n","EVAL: [600/698] Elapsed 0m 59s (remain 0m 9s) Loss: 0.0021(0.0029) \n","EVAL: [697/698] Elapsed 1m 8s (remain 0m 0s) Loss: 0.0000(0.0028) \n","Epoch 4 - avg_train_loss: 0.0012  avg_val_loss: 0.0028  time: 766s\n","Epoch 4 - Score: 0.8857\n","Epoch 4 - Save Best Score: 0.8857 Model\n","Epoch: [5][0/2877] Elapsed 0m 0s (remain 26m 25s) Loss: 0.0032(0.0032) Grad: 11223.0420  LR: 0.000004  \n","Epoch: [5][100/2877] Elapsed 0m 25s (remain 11m 33s) Loss: 0.0001(0.0012) Grad: 455.4806  LR: 0.000004  \n","Epoch: [5][200/2877] Elapsed 0m 49s (remain 10m 57s) Loss: 0.0005(0.0013) Grad: 6016.1475  LR: 0.000004  \n","Epoch: [5][300/2877] Elapsed 1m 13s (remain 10m 30s) Loss: 0.0006(0.0011) Grad: 2472.9146  LR: 0.000004  \n","Epoch: [5][400/2877] Elapsed 1m 37s (remain 10m 3s) Loss: 0.0001(0.0011) Grad: 469.6181  LR: 0.000004  \n","Epoch: [5][500/2877] Elapsed 2m 1s (remain 9m 37s) Loss: 0.0002(0.0010) Grad: 1814.7493  LR: 0.000004  \n","Epoch: [5][600/2877] Elapsed 2m 25s (remain 9m 12s) Loss: 0.0000(0.0011) Grad: 11.6601  LR: 0.000004  \n","Epoch: [5][700/2877] Elapsed 2m 49s (remain 8m 47s) Loss: 0.0003(0.0010) Grad: 3113.1709  LR: 0.000003  \n","Epoch: [5][800/2877] Elapsed 3m 13s (remain 8m 22s) Loss: 0.0000(0.0010) Grad: 190.2284  LR: 0.000003  \n","Epoch: [5][900/2877] Elapsed 3m 38s (remain 7m 58s) Loss: 0.0035(0.0010) Grad: 12416.1055  LR: 0.000003  \n","Epoch: [5][1000/2877] Elapsed 4m 2s (remain 7m 33s) Loss: 0.0002(0.0010) Grad: 1621.7025  LR: 0.000003  \n","Epoch: [5][1100/2877] Elapsed 4m 26s (remain 7m 9s) Loss: 0.0000(0.0010) Grad: 675.1713  LR: 0.000003  \n","Epoch: [5][1200/2877] Elapsed 4m 50s (remain 6m 44s) Loss: 0.0004(0.0010) Grad: 1841.0625  LR: 0.000003  \n","Epoch: [5][1300/2877] Elapsed 5m 14s (remain 6m 20s) Loss: 0.0000(0.0010) Grad: 442.9118  LR: 0.000002  \n","Epoch: [5][1400/2877] Elapsed 5m 38s (remain 5m 56s) Loss: 0.0013(0.0010) Grad: 2964.2271  LR: 0.000002  \n","Epoch: [5][1500/2877] Elapsed 6m 2s (remain 5m 32s) Loss: 0.0000(0.0010) Grad: 126.8396  LR: 0.000002  \n","Epoch: [5][1600/2877] Elapsed 6m 26s (remain 5m 8s) Loss: 0.0001(0.0010) Grad: 1172.4449  LR: 0.000002  \n","Epoch: [5][1700/2877] Elapsed 6m 50s (remain 4m 43s) Loss: 0.0005(0.0010) Grad: 2341.8010  LR: 0.000002  \n","Epoch: [5][1800/2877] Elapsed 7m 14s (remain 4m 19s) Loss: 0.0000(0.0010) Grad: 88.8031  LR: 0.000002  \n","Epoch: [5][1900/2877] Elapsed 7m 38s (remain 3m 55s) Loss: 0.0000(0.0010) Grad: 103.3372  LR: 0.000002  \n","Epoch: [5][2000/2877] Elapsed 8m 2s (remain 3m 31s) Loss: 0.0000(0.0010) Grad: 27.7838  LR: 0.000001  \n","Epoch: [5][2100/2877] Elapsed 8m 26s (remain 3m 7s) Loss: 0.0000(0.0010) Grad: 69.9570  LR: 0.000001  \n","Epoch: [5][2200/2877] Elapsed 8m 50s (remain 2m 43s) Loss: 0.0001(0.0010) Grad: 1073.8882  LR: 0.000001  \n","Epoch: [5][2300/2877] Elapsed 9m 15s (remain 2m 18s) Loss: 0.0003(0.0010) Grad: 1353.6932  LR: 0.000001  \n","Epoch: [5][2400/2877] Elapsed 9m 39s (remain 1m 54s) Loss: 0.0001(0.0010) Grad: 1217.3733  LR: 0.000001  \n","Epoch: [5][2500/2877] Elapsed 10m 3s (remain 1m 30s) Loss: 0.0006(0.0010) Grad: 5793.4258  LR: 0.000001  \n","Epoch: [5][2600/2877] Elapsed 10m 27s (remain 1m 6s) Loss: 0.0004(0.0010) Grad: 1985.2606  LR: 0.000000  \n","Epoch: [5][2700/2877] Elapsed 10m 51s (remain 0m 42s) Loss: 0.0002(0.0010) Grad: 4942.9990  LR: 0.000000  \n","Epoch: [5][2800/2877] Elapsed 11m 15s (remain 0m 18s) Loss: 0.0001(0.0010) Grad: 498.4635  LR: 0.000000  \n","Epoch: [5][2876/2877] Elapsed 11m 33s (remain 0m 0s) Loss: 0.0006(0.0010) Grad: 4587.5298  LR: 0.000000  \n","EVAL: [0/698] Elapsed 0m 0s (remain 4m 34s) Loss: 0.0001(0.0001) \n","EVAL: [100/698] Elapsed 0m 10s (remain 1m 1s) Loss: 0.0003(0.0024) \n","EVAL: [200/698] Elapsed 0m 20s (remain 0m 49s) Loss: 0.0001(0.0036) \n","EVAL: [300/698] Elapsed 0m 29s (remain 0m 39s) Loss: 0.0012(0.0036) \n","EVAL: [400/698] Elapsed 0m 39s (remain 0m 29s) Loss: 0.0012(0.0036) \n","EVAL: [500/698] Elapsed 0m 49s (remain 0m 19s) Loss: 0.0003(0.0035) \n","EVAL: [600/698] Elapsed 0m 59s (remain 0m 9s) Loss: 0.0016(0.0033) \n","EVAL: [697/698] Elapsed 1m 9s (remain 0m 0s) Loss: 0.0000(0.0032) \n","Epoch 5 - avg_train_loss: 0.0010  avg_val_loss: 0.0032  time: 766s\n","Epoch 5 - Score: 0.8854\n","========== fold: 4 training ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n","Epoch: [1][0/2850] Elapsed 0m 0s (remain 26m 0s) Loss: 0.1446(0.1446) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/2850] Elapsed 0m 25s (remain 11m 20s) Loss: 0.0356(0.1045) Grad: 16126.8691  LR: 0.000001  \n","Epoch: [1][200/2850] Elapsed 0m 49s (remain 10m 53s) Loss: 0.0063(0.0605) Grad: 1232.7656  LR: 0.000003  \n","Epoch: [1][300/2850] Elapsed 1m 14s (remain 10m 27s) Loss: 0.0040(0.0439) Grad: 955.4095  LR: 0.000004  \n","Epoch: [1][400/2850] Elapsed 1m 38s (remain 10m 1s) Loss: 0.0011(0.0344) Grad: 469.0217  LR: 0.000006  \n","Epoch: [1][500/2850] Elapsed 2m 3s (remain 9m 36s) Loss: 0.0035(0.0287) Grad: 2892.7832  LR: 0.000007  \n","Epoch: [1][600/2850] Elapsed 2m 27s (remain 9m 12s) Loss: 0.0011(0.0247) Grad: 989.6769  LR: 0.000008  \n","Epoch: [1][700/2850] Elapsed 2m 51s (remain 8m 45s) Loss: 0.0059(0.0218) Grad: 1710.9741  LR: 0.000010  \n","Epoch: [1][800/2850] Elapsed 3m 15s (remain 8m 20s) Loss: 0.0008(0.0195) Grad: 618.7720  LR: 0.000011  \n","Epoch: [1][900/2850] Elapsed 3m 39s (remain 7m 54s) Loss: 0.0016(0.0178) Grad: 405.9285  LR: 0.000013  \n","Epoch: [1][1000/2850] Elapsed 4m 3s (remain 7m 30s) Loss: 0.0130(0.0163) Grad: 5264.8228  LR: 0.000014  \n","Epoch: [1][1100/2850] Elapsed 4m 27s (remain 7m 5s) Loss: 0.0085(0.0152) Grad: 2111.5842  LR: 0.000015  \n","Epoch: [1][1200/2850] Elapsed 4m 51s (remain 6m 40s) Loss: 0.0002(0.0142) Grad: 88.7328  LR: 0.000017  \n","Epoch: [1][1300/2850] Elapsed 5m 15s (remain 6m 16s) Loss: 0.0020(0.0133) Grad: 830.9427  LR: 0.000018  \n","Epoch: [1][1400/2850] Elapsed 5m 39s (remain 5m 51s) Loss: 0.0034(0.0127) Grad: 1650.7826  LR: 0.000020  \n","Epoch: [1][1500/2850] Elapsed 6m 3s (remain 5m 26s) Loss: 0.0004(0.0121) Grad: 226.1637  LR: 0.000020  \n","Epoch: [1][1600/2850] Elapsed 6m 27s (remain 5m 2s) Loss: 0.0062(0.0115) Grad: 1975.5004  LR: 0.000020  \n","Epoch: [1][1700/2850] Elapsed 6m 51s (remain 4m 38s) Loss: 0.0010(0.0111) Grad: 217.1877  LR: 0.000020  \n","Epoch: [1][1800/2850] Elapsed 7m 15s (remain 4m 13s) Loss: 0.0009(0.0106) Grad: 192.7803  LR: 0.000019  \n","Epoch: [1][1900/2850] Elapsed 7m 40s (remain 3m 49s) Loss: 0.0022(0.0102) Grad: 1090.2335  LR: 0.000019  \n","Epoch: [1][2000/2850] Elapsed 8m 4s (remain 3m 25s) Loss: 0.0061(0.0098) Grad: 2140.7542  LR: 0.000019  \n","Epoch: [1][2100/2850] Elapsed 8m 28s (remain 3m 1s) Loss: 0.0034(0.0095) Grad: 1892.0469  LR: 0.000019  \n","Epoch: [1][2200/2850] Elapsed 8m 52s (remain 2m 36s) Loss: 0.0015(0.0092) Grad: 873.8580  LR: 0.000019  \n","Epoch: [1][2300/2850] Elapsed 9m 16s (remain 2m 12s) Loss: 0.0164(0.0090) Grad: 3318.3113  LR: 0.000019  \n","Epoch: [1][2400/2850] Elapsed 9m 40s (remain 1m 48s) Loss: 0.0016(0.0087) Grad: 285.8464  LR: 0.000018  \n","Epoch: [1][2500/2850] Elapsed 10m 4s (remain 1m 24s) Loss: 0.0012(0.0084) Grad: 502.7496  LR: 0.000018  \n","Epoch: [1][2600/2850] Elapsed 10m 28s (remain 1m 0s) Loss: 0.0166(0.0082) Grad: 3649.9226  LR: 0.000018  \n","Epoch: [1][2700/2850] Elapsed 10m 52s (remain 0m 36s) Loss: 0.0014(0.0080) Grad: 485.2932  LR: 0.000018  \n","Epoch: [1][2800/2850] Elapsed 11m 16s (remain 0m 11s) Loss: 0.0113(0.0079) Grad: 3228.3142  LR: 0.000018  \n","Epoch: [1][2849/2850] Elapsed 11m 28s (remain 0m 0s) Loss: 0.0013(0.0078) Grad: 703.7427  LR: 0.000018  \n","EVAL: [0/725] Elapsed 0m 0s (remain 4m 40s) Loss: 0.0052(0.0052) \n","EVAL: [100/725] Elapsed 0m 10s (remain 1m 3s) Loss: 0.0005(0.0020) \n","EVAL: [200/725] Elapsed 0m 20s (remain 0m 53s) Loss: 0.0038(0.0024) \n","EVAL: [300/725] Elapsed 0m 30s (remain 0m 42s) Loss: 0.0014(0.0023) \n","EVAL: [400/725] Elapsed 0m 40s (remain 0m 32s) Loss: 0.0140(0.0027) \n","EVAL: [500/725] Elapsed 0m 50s (remain 0m 22s) Loss: 0.0020(0.0026) \n","EVAL: [600/725] Elapsed 1m 0s (remain 0m 12s) Loss: 0.0006(0.0025) \n","EVAL: [700/725] Elapsed 1m 10s (remain 0m 2s) Loss: 0.0003(0.0023) \n","EVAL: [724/725] Elapsed 1m 12s (remain 0m 0s) Loss: 0.0052(0.0023) \n","Epoch 1 - avg_train_loss: 0.0078  avg_val_loss: 0.0023  time: 765s\n","Epoch 1 - Score: 0.8437\n","Epoch 1 - Save Best Score: 0.8437 Model\n","Epoch: [2][0/2850] Elapsed 0m 0s (remain 25m 56s) Loss: 0.0006(0.0006) Grad: 1905.7791  LR: 0.000018  \n","Epoch: [2][100/2850] Elapsed 0m 25s (remain 11m 37s) Loss: 0.0013(0.0015) Grad: 5132.4189  LR: 0.000018  \n","Epoch: [2][200/2850] Elapsed 0m 49s (remain 10m 57s) Loss: 0.0001(0.0020) Grad: 408.9510  LR: 0.000017  \n","Epoch: [2][300/2850] Elapsed 1m 13s (remain 10m 25s) Loss: 0.0090(0.0020) Grad: 14384.4160  LR: 0.000017  \n","Epoch: [2][400/2850] Elapsed 1m 38s (remain 10m 0s) Loss: 0.0003(0.0020) Grad: 1899.4073  LR: 0.000017  \n","Epoch: [2][500/2850] Elapsed 2m 2s (remain 9m 34s) Loss: 0.0012(0.0020) Grad: 3731.7783  LR: 0.000017  \n","Epoch: [2][600/2850] Elapsed 2m 26s (remain 9m 8s) Loss: 0.0000(0.0021) Grad: 65.3508  LR: 0.000017  \n","Epoch: [2][700/2850] Elapsed 2m 50s (remain 8m 43s) Loss: 0.0000(0.0020) Grad: 94.4778  LR: 0.000017  \n","Epoch: [2][800/2850] Elapsed 3m 14s (remain 8m 18s) Loss: 0.0022(0.0021) Grad: 6145.9272  LR: 0.000017  \n","Epoch: [2][900/2850] Elapsed 3m 39s (remain 7m 53s) Loss: 0.0008(0.0021) Grad: 2765.2329  LR: 0.000016  \n","Epoch: [2][1000/2850] Elapsed 4m 3s (remain 7m 29s) Loss: 0.0005(0.0021) Grad: 1679.6490  LR: 0.000016  \n","Epoch: [2][1100/2850] Elapsed 4m 27s (remain 7m 4s) Loss: 0.0048(0.0021) Grad: 10662.3672  LR: 0.000016  \n","Epoch: [2][1200/2850] Elapsed 4m 51s (remain 6m 40s) Loss: 0.0001(0.0020) Grad: 506.5823  LR: 0.000016  \n","Epoch: [2][1300/2850] Elapsed 5m 15s (remain 6m 15s) Loss: 0.0002(0.0020) Grad: 740.1779  LR: 0.000016  \n","Epoch: [2][1400/2850] Elapsed 5m 39s (remain 5m 51s) Loss: 0.0001(0.0020) Grad: 831.5156  LR: 0.000016  \n","Epoch: [2][1500/2850] Elapsed 6m 4s (remain 5m 27s) Loss: 0.0000(0.0020) Grad: 32.4541  LR: 0.000015  \n","Epoch: [2][1600/2850] Elapsed 6m 28s (remain 5m 2s) Loss: 0.0013(0.0020) Grad: 2385.3938  LR: 0.000015  \n","Epoch: [2][1700/2850] Elapsed 6m 52s (remain 4m 38s) Loss: 0.0000(0.0020) Grad: 195.0204  LR: 0.000015  \n","Epoch: [2][1800/2850] Elapsed 7m 16s (remain 4m 14s) Loss: 0.0000(0.0020) Grad: 178.3373  LR: 0.000015  \n","Epoch: [2][1900/2850] Elapsed 7m 40s (remain 3m 49s) Loss: 0.0009(0.0020) Grad: 3935.5452  LR: 0.000015  \n","Epoch: [2][2000/2850] Elapsed 8m 4s (remain 3m 25s) Loss: 0.0026(0.0020) Grad: 8580.3975  LR: 0.000015  \n","Epoch: [2][2100/2850] Elapsed 8m 29s (remain 3m 1s) Loss: 0.0177(0.0020) Grad: 39787.7812  LR: 0.000015  \n","Epoch: [2][2200/2850] Elapsed 8m 53s (remain 2m 37s) Loss: 0.0011(0.0020) Grad: 3106.3220  LR: 0.000014  \n","Epoch: [2][2300/2850] Elapsed 9m 17s (remain 2m 12s) Loss: 0.0003(0.0020) Grad: 1046.3867  LR: 0.000014  \n","Epoch: [2][2400/2850] Elapsed 9m 41s (remain 1m 48s) Loss: 0.0015(0.0020) Grad: 2802.7100  LR: 0.000014  \n","Epoch: [2][2500/2850] Elapsed 10m 5s (remain 1m 24s) Loss: 0.0002(0.0020) Grad: 1295.6213  LR: 0.000014  \n","Epoch: [2][2600/2850] Elapsed 10m 29s (remain 1m 0s) Loss: 0.0010(0.0020) Grad: 3206.2864  LR: 0.000014  \n","Epoch: [2][2700/2850] Elapsed 10m 54s (remain 0m 36s) Loss: 0.0141(0.0020) Grad: 40546.7500  LR: 0.000014  \n","Epoch: [2][2800/2850] Elapsed 11m 18s (remain 0m 11s) Loss: 0.0005(0.0020) Grad: 5042.9775  LR: 0.000013  \n","Epoch: [2][2849/2850] Elapsed 11m 30s (remain 0m 0s) Loss: 0.0003(0.0020) Grad: 8803.4443  LR: 0.000013  \n","EVAL: [0/725] Elapsed 0m 0s (remain 4m 37s) Loss: 0.0018(0.0018) \n","EVAL: [100/725] Elapsed 0m 10s (remain 1m 3s) Loss: 0.0005(0.0024) \n","EVAL: [200/725] Elapsed 0m 20s (remain 0m 52s) Loss: 0.0031(0.0031) \n","EVAL: [300/725] Elapsed 0m 30s (remain 0m 42s) Loss: 0.0001(0.0028) \n","EVAL: [400/725] Elapsed 0m 39s (remain 0m 32s) Loss: 0.0211(0.0032) \n","EVAL: [500/725] Elapsed 0m 49s (remain 0m 22s) Loss: 0.0050(0.0032) \n","EVAL: [600/725] Elapsed 0m 59s (remain 0m 12s) Loss: 0.0016(0.0031) \n","EVAL: [700/725] Elapsed 1m 9s (remain 0m 2s) Loss: 0.0000(0.0028) \n","EVAL: [724/725] Elapsed 1m 11s (remain 0m 0s) Loss: 0.0048(0.0027) \n","Epoch 2 - avg_train_loss: 0.0020  avg_val_loss: 0.0027  time: 766s\n","Epoch 2 - Score: 0.8707\n","Epoch 2 - Save Best Score: 0.8707 Model\n","Epoch: [3][0/2850] Elapsed 0m 0s (remain 25m 41s) Loss: 0.0035(0.0035) Grad: 12473.4541  LR: 0.000013  \n","Epoch: [3][100/2850] Elapsed 0m 25s (remain 11m 40s) Loss: 0.0007(0.0013) Grad: 1595.2050  LR: 0.000013  \n","Epoch: [3][200/2850] Elapsed 0m 49s (remain 10m 58s) Loss: 0.0015(0.0015) Grad: 4944.2925  LR: 0.000013  \n","Epoch: [3][300/2850] Elapsed 1m 13s (remain 10m 26s) Loss: 0.0000(0.0015) Grad: 47.1360  LR: 0.000013  \n","Epoch: [3][400/2850] Elapsed 1m 37s (remain 9m 58s) Loss: 0.0011(0.0015) Grad: 3343.6719  LR: 0.000013  \n","Epoch: [3][500/2850] Elapsed 2m 2s (remain 9m 32s) Loss: 0.0000(0.0015) Grad: 192.5270  LR: 0.000013  \n","Epoch: [3][600/2850] Elapsed 2m 26s (remain 9m 7s) Loss: 0.0006(0.0016) Grad: 2213.4863  LR: 0.000012  \n","Epoch: [3][700/2850] Elapsed 2m 50s (remain 8m 41s) Loss: 0.0001(0.0015) Grad: 2512.6638  LR: 0.000012  \n","Epoch: [3][800/2850] Elapsed 3m 14s (remain 8m 17s) Loss: 0.0005(0.0016) Grad: 1542.0792  LR: 0.000012  \n","Epoch: [3][900/2850] Elapsed 3m 38s (remain 7m 52s) Loss: 0.0000(0.0016) Grad: 140.1485  LR: 0.000012  \n","Epoch: [3][1000/2850] Elapsed 4m 2s (remain 7m 27s) Loss: 0.0001(0.0016) Grad: 598.7375  LR: 0.000012  \n","Epoch: [3][1100/2850] Elapsed 4m 26s (remain 7m 3s) Loss: 0.0027(0.0016) Grad: 14576.4951  LR: 0.000012  \n","Epoch: [3][1200/2850] Elapsed 4m 50s (remain 6m 38s) Loss: 0.0054(0.0016) Grad: 21869.7168  LR: 0.000011  \n","Epoch: [3][1300/2850] Elapsed 5m 14s (remain 6m 14s) Loss: 0.0006(0.0016) Grad: 4268.0439  LR: 0.000011  \n","Epoch: [3][1400/2850] Elapsed 5m 38s (remain 5m 50s) Loss: 0.0016(0.0016) Grad: 4507.5747  LR: 0.000011  \n","Epoch: [3][1500/2850] Elapsed 6m 2s (remain 5m 25s) Loss: 0.0026(0.0016) Grad: 12452.3926  LR: 0.000011  \n","Epoch: [3][1600/2850] Elapsed 6m 26s (remain 5m 1s) Loss: 0.0059(0.0016) Grad: 5394.1519  LR: 0.000011  \n","Epoch: [3][1700/2850] Elapsed 6m 50s (remain 4m 37s) Loss: 0.0000(0.0016) Grad: 16.1849  LR: 0.000011  \n","Epoch: [3][1800/2850] Elapsed 7m 14s (remain 4m 13s) Loss: 0.0000(0.0016) Grad: 137.1165  LR: 0.000011  \n","Epoch: [3][1900/2850] Elapsed 7m 38s (remain 3m 48s) Loss: 0.0010(0.0015) Grad: 3536.9126  LR: 0.000010  \n","Epoch: [3][2000/2850] Elapsed 8m 2s (remain 3m 24s) Loss: 0.0019(0.0016) Grad: 7141.6387  LR: 0.000010  \n","Epoch: [3][2100/2850] Elapsed 8m 26s (remain 3m 0s) Loss: 0.0019(0.0016) Grad: 7350.8979  LR: 0.000010  \n","Epoch: [3][2200/2850] Elapsed 8m 50s (remain 2m 36s) Loss: 0.0001(0.0015) Grad: 1468.3615  LR: 0.000010  \n","Epoch: [3][2300/2850] Elapsed 9m 14s (remain 2m 12s) Loss: 0.0001(0.0015) Grad: 325.4249  LR: 0.000010  \n","Epoch: [3][2400/2850] Elapsed 9m 38s (remain 1m 48s) Loss: 0.0024(0.0015) Grad: 4835.8574  LR: 0.000010  \n","Epoch: [3][2500/2850] Elapsed 10m 2s (remain 1m 24s) Loss: 0.0003(0.0015) Grad: 1342.6935  LR: 0.000009  \n","Epoch: [3][2600/2850] Elapsed 10m 26s (remain 1m 0s) Loss: 0.0005(0.0015) Grad: 3877.3262  LR: 0.000009  \n","Epoch: [3][2700/2850] Elapsed 10m 50s (remain 0m 35s) Loss: 0.0030(0.0015) Grad: 6879.4575  LR: 0.000009  \n","Epoch: [3][2800/2850] Elapsed 11m 14s (remain 0m 11s) Loss: 0.0002(0.0015) Grad: 777.6960  LR: 0.000009  \n","Epoch: [3][2849/2850] Elapsed 11m 26s (remain 0m 0s) Loss: 0.0000(0.0015) Grad: 261.4018  LR: 0.000009  \n","EVAL: [0/725] Elapsed 0m 0s (remain 4m 39s) Loss: 0.0014(0.0014) \n","EVAL: [100/725] Elapsed 0m 10s (remain 1m 3s) Loss: 0.0003(0.0025) \n","EVAL: [200/725] Elapsed 0m 20s (remain 0m 52s) Loss: 0.0060(0.0032) \n","EVAL: [300/725] Elapsed 0m 29s (remain 0m 42s) Loss: 0.0002(0.0029) \n","EVAL: [400/725] Elapsed 0m 39s (remain 0m 32s) Loss: 0.0158(0.0031) \n","EVAL: [500/725] Elapsed 0m 49s (remain 0m 22s) Loss: 0.0033(0.0031) \n","EVAL: [600/725] Elapsed 0m 59s (remain 0m 12s) Loss: 0.0031(0.0030) \n","EVAL: [700/725] Elapsed 1m 9s (remain 0m 2s) Loss: 0.0000(0.0028) \n","EVAL: [724/725] Elapsed 1m 11s (remain 0m 0s) Loss: 0.0057(0.0027) \n","Epoch 3 - avg_train_loss: 0.0015  avg_val_loss: 0.0027  time: 762s\n","Epoch 3 - Score: 0.8787\n","Epoch 3 - Save Best Score: 0.8787 Model\n","Epoch: [4][0/2850] Elapsed 0m 0s (remain 26m 14s) Loss: 0.0035(0.0035) Grad: 9023.6924  LR: 0.000009  \n","Epoch: [4][100/2850] Elapsed 0m 25s (remain 11m 38s) Loss: 0.0002(0.0013) Grad: 1172.5776  LR: 0.000009  \n","Epoch: [4][200/2850] Elapsed 0m 49s (remain 10m 54s) Loss: 0.0004(0.0014) Grad: 1342.2864  LR: 0.000009  \n","Epoch: [4][300/2850] Elapsed 1m 13s (remain 10m 24s) Loss: 0.0035(0.0013) Grad: 32332.2305  LR: 0.000008  \n","Epoch: [4][400/2850] Elapsed 1m 37s (remain 9m 57s) Loss: 0.0029(0.0012) Grad: 7998.9834  LR: 0.000008  \n","Epoch: [4][500/2850] Elapsed 2m 1s (remain 9m 31s) Loss: 0.0000(0.0012) Grad: 33.1729  LR: 0.000008  \n","Epoch: [4][600/2850] Elapsed 2m 26s (remain 9m 7s) Loss: 0.0020(0.0012) Grad: 4643.3740  LR: 0.000008  \n","Epoch: [4][700/2850] Elapsed 2m 50s (remain 8m 41s) Loss: 0.0004(0.0011) Grad: 1392.1964  LR: 0.000008  \n","Epoch: [4][800/2850] Elapsed 3m 14s (remain 8m 17s) Loss: 0.0006(0.0011) Grad: 3316.3271  LR: 0.000008  \n","Epoch: [4][900/2850] Elapsed 3m 38s (remain 7m 52s) Loss: 0.0000(0.0012) Grad: 12.1356  LR: 0.000007  \n","Epoch: [4][1000/2850] Elapsed 4m 2s (remain 7m 27s) Loss: 0.0001(0.0011) Grad: 246.1340  LR: 0.000007  \n","Epoch: [4][1100/2850] Elapsed 4m 26s (remain 7m 3s) Loss: 0.0020(0.0012) Grad: 23150.4512  LR: 0.000007  \n","Epoch: [4][1200/2850] Elapsed 4m 50s (remain 6m 38s) Loss: 0.0053(0.0012) Grad: 10701.5137  LR: 0.000007  \n","Epoch: [4][1300/2850] Elapsed 5m 14s (remain 6m 14s) Loss: 0.0004(0.0012) Grad: 2347.2073  LR: 0.000007  \n","Epoch: [4][1400/2850] Elapsed 5m 38s (remain 5m 50s) Loss: 0.0003(0.0012) Grad: 3098.7136  LR: 0.000007  \n","Epoch: [4][1500/2850] Elapsed 6m 2s (remain 5m 25s) Loss: 0.0000(0.0012) Grad: 105.4734  LR: 0.000007  \n","Epoch: [4][1600/2850] Elapsed 6m 26s (remain 5m 1s) Loss: 0.0000(0.0012) Grad: 171.0502  LR: 0.000006  \n","Epoch: [4][1700/2850] Elapsed 6m 51s (remain 4m 37s) Loss: 0.0000(0.0012) Grad: 19.4813  LR: 0.000006  \n","Epoch: [4][1800/2850] Elapsed 7m 15s (remain 4m 13s) Loss: 0.0001(0.0012) Grad: 2521.0173  LR: 0.000006  \n","Epoch: [4][1900/2850] Elapsed 7m 39s (remain 3m 49s) Loss: 0.0018(0.0012) Grad: 6907.5557  LR: 0.000006  \n","Epoch: [4][2000/2850] Elapsed 8m 3s (remain 3m 25s) Loss: 0.0003(0.0012) Grad: 2408.6843  LR: 0.000006  \n","Epoch: [4][2100/2850] Elapsed 8m 27s (remain 3m 0s) Loss: 0.0000(0.0012) Grad: 86.1383  LR: 0.000006  \n","Epoch: [4][2200/2850] Elapsed 8m 51s (remain 2m 36s) Loss: 0.0000(0.0012) Grad: 32.3052  LR: 0.000005  \n","Epoch: [4][2300/2850] Elapsed 9m 15s (remain 2m 12s) Loss: 0.0002(0.0013) Grad: 1416.2183  LR: 0.000005  \n","Epoch: [4][2400/2850] Elapsed 9m 39s (remain 1m 48s) Loss: 0.0001(0.0012) Grad: 920.7921  LR: 0.000005  \n","Epoch: [4][2500/2850] Elapsed 10m 4s (remain 1m 24s) Loss: 0.0000(0.0012) Grad: 301.1252  LR: 0.000005  \n","Epoch: [4][2600/2850] Elapsed 10m 28s (remain 1m 0s) Loss: 0.0001(0.0012) Grad: 615.7263  LR: 0.000005  \n","Epoch: [4][2700/2850] Elapsed 10m 52s (remain 0m 35s) Loss: 0.0001(0.0012) Grad: 653.8929  LR: 0.000005  \n","Epoch: [4][2800/2850] Elapsed 11m 16s (remain 0m 11s) Loss: 0.0059(0.0012) Grad: 15342.1055  LR: 0.000005  \n","Epoch: [4][2849/2850] Elapsed 11m 28s (remain 0m 0s) Loss: 0.0004(0.0012) Grad: 3310.3179  LR: 0.000004  \n","EVAL: [0/725] Elapsed 0m 0s (remain 4m 43s) Loss: 0.0029(0.0029) \n","EVAL: [100/725] Elapsed 0m 10s (remain 1m 3s) Loss: 0.0005(0.0028) \n","EVAL: [200/725] Elapsed 0m 20s (remain 0m 52s) Loss: 0.0028(0.0034) \n","EVAL: [300/725] Elapsed 0m 29s (remain 0m 42s) Loss: 0.0001(0.0031) \n","EVAL: [400/725] Elapsed 0m 39s (remain 0m 32s) Loss: 0.0065(0.0032) \n","EVAL: [500/725] Elapsed 0m 49s (remain 0m 22s) Loss: 0.0047(0.0032) \n","EVAL: [600/725] Elapsed 0m 59s (remain 0m 12s) Loss: 0.0040(0.0031) \n","EVAL: [700/725] Elapsed 1m 9s (remain 0m 2s) Loss: 0.0000(0.0029) \n","EVAL: [724/725] Elapsed 1m 11s (remain 0m 0s) Loss: 0.0060(0.0028) \n","Epoch 4 - avg_train_loss: 0.0012  avg_val_loss: 0.0028  time: 764s\n","Epoch 4 - Score: 0.8798\n","Epoch 4 - Save Best Score: 0.8798 Model\n","Epoch: [5][0/2850] Elapsed 0m 0s (remain 26m 5s) Loss: 0.0000(0.0000) Grad: 14.1400  LR: 0.000004  \n","Epoch: [5][100/2850] Elapsed 0m 25s (remain 11m 29s) Loss: 0.0010(0.0003) Grad: 28458.9082  LR: 0.000004  \n","Epoch: [5][200/2850] Elapsed 0m 49s (remain 10m 52s) Loss: 0.0022(0.0007) Grad: 4671.6260  LR: 0.000004  \n","Epoch: [5][300/2850] Elapsed 1m 13s (remain 10m 21s) Loss: 0.0000(0.0007) Grad: 259.0061  LR: 0.000004  \n","Epoch: [5][400/2850] Elapsed 1m 37s (remain 9m 54s) Loss: 0.0001(0.0008) Grad: 510.1262  LR: 0.000004  \n","Epoch: [5][500/2850] Elapsed 2m 1s (remain 9m 30s) Loss: 0.0000(0.0008) Grad: 23.6687  LR: 0.000004  \n","Epoch: [5][600/2850] Elapsed 2m 25s (remain 9m 5s) Loss: 0.0015(0.0008) Grad: 4359.0620  LR: 0.000004  \n","Epoch: [5][700/2850] Elapsed 2m 49s (remain 8m 40s) Loss: 0.0000(0.0008) Grad: 13.4906  LR: 0.000003  \n","Epoch: [5][800/2850] Elapsed 3m 13s (remain 8m 15s) Loss: 0.0196(0.0009) Grad: 136496.3906  LR: 0.000003  \n","Epoch: [5][900/2850] Elapsed 3m 37s (remain 7m 50s) Loss: 0.0000(0.0009) Grad: 261.2877  LR: 0.000003  \n","Epoch: [5][1000/2850] Elapsed 4m 1s (remain 7m 26s) Loss: 0.0000(0.0009) Grad: 5.7512  LR: 0.000003  \n","Epoch: [5][1100/2850] Elapsed 4m 25s (remain 7m 2s) Loss: 0.0000(0.0009) Grad: 46.6162  LR: 0.000003  \n","Epoch: [5][1200/2850] Elapsed 4m 49s (remain 6m 38s) Loss: 0.0001(0.0009) Grad: 782.7654  LR: 0.000003  \n","Epoch: [5][1300/2850] Elapsed 5m 14s (remain 6m 13s) Loss: 0.0048(0.0009) Grad: 17343.9414  LR: 0.000002  \n","Epoch: [5][1400/2850] Elapsed 5m 38s (remain 5m 49s) Loss: 0.0008(0.0009) Grad: 9056.8623  LR: 0.000002  \n","Epoch: [5][1500/2850] Elapsed 6m 2s (remain 5m 25s) Loss: 0.0000(0.0009) Grad: 74.2133  LR: 0.000002  \n","Epoch: [5][1600/2850] Elapsed 6m 26s (remain 5m 1s) Loss: 0.0002(0.0009) Grad: 1591.9796  LR: 0.000002  \n","Epoch: [5][1700/2850] Elapsed 6m 50s (remain 4m 37s) Loss: 0.0002(0.0009) Grad: 2094.2803  LR: 0.000002  \n","Epoch: [5][1800/2850] Elapsed 7m 14s (remain 4m 13s) Loss: 0.0003(0.0009) Grad: 5259.0156  LR: 0.000002  \n","Epoch: [5][1900/2850] Elapsed 7m 38s (remain 3m 48s) Loss: 0.0001(0.0009) Grad: 499.3783  LR: 0.000001  \n","Epoch: [5][2000/2850] Elapsed 8m 2s (remain 3m 24s) Loss: 0.0000(0.0009) Grad: 26.9433  LR: 0.000001  \n","Epoch: [5][2100/2850] Elapsed 8m 26s (remain 3m 0s) Loss: 0.0000(0.0009) Grad: 24.0375  LR: 0.000001  \n","Epoch: [5][2200/2850] Elapsed 8m 50s (remain 2m 36s) Loss: 0.0000(0.0009) Grad: 16.7411  LR: 0.000001  \n","Epoch: [5][2300/2850] Elapsed 9m 14s (remain 2m 12s) Loss: 0.0000(0.0009) Grad: 13.3743  LR: 0.000001  \n","Epoch: [5][2400/2850] Elapsed 9m 39s (remain 1m 48s) Loss: 0.0036(0.0009) Grad: 12244.5664  LR: 0.000001  \n","Epoch: [5][2500/2850] Elapsed 10m 3s (remain 1m 24s) Loss: 0.0000(0.0009) Grad: 402.1880  LR: 0.000001  \n","Epoch: [5][2600/2850] Elapsed 10m 27s (remain 1m 0s) Loss: 0.0001(0.0009) Grad: 479.2254  LR: 0.000000  \n","Epoch: [5][2700/2850] Elapsed 10m 51s (remain 0m 35s) Loss: 0.0067(0.0009) Grad: 8316.9600  LR: 0.000000  \n","Epoch: [5][2800/2850] Elapsed 11m 15s (remain 0m 11s) Loss: 0.0000(0.0009) Grad: 60.0253  LR: 0.000000  \n","Epoch: [5][2849/2850] Elapsed 11m 27s (remain 0m 0s) Loss: 0.0005(0.0009) Grad: 5367.6675  LR: 0.000000  \n","EVAL: [0/725] Elapsed 0m 0s (remain 4m 32s) Loss: 0.0021(0.0021) \n","EVAL: [100/725] Elapsed 0m 10s (remain 1m 3s) Loss: 0.0006(0.0033) \n","EVAL: [200/725] Elapsed 0m 20s (remain 0m 52s) Loss: 0.0036(0.0040) \n","EVAL: [300/725] Elapsed 0m 29s (remain 0m 42s) Loss: 0.0000(0.0037) \n","EVAL: [400/725] Elapsed 0m 39s (remain 0m 32s) Loss: 0.0065(0.0038) \n","EVAL: [500/725] Elapsed 0m 49s (remain 0m 22s) Loss: 0.0045(0.0037) \n","EVAL: [600/725] Elapsed 0m 59s (remain 0m 12s) Loss: 0.0047(0.0036) \n","EVAL: [700/725] Elapsed 1m 9s (remain 0m 2s) Loss: 0.0000(0.0033) \n","EVAL: [724/725] Elapsed 1m 11s (remain 0m 0s) Loss: 0.0067(0.0033) \n","Epoch 5 - avg_train_loss: 0.0009  avg_val_loss: 0.0033  time: 763s\n","Epoch 5 - Score: 0.8810\n","Epoch 5 - Save Best Score: 0.8810 Model\n","Best thres: 0.5, Score: 0.8816\n","Best thres: 0.5093749999999999, Score: 0.8817\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f39640d290374992aa246753125a91de","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1193874a74974cc59982c8d5e3ced585","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fc3c6209df394eefa2df9ce8dbb56830","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5536b7aaba7c41f28197e318b362ec75","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"160e78a145894001b2a1295627d80df9","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","\n"]}],"source":["if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","name":"nbme-exp023.ipynb","provenance":[{"file_id":"14l7vjaEJdKkFlJXP9EmrgKriPn4mqCbY","timestamp":1646539060597}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"papermill":{"default_parameters":{},"duration":641.572862,"end_time":"2022-02-27T11:39:50.972497","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-02-27T11:29:09.399635","version":"2.3.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"661a9a315f8646a49162891ae47c69e7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ef004a834af944abbd512fa3218642a1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_74fa3a6b51ad46958e58de5580cf5333","IPY_MODEL_810a830f3b6743b9b074867dd8e4e179","IPY_MODEL_7316ae87cfb849898eb022e100730ba2"]}},"ef004a834af944abbd512fa3218642a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"74fa3a6b51ad46958e58de5580cf5333":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c7cb034c107247cba318475c9952b4ac","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7d891639f26644e8a05d7fe38d178245"}},"810a830f3b6743b9b074867dd8e4e179":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c8a7a19edb074139baefe21f1901d4f4","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":42146,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":42146,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7ed0ca5ee62d45d89050f3caf3d528c9"}},"7316ae87cfb849898eb022e100730ba2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0b48bd338cc94fe396aa1b736b9a2507","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 42146/42146 [00:22&lt;00:00, 1885.59it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2ddd9fc857b549a4ba446dd64a1dd1d4"}},"c7cb034c107247cba318475c9952b4ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7d891639f26644e8a05d7fe38d178245":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c8a7a19edb074139baefe21f1901d4f4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7ed0ca5ee62d45d89050f3caf3d528c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0b48bd338cc94fe396aa1b736b9a2507":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2ddd9fc857b549a4ba446dd64a1dd1d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f319feca977544738ff2400ab23a9276":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_26b1a86ee1ff4ce2862c13d47be2b2d6","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9815ec90f12a4696a85db6dc629ec62a","IPY_MODEL_e032f2bf0bb241c2911087a6efe1ce0b","IPY_MODEL_19783f5141cb47f8aaa057fb01dda913"]}},"26b1a86ee1ff4ce2862c13d47be2b2d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9815ec90f12a4696a85db6dc629ec62a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_953c495e9f64430cbdd9184bb0bd35cb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7e210db5a5fe41f696351dc87d525ee4"}},"e032f2bf0bb241c2911087a6efe1ce0b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1ad701d95f084c98bd1bf0e9d7d498a9","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":143,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":143,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6d74dcf5002c4752af12a65c3aca2113"}},"19783f5141cb47f8aaa057fb01dda913":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1faca6dc4b0e43988d2f81cd209297be","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 143/143 [00:00&lt;00:00, 3251.56it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2c55e9e0223548fbbbe29b3e11e59d50"}},"953c495e9f64430cbdd9184bb0bd35cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7e210db5a5fe41f696351dc87d525ee4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1ad701d95f084c98bd1bf0e9d7d498a9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6d74dcf5002c4752af12a65c3aca2113":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1faca6dc4b0e43988d2f81cd209297be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2c55e9e0223548fbbbe29b3e11e59d50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f39640d290374992aa246753125a91de":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_410c3733ee43430eb55278748d07bc45","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5292a911912d43c2b80919e486b99de9","IPY_MODEL_ce26114873ed4c96b5ea391a41b18f68","IPY_MODEL_bec237aed5184115b697ea257f7b0c9b"]}},"410c3733ee43430eb55278748d07bc45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5292a911912d43c2b80919e486b99de9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3ca14e3fd6b84312b0af50a89d5ac7c1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8234c7d9369644dea7e5c7e8fe436771"}},"ce26114873ed4c96b5ea391a41b18f68":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5a3f361a320f480aa8a4115366073d32","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0f856d468c8a4c4c9c83b5b263745508"}},"bec237aed5184115b697ea257f7b0c9b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_490bfe688fe1419996b69f7de1cfee23","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2/2 [00:00&lt;00:00,  1.38it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_789324d1692d4f478d5b95491b03fc22"}},"3ca14e3fd6b84312b0af50a89d5ac7c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8234c7d9369644dea7e5c7e8fe436771":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5a3f361a320f480aa8a4115366073d32":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0f856d468c8a4c4c9c83b5b263745508":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"490bfe688fe1419996b69f7de1cfee23":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"789324d1692d4f478d5b95491b03fc22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1193874a74974cc59982c8d5e3ced585":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_84ea1506dcad4e01ad1cc35b76c0339a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8e243ea82e59492fb5b845e51a56347a","IPY_MODEL_55ac42bee2ce4f00841b8bd49a7c552d","IPY_MODEL_2281dd4891c640a0b31c23976223f2ba"]}},"84ea1506dcad4e01ad1cc35b76c0339a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8e243ea82e59492fb5b845e51a56347a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6919ba0239084b04988e1de02316c76e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_548835fe547d4114bfd39e5fac680635"}},"55ac42bee2ce4f00841b8bd49a7c552d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5068fb514bf143ba812fe202c3e7a83d","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1e0d277fe44242e19e3bec17a1cb7280"}},"2281dd4891c640a0b31c23976223f2ba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ab8e5c4cef00426fa0cf2fc25c51381a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2/2 [00:00&lt;00:00,  1.31it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2b311e42f1294339a07248b31db0c26c"}},"6919ba0239084b04988e1de02316c76e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"548835fe547d4114bfd39e5fac680635":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5068fb514bf143ba812fe202c3e7a83d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1e0d277fe44242e19e3bec17a1cb7280":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ab8e5c4cef00426fa0cf2fc25c51381a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2b311e42f1294339a07248b31db0c26c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fc3c6209df394eefa2df9ce8dbb56830":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3fb5b968d9ab4e88964b6b126c6023d9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d240d13622c14726a5639d44ef2421ec","IPY_MODEL_9e66574c8c0343ffb0477891bfe5e892","IPY_MODEL_219090e2dd934c1296f12660ea69b161"]}},"3fb5b968d9ab4e88964b6b126c6023d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d240d13622c14726a5639d44ef2421ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e7be4ec44f2a4183b295f486e250b414","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_44650208feba4c118904c7efc9887532"}},"9e66574c8c0343ffb0477891bfe5e892":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d1cd0285cfe34f188e9c779617d48448","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_331b7288a5024ce3a5036af53eb75cec"}},"219090e2dd934c1296f12660ea69b161":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_00419a15e9834e98b8a3459b62d01f8b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2/2 [00:00&lt;00:00,  1.25it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_05fdce5a55c1483a937d07a50bd9465e"}},"e7be4ec44f2a4183b295f486e250b414":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"44650208feba4c118904c7efc9887532":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d1cd0285cfe34f188e9c779617d48448":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"331b7288a5024ce3a5036af53eb75cec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"00419a15e9834e98b8a3459b62d01f8b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"05fdce5a55c1483a937d07a50bd9465e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5536b7aaba7c41f28197e318b362ec75":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5ecd28892bb84432935145e27ac71de7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3b50983bfae8445fa305d1edadd651af","IPY_MODEL_cdbf6aefee644006826f76e2f6722b07","IPY_MODEL_7c62f6a2b08c41a8bc3ecf2efa58c325"]}},"5ecd28892bb84432935145e27ac71de7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3b50983bfae8445fa305d1edadd651af":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8478e8bf8b6146b48e717b84c021e7ab","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b09413b459c8406882a16db62e8df9c0"}},"cdbf6aefee644006826f76e2f6722b07":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_72a8b4ea52534d4e9feab6c6fdd72a77","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_65d16c05424c4df0b79b5786be8bd5d1"}},"7c62f6a2b08c41a8bc3ecf2efa58c325":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_63aa4d26409d437fa76e5a156bb04791","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2/2 [00:01&lt;00:00,  1.16it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e48e5c946462499ab018748ccf80c5b5"}},"8478e8bf8b6146b48e717b84c021e7ab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b09413b459c8406882a16db62e8df9c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"72a8b4ea52534d4e9feab6c6fdd72a77":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"65d16c05424c4df0b79b5786be8bd5d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"63aa4d26409d437fa76e5a156bb04791":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e48e5c946462499ab018748ccf80c5b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"160e78a145894001b2a1295627d80df9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_008f77fefdba425ab2c755f515693e6f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_605151b49d7641a28ebd0ca083770c69","IPY_MODEL_8f8c8632070c4fa0a3182521f41e9c40","IPY_MODEL_dfb4641da88e47d3bafabbaa56bc6916"]}},"008f77fefdba425ab2c755f515693e6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"605151b49d7641a28ebd0ca083770c69":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_537dee640701470c8fc3cc29e7940bee","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6dc964a8934744608f92a6af0f0f923f"}},"8f8c8632070c4fa0a3182521f41e9c40":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_925a3ae98bd6488eb7cffdec89d768da","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d76610cad4f645f187b26f1b82733569"}},"dfb4641da88e47d3bafabbaa56bc6916":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b28fa99d1b1b4e4da668bcc50373e4cc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2/2 [00:01&lt;00:00,  1.11it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b8554928c5f141de8dfb94c04c2dda03"}},"537dee640701470c8fc3cc29e7940bee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6dc964a8934744608f92a6af0f0f923f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"925a3ae98bd6488eb7cffdec89d768da":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d76610cad4f645f187b26f1b82733569":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b28fa99d1b1b4e4da668bcc50373e4cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b8554928c5f141de8dfb94c04c2dda03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":5}