{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "strange-hospital",
   "metadata": {
    "id": "national-fancy"
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-spouse",
   "metadata": {
    "id": "copyrighted-centre"
   },
   "source": [
    "- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-intake",
   "metadata": {
    "id": "imported-offset"
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "packed-lebanon",
   "metadata": {
    "id": "complimentary-wyoming"
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"nbme-exp087\"\n",
    "ENV = \"local\"\n",
    "DEBUG_MODE = False\n",
    "SUBMISSION_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reduced-flesh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y03AHjwJAlGL",
    "outputId": "c33caf3f-c530-4628-bcbf-b9af87b252a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "passive-class",
   "metadata": {
    "id": "allied-circuit"
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    env=ENV\n",
    "    exp_name=EXP_NAME\n",
    "    debug=DEBUG_MODE\n",
    "    submission=SUBMISSION_MODE\n",
    "    apex=True\n",
    "    input_dir=None\n",
    "    output_dir=None\n",
    "    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n",
    "    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n",
    "    competition_name=\"nbme-score-clinical-patient-notes\"\n",
    "    id_col=\"id\"\n",
    "    target_col=\"location\"\n",
    "    pretrained_model_name=\"microsoft/deberta-v3-large\"\n",
    "    tokenizer=None\n",
    "    max_len=None\n",
    "    pseudo_plain_path='../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl'\n",
    "    #pseudo_plain_path=\"./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\"\n",
    "    n_pseudo_labels=100000\n",
    "    output_dim=1\n",
    "    dropout=0.2\n",
    "    num_workers=4\n",
    "    batch_size=3\n",
    "    lr=2e-5\n",
    "    betas=(0.9, 0.98)\n",
    "    weight_decay=0.1\n",
    "    alpha=1\n",
    "    gamma=2\n",
    "    smoothing=0.0001\n",
    "    num_warmup_steps_rate=0.1\n",
    "    batch_scheduler=True\n",
    "    epochs=1\n",
    "    n_fold=4\n",
    "    train_fold=[0, 1, 2, 3]\n",
    "    seed=71\n",
    "    gradient_accumulation_steps=2\n",
    "    max_grad_norm=1000\n",
    "    print_freq=100\n",
    "    train=True\n",
    "    inference=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "greenhouse-dinner",
   "metadata": {
    "id": "geographic-hindu"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.train_fold = [0, 1]\n",
    "\n",
    "if CFG.submission:\n",
    "    CFG.train = False\n",
    "    CFG.inference = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-japan",
   "metadata": {
    "id": "confident-fifth"
   },
   "source": [
    "## Directory Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "architectural-advocate",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "miniature-greeting",
    "outputId": "6a439b4d-636c-4026-8cfd-f86093855807"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "print(CFG.env)\n",
    "if CFG.env == \"colab\":\n",
    "    # colab環境\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "    # install packages\n",
    "    !pip install transformers==4.16.2\n",
    "    !pip install -q sentencepiece==0.1.96\n",
    "\n",
    "elif CFG.env == \"local\":\n",
    "    # ローカルサーバ\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "\n",
    "elif CFG.env == \"kaggle\":\n",
    "    # kaggle環境\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "above-worst",
   "metadata": {
    "id": "nMFg9zv8YGcx"
   },
   "outputs": [],
   "source": [
    "# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n",
    "# This must be done before importing transformers\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "if CFG.env == \"colab\":\n",
    "    input_dir = Path(\"./drive/MyDrive/00.kaggle/input/deberta-v2-3-fast-tokenizer\")\n",
    "    transformers_path = Path(\"/usr/local/lib/python3.7/dist-packages/transformers\")\n",
    "else:\n",
    "    input_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n",
    "    transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n",
    "\n",
    "convert_file = input_dir / \"convert_slow_tokenizer.py\"\n",
    "conversion_path = transformers_path/convert_file.name\n",
    "\n",
    "if conversion_path.exists():\n",
    "    conversion_path.unlink()\n",
    "\n",
    "shutil.copy(convert_file, transformers_path)\n",
    "deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n",
    "\n",
    "for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n",
    "    filepath = deberta_v2_path/filename\n",
    "    if filepath.exists():\n",
    "        filepath.unlink()\n",
    "\n",
    "    shutil.copy(input_dir/filename, filepath)\n",
    "    \n",
    "    \n",
    "from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "separate-albania",
   "metadata": {
    "id": "guilty-filename"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import BartModel,BertModel,BertTokenizer\n",
    "from transformers import DebertaModel,DebertaTokenizer\n",
    "from transformers import RobertaModel,RobertaTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-smith",
   "metadata": {
    "id": "cubic-designation"
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "thirty-genius",
   "metadata": {
    "id": "opposite-plasma"
   },
   "outputs": [],
   "source": [
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "processed-longitude",
   "metadata": {
    "id": "multiple-poland"
   },
   "outputs": [],
   "source": [
    "def create_labels_for_scoring(df):\n",
    "    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n",
    "    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, \"location\"]\n",
    "        if lst:\n",
    "            new_lst = \";\".join(lst)\n",
    "            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n",
    "\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df[\"location_for_create_labels\"].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, token_probs, tokenizer):\n",
    "    res = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n",
    "        encoded = tokenizer(\n",
    "            text=text,\n",
    "            max_length=CFG.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n",
    "            start, end = offset_mapping\n",
    "            res[i][start:end] = pred\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_predicted_location_str(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        # result = np.where(char_prob >= th)[0] + 1\n",
    "        result = np.where(char_prob >= th)[0]\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        # result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = [f\"{min(r)} {max(r) + 1}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def scoring(df, th=0.5, use_token_prob=True):\n",
    "    labels = create_labels_for_scoring(df)\n",
    "\n",
    "    if use_token_prob:\n",
    "        token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n",
    "        char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n",
    "    else:\n",
    "        char_probs = df[[str(i) for i in range(CFG.max_char_len)]].values\n",
    "        char_probs = [char_probs[i] for i in range(len(char_probs))]\n",
    "\n",
    "    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n",
    "    preds = get_predictions(predicted_location_str)\n",
    "\n",
    "    score = get_score(labels, preds)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_best_thres(oof_df):\n",
    "    def f1_opt(x):\n",
    "        return -1 * scoring(oof_df, th=x)\n",
    "\n",
    "    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n",
    "    return best_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "intermediate-tonight",
   "metadata": {
    "id": "seventh-fighter"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "turkish-attraction",
   "metadata": {
    "id": "fifty-boundary"
   },
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "governing-marina",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(texts, preds):\n",
    "    fix_tokenize_dict = {\n",
    "        'heart': ['h', 'eart'],\n",
    "        'hair': ['h', 'air'],\n",
    "        'adderal': ['a', 'dderal'],\n",
    "        'mother': ['m', 'other'],\n",
    "        'intermittent': ['i', 'ntermittent'],\n",
    "        'temperature': ['t', 'emperature'],\n",
    "        'episodes': ['e', 'pisodes'],\n",
    "        'no': ['n', 'o'],\n",
    "        'has': ['h', 'as'],\n",
    "        'LMP': ['L', 'MP'],\n",
    "        '10': ['1', '0'],\n",
    "        'blood': ['b', 'lood'],\n",
    "        'recurrent': ['r', 'ecurrent'],\n",
    "        'denies': ['d', 'enies'],\n",
    "        'sudden': ['s', 'udden'],\n",
    "        'Sexually': ['S', 'exually'],\n",
    "        'up': ['u', 'p'],\n",
    "        'wakes': ['w', 'akes'],\n",
    "        'sweats': ['s', 'weats'],\n",
    "        'hot': ['h', 'ot'],\n",
    "        'drenched': ['d', 'renched'],\n",
    "        'gnawing': ['g', 'nawing'],\n",
    "        'Uses': ['U', 'ses'],\n",
    "        'Begin': ['B', 'egin'],\n",
    "        'Nausea': ['N', 'ausea'],\n",
    "        'Burning': ['B', 'urning'],\n",
    "        'Started': ['S', 'tarted'],\n",
    "        'neurvousness': ['n', 'eurvousness'],\n",
    "        'constipation': ['c', 'onstipation'],\n",
    "        'nervousness': ['n', 'ervousness'],\n",
    "        'cold': ['c', 'old'],\n",
    "        'loss': ['l', 'oss'],\n",
    "        'CBC': ['C', 'BC'],\n",
    "        'Hx': ['H', 'x'],\n",
    "        'tingling': ['t', 'ingling'],\n",
    "        'feels': ['f', 'eels'],\n",
    "        'Lost': ['L', 'ost'],\n",
    "        'she': ['s', 'he'],\n",
    "        'racing': ['r', 'acing'],\n",
    "        'throat': ['t', 'hroat'],\n",
    "        'PATIENT': ['P', 'ATIENT'],\n",
    "        'recreational': ['r', 'ecreational'],\n",
    "        'clammy': ['c', 'lammy'],\n",
    "        'numbness': ['n', 'umbness'],\n",
    "        'like': ['l', 'ike'],\n",
    "        'reports': ['r', 'eports'],\n",
    "        'exercise': ['e', 'xercise'],\n",
    "        'started': ['s', 'tarted'],\n",
    "        'brough': ['b', 'rough'],\n",
    "        'Associated': ['A', 'ssociated'],\n",
    "        'exacerbated': ['e', 'xacerbated'],\n",
    "        'sharp': ['s', 'harp'],\n",
    "        'cannot': ['c', 'annot'],\n",
    "        'heavy': ['h', 'eavy'],\n",
    "        'fatigue': ['f', 'atigue'],\n",
    "        'trouble': ['t', 'rouble'],\n",
    "        'hearing': ['h', 'earing'],\n",
    "        'reduced': ['r', 'educed'],\n",
    "        'lack': ['l', 'ack'],\n",
    "        'vomiting': ['v', 'omiting'],\n",
    "        'generalized': ['g', 'eneralized'],\n",
    "        'body': ['b', 'ody'],\n",
    "        'all': ['a', 'll'],\n",
    "        'scratchy': ['s', 'cratchy'],\n",
    "        'mom': ['m', 'om'],\n",
    "        'discomfort': ['d', 'iscomfort'],\n",
    "        'CAD': ['C', 'AD'],\n",
    "        'Thyroid': ['T', 'hyroid'],\n",
    "        'BLADDER': ['B', 'LADDER'],\n",
    "        'diarrhea': ['d', 'iarrhea'],\n",
    "        'Started': ['S', 'tarted'],\n",
    "        'Vaginal': ['V', 'aginal'],\n",
    "        'sleeping': ['s', 'leeping'],\n",
    "        'UNCLE': ['U', 'NCLE'],\n",
    "        'USING': ['U', 'SING'],\n",
    "        'BURNING': ['B', 'URNING'],\n",
    "        'GETTING': ['G', 'ETTING'],\n",
    "        'ETOH': ['E', 'TOH'],\n",
    "        'ON': ['O', 'N'],\n",
    "        'INITIALLY': ['I', 'NITIALLY'],\n",
    "        'epigastric': ['e', 'pigastric'],\n",
    "        'occurs': ['o', 'ccurs'],\n",
    "        'began': ['b', 'egan'],\n",
    "        'alleviated': ['a', 'lleviated'],\n",
    "        'overwhelmed': ['o', 'verwhelmed'],\n",
    "        'clamminess': ['c', 'lamminess'],\n",
    "        'strongly': ['s', 'trongly'],\n",
    "        'lump': ['l', 'ump'],\n",
    "        'drugs': ['d', 'rugs'],\n",
    "        'chest': ['c', 'hest'],\n",
    "        'stuffy': ['s', 'tuffy'],\n",
    "        'changes': ['c', 'hanges'],\n",
    "        'trouble': ['t', 'rouble'],\n",
    "        'takes': ['t', 'akes'],\n",
    "        'tossing': ['t', 'ossing'],\n",
    "        'Fam': ['F', 'am'],\n",
    "        'sweating': ['s', 'weating'],\n",
    "        'dyspareunia': ['d', 'yspareunia'],\n",
    "        'irregular': ['i', 'rregular'],\n",
    "        'time': ['t', 'ime'],\n",
    "        'unpredictable': ['u', 'npredictable'],\n",
    "        'darkened': ['d', 'arkened'],\n",
    "        'anxiety': ['a', 'nxiety'],\n",
    "        'nervous': ['n', 'ervous'],\n",
    "        'TAKING': ['T', 'AKING'],\n",
    "        'losing': ['l', 'osing'],\n",
    "        'Difficulyt': ['D', 'ifficulyt'],\n",
    "        'Appetite': ['A', 'ppetite'],\n",
    "        'increased': ['i', 'ncreased'],\n",
    "        'fingers': ['f', 'ingers'],\n",
    "        'illicit': ['i', 'llicit'],\n",
    "        'claminess': ['c', 'laminess'],\n",
    "        'clamy': ['c', 'lamy'],\n",
    "        'Recently': ['R', 'ecently'],\n",
    "        'feeling': ['f', 'eeling'],\n",
    "        'aggrav': ['a', 'ggrav'],\n",
    "        'changing': ['c', 'hanging'],\n",
    "        'unable': ['u', 'nable'],\n",
    "        'SEEING': ['S', 'EEING'],\n",
    "        'staying': ['s', 'taying'],\n",
    "        'lightheadedness': ['l', 'ightheadedness'],\n",
    "        'lighheadeness': ['l', 'ighheadeness'],\n",
    "        'nail': ['n', 'ail'],\n",
    "        'pounding': ['p', 'ounding'],\n",
    "        'My': ['M', 'y'],\n",
    "        'Father': ['F', 'ather'],\n",
    "        'urinary': ['u', 'rinary'],\n",
    "        'pain': ['p', 'ain'],\n",
    "        'not': ['n', 'ot'],\n",
    "        'lower': ['l', 'ower'],\n",
    "        'menses': ['m', 'enses'],\n",
    "        'at': ['a', 't'],\n",
    "        'takes': ['t', 'akes'],\n",
    "        'initally': ['i', 'nitally'],\n",
    "        'melena': ['m', 'elena'],\n",
    "        'BOWEL': ['B', 'OWEL'],\n",
    "        'WEIGHT': ['W', 'EIGHT'],\n",
    "        'difficulty': ['d', 'ifficulty'],\n",
    "        'condo': ['c', 'ondo'],\n",
    "        'experiences': ['e', 'xperiences'],\n",
    "        'stuffy': ['s', 'tuffy'],\n",
    "        'rhinorrhea': ['r', 'hinorrhea'],\n",
    "        'felt': ['f', 'elt'],\n",
    "        'feverish': ['f', 'everish'],\n",
    "        'CYCLE': ['C', 'YCLE'],\n",
    "        'tampon': ['t', 'ampon'],\n",
    "        'Last': ['L', 'ast'],\n",
    "        'Son': ['S', 'on'],\n",
    "        'saw': ['s', 'aw'],\n",
    "        'tightness': ['t', 'ightness'],\n",
    "        'rash': ['r', 'ash'],\n",
    "        'ibuprofen': ['i', 'buprofen'],\n",
    "        'SCRATHY': ['S', 'CRATHY'],\n",
    "        'PHOTOPHOBIA': ['P', 'HOTOPHOBIA'],\n",
    "    }\n",
    "    preds_pp = preds.copy()\n",
    "    tk0 = tqdm(range(len(preds_pp)), total=len(preds_pp))\n",
    "    for raw_idx in tk0:\n",
    "        pred = preds[raw_idx]\n",
    "        text = texts[raw_idx]\n",
    "        if len(pred) != 0:\n",
    "            # pp1: indexが1から始まる予測値は0から始まるように修正 ## 0.88579 -> 0.88702\n",
    "            if pred[0][0] == 1:\n",
    "                preds_pp[raw_idx][0][0] = 0\n",
    "            for p_index, pp in enumerate(pred):\n",
    "                start, end = pred[p_index]\n",
    "                # pp2: startとendが同じ予測値はstartを前に１ずらす ## 0.88702 -> 0.88714\n",
    "                if start == end:\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                # pp3: 始点が改行の場合始点を1つ後ろにずらす ## 0.88714 -> 0.88746\n",
    "                if text[start] == '\\n':\n",
    "                    preds_pp[raw_idx][p_index][0] = start + 1\n",
    "                    start = start + 1\n",
    "                # pp4: 1-2などは-2で予測されることがあるので修正 ## 0.88746 -> 0.88747\n",
    "                if text[start-1].isdigit() and text[start] == '-' and text[start+1].isdigit():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                if text[start-1].isdigit() and text[start] == '/' and text[start+1].isdigit():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                # pp5: 67などは7で予測されることがあるので修正 ## 0.88747 -> 0.88748\n",
    "                if text[start-1].isdigit() and text[start].isdigit():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                # pp6: 文頭が大文字で始まるものは大文字部分が除かれて予測されることがあるので修正 ## 0.88748 -> 0.88761\n",
    "                if text[start-2] == '.' and text[start-1].isupper():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                if text[start-2] == ',' and text[start-1].isupper():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                if text[start-2] == ':' and text[start-1].isupper():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                if text[start-2] == '-' and text[start-1].isupper():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                # pp7: heart -> h + eart となっているようなものを修正する ## 0.88761 -> 0.88806\n",
    "                for key, fix_tokenize in fix_tokenize_dict.items():\n",
    "                    _s, s = fix_tokenize[0], fix_tokenize[1]\n",
    "                    if text[start-1].lower() == _s.lower() and text[start:start+len(s)].lower() == s.lower():\n",
    "                        preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                        start = start - 1\n",
    "    return preds_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aware-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_from_preds_list(preds):\n",
    "    results = []\n",
    "    for pred in preds:\n",
    "        s = []\n",
    "        for p in pred:\n",
    "            s.append(' '.join(list(map(str, p))))\n",
    "        s = ';'.join(s)\n",
    "        results.append(s)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "western-panic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trunc_pred(texts, preds):\n",
    "    preds_pp = preds.copy()\n",
    "    tk0 = tqdm(range(len(preds_pp)), total=len(preds_pp))\n",
    "    for raw_idx in tk0:\n",
    "        text = texts[raw_idx]\n",
    "        num_text = len(text)\n",
    "        preds_pp[raw_idx, num_text:] = 0\n",
    "    return preds_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "guilty-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label(pn_history, location_list, max_char_len):\n",
    "    label = np.zeros(max_char_len)\n",
    "    label[len(pn_history):] = -1\n",
    "    if len(location_list) > 0:\n",
    "        for location in location_list:\n",
    "            start, end = int(location[0]), int(location[1])\n",
    "            label[start:end] = 1\n",
    "    return label\n",
    "\n",
    "def get_preds_from_results(results, texts, max_char_len):\n",
    "    labels = []\n",
    "    for idx, result in enumerate(results):\n",
    "        label = create_label(texts[idx], result, max_char_len)\n",
    "        labels.append(label)\n",
    "    labels = np.stack(labels)\n",
    "    print(labels.shape)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-drill",
   "metadata": {
    "id": "unlimited-hotel"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adjusted-citizen",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "classical-machine",
    "outputId": "918ddc66-5d2d-44dc-c637-0145350ded5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 6), (143, 3), (42146, 3), (5, 4))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(CFG.input_dir / \"train.csv\")\n",
    "features = pd.read_csv(CFG.input_dir / \"features.csv\")\n",
    "patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n",
    "test = pd.read_csv(CFG.input_dir / \"test.csv\")\n",
    "\n",
    "train.shape, features.shape, patient_notes.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "monetary-geneva",
   "metadata": {
    "id": "vanilla-iceland"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n",
    "    print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-fishing",
   "metadata": {
    "id": "convenient-plant"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "universal-alberta",
   "metadata": {
    "id": "convertible-thunder"
   },
   "outputs": [],
   "source": [
    "def preprocess_features(features):\n",
    "    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "\n",
    "\n",
    "features = preprocess_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "hazardous-soundtrack",
   "metadata": {
    "id": "a7YBS_idYKtL"
   },
   "outputs": [],
   "source": [
    "features['feature_text'] = features['feature_text'].str.lower()\n",
    "patient_notes['pn_history'] = patient_notes['pn_history'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "other-sound",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "charitable-memphis",
    "outputId": "fca9323a-7ebb-469c-e494-f3ea3aa5db24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 8), (5, 6))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "searching-knife",
   "metadata": {
    "id": "governing-election"
   },
   "outputs": [],
   "source": [
    "train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n",
    "train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "theoretical-insider",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "negative-provincial",
    "outputId": "a14cf34d-27c0-41c1-a97f-a02cb70b1482"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4399\n",
       "1    8181\n",
       "2    1296\n",
       "3     287\n",
       "4      99\n",
       "5      27\n",
       "6       9\n",
       "7       1\n",
       "8       1\n",
       "Name: annotation_length, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n",
    "display(train['annotation_length'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-crowd",
   "metadata": {
    "id": "arbitrary-beatles"
   },
   "source": [
    "## CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "promising-alberta",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "important-murray",
    "outputId": "0f77c675-4cc4-4be5-9d99-346e40c7135a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    3575\n",
       "1    3575\n",
       "2    3575\n",
       "3    3575\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Fold = GroupKFold(n_splits=CFG.n_fold)\n",
    "groups = train['pn_num'].values\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-rebecca",
   "metadata": {
    "id": "configured-chemistry"
   },
   "source": [
    "## Setup tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "short-dodge",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hindu-contest",
    "outputId": "93f3e77c-2a10-4a0e-8e67-65805e82e164"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "if CFG.submission:\n",
    "    tokenizer = DebertaV2TokenizerFast.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n",
    "else:\n",
    "    tokenizer = DebertaV2TokenizerFast.from_pretrained(CFG.pretrained_model_name)\n",
    "    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n",
    "\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-knock",
   "metadata": {
    "id": "alleged-protein"
   },
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "featured-affect",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "18b47303dd5b48bfb118053a80c44a40",
      "971945a52a6d4f06ba35e5363d264037",
      "2d7cfbb1d1a54c0590e59de0b280ab22",
      "054630edadaa453fb86d66a20e49030f",
      "8fce27d68c1c41ec9a3c58d439c2a5e7",
      "e9a3708f7e4c486da3a09e066b6936fb",
      "cd1144e40332427fa3e8d5bc7f57b924",
      "28c710503f154bdea731991801a85a47",
      "8c57bf78a80247db9521bd5b163502ef",
      "747b8a73ce544c569f4d063fc9d6d18a",
      "c7aa62f5eaca401dbbfbfd5f843d6a59"
     ]
    },
    "id": "composed-stroke",
    "outputId": "71ab3663-99ff-4577-ac6f-28425ca4c488"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9313afd3784abcae61e50c9f84796c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 284\n"
     ]
    }
   ],
   "source": [
    "pn_history_lengths = []\n",
    "tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    pn_history_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(pn_history_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "detailed-philadelphia",
   "metadata": {
    "id": "emotional-region"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551aaea97e814d84893c3f9daff7093b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 28\n"
     ]
    }
   ],
   "source": [
    "feature_text_lengths = []\n",
    "tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    feature_text_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(feature_text_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "scientific-cisco",
   "metadata": {
    "id": "wrong-leisure"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 315\n"
     ]
    }
   ],
   "source": [
    "CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n",
    "\n",
    "print(\"max length:\", CFG.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "sound-storage",
   "metadata": {
    "id": "convenient-gospel"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36235d5241440f2ba7dc9231fc990d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 950\n"
     ]
    }
   ],
   "source": [
    "pn_history_lengths = []\n",
    "tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n",
    "for text in tk0:\n",
    "    length = len(text)\n",
    "    pn_history_lengths.append(length)\n",
    "\n",
    "CFG.max_char_len = max(pn_history_lengths)\n",
    "\n",
    "print(\"max length:\", CFG.max_char_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fixed-gather",
   "metadata": {
    "id": "representative-contributor"
   },
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, cfg, df, pseudo_label=None):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.max_char_len = self.cfg.max_char_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "        self.annotation_lengths = self.df[\"annotation_length\"].values\n",
    "        self.locations = self.df[\"location\"].values\n",
    "        if \"pseudo_idx\" in df.columns:\n",
    "            self.pseudo_idx = self.df[\"pseudo_idx\"].values\n",
    "            self.pseudo_label = pseudo_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def _create_mapping_from_token_to_char(self, pn_history):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        mapping_from_token_to_char = np.zeros(self.max_char_len)\n",
    "        offset_mapping = encoded[\"offset_mapping\"]\n",
    "        for i, offset in enumerate(offset_mapping):\n",
    "            start_idx, end_idx = offset\n",
    "            mapping_from_token_to_char[start_idx:end_idx] = i\n",
    "        return torch.tensor(mapping_from_token_to_char, dtype=torch.long)\n",
    "\n",
    "    def _create_label(self, pn_history, annotation_length, location_list):\n",
    "        label = np.zeros(self.max_char_len)\n",
    "        label[len(pn_history):] = -1\n",
    "        if annotation_length > 0:\n",
    "            for location in location_list:\n",
    "                for loc in [s.split() for s in location.split(\";\")]:\n",
    "                    start, end = int(loc[0]), int(loc[1])\n",
    "                    label[start:end] = 1\n",
    "        return torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        if not np.isnan(self.annotation_lengths[idx]):\n",
    "            label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n",
    "        else:\n",
    "            p_idx = int(self.pseudo_idx[idx])\n",
    "            label = torch.tensor(self.pseudo_label[p_idx], dtype=torch.float)\n",
    "        mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n",
    "        return input_, label, mapping_from_token_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "interesting-crown",
   "metadata": {
    "id": "decent-johnson"
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.max_char_len = self.cfg.max_char_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def _create_mapping_from_token_to_char(self, pn_history):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        mapping_from_token_to_char = np.zeros(self.max_char_len)\n",
    "        offset_mapping = encoded[\"offset_mapping\"]\n",
    "        for i, offset in enumerate(offset_mapping):\n",
    "            start_idx, end_idx = offset\n",
    "            mapping_from_token_to_char[start_idx:end_idx] = i\n",
    "        return torch.tensor(mapping_from_token_to_char, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n",
    "        return input_, mapping_from_token_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-syntax",
   "metadata": {
    "id": "arctic-joint"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "interior-history",
   "metadata": {
    "id": "qTRu8eKOTlcX"
   },
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import MaskedLMOutput\n",
    "\n",
    "class MaskedModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(\n",
    "                cfg.pretrained_model_name,\n",
    "                output_hidden_states=False\n",
    "                )\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        \n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.pretrained_model_name, config=self.config)\n",
    "            self.lm_head = AutoModelForMaskedLM.from_pretrained(cfg.pretrained_model_name, config=self.config).cls # [cls, lm_head]\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "            self.lm_head = AutoModelForMaskedLM(self.config).cls # [cls, lm_head]\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            #position_ids=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None):\n",
    "        \n",
    "        outputs = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            #position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,)\n",
    "        \n",
    "        sequence_output = outputs[0]\n",
    "        prediction_scores = self.lm_head(sequence_output)\n",
    "\n",
    "        masked_lm_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        return MaskedLMOutput(loss=masked_lm_loss,\n",
    "                              logits=prediction_scores,\n",
    "                              hidden_states=outputs.hidden_states,\n",
    "                              attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "waiting-balance",
   "metadata": {
    "id": "OJt_cHeyTmDS"
   },
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, model_config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if model_config_path is None:\n",
    "            self.model_config = AutoConfig.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "        else:\n",
    "            self.model_config = torch.load(model_config_path)\n",
    "\n",
    "        if pretrained:\n",
    "            self.backbone = AutoModel.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                config=self.model_config,\n",
    "            )\n",
    "            print(f\"Load weight from pretrained\")\n",
    "        else:\n",
    "            #self.backbone = AutoModel.from_config(self.model_config)\n",
    "            # itpt = AutoModelForMaskedLM.from_config(self.model_config)\n",
    "            #path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp010/checkpoint-130170/pytorch_model.bin\")\n",
    "            # path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n",
    "            # state_dict = torch.load(path)\n",
    "            # itpt.load_state_dict(state_dict)\n",
    "            #path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\")\n",
    "            path = str(Path(\"../output\") / CFG.competition_name /  \"nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\")\n",
    "            masked_model = MaskedModel(CFG, config_path=None, pretrained=True)\n",
    "            state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "            masked_model.load_state_dict(state)\n",
    "            self.backbone = masked_model.model\n",
    "            print(f\"Load weight from {path}\")\n",
    "            del state, masked_model; gc.collect()\n",
    "\n",
    "        self.lstm = nn.GRU(\n",
    "            input_size=self.model_config.hidden_size,\n",
    "            bidirectional=True,\n",
    "            hidden_size=self.model_config.hidden_size // 2,\n",
    "            num_layers=4,\n",
    "            dropout=self.cfg.dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(self.cfg.dropout),\n",
    "            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, mappings_from_token_to_char):\n",
    "        h = self.backbone(**inputs)[\"last_hidden_state\"]  # [batch, seq_len, d_model]\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.unsqueeze(2).expand(-1, -1, self.model_config.hidden_size)\n",
    "        h = torch.gather(h, 1, mappings_from_token_to_char)    # [batch, seq_len, d_model]\n",
    "        h, _ = self.lstm(h)\n",
    "        output = self.fc(h)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-genealogy",
   "metadata": {
    "id": "therapeutic-assembly"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "supposed-bernard",
   "metadata": {
    "id": "going-conversion"
   },
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    train_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    epoch,\n",
    "    scheduler,\n",
    "    device,\n",
    "):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels, mappings_from_token_to_char) in enumerate(train_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device) \n",
    "        batch_size = labels.size(0)\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            output = model(inputs, mappings_from_token_to_char)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if CFG.batch_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n",
    "            print(\n",
    "                \"Epoch: [{0}][{1}/{2}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                \"Grad: {grad_norm:.4f}  \"\n",
    "                \"LR: {lr:.6f}  \"\n",
    "                .format(\n",
    "                    epoch+1,\n",
    "                    step,\n",
    "                    len(train_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n",
    "                    loss=losses,\n",
    "                     grad_norm=grad_norm,\n",
    "                     lr=scheduler.get_lr()[0],\n",
    "                )\n",
    "            )\n",
    "    del output, loss, inputs, labels, mappings_from_token_to_char, scaler, grad_norm; gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "immediate-break",
   "metadata": {
    "id": "alleged-commonwealth"
   },
   "outputs": [],
   "source": [
    "def valid_fn(\n",
    "    val_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    device,\n",
    "):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels, mappings_from_token_to_char) in enumerate(val_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device) \n",
    "        batch_size = labels.size(0)\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            output = model(inputs, mappings_from_token_to_char)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "        loss = loss.mean()\n",
    "    \n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n",
    "            print(\n",
    "                \"EVAL: [{0}/{1}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                .format(\n",
    "                    step, len(val_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n",
    "                    loss=losses,\n",
    "                )\n",
    "            )\n",
    "    preds = np.concatenate(preds)\n",
    "    return losses.avg, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "oriented-arizona",
   "metadata": {
    "id": "middle-determination"
   },
   "outputs": [],
   "source": [
    "def inference_fn(test_dataloader, model, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preds = []\n",
    "    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n",
    "    for (inputs, mappings_from_token_to_char) in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs, mappings_from_token_to_char)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "polyphonic-astrology",
   "metadata": {
    "id": "familiar-participation"
   },
   "outputs": [],
   "source": [
    "def train_loop(df, i_fold, device):\n",
    "    print(f\"========== fold: {i_fold} training ==========\")\n",
    "    train_idx = df[df[\"fold\"] != i_fold].index\n",
    "    val_idx = df[df[\"fold\"] == i_fold].index\n",
    "\n",
    "    train_folds = df.loc[train_idx].reset_index(drop=True)\n",
    "    val_folds = df.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    if CFG.pseudo_plain_path is not None:\n",
    "        pseudo_plain = pd.read_pickle(CFG.pseudo_plain_path)\n",
    "        print(f\"get pseudo plain from {CFG.pseudo_plain_path}\")\n",
    "        pseudo_label_list = []\n",
    "        weights = [0.4433659049657008, 0.20859987143371844, 0.3480342236005807]\n",
    "        for exp_name in [\"nbme-exp060\", \"nbme-exp067\", \"nbme-exp083\"]:\n",
    "            #pseudo_label_path = f'./drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/{exp_name}/pseudo_labels_{i_fold}.npy'\n",
    "            pseudo_label_path = f'../output/nbme-score-clinical-patient-notes/{exp_name}/pseudo_labels_{i_fold}.npy'\n",
    "            pseudo_label = np.load(pseudo_label_path)\n",
    "            print(f\"get pseudo labels from {pseudo_label_path}\")\n",
    "            pseudo_label_list.append(pseudo_label)\n",
    "\n",
    "        pseudo_label = weights[0] * pseudo_label_list[0] + weights[1] * pseudo_label_list[1] + weights[2] * pseudo_label_list[2]\n",
    "        pseudo_label = trunc_pred(pseudo_plain[\"pn_history\"].values, pseudo_label)\n",
    "        predicted_location_str = get_predicted_location_str(pseudo_label, th=0.5)\n",
    "        preds = get_predictions(predicted_location_str)\n",
    "        results_postprocess = postprocess(pseudo_plain[\"pn_history\"].values, preds)\n",
    "        #results_postprocess = get_results_from_preds_list(results_postprocess)\n",
    "        pseudo_label = get_preds_from_results(results_postprocess, pseudo_plain[\"pn_history\"].values, pseudo_label.shape[1])\n",
    "        print(pseudo_plain.shape, pseudo_label.shape)\n",
    "\n",
    "        pseudo_plain['feature_text'] = pseudo_plain['feature_text'].str.lower()\n",
    "        pseudo_plain['pn_history'] = pseudo_plain['pn_history'].str.lower()\n",
    "\n",
    "        pseudo_plain[\"pseudo_idx\"] = np.arange(len(pseudo_plain))\n",
    "        pseudo_plain = pseudo_plain.sample(n=CFG.n_pseudo_labels)\n",
    "        print(pseudo_plain.shape)\n",
    "        train_folds = pd.concat([train_folds, pseudo_plain], axis=0, ignore_index=True)\n",
    "        print(train_folds.shape)\n",
    "\n",
    "    train_dataset = TrainingDataset(CFG, train_folds, pseudo_label)\n",
    "    val_dataset = TrainingDataset(CFG, val_folds)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "    model = CustomModel(CFG, model_config_path=None, pretrained=False)   # itptを使うため\n",
    "    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n",
    "    model.to(device)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in param_optimizer if not any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n",
    "        {\"params\": [p for n, p in param_optimizer if any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=CFG.lr,\n",
    "        betas=CFG.betas,\n",
    "        weight_decay=CFG.weight_decay,\n",
    "    )\n",
    "    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n",
    "    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_train_optimization_steps,\n",
    "    )\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    best_score = -1 * np.inf\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "        avg_loss = train_fn(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            scheduler,\n",
    "            device,\n",
    "        )\n",
    "        avg_val_loss, val_preds = valid_fn(\n",
    "            val_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "\n",
    "        # scoring\n",
    "        val_folds[[str(i) for i in range(CFG.max_char_len)]] = val_preds\n",
    "        score = scoring(val_folds, th=0.5, use_token_prob=False)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n",
    "        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"predictions\": val_preds,\n",
    "                },\n",
    "                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "            )\n",
    "\n",
    "    predictions = torch.load(\n",
    "        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )[\"predictions\"]\n",
    "    val_folds[[str(i) for i in range(CFG.max_char_len)]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return val_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-globe",
   "metadata": {
    "id": "coated-cameroon"
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "recreational-association",
   "metadata": {
    "id": "quality-expansion"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for i_fold in range(CFG.n_fold):\n",
    "            if i_fold in CFG.train_fold:\n",
    "                _oof_df = train_loop(train, i_fold, device)\n",
    "                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n",
    "        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    if CFG.submission:\n",
    "        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n",
    "    else:\n",
    "        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    best_thres = 0.5\n",
    "    best_score = 0.\n",
    "    for th in np.arange(0.45, 0.55, 0.01):\n",
    "        th = np.round(th, 2)\n",
    "        score = scoring(oof_df, th=th, use_token_prob=False)\n",
    "        if best_score < score:\n",
    "            best_thres = th\n",
    "            best_score = score\n",
    "    print(f\"best_thres: {best_thres}  score: {best_score:.5f}\")\n",
    "\n",
    "    if CFG.inference:\n",
    "        test_dataset = TestDataset(CFG, test)\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=CFG.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=CFG.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        predictions = []\n",
    "        for i_fold in CFG.train_fold:\n",
    "            if CFG.submission:\n",
    "                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n",
    "                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n",
    "            else:\n",
    "                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n",
    "\n",
    "            state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "            model.load_state_dict(state[\"model\"])\n",
    "            print(f\"load weights from {path}\")\n",
    "            test_char_probs = inference_fn(test_dataloader, model, device)\n",
    "            predictions.append(test_char_probs)\n",
    "\n",
    "            del state, test_char_probs, model; gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        predictions = np.mean(predictions, axis=0)\n",
    "        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n",
    "        test[CFG.target_col] = predicted_location_str\n",
    "        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n",
    "        test[[CFG.id_col, CFG.target_col]].to_csv(\n",
    "            CFG.output_dir / \"submission.csv\", index=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "committed-express",
   "metadata": {
    "id": "proprietary-civilian"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "get pseudo plain from ../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_0.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_0.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp083/pseudo_labels_0.npy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61997e803f414a33b150b58b53c1b397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce80697a5d3433c956dc73dc1dc5164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(612602, 950)\n",
      "(612602, 6) (612602, 950)\n",
      "(100000, 7)\n",
      "(110725, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\n",
      "Epoch: [1][0/36908] Elapsed 0m 1s (remain 939m 46s) Loss: 0.3478(0.3478) Grad: 133221.4844  LR: 0.000000  \n",
      "Epoch: [1][100/36908] Elapsed 1m 16s (remain 466m 40s) Loss: 0.3140(0.3351) Grad: 129115.7891  LR: 0.000001  \n",
      "Epoch: [1][200/36908] Elapsed 2m 32s (remain 464m 30s) Loss: 0.2282(0.3055) Grad: 103151.2578  LR: 0.000001  \n",
      "Epoch: [1][300/36908] Elapsed 3m 49s (remain 466m 11s) Loss: 0.1321(0.2622) Grad: 58362.4453  LR: 0.000002  \n",
      "Epoch: [1][400/36908] Elapsed 5m 4s (remain 462m 26s) Loss: 0.0445(0.2177) Grad: 21083.4688  LR: 0.000002  \n",
      "Epoch: [1][500/36908] Elapsed 6m 22s (remain 463m 41s) Loss: 0.0660(0.1838) Grad: 7377.8286  LR: 0.000003  \n",
      "Epoch: [1][600/36908] Elapsed 7m 37s (remain 460m 57s) Loss: 0.0679(0.1603) Grad: 8217.0537  LR: 0.000003  \n",
      "Epoch: [1][700/36908] Elapsed 8m 54s (remain 460m 23s) Loss: 0.0278(0.1428) Grad: 3409.7007  LR: 0.000004  \n",
      "Epoch: [1][800/36908] Elapsed 10m 13s (remain 461m 2s) Loss: 0.0400(0.1298) Grad: 3327.5098  LR: 0.000004  \n",
      "Epoch: [1][900/36908] Elapsed 11m 29s (remain 459m 26s) Loss: 0.0573(0.1198) Grad: 10283.5625  LR: 0.000005  \n",
      "Epoch: [1][1000/36908] Elapsed 12m 45s (remain 457m 48s) Loss: 0.0505(0.1120) Grad: 11763.9736  LR: 0.000005  \n",
      "Epoch: [1][1100/36908] Elapsed 14m 2s (remain 456m 28s) Loss: 0.0459(0.1053) Grad: 21883.6367  LR: 0.000006  \n",
      "Epoch: [1][1200/36908] Elapsed 15m 17s (remain 454m 51s) Loss: 0.0175(0.0991) Grad: 41070.5117  LR: 0.000007  \n",
      "Epoch: [1][1300/36908] Elapsed 16m 32s (remain 452m 53s) Loss: 0.0050(0.0932) Grad: 8480.1250  LR: 0.000007  \n",
      "Epoch: [1][1400/36908] Elapsed 17m 50s (remain 452m 7s) Loss: 0.0170(0.0877) Grad: 35309.1250  LR: 0.000008  \n",
      "Epoch: [1][1500/36908] Elapsed 19m 5s (remain 450m 23s) Loss: 0.0113(0.0833) Grad: 38556.6406  LR: 0.000008  \n",
      "Epoch: [1][1600/36908] Elapsed 20m 20s (remain 448m 34s) Loss: 0.0062(0.0792) Grad: 21815.0078  LR: 0.000009  \n",
      "Epoch: [1][1700/36908] Elapsed 21m 37s (remain 447m 30s) Loss: 0.0141(0.0754) Grad: 38851.5078  LR: 0.000009  \n",
      "Epoch: [1][1800/36908] Elapsed 22m 52s (remain 445m 58s) Loss: 0.0249(0.0721) Grad: 68221.1953  LR: 0.000010  \n",
      "Epoch: [1][1900/36908] Elapsed 24m 9s (remain 444m 43s) Loss: 0.0145(0.0690) Grad: 47027.3633  LR: 0.000010  \n",
      "Epoch: [1][2000/36908] Elapsed 25m 26s (remain 443m 42s) Loss: 0.0012(0.0662) Grad: 7873.7881  LR: 0.000011  \n",
      "Epoch: [1][2100/36908] Elapsed 26m 42s (remain 442m 30s) Loss: 0.0053(0.0636) Grad: 25190.5254  LR: 0.000011  \n",
      "Epoch: [1][2200/36908] Elapsed 27m 58s (remain 441m 12s) Loss: 0.0165(0.0612) Grad: 91853.7266  LR: 0.000012  \n",
      "Epoch: [1][2300/36908] Elapsed 29m 14s (remain 439m 53s) Loss: 0.0028(0.0591) Grad: 7625.6851  LR: 0.000012  \n",
      "Epoch: [1][2400/36908] Elapsed 30m 32s (remain 439m 2s) Loss: 0.0002(0.0572) Grad: 2769.5103  LR: 0.000013  \n",
      "Epoch: [1][2500/36908] Elapsed 31m 51s (remain 438m 13s) Loss: 0.0062(0.0553) Grad: 57224.0938  LR: 0.000014  \n",
      "Epoch: [1][2600/36908] Elapsed 33m 7s (remain 436m 56s) Loss: 0.0194(0.0536) Grad: 80376.8047  LR: 0.000014  \n",
      "Epoch: [1][2700/36908] Elapsed 34m 23s (remain 435m 35s) Loss: 0.0040(0.0520) Grad: 26123.8789  LR: 0.000015  \n",
      "Epoch: [1][2800/36908] Elapsed 35m 40s (remain 434m 20s) Loss: 0.0028(0.0505) Grad: 35721.2422  LR: 0.000015  \n",
      "Epoch: [1][2900/36908] Elapsed 36m 57s (remain 433m 18s) Loss: 0.0128(0.0492) Grad: 41662.2344  LR: 0.000016  \n",
      "Epoch: [1][3000/36908] Elapsed 38m 16s (remain 432m 25s) Loss: 0.0011(0.0478) Grad: 1535.4105  LR: 0.000016  \n",
      "Epoch: [1][3100/36908] Elapsed 39m 37s (remain 432m 3s) Loss: 0.0066(0.0466) Grad: 65111.9609  LR: 0.000017  \n",
      "Epoch: [1][3200/36908] Elapsed 40m 54s (remain 430m 48s) Loss: 0.0000(0.0455) Grad: 132.6875  LR: 0.000017  \n",
      "Epoch: [1][3300/36908] Elapsed 42m 11s (remain 429m 35s) Loss: 0.0021(0.0444) Grad: 9809.0488  LR: 0.000018  \n",
      "Epoch: [1][3400/36908] Elapsed 43m 29s (remain 428m 27s) Loss: 0.0040(0.0435) Grad: 8028.2246  LR: 0.000018  \n",
      "Epoch: [1][3500/36908] Elapsed 44m 46s (remain 427m 14s) Loss: 0.0007(0.0426) Grad: 760.9052  LR: 0.000019  \n",
      "Epoch: [1][3600/36908] Elapsed 46m 3s (remain 426m 0s) Loss: 0.0122(0.0416) Grad: 7085.4482  LR: 0.000020  \n",
      "Epoch: [1][3700/36908] Elapsed 47m 21s (remain 424m 52s) Loss: 0.0116(0.0406) Grad: 40542.4180  LR: 0.000020  \n",
      "Epoch: [1][3800/36908] Elapsed 48m 40s (remain 423m 54s) Loss: 0.0022(0.0398) Grad: 1636.3058  LR: 0.000020  \n",
      "Epoch: [1][3900/36908] Elapsed 49m 57s (remain 422m 38s) Loss: 0.0106(0.0391) Grad: 62749.2617  LR: 0.000020  \n",
      "Epoch: [1][4000/36908] Elapsed 51m 14s (remain 421m 26s) Loss: 0.0113(0.0383) Grad: 75213.1953  LR: 0.000020  \n",
      "Epoch: [1][4100/36908] Elapsed 52m 33s (remain 420m 27s) Loss: 0.0005(0.0376) Grad: 2126.4683  LR: 0.000020  \n",
      "Epoch: [1][4200/36908] Elapsed 53m 51s (remain 419m 21s) Loss: 0.0001(0.0369) Grad: 180.8231  LR: 0.000020  \n",
      "Epoch: [1][4300/36908] Elapsed 55m 10s (remain 418m 14s) Loss: 0.0042(0.0362) Grad: 71210.0859  LR: 0.000020  \n",
      "Epoch: [1][4400/36908] Elapsed 56m 26s (remain 416m 56s) Loss: 0.0415(0.0355) Grad: 664553.9375  LR: 0.000020  \n",
      "Epoch: [1][4500/36908] Elapsed 57m 48s (remain 416m 12s) Loss: 0.0000(0.0350) Grad: 66.9732  LR: 0.000020  \n",
      "Epoch: [1][4600/36908] Elapsed 59m 6s (remain 414m 59s) Loss: 0.0003(0.0344) Grad: 374.5260  LR: 0.000019  \n",
      "Epoch: [1][4700/36908] Elapsed 60m 21s (remain 413m 33s) Loss: 0.0227(0.0338) Grad: 163056.2188  LR: 0.000019  \n",
      "Epoch: [1][4800/36908] Elapsed 61m 40s (remain 412m 30s) Loss: 0.0002(0.0333) Grad: 280.1387  LR: 0.000019  \n",
      "Epoch: [1][4900/36908] Elapsed 62m 57s (remain 411m 10s) Loss: 0.0014(0.0328) Grad: 4085.8535  LR: 0.000019  \n",
      "Epoch: [1][5000/36908] Elapsed 64m 15s (remain 409m 56s) Loss: 0.0000(0.0323) Grad: 49.0743  LR: 0.000019  \n",
      "Epoch: [1][5100/36908] Elapsed 65m 31s (remain 408m 32s) Loss: 0.0026(0.0318) Grad: 39088.2031  LR: 0.000019  \n",
      "Epoch: [1][5200/36908] Elapsed 66m 47s (remain 407m 13s) Loss: 0.0002(0.0313) Grad: 110.9908  LR: 0.000019  \n",
      "Epoch: [1][5300/36908] Elapsed 68m 6s (remain 406m 8s) Loss: 0.0006(0.0308) Grad: 1580.3903  LR: 0.000019  \n",
      "Epoch: [1][5400/36908] Elapsed 69m 22s (remain 404m 43s) Loss: 0.0017(0.0304) Grad: 8712.1729  LR: 0.000019  \n",
      "Epoch: [1][5500/36908] Elapsed 70m 38s (remain 403m 20s) Loss: 0.0002(0.0300) Grad: 12779.2070  LR: 0.000019  \n",
      "Epoch: [1][5600/36908] Elapsed 71m 55s (remain 401m 59s) Loss: 0.0020(0.0295) Grad: 12501.1816  LR: 0.000019  \n",
      "Epoch: [1][5700/36908] Elapsed 73m 17s (remain 401m 9s) Loss: 0.0015(0.0292) Grad: 12771.2441  LR: 0.000019  \n",
      "Epoch: [1][5800/36908] Elapsed 74m 35s (remain 399m 58s) Loss: 0.0131(0.0287) Grad: 53504.8984  LR: 0.000019  \n",
      "Epoch: [1][5900/36908] Elapsed 75m 51s (remain 398m 38s) Loss: 0.0001(0.0283) Grad: 79.3407  LR: 0.000019  \n",
      "Epoch: [1][6000/36908] Elapsed 77m 9s (remain 397m 25s) Loss: 0.0001(0.0280) Grad: 126.8491  LR: 0.000019  \n",
      "Epoch: [1][6100/36908] Elapsed 78m 25s (remain 396m 0s) Loss: 0.0002(0.0277) Grad: 446.6558  LR: 0.000019  \n",
      "Epoch: [1][6200/36908] Elapsed 79m 44s (remain 394m 50s) Loss: 0.0012(0.0273) Grad: 9912.0537  LR: 0.000018  \n",
      "Epoch: [1][6300/36908] Elapsed 81m 1s (remain 393m 32s) Loss: 0.0065(0.0270) Grad: 46158.5195  LR: 0.000018  \n",
      "Epoch: [1][6400/36908] Elapsed 82m 17s (remain 392m 13s) Loss: 0.0014(0.0267) Grad: 14580.3691  LR: 0.000018  \n",
      "Epoch: [1][6500/36908] Elapsed 83m 35s (remain 390m 56s) Loss: 0.0001(0.0263) Grad: 64.8078  LR: 0.000018  \n",
      "Epoch: [1][6600/36908] Elapsed 84m 57s (remain 390m 5s) Loss: 0.0003(0.0261) Grad: 283.0856  LR: 0.000018  \n",
      "Epoch: [1][6700/36908] Elapsed 86m 14s (remain 388m 43s) Loss: 0.0001(0.0258) Grad: 82.4569  LR: 0.000018  \n",
      "Epoch: [1][6800/36908] Elapsed 87m 29s (remain 387m 19s) Loss: 0.0001(0.0255) Grad: 265.9905  LR: 0.000018  \n",
      "Epoch: [1][6900/36908] Elapsed 88m 49s (remain 386m 14s) Loss: 0.0013(0.0252) Grad: 50866.5508  LR: 0.000018  \n",
      "Epoch: [1][7000/36908] Elapsed 90m 6s (remain 384m 53s) Loss: 0.0026(0.0249) Grad: 13588.8389  LR: 0.000018  \n",
      "Epoch: [1][7100/36908] Elapsed 91m 23s (remain 383m 38s) Loss: 0.0006(0.0246) Grad: 1032.0343  LR: 0.000018  \n",
      "Epoch: [1][7200/36908] Elapsed 92m 40s (remain 382m 19s) Loss: 0.0107(0.0244) Grad: 22284.3652  LR: 0.000018  \n",
      "Epoch: [1][7300/36908] Elapsed 93m 56s (remain 380m 56s) Loss: 0.0032(0.0241) Grad: 13557.1045  LR: 0.000018  \n",
      "Epoch: [1][7400/36908] Elapsed 95m 12s (remain 379m 35s) Loss: 0.0181(0.0239) Grad: 29018.0508  LR: 0.000018  \n",
      "Epoch: [1][7500/36908] Elapsed 96m 29s (remain 378m 16s) Loss: 0.0020(0.0236) Grad: 18645.0039  LR: 0.000018  \n",
      "Epoch: [1][7600/36908] Elapsed 97m 44s (remain 376m 53s) Loss: 0.0149(0.0234) Grad: 197102.3125  LR: 0.000018  \n",
      "Epoch: [1][7700/36908] Elapsed 99m 1s (remain 375m 33s) Loss: 0.0069(0.0231) Grad: 25721.3184  LR: 0.000018  \n",
      "Epoch: [1][7800/36908] Elapsed 100m 20s (remain 374m 24s) Loss: 0.0001(0.0229) Grad: 142.1753  LR: 0.000018  \n",
      "Epoch: [1][7900/36908] Elapsed 101m 37s (remain 373m 6s) Loss: 0.0148(0.0227) Grad: 13517.3281  LR: 0.000017  \n",
      "Epoch: [1][8000/36908] Elapsed 102m 57s (remain 371m 58s) Loss: 0.0055(0.0225) Grad: 82951.8516  LR: 0.000017  \n",
      "Epoch: [1][8100/36908] Elapsed 104m 14s (remain 370m 40s) Loss: 0.0084(0.0222) Grad: 93876.6719  LR: 0.000017  \n",
      "Epoch: [1][8200/36908] Elapsed 105m 31s (remain 369m 22s) Loss: 0.0001(0.0220) Grad: 130.1609  LR: 0.000017  \n",
      "Epoch: [1][8300/36908] Elapsed 106m 48s (remain 368m 3s) Loss: 0.0005(0.0218) Grad: 2953.5859  LR: 0.000017  \n",
      "Epoch: [1][8400/36908] Elapsed 108m 11s (remain 367m 5s) Loss: 0.0046(0.0217) Grad: 17253.6836  LR: 0.000017  \n",
      "Epoch: [1][8500/36908] Elapsed 109m 28s (remain 365m 50s) Loss: 0.0027(0.0215) Grad: 30196.1719  LR: 0.000017  \n",
      "Epoch: [1][8600/36908] Elapsed 110m 45s (remain 364m 31s) Loss: 0.0000(0.0213) Grad: 43.6734  LR: 0.000017  \n",
      "Epoch: [1][8700/36908] Elapsed 112m 4s (remain 363m 20s) Loss: 0.0000(0.0211) Grad: 57.7761  LR: 0.000017  \n",
      "Epoch: [1][8800/36908] Elapsed 113m 22s (remain 362m 4s) Loss: 0.0002(0.0209) Grad: 1328.5134  LR: 0.000017  \n",
      "Epoch: [1][8900/36908] Elapsed 114m 38s (remain 360m 42s) Loss: 0.0024(0.0207) Grad: 17914.2812  LR: 0.000017  \n",
      "Epoch: [1][9000/36908] Elapsed 115m 55s (remain 359m 25s) Loss: 0.0000(0.0205) Grad: 124.3578  LR: 0.000017  \n",
      "Epoch: [1][9100/36908] Elapsed 117m 14s (remain 358m 12s) Loss: 0.0000(0.0203) Grad: 30.6402  LR: 0.000017  \n",
      "Epoch: [1][9200/36908] Elapsed 118m 29s (remain 356m 50s) Loss: 0.0008(0.0202) Grad: 41731.4180  LR: 0.000017  \n",
      "Epoch: [1][9300/36908] Elapsed 119m 47s (remain 355m 32s) Loss: 0.0021(0.0200) Grad: 71391.2266  LR: 0.000017  \n",
      "Epoch: [1][9400/36908] Elapsed 121m 4s (remain 354m 14s) Loss: 0.0157(0.0198) Grad: 108542.3203  LR: 0.000017  \n",
      "Epoch: [1][9500/36908] Elapsed 122m 20s (remain 352m 53s) Loss: 0.0000(0.0197) Grad: 35.6017  LR: 0.000017  \n",
      "Epoch: [1][9600/36908] Elapsed 123m 38s (remain 351m 40s) Loss: 0.0069(0.0195) Grad: 46678.9141  LR: 0.000016  \n",
      "Epoch: [1][9700/36908] Elapsed 124m 55s (remain 350m 21s) Loss: 0.0000(0.0194) Grad: 73.6081  LR: 0.000016  \n",
      "Epoch: [1][9800/36908] Elapsed 126m 10s (remain 348m 58s) Loss: 0.0000(0.0192) Grad: 14.9387  LR: 0.000016  \n",
      "Epoch: [1][9900/36908] Elapsed 127m 27s (remain 347m 38s) Loss: 0.0025(0.0191) Grad: 47889.0391  LR: 0.000016  \n",
      "Epoch: [1][10000/36908] Elapsed 128m 45s (remain 346m 25s) Loss: 0.0000(0.0189) Grad: 60.9318  LR: 0.000016  \n",
      "Epoch: [1][10100/36908] Elapsed 130m 3s (remain 345m 10s) Loss: 0.0034(0.0187) Grad: 14737.6631  LR: 0.000016  \n",
      "Epoch: [1][10200/36908] Elapsed 131m 19s (remain 343m 49s) Loss: 0.0080(0.0186) Grad: 93917.8906  LR: 0.000016  \n",
      "Epoch: [1][10300/36908] Elapsed 132m 35s (remain 342m 27s) Loss: 0.0000(0.0185) Grad: 46.0477  LR: 0.000016  \n",
      "Epoch: [1][10400/36908] Elapsed 133m 51s (remain 341m 7s) Loss: 0.0013(0.0183) Grad: 25869.7637  LR: 0.000016  \n",
      "Epoch: [1][10500/36908] Elapsed 135m 7s (remain 339m 47s) Loss: 0.0010(0.0182) Grad: 5290.2700  LR: 0.000016  \n",
      "Epoch: [1][10600/36908] Elapsed 136m 22s (remain 338m 24s) Loss: 0.0001(0.0181) Grad: 464.4157  LR: 0.000016  \n",
      "Epoch: [1][10700/36908] Elapsed 137m 38s (remain 337m 4s) Loss: 0.0026(0.0180) Grad: 36156.2227  LR: 0.000016  \n",
      "Epoch: [1][10800/36908] Elapsed 138m 54s (remain 335m 45s) Loss: 0.0057(0.0178) Grad: 83708.2969  LR: 0.000016  \n",
      "Epoch: [1][10900/36908] Elapsed 140m 10s (remain 334m 24s) Loss: 0.0000(0.0177) Grad: 24.6432  LR: 0.000016  \n",
      "Epoch: [1][11000/36908] Elapsed 141m 24s (remain 333m 1s) Loss: 0.0011(0.0176) Grad: 45480.8672  LR: 0.000016  \n",
      "Epoch: [1][11100/36908] Elapsed 142m 43s (remain 331m 47s) Loss: 0.0001(0.0174) Grad: 12418.7715  LR: 0.000016  \n",
      "Epoch: [1][11200/36908] Elapsed 144m 2s (remain 330m 35s) Loss: 0.0000(0.0173) Grad: 144.5588  LR: 0.000015  \n",
      "Epoch: [1][11300/36908] Elapsed 145m 18s (remain 329m 15s) Loss: 0.0000(0.0172) Grad: 116.2399  LR: 0.000015  \n",
      "Epoch: [1][11400/36908] Elapsed 146m 33s (remain 327m 53s) Loss: 0.0013(0.0171) Grad: 38647.0352  LR: 0.000015  \n",
      "Epoch: [1][11500/36908] Elapsed 147m 50s (remain 326m 34s) Loss: 0.0001(0.0170) Grad: 1522.3853  LR: 0.000015  \n",
      "Epoch: [1][11600/36908] Elapsed 149m 8s (remain 325m 20s) Loss: 0.0000(0.0168) Grad: 20.5658  LR: 0.000015  \n",
      "Epoch: [1][11700/36908] Elapsed 150m 24s (remain 324m 2s) Loss: 0.0007(0.0167) Grad: 2523.9319  LR: 0.000015  \n",
      "Epoch: [1][11800/36908] Elapsed 151m 42s (remain 322m 45s) Loss: 0.0001(0.0166) Grad: 1162.5861  LR: 0.000015  \n",
      "Epoch: [1][11900/36908] Elapsed 153m 0s (remain 321m 29s) Loss: 0.0016(0.0165) Grad: 17421.2598  LR: 0.000015  \n",
      "Epoch: [1][12000/36908] Elapsed 154m 16s (remain 320m 10s) Loss: 0.0000(0.0164) Grad: 302.2461  LR: 0.000015  \n",
      "Epoch: [1][12100/36908] Elapsed 155m 31s (remain 318m 50s) Loss: 0.0049(0.0163) Grad: 6673.7915  LR: 0.000015  \n",
      "Epoch: [1][12200/36908] Elapsed 156m 48s (remain 317m 32s) Loss: 0.0001(0.0162) Grad: 1824.7556  LR: 0.000015  \n",
      "Epoch: [1][12300/36908] Elapsed 158m 5s (remain 316m 14s) Loss: 0.0001(0.0161) Grad: 5477.8442  LR: 0.000015  \n",
      "Epoch: [1][12400/36908] Elapsed 159m 21s (remain 314m 55s) Loss: 0.0000(0.0160) Grad: 42.9496  LR: 0.000015  \n",
      "Epoch: [1][12500/36908] Elapsed 160m 39s (remain 313m 41s) Loss: 0.0011(0.0159) Grad: 4501.5488  LR: 0.000015  \n",
      "Epoch: [1][12600/36908] Elapsed 161m 56s (remain 312m 22s) Loss: 0.0114(0.0158) Grad: 54788.0664  LR: 0.000015  \n",
      "Epoch: [1][12700/36908] Elapsed 163m 11s (remain 311m 2s) Loss: 0.0001(0.0158) Grad: 2502.6951  LR: 0.000015  \n",
      "Epoch: [1][12800/36908] Elapsed 164m 27s (remain 309m 43s) Loss: 0.0023(0.0157) Grad: 126539.9297  LR: 0.000015  \n",
      "Epoch: [1][12900/36908] Elapsed 165m 43s (remain 308m 23s) Loss: 0.0046(0.0156) Grad: 298523.1562  LR: 0.000014  \n",
      "Epoch: [1][13000/36908] Elapsed 167m 0s (remain 307m 5s) Loss: 0.0002(0.0155) Grad: 2368.9766  LR: 0.000014  \n",
      "Epoch: [1][13100/36908] Elapsed 168m 15s (remain 305m 46s) Loss: 0.0004(0.0154) Grad: 7298.1567  LR: 0.000014  \n",
      "Epoch: [1][13200/36908] Elapsed 169m 31s (remain 304m 26s) Loss: 0.0003(0.0153) Grad: 3417.5715  LR: 0.000014  \n",
      "Epoch: [1][13300/36908] Elapsed 170m 46s (remain 303m 6s) Loss: 0.0102(0.0152) Grad: 231449.2969  LR: 0.000014  \n",
      "Epoch: [1][13400/36908] Elapsed 172m 3s (remain 301m 48s) Loss: 0.0000(0.0151) Grad: 36.4718  LR: 0.000014  \n",
      "Epoch: [1][13500/36908] Elapsed 173m 19s (remain 300m 30s) Loss: 0.0091(0.0151) Grad: 28449.4434  LR: 0.000014  \n",
      "Epoch: [1][13600/36908] Elapsed 174m 35s (remain 299m 11s) Loss: 0.0105(0.0150) Grad: 147739.0000  LR: 0.000014  \n",
      "Epoch: [1][13700/36908] Elapsed 175m 51s (remain 297m 51s) Loss: 0.0000(0.0149) Grad: 17.0447  LR: 0.000014  \n",
      "Epoch: [1][13800/36908] Elapsed 177m 6s (remain 296m 32s) Loss: 0.0000(0.0148) Grad: 367.2260  LR: 0.000014  \n",
      "Epoch: [1][13900/36908] Elapsed 178m 24s (remain 295m 16s) Loss: 0.0027(0.0147) Grad: 18393.2559  LR: 0.000014  \n",
      "Epoch: [1][14000/36908] Elapsed 179m 42s (remain 294m 1s) Loss: 0.0149(0.0147) Grad: 151280.8594  LR: 0.000014  \n",
      "Epoch: [1][14100/36908] Elapsed 180m 59s (remain 292m 43s) Loss: 0.0045(0.0146) Grad: 110880.2969  LR: 0.000014  \n",
      "Epoch: [1][14200/36908] Elapsed 182m 17s (remain 291m 27s) Loss: 0.0031(0.0145) Grad: 48138.2656  LR: 0.000014  \n",
      "Epoch: [1][14300/36908] Elapsed 183m 33s (remain 290m 9s) Loss: 0.0258(0.0144) Grad: 206606.1094  LR: 0.000014  \n",
      "Epoch: [1][14400/36908] Elapsed 184m 48s (remain 288m 50s) Loss: 0.0001(0.0144) Grad: 399.9692  LR: 0.000014  \n",
      "Epoch: [1][14500/36908] Elapsed 186m 4s (remain 287m 30s) Loss: 0.0000(0.0143) Grad: 5.5693  LR: 0.000013  \n",
      "Epoch: [1][14600/36908] Elapsed 187m 20s (remain 286m 13s) Loss: 0.0006(0.0142) Grad: 6387.0332  LR: 0.000013  \n",
      "Epoch: [1][14700/36908] Elapsed 188m 36s (remain 284m 54s) Loss: 0.0041(0.0141) Grad: 22210.6816  LR: 0.000013  \n",
      "Epoch: [1][14800/36908] Elapsed 189m 52s (remain 283m 35s) Loss: 0.0180(0.0141) Grad: 26606.9336  LR: 0.000013  \n",
      "Epoch: [1][14900/36908] Elapsed 191m 9s (remain 282m 19s) Loss: 0.0000(0.0140) Grad: 4.7824  LR: 0.000013  \n",
      "Epoch: [1][15000/36908] Elapsed 192m 26s (remain 281m 2s) Loss: 0.0000(0.0139) Grad: 81.9644  LR: 0.000013  \n",
      "Epoch: [1][15100/36908] Elapsed 193m 42s (remain 279m 43s) Loss: 0.0025(0.0139) Grad: 4482.7310  LR: 0.000013  \n",
      "Epoch: [1][15200/36908] Elapsed 194m 58s (remain 278m 25s) Loss: 0.0000(0.0138) Grad: 47.6577  LR: 0.000013  \n",
      "Epoch: [1][15300/36908] Elapsed 196m 13s (remain 277m 6s) Loss: 0.0018(0.0138) Grad: 10731.9512  LR: 0.000013  \n",
      "Epoch: [1][15400/36908] Elapsed 197m 30s (remain 275m 48s) Loss: 0.0004(0.0137) Grad: 1962.0082  LR: 0.000013  \n",
      "Epoch: [1][15500/36908] Elapsed 198m 46s (remain 274m 30s) Loss: 0.0143(0.0136) Grad: 35216.9297  LR: 0.000013  \n",
      "Epoch: [1][15600/36908] Elapsed 200m 2s (remain 273m 11s) Loss: 0.0000(0.0136) Grad: 35.3894  LR: 0.000013  \n",
      "Epoch: [1][15700/36908] Elapsed 201m 16s (remain 271m 51s) Loss: 0.0000(0.0135) Grad: 608.5057  LR: 0.000013  \n",
      "Epoch: [1][15800/36908] Elapsed 202m 33s (remain 270m 35s) Loss: 0.0109(0.0134) Grad: 119065.4766  LR: 0.000013  \n",
      "Epoch: [1][15900/36908] Elapsed 203m 50s (remain 269m 17s) Loss: 0.0000(0.0134) Grad: 64.2189  LR: 0.000013  \n",
      "Epoch: [1][16000/36908] Elapsed 205m 4s (remain 267m 57s) Loss: 0.0004(0.0133) Grad: 1209.1840  LR: 0.000013  \n",
      "Epoch: [1][16100/36908] Elapsed 206m 21s (remain 266m 39s) Loss: 0.0174(0.0133) Grad: 70090.6562  LR: 0.000013  \n",
      "Epoch: [1][16200/36908] Elapsed 207m 36s (remain 265m 21s) Loss: 0.0000(0.0132) Grad: 22.5657  LR: 0.000012  \n",
      "Epoch: [1][16300/36908] Elapsed 208m 52s (remain 264m 3s) Loss: 0.0000(0.0131) Grad: 20.1840  LR: 0.000012  \n",
      "Epoch: [1][16400/36908] Elapsed 210m 8s (remain 262m 44s) Loss: 0.0000(0.0131) Grad: 59.6775  LR: 0.000012  \n",
      "Epoch: [1][16500/36908] Elapsed 211m 25s (remain 261m 28s) Loss: 0.0126(0.0130) Grad: 22662.6055  LR: 0.000012  \n",
      "Epoch: [1][16600/36908] Elapsed 212m 42s (remain 260m 12s) Loss: 0.0000(0.0130) Grad: 812.6027  LR: 0.000012  \n",
      "Epoch: [1][16700/36908] Elapsed 213m 58s (remain 258m 53s) Loss: 0.0090(0.0129) Grad: 17650.0273  LR: 0.000012  \n",
      "Epoch: [1][16800/36908] Elapsed 215m 16s (remain 257m 37s) Loss: 0.0009(0.0129) Grad: 2398.0623  LR: 0.000012  \n",
      "Epoch: [1][16900/36908] Elapsed 216m 32s (remain 256m 20s) Loss: 0.0001(0.0128) Grad: 373.2151  LR: 0.000012  \n",
      "Epoch: [1][17000/36908] Elapsed 217m 49s (remain 255m 2s) Loss: 0.0000(0.0128) Grad: 21.5969  LR: 0.000012  \n",
      "Epoch: [1][17100/36908] Elapsed 219m 5s (remain 253m 45s) Loss: 0.0006(0.0127) Grad: 3928.6836  LR: 0.000012  \n",
      "Epoch: [1][17200/36908] Elapsed 220m 23s (remain 252m 29s) Loss: 0.0000(0.0126) Grad: 55.3412  LR: 0.000012  \n",
      "Epoch: [1][17300/36908] Elapsed 221m 42s (remain 251m 15s) Loss: 0.0000(0.0126) Grad: 107.2252  LR: 0.000012  \n",
      "Epoch: [1][17400/36908] Elapsed 223m 4s (remain 250m 4s) Loss: 0.0000(0.0126) Grad: 51.9044  LR: 0.000012  \n",
      "Epoch: [1][17500/36908] Elapsed 224m 26s (remain 248m 53s) Loss: 0.0002(0.0125) Grad: 1763.2604  LR: 0.000012  \n",
      "Epoch: [1][17600/36908] Elapsed 225m 50s (remain 247m 43s) Loss: 0.0223(0.0125) Grad: 31922.2656  LR: 0.000012  \n",
      "Epoch: [1][17700/36908] Elapsed 227m 13s (remain 246m 33s) Loss: 0.0068(0.0124) Grad: 42311.1797  LR: 0.000012  \n",
      "Epoch: [1][17800/36908] Elapsed 228m 35s (remain 245m 21s) Loss: 0.0538(0.0124) Grad: 106531.9062  LR: 0.000012  \n",
      "Epoch: [1][17900/36908] Elapsed 229m 57s (remain 244m 10s) Loss: 0.0001(0.0123) Grad: 886.5142  LR: 0.000011  \n",
      "Epoch: [1][18000/36908] Elapsed 231m 19s (remain 242m 57s) Loss: 0.0007(0.0123) Grad: 1121.7482  LR: 0.000011  \n",
      "Epoch: [1][18100/36908] Elapsed 232m 35s (remain 241m 39s) Loss: 0.0000(0.0122) Grad: 28.6196  LR: 0.000011  \n",
      "Epoch: [1][18200/36908] Elapsed 233m 53s (remain 240m 23s) Loss: 0.0000(0.0122) Grad: 101.2541  LR: 0.000011  \n",
      "Epoch: [1][18300/36908] Elapsed 235m 11s (remain 239m 7s) Loss: 0.0001(0.0121) Grad: 445.2995  LR: 0.000011  \n",
      "Epoch: [1][18400/36908] Elapsed 236m 28s (remain 237m 50s) Loss: 0.0142(0.0121) Grad: 18348.2852  LR: 0.000011  \n",
      "Epoch: [1][18500/36908] Elapsed 237m 44s (remain 236m 31s) Loss: 0.0000(0.0120) Grad: 62.8163  LR: 0.000011  \n",
      "Epoch: [1][18600/36908] Elapsed 239m 4s (remain 235m 17s) Loss: 0.0013(0.0120) Grad: 17170.6074  LR: 0.000011  \n",
      "Epoch: [1][18700/36908] Elapsed 240m 20s (remain 233m 59s) Loss: 0.0000(0.0119) Grad: 384.5988  LR: 0.000011  \n",
      "Epoch: [1][18800/36908] Elapsed 241m 42s (remain 232m 46s) Loss: 0.0000(0.0119) Grad: 23.8381  LR: 0.000011  \n",
      "Epoch: [1][18900/36908] Elapsed 242m 59s (remain 231m 29s) Loss: 0.0752(0.0118) Grad: 107707.5625  LR: 0.000011  \n",
      "Epoch: [1][19000/36908] Elapsed 244m 15s (remain 230m 11s) Loss: 0.0000(0.0118) Grad: 102.4395  LR: 0.000011  \n",
      "Epoch: [1][19100/36908] Elapsed 245m 34s (remain 228m 55s) Loss: 0.0000(0.0118) Grad: 73.5856  LR: 0.000011  \n",
      "Epoch: [1][19200/36908] Elapsed 246m 52s (remain 227m 39s) Loss: 0.0010(0.0117) Grad: 1721.8173  LR: 0.000011  \n",
      "Epoch: [1][19300/36908] Elapsed 248m 8s (remain 226m 22s) Loss: 0.0000(0.0117) Grad: 63.1625  LR: 0.000011  \n",
      "Epoch: [1][19400/36908] Elapsed 249m 26s (remain 225m 5s) Loss: 0.0079(0.0116) Grad: 196619.7500  LR: 0.000011  \n",
      "Epoch: [1][19500/36908] Elapsed 250m 48s (remain 223m 53s) Loss: 0.0004(0.0116) Grad: 5213.6230  LR: 0.000010  \n",
      "Epoch: [1][19600/36908] Elapsed 252m 5s (remain 222m 35s) Loss: 0.0005(0.0116) Grad: 36327.3125  LR: 0.000010  \n",
      "Epoch: [1][19700/36908] Elapsed 253m 22s (remain 221m 17s) Loss: 0.0000(0.0115) Grad: 30.3966  LR: 0.000010  \n",
      "Epoch: [1][19800/36908] Elapsed 254m 38s (remain 220m 0s) Loss: 0.0002(0.0115) Grad: 8319.6357  LR: 0.000010  \n",
      "Epoch: [1][19900/36908] Elapsed 255m 55s (remain 218m 42s) Loss: 0.0000(0.0114) Grad: 57.9640  LR: 0.000010  \n",
      "Epoch: [1][20000/36908] Elapsed 257m 12s (remain 217m 25s) Loss: 0.0046(0.0114) Grad: 23346.9707  LR: 0.000010  \n",
      "Epoch: [1][20100/36908] Elapsed 258m 29s (remain 216m 7s) Loss: 0.0000(0.0113) Grad: 15.3488  LR: 0.000010  \n",
      "Epoch: [1][20200/36908] Elapsed 259m 46s (remain 214m 50s) Loss: 0.0000(0.0113) Grad: 77.7950  LR: 0.000010  \n",
      "Epoch: [1][20300/36908] Elapsed 261m 8s (remain 213m 37s) Loss: 0.0019(0.0113) Grad: 8961.3262  LR: 0.000010  \n",
      "Epoch: [1][20400/36908] Elapsed 262m 27s (remain 212m 21s) Loss: 0.0000(0.0112) Grad: 11.1870  LR: 0.000010  \n",
      "Epoch: [1][20500/36908] Elapsed 263m 44s (remain 211m 4s) Loss: 0.0005(0.0112) Grad: 667.1589  LR: 0.000010  \n",
      "Epoch: [1][20600/36908] Elapsed 265m 0s (remain 209m 46s) Loss: 0.0002(0.0112) Grad: 1397.5854  LR: 0.000010  \n",
      "Epoch: [1][20700/36908] Elapsed 266m 18s (remain 208m 29s) Loss: 0.0000(0.0111) Grad: 121.8931  LR: 0.000010  \n",
      "Epoch: [1][20800/36908] Elapsed 267m 38s (remain 207m 14s) Loss: 0.0074(0.0111) Grad: 12189.6924  LR: 0.000010  \n",
      "Epoch: [1][20900/36908] Elapsed 268m 54s (remain 205m 56s) Loss: 0.0001(0.0110) Grad: 456.6334  LR: 0.000010  \n",
      "Epoch: [1][21000/36908] Elapsed 270m 10s (remain 204m 38s) Loss: 0.0000(0.0110) Grad: 10.6781  LR: 0.000010  \n",
      "Epoch: [1][21100/36908] Elapsed 271m 27s (remain 203m 21s) Loss: 0.0000(0.0110) Grad: 81.5721  LR: 0.000010  \n",
      "Epoch: [1][21200/36908] Elapsed 272m 45s (remain 202m 4s) Loss: 0.0184(0.0109) Grad: 201875.9062  LR: 0.000009  \n",
      "Epoch: [1][21300/36908] Elapsed 274m 0s (remain 200m 45s) Loss: 0.0001(0.0109) Grad: 423.2745  LR: 0.000009  \n",
      "Epoch: [1][21400/36908] Elapsed 275m 15s (remain 199m 27s) Loss: 0.0001(0.0109) Grad: 1509.8129  LR: 0.000009  \n",
      "Epoch: [1][21500/36908] Elapsed 276m 32s (remain 198m 9s) Loss: 0.0016(0.0108) Grad: 13113.1182  LR: 0.000009  \n",
      "Epoch: [1][21600/36908] Elapsed 277m 47s (remain 196m 51s) Loss: 0.0000(0.0108) Grad: 47.8058  LR: 0.000009  \n",
      "Epoch: [1][21700/36908] Elapsed 279m 3s (remain 195m 32s) Loss: 0.0038(0.0107) Grad: 11840.3779  LR: 0.000009  \n",
      "Epoch: [1][21800/36908] Elapsed 280m 18s (remain 194m 14s) Loss: 0.0000(0.0107) Grad: 15.3849  LR: 0.000009  \n",
      "Epoch: [1][21900/36908] Elapsed 281m 35s (remain 192m 57s) Loss: 0.0000(0.0107) Grad: 100.2003  LR: 0.000009  \n",
      "Epoch: [1][22000/36908] Elapsed 282m 52s (remain 191m 40s) Loss: 0.0000(0.0106) Grad: 95.2318  LR: 0.000009  \n",
      "Epoch: [1][22100/36908] Elapsed 284m 10s (remain 190m 23s) Loss: 0.0000(0.0106) Grad: 33.5942  LR: 0.000009  \n",
      "Epoch: [1][22200/36908] Elapsed 285m 25s (remain 189m 4s) Loss: 0.0000(0.0106) Grad: 101.2928  LR: 0.000009  \n",
      "Epoch: [1][22300/36908] Elapsed 286m 40s (remain 187m 46s) Loss: 0.0000(0.0105) Grad: 30.7216  LR: 0.000009  \n",
      "Epoch: [1][22400/36908] Elapsed 287m 58s (remain 186m 29s) Loss: 0.0001(0.0105) Grad: 2981.2268  LR: 0.000009  \n",
      "Epoch: [1][22500/36908] Elapsed 289m 16s (remain 185m 13s) Loss: 0.0000(0.0105) Grad: 32.0932  LR: 0.000009  \n",
      "Epoch: [1][22600/36908] Elapsed 290m 38s (remain 183m 59s) Loss: 0.0000(0.0104) Grad: 231.0546  LR: 0.000009  \n",
      "Epoch: [1][22700/36908] Elapsed 292m 0s (remain 182m 45s) Loss: 0.0004(0.0104) Grad: 2100.5894  LR: 0.000009  \n",
      "Epoch: [1][22800/36908] Elapsed 293m 23s (remain 181m 31s) Loss: 0.0059(0.0104) Grad: 25745.2422  LR: 0.000008  \n",
      "Epoch: [1][22900/36908] Elapsed 294m 48s (remain 180m 18s) Loss: 0.0000(0.0103) Grad: 5.0117  LR: 0.000008  \n",
      "Epoch: [1][23000/36908] Elapsed 296m 10s (remain 179m 4s) Loss: 0.0001(0.0103) Grad: 167.2419  LR: 0.000008  \n",
      "Epoch: [1][23100/36908] Elapsed 297m 32s (remain 177m 50s) Loss: 0.0000(0.0103) Grad: 4.1410  LR: 0.000008  \n",
      "Epoch: [1][23200/36908] Elapsed 298m 54s (remain 176m 35s) Loss: 0.0000(0.0102) Grad: 8.9843  LR: 0.000008  \n",
      "Epoch: [1][23300/36908] Elapsed 300m 11s (remain 175m 18s) Loss: 0.0022(0.0102) Grad: 37892.7539  LR: 0.000008  \n",
      "Epoch: [1][23400/36908] Elapsed 301m 28s (remain 174m 0s) Loss: 0.0001(0.0102) Grad: 2742.5676  LR: 0.000008  \n",
      "Epoch: [1][23500/36908] Elapsed 302m 43s (remain 172m 42s) Loss: 0.0172(0.0101) Grad: 105066.2656  LR: 0.000008  \n",
      "Epoch: [1][23600/36908] Elapsed 304m 0s (remain 171m 24s) Loss: 0.0063(0.0101) Grad: 59884.0312  LR: 0.000008  \n",
      "Epoch: [1][23700/36908] Elapsed 305m 16s (remain 170m 6s) Loss: 0.0020(0.0101) Grad: 8215.3447  LR: 0.000008  \n",
      "Epoch: [1][23800/36908] Elapsed 306m 31s (remain 168m 48s) Loss: 0.0028(0.0101) Grad: 8665.0449  LR: 0.000008  \n",
      "Epoch: [1][23900/36908] Elapsed 307m 48s (remain 167m 30s) Loss: 0.0156(0.0100) Grad: 18444.6641  LR: 0.000008  \n",
      "Epoch: [1][24000/36908] Elapsed 309m 3s (remain 166m 12s) Loss: 0.0000(0.0100) Grad: 16.9568  LR: 0.000008  \n",
      "Epoch: [1][24100/36908] Elapsed 310m 19s (remain 164m 54s) Loss: 0.0059(0.0100) Grad: 35892.0781  LR: 0.000008  \n",
      "Epoch: [1][24200/36908] Elapsed 311m 36s (remain 163m 36s) Loss: 0.0067(0.0100) Grad: 21057.9375  LR: 0.000008  \n",
      "Epoch: [1][24300/36908] Elapsed 312m 51s (remain 162m 18s) Loss: 0.0001(0.0099) Grad: 933.9551  LR: 0.000008  \n",
      "Epoch: [1][24400/36908] Elapsed 314m 7s (remain 161m 0s) Loss: 0.0003(0.0099) Grad: 3348.7273  LR: 0.000008  \n",
      "Epoch: [1][24500/36908] Elapsed 315m 24s (remain 159m 43s) Loss: 0.0006(0.0099) Grad: 14741.6992  LR: 0.000007  \n",
      "Epoch: [1][24600/36908] Elapsed 316m 40s (remain 158m 25s) Loss: 0.0000(0.0098) Grad: 421.1206  LR: 0.000007  \n",
      "Epoch: [1][24700/36908] Elapsed 317m 57s (remain 157m 8s) Loss: 0.0003(0.0098) Grad: 7480.2471  LR: 0.000007  \n",
      "Epoch: [1][24800/36908] Elapsed 319m 13s (remain 155m 50s) Loss: 0.0000(0.0098) Grad: 28.5733  LR: 0.000007  \n",
      "Epoch: [1][24900/36908] Elapsed 320m 29s (remain 154m 32s) Loss: 0.0001(0.0098) Grad: 3291.6133  LR: 0.000007  \n",
      "Epoch: [1][25000/36908] Elapsed 321m 45s (remain 153m 14s) Loss: 0.0024(0.0097) Grad: 23443.8340  LR: 0.000007  \n",
      "Epoch: [1][25100/36908] Elapsed 323m 1s (remain 151m 56s) Loss: 0.0000(0.0097) Grad: 206.1158  LR: 0.000007  \n",
      "Epoch: [1][25200/36908] Elapsed 324m 18s (remain 150m 39s) Loss: 0.0000(0.0097) Grad: 165.3674  LR: 0.000007  \n",
      "Epoch: [1][25300/36908] Elapsed 325m 38s (remain 149m 23s) Loss: 0.0000(0.0097) Grad: 358.4339  LR: 0.000007  \n",
      "Epoch: [1][25400/36908] Elapsed 327m 0s (remain 148m 8s) Loss: 0.0028(0.0096) Grad: 87741.4844  LR: 0.000007  \n",
      "Epoch: [1][25500/36908] Elapsed 328m 18s (remain 146m 51s) Loss: 0.0000(0.0096) Grad: 99.6219  LR: 0.000007  \n",
      "Epoch: [1][25600/36908] Elapsed 329m 35s (remain 145m 34s) Loss: 0.0000(0.0096) Grad: 42.2199  LR: 0.000007  \n",
      "Epoch: [1][25700/36908] Elapsed 330m 51s (remain 144m 16s) Loss: 0.0054(0.0096) Grad: 26637.3066  LR: 0.000007  \n",
      "Epoch: [1][25800/36908] Elapsed 332m 7s (remain 142m 58s) Loss: 0.0001(0.0095) Grad: 1983.5841  LR: 0.000007  \n",
      "Epoch: [1][25900/36908] Elapsed 333m 24s (remain 141m 41s) Loss: 0.0000(0.0095) Grad: 145.3276  LR: 0.000007  \n",
      "Epoch: [1][26000/36908] Elapsed 334m 40s (remain 140m 23s) Loss: 0.0000(0.0095) Grad: 60.0399  LR: 0.000007  \n",
      "Epoch: [1][26100/36908] Elapsed 335m 55s (remain 139m 5s) Loss: 0.0000(0.0095) Grad: 790.0769  LR: 0.000007  \n",
      "Epoch: [1][26200/36908] Elapsed 337m 12s (remain 137m 47s) Loss: 0.0003(0.0094) Grad: 8360.0186  LR: 0.000006  \n",
      "Epoch: [1][26300/36908] Elapsed 338m 34s (remain 136m 32s) Loss: 0.0000(0.0094) Grad: 5.3330  LR: 0.000006  \n",
      "Epoch: [1][26400/36908] Elapsed 339m 51s (remain 135m 15s) Loss: 0.0000(0.0094) Grad: 218.2376  LR: 0.000006  \n",
      "Epoch: [1][26500/36908] Elapsed 341m 6s (remain 133m 57s) Loss: 0.0000(0.0094) Grad: 106.4768  LR: 0.000006  \n",
      "Epoch: [1][26600/36908] Elapsed 342m 23s (remain 132m 39s) Loss: 0.0038(0.0093) Grad: 26467.6934  LR: 0.000006  \n",
      "Epoch: [1][26700/36908] Elapsed 343m 39s (remain 131m 22s) Loss: 0.0212(0.0093) Grad: 86689.0781  LR: 0.000006  \n",
      "Epoch: [1][26800/36908] Elapsed 344m 57s (remain 130m 5s) Loss: 0.0000(0.0093) Grad: 27.9218  LR: 0.000006  \n",
      "Epoch: [1][26900/36908] Elapsed 346m 14s (remain 128m 48s) Loss: 0.0000(0.0093) Grad: 12.3250  LR: 0.000006  \n",
      "Epoch: [1][27000/36908] Elapsed 347m 31s (remain 127m 30s) Loss: 0.0123(0.0092) Grad: 223931.7031  LR: 0.000006  \n",
      "Epoch: [1][27100/36908] Elapsed 348m 48s (remain 126m 13s) Loss: 0.0000(0.0092) Grad: 46.1954  LR: 0.000006  \n",
      "Epoch: [1][27200/36908] Elapsed 350m 6s (remain 124m 56s) Loss: 0.0000(0.0092) Grad: 101.0717  LR: 0.000006  \n",
      "Epoch: [1][27300/36908] Elapsed 351m 28s (remain 123m 41s) Loss: 0.0001(0.0092) Grad: 1244.9635  LR: 0.000006  \n",
      "Epoch: [1][27400/36908] Elapsed 352m 52s (remain 122m 25s) Loss: 0.0015(0.0091) Grad: 2074.0356  LR: 0.000006  \n",
      "Epoch: [1][27500/36908] Elapsed 354m 12s (remain 121m 9s) Loss: 0.0004(0.0091) Grad: 3122.2920  LR: 0.000006  \n",
      "Epoch: [1][27600/36908] Elapsed 355m 29s (remain 119m 52s) Loss: 0.0000(0.0091) Grad: 40.2206  LR: 0.000006  \n",
      "Epoch: [1][27700/36908] Elapsed 356m 49s (remain 118m 36s) Loss: 0.0007(0.0091) Grad: 19419.1680  LR: 0.000006  \n",
      "Epoch: [1][27800/36908] Elapsed 358m 5s (remain 117m 18s) Loss: 0.0000(0.0091) Grad: 10.1503  LR: 0.000005  \n",
      "Epoch: [1][27900/36908] Elapsed 359m 20s (remain 116m 0s) Loss: 0.0027(0.0090) Grad: 103624.9531  LR: 0.000005  \n",
      "Epoch: [1][28000/36908] Elapsed 360m 36s (remain 114m 42s) Loss: 0.0000(0.0090) Grad: 11.1713  LR: 0.000005  \n",
      "Epoch: [1][28100/36908] Elapsed 361m 53s (remain 113m 25s) Loss: 0.0000(0.0090) Grad: 13.8748  LR: 0.000005  \n",
      "Epoch: [1][28200/36908] Elapsed 363m 8s (remain 112m 7s) Loss: 0.0304(0.0090) Grad: 18247.8984  LR: 0.000005  \n",
      "Epoch: [1][28300/36908] Elapsed 364m 24s (remain 110m 49s) Loss: 0.0002(0.0089) Grad: 2393.8748  LR: 0.000005  \n",
      "Epoch: [1][28400/36908] Elapsed 365m 38s (remain 109m 31s) Loss: 0.0000(0.0089) Grad: 37.7479  LR: 0.000005  \n",
      "Epoch: [1][28500/36908] Elapsed 366m 53s (remain 108m 13s) Loss: 0.0001(0.0089) Grad: 1489.8942  LR: 0.000005  \n",
      "Epoch: [1][28600/36908] Elapsed 368m 9s (remain 106m 55s) Loss: 0.0017(0.0089) Grad: 44170.7070  LR: 0.000005  \n",
      "Epoch: [1][28700/36908] Elapsed 369m 24s (remain 105m 37s) Loss: 0.0023(0.0089) Grad: 26240.1582  LR: 0.000005  \n",
      "Epoch: [1][28800/36908] Elapsed 370m 40s (remain 104m 20s) Loss: 0.0000(0.0088) Grad: 19.7975  LR: 0.000005  \n",
      "Epoch: [1][28900/36908] Elapsed 371m 58s (remain 103m 3s) Loss: 0.0015(0.0088) Grad: 7864.1377  LR: 0.000005  \n",
      "Epoch: [1][29000/36908] Elapsed 373m 14s (remain 101m 45s) Loss: 0.0000(0.0088) Grad: 17.4404  LR: 0.000005  \n",
      "Epoch: [1][29100/36908] Elapsed 374m 32s (remain 100m 28s) Loss: 0.0000(0.0088) Grad: 50.1433  LR: 0.000005  \n",
      "Epoch: [1][29200/36908] Elapsed 375m 50s (remain 99m 11s) Loss: 0.0047(0.0087) Grad: 32113.2422  LR: 0.000005  \n",
      "Epoch: [1][29300/36908] Elapsed 377m 7s (remain 97m 54s) Loss: 0.0012(0.0087) Grad: 27808.5781  LR: 0.000005  \n",
      "Epoch: [1][29400/36908] Elapsed 378m 25s (remain 96m 37s) Loss: 0.0005(0.0087) Grad: 1576.8258  LR: 0.000005  \n",
      "Epoch: [1][29500/36908] Elapsed 379m 47s (remain 95m 21s) Loss: 0.0005(0.0087) Grad: 3119.3411  LR: 0.000004  \n",
      "Epoch: [1][29600/36908] Elapsed 381m 5s (remain 94m 4s) Loss: 0.0000(0.0087) Grad: 15.7357  LR: 0.000004  \n",
      "Epoch: [1][29700/36908] Elapsed 382m 26s (remain 92m 48s) Loss: 0.0114(0.0086) Grad: 39360.9883  LR: 0.000004  \n",
      "Epoch: [1][29800/36908] Elapsed 383m 44s (remain 91m 30s) Loss: 0.0000(0.0086) Grad: 18.0742  LR: 0.000004  \n",
      "Epoch: [1][29900/36908] Elapsed 384m 59s (remain 90m 13s) Loss: 0.0033(0.0086) Grad: 24444.8730  LR: 0.000004  \n",
      "Epoch: [1][30000/36908] Elapsed 386m 15s (remain 88m 55s) Loss: 0.0000(0.0086) Grad: 101.4328  LR: 0.000004  \n",
      "Epoch: [1][30100/36908] Elapsed 387m 35s (remain 87m 38s) Loss: 0.0000(0.0086) Grad: 9.1740  LR: 0.000004  \n",
      "Epoch: [1][30200/36908] Elapsed 388m 52s (remain 86m 21s) Loss: 0.0054(0.0086) Grad: 133345.3125  LR: 0.000004  \n",
      "Epoch: [1][30300/36908] Elapsed 390m 9s (remain 85m 4s) Loss: 0.0000(0.0085) Grad: 14.1369  LR: 0.000004  \n",
      "Epoch: [1][30400/36908] Elapsed 391m 26s (remain 83m 46s) Loss: 0.0030(0.0085) Grad: 25764.3418  LR: 0.000004  \n",
      "Epoch: [1][30500/36908] Elapsed 392m 42s (remain 82m 29s) Loss: 0.0023(0.0085) Grad: 11463.0039  LR: 0.000004  \n",
      "Epoch: [1][30600/36908] Elapsed 393m 57s (remain 81m 11s) Loss: 0.0002(0.0085) Grad: 310.6210  LR: 0.000004  \n",
      "Epoch: [1][30700/36908] Elapsed 395m 16s (remain 79m 54s) Loss: 0.0001(0.0085) Grad: 347.2364  LR: 0.000004  \n",
      "Epoch: [1][30800/36908] Elapsed 396m 31s (remain 78m 37s) Loss: 0.0032(0.0084) Grad: 134434.4375  LR: 0.000004  \n",
      "Epoch: [1][30900/36908] Elapsed 397m 47s (remain 77m 19s) Loss: 0.0000(0.0084) Grad: 482.4219  LR: 0.000004  \n",
      "Epoch: [1][31000/36908] Elapsed 399m 2s (remain 76m 2s) Loss: 0.0000(0.0084) Grad: 31.4562  LR: 0.000004  \n",
      "Epoch: [1][31100/36908] Elapsed 400m 19s (remain 74m 44s) Loss: 0.0018(0.0084) Grad: 17260.9355  LR: 0.000003  \n",
      "Epoch: [1][31200/36908] Elapsed 401m 40s (remain 73m 28s) Loss: 0.0062(0.0084) Grad: 9490.6758  LR: 0.000003  \n",
      "Epoch: [1][31300/36908] Elapsed 402m 55s (remain 72m 10s) Loss: 0.0000(0.0083) Grad: 18.6331  LR: 0.000003  \n",
      "Epoch: [1][31400/36908] Elapsed 404m 13s (remain 70m 53s) Loss: 0.0000(0.0083) Grad: 44.1296  LR: 0.000003  \n",
      "Epoch: [1][31500/36908] Elapsed 405m 30s (remain 69m 36s) Loss: 0.0000(0.0083) Grad: 26.0649  LR: 0.000003  \n",
      "Epoch: [1][31600/36908] Elapsed 406m 50s (remain 68m 19s) Loss: 0.0000(0.0083) Grad: 56.7489  LR: 0.000003  \n",
      "Epoch: [1][31700/36908] Elapsed 408m 7s (remain 67m 2s) Loss: 0.0003(0.0083) Grad: 3783.1699  LR: 0.000003  \n",
      "Epoch: [1][31800/36908] Elapsed 409m 24s (remain 65m 44s) Loss: 0.0000(0.0083) Grad: 9.5909  LR: 0.000003  \n",
      "Epoch: [1][31900/36908] Elapsed 410m 45s (remain 64m 28s) Loss: 0.0000(0.0082) Grad: 99.2319  LR: 0.000003  \n",
      "Epoch: [1][32000/36908] Elapsed 412m 1s (remain 63m 10s) Loss: 0.0000(0.0082) Grad: 24.2520  LR: 0.000003  \n",
      "Epoch: [1][32100/36908] Elapsed 413m 19s (remain 61m 53s) Loss: 0.0000(0.0082) Grad: 8.1061  LR: 0.000003  \n",
      "Epoch: [1][32200/36908] Elapsed 414m 34s (remain 60m 36s) Loss: 0.0024(0.0082) Grad: 14098.9004  LR: 0.000003  \n",
      "Epoch: [1][32300/36908] Elapsed 415m 50s (remain 59m 18s) Loss: 0.0006(0.0082) Grad: 13926.4492  LR: 0.000003  \n",
      "Epoch: [1][32400/36908] Elapsed 417m 7s (remain 58m 1s) Loss: 0.0067(0.0082) Grad: 65196.2812  LR: 0.000003  \n",
      "Epoch: [1][32500/36908] Elapsed 418m 27s (remain 56m 44s) Loss: 0.0000(0.0081) Grad: 98.7715  LR: 0.000003  \n",
      "Epoch: [1][32600/36908] Elapsed 419m 43s (remain 55m 27s) Loss: 0.0000(0.0081) Grad: 50.7546  LR: 0.000003  \n",
      "Epoch: [1][32700/36908] Elapsed 420m 59s (remain 54m 9s) Loss: 0.0000(0.0081) Grad: 4.7719  LR: 0.000003  \n",
      "Epoch: [1][32800/36908] Elapsed 422m 16s (remain 52m 52s) Loss: 0.0013(0.0081) Grad: 29890.2480  LR: 0.000002  \n",
      "Epoch: [1][32900/36908] Elapsed 423m 33s (remain 51m 35s) Loss: 0.0002(0.0081) Grad: 5527.3687  LR: 0.000002  \n",
      "Epoch: [1][33000/36908] Elapsed 424m 52s (remain 50m 18s) Loss: 0.0000(0.0081) Grad: 10.8979  LR: 0.000002  \n",
      "Epoch: [1][33100/36908] Elapsed 426m 6s (remain 49m 0s) Loss: 0.0079(0.0080) Grad: 58606.8047  LR: 0.000002  \n",
      "Epoch: [1][33200/36908] Elapsed 427m 23s (remain 47m 43s) Loss: 0.0000(0.0080) Grad: 9.6420  LR: 0.000002  \n",
      "Epoch: [1][33300/36908] Elapsed 428m 41s (remain 46m 25s) Loss: 0.0000(0.0080) Grad: 20.8319  LR: 0.000002  \n",
      "Epoch: [1][33400/36908] Elapsed 429m 56s (remain 45m 8s) Loss: 0.0039(0.0080) Grad: 62618.8945  LR: 0.000002  \n",
      "Epoch: [1][33500/36908] Elapsed 431m 12s (remain 43m 51s) Loss: 0.0000(0.0080) Grad: 10.8611  LR: 0.000002  \n",
      "Epoch: [1][33600/36908] Elapsed 432m 29s (remain 42m 33s) Loss: 0.0017(0.0080) Grad: 27133.9590  LR: 0.000002  \n",
      "Epoch: [1][33700/36908] Elapsed 433m 44s (remain 41m 16s) Loss: 0.0015(0.0080) Grad: 22004.5078  LR: 0.000002  \n",
      "Epoch: [1][33800/36908] Elapsed 435m 0s (remain 39m 59s) Loss: 0.0000(0.0079) Grad: 92.8485  LR: 0.000002  \n",
      "Epoch: [1][33900/36908] Elapsed 436m 18s (remain 38m 41s) Loss: 0.0062(0.0079) Grad: 65713.6406  LR: 0.000002  \n",
      "Epoch: [1][34000/36908] Elapsed 437m 35s (remain 37m 24s) Loss: 0.0003(0.0079) Grad: 8187.5630  LR: 0.000002  \n",
      "Epoch: [1][34100/36908] Elapsed 438m 53s (remain 36m 7s) Loss: 0.0131(0.0079) Grad: 338971.1875  LR: 0.000002  \n",
      "Epoch: [1][34200/36908] Elapsed 440m 10s (remain 34m 50s) Loss: 0.0000(0.0079) Grad: 53.4624  LR: 0.000002  \n",
      "Epoch: [1][34300/36908] Elapsed 441m 27s (remain 33m 33s) Loss: 0.0000(0.0079) Grad: 3.8174  LR: 0.000002  \n",
      "Epoch: [1][34400/36908] Elapsed 442m 44s (remain 32m 15s) Loss: 0.0000(0.0078) Grad: 75.0481  LR: 0.000002  \n",
      "Epoch: [1][34500/36908] Elapsed 444m 3s (remain 30m 58s) Loss: 0.0000(0.0078) Grad: 13.0339  LR: 0.000001  \n",
      "Epoch: [1][34600/36908] Elapsed 445m 23s (remain 29m 41s) Loss: 0.0006(0.0078) Grad: 25565.1895  LR: 0.000001  \n",
      "Epoch: [1][34700/36908] Elapsed 446m 39s (remain 28m 24s) Loss: 0.0000(0.0078) Grad: 90.7979  LR: 0.000001  \n",
      "Epoch: [1][34800/36908] Elapsed 447m 56s (remain 27m 7s) Loss: 0.0000(0.0078) Grad: 22.2755  LR: 0.000001  \n",
      "Epoch: [1][34900/36908] Elapsed 449m 13s (remain 25m 49s) Loss: 0.0000(0.0078) Grad: 12.5711  LR: 0.000001  \n",
      "Epoch: [1][35000/36908] Elapsed 450m 32s (remain 24m 32s) Loss: 0.0000(0.0078) Grad: 133.0712  LR: 0.000001  \n",
      "Epoch: [1][35100/36908] Elapsed 451m 50s (remain 23m 15s) Loss: 0.0000(0.0077) Grad: 14.0042  LR: 0.000001  \n",
      "Epoch: [1][35200/36908] Elapsed 453m 8s (remain 21m 58s) Loss: 0.0054(0.0077) Grad: 20126.8516  LR: 0.000001  \n",
      "Epoch: [1][35300/36908] Elapsed 454m 24s (remain 20m 41s) Loss: 0.0045(0.0077) Grad: 129631.5859  LR: 0.000001  \n",
      "Epoch: [1][35400/36908] Elapsed 455m 41s (remain 19m 23s) Loss: 0.0016(0.0077) Grad: 6750.5278  LR: 0.000001  \n",
      "Epoch: [1][35500/36908] Elapsed 456m 59s (remain 18m 6s) Loss: 0.0000(0.0077) Grad: 106.0913  LR: 0.000001  \n",
      "Epoch: [1][35600/36908] Elapsed 458m 20s (remain 16m 49s) Loss: 0.0000(0.0077) Grad: 562.0459  LR: 0.000001  \n",
      "Epoch: [1][35700/36908] Elapsed 459m 36s (remain 15m 32s) Loss: 0.0037(0.0077) Grad: 10398.7188  LR: 0.000001  \n",
      "Epoch: [1][35800/36908] Elapsed 460m 58s (remain 14m 15s) Loss: 0.0197(0.0076) Grad: 57053.1211  LR: 0.000001  \n",
      "Epoch: [1][35900/36908] Elapsed 462m 14s (remain 12m 57s) Loss: 0.0000(0.0076) Grad: 28.0277  LR: 0.000001  \n",
      "Epoch: [1][36000/36908] Elapsed 463m 30s (remain 11m 40s) Loss: 0.0144(0.0076) Grad: 65554.3828  LR: 0.000001  \n",
      "Epoch: [1][36100/36908] Elapsed 464m 46s (remain 10m 23s) Loss: 0.0005(0.0076) Grad: 26490.5117  LR: 0.000000  \n",
      "Epoch: [1][36200/36908] Elapsed 466m 3s (remain 9m 6s) Loss: 0.0004(0.0076) Grad: 43425.6094  LR: 0.000000  \n",
      "Epoch: [1][36300/36908] Elapsed 467m 19s (remain 7m 48s) Loss: 0.0000(0.0076) Grad: 28.1386  LR: 0.000000  \n",
      "Epoch: [1][36400/36908] Elapsed 468m 35s (remain 6m 31s) Loss: 0.0000(0.0076) Grad: 57.9711  LR: 0.000000  \n",
      "Epoch: [1][36500/36908] Elapsed 469m 52s (remain 5m 14s) Loss: 0.0000(0.0076) Grad: 1366.6324  LR: 0.000000  \n",
      "Epoch: [1][36600/36908] Elapsed 471m 10s (remain 3m 57s) Loss: 0.0074(0.0075) Grad: 36960.4023  LR: 0.000000  \n",
      "Epoch: [1][36700/36908] Elapsed 472m 26s (remain 2m 39s) Loss: 0.0000(0.0075) Grad: 266.0187  LR: 0.000000  \n",
      "Epoch: [1][36800/36908] Elapsed 473m 43s (remain 1m 22s) Loss: 0.0000(0.0075) Grad: 27.5836  LR: 0.000000  \n",
      "Epoch: [1][36900/36908] Elapsed 474m 59s (remain 0m 5s) Loss: 0.0273(0.0075) Grad: 316399.1250  LR: 0.000000  \n",
      "Epoch: [1][36907/36908] Elapsed 475m 4s (remain 0m 0s) Loss: 0.0000(0.0075) Grad: 50.3802  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 17m 20s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 31s (remain 5m 44s) Loss: 0.0245(0.0086) \n",
      "EVAL: [200/1192] Elapsed 1m 2s (remain 5m 9s) Loss: 0.0260(0.0097) \n",
      "EVAL: [300/1192] Elapsed 1m 32s (remain 4m 35s) Loss: 0.0038(0.0107) \n",
      "EVAL: [400/1192] Elapsed 2m 3s (remain 4m 4s) Loss: 0.0113(0.0115) \n",
      "EVAL: [500/1192] Elapsed 2m 33s (remain 3m 31s) Loss: 0.0072(0.0106) \n",
      "EVAL: [600/1192] Elapsed 3m 4s (remain 3m 1s) Loss: 0.0000(0.0109) \n",
      "EVAL: [700/1192] Elapsed 3m 34s (remain 2m 30s) Loss: 0.1225(0.0129) \n",
      "EVAL: [800/1192] Elapsed 4m 4s (remain 1m 59s) Loss: 0.0009(0.0133) \n",
      "EVAL: [900/1192] Elapsed 4m 33s (remain 1m 28s) Loss: 0.0001(0.0132) \n",
      "EVAL: [1000/1192] Elapsed 5m 3s (remain 0m 57s) Loss: 0.0000(0.0130) \n",
      "EVAL: [1100/1192] Elapsed 5m 34s (remain 0m 27s) Loss: 0.0218(0.0124) \n",
      "EVAL: [1191/1192] Elapsed 6m 1s (remain 0m 0s) Loss: 0.0000(0.0120) \n",
      "Epoch 1 - avg_train_loss: 0.0075  avg_val_loss: 0.0120  time: 28871s\n",
      "Epoch 1 - Score: 0.8891\n",
      "Epoch 1 - Save Best Score: 0.8891 Model\n",
      "========== fold: 1 training ==========\n",
      "get pseudo plain from ../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_1.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_1.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp083/pseudo_labels_1.npy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8322e00463ad4e138af49c32fc5c2962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60da0f5bc97a40dc92372dd737f3d203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(612602, 950)\n",
      "(612602, 6) (612602, 950)\n",
      "(100000, 7)\n",
      "(110725, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\n",
      "Epoch: [1][0/36908] Elapsed 0m 1s (remain 824m 4s) Loss: 0.3546(0.3546) Grad: 134267.3750  LR: 0.000000  \n",
      "Epoch: [1][100/36908] Elapsed 1m 18s (remain 476m 6s) Loss: 0.3239(0.3455) Grad: 124105.3906  LR: 0.000001  \n",
      "Epoch: [1][200/36908] Elapsed 2m 38s (remain 483m 48s) Loss: 0.2403(0.3164) Grad: 105108.6484  LR: 0.000001  \n",
      "Epoch: [1][300/36908] Elapsed 3m 59s (remain 486m 1s) Loss: 0.1399(0.2738) Grad: 65072.8203  LR: 0.000002  \n",
      "Epoch: [1][400/36908] Elapsed 5m 18s (remain 482m 51s) Loss: 0.0673(0.2288) Grad: 21708.5957  LR: 0.000002  \n",
      "Epoch: [1][500/36908] Elapsed 6m 35s (remain 478m 44s) Loss: 0.0542(0.1927) Grad: 4299.8325  LR: 0.000003  \n",
      "Epoch: [1][600/36908] Elapsed 7m 52s (remain 475m 42s) Loss: 0.0192(0.1677) Grad: 5956.5015  LR: 0.000003  \n",
      "Epoch: [1][700/36908] Elapsed 9m 9s (remain 472m 40s) Loss: 0.0277(0.1495) Grad: 3658.9551  LR: 0.000004  \n",
      "Epoch: [1][800/36908] Elapsed 10m 29s (remain 472m 46s) Loss: 0.0363(0.1355) Grad: 4899.2729  LR: 0.000004  \n",
      "Epoch: [1][900/36908] Elapsed 11m 47s (remain 471m 18s) Loss: 0.0149(0.1251) Grad: 5797.1289  LR: 0.000005  \n",
      "Epoch: [1][1000/36908] Elapsed 13m 5s (remain 469m 34s) Loss: 0.0436(0.1168) Grad: 8090.7153  LR: 0.000005  \n",
      "Epoch: [1][1100/36908] Elapsed 14m 21s (remain 467m 9s) Loss: 0.0470(0.1097) Grad: 11172.2812  LR: 0.000006  \n",
      "Epoch: [1][1200/36908] Elapsed 15m 39s (remain 465m 31s) Loss: 0.0108(0.1037) Grad: 8472.5391  LR: 0.000007  \n",
      "Epoch: [1][1300/36908] Elapsed 17m 2s (remain 466m 28s) Loss: 0.0297(0.0979) Grad: 56615.0195  LR: 0.000007  \n",
      "Epoch: [1][1400/36908] Elapsed 18m 21s (remain 465m 12s) Loss: 0.0147(0.0926) Grad: 56835.1875  LR: 0.000008  \n",
      "Epoch: [1][1500/36908] Elapsed 19m 40s (remain 464m 6s) Loss: 0.0364(0.0877) Grad: 16606.7988  LR: 0.000008  \n",
      "Epoch: [1][1600/36908] Elapsed 20m 58s (remain 462m 35s) Loss: 0.0029(0.0834) Grad: 11075.4482  LR: 0.000009  \n",
      "Epoch: [1][1700/36908] Elapsed 22m 16s (remain 461m 9s) Loss: 0.0049(0.0794) Grad: 6015.1484  LR: 0.000009  \n",
      "Epoch: [1][1800/36908] Elapsed 23m 37s (remain 460m 30s) Loss: 0.0085(0.0758) Grad: 24398.2461  LR: 0.000010  \n",
      "Epoch: [1][1900/36908] Elapsed 24m 55s (remain 459m 5s) Loss: 0.0018(0.0725) Grad: 799.4092  LR: 0.000010  \n",
      "Epoch: [1][2000/36908] Elapsed 26m 15s (remain 457m 57s) Loss: 0.0121(0.0696) Grad: 35595.9531  LR: 0.000011  \n",
      "Epoch: [1][2100/36908] Elapsed 27m 34s (remain 456m 53s) Loss: 0.0084(0.0669) Grad: 21392.3281  LR: 0.000011  \n",
      "Epoch: [1][2200/36908] Elapsed 28m 53s (remain 455m 37s) Loss: 0.0017(0.0644) Grad: 2511.3367  LR: 0.000012  \n",
      "Epoch: [1][2300/36908] Elapsed 30m 11s (remain 454m 6s) Loss: 0.0008(0.0621) Grad: 477.9555  LR: 0.000012  \n",
      "Epoch: [1][2400/36908] Elapsed 31m 29s (remain 452m 39s) Loss: 0.0176(0.0602) Grad: 48398.0195  LR: 0.000013  \n",
      "Epoch: [1][2500/36908] Elapsed 32m 48s (remain 451m 25s) Loss: 0.0212(0.0583) Grad: 42743.6719  LR: 0.000014  \n",
      "Epoch: [1][2600/36908] Elapsed 34m 6s (remain 449m 58s) Loss: 0.0325(0.0565) Grad: 81532.6719  LR: 0.000014  \n",
      "Epoch: [1][2700/36908] Elapsed 35m 26s (remain 448m 46s) Loss: 0.0028(0.0548) Grad: 11809.0977  LR: 0.000015  \n",
      "Epoch: [1][2800/36908] Elapsed 36m 45s (remain 447m 36s) Loss: 0.0016(0.0532) Grad: 4400.2124  LR: 0.000015  \n",
      "Epoch: [1][2900/36908] Elapsed 38m 6s (remain 446m 38s) Loss: 0.0021(0.0517) Grad: 1581.8047  LR: 0.000016  \n",
      "Epoch: [1][3000/36908] Elapsed 39m 24s (remain 445m 12s) Loss: 0.0018(0.0503) Grad: 5876.7969  LR: 0.000016  \n",
      "Epoch: [1][3100/36908] Elapsed 40m 42s (remain 443m 43s) Loss: 0.0015(0.0489) Grad: 3301.1726  LR: 0.000017  \n",
      "Epoch: [1][3200/36908] Elapsed 42m 3s (remain 442m 51s) Loss: 0.0008(0.0477) Grad: 398.3831  LR: 0.000017  \n",
      "Epoch: [1][3300/36908] Elapsed 43m 21s (remain 441m 29s) Loss: 0.0008(0.0465) Grad: 197.1658  LR: 0.000018  \n",
      "Epoch: [1][3400/36908] Elapsed 44m 39s (remain 440m 2s) Loss: 0.0000(0.0454) Grad: 62.1343  LR: 0.000018  \n",
      "Epoch: [1][3500/36908] Elapsed 45m 59s (remain 438m 48s) Loss: 0.0515(0.0444) Grad: 57825.3672  LR: 0.000019  \n",
      "Epoch: [1][3600/36908] Elapsed 47m 16s (remain 437m 19s) Loss: 0.0146(0.0434) Grad: 43825.8203  LR: 0.000020  \n",
      "Epoch: [1][3700/36908] Elapsed 48m 34s (remain 435m 49s) Loss: 0.0003(0.0425) Grad: 533.7203  LR: 0.000020  \n",
      "Epoch: [1][3800/36908] Elapsed 49m 52s (remain 434m 23s) Loss: 0.0002(0.0416) Grad: 165.9863  LR: 0.000020  \n",
      "Epoch: [1][3900/36908] Elapsed 51m 10s (remain 432m 57s) Loss: 0.0130(0.0407) Grad: 12220.8271  LR: 0.000020  \n",
      "Epoch: [1][4000/36908] Elapsed 52m 28s (remain 431m 37s) Loss: 0.0061(0.0399) Grad: 4472.3237  LR: 0.000020  \n",
      "Epoch: [1][4100/36908] Elapsed 53m 47s (remain 430m 17s) Loss: 0.0062(0.0391) Grad: 14779.5830  LR: 0.000020  \n",
      "Epoch: [1][4200/36908] Elapsed 55m 4s (remain 428m 47s) Loss: 0.0017(0.0383) Grad: 2321.9661  LR: 0.000020  \n",
      "Epoch: [1][4300/36908] Elapsed 56m 21s (remain 427m 13s) Loss: 0.0140(0.0376) Grad: 40961.5234  LR: 0.000020  \n",
      "Epoch: [1][4400/36908] Elapsed 57m 38s (remain 425m 46s) Loss: 0.0011(0.0369) Grad: 1341.1953  LR: 0.000020  \n",
      "Epoch: [1][4500/36908] Elapsed 58m 55s (remain 424m 13s) Loss: 0.0006(0.0363) Grad: 1422.7269  LR: 0.000020  \n",
      "Epoch: [1][4600/36908] Elapsed 60m 11s (remain 422m 39s) Loss: 0.0044(0.0357) Grad: 12348.2559  LR: 0.000019  \n",
      "Epoch: [1][4700/36908] Elapsed 61m 29s (remain 421m 17s) Loss: 0.0159(0.0350) Grad: 15271.4209  LR: 0.000019  \n",
      "Epoch: [1][4800/36908] Elapsed 62m 47s (remain 419m 56s) Loss: 0.0005(0.0344) Grad: 1317.1440  LR: 0.000019  \n",
      "Epoch: [1][4900/36908] Elapsed 64m 7s (remain 418m 48s) Loss: 0.0010(0.0338) Grad: 1898.1560  LR: 0.000019  \n",
      "Epoch: [1][5000/36908] Elapsed 65m 25s (remain 417m 24s) Loss: 0.0022(0.0333) Grad: 19525.6777  LR: 0.000019  \n",
      "Epoch: [1][5100/36908] Elapsed 66m 42s (remain 416m 0s) Loss: 0.0010(0.0327) Grad: 2294.9578  LR: 0.000019  \n",
      "Epoch: [1][5200/36908] Elapsed 68m 0s (remain 414m 36s) Loss: 0.0011(0.0322) Grad: 2239.7144  LR: 0.000019  \n",
      "Epoch: [1][5300/36908] Elapsed 69m 18s (remain 413m 13s) Loss: 0.0003(0.0317) Grad: 65.8386  LR: 0.000019  \n",
      "Epoch: [1][5400/36908] Elapsed 70m 35s (remain 411m 48s) Loss: 0.0021(0.0312) Grad: 5061.9150  LR: 0.000019  \n",
      "Epoch: [1][5500/36908] Elapsed 71m 55s (remain 410m 38s) Loss: 0.0121(0.0308) Grad: 3279.5024  LR: 0.000019  \n",
      "Epoch: [1][5600/36908] Elapsed 73m 14s (remain 409m 23s) Loss: 0.0001(0.0303) Grad: 94.9787  LR: 0.000019  \n",
      "Epoch: [1][5700/36908] Elapsed 74m 31s (remain 407m 59s) Loss: 0.0001(0.0299) Grad: 454.6441  LR: 0.000019  \n",
      "Epoch: [1][5800/36908] Elapsed 75m 47s (remain 406m 27s) Loss: 0.0001(0.0295) Grad: 63.1494  LR: 0.000019  \n",
      "Epoch: [1][5900/36908] Elapsed 77m 3s (remain 404m 55s) Loss: 0.0012(0.0291) Grad: 9384.3145  LR: 0.000019  \n",
      "Epoch: [1][6000/36908] Elapsed 78m 20s (remain 403m 28s) Loss: 0.0097(0.0287) Grad: 94387.7734  LR: 0.000019  \n",
      "Epoch: [1][6100/36908] Elapsed 79m 38s (remain 402m 10s) Loss: 0.0055(0.0283) Grad: 22558.8984  LR: 0.000019  \n",
      "Epoch: [1][6200/36908] Elapsed 80m 57s (remain 400m 52s) Loss: 0.0084(0.0279) Grad: 19559.9375  LR: 0.000018  \n",
      "Epoch: [1][6300/36908] Elapsed 82m 20s (remain 399m 57s) Loss: 0.0003(0.0276) Grad: 1292.4503  LR: 0.000018  \n",
      "Epoch: [1][6400/36908] Elapsed 83m 36s (remain 398m 28s) Loss: 0.0000(0.0272) Grad: 41.5091  LR: 0.000018  \n",
      "Epoch: [1][6500/36908] Elapsed 84m 53s (remain 397m 5s) Loss: 0.0003(0.0269) Grad: 2913.2268  LR: 0.000018  \n",
      "Epoch: [1][6600/36908] Elapsed 86m 12s (remain 395m 48s) Loss: 0.0005(0.0265) Grad: 5865.5107  LR: 0.000018  \n",
      "Epoch: [1][6700/36908] Elapsed 87m 29s (remain 394m 22s) Loss: 0.0088(0.0262) Grad: 6361.9565  LR: 0.000018  \n",
      "Epoch: [1][6800/36908] Elapsed 88m 46s (remain 392m 57s) Loss: 0.0145(0.0260) Grad: 22172.1426  LR: 0.000018  \n",
      "Epoch: [1][6900/36908] Elapsed 90m 3s (remain 391m 35s) Loss: 0.0068(0.0256) Grad: 67779.1328  LR: 0.000018  \n",
      "Epoch: [1][7000/36908] Elapsed 91m 22s (remain 390m 20s) Loss: 0.0029(0.0254) Grad: 10333.3213  LR: 0.000018  \n",
      "Epoch: [1][7100/36908] Elapsed 92m 39s (remain 388m 55s) Loss: 0.0002(0.0251) Grad: 658.5428  LR: 0.000018  \n",
      "Epoch: [1][7200/36908] Elapsed 93m 55s (remain 387m 28s) Loss: 0.0000(0.0248) Grad: 31.0696  LR: 0.000018  \n",
      "Epoch: [1][7300/36908] Elapsed 95m 12s (remain 386m 4s) Loss: 0.0000(0.0245) Grad: 12.4603  LR: 0.000018  \n",
      "Epoch: [1][7400/36908] Elapsed 96m 28s (remain 384m 39s) Loss: 0.0121(0.0242) Grad: 36869.9141  LR: 0.000018  \n",
      "Epoch: [1][7500/36908] Elapsed 97m 45s (remain 383m 14s) Loss: 0.0001(0.0240) Grad: 129.9558  LR: 0.000018  \n",
      "Epoch: [1][7600/36908] Elapsed 99m 4s (remain 382m 0s) Loss: 0.0008(0.0237) Grad: 7274.9272  LR: 0.000018  \n",
      "Epoch: [1][7700/36908] Elapsed 100m 21s (remain 380m 38s) Loss: 0.0000(0.0235) Grad: 50.5735  LR: 0.000018  \n",
      "Epoch: [1][7800/36908] Elapsed 101m 38s (remain 379m 14s) Loss: 0.0575(0.0233) Grad: 183374.4844  LR: 0.000018  \n",
      "Epoch: [1][7900/36908] Elapsed 102m 57s (remain 377m 58s) Loss: 0.0189(0.0230) Grad: 48843.8477  LR: 0.000017  \n",
      "Epoch: [1][8000/36908] Elapsed 104m 15s (remain 376m 39s) Loss: 0.0000(0.0228) Grad: 47.0075  LR: 0.000017  \n",
      "Epoch: [1][8100/36908] Elapsed 105m 34s (remain 375m 24s) Loss: 0.0044(0.0226) Grad: 45357.4180  LR: 0.000017  \n",
      "Epoch: [1][8200/36908] Elapsed 106m 55s (remain 374m 15s) Loss: 0.0032(0.0223) Grad: 23207.8945  LR: 0.000017  \n",
      "Epoch: [1][8300/36908] Elapsed 108m 13s (remain 372m 57s) Loss: 0.0000(0.0221) Grad: 161.2114  LR: 0.000017  \n",
      "Epoch: [1][8400/36908] Elapsed 109m 37s (remain 371m 59s) Loss: 0.0000(0.0219) Grad: 42.7983  LR: 0.000017  \n",
      "Epoch: [1][8500/36908] Elapsed 111m 1s (remain 371m 0s) Loss: 0.0001(0.0217) Grad: 61.7571  LR: 0.000017  \n",
      "Epoch: [1][8600/36908] Elapsed 112m 23s (remain 369m 55s) Loss: 0.0008(0.0215) Grad: 2070.5037  LR: 0.000017  \n",
      "Epoch: [1][8700/36908] Elapsed 113m 41s (remain 368m 33s) Loss: 0.0001(0.0213) Grad: 548.0672  LR: 0.000017  \n",
      "Epoch: [1][8800/36908] Elapsed 114m 59s (remain 367m 13s) Loss: 0.0139(0.0211) Grad: 22636.4883  LR: 0.000017  \n",
      "Epoch: [1][8900/36908] Elapsed 116m 17s (remain 365m 55s) Loss: 0.0000(0.0209) Grad: 19.2935  LR: 0.000017  \n",
      "Epoch: [1][9000/36908] Elapsed 117m 36s (remain 364m 38s) Loss: 0.0008(0.0207) Grad: 1231.3108  LR: 0.000017  \n",
      "Epoch: [1][9100/36908] Elapsed 118m 54s (remain 363m 17s) Loss: 0.0219(0.0205) Grad: 84989.9766  LR: 0.000017  \n",
      "Epoch: [1][9200/36908] Elapsed 120m 11s (remain 361m 56s) Loss: 0.0092(0.0204) Grad: 38916.8008  LR: 0.000017  \n",
      "Epoch: [1][9300/36908] Elapsed 121m 29s (remain 360m 37s) Loss: 0.0000(0.0202) Grad: 131.4334  LR: 0.000017  \n",
      "Epoch: [1][9400/36908] Elapsed 122m 47s (remain 359m 18s) Loss: 0.0011(0.0200) Grad: 9017.0332  LR: 0.000017  \n",
      "Epoch: [1][9500/36908] Elapsed 124m 7s (remain 358m 3s) Loss: 0.0016(0.0198) Grad: 16871.0293  LR: 0.000017  \n",
      "Epoch: [1][9600/36908] Elapsed 125m 24s (remain 356m 40s) Loss: 0.0001(0.0197) Grad: 590.6426  LR: 0.000016  \n",
      "Epoch: [1][9700/36908] Elapsed 126m 40s (remain 355m 16s) Loss: 0.0000(0.0195) Grad: 32.4886  LR: 0.000016  \n",
      "Epoch: [1][9800/36908] Elapsed 127m 57s (remain 353m 53s) Loss: 0.0000(0.0194) Grad: 133.8184  LR: 0.000016  \n",
      "Epoch: [1][9900/36908] Elapsed 129m 14s (remain 352m 32s) Loss: 0.0001(0.0192) Grad: 589.7045  LR: 0.000016  \n",
      "Epoch: [1][10000/36908] Elapsed 130m 35s (remain 351m 22s) Loss: 0.0242(0.0191) Grad: 155822.7031  LR: 0.000016  \n",
      "Epoch: [1][10100/36908] Elapsed 131m 53s (remain 350m 0s) Loss: 0.0080(0.0189) Grad: 24424.5508  LR: 0.000016  \n",
      "Epoch: [1][10200/36908] Elapsed 133m 9s (remain 348m 36s) Loss: 0.0000(0.0188) Grad: 86.5143  LR: 0.000016  \n",
      "Epoch: [1][10300/36908] Elapsed 134m 26s (remain 347m 14s) Loss: 0.0000(0.0186) Grad: 25.7789  LR: 0.000016  \n",
      "Epoch: [1][10400/36908] Elapsed 135m 42s (remain 345m 51s) Loss: 0.0000(0.0185) Grad: 76.8836  LR: 0.000016  \n",
      "Epoch: [1][10500/36908] Elapsed 136m 58s (remain 344m 27s) Loss: 0.0000(0.0184) Grad: 52.8891  LR: 0.000016  \n",
      "Epoch: [1][10600/36908] Elapsed 138m 15s (remain 343m 5s) Loss: 0.0042(0.0182) Grad: 338282.7188  LR: 0.000016  \n",
      "Epoch: [1][10700/36908] Elapsed 139m 33s (remain 341m 47s) Loss: 0.0067(0.0181) Grad: 31971.1504  LR: 0.000016  \n",
      "Epoch: [1][10800/36908] Elapsed 140m 52s (remain 340m 31s) Loss: 0.0001(0.0179) Grad: 6019.5112  LR: 0.000016  \n",
      "Epoch: [1][10900/36908] Elapsed 142m 11s (remain 339m 14s) Loss: 0.0004(0.0178) Grad: 5099.5396  LR: 0.000016  \n",
      "Epoch: [1][11000/36908] Elapsed 143m 32s (remain 338m 1s) Loss: 0.0000(0.0177) Grad: 221.3438  LR: 0.000016  \n",
      "Epoch: [1][11100/36908] Elapsed 144m 50s (remain 336m 43s) Loss: 0.0001(0.0176) Grad: 1587.8531  LR: 0.000016  \n",
      "Epoch: [1][11200/36908] Elapsed 146m 10s (remain 335m 28s) Loss: 0.0275(0.0175) Grad: 303977.8438  LR: 0.000015  \n",
      "Epoch: [1][11300/36908] Elapsed 147m 29s (remain 334m 12s) Loss: 0.0049(0.0173) Grad: 3856.2566  LR: 0.000015  \n",
      "Epoch: [1][11400/36908] Elapsed 148m 48s (remain 332m 54s) Loss: 0.0077(0.0172) Grad: 140707.2969  LR: 0.000015  \n",
      "Epoch: [1][11500/36908] Elapsed 150m 6s (remain 331m 35s) Loss: 0.0003(0.0171) Grad: 3977.4067  LR: 0.000015  \n",
      "Epoch: [1][11600/36908] Elapsed 151m 23s (remain 330m 15s) Loss: 0.0056(0.0170) Grad: 10407.4912  LR: 0.000015  \n",
      "Epoch: [1][11700/36908] Elapsed 152m 43s (remain 328m 59s) Loss: 0.0055(0.0169) Grad: 27865.8047  LR: 0.000015  \n",
      "Epoch: [1][11800/36908] Elapsed 154m 7s (remain 327m 55s) Loss: 0.0015(0.0168) Grad: 17385.3086  LR: 0.000015  \n",
      "Epoch: [1][11900/36908] Elapsed 155m 25s (remain 326m 36s) Loss: 0.0015(0.0167) Grad: 23678.4316  LR: 0.000015  \n",
      "Epoch: [1][12000/36908] Elapsed 156m 43s (remain 325m 16s) Loss: 0.0001(0.0166) Grad: 401.1040  LR: 0.000015  \n",
      "Epoch: [1][12100/36908] Elapsed 158m 2s (remain 323m 58s) Loss: 0.0000(0.0164) Grad: 23.1541  LR: 0.000015  \n",
      "Epoch: [1][12200/36908] Elapsed 159m 20s (remain 322m 40s) Loss: 0.0033(0.0163) Grad: 47566.0352  LR: 0.000015  \n",
      "Epoch: [1][12300/36908] Elapsed 160m 39s (remain 321m 23s) Loss: 0.0021(0.0162) Grad: 42056.8516  LR: 0.000015  \n",
      "Epoch: [1][12400/36908] Elapsed 161m 57s (remain 320m 4s) Loss: 0.0000(0.0161) Grad: 13.0130  LR: 0.000015  \n",
      "Epoch: [1][12500/36908] Elapsed 163m 16s (remain 318m 46s) Loss: 0.0512(0.0160) Grad: 198904.8281  LR: 0.000015  \n",
      "Epoch: [1][12600/36908] Elapsed 164m 35s (remain 317m 29s) Loss: 0.0080(0.0159) Grad: 18511.8125  LR: 0.000015  \n",
      "Epoch: [1][12700/36908] Elapsed 165m 54s (remain 316m 11s) Loss: 0.0035(0.0158) Grad: 17740.0723  LR: 0.000015  \n",
      "Epoch: [1][12800/36908] Elapsed 167m 10s (remain 314m 50s) Loss: 0.0183(0.0157) Grad: 135179.9219  LR: 0.000015  \n",
      "Epoch: [1][12900/36908] Elapsed 168m 27s (remain 313m 28s) Loss: 0.0000(0.0156) Grad: 62.5904  LR: 0.000014  \n",
      "Epoch: [1][13000/36908] Elapsed 169m 47s (remain 312m 14s) Loss: 0.0010(0.0155) Grad: 11223.4473  LR: 0.000014  \n",
      "Epoch: [1][13100/36908] Elapsed 171m 4s (remain 310m 52s) Loss: 0.0000(0.0154) Grad: 135.8216  LR: 0.000014  \n",
      "Epoch: [1][13200/36908] Elapsed 172m 22s (remain 309m 33s) Loss: 0.0076(0.0153) Grad: 45209.8516  LR: 0.000014  \n",
      "Epoch: [1][13300/36908] Elapsed 173m 42s (remain 308m 18s) Loss: 0.0383(0.0153) Grad: 248296.4375  LR: 0.000014  \n",
      "Epoch: [1][13400/36908] Elapsed 174m 59s (remain 306m 57s) Loss: 0.0040(0.0152) Grad: 37191.8945  LR: 0.000014  \n",
      "Epoch: [1][13500/36908] Elapsed 176m 15s (remain 305m 35s) Loss: 0.0096(0.0151) Grad: 30830.8672  LR: 0.000014  \n",
      "Epoch: [1][13600/36908] Elapsed 177m 37s (remain 304m 22s) Loss: 0.0000(0.0150) Grad: 14.9679  LR: 0.000014  \n",
      "Epoch: [1][13700/36908] Elapsed 178m 53s (remain 303m 1s) Loss: 0.0020(0.0149) Grad: 44043.5742  LR: 0.000014  \n",
      "Epoch: [1][13800/36908] Elapsed 180m 9s (remain 301m 39s) Loss: 0.0001(0.0148) Grad: 4912.2524  LR: 0.000014  \n",
      "Epoch: [1][13900/36908] Elapsed 181m 26s (remain 300m 18s) Loss: 0.0001(0.0148) Grad: 2848.9373  LR: 0.000014  \n",
      "Epoch: [1][14000/36908] Elapsed 182m 46s (remain 299m 2s) Loss: 0.0000(0.0147) Grad: 816.1650  LR: 0.000014  \n",
      "Epoch: [1][14100/36908] Elapsed 184m 3s (remain 297m 41s) Loss: 0.0000(0.0146) Grad: 410.1763  LR: 0.000014  \n",
      "Epoch: [1][14200/36908] Elapsed 185m 19s (remain 296m 19s) Loss: 0.0011(0.0145) Grad: 50917.0391  LR: 0.000014  \n",
      "Epoch: [1][14300/36908] Elapsed 186m 36s (remain 294m 59s) Loss: 0.0000(0.0144) Grad: 10.1633  LR: 0.000014  \n",
      "Epoch: [1][14400/36908] Elapsed 187m 56s (remain 293m 43s) Loss: 0.0369(0.0144) Grad: 587353.3750  LR: 0.000014  \n",
      "Epoch: [1][14500/36908] Elapsed 189m 15s (remain 292m 26s) Loss: 0.0012(0.0143) Grad: 12438.1572  LR: 0.000013  \n",
      "Epoch: [1][14600/36908] Elapsed 190m 34s (remain 291m 9s) Loss: 0.0000(0.0142) Grad: 47.1383  LR: 0.000013  \n",
      "Epoch: [1][14700/36908] Elapsed 191m 54s (remain 289m 53s) Loss: 0.0000(0.0141) Grad: 60.8643  LR: 0.000013  \n",
      "Epoch: [1][14800/36908] Elapsed 193m 12s (remain 288m 34s) Loss: 0.0001(0.0141) Grad: 941.6182  LR: 0.000013  \n",
      "Epoch: [1][14900/36908] Elapsed 194m 29s (remain 287m 14s) Loss: 0.0000(0.0140) Grad: 1527.9415  LR: 0.000013  \n",
      "Epoch: [1][15000/36908] Elapsed 195m 49s (remain 285m 58s) Loss: 0.0000(0.0139) Grad: 141.8680  LR: 0.000013  \n",
      "Epoch: [1][15100/36908] Elapsed 197m 7s (remain 284m 39s) Loss: 0.0006(0.0139) Grad: 21601.0918  LR: 0.000013  \n",
      "Epoch: [1][15200/36908] Elapsed 198m 25s (remain 283m 20s) Loss: 0.0000(0.0138) Grad: 1820.4893  LR: 0.000013  \n",
      "Epoch: [1][15300/36908] Elapsed 199m 43s (remain 282m 2s) Loss: 0.0082(0.0137) Grad: 132561.5000  LR: 0.000013  \n",
      "Epoch: [1][15400/36908] Elapsed 200m 59s (remain 280m 40s) Loss: 0.0000(0.0137) Grad: 22.4296  LR: 0.000013  \n",
      "Epoch: [1][15500/36908] Elapsed 202m 16s (remain 279m 21s) Loss: 0.0000(0.0136) Grad: 644.7150  LR: 0.000013  \n",
      "Epoch: [1][15600/36908] Elapsed 203m 36s (remain 278m 5s) Loss: 0.0002(0.0135) Grad: 2436.3347  LR: 0.000013  \n",
      "Epoch: [1][15700/36908] Elapsed 204m 53s (remain 276m 45s) Loss: 0.0141(0.0135) Grad: 111081.9062  LR: 0.000013  \n",
      "Epoch: [1][15800/36908] Elapsed 206m 10s (remain 275m 24s) Loss: 0.0000(0.0134) Grad: 91.4387  LR: 0.000013  \n",
      "Epoch: [1][15900/36908] Elapsed 207m 33s (remain 274m 12s) Loss: 0.0279(0.0133) Grad: 585398.6875  LR: 0.000013  \n",
      "Epoch: [1][16000/36908] Elapsed 208m 55s (remain 272m 59s) Loss: 0.0285(0.0133) Grad: 665355.9375  LR: 0.000013  \n",
      "Epoch: [1][16100/36908] Elapsed 210m 17s (remain 271m 45s) Loss: 0.0000(0.0132) Grad: 94.3422  LR: 0.000013  \n",
      "Epoch: [1][16200/36908] Elapsed 211m 35s (remain 270m 26s) Loss: 0.0001(0.0131) Grad: 2970.7271  LR: 0.000012  \n",
      "Epoch: [1][16300/36908] Elapsed 212m 52s (remain 269m 6s) Loss: 0.0135(0.0131) Grad: 104545.8672  LR: 0.000012  \n",
      "Epoch: [1][16400/36908] Elapsed 214m 10s (remain 267m 47s) Loss: 0.0021(0.0130) Grad: 34644.6992  LR: 0.000012  \n",
      "Epoch: [1][16500/36908] Elapsed 215m 30s (remain 266m 31s) Loss: 0.0001(0.0129) Grad: 7493.5103  LR: 0.000012  \n",
      "Epoch: [1][16600/36908] Elapsed 216m 47s (remain 265m 10s) Loss: 0.0000(0.0129) Grad: 19.1832  LR: 0.000012  \n",
      "Epoch: [1][16700/36908] Elapsed 218m 3s (remain 263m 49s) Loss: 0.0000(0.0128) Grad: 167.2113  LR: 0.000012  \n",
      "Epoch: [1][16800/36908] Elapsed 219m 22s (remain 262m 32s) Loss: 0.0202(0.0128) Grad: 74626.0000  LR: 0.000012  \n",
      "Epoch: [1][16900/36908] Elapsed 220m 46s (remain 261m 20s) Loss: 0.0093(0.0127) Grad: 148764.2344  LR: 0.000012  \n",
      "Epoch: [1][17000/36908] Elapsed 222m 3s (remain 260m 1s) Loss: 0.0003(0.0127) Grad: 15023.7080  LR: 0.000012  \n",
      "Epoch: [1][17100/36908] Elapsed 223m 19s (remain 258m 40s) Loss: 0.0000(0.0126) Grad: 23.8085  LR: 0.000012  \n",
      "Epoch: [1][17200/36908] Elapsed 224m 36s (remain 257m 20s) Loss: 0.0088(0.0126) Grad: 55479.2344  LR: 0.000012  \n",
      "Epoch: [1][17300/36908] Elapsed 225m 52s (remain 255m 59s) Loss: 0.0003(0.0125) Grad: 9534.8018  LR: 0.000012  \n",
      "Epoch: [1][17400/36908] Elapsed 227m 9s (remain 254m 38s) Loss: 0.0055(0.0125) Grad: 40945.9062  LR: 0.000012  \n",
      "Epoch: [1][17500/36908] Elapsed 228m 26s (remain 253m 19s) Loss: 0.0006(0.0124) Grad: 19114.1875  LR: 0.000012  \n",
      "Epoch: [1][17600/36908] Elapsed 229m 44s (remain 252m 0s) Loss: 0.0021(0.0123) Grad: 65391.5000  LR: 0.000012  \n",
      "Epoch: [1][17700/36908] Elapsed 231m 0s (remain 250m 39s) Loss: 0.0008(0.0123) Grad: 25054.0098  LR: 0.000012  \n",
      "Epoch: [1][17800/36908] Elapsed 232m 17s (remain 249m 19s) Loss: 0.0000(0.0122) Grad: 9.3163  LR: 0.000012  \n",
      "Epoch: [1][17900/36908] Elapsed 233m 33s (remain 247m 59s) Loss: 0.0084(0.0122) Grad: 283520.0938  LR: 0.000011  \n",
      "Epoch: [1][18000/36908] Elapsed 234m 51s (remain 246m 40s) Loss: 0.0049(0.0122) Grad: 53264.0391  LR: 0.000011  \n",
      "Epoch: [1][18100/36908] Elapsed 236m 8s (remain 245m 21s) Loss: 0.0001(0.0121) Grad: 4183.2729  LR: 0.000011  \n",
      "Epoch: [1][18200/36908] Elapsed 237m 29s (remain 244m 5s) Loss: 0.0009(0.0120) Grad: 4824.5664  LR: 0.000011  \n",
      "Epoch: [1][18300/36908] Elapsed 238m 51s (remain 242m 51s) Loss: 0.0000(0.0120) Grad: 179.8930  LR: 0.000011  \n",
      "Epoch: [1][18400/36908] Elapsed 240m 14s (remain 241m 37s) Loss: 0.0000(0.0120) Grad: 34.2055  LR: 0.000011  \n",
      "Epoch: [1][18500/36908] Elapsed 241m 38s (remain 240m 24s) Loss: 0.0001(0.0119) Grad: 5258.9385  LR: 0.000011  \n",
      "Epoch: [1][18600/36908] Elapsed 243m 2s (remain 239m 11s) Loss: 0.0139(0.0119) Grad: 175323.9844  LR: 0.000011  \n",
      "Epoch: [1][18700/36908] Elapsed 244m 25s (remain 237m 58s) Loss: 0.0000(0.0118) Grad: 33.8968  LR: 0.000011  \n",
      "Epoch: [1][18800/36908] Elapsed 245m 48s (remain 236m 44s) Loss: 0.0000(0.0118) Grad: 100.1948  LR: 0.000011  \n",
      "Epoch: [1][18900/36908] Elapsed 247m 13s (remain 235m 31s) Loss: 0.0015(0.0117) Grad: 38404.1758  LR: 0.000011  \n",
      "Epoch: [1][19000/36908] Elapsed 248m 37s (remain 234m 18s) Loss: 0.0047(0.0117) Grad: 57452.5078  LR: 0.000011  \n",
      "Epoch: [1][19100/36908] Elapsed 250m 0s (remain 233m 4s) Loss: 0.0087(0.0116) Grad: 69017.1484  LR: 0.000011  \n",
      "Epoch: [1][19200/36908] Elapsed 251m 24s (remain 231m 50s) Loss: 0.0275(0.0116) Grad: 384351.4062  LR: 0.000011  \n",
      "Epoch: [1][19300/36908] Elapsed 252m 47s (remain 230m 36s) Loss: 0.0000(0.0116) Grad: 446.0834  LR: 0.000011  \n",
      "Epoch: [1][19400/36908] Elapsed 254m 11s (remain 229m 22s) Loss: 0.0000(0.0115) Grad: 58.4094  LR: 0.000011  \n",
      "Epoch: [1][19500/36908] Elapsed 255m 36s (remain 228m 9s) Loss: 0.0000(0.0115) Grad: 337.6900  LR: 0.000010  \n",
      "Epoch: [1][19600/36908] Elapsed 256m 59s (remain 226m 54s) Loss: 0.0034(0.0114) Grad: 69862.7656  LR: 0.000010  \n",
      "Epoch: [1][19700/36908] Elapsed 258m 21s (remain 225m 39s) Loss: 0.0001(0.0114) Grad: 2740.1318  LR: 0.000010  \n",
      "Epoch: [1][19800/36908] Elapsed 259m 45s (remain 224m 24s) Loss: 0.0079(0.0113) Grad: 147927.5000  LR: 0.000010  \n",
      "Epoch: [1][19900/36908] Elapsed 261m 9s (remain 223m 10s) Loss: 0.0000(0.0113) Grad: 341.9470  LR: 0.000010  \n",
      "Epoch: [1][20000/36908] Elapsed 262m 32s (remain 221m 55s) Loss: 0.0000(0.0112) Grad: 6.9493  LR: 0.000010  \n",
      "Epoch: [1][20100/36908] Elapsed 263m 56s (remain 220m 40s) Loss: 0.0004(0.0112) Grad: 8171.4712  LR: 0.000010  \n",
      "Epoch: [1][20200/36908] Elapsed 265m 13s (remain 219m 21s) Loss: 0.0024(0.0112) Grad: 23757.3555  LR: 0.000010  \n",
      "Epoch: [1][20300/36908] Elapsed 266m 31s (remain 218m 1s) Loss: 0.0049(0.0111) Grad: 100807.4375  LR: 0.000010  \n",
      "Epoch: [1][20400/36908] Elapsed 267m 49s (remain 216m 42s) Loss: 0.0294(0.0111) Grad: 136516.7031  LR: 0.000010  \n",
      "Epoch: [1][20500/36908] Elapsed 269m 6s (remain 215m 21s) Loss: 0.0000(0.0111) Grad: 9.6301  LR: 0.000010  \n",
      "Epoch: [1][20600/36908] Elapsed 270m 22s (remain 214m 1s) Loss: 0.0000(0.0110) Grad: 11.2751  LR: 0.000010  \n",
      "Epoch: [1][20700/36908] Elapsed 271m 40s (remain 212m 42s) Loss: 0.0004(0.0110) Grad: 6125.1260  LR: 0.000010  \n",
      "Epoch: [1][20800/36908] Elapsed 272m 57s (remain 211m 21s) Loss: 0.0015(0.0109) Grad: 26024.4570  LR: 0.000010  \n",
      "Epoch: [1][20900/36908] Elapsed 274m 13s (remain 210m 1s) Loss: 0.0009(0.0109) Grad: 9569.6406  LR: 0.000010  \n",
      "Epoch: [1][21000/36908] Elapsed 275m 34s (remain 208m 44s) Loss: 0.0013(0.0109) Grad: 6416.2949  LR: 0.000010  \n",
      "Epoch: [1][21100/36908] Elapsed 276m 52s (remain 207m 24s) Loss: 0.0085(0.0108) Grad: 180948.5312  LR: 0.000010  \n",
      "Epoch: [1][21200/36908] Elapsed 278m 9s (remain 206m 4s) Loss: 0.0116(0.0108) Grad: 63684.1875  LR: 0.000009  \n",
      "Epoch: [1][21300/36908] Elapsed 279m 25s (remain 204m 44s) Loss: 0.0197(0.0108) Grad: 653833.0000  LR: 0.000009  \n",
      "Epoch: [1][21400/36908] Elapsed 280m 43s (remain 203m 24s) Loss: 0.0000(0.0107) Grad: 44.9259  LR: 0.000009  \n",
      "Epoch: [1][21500/36908] Elapsed 282m 0s (remain 202m 4s) Loss: 0.0000(0.0107) Grad: 173.7991  LR: 0.000009  \n",
      "Epoch: [1][21600/36908] Elapsed 283m 16s (remain 200m 44s) Loss: 0.0001(0.0107) Grad: 3195.5994  LR: 0.000009  \n",
      "Epoch: [1][21700/36908] Elapsed 284m 35s (remain 199m 25s) Loss: 0.0001(0.0106) Grad: 1803.4548  LR: 0.000009  \n",
      "Epoch: [1][21800/36908] Elapsed 285m 56s (remain 198m 8s) Loss: 0.0000(0.0106) Grad: 275.5122  LR: 0.000009  \n",
      "Epoch: [1][21900/36908] Elapsed 287m 14s (remain 196m 49s) Loss: 0.0013(0.0105) Grad: 3914.9050  LR: 0.000009  \n",
      "Epoch: [1][22000/36908] Elapsed 288m 31s (remain 195m 29s) Loss: 0.0114(0.0105) Grad: 33806.8320  LR: 0.000009  \n",
      "Epoch: [1][22100/36908] Elapsed 289m 53s (remain 194m 13s) Loss: 0.0408(0.0105) Grad: 265659.9375  LR: 0.000009  \n",
      "Epoch: [1][22200/36908] Elapsed 291m 11s (remain 192m 53s) Loss: 0.0001(0.0104) Grad: 869.1502  LR: 0.000009  \n",
      "Epoch: [1][22300/36908] Elapsed 292m 28s (remain 191m 34s) Loss: 0.0000(0.0104) Grad: 244.5597  LR: 0.000009  \n",
      "Epoch: [1][22400/36908] Elapsed 293m 48s (remain 190m 16s) Loss: 0.0001(0.0104) Grad: 377.3893  LR: 0.000009  \n",
      "Epoch: [1][22500/36908] Elapsed 295m 6s (remain 188m 56s) Loss: 0.0000(0.0103) Grad: 13.5623  LR: 0.000009  \n",
      "Epoch: [1][22600/36908] Elapsed 296m 23s (remain 187m 37s) Loss: 0.0000(0.0103) Grad: 13.1343  LR: 0.000009  \n",
      "Epoch: [1][22700/36908] Elapsed 297m 41s (remain 186m 18s) Loss: 0.0000(0.0103) Grad: 25.6328  LR: 0.000009  \n",
      "Epoch: [1][22800/36908] Elapsed 299m 2s (remain 185m 1s) Loss: 0.0000(0.0102) Grad: 109.1918  LR: 0.000008  \n",
      "Epoch: [1][22900/36908] Elapsed 300m 20s (remain 183m 41s) Loss: 0.0000(0.0102) Grad: 75.6167  LR: 0.000008  \n",
      "Epoch: [1][23000/36908] Elapsed 301m 38s (remain 182m 22s) Loss: 0.0001(0.0102) Grad: 747.1942  LR: 0.000008  \n",
      "Epoch: [1][23100/36908] Elapsed 302m 57s (remain 181m 4s) Loss: 0.0299(0.0101) Grad: 253621.3750  LR: 0.000008  \n",
      "Epoch: [1][23200/36908] Elapsed 304m 15s (remain 179m 45s) Loss: 0.0074(0.0101) Grad: 8254.5791  LR: 0.000008  \n",
      "Epoch: [1][23300/36908] Elapsed 305m 32s (remain 178m 25s) Loss: 0.0000(0.0101) Grad: 35.9317  LR: 0.000008  \n",
      "Epoch: [1][23400/36908] Elapsed 306m 54s (remain 177m 8s) Loss: 0.0056(0.0100) Grad: 33204.1719  LR: 0.000008  \n",
      "Epoch: [1][23500/36908] Elapsed 308m 18s (remain 175m 53s) Loss: 0.0002(0.0100) Grad: 2282.1375  LR: 0.000008  \n",
      "Epoch: [1][23600/36908] Elapsed 309m 41s (remain 174m 36s) Loss: 0.0000(0.0100) Grad: 12.7770  LR: 0.000008  \n",
      "Epoch: [1][23700/36908] Elapsed 311m 0s (remain 173m 18s) Loss: 0.0000(0.0100) Grad: 3.7958  LR: 0.000008  \n",
      "Epoch: [1][23800/36908] Elapsed 312m 17s (remain 171m 58s) Loss: 0.0002(0.0099) Grad: 1331.9966  LR: 0.000008  \n",
      "Epoch: [1][23900/36908] Elapsed 313m 33s (remain 170m 38s) Loss: 0.0000(0.0099) Grad: 102.9259  LR: 0.000008  \n",
      "Epoch: [1][24000/36908] Elapsed 314m 50s (remain 169m 18s) Loss: 0.0135(0.0099) Grad: 30454.3828  LR: 0.000008  \n",
      "Epoch: [1][24100/36908] Elapsed 316m 7s (remain 167m 59s) Loss: 0.0151(0.0098) Grad: 173820.4844  LR: 0.000008  \n",
      "Epoch: [1][24200/36908] Elapsed 317m 23s (remain 166m 39s) Loss: 0.0003(0.0098) Grad: 2309.0127  LR: 0.000008  \n",
      "Epoch: [1][24300/36908] Elapsed 318m 39s (remain 165m 19s) Loss: 0.0000(0.0098) Grad: 106.1457  LR: 0.000008  \n",
      "Epoch: [1][24400/36908] Elapsed 320m 1s (remain 164m 1s) Loss: 0.0030(0.0098) Grad: 21669.3398  LR: 0.000008  \n",
      "Epoch: [1][24500/36908] Elapsed 321m 18s (remain 162m 42s) Loss: 0.0000(0.0097) Grad: 26.8762  LR: 0.000007  \n",
      "Epoch: [1][24600/36908] Elapsed 322m 36s (remain 161m 23s) Loss: 0.0000(0.0097) Grad: 91.2759  LR: 0.000007  \n",
      "Epoch: [1][24700/36908] Elapsed 323m 55s (remain 160m 4s) Loss: 0.0001(0.0097) Grad: 514.4761  LR: 0.000007  \n",
      "Epoch: [1][24800/36908] Elapsed 325m 11s (remain 158m 44s) Loss: 0.0037(0.0097) Grad: 12645.5703  LR: 0.000007  \n",
      "Epoch: [1][24900/36908] Elapsed 326m 26s (remain 157m 24s) Loss: 0.0000(0.0096) Grad: 7.3870  LR: 0.000007  \n",
      "Epoch: [1][25000/36908] Elapsed 327m 44s (remain 156m 5s) Loss: 0.0008(0.0096) Grad: 839.5557  LR: 0.000007  \n",
      "Epoch: [1][25100/36908] Elapsed 329m 2s (remain 154m 46s) Loss: 0.0000(0.0096) Grad: 42.7907  LR: 0.000007  \n",
      "Epoch: [1][25200/36908] Elapsed 330m 20s (remain 153m 27s) Loss: 0.0064(0.0096) Grad: 27291.0879  LR: 0.000007  \n",
      "Epoch: [1][25300/36908] Elapsed 331m 39s (remain 152m 8s) Loss: 0.0034(0.0095) Grad: 22451.2363  LR: 0.000007  \n",
      "Epoch: [1][25400/36908] Elapsed 332m 56s (remain 150m 49s) Loss: 0.0000(0.0095) Grad: 12.3047  LR: 0.000007  \n",
      "Epoch: [1][25500/36908] Elapsed 334m 14s (remain 149m 30s) Loss: 0.0122(0.0095) Grad: 22330.6074  LR: 0.000007  \n",
      "Epoch: [1][25600/36908] Elapsed 335m 33s (remain 148m 12s) Loss: 0.0000(0.0095) Grad: 189.9300  LR: 0.000007  \n",
      "Epoch: [1][25700/36908] Elapsed 336m 50s (remain 146m 52s) Loss: 0.0000(0.0094) Grad: 28.9141  LR: 0.000007  \n",
      "Epoch: [1][25800/36908] Elapsed 338m 8s (remain 145m 33s) Loss: 0.0072(0.0094) Grad: 220337.2188  LR: 0.000007  \n",
      "Epoch: [1][25900/36908] Elapsed 339m 28s (remain 144m 15s) Loss: 0.0016(0.0094) Grad: 26441.7363  LR: 0.000007  \n",
      "Epoch: [1][26000/36908] Elapsed 340m 47s (remain 142m 57s) Loss: 0.0095(0.0094) Grad: 141740.9219  LR: 0.000007  \n",
      "Epoch: [1][26100/36908] Elapsed 342m 4s (remain 141m 38s) Loss: 0.0070(0.0093) Grad: 19034.6152  LR: 0.000007  \n",
      "Epoch: [1][26200/36908] Elapsed 343m 21s (remain 140m 18s) Loss: 0.0000(0.0093) Grad: 1073.3820  LR: 0.000006  \n",
      "Epoch: [1][26300/36908] Elapsed 344m 38s (remain 138m 59s) Loss: 0.0002(0.0093) Grad: 1717.0818  LR: 0.000006  \n",
      "Epoch: [1][26400/36908] Elapsed 345m 56s (remain 137m 40s) Loss: 0.0000(0.0093) Grad: 42.7679  LR: 0.000006  \n",
      "Epoch: [1][26500/36908] Elapsed 347m 15s (remain 136m 22s) Loss: 0.0000(0.0092) Grad: 298.8201  LR: 0.000006  \n",
      "Epoch: [1][26600/36908] Elapsed 348m 38s (remain 135m 5s) Loss: 0.0054(0.0092) Grad: 54998.1523  LR: 0.000006  \n",
      "Epoch: [1][26700/36908] Elapsed 349m 56s (remain 133m 46s) Loss: 0.0000(0.0092) Grad: 32.8838  LR: 0.000006  \n",
      "Epoch: [1][26800/36908] Elapsed 351m 15s (remain 132m 27s) Loss: 0.0127(0.0092) Grad: 60668.0703  LR: 0.000006  \n",
      "Epoch: [1][26900/36908] Elapsed 352m 35s (remain 131m 9s) Loss: 0.0000(0.0092) Grad: 8.2332  LR: 0.000006  \n",
      "Epoch: [1][27000/36908] Elapsed 353m 53s (remain 129m 50s) Loss: 0.0121(0.0091) Grad: 55543.5352  LR: 0.000006  \n",
      "Epoch: [1][27100/36908] Elapsed 355m 11s (remain 128m 31s) Loss: 0.0184(0.0091) Grad: 124959.3125  LR: 0.000006  \n",
      "Epoch: [1][27200/36908] Elapsed 356m 28s (remain 127m 12s) Loss: 0.0000(0.0091) Grad: 29.0771  LR: 0.000006  \n",
      "Epoch: [1][27300/36908] Elapsed 357m 49s (remain 125m 54s) Loss: 0.0002(0.0091) Grad: 1167.9250  LR: 0.000006  \n",
      "Epoch: [1][27400/36908] Elapsed 359m 7s (remain 124m 35s) Loss: 0.0000(0.0090) Grad: 257.2611  LR: 0.000006  \n",
      "Epoch: [1][27500/36908] Elapsed 360m 24s (remain 123m 16s) Loss: 0.0032(0.0090) Grad: 32653.9297  LR: 0.000006  \n",
      "Epoch: [1][27600/36908] Elapsed 361m 41s (remain 121m 57s) Loss: 0.0000(0.0090) Grad: 18.6992  LR: 0.000006  \n",
      "Epoch: [1][27700/36908] Elapsed 362m 59s (remain 120m 38s) Loss: 0.0000(0.0090) Grad: 16.8792  LR: 0.000006  \n",
      "Epoch: [1][27800/36908] Elapsed 364m 17s (remain 119m 20s) Loss: 0.0000(0.0090) Grad: 2.6280  LR: 0.000005  \n",
      "Epoch: [1][27900/36908] Elapsed 365m 36s (remain 118m 1s) Loss: 0.0047(0.0089) Grad: 28475.8984  LR: 0.000005  \n",
      "Epoch: [1][28000/36908] Elapsed 366m 54s (remain 116m 42s) Loss: 0.0000(0.0089) Grad: 28.2334  LR: 0.000005  \n",
      "Epoch: [1][28100/36908] Elapsed 368m 11s (remain 115m 23s) Loss: 0.0146(0.0089) Grad: 104261.1484  LR: 0.000005  \n",
      "Epoch: [1][28200/36908] Elapsed 369m 33s (remain 114m 6s) Loss: 0.0008(0.0089) Grad: 5414.9263  LR: 0.000005  \n",
      "Epoch: [1][28300/36908] Elapsed 370m 51s (remain 112m 47s) Loss: 0.0000(0.0088) Grad: 21.5106  LR: 0.000005  \n",
      "Epoch: [1][28400/36908] Elapsed 372m 8s (remain 111m 28s) Loss: 0.0464(0.0088) Grad: 83359.4219  LR: 0.000005  \n",
      "Epoch: [1][28500/36908] Elapsed 373m 26s (remain 110m 9s) Loss: 0.0003(0.0088) Grad: 2188.6592  LR: 0.000005  \n",
      "Epoch: [1][28600/36908] Elapsed 374m 43s (remain 108m 50s) Loss: 0.0000(0.0088) Grad: 4.7047  LR: 0.000005  \n",
      "Epoch: [1][28700/36908] Elapsed 375m 58s (remain 107m 30s) Loss: 0.0000(0.0088) Grad: 27.4977  LR: 0.000005  \n",
      "Epoch: [1][28800/36908] Elapsed 377m 16s (remain 106m 11s) Loss: 0.0096(0.0087) Grad: 104476.7031  LR: 0.000005  \n",
      "Epoch: [1][28900/36908] Elapsed 378m 34s (remain 104m 53s) Loss: 0.0009(0.0087) Grad: 3869.7812  LR: 0.000005  \n",
      "Epoch: [1][29000/36908] Elapsed 379m 56s (remain 103m 35s) Loss: 0.0000(0.0087) Grad: 6.1176  LR: 0.000005  \n",
      "Epoch: [1][29100/36908] Elapsed 381m 14s (remain 102m 16s) Loss: 0.0117(0.0087) Grad: 92924.5078  LR: 0.000005  \n",
      "Epoch: [1][29200/36908] Elapsed 382m 31s (remain 100m 57s) Loss: 0.0000(0.0087) Grad: 58.1861  LR: 0.000005  \n",
      "Epoch: [1][29300/36908] Elapsed 383m 48s (remain 99m 38s) Loss: 0.0031(0.0086) Grad: 35177.7070  LR: 0.000005  \n",
      "Epoch: [1][29400/36908] Elapsed 385m 4s (remain 98m 19s) Loss: 0.0002(0.0086) Grad: 1843.9171  LR: 0.000005  \n",
      "Epoch: [1][29500/36908] Elapsed 386m 23s (remain 97m 0s) Loss: 0.0000(0.0086) Grad: 145.6384  LR: 0.000004  \n",
      "Epoch: [1][29600/36908] Elapsed 387m 40s (remain 95m 41s) Loss: 0.0021(0.0086) Grad: 19651.8066  LR: 0.000004  \n",
      "Epoch: [1][29700/36908] Elapsed 388m 59s (remain 94m 23s) Loss: 0.0018(0.0086) Grad: 37926.3945  LR: 0.000004  \n",
      "Epoch: [1][29800/36908] Elapsed 390m 22s (remain 93m 5s) Loss: 0.0000(0.0085) Grad: 638.0759  LR: 0.000004  \n",
      "Epoch: [1][29900/36908] Elapsed 391m 45s (remain 91m 48s) Loss: 0.0000(0.0085) Grad: 22.5205  LR: 0.000004  \n",
      "Epoch: [1][30000/36908] Elapsed 393m 3s (remain 90m 29s) Loss: 0.0000(0.0085) Grad: 26.4950  LR: 0.000004  \n",
      "Epoch: [1][30100/36908] Elapsed 394m 24s (remain 89m 11s) Loss: 0.0025(0.0085) Grad: 162970.5000  LR: 0.000004  \n",
      "Epoch: [1][30200/36908] Elapsed 395m 42s (remain 87m 52s) Loss: 0.0034(0.0085) Grad: 112890.0469  LR: 0.000004  \n",
      "Epoch: [1][30300/36908] Elapsed 397m 4s (remain 86m 34s) Loss: 0.0349(0.0084) Grad: 493055.5312  LR: 0.000004  \n",
      "Epoch: [1][30400/36908] Elapsed 398m 28s (remain 85m 17s) Loss: 0.0014(0.0084) Grad: 63774.5469  LR: 0.000004  \n",
      "Epoch: [1][30500/36908] Elapsed 399m 47s (remain 83m 58s) Loss: 0.0000(0.0084) Grad: 10.3709  LR: 0.000004  \n",
      "Epoch: [1][30600/36908] Elapsed 401m 7s (remain 82m 40s) Loss: 0.0037(0.0084) Grad: 324559.2812  LR: 0.000004  \n",
      "Epoch: [1][30700/36908] Elapsed 402m 23s (remain 81m 21s) Loss: 0.0000(0.0084) Grad: 14.6504  LR: 0.000004  \n",
      "Epoch: [1][30800/36908] Elapsed 403m 38s (remain 80m 1s) Loss: 0.0093(0.0084) Grad: 99362.7500  LR: 0.000004  \n",
      "Epoch: [1][30900/36908] Elapsed 404m 55s (remain 78m 42s) Loss: 0.0014(0.0083) Grad: 23127.4199  LR: 0.000004  \n",
      "Epoch: [1][31000/36908] Elapsed 406m 13s (remain 77m 24s) Loss: 0.0062(0.0083) Grad: 104856.0312  LR: 0.000004  \n",
      "Epoch: [1][31100/36908] Elapsed 407m 30s (remain 76m 5s) Loss: 0.0000(0.0083) Grad: 11.9173  LR: 0.000003  \n",
      "Epoch: [1][31200/36908] Elapsed 408m 46s (remain 74m 46s) Loss: 0.0011(0.0083) Grad: 41791.0781  LR: 0.000003  \n",
      "Epoch: [1][31300/36908] Elapsed 410m 2s (remain 73m 27s) Loss: 0.0143(0.0083) Grad: 117594.9141  LR: 0.000003  \n",
      "Epoch: [1][31400/36908] Elapsed 411m 19s (remain 72m 8s) Loss: 0.0005(0.0082) Grad: 19170.3262  LR: 0.000003  \n",
      "Epoch: [1][31500/36908] Elapsed 412m 38s (remain 70m 49s) Loss: 0.0000(0.0082) Grad: 139.2952  LR: 0.000003  \n",
      "Epoch: [1][31600/36908] Elapsed 413m 56s (remain 69m 30s) Loss: 0.0000(0.0082) Grad: 25.6256  LR: 0.000003  \n",
      "Epoch: [1][31700/36908] Elapsed 415m 15s (remain 68m 12s) Loss: 0.0000(0.0082) Grad: 666.1160  LR: 0.000003  \n",
      "Epoch: [1][31800/36908] Elapsed 416m 32s (remain 66m 53s) Loss: 0.0054(0.0082) Grad: 78316.1094  LR: 0.000003  \n",
      "Epoch: [1][31900/36908] Elapsed 417m 53s (remain 65m 35s) Loss: 0.0000(0.0082) Grad: 42.7560  LR: 0.000003  \n",
      "Epoch: [1][32000/36908] Elapsed 419m 11s (remain 64m 16s) Loss: 0.0000(0.0081) Grad: 65.8701  LR: 0.000003  \n",
      "Epoch: [1][32100/36908] Elapsed 420m 29s (remain 62m 58s) Loss: 0.0103(0.0081) Grad: 25809.1914  LR: 0.000003  \n",
      "Epoch: [1][32200/36908] Elapsed 421m 46s (remain 61m 39s) Loss: 0.0010(0.0081) Grad: 36819.2500  LR: 0.000003  \n",
      "Epoch: [1][32300/36908] Elapsed 423m 8s (remain 60m 21s) Loss: 0.0000(0.0081) Grad: 144.1226  LR: 0.000003  \n",
      "Epoch: [1][32400/36908] Elapsed 424m 26s (remain 59m 2s) Loss: 0.0098(0.0081) Grad: 81967.3125  LR: 0.000003  \n",
      "Epoch: [1][32500/36908] Elapsed 425m 43s (remain 57m 43s) Loss: 0.0004(0.0081) Grad: 9472.1318  LR: 0.000003  \n",
      "Epoch: [1][32600/36908] Elapsed 427m 0s (remain 56m 24s) Loss: 0.0001(0.0080) Grad: 4915.6855  LR: 0.000003  \n",
      "Epoch: [1][32700/36908] Elapsed 428m 18s (remain 55m 6s) Loss: 0.0004(0.0080) Grad: 91240.2578  LR: 0.000003  \n",
      "Epoch: [1][32800/36908] Elapsed 429m 39s (remain 53m 47s) Loss: 0.0000(0.0080) Grad: 949.0314  LR: 0.000002  \n",
      "Epoch: [1][32900/36908] Elapsed 430m 58s (remain 52m 29s) Loss: 0.0018(0.0080) Grad: 32410.5176  LR: 0.000002  \n",
      "Epoch: [1][33000/36908] Elapsed 432m 15s (remain 51m 10s) Loss: 0.0000(0.0080) Grad: 288.2063  LR: 0.000002  \n",
      "Epoch: [1][33100/36908] Elapsed 433m 34s (remain 49m 51s) Loss: 0.0003(0.0080) Grad: 22998.0430  LR: 0.000002  \n",
      "Epoch: [1][33200/36908] Elapsed 434m 53s (remain 48m 33s) Loss: 0.0000(0.0080) Grad: 15.8064  LR: 0.000002  \n",
      "Epoch: [1][33300/36908] Elapsed 436m 15s (remain 47m 15s) Loss: 0.0001(0.0079) Grad: 5467.8008  LR: 0.000002  \n",
      "Epoch: [1][33400/36908] Elapsed 437m 37s (remain 45m 57s) Loss: 0.0000(0.0079) Grad: 182.3297  LR: 0.000002  \n",
      "Epoch: [1][33500/36908] Elapsed 439m 1s (remain 44m 38s) Loss: 0.0000(0.0079) Grad: 321.1671  LR: 0.000002  \n",
      "Epoch: [1][33600/36908] Elapsed 440m 24s (remain 43m 20s) Loss: 0.0000(0.0079) Grad: 3.2098  LR: 0.000002  \n",
      "Epoch: [1][33700/36908] Elapsed 441m 47s (remain 42m 2s) Loss: 0.0000(0.0079) Grad: 531.8574  LR: 0.000002  \n",
      "Epoch: [1][33800/36908] Elapsed 443m 9s (remain 40m 44s) Loss: 0.0000(0.0079) Grad: 1316.2332  LR: 0.000002  \n",
      "Epoch: [1][33900/36908] Elapsed 444m 33s (remain 39m 25s) Loss: 0.0000(0.0078) Grad: 92.5253  LR: 0.000002  \n",
      "Epoch: [1][34000/36908] Elapsed 446m 0s (remain 38m 7s) Loss: 0.0519(0.0078) Grad: 229190.5312  LR: 0.000002  \n",
      "Epoch: [1][34100/36908] Elapsed 447m 23s (remain 36m 49s) Loss: 0.0000(0.0078) Grad: 24.8103  LR: 0.000002  \n",
      "Epoch: [1][34200/36908] Elapsed 448m 46s (remain 35m 31s) Loss: 0.0032(0.0078) Grad: 322063.6562  LR: 0.000002  \n",
      "Epoch: [1][34300/36908] Elapsed 450m 8s (remain 34m 12s) Loss: 0.0001(0.0078) Grad: 6908.9907  LR: 0.000002  \n",
      "Epoch: [1][34400/36908] Elapsed 451m 31s (remain 32m 54s) Loss: 0.0000(0.0078) Grad: 192.4063  LR: 0.000002  \n",
      "Epoch: [1][34500/36908] Elapsed 452m 56s (remain 31m 36s) Loss: 0.0000(0.0078) Grad: 188.8735  LR: 0.000001  \n",
      "Epoch: [1][34600/36908] Elapsed 454m 20s (remain 30m 17s) Loss: 0.0000(0.0077) Grad: 1283.4447  LR: 0.000001  \n",
      "Epoch: [1][34700/36908] Elapsed 455m 42s (remain 28m 59s) Loss: 0.0000(0.0077) Grad: 431.4969  LR: 0.000001  \n",
      "Epoch: [1][34800/36908] Elapsed 457m 5s (remain 27m 40s) Loss: 0.0000(0.0077) Grad: 24.5112  LR: 0.000001  \n",
      "Epoch: [1][34900/36908] Elapsed 458m 27s (remain 26m 21s) Loss: 0.0000(0.0077) Grad: 509.9855  LR: 0.000001  \n",
      "Epoch: [1][35000/36908] Elapsed 459m 51s (remain 25m 3s) Loss: 0.0017(0.0077) Grad: 34048.9453  LR: 0.000001  \n",
      "Epoch: [1][35100/36908] Elapsed 461m 16s (remain 23m 44s) Loss: 0.0009(0.0077) Grad: 30391.5840  LR: 0.000001  \n",
      "Epoch: [1][35200/36908] Elapsed 462m 39s (remain 22m 26s) Loss: 0.0211(0.0077) Grad: 28451.9727  LR: 0.000001  \n",
      "Epoch: [1][35300/36908] Elapsed 464m 1s (remain 21m 7s) Loss: 0.0014(0.0076) Grad: 139610.4219  LR: 0.000001  \n",
      "Epoch: [1][35400/36908] Elapsed 465m 19s (remain 19m 48s) Loss: 0.0066(0.0076) Grad: 77100.7812  LR: 0.000001  \n",
      "Epoch: [1][35500/36908] Elapsed 466m 36s (remain 18m 29s) Loss: 0.0117(0.0076) Grad: 55867.9453  LR: 0.000001  \n",
      "Epoch: [1][35600/36908] Elapsed 467m 54s (remain 17m 10s) Loss: 0.0001(0.0076) Grad: 691.9588  LR: 0.000001  \n",
      "Epoch: [1][35700/36908] Elapsed 469m 11s (remain 15m 51s) Loss: 0.0114(0.0076) Grad: 57094.5859  LR: 0.000001  \n",
      "Epoch: [1][35800/36908] Elapsed 470m 28s (remain 14m 32s) Loss: 0.0027(0.0076) Grad: 21079.1543  LR: 0.000001  \n",
      "Epoch: [1][35900/36908] Elapsed 471m 45s (remain 13m 13s) Loss: 0.0000(0.0076) Grad: 11.1037  LR: 0.000001  \n",
      "Epoch: [1][36000/36908] Elapsed 473m 3s (remain 11m 55s) Loss: 0.0000(0.0075) Grad: 141.1562  LR: 0.000001  \n",
      "Epoch: [1][36100/36908] Elapsed 474m 19s (remain 10m 36s) Loss: 0.0000(0.0075) Grad: 31.2435  LR: 0.000000  \n",
      "Epoch: [1][36200/36908] Elapsed 475m 36s (remain 9m 17s) Loss: 0.0001(0.0075) Grad: 398.6662  LR: 0.000000  \n",
      "Epoch: [1][36300/36908] Elapsed 476m 53s (remain 7m 58s) Loss: 0.0000(0.0075) Grad: 6.9111  LR: 0.000000  \n",
      "Epoch: [1][36400/36908] Elapsed 478m 11s (remain 6m 39s) Loss: 0.0005(0.0075) Grad: 11333.7852  LR: 0.000000  \n",
      "Epoch: [1][36500/36908] Elapsed 479m 26s (remain 5m 20s) Loss: 0.0006(0.0075) Grad: 35150.7227  LR: 0.000000  \n",
      "Epoch: [1][36600/36908] Elapsed 480m 43s (remain 4m 1s) Loss: 0.0148(0.0075) Grad: 13099.4561  LR: 0.000000  \n",
      "Epoch: [1][36700/36908] Elapsed 482m 1s (remain 2m 43s) Loss: 0.0002(0.0074) Grad: 10417.1406  LR: 0.000000  \n",
      "Epoch: [1][36800/36908] Elapsed 483m 20s (remain 1m 24s) Loss: 0.0000(0.0074) Grad: 11.2284  LR: 0.000000  \n",
      "Epoch: [1][36900/36908] Elapsed 484m 38s (remain 0m 5s) Loss: 0.0039(0.0074) Grad: 17235.9180  LR: 0.000000  \n",
      "Epoch: [1][36907/36908] Elapsed 484m 44s (remain 0m 0s) Loss: 0.0001(0.0074) Grad: 2615.0977  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 19m 21s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 31s (remain 5m 36s) Loss: 0.0006(0.0084) \n",
      "EVAL: [200/1192] Elapsed 1m 1s (remain 5m 2s) Loss: 0.0000(0.0096) \n",
      "EVAL: [300/1192] Elapsed 1m 31s (remain 4m 30s) Loss: 0.0015(0.0148) \n",
      "EVAL: [400/1192] Elapsed 2m 1s (remain 3m 58s) Loss: 0.0436(0.0154) \n",
      "EVAL: [500/1192] Elapsed 2m 31s (remain 3m 28s) Loss: 0.0433(0.0142) \n",
      "EVAL: [600/1192] Elapsed 3m 2s (remain 2m 59s) Loss: 0.1950(0.0142) \n",
      "EVAL: [700/1192] Elapsed 3m 33s (remain 2m 29s) Loss: 0.0101(0.0165) \n",
      "EVAL: [800/1192] Elapsed 4m 6s (remain 2m 0s) Loss: 0.0064(0.0162) \n",
      "EVAL: [900/1192] Elapsed 4m 42s (remain 1m 31s) Loss: 0.0010(0.0156) \n",
      "EVAL: [1000/1192] Elapsed 5m 12s (remain 0m 59s) Loss: 0.0000(0.0149) \n",
      "EVAL: [1100/1192] Elapsed 5m 47s (remain 0m 28s) Loss: 0.0088(0.0142) \n",
      "EVAL: [1191/1192] Elapsed 6m 15s (remain 0m 0s) Loss: 0.0148(0.0134) \n",
      "Epoch 1 - avg_train_loss: 0.0074  avg_val_loss: 0.0134  time: 29464s\n",
      "Epoch 1 - Score: 0.8860\n",
      "Epoch 1 - Save Best Score: 0.8860 Model\n",
      "========== fold: 2 training ==========\n",
      "get pseudo plain from ../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_2.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_2.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp083/pseudo_labels_2.npy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb546146be394d299176bcc89b12dcfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6dc4efcc2b43e3a293de4a71e09841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(612602, 950)\n",
      "(612602, 6) (612602, 950)\n",
      "(100000, 7)\n",
      "(110725, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\n",
      "Epoch: [1][0/36908] Elapsed 0m 1s (remain 871m 9s) Loss: 0.3962(0.3962) Grad: 148616.3594  LR: 0.000000  \n",
      "Epoch: [1][100/36908] Elapsed 1m 18s (remain 477m 9s) Loss: 0.3696(0.3841) Grad: 143464.2812  LR: 0.000001  \n",
      "Epoch: [1][200/36908] Elapsed 2m 35s (remain 473m 14s) Loss: 0.2622(0.3514) Grad: 111898.1016  LR: 0.000001  \n",
      "Epoch: [1][300/36908] Elapsed 3m 52s (remain 471m 37s) Loss: 0.1530(0.3040) Grad: 70491.3047  LR: 0.000002  \n",
      "Epoch: [1][400/36908] Elapsed 5m 11s (remain 472m 5s) Loss: 0.0663(0.2535) Grad: 27429.1191  LR: 0.000002  \n",
      "Epoch: [1][500/36908] Elapsed 6m 28s (remain 470m 32s) Loss: 0.0297(0.2131) Grad: 5703.1504  LR: 0.000003  \n",
      "Epoch: [1][600/36908] Elapsed 7m 51s (remain 474m 38s) Loss: 0.0345(0.1853) Grad: 4583.4756  LR: 0.000003  \n",
      "Epoch: [1][700/36908] Elapsed 9m 16s (remain 479m 18s) Loss: 0.0751(0.1651) Grad: 9768.3955  LR: 0.000004  \n",
      "Epoch: [1][800/36908] Elapsed 10m 37s (remain 478m 58s) Loss: 0.0274(0.1496) Grad: 5711.5127  LR: 0.000004  \n",
      "Epoch: [1][900/36908] Elapsed 11m 55s (remain 476m 24s) Loss: 0.0834(0.1375) Grad: 16328.2529  LR: 0.000005  \n",
      "Epoch: [1][1000/36908] Elapsed 13m 11s (remain 473m 29s) Loss: 0.0165(0.1278) Grad: 4934.2227  LR: 0.000005  \n",
      "Epoch: [1][1100/36908] Elapsed 14m 28s (remain 470m 52s) Loss: 0.0386(0.1196) Grad: 10499.9404  LR: 0.000006  \n",
      "Epoch: [1][1200/36908] Elapsed 15m 45s (remain 468m 24s) Loss: 0.0146(0.1124) Grad: 31807.4453  LR: 0.000007  \n",
      "Epoch: [1][1300/36908] Elapsed 17m 2s (remain 466m 34s) Loss: 0.0568(0.1060) Grad: 52284.2930  LR: 0.000007  \n",
      "Epoch: [1][1400/36908] Elapsed 18m 19s (remain 464m 33s) Loss: 0.0080(0.0998) Grad: 11781.7021  LR: 0.000008  \n",
      "Epoch: [1][1500/36908] Elapsed 19m 36s (remain 462m 28s) Loss: 0.0095(0.0944) Grad: 46962.7500  LR: 0.000008  \n",
      "Epoch: [1][1600/36908] Elapsed 20m 52s (remain 460m 23s) Loss: 0.0040(0.0898) Grad: 7785.6499  LR: 0.000009  \n",
      "Epoch: [1][1700/36908] Elapsed 22m 9s (remain 458m 32s) Loss: 0.0013(0.0855) Grad: 1643.9207  LR: 0.000009  \n",
      "Epoch: [1][1800/36908] Elapsed 23m 31s (remain 458m 32s) Loss: 0.0042(0.0816) Grad: 26321.5469  LR: 0.000010  \n",
      "Epoch: [1][1900/36908] Elapsed 24m 55s (remain 458m 54s) Loss: 0.0030(0.0780) Grad: 10519.9023  LR: 0.000010  \n",
      "Epoch: [1][2000/36908] Elapsed 26m 11s (remain 456m 57s) Loss: 0.0031(0.0748) Grad: 26780.7324  LR: 0.000011  \n",
      "Epoch: [1][2100/36908] Elapsed 27m 28s (remain 455m 3s) Loss: 0.0054(0.0718) Grad: 2212.0518  LR: 0.000011  \n",
      "Epoch: [1][2200/36908] Elapsed 28m 45s (remain 453m 32s) Loss: 0.0389(0.0691) Grad: 71142.2500  LR: 0.000012  \n",
      "Epoch: [1][2300/36908] Elapsed 30m 4s (remain 452m 23s) Loss: 0.0044(0.0666) Grad: 84139.8672  LR: 0.000012  \n",
      "Epoch: [1][2400/36908] Elapsed 31m 21s (remain 450m 37s) Loss: 0.0330(0.0644) Grad: 63291.6172  LR: 0.000013  \n",
      "Epoch: [1][2500/36908] Elapsed 32m 38s (remain 449m 3s) Loss: 0.0255(0.0623) Grad: 86799.8828  LR: 0.000014  \n",
      "Epoch: [1][2600/36908] Elapsed 33m 57s (remain 447m 54s) Loss: 0.0163(0.0603) Grad: 65739.3906  LR: 0.000014  \n",
      "Epoch: [1][2700/36908] Elapsed 35m 14s (remain 446m 17s) Loss: 0.0022(0.0585) Grad: 32330.2109  LR: 0.000015  \n",
      "Epoch: [1][2800/36908] Elapsed 36m 29s (remain 444m 25s) Loss: 0.0000(0.0568) Grad: 202.4860  LR: 0.000015  \n",
      "Epoch: [1][2900/36908] Elapsed 37m 45s (remain 442m 36s) Loss: 0.0108(0.0552) Grad: 7780.4102  LR: 0.000016  \n",
      "Epoch: [1][3000/36908] Elapsed 39m 2s (remain 441m 3s) Loss: 0.0133(0.0537) Grad: 111937.2500  LR: 0.000016  \n",
      "Epoch: [1][3100/36908] Elapsed 40m 19s (remain 439m 31s) Loss: 0.0005(0.0523) Grad: 2637.8398  LR: 0.000017  \n",
      "Epoch: [1][3200/36908] Elapsed 41m 38s (remain 438m 31s) Loss: 0.0309(0.0509) Grad: 15245.0449  LR: 0.000017  \n",
      "Epoch: [1][3300/36908] Elapsed 42m 56s (remain 437m 12s) Loss: 0.0025(0.0497) Grad: 11004.6611  LR: 0.000018  \n",
      "Epoch: [1][3400/36908] Elapsed 44m 13s (remain 435m 39s) Loss: 0.0410(0.0485) Grad: 117943.4609  LR: 0.000018  \n",
      "Epoch: [1][3500/36908] Elapsed 45m 30s (remain 434m 16s) Loss: 0.0865(0.0475) Grad: 73412.9062  LR: 0.000019  \n",
      "Epoch: [1][3600/36908] Elapsed 46m 49s (remain 433m 3s) Loss: 0.0083(0.0464) Grad: 27529.4180  LR: 0.000020  \n",
      "Epoch: [1][3700/36908] Elapsed 48m 8s (remain 431m 59s) Loss: 0.0033(0.0455) Grad: 41203.5859  LR: 0.000020  \n",
      "Epoch: [1][3800/36908] Elapsed 49m 30s (remain 431m 10s) Loss: 0.0020(0.0445) Grad: 19890.1094  LR: 0.000020  \n",
      "Epoch: [1][3900/36908] Elapsed 50m 46s (remain 429m 39s) Loss: 0.0016(0.0436) Grad: 3031.6758  LR: 0.000020  \n",
      "Epoch: [1][4000/36908] Elapsed 52m 3s (remain 428m 9s) Loss: 0.0128(0.0428) Grad: 251499.9062  LR: 0.000020  \n",
      "Epoch: [1][4100/36908] Elapsed 53m 20s (remain 426m 46s) Loss: 0.0198(0.0420) Grad: 74601.9453  LR: 0.000020  \n",
      "Epoch: [1][4200/36908] Elapsed 54m 41s (remain 425m 45s) Loss: 0.0005(0.0411) Grad: 5750.8965  LR: 0.000020  \n",
      "Epoch: [1][4300/36908] Elapsed 56m 0s (remain 424m 38s) Loss: 0.0068(0.0403) Grad: 71131.7500  LR: 0.000020  \n",
      "Epoch: [1][4400/36908] Elapsed 57m 17s (remain 423m 8s) Loss: 0.0001(0.0396) Grad: 117.6252  LR: 0.000020  \n",
      "Epoch: [1][4500/36908] Elapsed 58m 34s (remain 421m 42s) Loss: 0.0112(0.0389) Grad: 21752.4434  LR: 0.000020  \n",
      "Epoch: [1][4600/36908] Elapsed 59m 52s (remain 420m 27s) Loss: 0.0005(0.0383) Grad: 915.0586  LR: 0.000019  \n",
      "Epoch: [1][4700/36908] Elapsed 61m 11s (remain 419m 13s) Loss: 0.0003(0.0376) Grad: 788.5002  LR: 0.000019  \n",
      "Epoch: [1][4800/36908] Elapsed 62m 28s (remain 417m 45s) Loss: 0.0097(0.0370) Grad: 400747.0625  LR: 0.000019  \n",
      "Epoch: [1][4900/36908] Elapsed 63m 46s (remain 416m 32s) Loss: 0.0004(0.0364) Grad: 1446.1556  LR: 0.000019  \n",
      "Epoch: [1][5000/36908] Elapsed 65m 4s (remain 415m 10s) Loss: 0.0050(0.0359) Grad: 68293.3047  LR: 0.000019  \n",
      "Epoch: [1][5100/36908] Elapsed 66m 24s (remain 414m 5s) Loss: 0.0115(0.0353) Grad: 61909.2344  LR: 0.000019  \n",
      "Epoch: [1][5200/36908] Elapsed 67m 41s (remain 412m 38s) Loss: 0.0143(0.0348) Grad: 68226.5156  LR: 0.000019  \n",
      "Epoch: [1][5300/36908] Elapsed 68m 57s (remain 411m 10s) Loss: 0.0128(0.0343) Grad: 390159.5312  LR: 0.000019  \n",
      "Epoch: [1][5400/36908] Elapsed 70m 14s (remain 409m 46s) Loss: 0.0079(0.0338) Grad: 526351.2500  LR: 0.000019  \n",
      "Epoch: [1][5500/36908] Elapsed 71m 33s (remain 408m 30s) Loss: 0.0007(0.0333) Grad: 8772.1074  LR: 0.000019  \n",
      "Epoch: [1][5600/36908] Elapsed 72m 51s (remain 407m 16s) Loss: 0.0023(0.0328) Grad: 45227.3906  LR: 0.000019  \n",
      "Epoch: [1][5700/36908] Elapsed 74m 9s (remain 405m 57s) Loss: 0.0019(0.0324) Grad: 48143.4258  LR: 0.000019  \n",
      "Epoch: [1][5800/36908] Elapsed 75m 27s (remain 404m 37s) Loss: 0.0003(0.0320) Grad: 1605.6558  LR: 0.000019  \n",
      "Epoch: [1][5900/36908] Elapsed 76m 44s (remain 403m 16s) Loss: 0.0166(0.0316) Grad: 147489.4688  LR: 0.000019  \n",
      "Epoch: [1][6000/36908] Elapsed 78m 2s (remain 401m 56s) Loss: 0.0001(0.0312) Grad: 259.7774  LR: 0.000019  \n",
      "Epoch: [1][6100/36908] Elapsed 79m 21s (remain 400m 42s) Loss: 0.0002(0.0308) Grad: 342.1107  LR: 0.000019  \n",
      "Epoch: [1][6200/36908] Elapsed 80m 41s (remain 399m 32s) Loss: 0.0006(0.0303) Grad: 1593.5236  LR: 0.000018  \n",
      "Epoch: [1][6300/36908] Elapsed 81m 58s (remain 398m 12s) Loss: 0.0017(0.0300) Grad: 8616.5518  LR: 0.000018  \n",
      "Epoch: [1][6400/36908] Elapsed 83m 16s (remain 396m 52s) Loss: 0.0002(0.0296) Grad: 269.5599  LR: 0.000018  \n",
      "Epoch: [1][6500/36908] Elapsed 84m 34s (remain 395m 34s) Loss: 0.0001(0.0292) Grad: 217.0266  LR: 0.000018  \n",
      "Epoch: [1][6600/36908] Elapsed 85m 53s (remain 394m 19s) Loss: 0.0001(0.0289) Grad: 158.1795  LR: 0.000018  \n",
      "Epoch: [1][6700/36908] Elapsed 87m 12s (remain 393m 8s) Loss: 0.0029(0.0286) Grad: 39346.9102  LR: 0.000018  \n",
      "Epoch: [1][6800/36908] Elapsed 88m 34s (remain 392m 5s) Loss: 0.0080(0.0283) Grad: 54981.3359  LR: 0.000018  \n",
      "Epoch: [1][6900/36908] Elapsed 89m 52s (remain 390m 46s) Loss: 0.0008(0.0279) Grad: 13011.9551  LR: 0.000018  \n",
      "Epoch: [1][7000/36908] Elapsed 91m 12s (remain 389m 36s) Loss: 0.0000(0.0276) Grad: 41.3788  LR: 0.000018  \n",
      "Epoch: [1][7100/36908] Elapsed 92m 36s (remain 388m 43s) Loss: 0.0002(0.0274) Grad: 914.4917  LR: 0.000018  \n",
      "Epoch: [1][7200/36908] Elapsed 93m 53s (remain 387m 18s) Loss: 0.0001(0.0270) Grad: 152.5757  LR: 0.000018  \n",
      "Epoch: [1][7300/36908] Elapsed 95m 11s (remain 386m 0s) Loss: 0.0001(0.0268) Grad: 957.1700  LR: 0.000018  \n",
      "Epoch: [1][7400/36908] Elapsed 96m 30s (remain 384m 46s) Loss: 0.0032(0.0265) Grad: 87710.6016  LR: 0.000018  \n",
      "Epoch: [1][7500/36908] Elapsed 97m 48s (remain 383m 25s) Loss: 0.0116(0.0262) Grad: 66152.7656  LR: 0.000018  \n",
      "Epoch: [1][7600/36908] Elapsed 99m 6s (remain 382m 6s) Loss: 0.0195(0.0259) Grad: 13313.5361  LR: 0.000018  \n",
      "Epoch: [1][7700/36908] Elapsed 100m 24s (remain 380m 49s) Loss: 0.0018(0.0257) Grad: 83147.5547  LR: 0.000018  \n",
      "Epoch: [1][7800/36908] Elapsed 101m 42s (remain 379m 29s) Loss: 0.0070(0.0254) Grad: 49353.9766  LR: 0.000018  \n",
      "Epoch: [1][7900/36908] Elapsed 102m 59s (remain 378m 8s) Loss: 0.0000(0.0252) Grad: 56.3916  LR: 0.000017  \n",
      "Epoch: [1][8000/36908] Elapsed 104m 18s (remain 376m 51s) Loss: 0.0059(0.0249) Grad: 119029.0859  LR: 0.000017  \n",
      "Epoch: [1][8100/36908] Elapsed 105m 38s (remain 375m 38s) Loss: 0.0161(0.0247) Grad: 377890.1562  LR: 0.000017  \n",
      "Epoch: [1][8200/36908] Elapsed 106m 55s (remain 374m 18s) Loss: 0.0126(0.0245) Grad: 121323.3672  LR: 0.000017  \n",
      "Epoch: [1][8300/36908] Elapsed 108m 14s (remain 373m 2s) Loss: 0.0023(0.0242) Grad: 54167.2656  LR: 0.000017  \n",
      "Epoch: [1][8400/36908] Elapsed 109m 35s (remain 371m 53s) Loss: 0.0206(0.0240) Grad: 171752.6094  LR: 0.000017  \n",
      "Epoch: [1][8500/36908] Elapsed 110m 59s (remain 370m 53s) Loss: 0.0076(0.0238) Grad: 138204.2031  LR: 0.000017  \n",
      "Epoch: [1][8600/36908] Elapsed 112m 17s (remain 369m 33s) Loss: 0.0000(0.0236) Grad: 96.9286  LR: 0.000017  \n",
      "Epoch: [1][8700/36908] Elapsed 113m 35s (remain 368m 13s) Loss: 0.0127(0.0234) Grad: 88339.8281  LR: 0.000017  \n",
      "Epoch: [1][8800/36908] Elapsed 114m 52s (remain 366m 52s) Loss: 0.0001(0.0232) Grad: 234.2818  LR: 0.000017  \n",
      "Epoch: [1][8900/36908] Elapsed 116m 11s (remain 365m 36s) Loss: 0.0026(0.0230) Grad: 238363.1250  LR: 0.000017  \n",
      "Epoch: [1][9000/36908] Elapsed 117m 29s (remain 364m 16s) Loss: 0.0026(0.0228) Grad: 53181.3828  LR: 0.000017  \n",
      "Epoch: [1][9100/36908] Elapsed 118m 47s (remain 362m 56s) Loss: 0.0009(0.0226) Grad: 32265.3359  LR: 0.000017  \n",
      "Epoch: [1][9200/36908] Elapsed 120m 4s (remain 361m 36s) Loss: 0.0001(0.0224) Grad: 132.2393  LR: 0.000017  \n",
      "Epoch: [1][9300/36908] Elapsed 121m 22s (remain 360m 16s) Loss: 0.0045(0.0222) Grad: 117418.7812  LR: 0.000017  \n",
      "Epoch: [1][9400/36908] Elapsed 122m 40s (remain 358m 57s) Loss: 0.0000(0.0220) Grad: 70.5026  LR: 0.000017  \n",
      "Epoch: [1][9500/36908] Elapsed 124m 3s (remain 357m 53s) Loss: 0.0116(0.0219) Grad: 105894.6094  LR: 0.000017  \n",
      "Epoch: [1][9600/36908] Elapsed 125m 21s (remain 356m 31s) Loss: 0.0002(0.0217) Grad: 6550.6094  LR: 0.000016  \n",
      "Epoch: [1][9700/36908] Elapsed 126m 37s (remain 355m 8s) Loss: 0.0005(0.0215) Grad: 7096.7739  LR: 0.000016  \n",
      "Epoch: [1][9800/36908] Elapsed 127m 54s (remain 353m 44s) Loss: 0.0007(0.0213) Grad: 2768.3481  LR: 0.000016  \n",
      "Epoch: [1][9900/36908] Elapsed 129m 11s (remain 352m 23s) Loss: 0.0001(0.0212) Grad: 365.5387  LR: 0.000016  \n",
      "Epoch: [1][10000/36908] Elapsed 130m 27s (remain 350m 59s) Loss: 0.0059(0.0210) Grad: 31016.4609  LR: 0.000016  \n",
      "Epoch: [1][10100/36908] Elapsed 131m 44s (remain 349m 37s) Loss: 0.0091(0.0208) Grad: 25193.0000  LR: 0.000016  \n",
      "Epoch: [1][10200/36908] Elapsed 133m 2s (remain 348m 17s) Loss: 0.0000(0.0207) Grad: 156.7466  LR: 0.000016  \n",
      "Epoch: [1][10300/36908] Elapsed 134m 20s (remain 346m 58s) Loss: 0.0000(0.0205) Grad: 136.4292  LR: 0.000016  \n",
      "Epoch: [1][10400/36908] Elapsed 135m 41s (remain 345m 49s) Loss: 0.0000(0.0204) Grad: 72.9221  LR: 0.000016  \n",
      "Epoch: [1][10500/36908] Elapsed 136m 58s (remain 344m 26s) Loss: 0.0005(0.0202) Grad: 11061.4980  LR: 0.000016  \n",
      "Epoch: [1][10600/36908] Elapsed 138m 14s (remain 343m 2s) Loss: 0.0007(0.0201) Grad: 1277.0605  LR: 0.000016  \n",
      "Epoch: [1][10700/36908] Elapsed 139m 33s (remain 341m 46s) Loss: 0.0001(0.0200) Grad: 475.8148  LR: 0.000016  \n",
      "Epoch: [1][10800/36908] Elapsed 140m 50s (remain 340m 24s) Loss: 0.0000(0.0198) Grad: 283.3879  LR: 0.000016  \n",
      "Epoch: [1][10900/36908] Elapsed 142m 6s (remain 339m 3s) Loss: 0.0005(0.0197) Grad: 41223.9102  LR: 0.000016  \n",
      "Epoch: [1][11000/36908] Elapsed 143m 24s (remain 337m 42s) Loss: 0.0269(0.0195) Grad: 260826.6562  LR: 0.000016  \n",
      "Epoch: [1][11100/36908] Elapsed 144m 43s (remain 336m 27s) Loss: 0.0000(0.0194) Grad: 30.0414  LR: 0.000016  \n",
      "Epoch: [1][11200/36908] Elapsed 146m 2s (remain 335m 9s) Loss: 0.0217(0.0193) Grad: 293166.0625  LR: 0.000015  \n",
      "Epoch: [1][11300/36908] Elapsed 147m 19s (remain 333m 50s) Loss: 0.0004(0.0191) Grad: 5039.9150  LR: 0.000015  \n",
      "Epoch: [1][11400/36908] Elapsed 148m 37s (remain 332m 30s) Loss: 0.0035(0.0190) Grad: 28740.2031  LR: 0.000015  \n",
      "Epoch: [1][11500/36908] Elapsed 149m 55s (remain 331m 10s) Loss: 0.0000(0.0189) Grad: 167.4584  LR: 0.000015  \n",
      "Epoch: [1][11600/36908] Elapsed 151m 14s (remain 329m 54s) Loss: 0.0001(0.0188) Grad: 187.5464  LR: 0.000015  \n",
      "Epoch: [1][11700/36908] Elapsed 152m 37s (remain 328m 48s) Loss: 0.0001(0.0187) Grad: 748.1405  LR: 0.000015  \n",
      "Epoch: [1][11800/36908] Elapsed 153m 59s (remain 327m 38s) Loss: 0.0075(0.0185) Grad: 68044.0391  LR: 0.000015  \n",
      "Epoch: [1][11900/36908] Elapsed 155m 18s (remain 326m 20s) Loss: 0.0000(0.0184) Grad: 71.0243  LR: 0.000015  \n",
      "Epoch: [1][12000/36908] Elapsed 156m 36s (remain 325m 0s) Loss: 0.0011(0.0183) Grad: 15783.5791  LR: 0.000015  \n",
      "Epoch: [1][12100/36908] Elapsed 157m 53s (remain 323m 41s) Loss: 0.0001(0.0182) Grad: 1414.0092  LR: 0.000015  \n",
      "Epoch: [1][12200/36908] Elapsed 159m 18s (remain 322m 36s) Loss: 0.0015(0.0181) Grad: 20481.8945  LR: 0.000015  \n",
      "Epoch: [1][12300/36908] Elapsed 160m 41s (remain 321m 26s) Loss: 0.0264(0.0180) Grad: 133090.8906  LR: 0.000015  \n",
      "Epoch: [1][12400/36908] Elapsed 162m 4s (remain 320m 16s) Loss: 0.0042(0.0179) Grad: 25409.9180  LR: 0.000015  \n",
      "Epoch: [1][12500/36908] Elapsed 163m 26s (remain 319m 6s) Loss: 0.0058(0.0178) Grad: 61376.9766  LR: 0.000015  \n",
      "Epoch: [1][12600/36908] Elapsed 164m 48s (remain 317m 55s) Loss: 0.0000(0.0177) Grad: 65.7892  LR: 0.000015  \n",
      "Epoch: [1][12700/36908] Elapsed 166m 12s (remain 316m 47s) Loss: 0.0281(0.0176) Grad: 304710.9688  LR: 0.000015  \n",
      "Epoch: [1][12800/36908] Elapsed 167m 36s (remain 315m 38s) Loss: 0.0074(0.0175) Grad: 118707.9766  LR: 0.000015  \n",
      "Epoch: [1][12900/36908] Elapsed 168m 59s (remain 314m 27s) Loss: 0.0006(0.0174) Grad: 4989.8086  LR: 0.000014  \n",
      "Epoch: [1][13000/36908] Elapsed 170m 23s (remain 313m 19s) Loss: 0.0000(0.0173) Grad: 71.5787  LR: 0.000014  \n",
      "Epoch: [1][13100/36908] Elapsed 171m 46s (remain 312m 8s) Loss: 0.0000(0.0172) Grad: 91.9021  LR: 0.000014  \n",
      "Epoch: [1][13200/36908] Elapsed 173m 9s (remain 310m 57s) Loss: 0.0000(0.0171) Grad: 812.6155  LR: 0.000014  \n",
      "Epoch: [1][13300/36908] Elapsed 174m 33s (remain 309m 49s) Loss: 0.0197(0.0170) Grad: 208882.3594  LR: 0.000014  \n",
      "Epoch: [1][13400/36908] Elapsed 175m 59s (remain 308m 43s) Loss: 0.0001(0.0169) Grad: 5735.4155  LR: 0.000014  \n",
      "Epoch: [1][13500/36908] Elapsed 177m 22s (remain 307m 31s) Loss: 0.0010(0.0168) Grad: 15087.5303  LR: 0.000014  \n",
      "Epoch: [1][13600/36908] Elapsed 178m 45s (remain 306m 19s) Loss: 0.0005(0.0167) Grad: 5888.1235  LR: 0.000014  \n",
      "Epoch: [1][13700/36908] Elapsed 180m 7s (remain 305m 6s) Loss: 0.0136(0.0166) Grad: 87661.2734  LR: 0.000014  \n",
      "Epoch: [1][13800/36908] Elapsed 181m 30s (remain 303m 53s) Loss: 0.0003(0.0165) Grad: 12945.7324  LR: 0.000014  \n",
      "Epoch: [1][13900/36908] Elapsed 182m 53s (remain 302m 41s) Loss: 0.0000(0.0164) Grad: 30.1720  LR: 0.000014  \n",
      "Epoch: [1][14000/36908] Elapsed 184m 10s (remain 301m 19s) Loss: 0.0018(0.0163) Grad: 22074.6562  LR: 0.000014  \n",
      "Epoch: [1][14100/36908] Elapsed 185m 28s (remain 299m 59s) Loss: 0.0124(0.0162) Grad: 62848.5547  LR: 0.000014  \n",
      "Epoch: [1][14200/36908] Elapsed 186m 51s (remain 298m 46s) Loss: 0.0011(0.0162) Grad: 15783.1670  LR: 0.000014  \n",
      "Epoch: [1][14300/36908] Elapsed 188m 8s (remain 297m 24s) Loss: 0.0000(0.0161) Grad: 47.7039  LR: 0.000014  \n",
      "Epoch: [1][14400/36908] Elapsed 189m 24s (remain 296m 1s) Loss: 0.0009(0.0160) Grad: 9010.1592  LR: 0.000014  \n",
      "Epoch: [1][14500/36908] Elapsed 190m 40s (remain 294m 38s) Loss: 0.0043(0.0159) Grad: 57147.2266  LR: 0.000013  \n",
      "Epoch: [1][14600/36908] Elapsed 191m 57s (remain 293m 16s) Loss: 0.0042(0.0158) Grad: 50203.5781  LR: 0.000013  \n",
      "Epoch: [1][14700/36908] Elapsed 193m 16s (remain 291m 57s) Loss: 0.0180(0.0157) Grad: 76831.1406  LR: 0.000013  \n",
      "Epoch: [1][14800/36908] Elapsed 194m 34s (remain 290m 36s) Loss: 0.0276(0.0157) Grad: 205928.7344  LR: 0.000013  \n",
      "Epoch: [1][14900/36908] Elapsed 195m 50s (remain 289m 14s) Loss: 0.0024(0.0156) Grad: 2961.6138  LR: 0.000013  \n",
      "Epoch: [1][15000/36908] Elapsed 197m 6s (remain 287m 50s) Loss: 0.0000(0.0155) Grad: 176.5142  LR: 0.000013  \n",
      "Epoch: [1][15100/36908] Elapsed 198m 22s (remain 286m 28s) Loss: 0.0000(0.0154) Grad: 68.0257  LR: 0.000013  \n",
      "Epoch: [1][15200/36908] Elapsed 199m 39s (remain 285m 6s) Loss: 0.0008(0.0154) Grad: 2450.3604  LR: 0.000013  \n",
      "Epoch: [1][15300/36908] Elapsed 201m 0s (remain 283m 51s) Loss: 0.0123(0.0153) Grad: 82102.1328  LR: 0.000013  \n",
      "Epoch: [1][15400/36908] Elapsed 202m 20s (remain 282m 34s) Loss: 0.0000(0.0152) Grad: 59.9304  LR: 0.000013  \n",
      "Epoch: [1][15500/36908] Elapsed 203m 39s (remain 281m 15s) Loss: 0.0000(0.0152) Grad: 467.2850  LR: 0.000013  \n",
      "Epoch: [1][15600/36908] Elapsed 204m 59s (remain 279m 57s) Loss: 0.0055(0.0151) Grad: 38834.7188  LR: 0.000013  \n",
      "Epoch: [1][15700/36908] Elapsed 206m 15s (remain 278m 35s) Loss: 0.0000(0.0150) Grad: 18.0475  LR: 0.000013  \n",
      "Epoch: [1][15800/36908] Elapsed 207m 31s (remain 277m 13s) Loss: 0.0153(0.0149) Grad: 130180.8516  LR: 0.000013  \n",
      "Epoch: [1][15900/36908] Elapsed 208m 50s (remain 275m 53s) Loss: 0.0000(0.0149) Grad: 437.0227  LR: 0.000013  \n",
      "Epoch: [1][16000/36908] Elapsed 210m 11s (remain 274m 38s) Loss: 0.0003(0.0148) Grad: 5684.7031  LR: 0.000013  \n",
      "Epoch: [1][16100/36908] Elapsed 211m 27s (remain 273m 16s) Loss: 0.0003(0.0147) Grad: 12329.6270  LR: 0.000013  \n",
      "Epoch: [1][16200/36908] Elapsed 212m 44s (remain 271m 54s) Loss: 0.0000(0.0147) Grad: 64.7588  LR: 0.000012  \n",
      "Epoch: [1][16300/36908] Elapsed 214m 1s (remain 270m 33s) Loss: 0.0002(0.0146) Grad: 10179.2129  LR: 0.000012  \n",
      "Epoch: [1][16400/36908] Elapsed 215m 18s (remain 269m 12s) Loss: 0.0000(0.0145) Grad: 18.4522  LR: 0.000012  \n",
      "Epoch: [1][16500/36908] Elapsed 216m 34s (remain 267m 50s) Loss: 0.0000(0.0145) Grad: 141.6642  LR: 0.000012  \n",
      "Epoch: [1][16600/36908] Elapsed 217m 52s (remain 266m 30s) Loss: 0.0019(0.0144) Grad: 11436.5547  LR: 0.000012  \n",
      "Epoch: [1][16700/36908] Elapsed 219m 10s (remain 265m 11s) Loss: 0.0042(0.0143) Grad: 13339.1875  LR: 0.000012  \n",
      "Epoch: [1][16800/36908] Elapsed 220m 32s (remain 263m 56s) Loss: 0.0002(0.0143) Grad: 4406.2324  LR: 0.000012  \n",
      "Epoch: [1][16900/36908] Elapsed 221m 49s (remain 262m 35s) Loss: 0.0006(0.0142) Grad: 13435.7930  LR: 0.000012  \n",
      "Epoch: [1][17000/36908] Elapsed 223m 5s (remain 261m 13s) Loss: 0.0058(0.0142) Grad: 278840.2500  LR: 0.000012  \n",
      "Epoch: [1][17100/36908] Elapsed 224m 23s (remain 259m 53s) Loss: 0.0064(0.0141) Grad: 47087.1016  LR: 0.000012  \n",
      "Epoch: [1][17200/36908] Elapsed 225m 41s (remain 258m 34s) Loss: 0.0003(0.0141) Grad: 28858.2480  LR: 0.000012  \n",
      "Epoch: [1][17300/36908] Elapsed 226m 58s (remain 257m 13s) Loss: 0.0000(0.0140) Grad: 55.4159  LR: 0.000012  \n",
      "Epoch: [1][17400/36908] Elapsed 228m 15s (remain 255m 53s) Loss: 0.0000(0.0139) Grad: 21.4168  LR: 0.000012  \n",
      "Epoch: [1][17500/36908] Elapsed 229m 34s (remain 254m 34s) Loss: 0.0000(0.0139) Grad: 46.6713  LR: 0.000012  \n",
      "Epoch: [1][17600/36908] Elapsed 230m 51s (remain 253m 13s) Loss: 0.0039(0.0138) Grad: 84323.8047  LR: 0.000012  \n",
      "Epoch: [1][17700/36908] Elapsed 232m 8s (remain 251m 53s) Loss: 0.0000(0.0138) Grad: 199.1158  LR: 0.000012  \n",
      "Epoch: [1][17800/36908] Elapsed 233m 26s (remain 250m 33s) Loss: 0.0000(0.0137) Grad: 31.5190  LR: 0.000012  \n",
      "Epoch: [1][17900/36908] Elapsed 234m 44s (remain 249m 14s) Loss: 0.0025(0.0137) Grad: 54993.7891  LR: 0.000011  \n",
      "Epoch: [1][18000/36908] Elapsed 236m 1s (remain 247m 54s) Loss: 0.0000(0.0136) Grad: 27.4915  LR: 0.000011  \n",
      "Epoch: [1][18100/36908] Elapsed 237m 18s (remain 246m 33s) Loss: 0.0007(0.0135) Grad: 27877.4297  LR: 0.000011  \n",
      "Epoch: [1][18200/36908] Elapsed 238m 34s (remain 245m 12s) Loss: 0.0005(0.0135) Grad: 24933.1445  LR: 0.000011  \n",
      "Epoch: [1][18300/36908] Elapsed 239m 53s (remain 243m 54s) Loss: 0.0000(0.0134) Grad: 313.6064  LR: 0.000011  \n",
      "Epoch: [1][18400/36908] Elapsed 241m 12s (remain 242m 35s) Loss: 0.0000(0.0134) Grad: 90.7590  LR: 0.000011  \n",
      "Epoch: [1][18500/36908] Elapsed 242m 29s (remain 241m 15s) Loss: 0.0004(0.0133) Grad: 6507.2891  LR: 0.000011  \n",
      "Epoch: [1][18600/36908] Elapsed 243m 47s (remain 239m 56s) Loss: 0.0041(0.0133) Grad: 87121.5938  LR: 0.000011  \n",
      "Epoch: [1][18700/36908] Elapsed 245m 6s (remain 238m 37s) Loss: 0.0000(0.0132) Grad: 199.0317  LR: 0.000011  \n",
      "Epoch: [1][18800/36908] Elapsed 246m 28s (remain 237m 22s) Loss: 0.0436(0.0132) Grad: 1061336.2500  LR: 0.000011  \n",
      "Epoch: [1][18900/36908] Elapsed 247m 46s (remain 236m 2s) Loss: 0.0014(0.0131) Grad: 2778.7979  LR: 0.000011  \n",
      "Epoch: [1][19000/36908] Elapsed 249m 3s (remain 234m 43s) Loss: 0.0026(0.0131) Grad: 109066.8125  LR: 0.000011  \n",
      "Epoch: [1][19100/36908] Elapsed 250m 22s (remain 233m 24s) Loss: 0.0000(0.0130) Grad: 165.4318  LR: 0.000011  \n",
      "Epoch: [1][19200/36908] Elapsed 251m 41s (remain 232m 6s) Loss: 0.0703(0.0130) Grad: 1282182.7500  LR: 0.000011  \n",
      "Epoch: [1][19300/36908] Elapsed 252m 58s (remain 230m 46s) Loss: 0.0000(0.0129) Grad: 801.9547  LR: 0.000011  \n",
      "Epoch: [1][19400/36908] Elapsed 254m 15s (remain 229m 26s) Loss: 0.0000(0.0129) Grad: 50.4946  LR: 0.000011  \n",
      "Epoch: [1][19500/36908] Elapsed 255m 33s (remain 228m 7s) Loss: 0.0002(0.0128) Grad: 6955.7837  LR: 0.000010  \n",
      "Epoch: [1][19600/36908] Elapsed 256m 54s (remain 226m 50s) Loss: 0.0112(0.0128) Grad: 140857.3281  LR: 0.000010  \n",
      "Epoch: [1][19700/36908] Elapsed 258m 12s (remain 225m 31s) Loss: 0.0049(0.0127) Grad: 305855.6875  LR: 0.000010  \n",
      "Epoch: [1][19800/36908] Elapsed 259m 30s (remain 224m 11s) Loss: 0.0115(0.0127) Grad: 145924.0469  LR: 0.000010  \n",
      "Epoch: [1][19900/36908] Elapsed 260m 47s (remain 222m 52s) Loss: 0.0000(0.0126) Grad: 231.7043  LR: 0.000010  \n",
      "Epoch: [1][20000/36908] Elapsed 262m 6s (remain 221m 33s) Loss: 0.0000(0.0126) Grad: 199.3017  LR: 0.000010  \n",
      "Epoch: [1][20100/36908] Elapsed 263m 24s (remain 220m 14s) Loss: 0.0000(0.0125) Grad: 32.5102  LR: 0.000010  \n",
      "Epoch: [1][20200/36908] Elapsed 264m 41s (remain 218m 54s) Loss: 0.0000(0.0125) Grad: 51.8057  LR: 0.000010  \n",
      "Epoch: [1][20300/36908] Elapsed 266m 1s (remain 217m 36s) Loss: 0.0013(0.0124) Grad: 8063.0010  LR: 0.000010  \n",
      "Epoch: [1][20400/36908] Elapsed 267m 18s (remain 216m 17s) Loss: 0.0000(0.0124) Grad: 34.5858  LR: 0.000010  \n",
      "Epoch: [1][20500/36908] Elapsed 268m 35s (remain 214m 57s) Loss: 0.0001(0.0124) Grad: 2326.0623  LR: 0.000010  \n",
      "Epoch: [1][20600/36908] Elapsed 269m 51s (remain 213m 36s) Loss: 0.0008(0.0123) Grad: 21945.0781  LR: 0.000010  \n",
      "Epoch: [1][20700/36908] Elapsed 271m 8s (remain 212m 16s) Loss: 0.0228(0.0123) Grad: 285741.5312  LR: 0.000010  \n",
      "Epoch: [1][20800/36908] Elapsed 272m 26s (remain 210m 57s) Loss: 0.0000(0.0122) Grad: 15.1681  LR: 0.000010  \n",
      "Epoch: [1][20900/36908] Elapsed 273m 43s (remain 209m 37s) Loss: 0.0000(0.0122) Grad: 99.4441  LR: 0.000010  \n",
      "Epoch: [1][21000/36908] Elapsed 274m 59s (remain 208m 17s) Loss: 0.0002(0.0121) Grad: 4352.4878  LR: 0.000010  \n",
      "Epoch: [1][21100/36908] Elapsed 276m 20s (remain 207m 0s) Loss: 0.0011(0.0121) Grad: 3199.9500  LR: 0.000010  \n",
      "Epoch: [1][21200/36908] Elapsed 277m 38s (remain 205m 41s) Loss: 0.0000(0.0121) Grad: 174.5918  LR: 0.000009  \n",
      "Epoch: [1][21300/36908] Elapsed 278m 56s (remain 204m 22s) Loss: 0.0054(0.0120) Grad: 47284.8750  LR: 0.000009  \n",
      "Epoch: [1][21400/36908] Elapsed 280m 14s (remain 203m 3s) Loss: 0.0007(0.0120) Grad: 3857.5471  LR: 0.000009  \n",
      "Epoch: [1][21500/36908] Elapsed 281m 33s (remain 201m 45s) Loss: 0.0030(0.0119) Grad: 106844.1484  LR: 0.000009  \n",
      "Epoch: [1][21600/36908] Elapsed 282m 50s (remain 200m 25s) Loss: 0.0014(0.0119) Grad: 14413.7002  LR: 0.000009  \n",
      "Epoch: [1][21700/36908] Elapsed 284m 9s (remain 199m 7s) Loss: 0.0005(0.0118) Grad: 4521.5039  LR: 0.000009  \n",
      "Epoch: [1][21800/36908] Elapsed 285m 29s (remain 197m 50s) Loss: 0.0070(0.0118) Grad: 10051.0840  LR: 0.000009  \n",
      "Epoch: [1][21900/36908] Elapsed 286m 47s (remain 196m 30s) Loss: 0.0000(0.0118) Grad: 173.8684  LR: 0.000009  \n",
      "Epoch: [1][22000/36908] Elapsed 288m 6s (remain 195m 12s) Loss: 0.0001(0.0117) Grad: 1780.3854  LR: 0.000009  \n",
      "Epoch: [1][22100/36908] Elapsed 289m 29s (remain 193m 57s) Loss: 0.0008(0.0117) Grad: 43532.6289  LR: 0.000009  \n",
      "Epoch: [1][22200/36908] Elapsed 290m 52s (remain 192m 41s) Loss: 0.0004(0.0117) Grad: 28685.9160  LR: 0.000009  \n",
      "Epoch: [1][22300/36908] Elapsed 292m 14s (remain 191m 25s) Loss: 0.0001(0.0116) Grad: 959.4333  LR: 0.000009  \n",
      "Epoch: [1][22400/36908] Elapsed 293m 39s (remain 190m 10s) Loss: 0.0000(0.0116) Grad: 135.7296  LR: 0.000009  \n",
      "Epoch: [1][22500/36908] Elapsed 295m 3s (remain 188m 55s) Loss: 0.0001(0.0115) Grad: 2942.7539  LR: 0.000009  \n",
      "Epoch: [1][22600/36908] Elapsed 296m 27s (remain 187m 39s) Loss: 0.0003(0.0115) Grad: 7244.5557  LR: 0.000009  \n",
      "Epoch: [1][22700/36908] Elapsed 297m 50s (remain 186m 23s) Loss: 0.0001(0.0115) Grad: 8083.0820  LR: 0.000009  \n",
      "Epoch: [1][22800/36908] Elapsed 299m 12s (remain 185m 7s) Loss: 0.0000(0.0114) Grad: 7.2848  LR: 0.000008  \n",
      "Epoch: [1][22900/36908] Elapsed 300m 31s (remain 183m 48s) Loss: 0.0117(0.0114) Grad: 111165.0078  LR: 0.000008  \n",
      "Epoch: [1][23000/36908] Elapsed 301m 50s (remain 182m 29s) Loss: 0.0040(0.0114) Grad: 27746.0820  LR: 0.000008  \n",
      "Epoch: [1][23100/36908] Elapsed 303m 8s (remain 181m 11s) Loss: 0.0054(0.0113) Grad: 123760.2188  LR: 0.000008  \n",
      "Epoch: [1][23200/36908] Elapsed 304m 26s (remain 179m 51s) Loss: 0.0017(0.0113) Grad: 37875.0352  LR: 0.000008  \n",
      "Epoch: [1][23300/36908] Elapsed 305m 43s (remain 178m 32s) Loss: 0.0000(0.0113) Grad: 17.8751  LR: 0.000008  \n",
      "Epoch: [1][23400/36908] Elapsed 307m 0s (remain 177m 12s) Loss: 0.0000(0.0112) Grad: 10.4817  LR: 0.000008  \n",
      "Epoch: [1][23500/36908] Elapsed 308m 19s (remain 175m 53s) Loss: 0.0000(0.0112) Grad: 45.6105  LR: 0.000008  \n",
      "Epoch: [1][23600/36908] Elapsed 309m 40s (remain 174m 36s) Loss: 0.0090(0.0112) Grad: 45766.3438  LR: 0.000008  \n",
      "Epoch: [1][23700/36908] Elapsed 310m 57s (remain 173m 16s) Loss: 0.0000(0.0111) Grad: 5.7556  LR: 0.000008  \n",
      "Epoch: [1][23800/36908] Elapsed 312m 16s (remain 171m 57s) Loss: 0.0029(0.0111) Grad: 153683.0469  LR: 0.000008  \n",
      "Epoch: [1][23900/36908] Elapsed 313m 37s (remain 170m 40s) Loss: 0.0035(0.0111) Grad: 7786.4312  LR: 0.000008  \n",
      "Epoch: [1][24000/36908] Elapsed 314m 58s (remain 169m 23s) Loss: 0.0129(0.0110) Grad: 386706.0312  LR: 0.000008  \n",
      "Epoch: [1][24100/36908] Elapsed 316m 17s (remain 168m 4s) Loss: 0.0080(0.0110) Grad: 60210.9102  LR: 0.000008  \n",
      "Epoch: [1][24200/36908] Elapsed 317m 35s (remain 166m 45s) Loss: 0.0000(0.0110) Grad: 38.5688  LR: 0.000008  \n",
      "Epoch: [1][24300/36908] Elapsed 318m 54s (remain 165m 26s) Loss: 0.0062(0.0109) Grad: 121695.0391  LR: 0.000008  \n",
      "Epoch: [1][24400/36908] Elapsed 320m 12s (remain 164m 7s) Loss: 0.0002(0.0109) Grad: 9064.1807  LR: 0.000008  \n",
      "Epoch: [1][24500/36908] Elapsed 321m 28s (remain 162m 47s) Loss: 0.0001(0.0109) Grad: 23177.4980  LR: 0.000007  \n",
      "Epoch: [1][24600/36908] Elapsed 322m 44s (remain 161m 27s) Loss: 0.0002(0.0108) Grad: 7828.8242  LR: 0.000007  \n",
      "Epoch: [1][24700/36908] Elapsed 324m 2s (remain 160m 8s) Loss: 0.0000(0.0108) Grad: 20.6012  LR: 0.000007  \n",
      "Epoch: [1][24800/36908] Elapsed 325m 21s (remain 158m 49s) Loss: 0.0083(0.0108) Grad: 133949.8281  LR: 0.000007  \n",
      "Epoch: [1][24900/36908] Elapsed 326m 38s (remain 157m 30s) Loss: 0.0001(0.0107) Grad: 1939.5061  LR: 0.000007  \n",
      "Epoch: [1][25000/36908] Elapsed 327m 54s (remain 156m 10s) Loss: 0.0001(0.0107) Grad: 7819.6309  LR: 0.000007  \n",
      "Epoch: [1][25100/36908] Elapsed 329m 12s (remain 154m 51s) Loss: 0.0000(0.0107) Grad: 12.8692  LR: 0.000007  \n",
      "Epoch: [1][25200/36908] Elapsed 330m 30s (remain 153m 32s) Loss: 0.0016(0.0106) Grad: 37021.9375  LR: 0.000007  \n",
      "Epoch: [1][25300/36908] Elapsed 331m 51s (remain 152m 14s) Loss: 0.0000(0.0106) Grad: 31.4419  LR: 0.000007  \n",
      "Epoch: [1][25400/36908] Elapsed 333m 8s (remain 150m 55s) Loss: 0.0000(0.0106) Grad: 22.0609  LR: 0.000007  \n",
      "Epoch: [1][25500/36908] Elapsed 334m 24s (remain 149m 35s) Loss: 0.0000(0.0105) Grad: 22.6990  LR: 0.000007  \n",
      "Epoch: [1][25600/36908] Elapsed 335m 42s (remain 148m 16s) Loss: 0.0000(0.0105) Grad: 409.5409  LR: 0.000007  \n",
      "Epoch: [1][25700/36908] Elapsed 336m 58s (remain 146m 56s) Loss: 0.0000(0.0105) Grad: 151.5838  LR: 0.000007  \n",
      "Epoch: [1][25800/36908] Elapsed 338m 13s (remain 145m 36s) Loss: 0.0075(0.0105) Grad: 176770.9531  LR: 0.000007  \n",
      "Epoch: [1][25900/36908] Elapsed 339m 31s (remain 144m 17s) Loss: 0.0000(0.0104) Grad: 32.4496  LR: 0.000007  \n",
      "Epoch: [1][26000/36908] Elapsed 340m 50s (remain 142m 58s) Loss: 0.0405(0.0104) Grad: 418939.2188  LR: 0.000007  \n",
      "Epoch: [1][26100/36908] Elapsed 342m 6s (remain 141m 39s) Loss: 0.0000(0.0104) Grad: 39.6067  LR: 0.000007  \n",
      "Epoch: [1][26200/36908] Elapsed 343m 24s (remain 140m 19s) Loss: 0.0020(0.0104) Grad: 45273.5938  LR: 0.000006  \n",
      "Epoch: [1][26300/36908] Elapsed 344m 42s (remain 139m 0s) Loss: 0.0027(0.0103) Grad: 637022.2500  LR: 0.000006  \n",
      "Epoch: [1][26400/36908] Elapsed 345m 59s (remain 137m 41s) Loss: 0.0062(0.0103) Grad: 211060.5938  LR: 0.000006  \n",
      "Epoch: [1][26500/36908] Elapsed 347m 17s (remain 136m 22s) Loss: 0.0000(0.0103) Grad: 64.6762  LR: 0.000006  \n",
      "Epoch: [1][26600/36908] Elapsed 348m 33s (remain 135m 3s) Loss: 0.0000(0.0103) Grad: 170.6744  LR: 0.000006  \n",
      "Epoch: [1][26700/36908] Elapsed 349m 50s (remain 133m 44s) Loss: 0.0002(0.0102) Grad: 6198.2607  LR: 0.000006  \n",
      "Epoch: [1][26800/36908] Elapsed 351m 12s (remain 132m 26s) Loss: 0.0009(0.0102) Grad: 4222.4380  LR: 0.000006  \n",
      "Epoch: [1][26900/36908] Elapsed 352m 29s (remain 131m 7s) Loss: 0.0003(0.0102) Grad: 12470.6465  LR: 0.000006  \n",
      "Epoch: [1][27000/36908] Elapsed 353m 51s (remain 129m 50s) Loss: 0.0000(0.0101) Grad: 62.4990  LR: 0.000006  \n",
      "Epoch: [1][27100/36908] Elapsed 355m 13s (remain 128m 32s) Loss: 0.0037(0.0101) Grad: 106447.0625  LR: 0.000006  \n",
      "Epoch: [1][27200/36908] Elapsed 356m 29s (remain 127m 13s) Loss: 0.0129(0.0101) Grad: 177744.2656  LR: 0.000006  \n",
      "Epoch: [1][27300/36908] Elapsed 357m 46s (remain 125m 53s) Loss: 0.0007(0.0101) Grad: 47102.3164  LR: 0.000006  \n",
      "Epoch: [1][27400/36908] Elapsed 359m 5s (remain 124m 35s) Loss: 0.0033(0.0100) Grad: 56251.7305  LR: 0.000006  \n",
      "Epoch: [1][27500/36908] Elapsed 360m 23s (remain 123m 16s) Loss: 0.0000(0.0100) Grad: 105.0703  LR: 0.000006  \n",
      "Epoch: [1][27600/36908] Elapsed 361m 39s (remain 121m 57s) Loss: 0.0000(0.0100) Grad: 135.1349  LR: 0.000006  \n",
      "Epoch: [1][27700/36908] Elapsed 362m 57s (remain 120m 38s) Loss: 0.0000(0.0100) Grad: 41.7659  LR: 0.000006  \n",
      "Epoch: [1][27800/36908] Elapsed 364m 20s (remain 119m 21s) Loss: 0.0012(0.0099) Grad: 52745.4492  LR: 0.000005  \n",
      "Epoch: [1][27900/36908] Elapsed 365m 36s (remain 118m 1s) Loss: 0.0118(0.0099) Grad: 190102.2188  LR: 0.000005  \n",
      "Epoch: [1][28000/36908] Elapsed 366m 53s (remain 116m 42s) Loss: 0.0000(0.0099) Grad: 4404.1714  LR: 0.000005  \n",
      "Epoch: [1][28100/36908] Elapsed 368m 13s (remain 115m 24s) Loss: 0.0009(0.0099) Grad: 16164.7520  LR: 0.000005  \n",
      "Epoch: [1][28200/36908] Elapsed 369m 37s (remain 114m 7s) Loss: 0.0000(0.0098) Grad: 12.1029  LR: 0.000005  \n",
      "Epoch: [1][28300/36908] Elapsed 370m 57s (remain 112m 49s) Loss: 0.0000(0.0098) Grad: 42.0986  LR: 0.000005  \n",
      "Epoch: [1][28400/36908] Elapsed 372m 14s (remain 111m 29s) Loss: 0.0000(0.0098) Grad: 219.9272  LR: 0.000005  \n",
      "Epoch: [1][28500/36908] Elapsed 373m 30s (remain 110m 10s) Loss: 0.0000(0.0098) Grad: 378.9508  LR: 0.000005  \n",
      "Epoch: [1][28600/36908] Elapsed 374m 48s (remain 108m 51s) Loss: 0.0004(0.0098) Grad: 14439.4756  LR: 0.000005  \n",
      "Epoch: [1][28700/36908] Elapsed 376m 12s (remain 107m 34s) Loss: 0.0000(0.0097) Grad: 31.1907  LR: 0.000005  \n",
      "Epoch: [1][28800/36908] Elapsed 377m 31s (remain 106m 15s) Loss: 0.0000(0.0097) Grad: 82.9391  LR: 0.000005  \n",
      "Epoch: [1][28900/36908] Elapsed 378m 48s (remain 104m 56s) Loss: 0.0000(0.0097) Grad: 28.2838  LR: 0.000005  \n",
      "Epoch: [1][29000/36908] Elapsed 380m 4s (remain 103m 37s) Loss: 0.0000(0.0097) Grad: 6259.2656  LR: 0.000005  \n",
      "Epoch: [1][29100/36908] Elapsed 381m 21s (remain 102m 18s) Loss: 0.0000(0.0096) Grad: 414.5861  LR: 0.000005  \n",
      "Epoch: [1][29200/36908] Elapsed 382m 43s (remain 101m 0s) Loss: 0.0477(0.0096) Grad: 220497.7188  LR: 0.000005  \n",
      "Epoch: [1][29300/36908] Elapsed 384m 5s (remain 99m 43s) Loss: 0.0004(0.0096) Grad: 9363.4590  LR: 0.000005  \n",
      "Epoch: [1][29400/36908] Elapsed 385m 22s (remain 98m 23s) Loss: 0.0146(0.0096) Grad: 229245.6562  LR: 0.000005  \n",
      "Epoch: [1][29500/36908] Elapsed 386m 40s (remain 97m 5s) Loss: 0.0000(0.0095) Grad: 419.1671  LR: 0.000004  \n",
      "Epoch: [1][29600/36908] Elapsed 388m 0s (remain 95m 46s) Loss: 0.0000(0.0095) Grad: 6614.3911  LR: 0.000004  \n",
      "Epoch: [1][29700/36908] Elapsed 389m 17s (remain 94m 27s) Loss: 0.0000(0.0095) Grad: 18.1647  LR: 0.000004  \n",
      "Epoch: [1][29800/36908] Elapsed 390m 35s (remain 93m 8s) Loss: 0.0000(0.0095) Grad: 5.6492  LR: 0.000004  \n",
      "Epoch: [1][29900/36908] Elapsed 391m 56s (remain 91m 50s) Loss: 0.0000(0.0095) Grad: 17.9482  LR: 0.000004  \n",
      "Epoch: [1][30000/36908] Elapsed 393m 14s (remain 90m 32s) Loss: 0.0152(0.0094) Grad: 214165.7344  LR: 0.000004  \n",
      "Epoch: [1][30100/36908] Elapsed 394m 31s (remain 89m 13s) Loss: 0.0003(0.0094) Grad: 52503.4648  LR: 0.000004  \n",
      "Epoch: [1][30200/36908] Elapsed 395m 47s (remain 87m 53s) Loss: 0.0000(0.0094) Grad: 467.9054  LR: 0.000004  \n",
      "Epoch: [1][30300/36908] Elapsed 397m 4s (remain 86m 34s) Loss: 0.0000(0.0094) Grad: 28.5275  LR: 0.000004  \n",
      "Epoch: [1][30400/36908] Elapsed 398m 22s (remain 85m 16s) Loss: 0.0040(0.0094) Grad: 66896.5391  LR: 0.000004  \n",
      "Epoch: [1][30500/36908] Elapsed 399m 41s (remain 83m 57s) Loss: 0.0000(0.0093) Grad: 11.8082  LR: 0.000004  \n",
      "Epoch: [1][30600/36908] Elapsed 400m 57s (remain 82m 38s) Loss: 0.0001(0.0093) Grad: 1095.7866  LR: 0.000004  \n",
      "Epoch: [1][30700/36908] Elapsed 402m 13s (remain 81m 19s) Loss: 0.0000(0.0093) Grad: 14.9239  LR: 0.000004  \n",
      "Epoch: [1][30800/36908] Elapsed 403m 29s (remain 80m 0s) Loss: 0.0000(0.0093) Grad: 30.4121  LR: 0.000004  \n",
      "Epoch: [1][30900/36908] Elapsed 404m 48s (remain 78m 41s) Loss: 0.0020(0.0092) Grad: 58741.7656  LR: 0.000004  \n",
      "Epoch: [1][31000/36908] Elapsed 406m 11s (remain 77m 23s) Loss: 0.0000(0.0092) Grad: 2.2811  LR: 0.000004  \n",
      "Epoch: [1][31100/36908] Elapsed 407m 32s (remain 76m 5s) Loss: 0.0483(0.0092) Grad: 667644.6875  LR: 0.000003  \n",
      "Epoch: [1][31200/36908] Elapsed 408m 49s (remain 74m 46s) Loss: 0.0000(0.0092) Grad: 1342.9767  LR: 0.000003  \n",
      "Epoch: [1][31300/36908] Elapsed 410m 5s (remain 73m 27s) Loss: 0.0065(0.0092) Grad: 217678.7969  LR: 0.000003  \n",
      "Epoch: [1][31400/36908] Elapsed 411m 24s (remain 72m 9s) Loss: 0.0000(0.0092) Grad: 2465.6777  LR: 0.000003  \n",
      "Epoch: [1][31500/36908] Elapsed 412m 43s (remain 70m 50s) Loss: 0.0000(0.0091) Grad: 25.7837  LR: 0.000003  \n",
      "Epoch: [1][31600/36908] Elapsed 414m 0s (remain 69m 31s) Loss: 0.0001(0.0091) Grad: 3857.9136  LR: 0.000003  \n",
      "Epoch: [1][31700/36908] Elapsed 415m 21s (remain 68m 13s) Loss: 0.0000(0.0091) Grad: 1064.9637  LR: 0.000003  \n",
      "Epoch: [1][31800/36908] Elapsed 416m 41s (remain 66m 55s) Loss: 0.0007(0.0091) Grad: 95100.1094  LR: 0.000003  \n",
      "Epoch: [1][31900/36908] Elapsed 417m 59s (remain 65m 36s) Loss: 0.0049(0.0091) Grad: 55076.3242  LR: 0.000003  \n",
      "Epoch: [1][32000/36908] Elapsed 419m 16s (remain 64m 17s) Loss: 0.0082(0.0090) Grad: 162298.7812  LR: 0.000003  \n",
      "Epoch: [1][32100/36908] Elapsed 420m 33s (remain 62m 58s) Loss: 0.0000(0.0090) Grad: 5.1960  LR: 0.000003  \n",
      "Epoch: [1][32200/36908] Elapsed 421m 50s (remain 61m 39s) Loss: 0.0000(0.0090) Grad: 4.0768  LR: 0.000003  \n",
      "Epoch: [1][32300/36908] Elapsed 423m 12s (remain 60m 21s) Loss: 0.0000(0.0090) Grad: 9.3138  LR: 0.000003  \n",
      "Epoch: [1][32400/36908] Elapsed 424m 29s (remain 59m 2s) Loss: 0.0071(0.0090) Grad: 232782.9375  LR: 0.000003  \n",
      "Epoch: [1][32500/36908] Elapsed 425m 47s (remain 57m 44s) Loss: 0.0106(0.0089) Grad: 484116.2500  LR: 0.000003  \n",
      "Epoch: [1][32600/36908] Elapsed 427m 5s (remain 56m 25s) Loss: 0.0000(0.0089) Grad: 52.9033  LR: 0.000003  \n",
      "Epoch: [1][32700/36908] Elapsed 428m 24s (remain 55m 6s) Loss: 0.0000(0.0089) Grad: 11.9893  LR: 0.000003  \n",
      "Epoch: [1][32800/36908] Elapsed 429m 41s (remain 53m 48s) Loss: 0.0000(0.0089) Grad: 3.2547  LR: 0.000002  \n",
      "Epoch: [1][32900/36908] Elapsed 430m 59s (remain 52m 29s) Loss: 0.0000(0.0089) Grad: 1193.3403  LR: 0.000002  \n",
      "Epoch: [1][33000/36908] Elapsed 432m 17s (remain 51m 10s) Loss: 0.0007(0.0089) Grad: 10276.9385  LR: 0.000002  \n",
      "Epoch: [1][33100/36908] Elapsed 433m 34s (remain 49m 51s) Loss: 0.0000(0.0088) Grad: 16.5276  LR: 0.000002  \n",
      "Epoch: [1][33200/36908] Elapsed 434m 53s (remain 48m 33s) Loss: 0.0004(0.0088) Grad: 12381.6104  LR: 0.000002  \n",
      "Epoch: [1][33300/36908] Elapsed 436m 11s (remain 47m 14s) Loss: 0.0000(0.0088) Grad: 15.5453  LR: 0.000002  \n",
      "Epoch: [1][33400/36908] Elapsed 437m 29s (remain 45m 56s) Loss: 0.0000(0.0088) Grad: 44.3538  LR: 0.000002  \n",
      "Epoch: [1][33500/36908] Elapsed 438m 48s (remain 44m 37s) Loss: 0.0000(0.0088) Grad: 29.6644  LR: 0.000002  \n",
      "Epoch: [1][33600/36908] Elapsed 440m 9s (remain 43m 19s) Loss: 0.0003(0.0088) Grad: 19252.1348  LR: 0.000002  \n",
      "Epoch: [1][33700/36908] Elapsed 441m 27s (remain 42m 0s) Loss: 0.0000(0.0087) Grad: 9.7942  LR: 0.000002  \n",
      "Epoch: [1][33800/36908] Elapsed 442m 44s (remain 40m 41s) Loss: 0.0062(0.0087) Grad: 389795.7812  LR: 0.000002  \n",
      "Epoch: [1][33900/36908] Elapsed 444m 1s (remain 39m 23s) Loss: 0.0000(0.0087) Grad: 22.1445  LR: 0.000002  \n",
      "Epoch: [1][34000/36908] Elapsed 445m 21s (remain 38m 4s) Loss: 0.0000(0.0087) Grad: 464.3358  LR: 0.000002  \n",
      "Epoch: [1][34100/36908] Elapsed 446m 40s (remain 36m 46s) Loss: 0.0000(0.0087) Grad: 9.1082  LR: 0.000002  \n",
      "Epoch: [1][34200/36908] Elapsed 447m 57s (remain 35m 27s) Loss: 0.0000(0.0086) Grad: 40.2434  LR: 0.000002  \n",
      "Epoch: [1][34300/36908] Elapsed 449m 15s (remain 34m 8s) Loss: 0.0000(0.0086) Grad: 12.9324  LR: 0.000002  \n",
      "Epoch: [1][34400/36908] Elapsed 450m 34s (remain 32m 50s) Loss: 0.0000(0.0086) Grad: 124.4669  LR: 0.000002  \n",
      "Epoch: [1][34500/36908] Elapsed 451m 52s (remain 31m 31s) Loss: 0.0000(0.0086) Grad: 9.2824  LR: 0.000001  \n",
      "Epoch: [1][34600/36908] Elapsed 453m 9s (remain 30m 12s) Loss: 0.0030(0.0086) Grad: 45595.5586  LR: 0.000001  \n",
      "Epoch: [1][34700/36908] Elapsed 454m 29s (remain 28m 54s) Loss: 0.0082(0.0086) Grad: 58369.4336  LR: 0.000001  \n",
      "Epoch: [1][34800/36908] Elapsed 455m 46s (remain 27m 35s) Loss: 0.0000(0.0086) Grad: 34.1553  LR: 0.000001  \n",
      "Epoch: [1][34900/36908] Elapsed 457m 3s (remain 26m 17s) Loss: 0.0000(0.0085) Grad: 77.2789  LR: 0.000001  \n",
      "Epoch: [1][35000/36908] Elapsed 458m 20s (remain 24m 58s) Loss: 0.0009(0.0085) Grad: 6661.7300  LR: 0.000001  \n",
      "Epoch: [1][35100/36908] Elapsed 459m 37s (remain 23m 39s) Loss: 0.0003(0.0085) Grad: 559.3602  LR: 0.000001  \n",
      "Epoch: [1][35200/36908] Elapsed 460m 54s (remain 22m 21s) Loss: 0.0000(0.0085) Grad: 11.9827  LR: 0.000001  \n",
      "Epoch: [1][35300/36908] Elapsed 462m 12s (remain 21m 2s) Loss: 0.0003(0.0085) Grad: 8983.5947  LR: 0.000001  \n",
      "Epoch: [1][35400/36908] Elapsed 463m 31s (remain 19m 43s) Loss: 0.0000(0.0085) Grad: 26.1364  LR: 0.000001  \n",
      "Epoch: [1][35500/36908] Elapsed 464m 52s (remain 18m 25s) Loss: 0.0066(0.0085) Grad: 61322.0938  LR: 0.000001  \n",
      "Epoch: [1][35600/36908] Elapsed 466m 8s (remain 17m 6s) Loss: 0.0000(0.0084) Grad: 6.5776  LR: 0.000001  \n",
      "Epoch: [1][35700/36908] Elapsed 467m 24s (remain 15m 48s) Loss: 0.0003(0.0084) Grad: 42309.2500  LR: 0.000001  \n",
      "Epoch: [1][35800/36908] Elapsed 468m 41s (remain 14m 29s) Loss: 0.0001(0.0084) Grad: 1455.9484  LR: 0.000001  \n",
      "Epoch: [1][35900/36908] Elapsed 470m 1s (remain 13m 11s) Loss: 0.0018(0.0084) Grad: 9878.9580  LR: 0.000001  \n",
      "Epoch: [1][36000/36908] Elapsed 471m 18s (remain 11m 52s) Loss: 0.0000(0.0084) Grad: 652.0712  LR: 0.000001  \n",
      "Epoch: [1][36100/36908] Elapsed 472m 35s (remain 10m 33s) Loss: 0.0000(0.0084) Grad: 80.8711  LR: 0.000000  \n",
      "Epoch: [1][36200/36908] Elapsed 473m 51s (remain 9m 15s) Loss: 0.0000(0.0083) Grad: 6.0464  LR: 0.000000  \n",
      "Epoch: [1][36300/36908] Elapsed 475m 10s (remain 7m 56s) Loss: 0.0000(0.0083) Grad: 3.3434  LR: 0.000000  \n",
      "Epoch: [1][36400/36908] Elapsed 476m 29s (remain 6m 38s) Loss: 0.0001(0.0083) Grad: 677.5966  LR: 0.000000  \n",
      "Epoch: [1][36500/36908] Elapsed 477m 46s (remain 5m 19s) Loss: 0.0000(0.0083) Grad: 17.2518  LR: 0.000000  \n",
      "Epoch: [1][36600/36908] Elapsed 479m 2s (remain 4m 1s) Loss: 0.0036(0.0083) Grad: 26779.6035  LR: 0.000000  \n",
      "Epoch: [1][36700/36908] Elapsed 480m 18s (remain 2m 42s) Loss: 0.0008(0.0083) Grad: 6632.3213  LR: 0.000000  \n",
      "Epoch: [1][36800/36908] Elapsed 481m 34s (remain 1m 24s) Loss: 0.0000(0.0083) Grad: 5.6029  LR: 0.000000  \n",
      "Epoch: [1][36900/36908] Elapsed 482m 52s (remain 0m 5s) Loss: 0.0031(0.0082) Grad: 69028.5234  LR: 0.000000  \n",
      "Epoch: [1][36907/36908] Elapsed 482m 58s (remain 0m 0s) Loss: 0.0003(0.0082) Grad: 1411.9882  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 1s (remain 21m 41s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 33s (remain 5m 59s) Loss: 0.0598(0.0106) \n",
      "EVAL: [200/1192] Elapsed 1m 3s (remain 5m 13s) Loss: 0.0162(0.0106) \n",
      "EVAL: [300/1192] Elapsed 1m 34s (remain 4m 38s) Loss: 0.0086(0.0099) \n",
      "EVAL: [400/1192] Elapsed 2m 4s (remain 4m 5s) Loss: 0.0000(0.0107) \n",
      "EVAL: [500/1192] Elapsed 2m 34s (remain 3m 33s) Loss: 0.0000(0.0100) \n",
      "EVAL: [600/1192] Elapsed 3m 6s (remain 3m 2s) Loss: 0.0006(0.0102) \n",
      "EVAL: [700/1192] Elapsed 3m 37s (remain 2m 32s) Loss: 0.0188(0.0117) \n",
      "EVAL: [800/1192] Elapsed 4m 7s (remain 2m 0s) Loss: 0.0000(0.0116) \n",
      "EVAL: [900/1192] Elapsed 4m 37s (remain 1m 29s) Loss: 0.0223(0.0117) \n",
      "EVAL: [1000/1192] Elapsed 5m 7s (remain 0m 58s) Loss: 0.0000(0.0114) \n",
      "EVAL: [1100/1192] Elapsed 5m 37s (remain 0m 27s) Loss: 0.0424(0.0109) \n",
      "EVAL: [1191/1192] Elapsed 6m 4s (remain 0m 0s) Loss: 0.0000(0.0105) \n",
      "Epoch 1 - avg_train_loss: 0.0082  avg_val_loss: 0.0105  time: 29348s\n",
      "Epoch 1 - Score: 0.8957\n",
      "Epoch 1 - Save Best Score: 0.8957 Model\n",
      "========== fold: 3 training ==========\n",
      "get pseudo plain from ../output/nbme-score-clinical-patient-notes/make_pseudo_dataset/pseudo_plain.pkl\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp060/pseudo_labels_3.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp067/pseudo_labels_3.npy\n",
      "get pseudo labels from ../output/nbme-score-clinical-patient-notes/nbme-exp083/pseudo_labels_3.npy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb062c239104ed78b3fc07e31da204c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c7848e345c4009a4a1e6f377095bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(612602, 950)\n",
      "(612602, 6) (612602, 950)\n",
      "(100000, 7)\n",
      "(110725, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp073/microsoft-deberta-v3-large-mlm-epoch-12.bin\n",
      "Epoch: [1][0/36908] Elapsed 0m 1s (remain 877m 25s) Loss: 0.3772(0.3772) Grad: 139366.2500  LR: 0.000000  \n",
      "Epoch: [1][100/36908] Elapsed 1m 19s (remain 483m 54s) Loss: 0.3453(0.3647) Grad: 130952.0938  LR: 0.000001  \n",
      "Epoch: [1][200/36908] Elapsed 2m 36s (remain 476m 34s) Loss: 0.2555(0.3350) Grad: 107312.0781  LR: 0.000001  \n",
      "Epoch: [1][300/36908] Elapsed 3m 54s (remain 474m 57s) Loss: 0.1606(0.2910) Grad: 60282.2812  LR: 0.000002  \n",
      "Epoch: [1][400/36908] Elapsed 5m 14s (remain 476m 49s) Loss: 0.0570(0.2434) Grad: 30825.7168  LR: 0.000002  \n",
      "Epoch: [1][500/36908] Elapsed 6m 33s (remain 476m 3s) Loss: 0.0318(0.2043) Grad: 4247.7798  LR: 0.000003  \n",
      "Epoch: [1][600/36908] Elapsed 7m 50s (remain 473m 28s) Loss: 0.0393(0.1774) Grad: 6337.9214  LR: 0.000003  \n",
      "Epoch: [1][700/36908] Elapsed 9m 7s (remain 471m 8s) Loss: 0.0896(0.1581) Grad: 13682.9629  LR: 0.000004  \n",
      "Epoch: [1][800/36908] Elapsed 10m 24s (remain 469m 4s) Loss: 0.0131(0.1437) Grad: 5799.9165  LR: 0.000004  \n",
      "Epoch: [1][900/36908] Elapsed 11m 43s (remain 468m 19s) Loss: 0.0172(0.1324) Grad: 4974.3359  LR: 0.000005  \n",
      "Epoch: [1][1000/36908] Elapsed 13m 0s (remain 466m 32s) Loss: 0.0518(0.1228) Grad: 10195.4346  LR: 0.000005  \n",
      "Epoch: [1][1100/36908] Elapsed 14m 17s (remain 464m 39s) Loss: 0.0168(0.1153) Grad: 8441.8770  LR: 0.000006  \n",
      "Epoch: [1][1200/36908] Elapsed 15m 35s (remain 463m 29s) Loss: 0.0065(0.1082) Grad: 5434.5542  LR: 0.000007  \n",
      "Epoch: [1][1300/36908] Elapsed 16m 53s (remain 462m 25s) Loss: 0.0323(0.1017) Grad: 95906.6172  LR: 0.000007  \n",
      "Epoch: [1][1400/36908] Elapsed 18m 10s (remain 460m 42s) Loss: 0.0236(0.0956) Grad: 61527.9258  LR: 0.000008  \n",
      "Epoch: [1][1500/36908] Elapsed 19m 29s (remain 459m 36s) Loss: 0.0368(0.0905) Grad: 77821.3984  LR: 0.000008  \n",
      "Epoch: [1][1600/36908] Elapsed 20m 48s (remain 458m 52s) Loss: 0.0178(0.0859) Grad: 83472.8281  LR: 0.000009  \n",
      "Epoch: [1][1700/36908] Elapsed 22m 6s (remain 457m 33s) Loss: 0.0182(0.0817) Grad: 82593.0000  LR: 0.000009  \n",
      "Epoch: [1][1800/36908] Elapsed 23m 23s (remain 456m 3s) Loss: 0.0040(0.0780) Grad: 4975.6567  LR: 0.000010  \n",
      "Epoch: [1][1900/36908] Elapsed 24m 43s (remain 455m 11s) Loss: 0.0133(0.0747) Grad: 31437.9648  LR: 0.000010  \n",
      "Epoch: [1][2000/36908] Elapsed 26m 3s (remain 454m 31s) Loss: 0.0154(0.0717) Grad: 74707.0078  LR: 0.000011  \n",
      "Epoch: [1][2100/36908] Elapsed 27m 21s (remain 453m 16s) Loss: 0.0015(0.0691) Grad: 3572.7090  LR: 0.000011  \n",
      "Epoch: [1][2200/36908] Elapsed 28m 39s (remain 451m 55s) Loss: 0.0257(0.0666) Grad: 67610.2344  LR: 0.000012  \n",
      "Epoch: [1][2300/36908] Elapsed 29m 57s (remain 450m 34s) Loss: 0.0029(0.0642) Grad: 19542.1895  LR: 0.000012  \n",
      "Epoch: [1][2400/36908] Elapsed 31m 17s (remain 449m 36s) Loss: 0.0029(0.0620) Grad: 71683.6562  LR: 0.000013  \n",
      "Epoch: [1][2500/36908] Elapsed 32m 38s (remain 448m 59s) Loss: 0.0228(0.0600) Grad: 107844.3906  LR: 0.000014  \n",
      "Epoch: [1][2600/36908] Elapsed 33m 56s (remain 447m 39s) Loss: 0.0005(0.0581) Grad: 2483.9602  LR: 0.000014  \n",
      "Epoch: [1][2700/36908] Elapsed 35m 14s (remain 446m 16s) Loss: 0.0014(0.0564) Grad: 3975.5813  LR: 0.000015  \n",
      "Epoch: [1][2800/36908] Elapsed 36m 32s (remain 444m 53s) Loss: 0.0010(0.0548) Grad: 1711.0812  LR: 0.000015  \n",
      "Epoch: [1][2900/36908] Elapsed 37m 51s (remain 443m 43s) Loss: 0.0027(0.0532) Grad: 19307.9336  LR: 0.000016  \n",
      "Epoch: [1][3000/36908] Elapsed 39m 9s (remain 442m 29s) Loss: 0.0082(0.0518) Grad: 23824.5664  LR: 0.000016  \n",
      "Epoch: [1][3100/36908] Elapsed 40m 27s (remain 441m 7s) Loss: 0.0008(0.0505) Grad: 8042.0151  LR: 0.000017  \n",
      "Epoch: [1][3200/36908] Elapsed 41m 45s (remain 439m 46s) Loss: 0.0052(0.0493) Grad: 86315.2266  LR: 0.000017  \n",
      "Epoch: [1][3300/36908] Elapsed 43m 6s (remain 438m 54s) Loss: 0.0199(0.0480) Grad: 13697.4688  LR: 0.000018  \n",
      "Epoch: [1][3400/36908] Elapsed 44m 24s (remain 437m 33s) Loss: 0.0037(0.0470) Grad: 28398.2598  LR: 0.000018  \n",
      "Epoch: [1][3500/36908] Elapsed 45m 42s (remain 436m 10s) Loss: 0.0008(0.0459) Grad: 534.8162  LR: 0.000019  \n",
      "Epoch: [1][3600/36908] Elapsed 47m 0s (remain 434m 47s) Loss: 0.0071(0.0449) Grad: 63496.0820  LR: 0.000020  \n",
      "Epoch: [1][3700/36908] Elapsed 48m 19s (remain 433m 35s) Loss: 0.0116(0.0439) Grad: 31512.3086  LR: 0.000020  \n",
      "Epoch: [1][3800/36908] Elapsed 49m 39s (remain 432m 28s) Loss: 0.0005(0.0431) Grad: 2182.3206  LR: 0.000020  \n",
      "Epoch: [1][3900/36908] Elapsed 50m 57s (remain 431m 11s) Loss: 0.0010(0.0422) Grad: 7291.6084  LR: 0.000020  \n",
      "Epoch: [1][4000/36908] Elapsed 52m 15s (remain 429m 50s) Loss: 0.0003(0.0413) Grad: 4170.0693  LR: 0.000020  \n",
      "Epoch: [1][4100/36908] Elapsed 53m 36s (remain 428m 49s) Loss: 0.0021(0.0406) Grad: 46761.4766  LR: 0.000020  \n",
      "Epoch: [1][4200/36908] Elapsed 54m 54s (remain 427m 31s) Loss: 0.0044(0.0398) Grad: 203158.0312  LR: 0.000020  \n",
      "Epoch: [1][4300/36908] Elapsed 56m 12s (remain 426m 10s) Loss: 0.0003(0.0390) Grad: 483.6914  LR: 0.000020  \n",
      "Epoch: [1][4400/36908] Elapsed 57m 32s (remain 425m 0s) Loss: 0.0044(0.0384) Grad: 152961.8125  LR: 0.000020  \n",
      "Epoch: [1][4500/36908] Elapsed 58m 51s (remain 423m 45s) Loss: 0.0001(0.0377) Grad: 126.9606  LR: 0.000020  \n",
      "Epoch: [1][4600/36908] Elapsed 60m 10s (remain 422m 29s) Loss: 0.0001(0.0371) Grad: 56.4431  LR: 0.000019  \n",
      "Epoch: [1][4700/36908] Elapsed 61m 28s (remain 421m 11s) Loss: 0.0069(0.0364) Grad: 53275.6836  LR: 0.000019  \n",
      "Epoch: [1][4800/36908] Elapsed 62m 48s (remain 419m 59s) Loss: 0.0008(0.0359) Grad: 20925.2383  LR: 0.000019  \n",
      "Epoch: [1][4900/36908] Elapsed 64m 7s (remain 418m 45s) Loss: 0.0002(0.0353) Grad: 242.7909  LR: 0.000019  \n",
      "Epoch: [1][5000/36908] Elapsed 65m 25s (remain 417m 27s) Loss: 0.0051(0.0347) Grad: 40791.6992  LR: 0.000019  \n",
      "Epoch: [1][5100/36908] Elapsed 66m 42s (remain 416m 0s) Loss: 0.0768(0.0342) Grad: 85407.1797  LR: 0.000019  \n",
      "Epoch: [1][5200/36908] Elapsed 68m 0s (remain 414m 35s) Loss: 0.0199(0.0337) Grad: 30655.9355  LR: 0.000019  \n",
      "Epoch: [1][5300/36908] Elapsed 69m 18s (remain 413m 15s) Loss: 0.0005(0.0332) Grad: 487.2214  LR: 0.000019  \n",
      "Epoch: [1][5400/36908] Elapsed 70m 36s (remain 411m 54s) Loss: 0.0026(0.0327) Grad: 11878.5283  LR: 0.000019  \n",
      "Epoch: [1][5500/36908] Elapsed 71m 54s (remain 410m 33s) Loss: 0.0023(0.0322) Grad: 5977.2153  LR: 0.000019  \n",
      "Epoch: [1][5600/36908] Elapsed 73m 11s (remain 409m 8s) Loss: 0.0002(0.0317) Grad: 381.9124  LR: 0.000019  \n",
      "Epoch: [1][5700/36908] Elapsed 74m 29s (remain 407m 43s) Loss: 0.0000(0.0313) Grad: 41.4418  LR: 0.000019  \n",
      "Epoch: [1][5800/36908] Elapsed 75m 47s (remain 406m 26s) Loss: 0.0165(0.0308) Grad: 3576.7622  LR: 0.000019  \n",
      "Epoch: [1][5900/36908] Elapsed 77m 6s (remain 405m 10s) Loss: 0.0001(0.0304) Grad: 80.9867  LR: 0.000019  \n",
      "Epoch: [1][6000/36908] Elapsed 78m 23s (remain 403m 45s) Loss: 0.0003(0.0300) Grad: 937.6409  LR: 0.000019  \n",
      "Epoch: [1][6100/36908] Elapsed 79m 41s (remain 402m 24s) Loss: 0.0002(0.0296) Grad: 9121.9492  LR: 0.000019  \n",
      "Epoch: [1][6200/36908] Elapsed 80m 58s (remain 400m 58s) Loss: 0.0008(0.0293) Grad: 1009.4432  LR: 0.000018  \n",
      "Epoch: [1][6300/36908] Elapsed 82m 15s (remain 399m 32s) Loss: 0.0000(0.0289) Grad: 100.0031  LR: 0.000018  \n",
      "Epoch: [1][6400/36908] Elapsed 83m 36s (remain 398m 29s) Loss: 0.0224(0.0285) Grad: 127259.7422  LR: 0.000018  \n",
      "Epoch: [1][6500/36908] Elapsed 84m 53s (remain 397m 4s) Loss: 0.0002(0.0282) Grad: 286.7711  LR: 0.000018  \n",
      "Epoch: [1][6600/36908] Elapsed 86m 10s (remain 395m 38s) Loss: 0.0002(0.0279) Grad: 163.5601  LR: 0.000018  \n",
      "Epoch: [1][6700/36908] Elapsed 87m 27s (remain 394m 13s) Loss: 0.0032(0.0275) Grad: 9723.7656  LR: 0.000018  \n",
      "Epoch: [1][6800/36908] Elapsed 88m 45s (remain 392m 55s) Loss: 0.0001(0.0272) Grad: 64.3143  LR: 0.000018  \n",
      "Epoch: [1][6900/36908] Elapsed 90m 4s (remain 391m 41s) Loss: 0.0003(0.0269) Grad: 721.7730  LR: 0.000018  \n",
      "Epoch: [1][7000/36908] Elapsed 91m 22s (remain 390m 20s) Loss: 0.0001(0.0266) Grad: 90.0552  LR: 0.000018  \n",
      "Epoch: [1][7100/36908] Elapsed 92m 40s (remain 389m 0s) Loss: 0.0031(0.0263) Grad: 4526.1201  LR: 0.000018  \n",
      "Epoch: [1][7200/36908] Elapsed 93m 58s (remain 387m 42s) Loss: 0.0004(0.0260) Grad: 470.2819  LR: 0.000018  \n",
      "Epoch: [1][7300/36908] Elapsed 95m 18s (remain 386m 28s) Loss: 0.0020(0.0257) Grad: 12116.0127  LR: 0.000018  \n",
      "Epoch: [1][7400/36908] Elapsed 96m 36s (remain 385m 8s) Loss: 0.0006(0.0255) Grad: 3821.7334  LR: 0.000018  \n",
      "Epoch: [1][7500/36908] Elapsed 97m 54s (remain 383m 50s) Loss: 0.0011(0.0252) Grad: 5498.6455  LR: 0.000018  \n",
      "Epoch: [1][7600/36908] Elapsed 99m 13s (remain 382m 35s) Loss: 0.0026(0.0249) Grad: 33281.5703  LR: 0.000018  \n",
      "Epoch: [1][7700/36908] Elapsed 100m 31s (remain 381m 16s) Loss: 0.0001(0.0247) Grad: 208.7263  LR: 0.000018  \n",
      "Epoch: [1][7800/36908] Elapsed 101m 49s (remain 379m 57s) Loss: 0.0005(0.0244) Grad: 2125.8003  LR: 0.000018  \n",
      "Epoch: [1][7900/36908] Elapsed 103m 8s (remain 378m 39s) Loss: 0.0026(0.0242) Grad: 7436.8311  LR: 0.000017  \n",
      "Epoch: [1][8000/36908] Elapsed 104m 28s (remain 377m 28s) Loss: 0.0002(0.0239) Grad: 445.5814  LR: 0.000017  \n",
      "Epoch: [1][8100/36908] Elapsed 105m 47s (remain 376m 13s) Loss: 0.0007(0.0237) Grad: 1834.6801  LR: 0.000017  \n",
      "Epoch: [1][8200/36908] Elapsed 107m 5s (remain 374m 53s) Loss: 0.0001(0.0235) Grad: 91.7164  LR: 0.000017  \n",
      "Epoch: [1][8300/36908] Elapsed 108m 25s (remain 373m 40s) Loss: 0.0000(0.0233) Grad: 123.4489  LR: 0.000017  \n",
      "Epoch: [1][8400/36908] Elapsed 109m 45s (remain 372m 24s) Loss: 0.0000(0.0231) Grad: 37.4944  LR: 0.000017  \n",
      "Epoch: [1][8500/36908] Elapsed 111m 3s (remain 371m 7s) Loss: 0.0150(0.0229) Grad: 39610.0977  LR: 0.000017  \n",
      "Epoch: [1][8600/36908] Elapsed 112m 25s (remain 370m 0s) Loss: 0.0001(0.0226) Grad: 661.8862  LR: 0.000017  \n",
      "Epoch: [1][8700/36908] Elapsed 113m 42s (remain 368m 36s) Loss: 0.0000(0.0224) Grad: 95.2581  LR: 0.000017  \n",
      "Epoch: [1][8800/36908] Elapsed 114m 59s (remain 367m 13s) Loss: 0.0026(0.0222) Grad: 32149.2188  LR: 0.000017  \n",
      "Epoch: [1][8900/36908] Elapsed 116m 15s (remain 365m 48s) Loss: 0.0000(0.0220) Grad: 71.6262  LR: 0.000017  \n",
      "Epoch: [1][9000/36908] Elapsed 117m 32s (remain 364m 26s) Loss: 0.0000(0.0218) Grad: 271.1232  LR: 0.000017  \n",
      "Epoch: [1][9100/36908] Elapsed 118m 53s (remain 363m 16s) Loss: 0.0011(0.0217) Grad: 4321.9409  LR: 0.000017  \n",
      "Epoch: [1][9200/36908] Elapsed 120m 10s (remain 361m 53s) Loss: 0.0000(0.0215) Grad: 99.2799  LR: 0.000017  \n",
      "Epoch: [1][9300/36908] Elapsed 121m 27s (remain 360m 29s) Loss: 0.0278(0.0213) Grad: 26534.0781  LR: 0.000017  \n",
      "Epoch: [1][9400/36908] Elapsed 122m 45s (remain 359m 9s) Loss: 0.0025(0.0211) Grad: 13213.8467  LR: 0.000017  \n",
      "Epoch: [1][9500/36908] Elapsed 124m 6s (remain 357m 59s) Loss: 0.0010(0.0209) Grad: 2744.3835  LR: 0.000017  \n",
      "Epoch: [1][9600/36908] Elapsed 125m 23s (remain 356m 37s) Loss: 0.0067(0.0207) Grad: 17922.6445  LR: 0.000016  \n",
      "Epoch: [1][9700/36908] Elapsed 126m 40s (remain 355m 17s) Loss: 0.0001(0.0206) Grad: 98.7626  LR: 0.000016  \n",
      "Epoch: [1][9800/36908] Elapsed 127m 58s (remain 353m 57s) Loss: 0.0001(0.0204) Grad: 534.2458  LR: 0.000016  \n",
      "Epoch: [1][9900/36908] Elapsed 129m 15s (remain 352m 34s) Loss: 0.0001(0.0202) Grad: 110.5514  LR: 0.000016  \n",
      "Epoch: [1][10000/36908] Elapsed 130m 32s (remain 351m 13s) Loss: 0.0028(0.0201) Grad: 4253.3994  LR: 0.000016  \n",
      "Epoch: [1][10100/36908] Elapsed 131m 51s (remain 349m 55s) Loss: 0.0038(0.0199) Grad: 21858.7812  LR: 0.000016  \n",
      "Epoch: [1][10200/36908] Elapsed 133m 7s (remain 348m 32s) Loss: 0.0002(0.0198) Grad: 1766.1611  LR: 0.000016  \n",
      "Epoch: [1][10300/36908] Elapsed 134m 26s (remain 347m 14s) Loss: 0.0009(0.0196) Grad: 756.7286  LR: 0.000016  \n",
      "Epoch: [1][10400/36908] Elapsed 135m 42s (remain 345m 51s) Loss: 0.0030(0.0195) Grad: 29867.6152  LR: 0.000016  \n",
      "Epoch: [1][10500/36908] Elapsed 136m 59s (remain 344m 28s) Loss: 0.0054(0.0193) Grad: 11082.3223  LR: 0.000016  \n",
      "Epoch: [1][10600/36908] Elapsed 138m 20s (remain 343m 18s) Loss: 0.0025(0.0192) Grad: 2401.2808  LR: 0.000016  \n",
      "Epoch: [1][10700/36908] Elapsed 139m 38s (remain 341m 58s) Loss: 0.0053(0.0190) Grad: 10012.8213  LR: 0.000016  \n",
      "Epoch: [1][10800/36908] Elapsed 141m 1s (remain 340m 51s) Loss: 0.0000(0.0189) Grad: 7.6397  LR: 0.000016  \n",
      "Epoch: [1][10900/36908] Elapsed 142m 23s (remain 339m 43s) Loss: 0.0010(0.0188) Grad: 2342.6479  LR: 0.000016  \n",
      "Epoch: [1][11000/36908] Elapsed 143m 41s (remain 338m 22s) Loss: 0.0003(0.0186) Grad: 912.2682  LR: 0.000016  \n",
      "Epoch: [1][11100/36908] Elapsed 144m 57s (remain 337m 0s) Loss: 0.0085(0.0185) Grad: 17450.6562  LR: 0.000016  \n",
      "Epoch: [1][11200/36908] Elapsed 146m 14s (remain 335m 37s) Loss: 0.0001(0.0184) Grad: 310.5153  LR: 0.000015  \n",
      "Epoch: [1][11300/36908] Elapsed 147m 30s (remain 334m 14s) Loss: 0.0001(0.0183) Grad: 100.9499  LR: 0.000015  \n",
      "Epoch: [1][11400/36908] Elapsed 148m 48s (remain 332m 55s) Loss: 0.0002(0.0181) Grad: 1073.4541  LR: 0.000015  \n",
      "Epoch: [1][11500/36908] Elapsed 150m 8s (remain 331m 41s) Loss: 0.0073(0.0180) Grad: 2037.9402  LR: 0.000015  \n",
      "Epoch: [1][11600/36908] Elapsed 151m 26s (remain 330m 22s) Loss: 0.0007(0.0179) Grad: 1483.3578  LR: 0.000015  \n",
      "Epoch: [1][11700/36908] Elapsed 152m 43s (remain 329m 0s) Loss: 0.0001(0.0178) Grad: 358.5371  LR: 0.000015  \n",
      "Epoch: [1][11800/36908] Elapsed 154m 1s (remain 327m 40s) Loss: 0.0003(0.0176) Grad: 2260.3730  LR: 0.000015  \n",
      "Epoch: [1][11900/36908] Elapsed 155m 18s (remain 326m 20s) Loss: 0.0000(0.0175) Grad: 32.0060  LR: 0.000015  \n",
      "Epoch: [1][12000/36908] Elapsed 156m 34s (remain 324m 58s) Loss: 0.0060(0.0174) Grad: 55943.6250  LR: 0.000015  \n",
      "Epoch: [1][12100/36908] Elapsed 157m 51s (remain 323m 36s) Loss: 0.0000(0.0173) Grad: 14.0599  LR: 0.000015  \n",
      "Epoch: [1][12200/36908] Elapsed 159m 8s (remain 322m 15s) Loss: 0.0000(0.0172) Grad: 68.1129  LR: 0.000015  \n",
      "Epoch: [1][12300/36908] Elapsed 160m 26s (remain 320m 57s) Loss: 0.0087(0.0171) Grad: 6377.0278  LR: 0.000015  \n",
      "Epoch: [1][12400/36908] Elapsed 161m 46s (remain 319m 41s) Loss: 0.0001(0.0170) Grad: 75.9589  LR: 0.000015  \n",
      "Epoch: [1][12500/36908] Elapsed 163m 2s (remain 318m 19s) Loss: 0.0000(0.0169) Grad: 55.4090  LR: 0.000015  \n",
      "Epoch: [1][12600/36908] Elapsed 164m 22s (remain 317m 3s) Loss: 0.0023(0.0168) Grad: 6101.1797  LR: 0.000015  \n",
      "Epoch: [1][12700/36908] Elapsed 165m 40s (remain 315m 46s) Loss: 0.0000(0.0167) Grad: 18.2867  LR: 0.000015  \n",
      "Epoch: [1][12800/36908] Elapsed 166m 56s (remain 314m 24s) Loss: 0.0009(0.0166) Grad: 594.2085  LR: 0.000015  \n",
      "Epoch: [1][12900/36908] Elapsed 168m 15s (remain 313m 6s) Loss: 0.0000(0.0165) Grad: 9.0111  LR: 0.000014  \n",
      "Epoch: [1][13000/36908] Elapsed 169m 33s (remain 311m 48s) Loss: 0.0053(0.0164) Grad: 12488.0752  LR: 0.000014  \n",
      "Epoch: [1][13100/36908] Elapsed 170m 50s (remain 310m 26s) Loss: 0.0001(0.0163) Grad: 285.5062  LR: 0.000014  \n",
      "Epoch: [1][13200/36908] Elapsed 172m 6s (remain 309m 4s) Loss: 0.0000(0.0162) Grad: 55.3023  LR: 0.000014  \n",
      "Epoch: [1][13300/36908] Elapsed 173m 22s (remain 307m 42s) Loss: 0.0159(0.0161) Grad: 6077.3540  LR: 0.000014  \n",
      "Epoch: [1][13400/36908] Elapsed 174m 41s (remain 306m 25s) Loss: 0.0050(0.0160) Grad: 8670.2773  LR: 0.000014  \n",
      "Epoch: [1][13500/36908] Elapsed 176m 0s (remain 305m 8s) Loss: 0.0034(0.0159) Grad: 1188.0691  LR: 0.000014  \n",
      "Epoch: [1][13600/36908] Elapsed 177m 22s (remain 303m 56s) Loss: 0.0262(0.0159) Grad: 28009.5820  LR: 0.000014  \n",
      "Epoch: [1][13700/36908] Elapsed 178m 40s (remain 302m 38s) Loss: 0.0000(0.0158) Grad: 25.6229  LR: 0.000014  \n",
      "Epoch: [1][13800/36908] Elapsed 179m 58s (remain 301m 19s) Loss: 0.0073(0.0157) Grad: 44733.2578  LR: 0.000014  \n",
      "Epoch: [1][13900/36908] Elapsed 181m 17s (remain 300m 3s) Loss: 0.0001(0.0156) Grad: 230.9746  LR: 0.000014  \n",
      "Epoch: [1][14000/36908] Elapsed 182m 41s (remain 298m 54s) Loss: 0.0007(0.0155) Grad: 1574.3456  LR: 0.000014  \n",
      "Epoch: [1][14100/36908] Elapsed 183m 59s (remain 297m 34s) Loss: 0.0000(0.0154) Grad: 28.7803  LR: 0.000014  \n",
      "Epoch: [1][14200/36908] Elapsed 185m 16s (remain 296m 15s) Loss: 0.0000(0.0153) Grad: 120.7225  LR: 0.000014  \n",
      "Epoch: [1][14300/36908] Elapsed 186m 34s (remain 294m 55s) Loss: 0.0001(0.0153) Grad: 266.1989  LR: 0.000014  \n",
      "Epoch: [1][14400/36908] Elapsed 187m 52s (remain 293m 37s) Loss: 0.0048(0.0152) Grad: 18651.9746  LR: 0.000014  \n",
      "Epoch: [1][14500/36908] Elapsed 189m 14s (remain 292m 25s) Loss: 0.0000(0.0151) Grad: 23.2940  LR: 0.000013  \n",
      "Epoch: [1][14600/36908] Elapsed 190m 34s (remain 291m 8s) Loss: 0.0006(0.0150) Grad: 4584.8521  LR: 0.000013  \n",
      "Epoch: [1][14700/36908] Elapsed 191m 52s (remain 289m 50s) Loss: 0.0024(0.0149) Grad: 11821.2041  LR: 0.000013  \n",
      "Epoch: [1][14800/36908] Elapsed 193m 9s (remain 288m 30s) Loss: 0.0002(0.0149) Grad: 2287.3452  LR: 0.000013  \n",
      "Epoch: [1][14900/36908] Elapsed 194m 28s (remain 287m 12s) Loss: 0.0052(0.0148) Grad: 245563.4688  LR: 0.000013  \n",
      "Epoch: [1][15000/36908] Elapsed 195m 47s (remain 285m 56s) Loss: 0.0019(0.0147) Grad: 12308.9844  LR: 0.000013  \n",
      "Epoch: [1][15100/36908] Elapsed 197m 5s (remain 284m 36s) Loss: 0.0007(0.0146) Grad: 2151.3218  LR: 0.000013  \n",
      "Epoch: [1][15200/36908] Elapsed 198m 24s (remain 283m 19s) Loss: 0.0087(0.0146) Grad: 10315.7412  LR: 0.000013  \n",
      "Epoch: [1][15300/36908] Elapsed 199m 42s (remain 282m 0s) Loss: 0.0046(0.0145) Grad: 3877.4604  LR: 0.000013  \n",
      "Epoch: [1][15400/36908] Elapsed 200m 59s (remain 280m 41s) Loss: 0.0000(0.0144) Grad: 107.5223  LR: 0.000013  \n",
      "Epoch: [1][15500/36908] Elapsed 202m 17s (remain 279m 21s) Loss: 0.0082(0.0144) Grad: 12637.1885  LR: 0.000013  \n",
      "Epoch: [1][15600/36908] Elapsed 203m 34s (remain 278m 1s) Loss: 0.0000(0.0143) Grad: 11.3657  LR: 0.000013  \n",
      "Epoch: [1][15700/36908] Elapsed 204m 51s (remain 276m 42s) Loss: 0.0000(0.0142) Grad: 47.8417  LR: 0.000013  \n",
      "Epoch: [1][15800/36908] Elapsed 206m 10s (remain 275m 24s) Loss: 0.0025(0.0141) Grad: 7428.8301  LR: 0.000013  \n",
      "Epoch: [1][15900/36908] Elapsed 207m 34s (remain 274m 13s) Loss: 0.0001(0.0141) Grad: 282.7628  LR: 0.000013  \n",
      "Epoch: [1][16000/36908] Elapsed 208m 52s (remain 272m 54s) Loss: 0.0002(0.0140) Grad: 1288.3301  LR: 0.000013  \n",
      "Epoch: [1][16100/36908] Elapsed 210m 10s (remain 271m 36s) Loss: 0.0001(0.0139) Grad: 351.9977  LR: 0.000013  \n",
      "Epoch: [1][16200/36908] Elapsed 211m 29s (remain 270m 18s) Loss: 0.0000(0.0139) Grad: 131.9019  LR: 0.000012  \n",
      "Epoch: [1][16300/36908] Elapsed 212m 47s (remain 269m 0s) Loss: 0.0003(0.0138) Grad: 5367.1250  LR: 0.000012  \n",
      "Epoch: [1][16400/36908] Elapsed 214m 3s (remain 267m 38s) Loss: 0.0122(0.0138) Grad: 24402.1250  LR: 0.000012  \n",
      "Epoch: [1][16500/36908] Elapsed 215m 19s (remain 266m 17s) Loss: 0.0007(0.0137) Grad: 1331.9978  LR: 0.000012  \n",
      "Epoch: [1][16600/36908] Elapsed 216m 37s (remain 264m 58s) Loss: 0.0005(0.0136) Grad: 625.1984  LR: 0.000012  \n",
      "Epoch: [1][16700/36908] Elapsed 217m 53s (remain 263m 37s) Loss: 0.0261(0.0136) Grad: 33969.8320  LR: 0.000012  \n",
      "Epoch: [1][16800/36908] Elapsed 219m 9s (remain 262m 17s) Loss: 0.0000(0.0135) Grad: 57.5406  LR: 0.000012  \n",
      "Epoch: [1][16900/36908] Elapsed 220m 26s (remain 260m 57s) Loss: 0.0000(0.0135) Grad: 32.9510  LR: 0.000012  \n",
      "Epoch: [1][17000/36908] Elapsed 221m 44s (remain 259m 38s) Loss: 0.0000(0.0134) Grad: 33.1379  LR: 0.000012  \n",
      "Epoch: [1][17100/36908] Elapsed 223m 6s (remain 258m 24s) Loss: 0.0000(0.0133) Grad: 27.0608  LR: 0.000012  \n",
      "Epoch: [1][17200/36908] Elapsed 224m 23s (remain 257m 4s) Loss: 0.0030(0.0133) Grad: 17588.9473  LR: 0.000012  \n",
      "Epoch: [1][17300/36908] Elapsed 225m 39s (remain 255m 43s) Loss: 0.0000(0.0132) Grad: 58.5945  LR: 0.000012  \n",
      "Epoch: [1][17400/36908] Elapsed 226m 54s (remain 254m 22s) Loss: 0.0000(0.0132) Grad: 42.4200  LR: 0.000012  \n",
      "Epoch: [1][17500/36908] Elapsed 228m 12s (remain 253m 3s) Loss: 0.0001(0.0131) Grad: 550.4005  LR: 0.000012  \n",
      "Epoch: [1][17600/36908] Elapsed 229m 28s (remain 251m 43s) Loss: 0.0001(0.0131) Grad: 385.3363  LR: 0.000012  \n",
      "Epoch: [1][17700/36908] Elapsed 230m 46s (remain 250m 24s) Loss: 0.0000(0.0130) Grad: 41.7445  LR: 0.000012  \n",
      "Epoch: [1][17800/36908] Elapsed 232m 3s (remain 249m 4s) Loss: 0.0007(0.0130) Grad: 4084.7271  LR: 0.000012  \n",
      "Epoch: [1][17900/36908] Elapsed 233m 19s (remain 247m 44s) Loss: 0.0009(0.0129) Grad: 4414.1831  LR: 0.000011  \n",
      "Epoch: [1][18000/36908] Elapsed 234m 35s (remain 246m 23s) Loss: 0.0000(0.0128) Grad: 60.3003  LR: 0.000011  \n",
      "Epoch: [1][18100/36908] Elapsed 235m 52s (remain 245m 4s) Loss: 0.0100(0.0128) Grad: 71823.3906  LR: 0.000011  \n",
      "Epoch: [1][18200/36908] Elapsed 237m 10s (remain 243m 46s) Loss: 0.0057(0.0127) Grad: 12592.7676  LR: 0.000011  \n",
      "Epoch: [1][18300/36908] Elapsed 238m 26s (remain 242m 26s) Loss: 0.0000(0.0127) Grad: 1.8653  LR: 0.000011  \n",
      "Epoch: [1][18400/36908] Elapsed 239m 43s (remain 241m 5s) Loss: 0.0000(0.0126) Grad: 1.6074  LR: 0.000011  \n",
      "Epoch: [1][18500/36908] Elapsed 240m 58s (remain 239m 44s) Loss: 0.0052(0.0126) Grad: 5643.9673  LR: 0.000011  \n",
      "Epoch: [1][18600/36908] Elapsed 242m 15s (remain 238m 26s) Loss: 0.0013(0.0125) Grad: 17105.0742  LR: 0.000011  \n",
      "Epoch: [1][18700/36908] Elapsed 243m 31s (remain 237m 6s) Loss: 0.0000(0.0125) Grad: 45.6404  LR: 0.000011  \n",
      "Epoch: [1][18800/36908] Elapsed 244m 49s (remain 235m 47s) Loss: 0.0000(0.0124) Grad: 84.4178  LR: 0.000011  \n",
      "Epoch: [1][18900/36908] Elapsed 246m 7s (remain 234m 28s) Loss: 0.0001(0.0124) Grad: 141.9292  LR: 0.000011  \n",
      "Epoch: [1][19000/36908] Elapsed 247m 28s (remain 233m 13s) Loss: 0.0000(0.0123) Grad: 18.1013  LR: 0.000011  \n",
      "Epoch: [1][19100/36908] Elapsed 248m 50s (remain 231m 59s) Loss: 0.0026(0.0123) Grad: 49553.7422  LR: 0.000011  \n",
      "Epoch: [1][19200/36908] Elapsed 250m 7s (remain 230m 39s) Loss: 0.0000(0.0122) Grad: 37.9524  LR: 0.000011  \n",
      "Epoch: [1][19300/36908] Elapsed 251m 27s (remain 229m 22s) Loss: 0.0000(0.0122) Grad: 9.0174  LR: 0.000011  \n",
      "Epoch: [1][19400/36908] Elapsed 252m 44s (remain 228m 3s) Loss: 0.0017(0.0121) Grad: 2956.9392  LR: 0.000011  \n",
      "Epoch: [1][19500/36908] Elapsed 254m 0s (remain 226m 44s) Loss: 0.0001(0.0121) Grad: 392.1840  LR: 0.000010  \n",
      "Epoch: [1][19600/36908] Elapsed 255m 19s (remain 225m 26s) Loss: 0.0057(0.0120) Grad: 6170.4111  LR: 0.000010  \n",
      "Epoch: [1][19700/36908] Elapsed 256m 37s (remain 224m 8s) Loss: 0.0000(0.0120) Grad: 10.0005  LR: 0.000010  \n",
      "Epoch: [1][19800/36908] Elapsed 257m 55s (remain 222m 49s) Loss: 0.0005(0.0120) Grad: 469.4912  LR: 0.000010  \n",
      "Epoch: [1][19900/36908] Elapsed 259m 11s (remain 221m 30s) Loss: 0.0001(0.0119) Grad: 56.3199  LR: 0.000010  \n",
      "Epoch: [1][20000/36908] Elapsed 260m 28s (remain 220m 10s) Loss: 0.0020(0.0119) Grad: 3816.0193  LR: 0.000010  \n",
      "Epoch: [1][20100/36908] Elapsed 261m 44s (remain 218m 51s) Loss: 0.0000(0.0118) Grad: 111.0782  LR: 0.000010  \n",
      "Epoch: [1][20200/36908] Elapsed 263m 2s (remain 217m 32s) Loss: 0.0001(0.0118) Grad: 127.7005  LR: 0.000010  \n",
      "Epoch: [1][20300/36908] Elapsed 264m 20s (remain 216m 14s) Loss: 0.0045(0.0117) Grad: 5511.9546  LR: 0.000010  \n",
      "Epoch: [1][20400/36908] Elapsed 265m 38s (remain 214m 56s) Loss: 0.0032(0.0117) Grad: 35816.3438  LR: 0.000010  \n",
      "Epoch: [1][20500/36908] Elapsed 266m 59s (remain 213m 40s) Loss: 0.0005(0.0117) Grad: 3574.8999  LR: 0.000010  \n",
      "Epoch: [1][20600/36908] Elapsed 268m 17s (remain 212m 22s) Loss: 0.0004(0.0116) Grad: 3515.1145  LR: 0.000010  \n",
      "Epoch: [1][20700/36908] Elapsed 269m 34s (remain 211m 2s) Loss: 0.0000(0.0116) Grad: 148.1356  LR: 0.000010  \n",
      "Epoch: [1][20800/36908] Elapsed 270m 51s (remain 209m 44s) Loss: 0.0001(0.0115) Grad: 199.7247  LR: 0.000010  \n",
      "Epoch: [1][20900/36908] Elapsed 272m 8s (remain 208m 25s) Loss: 0.0001(0.0115) Grad: 210.1956  LR: 0.000010  \n",
      "Epoch: [1][21000/36908] Elapsed 273m 26s (remain 207m 6s) Loss: 0.0091(0.0115) Grad: 34028.5547  LR: 0.000010  \n",
      "Epoch: [1][21100/36908] Elapsed 274m 46s (remain 205m 50s) Loss: 0.0010(0.0114) Grad: 9237.9424  LR: 0.000010  \n",
      "Epoch: [1][21200/36908] Elapsed 276m 9s (remain 204m 35s) Loss: 0.0004(0.0114) Grad: 1949.4829  LR: 0.000009  \n",
      "Epoch: [1][21300/36908] Elapsed 277m 25s (remain 203m 16s) Loss: 0.0001(0.0113) Grad: 190.6087  LR: 0.000009  \n",
      "Epoch: [1][21400/36908] Elapsed 278m 41s (remain 201m 56s) Loss: 0.0000(0.0113) Grad: 8.8970  LR: 0.000009  \n",
      "Epoch: [1][21500/36908] Elapsed 279m 57s (remain 200m 36s) Loss: 0.0021(0.0113) Grad: 3938.5945  LR: 0.000009  \n",
      "Epoch: [1][21600/36908] Elapsed 281m 14s (remain 199m 18s) Loss: 0.0023(0.0112) Grad: 2100.6870  LR: 0.000009  \n",
      "Epoch: [1][21700/36908] Elapsed 282m 32s (remain 197m 59s) Loss: 0.0032(0.0112) Grad: 20452.9395  LR: 0.000009  \n",
      "Epoch: [1][21800/36908] Elapsed 283m 49s (remain 196m 40s) Loss: 0.0000(0.0112) Grad: 89.2197  LR: 0.000009  \n",
      "Epoch: [1][21900/36908] Elapsed 285m 5s (remain 195m 21s) Loss: 0.0008(0.0111) Grad: 787.5074  LR: 0.000009  \n",
      "Epoch: [1][22000/36908] Elapsed 286m 23s (remain 194m 2s) Loss: 0.0000(0.0111) Grad: 337.1304  LR: 0.000009  \n",
      "Epoch: [1][22100/36908] Elapsed 287m 43s (remain 192m 45s) Loss: 0.0023(0.0110) Grad: 10160.4219  LR: 0.000009  \n",
      "Epoch: [1][22200/36908] Elapsed 289m 1s (remain 191m 27s) Loss: 0.0001(0.0110) Grad: 932.0854  LR: 0.000009  \n",
      "Epoch: [1][22300/36908] Elapsed 290m 18s (remain 190m 9s) Loss: 0.0001(0.0110) Grad: 213.0247  LR: 0.000009  \n",
      "Epoch: [1][22400/36908] Elapsed 291m 37s (remain 188m 51s) Loss: 0.0001(0.0109) Grad: 175.1613  LR: 0.000009  \n",
      "Epoch: [1][22500/36908] Elapsed 292m 56s (remain 187m 33s) Loss: 0.0027(0.0109) Grad: 4778.8716  LR: 0.000009  \n",
      "Epoch: [1][22600/36908] Elapsed 294m 14s (remain 186m 15s) Loss: 0.0009(0.0109) Grad: 6136.6143  LR: 0.000009  \n",
      "Epoch: [1][22700/36908] Elapsed 295m 32s (remain 184m 57s) Loss: 0.0076(0.0108) Grad: 37282.2539  LR: 0.000009  \n",
      "Epoch: [1][22800/36908] Elapsed 296m 49s (remain 183m 38s) Loss: 0.0010(0.0108) Grad: 4490.2256  LR: 0.000008  \n",
      "Epoch: [1][22900/36908] Elapsed 298m 7s (remain 182m 20s) Loss: 0.0001(0.0108) Grad: 399.0173  LR: 0.000008  \n",
      "Epoch: [1][23000/36908] Elapsed 299m 28s (remain 181m 4s) Loss: 0.0001(0.0107) Grad: 622.8721  LR: 0.000008  \n",
      "Epoch: [1][23100/36908] Elapsed 300m 45s (remain 179m 45s) Loss: 0.0000(0.0107) Grad: 85.0380  LR: 0.000008  \n",
      "Epoch: [1][23200/36908] Elapsed 302m 3s (remain 178m 27s) Loss: 0.0004(0.0107) Grad: 1927.3143  LR: 0.000008  \n",
      "Epoch: [1][23300/36908] Elapsed 303m 23s (remain 177m 10s) Loss: 0.0000(0.0106) Grad: 7.0949  LR: 0.000008  \n",
      "Epoch: [1][23400/36908] Elapsed 304m 46s (remain 175m 54s) Loss: 0.0029(0.0106) Grad: 9694.5020  LR: 0.000008  \n",
      "Epoch: [1][23500/36908] Elapsed 306m 3s (remain 174m 36s) Loss: 0.0000(0.0106) Grad: 41.4980  LR: 0.000008  \n",
      "Epoch: [1][23600/36908] Elapsed 307m 19s (remain 173m 16s) Loss: 0.0064(0.0105) Grad: 20502.2773  LR: 0.000008  \n",
      "Epoch: [1][23700/36908] Elapsed 308m 36s (remain 171m 57s) Loss: 0.0000(0.0105) Grad: 15.9802  LR: 0.000008  \n",
      "Epoch: [1][23800/36908] Elapsed 309m 54s (remain 170m 39s) Loss: 0.0053(0.0105) Grad: 18500.3496  LR: 0.000008  \n",
      "Epoch: [1][23900/36908] Elapsed 311m 10s (remain 169m 20s) Loss: 0.0008(0.0105) Grad: 2384.7205  LR: 0.000008  \n",
      "Epoch: [1][24000/36908] Elapsed 312m 28s (remain 168m 2s) Loss: 0.0000(0.0104) Grad: 15.0845  LR: 0.000008  \n",
      "Epoch: [1][24100/36908] Elapsed 313m 45s (remain 166m 43s) Loss: 0.0001(0.0104) Grad: 1040.4147  LR: 0.000008  \n",
      "Epoch: [1][24200/36908] Elapsed 315m 4s (remain 165m 25s) Loss: 0.0000(0.0104) Grad: 29.4277  LR: 0.000008  \n",
      "Epoch: [1][24300/36908] Elapsed 316m 22s (remain 164m 7s) Loss: 0.0005(0.0103) Grad: 1312.3840  LR: 0.000008  \n",
      "Epoch: [1][24400/36908] Elapsed 317m 39s (remain 162m 49s) Loss: 0.0000(0.0103) Grad: 542.1614  LR: 0.000008  \n",
      "Epoch: [1][24500/36908] Elapsed 318m 55s (remain 161m 30s) Loss: 0.0370(0.0103) Grad: 65023.6211  LR: 0.000007  \n",
      "Epoch: [1][24600/36908] Elapsed 320m 13s (remain 160m 12s) Loss: 0.0000(0.0102) Grad: 43.4904  LR: 0.000007  \n",
      "Epoch: [1][24700/36908] Elapsed 321m 30s (remain 158m 53s) Loss: 0.0357(0.0102) Grad: 191690.2188  LR: 0.000007  \n",
      "Epoch: [1][24800/36908] Elapsed 322m 47s (remain 157m 34s) Loss: 0.0000(0.0102) Grad: 256.7356  LR: 0.000007  \n",
      "Epoch: [1][24900/36908] Elapsed 324m 3s (remain 156m 15s) Loss: 0.0004(0.0102) Grad: 10256.7295  LR: 0.000007  \n",
      "Epoch: [1][25000/36908] Elapsed 325m 19s (remain 154m 56s) Loss: 0.0038(0.0101) Grad: 31665.1328  LR: 0.000007  \n",
      "Epoch: [1][25100/36908] Elapsed 326m 40s (remain 153m 39s) Loss: 0.0000(0.0101) Grad: 6.0906  LR: 0.000007  \n",
      "Epoch: [1][25200/36908] Elapsed 327m 56s (remain 152m 20s) Loss: 0.0020(0.0101) Grad: 19726.7930  LR: 0.000007  \n",
      "Epoch: [1][25300/36908] Elapsed 329m 14s (remain 151m 2s) Loss: 0.0000(0.0100) Grad: 80.2351  LR: 0.000007  \n",
      "Epoch: [1][25400/36908] Elapsed 330m 31s (remain 149m 43s) Loss: 0.0000(0.0100) Grad: 58.1124  LR: 0.000007  \n",
      "Epoch: [1][25500/36908] Elapsed 331m 47s (remain 148m 24s) Loss: 0.0001(0.0100) Grad: 1733.6232  LR: 0.000007  \n",
      "Epoch: [1][25600/36908] Elapsed 333m 3s (remain 147m 5s) Loss: 0.0000(0.0099) Grad: 31.8479  LR: 0.000007  \n",
      "Epoch: [1][25700/36908] Elapsed 334m 19s (remain 145m 47s) Loss: 0.0000(0.0099) Grad: 183.1365  LR: 0.000007  \n",
      "Epoch: [1][25800/36908] Elapsed 335m 38s (remain 144m 29s) Loss: 0.0004(0.0099) Grad: 6922.8872  LR: 0.000007  \n",
      "Epoch: [1][25900/36908] Elapsed 336m 54s (remain 143m 10s) Loss: 0.0271(0.0099) Grad: 56000.7422  LR: 0.000007  \n",
      "Epoch: [1][26000/36908] Elapsed 338m 10s (remain 141m 51s) Loss: 0.0000(0.0098) Grad: 16.5711  LR: 0.000007  \n",
      "Epoch: [1][26100/36908] Elapsed 339m 26s (remain 140m 32s) Loss: 0.0488(0.0098) Grad: 177660.8750  LR: 0.000007  \n",
      "Epoch: [1][26200/36908] Elapsed 340m 43s (remain 139m 14s) Loss: 0.0000(0.0098) Grad: 998.4138  LR: 0.000006  \n",
      "Epoch: [1][26300/36908] Elapsed 342m 4s (remain 137m 57s) Loss: 0.0000(0.0098) Grad: 265.9114  LR: 0.000006  \n",
      "Epoch: [1][26400/36908] Elapsed 343m 22s (remain 136m 39s) Loss: 0.0000(0.0097) Grad: 148.0064  LR: 0.000006  \n",
      "Epoch: [1][26500/36908] Elapsed 344m 40s (remain 135m 21s) Loss: 0.0008(0.0097) Grad: 16785.2461  LR: 0.000006  \n",
      "Epoch: [1][26600/36908] Elapsed 345m 57s (remain 134m 2s) Loss: 0.0007(0.0097) Grad: 31601.1816  LR: 0.000006  \n",
      "Epoch: [1][26700/36908] Elapsed 347m 15s (remain 132m 44s) Loss: 0.0022(0.0097) Grad: 51995.9375  LR: 0.000006  \n",
      "Epoch: [1][26800/36908] Elapsed 348m 35s (remain 131m 27s) Loss: 0.0270(0.0096) Grad: 71589.0234  LR: 0.000006  \n",
      "Epoch: [1][26900/36908] Elapsed 349m 57s (remain 130m 11s) Loss: 0.0000(0.0096) Grad: 29.8871  LR: 0.000006  \n",
      "Epoch: [1][27000/36908] Elapsed 351m 21s (remain 128m 55s) Loss: 0.0004(0.0096) Grad: 3306.0557  LR: 0.000006  \n",
      "Epoch: [1][27100/36908] Elapsed 352m 39s (remain 127m 37s) Loss: 0.0000(0.0096) Grad: 159.6065  LR: 0.000006  \n",
      "Epoch: [1][27200/36908] Elapsed 353m 57s (remain 126m 18s) Loss: 0.0000(0.0095) Grad: 78.4989  LR: 0.000006  \n",
      "Epoch: [1][27300/36908] Elapsed 355m 14s (remain 125m 0s) Loss: 0.0000(0.0095) Grad: 240.9211  LR: 0.000006  \n",
      "Epoch: [1][27400/36908] Elapsed 356m 33s (remain 123m 42s) Loss: 0.0043(0.0095) Grad: 62869.8164  LR: 0.000006  \n",
      "Epoch: [1][27500/36908] Elapsed 357m 53s (remain 122m 25s) Loss: 0.0000(0.0095) Grad: 1317.0806  LR: 0.000006  \n",
      "Epoch: [1][27600/36908] Elapsed 359m 10s (remain 121m 6s) Loss: 0.0000(0.0095) Grad: 58.7016  LR: 0.000006  \n",
      "Epoch: [1][27700/36908] Elapsed 360m 28s (remain 119m 48s) Loss: 0.0001(0.0094) Grad: 1299.4087  LR: 0.000006  \n",
      "Epoch: [1][27800/36908] Elapsed 361m 45s (remain 118m 30s) Loss: 0.0121(0.0094) Grad: 36290.3906  LR: 0.000005  \n",
      "Epoch: [1][27900/36908] Elapsed 363m 4s (remain 117m 12s) Loss: 0.0000(0.0094) Grad: 631.0373  LR: 0.000005  \n",
      "Epoch: [1][28000/36908] Elapsed 364m 22s (remain 115m 54s) Loss: 0.0000(0.0094) Grad: 614.7653  LR: 0.000005  \n",
      "Epoch: [1][28100/36908] Elapsed 365m 44s (remain 114m 37s) Loss: 0.0000(0.0093) Grad: 14.8387  LR: 0.000005  \n",
      "Epoch: [1][28200/36908] Elapsed 367m 2s (remain 113m 19s) Loss: 0.0000(0.0093) Grad: 18.0640  LR: 0.000005  \n",
      "Epoch: [1][28300/36908] Elapsed 368m 19s (remain 112m 1s) Loss: 0.0040(0.0093) Grad: 29154.9570  LR: 0.000005  \n",
      "Epoch: [1][28400/36908] Elapsed 369m 37s (remain 110m 43s) Loss: 0.0177(0.0093) Grad: 270136.7188  LR: 0.000005  \n",
      "Epoch: [1][28500/36908] Elapsed 370m 55s (remain 109m 24s) Loss: 0.0125(0.0092) Grad: 168307.9375  LR: 0.000005  \n",
      "Epoch: [1][28600/36908] Elapsed 372m 12s (remain 108m 6s) Loss: 0.0000(0.0092) Grad: 306.3230  LR: 0.000005  \n",
      "Epoch: [1][28700/36908] Elapsed 373m 28s (remain 106m 47s) Loss: 0.0005(0.0092) Grad: 69287.3203  LR: 0.000005  \n",
      "Epoch: [1][28800/36908] Elapsed 374m 46s (remain 105m 29s) Loss: 0.0000(0.0092) Grad: 88.9738  LR: 0.000005  \n",
      "Epoch: [1][28900/36908] Elapsed 376m 5s (remain 104m 11s) Loss: 0.0000(0.0092) Grad: 32.7226  LR: 0.000005  \n",
      "Epoch: [1][29000/36908] Elapsed 377m 21s (remain 102m 53s) Loss: 0.0000(0.0091) Grad: 7.0824  LR: 0.000005  \n",
      "Epoch: [1][29100/36908] Elapsed 378m 37s (remain 101m 34s) Loss: 0.0009(0.0091) Grad: 34084.6289  LR: 0.000005  \n",
      "Epoch: [1][29200/36908] Elapsed 379m 54s (remain 100m 16s) Loss: 0.0000(0.0091) Grad: 264.6328  LR: 0.000005  \n",
      "Epoch: [1][29300/36908] Elapsed 381m 12s (remain 98m 58s) Loss: 0.0012(0.0091) Grad: 37080.8359  LR: 0.000005  \n",
      "Epoch: [1][29400/36908] Elapsed 382m 32s (remain 97m 40s) Loss: 0.0000(0.0090) Grad: 35.7423  LR: 0.000005  \n",
      "Epoch: [1][29500/36908] Elapsed 383m 50s (remain 96m 22s) Loss: 0.0000(0.0090) Grad: 1933.2336  LR: 0.000004  \n",
      "Epoch: [1][29600/36908] Elapsed 385m 7s (remain 95m 3s) Loss: 0.0000(0.0090) Grad: 43.9791  LR: 0.000004  \n",
      "Epoch: [1][29700/36908] Elapsed 386m 23s (remain 93m 45s) Loss: 0.0295(0.0090) Grad: 151289.4844  LR: 0.000004  \n",
      "Epoch: [1][29800/36908] Elapsed 387m 40s (remain 92m 27s) Loss: 0.0000(0.0090) Grad: 79.5834  LR: 0.000004  \n",
      "Epoch: [1][29900/36908] Elapsed 389m 0s (remain 91m 9s) Loss: 0.0000(0.0089) Grad: 1019.5091  LR: 0.000004  \n",
      "Epoch: [1][30000/36908] Elapsed 390m 19s (remain 89m 51s) Loss: 0.0177(0.0089) Grad: 69300.9688  LR: 0.000004  \n",
      "Epoch: [1][30100/36908] Elapsed 391m 37s (remain 88m 33s) Loss: 0.0088(0.0089) Grad: 264282.8438  LR: 0.000004  \n",
      "Epoch: [1][30200/36908] Elapsed 392m 55s (remain 87m 15s) Loss: 0.0000(0.0089) Grad: 1294.1356  LR: 0.000004  \n",
      "Epoch: [1][30300/36908] Elapsed 394m 13s (remain 85m 57s) Loss: 0.0040(0.0089) Grad: 21231.8125  LR: 0.000004  \n",
      "Epoch: [1][30400/36908] Elapsed 395m 30s (remain 84m 39s) Loss: 0.0000(0.0088) Grad: 27.1550  LR: 0.000004  \n",
      "Epoch: [1][30500/36908] Elapsed 396m 47s (remain 83m 21s) Loss: 0.0003(0.0088) Grad: 19420.8496  LR: 0.000004  \n",
      "Epoch: [1][30600/36908] Elapsed 398m 5s (remain 82m 2s) Loss: 0.0000(0.0088) Grad: 51.5137  LR: 0.000004  \n",
      "Epoch: [1][30700/36908] Elapsed 399m 25s (remain 80m 45s) Loss: 0.0015(0.0088) Grad: 2635.3013  LR: 0.000004  \n",
      "Epoch: [1][30800/36908] Elapsed 400m 43s (remain 79m 27s) Loss: 0.0000(0.0088) Grad: 28.0057  LR: 0.000004  \n",
      "Epoch: [1][30900/36908] Elapsed 402m 0s (remain 78m 8s) Loss: 0.0001(0.0088) Grad: 5042.7759  LR: 0.000004  \n",
      "Epoch: [1][31000/36908] Elapsed 403m 18s (remain 76m 50s) Loss: 0.0028(0.0087) Grad: 70390.5703  LR: 0.000004  \n",
      "Epoch: [1][31100/36908] Elapsed 404m 37s (remain 75m 32s) Loss: 0.0241(0.0087) Grad: 67181.4453  LR: 0.000003  \n",
      "Epoch: [1][31200/36908] Elapsed 405m 55s (remain 74m 14s) Loss: 0.0000(0.0087) Grad: 6.4405  LR: 0.000003  \n",
      "Epoch: [1][31300/36908] Elapsed 407m 13s (remain 72m 56s) Loss: 0.0033(0.0087) Grad: 79908.7891  LR: 0.000003  \n",
      "Epoch: [1][31400/36908] Elapsed 408m 32s (remain 71m 38s) Loss: 0.0000(0.0087) Grad: 10.3102  LR: 0.000003  \n",
      "Epoch: [1][31500/36908] Elapsed 409m 51s (remain 70m 20s) Loss: 0.0015(0.0086) Grad: 43147.5117  LR: 0.000003  \n",
      "Epoch: [1][31600/36908] Elapsed 411m 9s (remain 69m 2s) Loss: 0.0000(0.0086) Grad: 6.1027  LR: 0.000003  \n",
      "Epoch: [1][31700/36908] Elapsed 412m 26s (remain 67m 44s) Loss: 0.0000(0.0086) Grad: 20.6894  LR: 0.000003  \n",
      "Epoch: [1][31800/36908] Elapsed 413m 49s (remain 66m 27s) Loss: 0.0127(0.0086) Grad: 50344.5547  LR: 0.000003  \n",
      "Epoch: [1][31900/36908] Elapsed 415m 12s (remain 65m 10s) Loss: 0.0000(0.0086) Grad: 40.1224  LR: 0.000003  \n",
      "Epoch: [1][32000/36908] Elapsed 416m 36s (remain 63m 52s) Loss: 0.0002(0.0086) Grad: 10967.1865  LR: 0.000003  \n",
      "Epoch: [1][32100/36908] Elapsed 417m 52s (remain 62m 34s) Loss: 0.0000(0.0085) Grad: 12.6095  LR: 0.000003  \n",
      "Epoch: [1][32200/36908] Elapsed 419m 9s (remain 61m 16s) Loss: 0.0043(0.0085) Grad: 55850.2539  LR: 0.000003  \n",
      "Epoch: [1][32300/36908] Elapsed 420m 25s (remain 59m 57s) Loss: 0.0000(0.0085) Grad: 30.1863  LR: 0.000003  \n",
      "Epoch: [1][32400/36908] Elapsed 421m 43s (remain 58m 39s) Loss: 0.0152(0.0085) Grad: 446753.8750  LR: 0.000003  \n",
      "Epoch: [1][32500/36908] Elapsed 423m 1s (remain 57m 21s) Loss: 0.0049(0.0085) Grad: 306981.0312  LR: 0.000003  \n",
      "Epoch: [1][32600/36908] Elapsed 424m 18s (remain 56m 3s) Loss: 0.0000(0.0084) Grad: 498.1601  LR: 0.000003  \n",
      "Epoch: [1][32700/36908] Elapsed 425m 38s (remain 54m 45s) Loss: 0.0000(0.0084) Grad: 25.6873  LR: 0.000003  \n",
      "Epoch: [1][32800/36908] Elapsed 426m 57s (remain 53m 27s) Loss: 0.0000(0.0084) Grad: 16.8037  LR: 0.000002  \n",
      "Epoch: [1][32900/36908] Elapsed 428m 15s (remain 52m 9s) Loss: 0.0004(0.0084) Grad: 72147.0781  LR: 0.000002  \n",
      "Epoch: [1][33000/36908] Elapsed 429m 33s (remain 50m 51s) Loss: 0.0001(0.0084) Grad: 4011.1619  LR: 0.000002  \n",
      "Epoch: [1][33100/36908] Elapsed 430m 51s (remain 49m 33s) Loss: 0.0000(0.0084) Grad: 119.0128  LR: 0.000002  \n",
      "Epoch: [1][33200/36908] Elapsed 432m 9s (remain 48m 15s) Loss: 0.0000(0.0084) Grad: 15.9192  LR: 0.000002  \n",
      "Epoch: [1][33300/36908] Elapsed 433m 26s (remain 46m 56s) Loss: 0.0004(0.0083) Grad: 14549.6934  LR: 0.000002  \n",
      "Epoch: [1][33400/36908] Elapsed 434m 45s (remain 45m 38s) Loss: 0.0000(0.0083) Grad: 131.0330  LR: 0.000002  \n",
      "Epoch: [1][33500/36908] Elapsed 436m 5s (remain 44m 20s) Loss: 0.0010(0.0083) Grad: 28566.0781  LR: 0.000002  \n",
      "Epoch: [1][33600/36908] Elapsed 437m 24s (remain 43m 2s) Loss: 0.0077(0.0083) Grad: 61234.6094  LR: 0.000002  \n",
      "Epoch: [1][33700/36908] Elapsed 438m 44s (remain 41m 45s) Loss: 0.0004(0.0083) Grad: 2908.8740  LR: 0.000002  \n",
      "Epoch: [1][33800/36908] Elapsed 440m 6s (remain 40m 27s) Loss: 0.0000(0.0082) Grad: 606.1666  LR: 0.000002  \n",
      "Epoch: [1][33900/36908] Elapsed 441m 22s (remain 39m 8s) Loss: 0.0013(0.0082) Grad: 23240.7363  LR: 0.000002  \n",
      "Epoch: [1][34000/36908] Elapsed 442m 39s (remain 37m 50s) Loss: 0.0000(0.0082) Grad: 69.2908  LR: 0.000002  \n",
      "Epoch: [1][34100/36908] Elapsed 443m 55s (remain 36m 32s) Loss: 0.0009(0.0082) Grad: 16050.3691  LR: 0.000002  \n",
      "Epoch: [1][34200/36908] Elapsed 445m 13s (remain 35m 14s) Loss: 0.0000(0.0082) Grad: 136.3231  LR: 0.000002  \n",
      "Epoch: [1][34300/36908] Elapsed 446m 28s (remain 33m 56s) Loss: 0.0011(0.0082) Grad: 37123.1094  LR: 0.000002  \n",
      "Epoch: [1][34400/36908] Elapsed 447m 44s (remain 32m 37s) Loss: 0.0002(0.0082) Grad: 1518.0481  LR: 0.000002  \n",
      "Epoch: [1][34500/36908] Elapsed 449m 2s (remain 31m 19s) Loss: 0.0007(0.0081) Grad: 3048.4714  LR: 0.000001  \n",
      "Epoch: [1][34600/36908] Elapsed 450m 21s (remain 30m 1s) Loss: 0.0000(0.0081) Grad: 5.0642  LR: 0.000001  \n",
      "Epoch: [1][34700/36908] Elapsed 451m 39s (remain 28m 43s) Loss: 0.0064(0.0081) Grad: 14712.7246  LR: 0.000001  \n",
      "Epoch: [1][34800/36908] Elapsed 452m 55s (remain 27m 25s) Loss: 0.0000(0.0081) Grad: 20.5910  LR: 0.000001  \n",
      "Epoch: [1][34900/36908] Elapsed 454m 12s (remain 26m 7s) Loss: 0.0000(0.0081) Grad: 68.9021  LR: 0.000001  \n",
      "Epoch: [1][35000/36908] Elapsed 455m 28s (remain 24m 48s) Loss: 0.0001(0.0081) Grad: 618.4919  LR: 0.000001  \n",
      "Epoch: [1][35100/36908] Elapsed 456m 46s (remain 23m 30s) Loss: 0.0169(0.0080) Grad: 212634.2812  LR: 0.000001  \n",
      "Epoch: [1][35200/36908] Elapsed 458m 4s (remain 22m 12s) Loss: 0.0000(0.0080) Grad: 116.0275  LR: 0.000001  \n",
      "Epoch: [1][35300/36908] Elapsed 459m 21s (remain 20m 54s) Loss: 0.0000(0.0080) Grad: 22.7721  LR: 0.000001  \n",
      "Epoch: [1][35400/36908] Elapsed 460m 37s (remain 19m 36s) Loss: 0.0004(0.0080) Grad: 5179.7925  LR: 0.000001  \n",
      "Epoch: [1][35500/36908] Elapsed 461m 55s (remain 18m 18s) Loss: 0.0000(0.0080) Grad: 187.8734  LR: 0.000001  \n",
      "Epoch: [1][35600/36908] Elapsed 463m 18s (remain 17m 0s) Loss: 0.0023(0.0080) Grad: 6371.0000  LR: 0.000001  \n",
      "Epoch: [1][35700/36908] Elapsed 464m 36s (remain 15m 42s) Loss: 0.0000(0.0079) Grad: 10.0154  LR: 0.000001  \n",
      "Epoch: [1][35800/36908] Elapsed 465m 52s (remain 14m 24s) Loss: 0.0000(0.0079) Grad: 12.8344  LR: 0.000001  \n",
      "Epoch: [1][35900/36908] Elapsed 467m 8s (remain 13m 6s) Loss: 0.0000(0.0079) Grad: 326.6019  LR: 0.000001  \n",
      "Epoch: [1][36000/36908] Elapsed 468m 25s (remain 11m 48s) Loss: 0.0000(0.0079) Grad: 9.7396  LR: 0.000001  \n",
      "Epoch: [1][36100/36908] Elapsed 469m 44s (remain 10m 30s) Loss: 0.0001(0.0079) Grad: 1358.0491  LR: 0.000000  \n",
      "Epoch: [1][36200/36908] Elapsed 471m 2s (remain 9m 11s) Loss: 0.0000(0.0079) Grad: 14.8355  LR: 0.000000  \n",
      "Epoch: [1][36300/36908] Elapsed 472m 20s (remain 7m 53s) Loss: 0.0012(0.0079) Grad: 13998.0000  LR: 0.000000  \n",
      "Epoch: [1][36400/36908] Elapsed 473m 38s (remain 6m 35s) Loss: 0.0000(0.0079) Grad: 276.2139  LR: 0.000000  \n",
      "Epoch: [1][36500/36908] Elapsed 474m 56s (remain 5m 17s) Loss: 0.0001(0.0078) Grad: 1242.7891  LR: 0.000000  \n",
      "Epoch: [1][36600/36908] Elapsed 476m 14s (remain 3m 59s) Loss: 0.0118(0.0078) Grad: 113954.9766  LR: 0.000000  \n",
      "Epoch: [1][36700/36908] Elapsed 477m 32s (remain 2m 41s) Loss: 0.0000(0.0078) Grad: 129.0437  LR: 0.000000  \n",
      "Epoch: [1][36800/36908] Elapsed 478m 50s (remain 1m 23s) Loss: 0.0017(0.0078) Grad: 15750.0977  LR: 0.000000  \n",
      "Epoch: [1][36900/36908] Elapsed 480m 9s (remain 0m 5s) Loss: 0.0001(0.0078) Grad: 1570.5706  LR: 0.000000  \n",
      "Epoch: [1][36907/36908] Elapsed 480m 14s (remain 0m 0s) Loss: 0.0000(0.0078) Grad: 38.0652  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 1s (remain 21m 17s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 32s (remain 5m 46s) Loss: 0.0454(0.0091) \n",
      "EVAL: [200/1192] Elapsed 1m 3s (remain 5m 12s) Loss: 0.0168(0.0087) \n",
      "EVAL: [300/1192] Elapsed 1m 35s (remain 4m 42s) Loss: 0.0127(0.0096) \n",
      "EVAL: [400/1192] Elapsed 2m 5s (remain 4m 8s) Loss: 0.0000(0.0093) \n",
      "EVAL: [500/1192] Elapsed 2m 35s (remain 3m 35s) Loss: 0.0499(0.0092) \n",
      "EVAL: [600/1192] Elapsed 3m 6s (remain 3m 3s) Loss: 0.0248(0.0095) \n",
      "EVAL: [700/1192] Elapsed 3m 38s (remain 2m 32s) Loss: 0.0068(0.0107) \n",
      "EVAL: [800/1192] Elapsed 4m 12s (remain 2m 3s) Loss: 0.0258(0.0109) \n",
      "EVAL: [900/1192] Elapsed 4m 43s (remain 1m 31s) Loss: 0.0010(0.0110) \n",
      "EVAL: [1000/1192] Elapsed 5m 13s (remain 0m 59s) Loss: 0.0000(0.0106) \n",
      "EVAL: [1100/1192] Elapsed 5m 43s (remain 0m 28s) Loss: 0.0279(0.0103) \n",
      "EVAL: [1191/1192] Elapsed 6m 11s (remain 0m 0s) Loss: 0.0000(0.0101) \n",
      "Epoch 1 - avg_train_loss: 0.0078  avg_val_loss: 0.0101  time: 29190s\n",
      "Epoch 1 - Score: 0.8969\n",
      "Epoch 1 - Save Best Score: 0.8969 Model\n",
      "best_thres: 0.46  score: 0.89198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n",
      "load weights from ../output/nbme-score-clinical-patient-notes/nbme-exp086/fold0_best.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9f1e2fe69d642a093575f5a5890491c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n",
      "load weights from ../output/nbme-score-clinical-patient-notes/nbme-exp086/fold1_best.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53179243858746b491acb2d904d5ac53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x7f8048c3b170>\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 132, in __del__\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n",
      "load weights from ../output/nbme-score-clinical-patient-notes/nbme-exp086/fold2_best.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493c71c24a18475ebe317fe6c5caf36e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n",
      "load weights from ../output/nbme-score-clinical-patient-notes/nbme-exp086/fold3_best.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b080d2c96254a72866f5fa9c5c15125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "name": "nbme-exp085.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 641.572862,
   "end_time": "2022-02-27T11:39:50.972497",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-27T11:29:09.399635",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "054630edadaa453fb86d66a20e49030f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_747b8a73ce544c569f4d063fc9d6d18a",
      "placeholder": "​",
      "style": "IPY_MODEL_c7aa62f5eaca401dbbfbfd5f843d6a59",
      "value": " 28720/42146 [00:27&lt;00:07, 1839.83it/s]"
     }
    },
    "18b47303dd5b48bfb118053a80c44a40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_971945a52a6d4f06ba35e5363d264037",
       "IPY_MODEL_2d7cfbb1d1a54c0590e59de0b280ab22",
       "IPY_MODEL_054630edadaa453fb86d66a20e49030f"
      ],
      "layout": "IPY_MODEL_8fce27d68c1c41ec9a3c58d439c2a5e7"
     }
    },
    "28c710503f154bdea731991801a85a47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d7cfbb1d1a54c0590e59de0b280ab22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28c710503f154bdea731991801a85a47",
      "max": 42146,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8c57bf78a80247db9521bd5b163502ef",
      "value": 28905
     }
    },
    "747b8a73ce544c569f4d063fc9d6d18a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c57bf78a80247db9521bd5b163502ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8fce27d68c1c41ec9a3c58d439c2a5e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "971945a52a6d4f06ba35e5363d264037": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9a3708f7e4c486da3a09e066b6936fb",
      "placeholder": "​",
      "style": "IPY_MODEL_cd1144e40332427fa3e8d5bc7f57b924",
      "value": " 69%"
     }
    },
    "c7aa62f5eaca401dbbfbfd5f843d6a59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd1144e40332427fa3e8d5bc7f57b924": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e9a3708f7e4c486da3a09e066b6936fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
