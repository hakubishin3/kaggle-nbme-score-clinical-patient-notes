{"cells":[{"cell_type":"markdown","id":"blind-kingdom","metadata":{"id":"blind-kingdom"},"source":["## References"]},{"cell_type":"markdown","id":"antique-glenn","metadata":{"id":"antique-glenn"},"source":["- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"]},{"cell_type":"markdown","id":"bored-ministry","metadata":{"id":"bored-ministry"},"source":["## Configurations"]},{"cell_type":"code","execution_count":null,"id":"deadly-confidence","metadata":{"id":"deadly-confidence"},"outputs":[],"source":["EXP_NAME = \"nbme-exp021\"\n","ENV = \"colab\"\n","DEBUG_MODE = False\n","SUBMISSION_MODE = False"]},{"cell_type":"code","execution_count":null,"id":"aware-worcester","metadata":{"id":"aware-worcester"},"outputs":[],"source":["class CFG:\n","    env=ENV\n","    exp_name=EXP_NAME\n","    debug=DEBUG_MODE\n","    submission=SUBMISSION_MODE\n","    apex=True\n","    input_dir=None\n","    output_dir=None\n","    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n","    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n","    competition_name=\"nbme-score-clinical-patient-notes\"\n","    id_col=\"id\"\n","    target_col=\"location\"\n","    pretrained_model_name=\"microsoft/deberta-v3-large\"\n","    tokenizer=None\n","    max_len=None\n","    output_dim=1\n","    dropout=0.2\n","    num_workers=4\n","    batch_size=4\n","    lr=2e-5\n","    betas=(0.9, 0.98)\n","    weight_decay=0.1\n","    num_warmup_steps_rate=0.1\n","    batch_scheduler=True\n","    epochs=5\n","    n_fold=5\n","    train_fold=[0, 1, 2, 3, 4]\n","    seed=71\n","    gradient_accumulation_steps=2\n","    max_grad_norm=1000\n","    print_freq=100\n","    train=True\n","    inference=True"]},{"cell_type":"code","execution_count":null,"id":"personalized-death","metadata":{"id":"personalized-death"},"outputs":[],"source":["if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.train_fold = [0, 1]\n","\n","if CFG.submission:\n","    CFG.train = False\n","    CFG.inference = True"]},{"cell_type":"markdown","id":"cardiovascular-neutral","metadata":{"id":"cardiovascular-neutral"},"source":["## Directory Settings"]},{"cell_type":"code","execution_count":null,"id":"checked-boards","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12792,"status":"ok","timestamp":1646444848429,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"checked-boards","outputId":"0597edb8-86af-4966-fa91-ef014681da06"},"outputs":[{"output_type":"stream","name":"stdout","text":["colab\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 4.3 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n"]}],"source":["import sys\n","from pathlib import Path\n","\n","\n","print(CFG.env)\n","if CFG.env == \"colab\":\n","    # colab環境\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","    # install packages\n","    !pip install transformers\n","    !pip install sentencepiece\n","\n","elif CFG.env == \"local\":\n","    # ローカルサーバ\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","\n","elif CFG.env == \"kaggle\":\n","    # kaggle環境\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./\")"]},{"cell_type":"code","source":["# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n","# This must be done before importing transformers\n","import shutil\n","from pathlib import Path\n","\n","if CFG.env == \"colab\":\n","    input_dir = Path(\"./drive/MyDrive/00.kaggle/input/deberta-v2-3-fast-tokenizer\")\n","    transformers_path = Path(\"/usr/local/lib/python3.7/dist-packages/transformers\")\n","else:\n","    input_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n","    transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n","\n","convert_file = input_dir / \"convert_slow_tokenizer.py\"\n","conversion_path = transformers_path/convert_file.name\n","\n","if conversion_path.exists():\n","    conversion_path.unlink()\n","\n","shutil.copy(convert_file, transformers_path)\n","deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n","\n","for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n","    filepath = deberta_v2_path/filename\n","    if filepath.exists():\n","        filepath.unlink()\n","\n","    shutil.copy(input_dir/filename, filepath)\n","    \n","    \n","from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast"],"metadata":{"id":"iGai035Rvu1Z"},"id":"iGai035Rvu1Z","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"vital-mexico","metadata":{"id":"vital-mexico"},"outputs":[],"source":["import gc\n","import os\n","import ast\n","import time\n","import math\n","import random\n","import itertools\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","from scipy.optimize import minimize\n","from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as T\n","from torchvision.io import read_image\n","from torch.utils.data import DataLoader, Dataset\n","\n","from transformers import AutoModelForMaskedLM\n","from transformers import BartModel,BertModel,BertTokenizer\n","from transformers import DebertaModel,DebertaTokenizer\n","from transformers import RobertaModel,RobertaTokenizer\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","id":"economic-ladder","metadata":{"id":"economic-ladder"},"source":["## Utilities"]},{"cell_type":"code","execution_count":null,"id":"desperate-keyboard","metadata":{"id":"desperate-keyboard"},"outputs":[],"source":["def micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on binary arrays.\n","\n","    Args:\n","        preds (list of lists of ints): Predictions.\n","        truths (list of lists of ints): Ground truths.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    # Micro : aggregating over all instances\n","    preds = np.concatenate(preds)\n","    truths = np.concatenate(truths)\n","    return f1_score(truths, preds)\n","\n","\n","def spans_to_binary(spans, length=None):\n","    \"\"\"\n","    Converts spans to a binary array indicating whether each character is in the span.\n","\n","    Args:\n","        spans (list of lists of two ints): Spans.\n","\n","    Returns:\n","        np array [length]: Binarized spans.\n","    \"\"\"\n","    length = np.max(spans) if length is None else length\n","    binary = np.zeros(length)\n","    for start, end in spans:\n","        binary[start:end] = 1\n","    return binary\n","\n","\n","def span_micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on spans.\n","\n","    Args:\n","        preds (list of lists of two ints): Prediction spans.\n","        truths (list of lists of two ints): Ground truth spans.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    bin_preds = []\n","    bin_truths = []\n","    for pred, truth in zip(preds, truths):\n","        if not len(pred) and not len(truth):\n","            continue\n","        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n","        bin_preds.append(spans_to_binary(pred, length))\n","        bin_truths.append(spans_to_binary(truth, length))\n","    return micro_f1(bin_preds, bin_truths)\n","\n","\n","def get_score(y_true, y_pred):\n","    score = span_micro_f1(y_true, y_pred)\n","    return score"]},{"cell_type":"code","execution_count":null,"id":"flexible-wednesday","metadata":{"id":"flexible-wednesday"},"outputs":[],"source":["def create_labels_for_scoring(df):\n","    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n","    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n","    for i in range(len(df)):\n","        lst = df.loc[i, \"location\"]\n","        if lst:\n","            new_lst = \";\".join(lst)\n","            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n","\n","    # create labels\n","    truths = []\n","    for location_list in df[\"location_for_create_labels\"].values:\n","        truth = []\n","        if len(location_list) > 0:\n","            location = location_list[0]\n","            for loc in [s.split() for s in location.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                truth.append([start, end])\n","        truths.append(truth)\n","\n","    return truths\n","\n","\n","def get_char_probs(texts, token_probs, tokenizer):\n","    res = [np.zeros(len(t)) for t in texts]\n","    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n","        encoded = tokenizer(\n","            text=text,\n","            max_length=CFG.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n","            start, end = offset_mapping\n","            res[i][start:end] = pred\n","    return res\n","\n","\n","def get_predicted_location_str(char_probs, th=0.5):\n","    results = []\n","    for char_prob in char_probs:\n","        result = np.where(char_prob >= th)[0] + 1\n","        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n","        result = [f\"{min(r)} {max(r)}\" for r in result]\n","        result = \";\".join(result)\n","        results.append(result)\n","    return results\n","\n","\n","def get_predictions(results):\n","    predictions = []\n","    for result in results:\n","        prediction = []\n","        if result != \"\":\n","            for loc in [s.split() for s in result.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                prediction.append([start, end])\n","        predictions.append(prediction)\n","    return predictions\n","\n","\n","def scoring(df, th=0.5):\n","    labels = create_labels_for_scoring(df)\n","\n","    token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n","    char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n","    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n","    preds = get_predictions(predicted_location_str)\n","\n","    score = get_score(labels, preds)\n","    return score\n","\n","\n","def get_best_thres(oof_df):\n","    def f1_opt(x):\n","        return -1 * scoring(oof_df, th=x)\n","\n","    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n","    return best_thres"]},{"cell_type":"code","execution_count":null,"id":"logical-chemistry","metadata":{"id":"logical-chemistry"},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return \"%dm %ds\" % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":null,"id":"gorgeous-record","metadata":{"id":"gorgeous-record"},"outputs":[],"source":["seed_everything()"]},{"cell_type":"markdown","id":"frozen-africa","metadata":{"id":"frozen-africa"},"source":["## Data Loading"]},{"cell_type":"code","execution_count":null,"id":"shaped-metallic","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1695,"status":"ok","timestamp":1646444854520,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"shaped-metallic","outputId":"d8e584e2-1609-417a-88e2-8c32702dd99e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14300, 6), (143, 3), (42146, 3), (5, 4))"]},"metadata":{},"execution_count":11}],"source":["train = pd.read_csv(CFG.input_dir / \"train.csv\")\n","features = pd.read_csv(CFG.input_dir / \"features.csv\")\n","patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n","test = pd.read_csv(CFG.input_dir / \"test.csv\")\n","\n","train.shape, features.shape, patient_notes.shape, test.shape"]},{"cell_type":"code","execution_count":null,"id":"visible-australia","metadata":{"id":"visible-australia"},"outputs":[],"source":["if CFG.debug:\n","    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n","    print(train.shape)"]},{"cell_type":"markdown","id":"hydraulic-gibson","metadata":{"id":"hydraulic-gibson"},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":null,"id":"interpreted-northeast","metadata":{"id":"interpreted-northeast"},"outputs":[],"source":["def preprocess_features(features):\n","    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n","    return features\n","\n","\n","features = preprocess_features(features)"]},{"cell_type":"code","execution_count":null,"id":"martial-blind","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1646444854521,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"martial-blind","outputId":"df678373-f04f-4edd-8c4f-bb6190b101a2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14300, 8), (5, 6))"]},"metadata":{},"execution_count":14}],"source":["train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","\n","train.shape, test.shape"]},{"cell_type":"code","execution_count":null,"id":"electoral-favor","metadata":{"id":"electoral-favor"},"outputs":[],"source":["train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n","train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"]},{"cell_type":"code","execution_count":null,"id":"reported-parade","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1646444854522,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"reported-parade","outputId":"bc608d2a-1cc7-4f1c-d024-703f723e75bc"},"outputs":[{"output_type":"display_data","data":{"text/plain":["0    4399\n","1    8181\n","2    1296\n","3     287\n","4      99\n","5      27\n","6       9\n","7       1\n","8       1\n","Name: annotation_length, dtype: int64"]},"metadata":{}}],"source":["train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n","display(train['annotation_length'].value_counts().sort_index())"]},{"cell_type":"markdown","id":"enabling-relevance","metadata":{"id":"enabling-relevance"},"source":["## CV split"]},{"cell_type":"code","execution_count":null,"id":"mature-coalition","metadata":{"id":"mature-coalition"},"outputs":[],"source":["def get_groupkfold(df, group_name):\n","    groups = df[group_name].unique()\n","\n","    kf = KFold(\n","        n_splits=CFG.n_fold,\n","        shuffle=True,\n","        random_state=CFG.seed,\n","    )\n","    folds_ids = []\n","    for i_fold, (_, val_group_idx) in enumerate(kf.split(groups)):\n","        val_group = groups[val_group_idx]\n","        is_val = df[group_name].isin(val_group)\n","        val_idx = df[is_val].index\n","        df.loc[val_idx, \"fold\"] = int(i_fold)\n","\n","    df[\"fold\"] = df[\"fold\"].astype(int)\n","    return df"]},{"cell_type":"code","execution_count":null,"id":"every-minutes","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1646444854523,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"every-minutes","outputId":"768cbf1d-aea4-4c0a-897f-ebb9410add0d"},"outputs":[{"output_type":"display_data","data":{"text/plain":["fold\n","0    2902\n","1    2894\n","2    2813\n","3    2791\n","4    2900\n","dtype: int64"]},"metadata":{}}],"source":["train = get_groupkfold(train, \"pn_num\")\n","display(train.groupby(\"fold\").size())"]},{"cell_type":"markdown","id":"subjective-entrance","metadata":{"id":"subjective-entrance"},"source":["## Setup tokenizer"]},{"cell_type":"code","execution_count":null,"id":"dramatic-afghanistan","metadata":{"id":"dramatic-afghanistan","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646444864392,"user_tz":-540,"elapsed":9879,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}},"outputId":"2458e869-944c-405f-b9a7-0c79f55b67c8"},"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["if CFG.submission:\n","    tokenizer = DebertaV2TokenizerFast.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n","else:\n","    tokenizer = DebertaV2TokenizerFast.from_pretrained(CFG.pretrained_model_name)\n","    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n","\n","CFG.tokenizer = tokenizer"]},{"cell_type":"markdown","id":"divided-arrow","metadata":{"id":"divided-arrow"},"source":["## Create dataset"]},{"cell_type":"code","execution_count":null,"id":"immune-campbell","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["9771bf3c1a0549f582d6382e402a6eda","a6f56248343d4f949e74dfc6ac6255e8","f06dbf572a3a4f5b8aaf3ac42993c730","53596cda0a8e4d5a80c8205230d21dcf","8ee0c2578d41408c991cb8fb58068a8c","67a63039cc734afc97c10f0b745adef3","82ea708e5be245f88eee8dfc8bba268e","eb67a778ba0046f0ab0d009f1043bba4","566ab2dc971b41189a8ab3e67ca2ca7b","7ce61b097fa745f780ce3272c8f25d19","5277b94b3f9b4d81919118f3c6f17a88"]},"executionInfo":{"elapsed":34341,"status":"ok","timestamp":1646444898728,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"immune-campbell","outputId":"5a9b3adf-b0d8-44f4-c06d-f4efaa5c1424"},"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9771bf3c1a0549f582d6382e402a6eda","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/42146 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 323\n"]}],"source":["pn_history_lengths = []\n","tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    pn_history_lengths.append(length)\n","\n","print(\"max length:\", np.max(pn_history_lengths))"]},{"cell_type":"code","execution_count":null,"id":"northern-branch","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["e6e0aab637d54747808a3b9e6ae839fb","374e075cea454b6d85582ae160c08145","fefee9c4c04e4d318082e61d13f59bf1","9bc87f90b3954f38b1b047209bb5b6a8","1a4b56e532e148aea31e14aeb5b12789","664689527d8b4021b63563511be585ae","622e4ebad3f3438b92d9608b02e7cea2","efa38883d7c4458d9f5d36e33ebc33d8","17c350bd64084c33baaee5c14fd78461","b59cb54c6d204b0c9a76d8346681a468","dee6e951eae74d859829e4fd4902ef87"]},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1646444898729,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"northern-branch","outputId":"5130a41d-28a3-4a9b-ba35-176a41678879"},"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e6e0aab637d54747808a3b9e6ae839fb","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/143 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 28\n"]}],"source":["feature_text_lengths = []\n","tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    feature_text_lengths.append(length)\n","\n","print(\"max length:\", np.max(feature_text_lengths))"]},{"cell_type":"code","execution_count":null,"id":"oriental-jacksonville","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1646444898729,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"oriental-jacksonville","outputId":"a7a80420-c86a-425b-9d13-038af2879438"},"outputs":[{"output_type":"stream","name":"stdout","text":["max length: 354\n"]}],"source":["CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n","\n","print(\"max length:\", CFG.max_len)"]},{"cell_type":"code","execution_count":null,"id":"flexible-trainer","metadata":{"id":"flexible-trainer"},"outputs":[],"source":["class TrainingDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","        self.annotation_lengths = self.df[\"annotation_length\"].values\n","        self.locations = self.df[\"location\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def _create_label(self, pn_history, annotation_length, location_list):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        offset_mapping = encoded[\"offset_mapping\"]\n","        ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n","        label = np.zeros(len(offset_mapping))\n","        label[ignore_idxes] = -1\n","\n","        if annotation_length > 0:\n","            for location in location_list:\n","                for loc in [s.split() for s in location.split(\";\")]:\n","                    start, end = int(loc[0]), int(loc[1])\n","                    start_idx = -1\n","                    end_idx = -1\n","                    for idx in range(len(offset_mapping)):\n","                        if (start_idx == -1) & (start < offset_mapping[idx][0]):\n","                            start_idx = idx - 1\n","                        if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n","                            end_idx = idx + 1\n","                    if start_idx == -1:\n","                        start_idx = end_idx\n","                    if (start_idx != -1) & (end_idx != -1):\n","                        label[start_idx:end_idx] = 1\n","\n","        return torch.tensor(label, dtype=torch.float)\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n","        return input_, label"]},{"cell_type":"code","execution_count":null,"id":"stock-robertson","metadata":{"id":"stock-robertson"},"outputs":[],"source":["class TestDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        return input_"]},{"cell_type":"markdown","id":"chemical-lucas","metadata":{"id":"chemical-lucas"},"source":["## Model"]},{"cell_type":"code","execution_count":null,"id":"animated-array","metadata":{"id":"animated-array"},"outputs":[],"source":["class CustomModel(nn.Module):\n","    def __init__(self, cfg, model_config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        if model_config_path is None:\n","            self.model_config = AutoConfig.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                output_hidden_states=True,\n","            )\n","        else:\n","            self.model_config = torch.load(model_config_path)\n","\n","        if pretrained:\n","            self.backbone = AutoModel.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                config=self.model_config,\n","            )\n","            print(f\"Load weight from pretrained\")\n","        else:\n","            self.backbone = AutoModel.from_config(self.model_config)\n","\n","        \"\"\"\n","        itpt = AutoModelForMaskedLM.from_config(self.model_config)\n","        #path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp009/checkpoint-129000/pytorch_model.bin\")\n","        path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n","        state_dict = torch.load(path)\n","        itpt.load_state_dict(state_dict)\n","        self.backbone = itpt.deberta\n","        print(f\"Load weight from {path}\")\n","        \"\"\"\n","\n","        self.fc = nn.Sequential(\n","            nn.Dropout(self.cfg.dropout),\n","            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n","        )\n","\n","    def forward(self, inputs):\n","        h = self.backbone(**inputs)[\"last_hidden_state\"]\n","        output = self.fc(h)\n","        return output"]},{"cell_type":"markdown","id":"thorough-bristol","metadata":{"id":"thorough-bristol"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"id":"talented-quantity","metadata":{"id":"talented-quantity"},"outputs":[],"source":["def train_fn(\n","    train_dataloader,\n","    model,\n","    criterion,\n","    optimizer,\n","    epoch,\n","    scheduler,\n","    device,\n","):\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(train_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","\n","        pos_nums = (labels == 1).sum(axis=1)\n","        pos_nums = torch.tensor([[pos_num] * CFG.max_len for pos_num in pos_nums]).view(-1, 1).to(device)\n","        pos_nums = torch.masked_select(pos_nums, labels.view(-1, 1) != -1)\n","        weight = []\n","        for pos_num in pos_nums:\n","            if pos_num == 0:\n","                weight.append(3.0)\n","            else:\n","                weight.append(1.0)\n","        weight = torch.tensor(weight).to(device)\n","        loss = loss * weight\n","\n","        loss = loss.mean()\n","\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","\n","        if CFG.batch_scheduler:\n","            scheduler.step()\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n","            print(\n","                \"Epoch: [{0}][{1}/{2}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                \"Grad: {grad_norm:.4f}  \"\n","                \"LR: {lr:.6f}  \"\n","                .format(\n","                    epoch+1,\n","                    step,\n","                    len(train_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n","                    loss=losses,\n","                     grad_norm=grad_norm,\n","                     lr=scheduler.get_lr()[0],\n","                )\n","            )\n","    return losses.avg"]},{"cell_type":"code","execution_count":null,"id":"figured-cooperative","metadata":{"id":"figured-cooperative"},"outputs":[],"source":["def valid_fn(\n","    val_dataloader,\n","    model,\n","    criterion,\n","    device,\n","):\n","    model.eval()\n","    preds = []\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(val_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.no_grad():\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","\n","        pos_nums = (labels == 1).sum(axis=1)\n","        pos_nums = torch.tensor([[pos_num] * CFG.max_len for pos_num in pos_nums]).view(-1, 1).to(device)\n","        pos_nums = torch.masked_select(pos_nums, labels.view(-1, 1) != -1)\n","        weight = []\n","        for pos_num in pos_nums:\n","            if pos_num == 0:\n","                weight.append(3.0)\n","            else:\n","                weight.append(1.0)\n","        weight = torch.tensor(weight).to(device)\n","        loss = loss * weight\n","\n","        loss = loss.mean()\n","\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n","            print(\n","                \"EVAL: [{0}/{1}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                .format(\n","                    step, len(val_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n","                    loss=losses,\n","                )\n","            )\n","    preds = np.concatenate(preds)\n","    return losses.avg, preds"]},{"cell_type":"code","execution_count":null,"id":"played-pointer","metadata":{"id":"played-pointer"},"outputs":[],"source":["def inference_fn(test_dataloader, model, device):\n","    model.eval()\n","    model.to(device)\n","    preds = []\n","    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n","    for inputs in tk0:\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        with torch.no_grad():\n","            output = model(inputs)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","    preds = np.concatenate(preds)\n","    return preds"]},{"cell_type":"code","execution_count":null,"id":"brazilian-nigeria","metadata":{"id":"brazilian-nigeria"},"outputs":[],"source":["def train_loop(df, i_fold, device):\n","    print(f\"========== fold: {i_fold} training ==========\")\n","    train_idx = df[df[\"fold\"] != i_fold].index\n","    val_idx = df[df[\"fold\"] == i_fold].index\n","\n","    train_folds = df.loc[train_idx].reset_index(drop=True)\n","    val_folds = df.loc[val_idx].reset_index(drop=True)\n","\n","    train_dataset = TrainingDataset(CFG, train_folds)\n","    val_dataset = TrainingDataset(CFG, val_folds)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=True,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=True,\n","    )\n","    val_dataloader = DataLoader(\n","        val_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=False,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=False,\n","    )\n","\n","    model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n","    model.to(device)\n","\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\"params\": [p for n, p in param_optimizer if not any(\n","            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n","        {\"params\": [p for n, p in param_optimizer if any(\n","            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n","    ]\n","    optimizer = AdamW(\n","        optimizer_grouped_parameters,\n","        lr=CFG.lr,\n","        betas=CFG.betas,\n","        weight_decay=CFG.weight_decay,\n","    )\n","    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n","    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        num_training_steps=num_train_optimization_steps,\n","    )\n","\n","    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    best_score = -1 * np.inf\n","\n","    for epoch in range(CFG.epochs):\n","        start_time = time.time()\n","        avg_loss = train_fn(\n","            train_dataloader,\n","            model,\n","            criterion,\n","            optimizer,\n","            epoch,\n","            scheduler,\n","            device,\n","        )\n","        avg_val_loss, val_preds = valid_fn(\n","            val_dataloader,\n","            model,\n","            criterion,\n","            device,\n","        )\n","\n","        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n","            scheduler.step()\n","\n","        # scoring\n","        val_folds[[str(i) for i in range(CFG.max_len)]] = val_preds\n","        score = scoring(val_folds, th=0.5)\n","\n","        elapsed = time.time() - start_time\n","\n","        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n","        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n","        if score > best_score:\n","            best_score = score\n","            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n","            torch.save({\n","                \"model\": model.state_dict(),\n","                \"predictions\": val_preds,\n","                },\n","                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","            )\n","\n","    predictions = torch.load(\n","        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","        map_location=torch.device(\"cpu\"),\n","    )[\"predictions\"]\n","    val_folds[[str(i) for i in range(CFG.max_len)]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return val_folds"]},{"cell_type":"markdown","id":"bearing-switch","metadata":{"id":"bearing-switch"},"source":["## Main"]},{"cell_type":"code","execution_count":null,"id":"desperate-crime","metadata":{"id":"desperate-crime"},"outputs":[],"source":["def main():\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    if CFG.train:\n","        oof_df = pd.DataFrame()\n","        for i_fold in range(CFG.n_fold):\n","            if i_fold in CFG.train_fold:\n","                _oof_df = train_loop(train, i_fold, device)\n","                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n","        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    if CFG.submission:\n","        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n","    else:\n","        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    score = scoring(oof_df, th=0.5)\n","    print(f\"Best thres: 0.5, Score: {score:.4f}\")\n","    best_thres = get_best_thres(oof_df)\n","    score = scoring(oof_df, th=best_thres)\n","    print(f\"Best thres: {best_thres}, Score: {score:.4f}\")\n","\n","    if CFG.inference:\n","        test_dataset = TestDataset(CFG, test)\n","        test_dataloader = DataLoader(\n","            test_dataset,\n","            batch_size=CFG.batch_size,\n","            shuffle=False,\n","            num_workers=CFG.num_workers,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","        predictions = []\n","        for i_fold in CFG.train_fold:\n","            if CFG.submission:\n","                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n","                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n","            else:\n","                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n","\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            model.load_state_dict(state[\"model\"])\n","            test_token_probs = inference_fn(test_dataloader, model, device)\n","            test[[f\"fold{i_fold}_{i}\" for i in range(CFG.max_len)]] = test_token_probs\n","            test_char_probs = get_char_probs(test[\"pn_history\"].values, test_token_probs, CFG.tokenizer)\n","            predictions.append(test_char_probs)\n","\n","            del state, test_token_probs, model; gc.collect()\n","            torch.cuda.empty_cache()\n","\n","        predictions = np.mean(predictions, axis=0)\n","        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n","        test[CFG.target_col] = predicted_location_str\n","        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n","        test[[CFG.id_col, CFG.target_col]].to_csv(\n","            CFG.output_dir / \"submission.csv\", index=False\n","        )"]},{"cell_type":"code","execution_count":null,"id":"graduate-vision","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["2d624ee031534bcda734e02496078a16","07dd4e926a6a4902a296e1f49d960653","0025679f30f14eae8cf1dc5ba7a3d0cf","9b5704ce56db4c0fad11e555cc31c138","a37c75cc4bcd4d1496439cab94d346fa","74b1db0352d642de89931603066de877","5d539ad71f6f4fbcb972361d06f12ba7","70b07b299ef848a0b1128ad5b9312a5d","9ec1a62a7fc542f2a580850d4f11784e","8941f98fdce5478c85a3071871b52299","32d65665b0ff4bd6bfed10e0aaaf4f49","22f43d6b8c994773a844003a8381204d","c203051f4ade4dcb92facf6ea808af94","60d5cf602a6a409cb6e9c3305284526b","b0b7de67fe9c487ea1f77dcba4b73aaf","9bba4c8c91f74a9895404db4a405bfde","28ccab34aa244e02bf17242eed9a5d1b","6013524181d44d038c2b8db40d11c36b","da4d29e8f5c144b18d7f510257cc8d23","146fa233e89643edb1fcc0ce5b7d91f6","d0191b7be9804b7bb89c86c0ba70c6d4","96973aa8207c4502a0fd2475a3ef7df7","103908b1f9c44f74a236034bc467e3ed","974cfaef615e4c1eb712d202eaaadcc4","e387f43ccfa44a88b5c86347c6a2e81a","7e3ef616abae415297b2e6f6d04eff04","2ef885e81bd5458ba9c1de718ddb8583","dd8fe517cbc847129e37f3b7b6420d65","63c765ac577f41bbb80f181e7e7f3a09","c7b33dfc0955442bbce76fe0fe67302e","f78d14ec18a4465baac5c0e238420b94","506c780410ce4660b1d03bf775bc0fcb","3d54f9bbdbc54794a537c3b43f3e191e","a2b21e18492f4e45a4b210b07beb8934","27c7ff08010e4199941161a8a5d75387","314f01b476f841faac8b43edc281bdf1","af2b16b279e641a581b6c29353868205","f2674a6b545e40f49fe4a1a58deabe93","01268b4c5ef34a7eb9fa26afbea0d45c","7f571ce5ef3e4ad2a5bc4bcbf66e7532","8129bd51909d45d4a088b7edecd0b959","636522976ea3430e8ef91a5fe9b2cf8a","1c03d37eba784be4bee78e53b3bdb7b5","92bc6917133148cbbf87db00e7e40805","e98dfc73998740af9770e614420913f5","73c2cf6cb2a74a4fb7a3b03e7cc3a9a7","e174ad484db142cfaf911a730cd97625","066594bdb7c24c068613869d69aef40a","99bfd37198fd43ffb6cc9b44341f6df8","ae14cd4fbd614aaabc433aef06177d11","f813077fe25d47909de1b563ea50573a","e46367582d194a96ac46adef6789dd09","ada34b7955d04a21a02ba2d70c011d53","8ec840ed29b843df975c6ffb4dab03cc","2519d45e725c48caae5d98f27136b1cc","040168b615644c17bc1eced828f836c6","4ce2358e15f047008f91bfe22e2178eb","91d28013a9ff4c10af50749abb28af80","ef4efd34557f42349460d46e9708deaa","4f1a70d9da3542fbb64d346136eee3b6","2d6a562dbdf64f528cc21a7e47077a86","1de8e9089c8d4e62b34404544a604106","ab88b315c4cf4b76956fb04662bf2531","6fcdcd5071ee4411bcdc92fb2b95d99d","73860debd7834167b72d6902d4bebdf5","c591425dbc1447a988024edc28ae5071"]},"executionInfo":{"elapsed":24741575,"status":"ok","timestamp":1646469640801,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"graduate-vision","outputId":"98d9dbec-058d-4987-f31d-dfd638b2d147"},"outputs":[{"output_type":"stream","name":"stdout","text":["========== fold: 0 training ==========\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2d624ee031534bcda734e02496078a16","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/833M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n","Epoch: [1][0/2849] Elapsed 0m 1s (remain 50m 46s) Loss: 0.3878(0.3878) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/2849] Elapsed 0m 31s (remain 14m 4s) Loss: 0.2156(0.3392) Grad: 20134.2695  LR: 0.000001  \n","Epoch: [1][200/2849] Elapsed 0m 59s (remain 13m 6s) Loss: 0.0306(0.2094) Grad: 545.0804  LR: 0.000003  \n","Epoch: [1][300/2849] Elapsed 1m 28s (remain 12m 27s) Loss: 0.0202(0.1518) Grad: 4092.7451  LR: 0.000004  \n","Epoch: [1][400/2849] Elapsed 1m 56s (remain 11m 51s) Loss: 0.0150(0.1198) Grad: 4643.6265  LR: 0.000006  \n","Epoch: [1][500/2849] Elapsed 2m 24s (remain 11m 19s) Loss: 0.0136(0.0993) Grad: 3450.7693  LR: 0.000007  \n","Epoch: [1][600/2849] Elapsed 2m 53s (remain 10m 48s) Loss: 0.0031(0.0855) Grad: 710.8674  LR: 0.000008  \n","Epoch: [1][700/2849] Elapsed 3m 21s (remain 10m 17s) Loss: 0.0025(0.0759) Grad: 880.6495  LR: 0.000010  \n","Epoch: [1][800/2849] Elapsed 3m 49s (remain 9m 47s) Loss: 0.0179(0.0683) Grad: 572.2186  LR: 0.000011  \n","Epoch: [1][900/2849] Elapsed 4m 18s (remain 9m 19s) Loss: 0.0022(0.0620) Grad: 690.1661  LR: 0.000013  \n","Epoch: [1][1000/2849] Elapsed 4m 46s (remain 8m 49s) Loss: 0.0203(0.0568) Grad: 4463.9355  LR: 0.000014  \n","Epoch: [1][1100/2849] Elapsed 5m 15s (remain 8m 20s) Loss: 0.0117(0.0527) Grad: 1133.2859  LR: 0.000015  \n","Epoch: [1][1200/2849] Elapsed 5m 43s (remain 7m 51s) Loss: 0.0014(0.0493) Grad: 367.3742  LR: 0.000017  \n","Epoch: [1][1300/2849] Elapsed 6m 11s (remain 7m 22s) Loss: 0.0160(0.0462) Grad: 1097.0969  LR: 0.000018  \n","Epoch: [1][1400/2849] Elapsed 6m 40s (remain 6m 53s) Loss: 0.0100(0.0438) Grad: 3797.7466  LR: 0.000020  \n","Epoch: [1][1500/2849] Elapsed 7m 8s (remain 6m 25s) Loss: 0.0076(0.0415) Grad: 2379.4026  LR: 0.000020  \n","Epoch: [1][1600/2849] Elapsed 7m 37s (remain 5m 56s) Loss: 0.0028(0.0398) Grad: 480.7053  LR: 0.000020  \n","Epoch: [1][1700/2849] Elapsed 8m 5s (remain 5m 27s) Loss: 0.0034(0.0382) Grad: 1048.1117  LR: 0.000020  \n","Epoch: [1][1800/2849] Elapsed 8m 34s (remain 4m 59s) Loss: 0.0009(0.0368) Grad: 120.6372  LR: 0.000019  \n","Epoch: [1][1900/2849] Elapsed 9m 3s (remain 4m 30s) Loss: 0.0073(0.0354) Grad: 1429.8582  LR: 0.000019  \n","Epoch: [1][2000/2849] Elapsed 9m 31s (remain 4m 2s) Loss: 0.0087(0.0342) Grad: 1363.5835  LR: 0.000019  \n","Epoch: [1][2100/2849] Elapsed 9m 59s (remain 3m 33s) Loss: 0.0142(0.0331) Grad: 6845.5459  LR: 0.000019  \n","Epoch: [1][2200/2849] Elapsed 10m 28s (remain 3m 4s) Loss: 0.0189(0.0320) Grad: 656.9371  LR: 0.000019  \n","Epoch: [1][2300/2849] Elapsed 10m 56s (remain 2m 36s) Loss: 0.0086(0.0310) Grad: 802.3376  LR: 0.000019  \n","Epoch: [1][2400/2849] Elapsed 11m 24s (remain 2m 7s) Loss: 0.0005(0.0302) Grad: 66.6267  LR: 0.000018  \n","Epoch: [1][2500/2849] Elapsed 11m 52s (remain 1m 39s) Loss: 0.0002(0.0293) Grad: 36.5889  LR: 0.000018  \n","Epoch: [1][2600/2849] Elapsed 12m 21s (remain 1m 10s) Loss: 0.0090(0.0285) Grad: 2543.7102  LR: 0.000018  \n","Epoch: [1][2700/2849] Elapsed 12m 49s (remain 0m 42s) Loss: 0.0309(0.0277) Grad: 3012.1694  LR: 0.000018  \n","Epoch: [1][2800/2849] Elapsed 13m 17s (remain 0m 13s) Loss: 0.0108(0.0271) Grad: 901.5841  LR: 0.000018  \n","Epoch: [1][2848/2849] Elapsed 13m 31s (remain 0m 0s) Loss: 0.0082(0.0267) Grad: 1437.4315  LR: 0.000018  \n","EVAL: [0/726] Elapsed 0m 0s (remain 6m 4s) Loss: 0.0021(0.0021) \n","EVAL: [100/726] Elapsed 0m 17s (remain 1m 50s) Loss: 0.0058(0.0073) \n","EVAL: [200/726] Elapsed 0m 35s (remain 1m 32s) Loss: 0.0005(0.0082) \n","EVAL: [300/726] Elapsed 0m 52s (remain 1m 14s) Loss: 0.0001(0.0083) \n","EVAL: [400/726] Elapsed 1m 10s (remain 0m 56s) Loss: 0.0046(0.0100) \n","EVAL: [500/726] Elapsed 1m 27s (remain 0m 39s) Loss: 0.0421(0.0102) \n","EVAL: [600/726] Elapsed 1m 45s (remain 0m 21s) Loss: 0.0109(0.0096) \n","EVAL: [700/726] Elapsed 2m 2s (remain 0m 4s) Loss: 0.0032(0.0091) \n","EVAL: [725/726] Elapsed 2m 6s (remain 0m 0s) Loss: 0.0001(0.0089) \n","Epoch 1 - avg_train_loss: 0.0267  avg_val_loss: 0.0089  time: 950s\n","Epoch 1 - Score: 0.8504\n","Epoch 1 - Save Best Score: 0.8504 Model\n","Epoch: [2][0/2849] Elapsed 0m 0s (remain 27m 53s) Loss: 0.0031(0.0031) Grad: 7347.4014  LR: 0.000018  \n","Epoch: [2][100/2849] Elapsed 0m 29s (remain 13m 9s) Loss: 0.0039(0.0077) Grad: 9182.6191  LR: 0.000018  \n","Epoch: [2][200/2849] Elapsed 0m 57s (remain 12m 42s) Loss: 0.0001(0.0070) Grad: 207.4477  LR: 0.000017  \n","Epoch: [2][300/2849] Elapsed 1m 25s (remain 11m 59s) Loss: 0.0062(0.0073) Grad: 13145.6465  LR: 0.000017  \n","Epoch: [2][400/2849] Elapsed 1m 52s (remain 11m 24s) Loss: 0.0004(0.0076) Grad: 1538.2032  LR: 0.000017  \n","Epoch: [2][500/2849] Elapsed 2m 19s (remain 10m 53s) Loss: 0.0001(0.0073) Grad: 371.8474  LR: 0.000017  \n","Epoch: [2][600/2849] Elapsed 2m 46s (remain 10m 23s) Loss: 0.0052(0.0071) Grad: 12091.9727  LR: 0.000017  \n","Epoch: [2][700/2849] Elapsed 3m 13s (remain 9m 54s) Loss: 0.0290(0.0070) Grad: 45744.3164  LR: 0.000017  \n","Epoch: [2][800/2849] Elapsed 3m 41s (remain 9m 25s) Loss: 0.0158(0.0075) Grad: 26777.8613  LR: 0.000017  \n","Epoch: [2][900/2849] Elapsed 4m 8s (remain 8m 56s) Loss: 0.0001(0.0074) Grad: 169.2677  LR: 0.000016  \n","Epoch: [2][1000/2849] Elapsed 4m 35s (remain 8m 28s) Loss: 0.0014(0.0073) Grad: 16929.3848  LR: 0.000016  \n","Epoch: [2][1100/2849] Elapsed 5m 2s (remain 8m 0s) Loss: 0.0010(0.0074) Grad: 7705.0415  LR: 0.000016  \n","Epoch: [2][1200/2849] Elapsed 5m 29s (remain 7m 32s) Loss: 0.0110(0.0075) Grad: 16844.2871  LR: 0.000016  \n","Epoch: [2][1300/2849] Elapsed 5m 56s (remain 7m 4s) Loss: 0.0106(0.0074) Grad: 290573.0625  LR: 0.000016  \n","Epoch: [2][1400/2849] Elapsed 6m 24s (remain 6m 36s) Loss: 0.0001(0.0073) Grad: 420.2684  LR: 0.000016  \n","Epoch: [2][1500/2849] Elapsed 6m 51s (remain 6m 9s) Loss: 0.0150(0.0074) Grad: 18297.7363  LR: 0.000015  \n","Epoch: [2][1600/2849] Elapsed 7m 18s (remain 5m 41s) Loss: 0.0046(0.0073) Grad: 18112.7480  LR: 0.000015  \n","Epoch: [2][1700/2849] Elapsed 7m 45s (remain 5m 14s) Loss: 0.0011(0.0073) Grad: 14260.3984  LR: 0.000015  \n","Epoch: [2][1800/2849] Elapsed 8m 12s (remain 4m 46s) Loss: 0.0178(0.0074) Grad: 24895.4805  LR: 0.000015  \n","Epoch: [2][1900/2849] Elapsed 8m 39s (remain 4m 19s) Loss: 0.0042(0.0075) Grad: 7927.5415  LR: 0.000015  \n","Epoch: [2][2000/2849] Elapsed 9m 7s (remain 3m 51s) Loss: 0.0003(0.0074) Grad: 4105.9111  LR: 0.000015  \n","Epoch: [2][2100/2849] Elapsed 9m 34s (remain 3m 24s) Loss: 0.0024(0.0074) Grad: 6401.3325  LR: 0.000014  \n","Epoch: [2][2200/2849] Elapsed 10m 1s (remain 2m 57s) Loss: 0.0016(0.0073) Grad: 3949.9966  LR: 0.000014  \n","Epoch: [2][2300/2849] Elapsed 10m 28s (remain 2m 29s) Loss: 0.0590(0.0072) Grad: 423726.4062  LR: 0.000014  \n","Epoch: [2][2400/2849] Elapsed 10m 55s (remain 2m 2s) Loss: 0.0001(0.0074) Grad: 825.5946  LR: 0.000014  \n","Epoch: [2][2500/2849] Elapsed 11m 22s (remain 1m 35s) Loss: 0.0005(0.0074) Grad: 2746.0649  LR: 0.000014  \n","Epoch: [2][2600/2849] Elapsed 11m 49s (remain 1m 7s) Loss: 0.0092(0.0074) Grad: 21073.5273  LR: 0.000014  \n","Epoch: [2][2700/2849] Elapsed 12m 17s (remain 0m 40s) Loss: 0.0237(0.0073) Grad: 35603.1602  LR: 0.000014  \n","Epoch: [2][2800/2849] Elapsed 12m 44s (remain 0m 13s) Loss: 0.0003(0.0073) Grad: 1034.7826  LR: 0.000013  \n","Epoch: [2][2848/2849] Elapsed 12m 57s (remain 0m 0s) Loss: 0.0016(0.0073) Grad: 6472.8657  LR: 0.000013  \n","EVAL: [0/726] Elapsed 0m 0s (remain 5m 31s) Loss: 0.0008(0.0008) \n","EVAL: [100/726] Elapsed 0m 17s (remain 1m 50s) Loss: 0.0082(0.0103) \n","EVAL: [200/726] Elapsed 0m 35s (remain 1m 32s) Loss: 0.0000(0.0098) \n","EVAL: [300/726] Elapsed 0m 52s (remain 1m 14s) Loss: 0.0000(0.0100) \n","EVAL: [400/726] Elapsed 1m 9s (remain 0m 56s) Loss: 0.0017(0.0121) \n","EVAL: [500/726] Elapsed 1m 27s (remain 0m 39s) Loss: 0.0560(0.0120) \n","EVAL: [600/726] Elapsed 1m 44s (remain 0m 21s) Loss: 0.0008(0.0112) \n","EVAL: [700/726] Elapsed 2m 1s (remain 0m 4s) Loss: 0.0053(0.0107) \n","EVAL: [725/726] Elapsed 2m 6s (remain 0m 0s) Loss: 0.0000(0.0105) \n","Epoch 2 - avg_train_loss: 0.0073  avg_val_loss: 0.0105  time: 907s\n","Epoch 2 - Score: 0.8706\n","Epoch 2 - Save Best Score: 0.8706 Model\n","Epoch: [3][0/2849] Elapsed 0m 0s (remain 28m 3s) Loss: 0.0168(0.0168) Grad: 114846.7031  LR: 0.000013  \n","Epoch: [3][100/2849] Elapsed 0m 30s (remain 13m 52s) Loss: 0.0139(0.0077) Grad: 13630.2080  LR: 0.000013  \n","Epoch: [3][200/2849] Elapsed 0m 59s (remain 13m 4s) Loss: 0.0038(0.0070) Grad: 19371.3262  LR: 0.000013  \n","Epoch: [3][300/2849] Elapsed 1m 27s (remain 12m 22s) Loss: 0.0001(0.0066) Grad: 322.9385  LR: 0.000013  \n","Epoch: [3][400/2849] Elapsed 1m 55s (remain 11m 45s) Loss: 0.0016(0.0062) Grad: 8359.2256  LR: 0.000013  \n","Epoch: [3][500/2849] Elapsed 2m 23s (remain 11m 12s) Loss: 0.0001(0.0063) Grad: 273.8507  LR: 0.000013  \n","Epoch: [3][600/2849] Elapsed 2m 51s (remain 10m 41s) Loss: 0.0000(0.0062) Grad: 46.4527  LR: 0.000012  \n","Epoch: [3][700/2849] Elapsed 3m 19s (remain 10m 10s) Loss: 0.0001(0.0061) Grad: 282.8623  LR: 0.000012  \n","Epoch: [3][800/2849] Elapsed 3m 47s (remain 9m 41s) Loss: 0.0003(0.0059) Grad: 1906.5934  LR: 0.000012  \n","Epoch: [3][900/2849] Elapsed 4m 15s (remain 9m 12s) Loss: 0.0005(0.0060) Grad: 1501.0868  LR: 0.000012  \n","Epoch: [3][1000/2849] Elapsed 4m 44s (remain 8m 44s) Loss: 0.0030(0.0060) Grad: 6699.0557  LR: 0.000012  \n","Epoch: [3][1100/2849] Elapsed 5m 12s (remain 8m 16s) Loss: 0.0002(0.0062) Grad: 893.4235  LR: 0.000012  \n","Epoch: [3][1200/2849] Elapsed 5m 40s (remain 7m 47s) Loss: 0.0373(0.0062) Grad: 97679.4844  LR: 0.000011  \n","Epoch: [3][1300/2849] Elapsed 6m 8s (remain 7m 18s) Loss: 0.0033(0.0062) Grad: 2783.7290  LR: 0.000011  \n","Epoch: [3][1400/2849] Elapsed 6m 36s (remain 6m 49s) Loss: 0.0181(0.0061) Grad: 35809.9531  LR: 0.000011  \n","Epoch: [3][1500/2849] Elapsed 7m 4s (remain 6m 21s) Loss: 0.0305(0.0062) Grad: 24891.7441  LR: 0.000011  \n","Epoch: [3][1600/2849] Elapsed 7m 32s (remain 5m 52s) Loss: 0.0000(0.0061) Grad: 102.4371  LR: 0.000011  \n","Epoch: [3][1700/2849] Elapsed 8m 0s (remain 5m 24s) Loss: 0.0204(0.0061) Grad: 40146.1523  LR: 0.000011  \n","Epoch: [3][1800/2849] Elapsed 8m 28s (remain 4m 55s) Loss: 0.0116(0.0061) Grad: 41664.8672  LR: 0.000011  \n","Epoch: [3][1900/2849] Elapsed 8m 56s (remain 4m 27s) Loss: 0.0003(0.0060) Grad: 1186.6401  LR: 0.000010  \n","Epoch: [3][2000/2849] Elapsed 9m 25s (remain 3m 59s) Loss: 0.0129(0.0059) Grad: 14460.8955  LR: 0.000010  \n","Epoch: [3][2100/2849] Elapsed 9m 53s (remain 3m 31s) Loss: 0.0000(0.0059) Grad: 78.2543  LR: 0.000010  \n","Epoch: [3][2200/2849] Elapsed 10m 21s (remain 3m 2s) Loss: 0.0026(0.0059) Grad: 6816.6221  LR: 0.000010  \n","Epoch: [3][2300/2849] Elapsed 10m 49s (remain 2m 34s) Loss: 0.0027(0.0059) Grad: 7031.8623  LR: 0.000010  \n","Epoch: [3][2400/2849] Elapsed 11m 16s (remain 2m 6s) Loss: 0.0230(0.0059) Grad: 147720.0000  LR: 0.000010  \n","Epoch: [3][2500/2849] Elapsed 11m 45s (remain 1m 38s) Loss: 0.0048(0.0059) Grad: 16866.8027  LR: 0.000009  \n","Epoch: [3][2600/2849] Elapsed 12m 12s (remain 1m 9s) Loss: 0.0541(0.0059) Grad: 17439.6250  LR: 0.000009  \n","Epoch: [3][2700/2849] Elapsed 12m 40s (remain 0m 41s) Loss: 0.0043(0.0059) Grad: 25641.1992  LR: 0.000009  \n","Epoch: [3][2800/2849] Elapsed 13m 8s (remain 0m 13s) Loss: 0.0032(0.0059) Grad: 5335.9136  LR: 0.000009  \n","Epoch: [3][2848/2849] Elapsed 13m 22s (remain 0m 0s) Loss: 0.0002(0.0059) Grad: 11454.4346  LR: 0.000009  \n","EVAL: [0/726] Elapsed 0m 0s (remain 5m 57s) Loss: 0.0022(0.0022) \n","EVAL: [100/726] Elapsed 0m 18s (remain 1m 56s) Loss: 0.0018(0.0084) \n","EVAL: [200/726] Elapsed 0m 37s (remain 1m 37s) Loss: 0.0000(0.0092) \n","EVAL: [300/726] Elapsed 0m 55s (remain 1m 18s) Loss: 0.0000(0.0088) \n","EVAL: [400/726] Elapsed 1m 14s (remain 0m 59s) Loss: 0.0036(0.0107) \n","EVAL: [500/726] Elapsed 1m 32s (remain 0m 41s) Loss: 0.0512(0.0104) \n","EVAL: [600/726] Elapsed 1m 50s (remain 0m 23s) Loss: 0.0007(0.0095) \n","EVAL: [700/726] Elapsed 2m 9s (remain 0m 4s) Loss: 0.0016(0.0090) \n","EVAL: [725/726] Elapsed 2m 13s (remain 0m 0s) Loss: 0.0000(0.0088) \n","Epoch 3 - avg_train_loss: 0.0059  avg_val_loss: 0.0088  time: 949s\n","Epoch 3 - Score: 0.8756\n","Epoch 3 - Save Best Score: 0.8756 Model\n","Epoch: [4][0/2849] Elapsed 0m 0s (remain 27m 1s) Loss: 0.0003(0.0003) Grad: 2429.1458  LR: 0.000009  \n","Epoch: [4][100/2849] Elapsed 0m 30s (remain 13m 37s) Loss: 0.0025(0.0042) Grad: 9545.5508  LR: 0.000009  \n","Epoch: [4][200/2849] Elapsed 0m 58s (remain 12m 49s) Loss: 0.0010(0.0054) Grad: 2717.3999  LR: 0.000009  \n","Epoch: [4][300/2849] Elapsed 1m 25s (remain 12m 5s) Loss: 0.0077(0.0054) Grad: 311214.7812  LR: 0.000008  \n","Epoch: [4][400/2849] Elapsed 1m 53s (remain 11m 30s) Loss: 0.0001(0.0049) Grad: 473.2066  LR: 0.000008  \n","Epoch: [4][500/2849] Elapsed 2m 20s (remain 10m 57s) Loss: 0.0004(0.0047) Grad: 2305.2419  LR: 0.000008  \n","Epoch: [4][600/2849] Elapsed 2m 47s (remain 10m 27s) Loss: 0.0028(0.0046) Grad: 49145.6523  LR: 0.000008  \n","Epoch: [4][700/2849] Elapsed 3m 15s (remain 9m 57s) Loss: 0.0297(0.0047) Grad: 17167.4766  LR: 0.000008  \n","Epoch: [4][800/2849] Elapsed 3m 42s (remain 9m 28s) Loss: 0.0006(0.0048) Grad: 2006.1934  LR: 0.000008  \n","Epoch: [4][900/2849] Elapsed 4m 9s (remain 8m 59s) Loss: 0.0001(0.0047) Grad: 212.9761  LR: 0.000007  \n","Epoch: [4][1000/2849] Elapsed 4m 36s (remain 8m 30s) Loss: 0.0002(0.0048) Grad: 1514.8510  LR: 0.000007  \n","Epoch: [4][1100/2849] Elapsed 5m 4s (remain 8m 2s) Loss: 0.0006(0.0048) Grad: 9703.2959  LR: 0.000007  \n","Epoch: [4][1200/2849] Elapsed 5m 31s (remain 7m 34s) Loss: 0.0005(0.0047) Grad: 3865.0947  LR: 0.000007  \n","Epoch: [4][1300/2849] Elapsed 5m 58s (remain 7m 7s) Loss: 0.0138(0.0047) Grad: 63521.5703  LR: 0.000007  \n","Epoch: [4][1400/2849] Elapsed 6m 26s (remain 6m 39s) Loss: 0.0000(0.0046) Grad: 111.6719  LR: 0.000007  \n","Epoch: [4][1500/2849] Elapsed 6m 53s (remain 6m 11s) Loss: 0.0003(0.0046) Grad: 1528.5753  LR: 0.000007  \n","Epoch: [4][1600/2849] Elapsed 7m 20s (remain 5m 43s) Loss: 0.0046(0.0047) Grad: 10921.9688  LR: 0.000006  \n","Epoch: [4][1700/2849] Elapsed 7m 48s (remain 5m 16s) Loss: 0.0113(0.0047) Grad: 80336.3594  LR: 0.000006  \n","Epoch: [4][1800/2849] Elapsed 8m 15s (remain 4m 48s) Loss: 0.0032(0.0047) Grad: 4536.4180  LR: 0.000006  \n","Epoch: [4][1900/2849] Elapsed 8m 42s (remain 4m 20s) Loss: 0.0000(0.0047) Grad: 63.0168  LR: 0.000006  \n","Epoch: [4][2000/2849] Elapsed 9m 10s (remain 3m 53s) Loss: 0.0006(0.0048) Grad: 3299.4773  LR: 0.000006  \n","Epoch: [4][2100/2849] Elapsed 9m 37s (remain 3m 25s) Loss: 0.0207(0.0049) Grad: 106147.8359  LR: 0.000006  \n","Epoch: [4][2200/2849] Elapsed 10m 4s (remain 2m 58s) Loss: 0.0001(0.0048) Grad: 295.6296  LR: 0.000005  \n","Epoch: [4][2300/2849] Elapsed 10m 32s (remain 2m 30s) Loss: 0.0014(0.0049) Grad: 7036.6631  LR: 0.000005  \n","Epoch: [4][2400/2849] Elapsed 10m 59s (remain 2m 3s) Loss: 0.0020(0.0050) Grad: 14515.7588  LR: 0.000005  \n","Epoch: [4][2500/2849] Elapsed 11m 26s (remain 1m 35s) Loss: 0.0013(0.0050) Grad: 9270.0791  LR: 0.000005  \n","Epoch: [4][2600/2849] Elapsed 11m 54s (remain 1m 8s) Loss: 0.0000(0.0050) Grad: 95.9710  LR: 0.000005  \n","Epoch: [4][2700/2849] Elapsed 12m 21s (remain 0m 40s) Loss: 0.0049(0.0050) Grad: 54398.2266  LR: 0.000005  \n","Epoch: [4][2800/2849] Elapsed 12m 48s (remain 0m 13s) Loss: 0.0000(0.0050) Grad: 139.8542  LR: 0.000005  \n","Epoch: [4][2848/2849] Elapsed 13m 2s (remain 0m 0s) Loss: 0.0115(0.0050) Grad: 33995.4023  LR: 0.000004  \n","EVAL: [0/726] Elapsed 0m 0s (remain 5m 35s) Loss: 0.0005(0.0005) \n","EVAL: [100/726] Elapsed 0m 18s (remain 1m 51s) Loss: 0.0054(0.0092) \n","EVAL: [200/726] Elapsed 0m 36s (remain 1m 34s) Loss: 0.0000(0.0093) \n","EVAL: [300/726] Elapsed 0m 53s (remain 1m 16s) Loss: 0.0000(0.0093) \n","EVAL: [400/726] Elapsed 1m 11s (remain 0m 58s) Loss: 0.0014(0.0111) \n","EVAL: [500/726] Elapsed 1m 29s (remain 0m 40s) Loss: 0.0583(0.0111) \n","EVAL: [600/726] Elapsed 1m 47s (remain 0m 22s) Loss: 0.0059(0.0103) \n","EVAL: [700/726] Elapsed 2m 4s (remain 0m 4s) Loss: 0.0010(0.0098) \n","EVAL: [725/726] Elapsed 2m 8s (remain 0m 0s) Loss: 0.0000(0.0096) \n","Epoch 4 - avg_train_loss: 0.0050  avg_val_loss: 0.0096  time: 916s\n","Epoch 4 - Score: 0.8815\n","Epoch 4 - Save Best Score: 0.8815 Model\n","Epoch: [5][0/2849] Elapsed 0m 0s (remain 30m 48s) Loss: 0.0010(0.0010) Grad: 5623.1089  LR: 0.000004  \n","Epoch: [5][100/2849] Elapsed 0m 32s (remain 14m 34s) Loss: 0.0003(0.0039) Grad: 3279.7559  LR: 0.000004  \n","Epoch: [5][200/2849] Elapsed 1m 2s (remain 13m 40s) Loss: 0.0006(0.0036) Grad: 2845.1589  LR: 0.000004  \n","Epoch: [5][300/2849] Elapsed 1m 31s (remain 12m 57s) Loss: 0.0000(0.0038) Grad: 37.1909  LR: 0.000004  \n","Epoch: [5][400/2849] Elapsed 2m 1s (remain 12m 19s) Loss: 0.0002(0.0035) Grad: 1762.0087  LR: 0.000004  \n","Epoch: [5][500/2849] Elapsed 2m 30s (remain 11m 45s) Loss: 0.0014(0.0036) Grad: 10439.8115  LR: 0.000004  \n","Epoch: [5][600/2849] Elapsed 3m 0s (remain 11m 13s) Loss: 0.0783(0.0038) Grad: 396400.7812  LR: 0.000004  \n","Epoch: [5][700/2849] Elapsed 3m 29s (remain 10m 41s) Loss: 0.0067(0.0037) Grad: 82894.0625  LR: 0.000003  \n","Epoch: [5][800/2849] Elapsed 3m 58s (remain 10m 10s) Loss: 0.0008(0.0038) Grad: 9976.5068  LR: 0.000003  \n","Epoch: [5][900/2849] Elapsed 4m 28s (remain 9m 40s) Loss: 0.0000(0.0036) Grad: 21.2765  LR: 0.000003  \n","Epoch: [5][1000/2849] Elapsed 4m 57s (remain 9m 9s) Loss: 0.0023(0.0036) Grad: 9867.4951  LR: 0.000003  \n","Epoch: [5][1100/2849] Elapsed 5m 27s (remain 8m 39s) Loss: 0.0036(0.0036) Grad: 28620.8223  LR: 0.000003  \n","Epoch: [5][1200/2849] Elapsed 5m 56s (remain 8m 9s) Loss: 0.0042(0.0035) Grad: 32769.3828  LR: 0.000003  \n","Epoch: [5][1300/2849] Elapsed 6m 26s (remain 7m 39s) Loss: 0.0046(0.0036) Grad: 74827.3984  LR: 0.000002  \n","Epoch: [5][1400/2849] Elapsed 6m 55s (remain 7m 9s) Loss: 0.0668(0.0037) Grad: 104240.5938  LR: 0.000002  \n","Epoch: [5][1500/2849] Elapsed 7m 24s (remain 6m 39s) Loss: 0.0000(0.0037) Grad: 126.0250  LR: 0.000002  \n","Epoch: [5][1600/2849] Elapsed 7m 54s (remain 6m 9s) Loss: 0.0000(0.0038) Grad: 31.6088  LR: 0.000002  \n","Epoch: [5][1700/2849] Elapsed 8m 23s (remain 5m 39s) Loss: 0.0000(0.0039) Grad: 80.4595  LR: 0.000002  \n","Epoch: [5][1800/2849] Elapsed 8m 52s (remain 5m 9s) Loss: 0.0018(0.0039) Grad: 13591.4062  LR: 0.000002  \n","Epoch: [5][1900/2849] Elapsed 9m 21s (remain 4m 40s) Loss: 0.0104(0.0039) Grad: 15726.6914  LR: 0.000001  \n","Epoch: [5][2000/2849] Elapsed 9m 51s (remain 4m 10s) Loss: 0.0002(0.0039) Grad: 942.3217  LR: 0.000001  \n","Epoch: [5][2100/2849] Elapsed 10m 20s (remain 3m 40s) Loss: 0.0060(0.0040) Grad: 7209.4082  LR: 0.000001  \n","Epoch: [5][2200/2849] Elapsed 10m 49s (remain 3m 11s) Loss: 0.0007(0.0040) Grad: 6181.0425  LR: 0.000001  \n","Epoch: [5][2300/2849] Elapsed 11m 19s (remain 2m 41s) Loss: 0.0000(0.0041) Grad: 130.6813  LR: 0.000001  \n","Epoch: [5][2400/2849] Elapsed 11m 48s (remain 2m 12s) Loss: 0.0001(0.0040) Grad: 275.1664  LR: 0.000001  \n","Epoch: [5][2500/2849] Elapsed 12m 17s (remain 1m 42s) Loss: 0.0000(0.0040) Grad: 68.7790  LR: 0.000001  \n","Epoch: [5][2600/2849] Elapsed 12m 47s (remain 1m 13s) Loss: 0.0003(0.0040) Grad: 3145.1738  LR: 0.000000  \n","Epoch: [5][2700/2849] Elapsed 13m 16s (remain 0m 43s) Loss: 0.0001(0.0040) Grad: 328.6193  LR: 0.000000  \n","Epoch: [5][2800/2849] Elapsed 13m 45s (remain 0m 14s) Loss: 0.0007(0.0039) Grad: 4757.6221  LR: 0.000000  \n","Epoch: [5][2848/2849] Elapsed 13m 59s (remain 0m 0s) Loss: 0.0032(0.0039) Grad: 8081.5869  LR: 0.000000  \n","EVAL: [0/726] Elapsed 0m 0s (remain 6m 10s) Loss: 0.0004(0.0004) \n","EVAL: [100/726] Elapsed 0m 20s (remain 2m 5s) Loss: 0.0051(0.0104) \n","EVAL: [200/726] Elapsed 0m 40s (remain 1m 44s) Loss: 0.0000(0.0105) \n","EVAL: [300/726] Elapsed 0m 59s (remain 1m 24s) Loss: 0.0000(0.0103) \n","EVAL: [400/726] Elapsed 1m 19s (remain 1m 4s) Loss: 0.0011(0.0121) \n","EVAL: [500/726] Elapsed 1m 39s (remain 0m 44s) Loss: 0.0654(0.0121) \n","EVAL: [600/726] Elapsed 1m 59s (remain 0m 24s) Loss: 0.0078(0.0111) \n","EVAL: [700/726] Elapsed 2m 18s (remain 0m 4s) Loss: 0.0008(0.0106) \n","EVAL: [725/726] Elapsed 2m 23s (remain 0m 0s) Loss: 0.0000(0.0104) \n","Epoch 5 - avg_train_loss: 0.0039  avg_val_loss: 0.0104  time: 997s\n","Epoch 5 - Score: 0.8842\n","Epoch 5 - Save Best Score: 0.8842 Model\n","========== fold: 1 training ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n","Epoch: [1][0/2851] Elapsed 0m 0s (remain 34m 13s) Loss: 0.3657(0.3657) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/2851] Elapsed 0m 32s (remain 14m 32s) Loss: 0.1824(0.5024) Grad: 15783.0342  LR: 0.000001  \n","Epoch: [1][200/2851] Elapsed 0m 59s (remain 13m 4s) Loss: 0.0341(0.3037) Grad: 432.4749  LR: 0.000003  \n","Epoch: [1][300/2851] Elapsed 1m 26s (remain 12m 16s) Loss: 0.0300(0.2131) Grad: 1725.2284  LR: 0.000004  \n","Epoch: [1][400/2851] Elapsed 1m 54s (remain 11m 38s) Loss: 0.0118(0.1656) Grad: 3490.2212  LR: 0.000006  \n","Epoch: [1][500/2851] Elapsed 2m 21s (remain 11m 4s) Loss: 0.0046(0.1362) Grad: 927.0214  LR: 0.000007  \n","Epoch: [1][600/2851] Elapsed 2m 49s (remain 10m 34s) Loss: 0.0267(0.1166) Grad: 9656.2939  LR: 0.000008  \n","Epoch: [1][700/2851] Elapsed 3m 16s (remain 10m 3s) Loss: 0.0063(0.1018) Grad: 791.7946  LR: 0.000010  \n","Epoch: [1][800/2851] Elapsed 3m 44s (remain 9m 34s) Loss: 0.0016(0.0906) Grad: 534.2604  LR: 0.000011  \n","Epoch: [1][900/2851] Elapsed 4m 11s (remain 9m 4s) Loss: 0.0051(0.0821) Grad: 1448.3289  LR: 0.000013  \n","Epoch: [1][1000/2851] Elapsed 4m 38s (remain 8m 35s) Loss: 0.0143(0.0752) Grad: 3357.4932  LR: 0.000014  \n","Epoch: [1][1100/2851] Elapsed 5m 6s (remain 8m 6s) Loss: 0.0094(0.0693) Grad: 1275.9447  LR: 0.000015  \n","Epoch: [1][1200/2851] Elapsed 5m 33s (remain 7m 38s) Loss: 0.0068(0.0645) Grad: 766.9757  LR: 0.000017  \n","Epoch: [1][1300/2851] Elapsed 6m 1s (remain 7m 10s) Loss: 0.0100(0.0604) Grad: 988.0224  LR: 0.000018  \n","Epoch: [1][1400/2851] Elapsed 6m 28s (remain 6m 41s) Loss: 0.0176(0.0568) Grad: 1501.2217  LR: 0.000020  \n","Epoch: [1][1500/2851] Elapsed 6m 55s (remain 6m 13s) Loss: 0.0044(0.0538) Grad: 854.5686  LR: 0.000020  \n","Epoch: [1][1600/2851] Elapsed 7m 23s (remain 5m 45s) Loss: 0.0225(0.0514) Grad: 3545.6760  LR: 0.000020  \n","Epoch: [1][1700/2851] Elapsed 7m 50s (remain 5m 18s) Loss: 0.0103(0.0490) Grad: 1109.7802  LR: 0.000020  \n","Epoch: [1][1800/2851] Elapsed 8m 18s (remain 4m 50s) Loss: 0.0305(0.0469) Grad: 2027.2142  LR: 0.000019  \n","Epoch: [1][1900/2851] Elapsed 8m 45s (remain 4m 22s) Loss: 0.0327(0.0449) Grad: 2691.7610  LR: 0.000019  \n","Epoch: [1][2000/2851] Elapsed 9m 13s (remain 3m 55s) Loss: 0.0199(0.0431) Grad: 1413.9847  LR: 0.000019  \n","Epoch: [1][2100/2851] Elapsed 9m 40s (remain 3m 27s) Loss: 0.0164(0.0415) Grad: 568.1262  LR: 0.000019  \n","Epoch: [1][2200/2851] Elapsed 10m 8s (remain 2m 59s) Loss: 0.0351(0.0401) Grad: 6350.9033  LR: 0.000019  \n","Epoch: [1][2300/2851] Elapsed 10m 35s (remain 2m 31s) Loss: 0.0009(0.0388) Grad: 184.6418  LR: 0.000019  \n","Epoch: [1][2400/2851] Elapsed 11m 2s (remain 2m 4s) Loss: 0.0001(0.0375) Grad: 12.2986  LR: 0.000018  \n","Epoch: [1][2500/2851] Elapsed 11m 30s (remain 1m 36s) Loss: 0.0154(0.0364) Grad: 3574.7148  LR: 0.000018  \n","Epoch: [1][2600/2851] Elapsed 11m 57s (remain 1m 8s) Loss: 0.0049(0.0353) Grad: 543.4828  LR: 0.000018  \n","Epoch: [1][2700/2851] Elapsed 12m 24s (remain 0m 41s) Loss: 0.0011(0.0343) Grad: 370.7779  LR: 0.000018  \n","Epoch: [1][2800/2851] Elapsed 12m 52s (remain 0m 13s) Loss: 0.0004(0.0334) Grad: 52.3288  LR: 0.000018  \n","Epoch: [1][2850/2851] Elapsed 13m 6s (remain 0m 0s) Loss: 0.0065(0.0329) Grad: 4432.0991  LR: 0.000018  \n","EVAL: [0/724] Elapsed 0m 0s (remain 6m 13s) Loss: 0.0012(0.0012) \n","EVAL: [100/724] Elapsed 0m 18s (remain 1m 51s) Loss: 0.0026(0.0078) \n","EVAL: [200/724] Elapsed 0m 35s (remain 1m 33s) Loss: 0.0001(0.0092) \n","EVAL: [300/724] Elapsed 0m 53s (remain 1m 15s) Loss: 0.0108(0.0092) \n","EVAL: [400/724] Elapsed 1m 11s (remain 0m 57s) Loss: 0.0000(0.0110) \n","EVAL: [500/724] Elapsed 1m 28s (remain 0m 39s) Loss: 0.0093(0.0117) \n","EVAL: [600/724] Elapsed 1m 46s (remain 0m 21s) Loss: 0.0040(0.0111) \n","EVAL: [700/724] Elapsed 2m 3s (remain 0m 4s) Loss: 0.0000(0.0102) \n","EVAL: [723/724] Elapsed 2m 7s (remain 0m 0s) Loss: 0.0013(0.0102) \n","Epoch 1 - avg_train_loss: 0.0329  avg_val_loss: 0.0102  time: 918s\n","Epoch 1 - Score: 0.8241\n","Epoch 1 - Save Best Score: 0.8241 Model\n","Epoch: [2][0/2851] Elapsed 0m 0s (remain 32m 47s) Loss: 0.0038(0.0038) Grad: 12773.7695  LR: 0.000018  \n","Epoch: [2][100/2851] Elapsed 0m 29s (remain 13m 22s) Loss: 0.0024(0.0106) Grad: 8878.0811  LR: 0.000018  \n","Epoch: [2][200/2851] Elapsed 0m 58s (remain 12m 46s) Loss: 0.0071(0.0098) Grad: 19565.8965  LR: 0.000017  \n","Epoch: [2][300/2851] Elapsed 1m 25s (remain 12m 6s) Loss: 0.0056(0.0088) Grad: 12093.2988  LR: 0.000017  \n","Epoch: [2][400/2851] Elapsed 1m 53s (remain 11m 30s) Loss: 0.0041(0.0085) Grad: 11076.1055  LR: 0.000017  \n","Epoch: [2][500/2851] Elapsed 2m 20s (remain 10m 58s) Loss: 0.0076(0.0083) Grad: 14627.2402  LR: 0.000017  \n","Epoch: [2][600/2851] Elapsed 2m 47s (remain 10m 27s) Loss: 0.0051(0.0081) Grad: 12367.2910  LR: 0.000017  \n","Epoch: [2][700/2851] Elapsed 3m 14s (remain 9m 58s) Loss: 0.0711(0.0082) Grad: 105162.4609  LR: 0.000017  \n","Epoch: [2][800/2851] Elapsed 3m 42s (remain 9m 29s) Loss: 0.0013(0.0080) Grad: 2707.5112  LR: 0.000017  \n","Epoch: [2][900/2851] Elapsed 4m 9s (remain 9m 0s) Loss: 0.0002(0.0084) Grad: 567.9963  LR: 0.000016  \n","Epoch: [2][1000/2851] Elapsed 4m 36s (remain 8m 31s) Loss: 0.0013(0.0083) Grad: 21063.7617  LR: 0.000016  \n","Epoch: [2][1100/2851] Elapsed 5m 4s (remain 8m 3s) Loss: 0.0625(0.0082) Grad: 94448.3906  LR: 0.000016  \n","Epoch: [2][1200/2851] Elapsed 5m 31s (remain 7m 35s) Loss: 0.0000(0.0080) Grad: 58.3542  LR: 0.000016  \n","Epoch: [2][1300/2851] Elapsed 5m 58s (remain 7m 7s) Loss: 0.0117(0.0082) Grad: 37814.5898  LR: 0.000016  \n","Epoch: [2][1400/2851] Elapsed 6m 26s (remain 6m 39s) Loss: 0.0140(0.0081) Grad: 115820.6641  LR: 0.000016  \n","Epoch: [2][1500/2851] Elapsed 6m 53s (remain 6m 11s) Loss: 0.0000(0.0080) Grad: 98.9912  LR: 0.000015  \n","Epoch: [2][1600/2851] Elapsed 7m 20s (remain 5m 44s) Loss: 0.0008(0.0079) Grad: 2902.7471  LR: 0.000015  \n","Epoch: [2][1700/2851] Elapsed 7m 47s (remain 5m 16s) Loss: 0.0027(0.0078) Grad: 6318.1167  LR: 0.000015  \n","Epoch: [2][1800/2851] Elapsed 8m 15s (remain 4m 48s) Loss: 0.0000(0.0077) Grad: 89.4439  LR: 0.000015  \n","Epoch: [2][1900/2851] Elapsed 8m 42s (remain 4m 21s) Loss: 0.0026(0.0077) Grad: 4063.2771  LR: 0.000015  \n","Epoch: [2][2000/2851] Elapsed 9m 10s (remain 3m 53s) Loss: 0.0025(0.0077) Grad: 5858.2412  LR: 0.000015  \n","Epoch: [2][2100/2851] Elapsed 9m 37s (remain 3m 26s) Loss: 0.0000(0.0077) Grad: 135.8919  LR: 0.000015  \n","Epoch: [2][2200/2851] Elapsed 10m 4s (remain 2m 58s) Loss: 0.0026(0.0076) Grad: 9126.0566  LR: 0.000014  \n","Epoch: [2][2300/2851] Elapsed 10m 31s (remain 2m 31s) Loss: 0.0308(0.0075) Grad: 43499.0898  LR: 0.000014  \n","Epoch: [2][2400/2851] Elapsed 10m 59s (remain 2m 3s) Loss: 0.0000(0.0074) Grad: 90.9065  LR: 0.000014  \n","Epoch: [2][2500/2851] Elapsed 11m 26s (remain 1m 36s) Loss: 0.0244(0.0074) Grad: 8296.3232  LR: 0.000014  \n","Epoch: [2][2600/2851] Elapsed 11m 54s (remain 1m 8s) Loss: 0.0003(0.0073) Grad: 3899.1682  LR: 0.000014  \n","Epoch: [2][2700/2851] Elapsed 12m 21s (remain 0m 41s) Loss: 0.0833(0.0073) Grad: 55619.2852  LR: 0.000014  \n","Epoch: [2][2800/2851] Elapsed 12m 48s (remain 0m 13s) Loss: 0.0042(0.0073) Grad: 24140.4590  LR: 0.000013  \n","Epoch: [2][2850/2851] Elapsed 13m 2s (remain 0m 0s) Loss: 0.0000(0.0073) Grad: 150.0847  LR: 0.000013  \n","EVAL: [0/724] Elapsed 0m 0s (remain 5m 58s) Loss: 0.0006(0.0006) \n","EVAL: [100/724] Elapsed 0m 18s (remain 1m 51s) Loss: 0.0021(0.0098) \n","EVAL: [200/724] Elapsed 0m 35s (remain 1m 33s) Loss: 0.0000(0.0114) \n","EVAL: [300/724] Elapsed 0m 53s (remain 1m 14s) Loss: 0.0237(0.0111) \n","EVAL: [400/724] Elapsed 1m 10s (remain 0m 56s) Loss: 0.0000(0.0114) \n","EVAL: [500/724] Elapsed 1m 28s (remain 0m 39s) Loss: 0.0167(0.0126) \n","EVAL: [600/724] Elapsed 1m 45s (remain 0m 21s) Loss: 0.0008(0.0119) \n","EVAL: [700/724] Elapsed 2m 3s (remain 0m 4s) Loss: 0.0000(0.0109) \n","EVAL: [723/724] Elapsed 2m 6s (remain 0m 0s) Loss: 0.0000(0.0107) \n","Epoch 2 - avg_train_loss: 0.0073  avg_val_loss: 0.0107  time: 913s\n","Epoch 2 - Score: 0.8688\n","Epoch 2 - Save Best Score: 0.8688 Model\n","Epoch: [3][0/2851] Elapsed 0m 0s (remain 28m 4s) Loss: 0.0114(0.0114) Grad: 25141.8828  LR: 0.000013  \n","Epoch: [3][100/2851] Elapsed 0m 28s (remain 13m 7s) Loss: 0.0067(0.0067) Grad: 9762.4902  LR: 0.000013  \n","Epoch: [3][200/2851] Elapsed 0m 58s (remain 12m 46s) Loss: 0.0000(0.0055) Grad: 49.7985  LR: 0.000013  \n","Epoch: [3][300/2851] Elapsed 1m 25s (remain 12m 3s) Loss: 0.0041(0.0058) Grad: 12371.1094  LR: 0.000013  \n","Epoch: [3][400/2851] Elapsed 1m 52s (remain 11m 27s) Loss: 0.0126(0.0056) Grad: 53496.5234  LR: 0.000013  \n","Epoch: [3][500/2851] Elapsed 2m 19s (remain 10m 55s) Loss: 0.0001(0.0058) Grad: 548.2571  LR: 0.000013  \n","Epoch: [3][600/2851] Elapsed 2m 47s (remain 10m 26s) Loss: 0.0048(0.0061) Grad: 11576.9219  LR: 0.000012  \n","Epoch: [3][700/2851] Elapsed 3m 14s (remain 9m 56s) Loss: 0.0032(0.0060) Grad: 7546.4961  LR: 0.000012  \n","Epoch: [3][800/2851] Elapsed 3m 41s (remain 9m 27s) Loss: 0.0015(0.0059) Grad: 8461.9727  LR: 0.000012  \n","Epoch: [3][900/2851] Elapsed 4m 8s (remain 8m 58s) Loss: 0.0004(0.0059) Grad: 2335.0098  LR: 0.000012  \n","Epoch: [3][1000/2851] Elapsed 4m 35s (remain 8m 29s) Loss: 0.0079(0.0058) Grad: 60124.6094  LR: 0.000012  \n","Epoch: [3][1100/2851] Elapsed 5m 3s (remain 8m 1s) Loss: 0.0008(0.0058) Grad: 6891.5117  LR: 0.000012  \n","Epoch: [3][1200/2851] Elapsed 5m 30s (remain 7m 33s) Loss: 0.0001(0.0058) Grad: 451.4682  LR: 0.000011  \n","Epoch: [3][1300/2851] Elapsed 5m 57s (remain 7m 6s) Loss: 0.0001(0.0059) Grad: 264.4215  LR: 0.000011  \n","Epoch: [3][1400/2851] Elapsed 6m 24s (remain 6m 38s) Loss: 0.0447(0.0059) Grad: 14656.1738  LR: 0.000011  \n","Epoch: [3][1500/2851] Elapsed 6m 52s (remain 6m 10s) Loss: 0.0014(0.0059) Grad: 3779.3374  LR: 0.000011  \n","Epoch: [3][1600/2851] Elapsed 7m 19s (remain 5m 42s) Loss: 0.0002(0.0058) Grad: 914.3656  LR: 0.000011  \n","Epoch: [3][1700/2851] Elapsed 7m 46s (remain 5m 15s) Loss: 0.0246(0.0058) Grad: 42695.3320  LR: 0.000011  \n","Epoch: [3][1800/2851] Elapsed 8m 13s (remain 4m 47s) Loss: 0.0008(0.0057) Grad: 17064.7285  LR: 0.000011  \n","Epoch: [3][1900/2851] Elapsed 8m 41s (remain 4m 20s) Loss: 0.0066(0.0057) Grad: 11924.6787  LR: 0.000010  \n","Epoch: [3][2000/2851] Elapsed 9m 8s (remain 3m 52s) Loss: 0.0022(0.0058) Grad: 3407.8086  LR: 0.000010  \n","Epoch: [3][2100/2851] Elapsed 9m 35s (remain 3m 25s) Loss: 0.0001(0.0058) Grad: 263.2248  LR: 0.000010  \n","Epoch: [3][2200/2851] Elapsed 10m 2s (remain 2m 58s) Loss: 0.0012(0.0057) Grad: 3894.3384  LR: 0.000010  \n","Epoch: [3][2300/2851] Elapsed 10m 30s (remain 2m 30s) Loss: 0.0002(0.0057) Grad: 648.2369  LR: 0.000010  \n","Epoch: [3][2400/2851] Elapsed 10m 57s (remain 2m 3s) Loss: 0.0009(0.0058) Grad: 9428.7188  LR: 0.000010  \n","Epoch: [3][2500/2851] Elapsed 11m 24s (remain 1m 35s) Loss: 0.0000(0.0058) Grad: 172.7346  LR: 0.000009  \n","Epoch: [3][2600/2851] Elapsed 11m 51s (remain 1m 8s) Loss: 0.0001(0.0058) Grad: 476.9169  LR: 0.000009  \n","Epoch: [3][2700/2851] Elapsed 12m 19s (remain 0m 41s) Loss: 0.0077(0.0058) Grad: 22270.6992  LR: 0.000009  \n","Epoch: [3][2800/2851] Elapsed 12m 46s (remain 0m 13s) Loss: 0.0000(0.0058) Grad: 70.9018  LR: 0.000009  \n","Epoch: [3][2850/2851] Elapsed 13m 0s (remain 0m 0s) Loss: 0.0001(0.0057) Grad: 430.3522  LR: 0.000009  \n","EVAL: [0/724] Elapsed 0m 0s (remain 6m 2s) Loss: 0.0006(0.0006) \n","EVAL: [100/724] Elapsed 0m 18s (remain 1m 51s) Loss: 0.0015(0.0093) \n","EVAL: [200/724] Elapsed 0m 35s (remain 1m 33s) Loss: 0.0000(0.0100) \n","EVAL: [300/724] Elapsed 0m 53s (remain 1m 15s) Loss: 0.0250(0.0099) \n","EVAL: [400/724] Elapsed 1m 10s (remain 0m 57s) Loss: 0.0000(0.0100) \n","EVAL: [500/724] Elapsed 1m 28s (remain 0m 39s) Loss: 0.0209(0.0113) \n","EVAL: [600/724] Elapsed 1m 45s (remain 0m 21s) Loss: 0.0051(0.0106) \n","EVAL: [700/724] Elapsed 2m 3s (remain 0m 4s) Loss: 0.0000(0.0097) \n","EVAL: [723/724] Elapsed 2m 7s (remain 0m 0s) Loss: 0.0000(0.0096) \n","Epoch 3 - avg_train_loss: 0.0057  avg_val_loss: 0.0096  time: 912s\n","Epoch 3 - Score: 0.8792\n","Epoch 3 - Save Best Score: 0.8792 Model\n","Epoch: [4][0/2851] Elapsed 0m 0s (remain 28m 29s) Loss: 0.0001(0.0001) Grad: 638.8223  LR: 0.000009  \n","Epoch: [4][100/2851] Elapsed 0m 29s (remain 13m 14s) Loss: 0.0015(0.0029) Grad: 9192.4619  LR: 0.000009  \n","Epoch: [4][200/2851] Elapsed 0m 57s (remain 12m 42s) Loss: 0.0003(0.0041) Grad: 1141.2418  LR: 0.000009  \n","Epoch: [4][300/2851] Elapsed 1m 25s (remain 12m 2s) Loss: 0.0021(0.0039) Grad: 13435.0381  LR: 0.000008  \n","Epoch: [4][400/2851] Elapsed 1m 52s (remain 11m 27s) Loss: 0.0204(0.0041) Grad: 129778.1328  LR: 0.000008  \n","Epoch: [4][500/2851] Elapsed 2m 19s (remain 10m 55s) Loss: 0.0000(0.0040) Grad: 68.3550  LR: 0.000008  \n","Epoch: [4][600/2851] Elapsed 2m 46s (remain 10m 24s) Loss: 0.0003(0.0044) Grad: 1511.6434  LR: 0.000008  \n","Epoch: [4][700/2851] Elapsed 3m 14s (remain 9m 55s) Loss: 0.0000(0.0042) Grad: 54.2872  LR: 0.000008  \n","Epoch: [4][800/2851] Elapsed 3m 41s (remain 9m 26s) Loss: 0.0001(0.0042) Grad: 233.8623  LR: 0.000008  \n","Epoch: [4][900/2851] Elapsed 4m 8s (remain 8m 58s) Loss: 0.0000(0.0043) Grad: 142.4231  LR: 0.000007  \n","Epoch: [4][1000/2851] Elapsed 4m 36s (remain 8m 30s) Loss: 0.0013(0.0044) Grad: 4000.3083  LR: 0.000007  \n","Epoch: [4][1100/2851] Elapsed 5m 3s (remain 8m 2s) Loss: 0.0031(0.0046) Grad: 6481.0601  LR: 0.000007  \n","Epoch: [4][1200/2851] Elapsed 5m 30s (remain 7m 34s) Loss: 0.0000(0.0047) Grad: 226.7188  LR: 0.000007  \n","Epoch: [4][1300/2851] Elapsed 5m 58s (remain 7m 6s) Loss: 0.0045(0.0048) Grad: 9934.4092  LR: 0.000007  \n","Epoch: [4][1400/2851] Elapsed 6m 25s (remain 6m 38s) Loss: 0.0002(0.0048) Grad: 1335.7296  LR: 0.000007  \n","Epoch: [4][1500/2851] Elapsed 6m 52s (remain 6m 11s) Loss: 0.0005(0.0048) Grad: 2103.2109  LR: 0.000007  \n","Epoch: [4][1600/2851] Elapsed 7m 19s (remain 5m 43s) Loss: 0.0111(0.0048) Grad: 24581.5332  LR: 0.000006  \n","Epoch: [4][1700/2851] Elapsed 7m 47s (remain 5m 15s) Loss: 0.0001(0.0047) Grad: 346.2092  LR: 0.000006  \n","Epoch: [4][1800/2851] Elapsed 8m 14s (remain 4m 48s) Loss: 0.0007(0.0047) Grad: 2962.1426  LR: 0.000006  \n","Epoch: [4][1900/2851] Elapsed 8m 41s (remain 4m 20s) Loss: 0.0000(0.0047) Grad: 25.4320  LR: 0.000006  \n","Epoch: [4][2000/2851] Elapsed 9m 8s (remain 3m 53s) Loss: 0.0237(0.0047) Grad: 26441.0742  LR: 0.000006  \n","Epoch: [4][2100/2851] Elapsed 9m 36s (remain 3m 25s) Loss: 0.0000(0.0047) Grad: 64.1453  LR: 0.000006  \n","Epoch: [4][2200/2851] Elapsed 10m 3s (remain 2m 58s) Loss: 0.0001(0.0047) Grad: 3797.3154  LR: 0.000005  \n","Epoch: [4][2300/2851] Elapsed 10m 30s (remain 2m 30s) Loss: 0.0001(0.0048) Grad: 920.4420  LR: 0.000005  \n","Epoch: [4][2400/2851] Elapsed 10m 58s (remain 2m 3s) Loss: 0.0001(0.0048) Grad: 3984.0322  LR: 0.000005  \n","Epoch: [4][2500/2851] Elapsed 11m 25s (remain 1m 35s) Loss: 0.0004(0.0047) Grad: 2910.4631  LR: 0.000005  \n","Epoch: [4][2600/2851] Elapsed 11m 53s (remain 1m 8s) Loss: 0.0077(0.0047) Grad: 11026.7939  LR: 0.000005  \n","Epoch: [4][2700/2851] Elapsed 12m 20s (remain 0m 41s) Loss: 0.0000(0.0047) Grad: 210.7311  LR: 0.000005  \n","Epoch: [4][2800/2851] Elapsed 12m 47s (remain 0m 13s) Loss: 0.0000(0.0047) Grad: 374.0531  LR: 0.000005  \n","Epoch: [4][2850/2851] Elapsed 13m 1s (remain 0m 0s) Loss: 0.0001(0.0047) Grad: 752.7667  LR: 0.000004  \n","EVAL: [0/724] Elapsed 0m 0s (remain 5m 54s) Loss: 0.0006(0.0006) \n","EVAL: [100/724] Elapsed 0m 18s (remain 1m 51s) Loss: 0.0036(0.0108) \n","EVAL: [200/724] Elapsed 0m 35s (remain 1m 33s) Loss: 0.0000(0.0106) \n","EVAL: [300/724] Elapsed 0m 53s (remain 1m 14s) Loss: 0.0268(0.0107) \n","EVAL: [400/724] Elapsed 1m 10s (remain 0m 57s) Loss: 0.0000(0.0111) \n","EVAL: [500/724] Elapsed 1m 28s (remain 0m 39s) Loss: 0.0204(0.0125) \n","EVAL: [600/724] Elapsed 1m 46s (remain 0m 21s) Loss: 0.0017(0.0118) \n","EVAL: [700/724] Elapsed 2m 4s (remain 0m 4s) Loss: 0.0000(0.0108) \n","EVAL: [723/724] Elapsed 2m 7s (remain 0m 0s) Loss: 0.0000(0.0107) \n","Epoch 4 - avg_train_loss: 0.0047  avg_val_loss: 0.0107  time: 913s\n","Epoch 4 - Score: 0.8819\n","Epoch 4 - Save Best Score: 0.8819 Model\n","Epoch: [5][0/2851] Elapsed 0m 0s (remain 29m 33s) Loss: 0.0026(0.0026) Grad: 17122.0664  LR: 0.000004  \n","Epoch: [5][100/2851] Elapsed 0m 29s (remain 13m 36s) Loss: 0.0032(0.0036) Grad: 7918.0625  LR: 0.000004  \n","Epoch: [5][200/2851] Elapsed 0m 59s (remain 12m 58s) Loss: 0.0000(0.0046) Grad: 25.6839  LR: 0.000004  \n","Epoch: [5][300/2851] Elapsed 1m 26s (remain 12m 12s) Loss: 0.0001(0.0039) Grad: 269.0901  LR: 0.000004  \n","Epoch: [5][400/2851] Elapsed 1m 53s (remain 11m 35s) Loss: 0.0013(0.0038) Grad: 10045.0166  LR: 0.000004  \n","Epoch: [5][500/2851] Elapsed 2m 21s (remain 11m 2s) Loss: 0.0000(0.0041) Grad: 137.6684  LR: 0.000004  \n","Epoch: [5][600/2851] Elapsed 2m 48s (remain 10m 31s) Loss: 0.0098(0.0039) Grad: 26585.4414  LR: 0.000004  \n","Epoch: [5][700/2851] Elapsed 3m 16s (remain 10m 1s) Loss: 0.0019(0.0038) Grad: 48538.5469  LR: 0.000003  \n","Epoch: [5][800/2851] Elapsed 3m 43s (remain 9m 31s) Loss: 0.0036(0.0036) Grad: 5818.0166  LR: 0.000003  \n","Epoch: [5][900/2851] Elapsed 4m 10s (remain 9m 2s) Loss: 0.0017(0.0037) Grad: 8699.4053  LR: 0.000003  \n","Epoch: [5][1000/2851] Elapsed 4m 38s (remain 8m 33s) Loss: 0.0019(0.0036) Grad: 12089.2969  LR: 0.000003  \n","Epoch: [5][1100/2851] Elapsed 5m 5s (remain 8m 5s) Loss: 0.0007(0.0036) Grad: 5044.0425  LR: 0.000003  \n","Epoch: [5][1200/2851] Elapsed 5m 33s (remain 7m 37s) Loss: 0.0000(0.0035) Grad: 9.9836  LR: 0.000003  \n","Epoch: [5][1300/2851] Elapsed 6m 0s (remain 7m 9s) Loss: 0.0002(0.0035) Grad: 3660.1575  LR: 0.000002  \n","Epoch: [5][1400/2851] Elapsed 6m 27s (remain 6m 41s) Loss: 0.0001(0.0036) Grad: 572.8251  LR: 0.000002  \n","Epoch: [5][1500/2851] Elapsed 6m 55s (remain 6m 13s) Loss: 0.0000(0.0036) Grad: 169.3833  LR: 0.000002  \n","Epoch: [5][1600/2851] Elapsed 7m 22s (remain 5m 45s) Loss: 0.0000(0.0037) Grad: 51.1350  LR: 0.000002  \n","Epoch: [5][1700/2851] Elapsed 7m 49s (remain 5m 17s) Loss: 0.0010(0.0037) Grad: 4979.5415  LR: 0.000002  \n","Epoch: [5][1800/2851] Elapsed 8m 17s (remain 4m 50s) Loss: 0.0001(0.0037) Grad: 204.3270  LR: 0.000002  \n","Epoch: [5][1900/2851] Elapsed 8m 44s (remain 4m 22s) Loss: 0.0054(0.0037) Grad: 12054.1123  LR: 0.000001  \n","Epoch: [5][2000/2851] Elapsed 9m 12s (remain 3m 54s) Loss: 0.0000(0.0037) Grad: 130.0967  LR: 0.000001  \n","Epoch: [5][2100/2851] Elapsed 9m 39s (remain 3m 26s) Loss: 0.0000(0.0037) Grad: 44.2957  LR: 0.000001  \n","Epoch: [5][2200/2851] Elapsed 10m 6s (remain 2m 59s) Loss: 0.0052(0.0038) Grad: 34342.0859  LR: 0.000001  \n","Epoch: [5][2300/2851] Elapsed 10m 34s (remain 2m 31s) Loss: 0.0000(0.0039) Grad: 68.7698  LR: 0.000001  \n","Epoch: [5][2400/2851] Elapsed 11m 1s (remain 2m 4s) Loss: 0.0001(0.0038) Grad: 636.3171  LR: 0.000001  \n","Epoch: [5][2500/2851] Elapsed 11m 29s (remain 1m 36s) Loss: 0.0000(0.0038) Grad: 147.0654  LR: 0.000001  \n","Epoch: [5][2600/2851] Elapsed 11m 57s (remain 1m 8s) Loss: 0.0009(0.0038) Grad: 5213.1064  LR: 0.000000  \n","Epoch: [5][2700/2851] Elapsed 12m 24s (remain 0m 41s) Loss: 0.0399(0.0039) Grad: 20700.6113  LR: 0.000000  \n","Epoch: [5][2800/2851] Elapsed 12m 52s (remain 0m 13s) Loss: 0.0000(0.0039) Grad: 95.2122  LR: 0.000000  \n","Epoch: [5][2850/2851] Elapsed 13m 5s (remain 0m 0s) Loss: 0.0000(0.0039) Grad: 38.5166  LR: 0.000000  \n","EVAL: [0/724] Elapsed 0m 0s (remain 5m 48s) Loss: 0.0008(0.0008) \n","EVAL: [100/724] Elapsed 0m 18s (remain 1m 52s) Loss: 0.0037(0.0117) \n","EVAL: [200/724] Elapsed 0m 36s (remain 1m 34s) Loss: 0.0000(0.0117) \n","EVAL: [300/724] Elapsed 0m 53s (remain 1m 15s) Loss: 0.0190(0.0116) \n","EVAL: [400/724] Elapsed 1m 11s (remain 0m 57s) Loss: 0.0000(0.0118) \n","EVAL: [500/724] Elapsed 1m 29s (remain 0m 39s) Loss: 0.0241(0.0134) \n","EVAL: [600/724] Elapsed 1m 47s (remain 0m 21s) Loss: 0.0043(0.0125) \n","EVAL: [700/724] Elapsed 2m 4s (remain 0m 4s) Loss: 0.0000(0.0115) \n","EVAL: [723/724] Elapsed 2m 8s (remain 0m 0s) Loss: 0.0000(0.0114) \n","Epoch 5 - avg_train_loss: 0.0039  avg_val_loss: 0.0114  time: 919s\n","Epoch 5 - Score: 0.8811\n","========== fold: 2 training ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n","Epoch: [1][0/2871] Elapsed 0m 0s (remain 32m 23s) Loss: 0.6295(0.6295) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/2871] Elapsed 0m 28s (remain 12m 55s) Loss: 0.3247(0.4019) Grad: 32686.1035  LR: 0.000001  \n","Epoch: [1][200/2871] Elapsed 0m 56s (remain 12m 23s) Loss: 0.0313(0.2435) Grad: 720.5998  LR: 0.000003  \n","Epoch: [1][300/2871] Elapsed 1m 23s (remain 11m 54s) Loss: 0.0544(0.1772) Grad: 1523.7477  LR: 0.000004  \n","Epoch: [1][400/2871] Elapsed 1m 51s (remain 11m 26s) Loss: 0.0184(0.1402) Grad: 1392.6342  LR: 0.000006  \n","Epoch: [1][500/2871] Elapsed 2m 19s (remain 10m 58s) Loss: 0.0261(0.1154) Grad: 1428.7633  LR: 0.000007  \n","Epoch: [1][600/2871] Elapsed 2m 47s (remain 10m 34s) Loss: 0.0084(0.0990) Grad: 2160.4578  LR: 0.000008  \n","Epoch: [1][700/2871] Elapsed 3m 15s (remain 10m 5s) Loss: 0.0171(0.0877) Grad: 3584.4790  LR: 0.000010  \n","Epoch: [1][800/2871] Elapsed 3m 43s (remain 9m 36s) Loss: 0.0037(0.0787) Grad: 1740.1102  LR: 0.000011  \n","Epoch: [1][900/2871] Elapsed 4m 10s (remain 9m 8s) Loss: 0.0069(0.0715) Grad: 7683.8223  LR: 0.000013  \n","Epoch: [1][1000/2871] Elapsed 4m 38s (remain 8m 40s) Loss: 0.0047(0.0657) Grad: 717.2587  LR: 0.000014  \n","Epoch: [1][1100/2871] Elapsed 5m 6s (remain 8m 12s) Loss: 0.0182(0.0608) Grad: 8407.3750  LR: 0.000015  \n","Epoch: [1][1200/2871] Elapsed 5m 33s (remain 7m 44s) Loss: 0.0028(0.0566) Grad: 460.9018  LR: 0.000017  \n","Epoch: [1][1300/2871] Elapsed 6m 1s (remain 7m 16s) Loss: 0.0051(0.0531) Grad: 700.0232  LR: 0.000018  \n","Epoch: [1][1400/2871] Elapsed 6m 29s (remain 6m 48s) Loss: 0.0018(0.0502) Grad: 240.8737  LR: 0.000020  \n","Epoch: [1][1500/2871] Elapsed 6m 56s (remain 6m 20s) Loss: 0.0611(0.0477) Grad: 12339.9463  LR: 0.000020  \n","Epoch: [1][1600/2871] Elapsed 7m 24s (remain 5m 52s) Loss: 0.0109(0.0454) Grad: 1084.1603  LR: 0.000020  \n","Epoch: [1][1700/2871] Elapsed 7m 52s (remain 5m 24s) Loss: 0.0417(0.0434) Grad: 4377.3311  LR: 0.000020  \n","Epoch: [1][1800/2871] Elapsed 8m 20s (remain 4m 57s) Loss: 0.0198(0.0415) Grad: 1512.6030  LR: 0.000019  \n","Epoch: [1][1900/2871] Elapsed 8m 47s (remain 4m 29s) Loss: 0.0074(0.0400) Grad: 432.1106  LR: 0.000019  \n","Epoch: [1][2000/2871] Elapsed 9m 15s (remain 4m 1s) Loss: 0.0045(0.0386) Grad: 1034.7826  LR: 0.000019  \n","Epoch: [1][2100/2871] Elapsed 9m 43s (remain 3m 33s) Loss: 0.0205(0.0373) Grad: 2685.2522  LR: 0.000019  \n","Epoch: [1][2200/2871] Elapsed 10m 10s (remain 3m 5s) Loss: 0.0110(0.0360) Grad: 702.3373  LR: 0.000019  \n","Epoch: [1][2300/2871] Elapsed 10m 38s (remain 2m 38s) Loss: 0.0148(0.0349) Grad: 1911.1053  LR: 0.000019  \n","Epoch: [1][2400/2871] Elapsed 11m 6s (remain 2m 10s) Loss: 0.0050(0.0338) Grad: 648.1485  LR: 0.000019  \n","Epoch: [1][2500/2871] Elapsed 11m 33s (remain 1m 42s) Loss: 0.0009(0.0327) Grad: 266.4473  LR: 0.000018  \n","Epoch: [1][2600/2871] Elapsed 12m 1s (remain 1m 14s) Loss: 0.0023(0.0318) Grad: 609.7098  LR: 0.000018  \n","Epoch: [1][2700/2871] Elapsed 12m 28s (remain 0m 47s) Loss: 0.0006(0.0309) Grad: 112.6430  LR: 0.000018  \n","Epoch: [1][2800/2871] Elapsed 12m 56s (remain 0m 19s) Loss: 0.0088(0.0302) Grad: 919.2103  LR: 0.000018  \n","Epoch: [1][2870/2871] Elapsed 13m 15s (remain 0m 0s) Loss: 0.0292(0.0297) Grad: 8248.8467  LR: 0.000018  \n","EVAL: [0/704] Elapsed 0m 0s (remain 6m 45s) Loss: 0.0030(0.0030) \n","EVAL: [100/704] Elapsed 0m 18s (remain 1m 49s) Loss: 0.0052(0.0069) \n","EVAL: [200/704] Elapsed 0m 36s (remain 1m 30s) Loss: 0.0002(0.0077) \n","EVAL: [300/704] Elapsed 0m 53s (remain 1m 11s) Loss: 0.0007(0.0072) \n","EVAL: [400/704] Elapsed 1m 11s (remain 0m 53s) Loss: 0.0044(0.0082) \n","EVAL: [500/704] Elapsed 1m 29s (remain 0m 36s) Loss: 0.0042(0.0087) \n","EVAL: [600/704] Elapsed 1m 46s (remain 0m 18s) Loss: 0.0008(0.0089) \n","EVAL: [700/704] Elapsed 2m 4s (remain 0m 0s) Loss: 0.0008(0.0084) \n","EVAL: [703/704] Elapsed 2m 4s (remain 0m 0s) Loss: 0.0001(0.0083) \n","Epoch 1 - avg_train_loss: 0.0297  avg_val_loss: 0.0083  time: 925s\n","Epoch 1 - Score: 0.8489\n","Epoch 1 - Save Best Score: 0.8489 Model\n","Epoch: [2][0/2871] Elapsed 0m 0s (remain 35m 28s) Loss: 0.0008(0.0008) Grad: 2421.1860  LR: 0.000018  \n","Epoch: [2][100/2871] Elapsed 0m 30s (remain 14m 8s) Loss: 0.0112(0.0071) Grad: 22298.8750  LR: 0.000018  \n","Epoch: [2][200/2871] Elapsed 1m 0s (remain 13m 21s) Loss: 0.0016(0.0070) Grad: 5652.0156  LR: 0.000017  \n","Epoch: [2][300/2871] Elapsed 1m 28s (remain 12m 34s) Loss: 0.0064(0.0064) Grad: 9032.6094  LR: 0.000017  \n","Epoch: [2][400/2871] Elapsed 1m 56s (remain 11m 55s) Loss: 0.0123(0.0063) Grad: 12697.6670  LR: 0.000017  \n","Epoch: [2][500/2871] Elapsed 2m 24s (remain 11m 21s) Loss: 0.0168(0.0065) Grad: 19863.9766  LR: 0.000017  \n","Epoch: [2][600/2871] Elapsed 2m 51s (remain 10m 48s) Loss: 0.0368(0.0067) Grad: 25172.7695  LR: 0.000017  \n","Epoch: [2][700/2871] Elapsed 3m 19s (remain 10m 17s) Loss: 0.0112(0.0067) Grad: 14590.0781  LR: 0.000017  \n","Epoch: [2][800/2871] Elapsed 3m 47s (remain 9m 47s) Loss: 0.0038(0.0067) Grad: 2654.1121  LR: 0.000017  \n","Epoch: [2][900/2871] Elapsed 4m 15s (remain 9m 18s) Loss: 0.0033(0.0065) Grad: 3639.8062  LR: 0.000016  \n","Epoch: [2][1000/2871] Elapsed 4m 43s (remain 8m 48s) Loss: 0.0066(0.0066) Grad: 12207.2266  LR: 0.000016  \n","Epoch: [2][1100/2871] Elapsed 5m 10s (remain 8m 19s) Loss: 0.0058(0.0066) Grad: 14848.8066  LR: 0.000016  \n","Epoch: [2][1200/2871] Elapsed 5m 38s (remain 7m 50s) Loss: 0.0096(0.0066) Grad: 6796.6909  LR: 0.000016  \n","Epoch: [2][1300/2871] Elapsed 6m 5s (remain 7m 21s) Loss: 0.0004(0.0067) Grad: 1642.6417  LR: 0.000016  \n","Epoch: [2][1400/2871] Elapsed 6m 33s (remain 6m 53s) Loss: 0.0001(0.0067) Grad: 383.5661  LR: 0.000016  \n","Epoch: [2][1500/2871] Elapsed 7m 1s (remain 6m 24s) Loss: 0.0089(0.0069) Grad: 144256.0156  LR: 0.000015  \n","Epoch: [2][1600/2871] Elapsed 7m 29s (remain 5m 56s) Loss: 0.0018(0.0071) Grad: 6361.9429  LR: 0.000015  \n","Epoch: [2][1700/2871] Elapsed 7m 56s (remain 5m 27s) Loss: 0.0043(0.0071) Grad: 8454.2598  LR: 0.000015  \n","Epoch: [2][1800/2871] Elapsed 8m 24s (remain 4m 59s) Loss: 0.0215(0.0072) Grad: 25610.3848  LR: 0.000015  \n","Epoch: [2][1900/2871] Elapsed 8m 52s (remain 4m 31s) Loss: 0.0007(0.0071) Grad: 4090.0237  LR: 0.000015  \n","Epoch: [2][2000/2871] Elapsed 9m 19s (remain 4m 3s) Loss: 0.0026(0.0072) Grad: 17562.8887  LR: 0.000015  \n","Epoch: [2][2100/2871] Elapsed 9m 47s (remain 3m 35s) Loss: 0.0003(0.0072) Grad: 946.0994  LR: 0.000015  \n","Epoch: [2][2200/2871] Elapsed 10m 14s (remain 3m 7s) Loss: 0.0004(0.0072) Grad: 2215.6958  LR: 0.000014  \n","Epoch: [2][2300/2871] Elapsed 10m 42s (remain 2m 39s) Loss: 0.0057(0.0072) Grad: 5513.7329  LR: 0.000014  \n","Epoch: [2][2400/2871] Elapsed 11m 10s (remain 2m 11s) Loss: 0.0002(0.0071) Grad: 1522.3755  LR: 0.000014  \n","Epoch: [2][2500/2871] Elapsed 11m 37s (remain 1m 43s) Loss: 0.0209(0.0070) Grad: 76537.8438  LR: 0.000014  \n","Epoch: [2][2600/2871] Elapsed 12m 5s (remain 1m 15s) Loss: 0.0000(0.0070) Grad: 221.7653  LR: 0.000014  \n","Epoch: [2][2700/2871] Elapsed 12m 33s (remain 0m 47s) Loss: 0.0063(0.0070) Grad: 24073.7852  LR: 0.000014  \n","Epoch: [2][2800/2871] Elapsed 13m 1s (remain 0m 19s) Loss: 0.0267(0.0069) Grad: 38140.8672  LR: 0.000013  \n","Epoch: [2][2870/2871] Elapsed 13m 20s (remain 0m 0s) Loss: 0.0353(0.0069) Grad: 103805.5703  LR: 0.000013  \n","EVAL: [0/704] Elapsed 0m 0s (remain 6m 26s) Loss: 0.0023(0.0023) \n","EVAL: [100/704] Elapsed 0m 18s (remain 1m 49s) Loss: 0.0009(0.0092) \n","EVAL: [200/704] Elapsed 0m 36s (remain 1m 30s) Loss: 0.0000(0.0092) \n","EVAL: [300/704] Elapsed 0m 54s (remain 1m 12s) Loss: 0.0001(0.0079) \n","EVAL: [400/704] Elapsed 1m 11s (remain 0m 54s) Loss: 0.0068(0.0091) \n","EVAL: [500/704] Elapsed 1m 29s (remain 0m 36s) Loss: 0.0107(0.0099) \n","EVAL: [600/704] Elapsed 1m 47s (remain 0m 18s) Loss: 0.0001(0.0103) \n","EVAL: [700/704] Elapsed 2m 5s (remain 0m 0s) Loss: 0.0000(0.0096) \n","EVAL: [703/704] Elapsed 2m 5s (remain 0m 0s) Loss: 0.0000(0.0095) \n","Epoch 2 - avg_train_loss: 0.0069  avg_val_loss: 0.0095  time: 930s\n","Epoch 2 - Score: 0.8668\n","Epoch 2 - Save Best Score: 0.8668 Model\n","Epoch: [3][0/2871] Elapsed 0m 0s (remain 29m 53s) Loss: 0.0003(0.0003) Grad: 1771.1228  LR: 0.000013  \n","Epoch: [3][100/2871] Elapsed 0m 30s (remain 13m 50s) Loss: 0.0560(0.0056) Grad: 280855.9062  LR: 0.000013  \n","Epoch: [3][200/2871] Elapsed 0m 59s (remain 13m 10s) Loss: 0.0035(0.0058) Grad: 19948.1465  LR: 0.000013  \n","Epoch: [3][300/2871] Elapsed 1m 27s (remain 12m 23s) Loss: 0.0000(0.0054) Grad: 93.3687  LR: 0.000013  \n","Epoch: [3][400/2871] Elapsed 1m 54s (remain 11m 46s) Loss: 0.0001(0.0056) Grad: 454.8390  LR: 0.000013  \n","Epoch: [3][500/2871] Elapsed 2m 22s (remain 11m 13s) Loss: 0.0001(0.0052) Grad: 237.9449  LR: 0.000013  \n","Epoch: [3][600/2871] Elapsed 2m 50s (remain 10m 42s) Loss: 0.0000(0.0055) Grad: 29.2055  LR: 0.000012  \n","Epoch: [3][700/2871] Elapsed 3m 17s (remain 10m 12s) Loss: 0.0017(0.0054) Grad: 5334.4668  LR: 0.000012  \n","Epoch: [3][800/2871] Elapsed 3m 45s (remain 9m 43s) Loss: 0.0009(0.0056) Grad: 4174.5679  LR: 0.000012  \n","Epoch: [3][900/2871] Elapsed 4m 13s (remain 9m 13s) Loss: 0.0001(0.0055) Grad: 2380.2148  LR: 0.000012  \n","Epoch: [3][1000/2871] Elapsed 4m 41s (remain 8m 45s) Loss: 0.0172(0.0056) Grad: 168135.2812  LR: 0.000012  \n","Epoch: [3][1100/2871] Elapsed 5m 8s (remain 8m 16s) Loss: 0.0011(0.0056) Grad: 2890.4019  LR: 0.000012  \n","Epoch: [3][1200/2871] Elapsed 5m 36s (remain 7m 48s) Loss: 0.0019(0.0057) Grad: 18343.6152  LR: 0.000011  \n","Epoch: [3][1300/2871] Elapsed 6m 4s (remain 7m 19s) Loss: 0.0013(0.0056) Grad: 7735.8799  LR: 0.000011  \n","Epoch: [3][1400/2871] Elapsed 6m 31s (remain 6m 51s) Loss: 0.0017(0.0057) Grad: 9157.7676  LR: 0.000011  \n","Epoch: [3][1500/2871] Elapsed 6m 59s (remain 6m 23s) Loss: 0.0000(0.0057) Grad: 12.8362  LR: 0.000011  \n","Epoch: [3][1600/2871] Elapsed 7m 27s (remain 5m 54s) Loss: 0.0005(0.0057) Grad: 6908.3613  LR: 0.000011  \n","Epoch: [3][1700/2871] Elapsed 7m 55s (remain 5m 26s) Loss: 0.0041(0.0057) Grad: 10198.7754  LR: 0.000011  \n","Epoch: [3][1800/2871] Elapsed 8m 22s (remain 4m 58s) Loss: 0.0004(0.0057) Grad: 1696.5233  LR: 0.000011  \n","Epoch: [3][1900/2871] Elapsed 8m 50s (remain 4m 30s) Loss: 0.0002(0.0057) Grad: 655.4191  LR: 0.000010  \n","Epoch: [3][2000/2871] Elapsed 9m 18s (remain 4m 2s) Loss: 0.0011(0.0057) Grad: 6025.6680  LR: 0.000010  \n","Epoch: [3][2100/2871] Elapsed 9m 45s (remain 3m 34s) Loss: 0.0028(0.0058) Grad: 7325.2280  LR: 0.000010  \n","Epoch: [3][2200/2871] Elapsed 10m 12s (remain 3m 6s) Loss: 0.0056(0.0058) Grad: 6960.3760  LR: 0.000010  \n","Epoch: [3][2300/2871] Elapsed 10m 40s (remain 2m 38s) Loss: 0.0007(0.0058) Grad: 3426.7761  LR: 0.000010  \n","Epoch: [3][2400/2871] Elapsed 11m 8s (remain 2m 10s) Loss: 0.0051(0.0059) Grad: 14591.3184  LR: 0.000010  \n","Epoch: [3][2500/2871] Elapsed 11m 35s (remain 1m 42s) Loss: 0.0000(0.0058) Grad: 134.3880  LR: 0.000009  \n","Epoch: [3][2600/2871] Elapsed 12m 3s (remain 1m 15s) Loss: 0.0026(0.0059) Grad: 3725.5916  LR: 0.000009  \n","Epoch: [3][2700/2871] Elapsed 12m 30s (remain 0m 47s) Loss: 0.0001(0.0060) Grad: 434.8977  LR: 0.000009  \n","Epoch: [3][2800/2871] Elapsed 12m 58s (remain 0m 19s) Loss: 0.0143(0.0060) Grad: 25853.3652  LR: 0.000009  \n","Epoch: [3][2870/2871] Elapsed 13m 17s (remain 0m 0s) Loss: 0.0001(0.0060) Grad: 238.2849  LR: 0.000009  \n","EVAL: [0/704] Elapsed 0m 0s (remain 6m 6s) Loss: 0.0026(0.0026) \n","EVAL: [100/704] Elapsed 0m 18s (remain 1m 49s) Loss: 0.0037(0.0098) \n","EVAL: [200/704] Elapsed 0m 35s (remain 1m 30s) Loss: 0.0000(0.0098) \n","EVAL: [300/704] Elapsed 0m 53s (remain 1m 11s) Loss: 0.0001(0.0086) \n","EVAL: [400/704] Elapsed 1m 11s (remain 0m 53s) Loss: 0.0043(0.0093) \n","EVAL: [500/704] Elapsed 1m 28s (remain 0m 36s) Loss: 0.0077(0.0099) \n","EVAL: [600/704] Elapsed 1m 46s (remain 0m 18s) Loss: 0.0001(0.0101) \n","EVAL: [700/704] Elapsed 2m 4s (remain 0m 0s) Loss: 0.0000(0.0093) \n","EVAL: [703/704] Elapsed 2m 4s (remain 0m 0s) Loss: 0.0000(0.0093) \n","Epoch 3 - avg_train_loss: 0.0060  avg_val_loss: 0.0093  time: 927s\n","Epoch 3 - Score: 0.8723\n","Epoch 3 - Save Best Score: 0.8723 Model\n","Epoch: [4][0/2871] Elapsed 0m 0s (remain 31m 42s) Loss: 0.0000(0.0000) Grad: 280.8736  LR: 0.000009  \n","Epoch: [4][100/2871] Elapsed 0m 30s (remain 14m 6s) Loss: 0.0285(0.0070) Grad: 51072.8594  LR: 0.000009  \n","Epoch: [4][200/2871] Elapsed 1m 0s (remain 13m 19s) Loss: 0.0000(0.0052) Grad: 80.2661  LR: 0.000009  \n","Epoch: [4][300/2871] Elapsed 1m 27s (remain 12m 30s) Loss: 0.0054(0.0051) Grad: 14824.5664  LR: 0.000008  \n","Epoch: [4][400/2871] Elapsed 1m 55s (remain 11m 51s) Loss: 0.0000(0.0047) Grad: 33.0985  LR: 0.000008  \n","Epoch: [4][500/2871] Elapsed 2m 23s (remain 11m 16s) Loss: 0.0061(0.0045) Grad: 17037.2285  LR: 0.000008  \n","Epoch: [4][600/2871] Elapsed 2m 50s (remain 10m 44s) Loss: 0.0011(0.0046) Grad: 8314.2061  LR: 0.000008  \n","Epoch: [4][700/2871] Elapsed 3m 18s (remain 10m 13s) Loss: 0.0044(0.0045) Grad: 7349.3867  LR: 0.000008  \n","Epoch: [4][800/2871] Elapsed 3m 45s (remain 9m 43s) Loss: 0.0008(0.0047) Grad: 4853.9883  LR: 0.000008  \n","Epoch: [4][900/2871] Elapsed 4m 13s (remain 9m 14s) Loss: 0.0001(0.0047) Grad: 507.1894  LR: 0.000007  \n","Epoch: [4][1000/2871] Elapsed 4m 40s (remain 8m 44s) Loss: 0.0001(0.0046) Grad: 389.4208  LR: 0.000007  \n","Epoch: [4][1100/2871] Elapsed 5m 8s (remain 8m 15s) Loss: 0.0120(0.0045) Grad: 72031.0938  LR: 0.000007  \n","Epoch: [4][1200/2871] Elapsed 5m 35s (remain 7m 47s) Loss: 0.0000(0.0046) Grad: 40.3346  LR: 0.000007  \n","Epoch: [4][1300/2871] Elapsed 6m 3s (remain 7m 18s) Loss: 0.0094(0.0047) Grad: 25749.9551  LR: 0.000007  \n","Epoch: [4][1400/2871] Elapsed 6m 31s (remain 6m 50s) Loss: 0.0000(0.0047) Grad: 51.8587  LR: 0.000007  \n","Epoch: [4][1500/2871] Elapsed 6m 58s (remain 6m 22s) Loss: 0.0018(0.0046) Grad: 6656.0244  LR: 0.000007  \n","Epoch: [4][1600/2871] Elapsed 7m 26s (remain 5m 54s) Loss: 0.0023(0.0046) Grad: 4186.6587  LR: 0.000006  \n","Epoch: [4][1700/2871] Elapsed 7m 53s (remain 5m 25s) Loss: 0.0097(0.0046) Grad: 6853.2671  LR: 0.000006  \n","Epoch: [4][1800/2871] Elapsed 8m 21s (remain 4m 57s) Loss: 0.0075(0.0047) Grad: 7630.2729  LR: 0.000006  \n","Epoch: [4][1900/2871] Elapsed 8m 48s (remain 4m 29s) Loss: 0.0000(0.0046) Grad: 255.9449  LR: 0.000006  \n","Epoch: [4][2000/2871] Elapsed 9m 16s (remain 4m 1s) Loss: 0.0001(0.0046) Grad: 851.2552  LR: 0.000006  \n","Epoch: [4][2100/2871] Elapsed 9m 44s (remain 3m 34s) Loss: 0.0031(0.0046) Grad: 11419.6553  LR: 0.000006  \n","Epoch: [4][2200/2871] Elapsed 10m 11s (remain 3m 6s) Loss: 0.0103(0.0047) Grad: 72372.7109  LR: 0.000005  \n","Epoch: [4][2300/2871] Elapsed 10m 39s (remain 2m 38s) Loss: 0.0000(0.0047) Grad: 119.5378  LR: 0.000005  \n","Epoch: [4][2400/2871] Elapsed 11m 6s (remain 2m 10s) Loss: 0.0012(0.0047) Grad: 10444.3926  LR: 0.000005  \n","Epoch: [4][2500/2871] Elapsed 11m 34s (remain 1m 42s) Loss: 0.0004(0.0048) Grad: 7690.1689  LR: 0.000005  \n","Epoch: [4][2600/2871] Elapsed 12m 1s (remain 1m 14s) Loss: 0.0024(0.0047) Grad: 7064.7051  LR: 0.000005  \n","Epoch: [4][2700/2871] Elapsed 12m 29s (remain 0m 47s) Loss: 0.0001(0.0047) Grad: 1666.5427  LR: 0.000005  \n","Epoch: [4][2800/2871] Elapsed 12m 57s (remain 0m 19s) Loss: 0.0147(0.0047) Grad: 30151.5098  LR: 0.000005  \n","Epoch: [4][2870/2871] Elapsed 13m 16s (remain 0m 0s) Loss: 0.0001(0.0047) Grad: 786.7723  LR: 0.000004  \n","EVAL: [0/704] Elapsed 0m 0s (remain 6m 5s) Loss: 0.0023(0.0023) \n","EVAL: [100/704] Elapsed 0m 18s (remain 1m 49s) Loss: 0.0017(0.0094) \n","EVAL: [200/704] Elapsed 0m 35s (remain 1m 30s) Loss: 0.0000(0.0095) \n","EVAL: [300/704] Elapsed 0m 53s (remain 1m 11s) Loss: 0.0000(0.0083) \n","EVAL: [400/704] Elapsed 1m 11s (remain 0m 53s) Loss: 0.0076(0.0095) \n","EVAL: [500/704] Elapsed 1m 28s (remain 0m 35s) Loss: 0.0090(0.0101) \n","EVAL: [600/704] Elapsed 1m 46s (remain 0m 18s) Loss: 0.0000(0.0105) \n","EVAL: [700/704] Elapsed 2m 4s (remain 0m 0s) Loss: 0.0000(0.0098) \n","EVAL: [703/704] Elapsed 2m 4s (remain 0m 0s) Loss: 0.0000(0.0098) \n","Epoch 4 - avg_train_loss: 0.0047  avg_val_loss: 0.0098  time: 925s\n","Epoch 4 - Score: 0.8734\n","Epoch 4 - Save Best Score: 0.8734 Model\n","Epoch: [5][0/2871] Elapsed 0m 0s (remain 33m 10s) Loss: 0.0013(0.0013) Grad: 5829.4146  LR: 0.000004  \n","Epoch: [5][100/2871] Elapsed 0m 30s (remain 14m 5s) Loss: 0.0000(0.0033) Grad: 244.9900  LR: 0.000004  \n","Epoch: [5][200/2871] Elapsed 1m 0s (remain 13m 21s) Loss: 0.0051(0.0040) Grad: 36372.0664  LR: 0.000004  \n","Epoch: [5][300/2871] Elapsed 1m 27s (remain 12m 30s) Loss: 0.0000(0.0039) Grad: 46.9081  LR: 0.000004  \n","Epoch: [5][400/2871] Elapsed 1m 55s (remain 11m 51s) Loss: 0.0021(0.0037) Grad: 24112.6523  LR: 0.000004  \n","Epoch: [5][500/2871] Elapsed 2m 23s (remain 11m 17s) Loss: 0.0038(0.0041) Grad: 4951.6558  LR: 0.000004  \n","Epoch: [5][600/2871] Elapsed 2m 50s (remain 10m 44s) Loss: 0.0000(0.0037) Grad: 80.0951  LR: 0.000004  \n","Epoch: [5][700/2871] Elapsed 3m 18s (remain 10m 14s) Loss: 0.0000(0.0037) Grad: 55.9873  LR: 0.000003  \n","Epoch: [5][800/2871] Elapsed 3m 46s (remain 9m 44s) Loss: 0.0023(0.0036) Grad: 8368.7920  LR: 0.000003  \n","Epoch: [5][900/2871] Elapsed 4m 13s (remain 9m 14s) Loss: 0.0186(0.0037) Grad: 175612.2969  LR: 0.000003  \n","Epoch: [5][1000/2871] Elapsed 4m 41s (remain 8m 45s) Loss: 0.0001(0.0038) Grad: 947.8735  LR: 0.000003  \n","Epoch: [5][1100/2871] Elapsed 5m 9s (remain 8m 16s) Loss: 0.0000(0.0037) Grad: 95.5700  LR: 0.000003  \n","Epoch: [5][1200/2871] Elapsed 5m 36s (remain 7m 48s) Loss: 0.0000(0.0037) Grad: 239.9867  LR: 0.000003  \n","Epoch: [5][1300/2871] Elapsed 6m 4s (remain 7m 19s) Loss: 0.0000(0.0038) Grad: 52.7058  LR: 0.000002  \n","Epoch: [5][1400/2871] Elapsed 6m 32s (remain 6m 51s) Loss: 0.0001(0.0038) Grad: 2118.5874  LR: 0.000002  \n","Epoch: [5][1500/2871] Elapsed 6m 59s (remain 6m 23s) Loss: 0.0002(0.0037) Grad: 1768.7151  LR: 0.000002  \n","Epoch: [5][1600/2871] Elapsed 7m 27s (remain 5m 55s) Loss: 0.0088(0.0037) Grad: 147439.8125  LR: 0.000002  \n","Epoch: [5][1700/2871] Elapsed 7m 55s (remain 5m 26s) Loss: 0.0000(0.0038) Grad: 122.2550  LR: 0.000002  \n","Epoch: [5][1800/2871] Elapsed 8m 23s (remain 4m 58s) Loss: 0.0042(0.0038) Grad: 7390.7520  LR: 0.000002  \n","Epoch: [5][1900/2871] Elapsed 8m 50s (remain 4m 30s) Loss: 0.0000(0.0038) Grad: 102.1261  LR: 0.000002  \n","Epoch: [5][2000/2871] Elapsed 9m 18s (remain 4m 2s) Loss: 0.0010(0.0039) Grad: 9623.7822  LR: 0.000001  \n","Epoch: [5][2100/2871] Elapsed 9m 45s (remain 3m 34s) Loss: 0.0002(0.0040) Grad: 1916.0809  LR: 0.000001  \n","Epoch: [5][2200/2871] Elapsed 10m 13s (remain 3m 6s) Loss: 0.0391(0.0040) Grad: 66640.2969  LR: 0.000001  \n","Epoch: [5][2300/2871] Elapsed 10m 40s (remain 2m 38s) Loss: 0.0000(0.0040) Grad: 58.5023  LR: 0.000001  \n","Epoch: [5][2400/2871] Elapsed 11m 8s (remain 2m 10s) Loss: 0.0085(0.0039) Grad: 39842.5547  LR: 0.000001  \n","Epoch: [5][2500/2871] Elapsed 11m 36s (remain 1m 42s) Loss: 0.0004(0.0039) Grad: 15421.3262  LR: 0.000001  \n","Epoch: [5][2600/2871] Elapsed 12m 3s (remain 1m 15s) Loss: 0.0000(0.0039) Grad: 46.4522  LR: 0.000000  \n","Epoch: [5][2700/2871] Elapsed 12m 31s (remain 0m 47s) Loss: 0.0003(0.0039) Grad: 2435.5759  LR: 0.000000  \n","Epoch: [5][2800/2871] Elapsed 12m 59s (remain 0m 19s) Loss: 0.0000(0.0039) Grad: 57.3186  LR: 0.000000  \n","Epoch: [5][2870/2871] Elapsed 13m 18s (remain 0m 0s) Loss: 0.0267(0.0038) Grad: 30736.4199  LR: 0.000000  \n","EVAL: [0/704] Elapsed 0m 0s (remain 5m 53s) Loss: 0.0020(0.0020) \n","EVAL: [100/704] Elapsed 0m 18s (remain 1m 48s) Loss: 0.0014(0.0106) \n","EVAL: [200/704] Elapsed 0m 35s (remain 1m 29s) Loss: 0.0000(0.0106) \n","EVAL: [300/704] Elapsed 0m 53s (remain 1m 12s) Loss: 0.0000(0.0093) \n","EVAL: [400/704] Elapsed 1m 11s (remain 0m 54s) Loss: 0.0085(0.0105) \n","EVAL: [500/704] Elapsed 1m 29s (remain 0m 36s) Loss: 0.0095(0.0112) \n","EVAL: [600/704] Elapsed 1m 47s (remain 0m 18s) Loss: 0.0000(0.0115) \n","EVAL: [700/704] Elapsed 2m 4s (remain 0m 0s) Loss: 0.0000(0.0106) \n","EVAL: [703/704] Elapsed 2m 5s (remain 0m 0s) Loss: 0.0000(0.0106) \n","Epoch 5 - avg_train_loss: 0.0038  avg_val_loss: 0.0106  time: 928s\n","Epoch 5 - Score: 0.8743\n","Epoch 5 - Save Best Score: 0.8743 Model\n","========== fold: 3 training ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n","Epoch: [1][0/2877] Elapsed 0m 0s (remain 38m 19s) Loss: 0.6015(0.6015) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/2877] Elapsed 0m 32s (remain 14m 42s) Loss: 0.2853(0.5837) Grad: 24102.8477  LR: 0.000001  \n","Epoch: [1][200/2877] Elapsed 1m 0s (remain 13m 22s) Loss: 0.0582(0.3563) Grad: 1330.6440  LR: 0.000003  \n","Epoch: [1][300/2877] Elapsed 1m 28s (remain 12m 34s) Loss: 0.0192(0.2501) Grad: 684.8483  LR: 0.000004  \n","Epoch: [1][400/2877] Elapsed 1m 55s (remain 11m 55s) Loss: 0.0084(0.1935) Grad: 2758.9419  LR: 0.000006  \n","Epoch: [1][500/2877] Elapsed 2m 23s (remain 11m 21s) Loss: 0.0094(0.1587) Grad: 1798.7690  LR: 0.000007  \n","Epoch: [1][600/2877] Elapsed 2m 52s (remain 10m 52s) Loss: 0.0059(0.1351) Grad: 2311.5518  LR: 0.000008  \n","Epoch: [1][700/2877] Elapsed 3m 19s (remain 10m 20s) Loss: 0.0182(0.1179) Grad: 7369.1953  LR: 0.000010  \n","Epoch: [1][800/2877] Elapsed 3m 47s (remain 9m 49s) Loss: 0.0037(0.1050) Grad: 481.0754  LR: 0.000011  \n","Epoch: [1][900/2877] Elapsed 4m 15s (remain 9m 19s) Loss: 0.0055(0.0952) Grad: 3917.4792  LR: 0.000013  \n","Epoch: [1][1000/2877] Elapsed 4m 42s (remain 8m 49s) Loss: 0.0034(0.0870) Grad: 720.9545  LR: 0.000014  \n","Epoch: [1][1100/2877] Elapsed 5m 10s (remain 8m 20s) Loss: 0.0046(0.0802) Grad: 722.2343  LR: 0.000015  \n","Epoch: [1][1200/2877] Elapsed 5m 38s (remain 7m 51s) Loss: 0.0161(0.0747) Grad: 2252.8652  LR: 0.000017  \n","Epoch: [1][1300/2877] Elapsed 6m 5s (remain 7m 23s) Loss: 0.0045(0.0697) Grad: 563.5979  LR: 0.000018  \n","Epoch: [1][1400/2877] Elapsed 6m 33s (remain 6m 54s) Loss: 0.0031(0.0657) Grad: 1040.7354  LR: 0.000019  \n","Epoch: [1][1500/2877] Elapsed 7m 1s (remain 6m 26s) Loss: 0.0118(0.0621) Grad: 1839.4492  LR: 0.000020  \n","Epoch: [1][1600/2877] Elapsed 7m 28s (remain 5m 57s) Loss: 0.0388(0.0590) Grad: 3299.6938  LR: 0.000020  \n","Epoch: [1][1700/2877] Elapsed 7m 56s (remain 5m 29s) Loss: 0.0119(0.0562) Grad: 2330.2727  LR: 0.000020  \n","Epoch: [1][1800/2877] Elapsed 8m 24s (remain 5m 1s) Loss: 0.0017(0.0537) Grad: 431.2888  LR: 0.000019  \n","Epoch: [1][1900/2877] Elapsed 8m 51s (remain 4m 33s) Loss: 0.0066(0.0514) Grad: 2416.5784  LR: 0.000019  \n","Epoch: [1][2000/2877] Elapsed 9m 19s (remain 4m 4s) Loss: 0.0116(0.0494) Grad: 2663.3914  LR: 0.000019  \n","Epoch: [1][2100/2877] Elapsed 9m 47s (remain 3m 36s) Loss: 0.0081(0.0475) Grad: 863.3430  LR: 0.000019  \n","Epoch: [1][2200/2877] Elapsed 10m 15s (remain 3m 8s) Loss: 0.0027(0.0458) Grad: 353.8849  LR: 0.000019  \n","Epoch: [1][2300/2877] Elapsed 10m 42s (remain 2m 40s) Loss: 0.0193(0.0443) Grad: 4674.0205  LR: 0.000019  \n","Epoch: [1][2400/2877] Elapsed 11m 10s (remain 2m 12s) Loss: 0.0063(0.0428) Grad: 495.2465  LR: 0.000019  \n","Epoch: [1][2500/2877] Elapsed 11m 38s (remain 1m 44s) Loss: 0.0185(0.0415) Grad: 5203.4297  LR: 0.000018  \n","Epoch: [1][2600/2877] Elapsed 12m 6s (remain 1m 17s) Loss: 0.0103(0.0403) Grad: 1136.8954  LR: 0.000018  \n","Epoch: [1][2700/2877] Elapsed 12m 33s (remain 0m 49s) Loss: 0.0068(0.0392) Grad: 3230.8955  LR: 0.000018  \n","Epoch: [1][2800/2877] Elapsed 13m 1s (remain 0m 21s) Loss: 0.0042(0.0381) Grad: 594.3098  LR: 0.000018  \n","Epoch: [1][2876/2877] Elapsed 13m 22s (remain 0m 0s) Loss: 0.0020(0.0373) Grad: 170.1231  LR: 0.000018  \n","EVAL: [0/698] Elapsed 0m 0s (remain 6m 35s) Loss: 0.0018(0.0018) \n","EVAL: [100/698] Elapsed 0m 18s (remain 1m 47s) Loss: 0.0009(0.0048) \n","EVAL: [200/698] Elapsed 0m 35s (remain 1m 28s) Loss: 0.0087(0.0073) \n","EVAL: [300/698] Elapsed 0m 53s (remain 1m 10s) Loss: 0.0003(0.0076) \n","EVAL: [400/698] Elapsed 1m 11s (remain 0m 52s) Loss: 0.0254(0.0086) \n","EVAL: [500/698] Elapsed 1m 28s (remain 0m 34s) Loss: 0.0077(0.0087) \n","EVAL: [600/698] Elapsed 1m 46s (remain 0m 17s) Loss: 0.0061(0.0084) \n","EVAL: [697/698] Elapsed 2m 3s (remain 0m 0s) Loss: 0.0001(0.0083) \n","Epoch 1 - avg_train_loss: 0.0373  avg_val_loss: 0.0083  time: 930s\n","Epoch 1 - Score: 0.8541\n","Epoch 1 - Save Best Score: 0.8541 Model\n","Epoch: [2][0/2877] Elapsed 0m 0s (remain 33m 35s) Loss: 0.0007(0.0007) Grad: 2244.4385  LR: 0.000018  \n","Epoch: [2][100/2877] Elapsed 0m 30s (remain 14m 7s) Loss: 0.0038(0.0077) Grad: 10126.7334  LR: 0.000018  \n","Epoch: [2][200/2877] Elapsed 1m 0s (remain 13m 23s) Loss: 0.0029(0.0080) Grad: 5899.7959  LR: 0.000017  \n","Epoch: [2][300/2877] Elapsed 1m 28s (remain 12m 35s) Loss: 0.0142(0.0072) Grad: 41771.7305  LR: 0.000017  \n","Epoch: [2][400/2877] Elapsed 1m 55s (remain 11m 56s) Loss: 0.0261(0.0068) Grad: 57577.3516  LR: 0.000017  \n","Epoch: [2][500/2877] Elapsed 2m 23s (remain 11m 21s) Loss: 0.0011(0.0070) Grad: 5867.7451  LR: 0.000017  \n","Epoch: [2][600/2877] Elapsed 2m 51s (remain 10m 49s) Loss: 0.0003(0.0072) Grad: 1230.6779  LR: 0.000017  \n","Epoch: [2][700/2877] Elapsed 3m 19s (remain 10m 18s) Loss: 0.0072(0.0074) Grad: 26061.9551  LR: 0.000017  \n","Epoch: [2][800/2877] Elapsed 3m 47s (remain 9m 48s) Loss: 0.0154(0.0076) Grad: 13441.0908  LR: 0.000017  \n","Epoch: [2][900/2877] Elapsed 4m 15s (remain 9m 19s) Loss: 0.0050(0.0076) Grad: 25946.4941  LR: 0.000016  \n","Epoch: [2][1000/2877] Elapsed 4m 42s (remain 8m 49s) Loss: 0.0014(0.0075) Grad: 7696.4463  LR: 0.000016  \n","Epoch: [2][1100/2877] Elapsed 5m 10s (remain 8m 20s) Loss: 0.0015(0.0074) Grad: 5487.9331  LR: 0.000016  \n","Epoch: [2][1200/2877] Elapsed 5m 38s (remain 7m 51s) Loss: 0.0076(0.0074) Grad: 16578.6152  LR: 0.000016  \n","Epoch: [2][1300/2877] Elapsed 6m 5s (remain 7m 22s) Loss: 0.0081(0.0073) Grad: 21508.0293  LR: 0.000016  \n","Epoch: [2][1400/2877] Elapsed 6m 33s (remain 6m 54s) Loss: 0.0123(0.0073) Grad: 24850.4258  LR: 0.000016  \n","Epoch: [2][1500/2877] Elapsed 7m 1s (remain 6m 26s) Loss: 0.0002(0.0073) Grad: 565.9500  LR: 0.000015  \n","Epoch: [2][1600/2877] Elapsed 7m 28s (remain 5m 57s) Loss: 0.0007(0.0074) Grad: 2871.2122  LR: 0.000015  \n","Epoch: [2][1700/2877] Elapsed 7m 56s (remain 5m 29s) Loss: 0.0003(0.0073) Grad: 930.0386  LR: 0.000015  \n","Epoch: [2][1800/2877] Elapsed 8m 23s (remain 5m 1s) Loss: 0.0062(0.0075) Grad: 13786.7285  LR: 0.000015  \n","Epoch: [2][1900/2877] Elapsed 8m 51s (remain 4m 32s) Loss: 0.0052(0.0075) Grad: 16774.3145  LR: 0.000015  \n","Epoch: [2][2000/2877] Elapsed 9m 19s (remain 4m 4s) Loss: 0.0018(0.0075) Grad: 3993.9263  LR: 0.000015  \n","Epoch: [2][2100/2877] Elapsed 9m 46s (remain 3m 36s) Loss: 0.0127(0.0074) Grad: 23230.9941  LR: 0.000015  \n","Epoch: [2][2200/2877] Elapsed 10m 14s (remain 3m 8s) Loss: 0.0171(0.0073) Grad: 13311.1816  LR: 0.000014  \n","Epoch: [2][2300/2877] Elapsed 10m 42s (remain 2m 40s) Loss: 0.0026(0.0073) Grad: 18123.5996  LR: 0.000014  \n","Epoch: [2][2400/2877] Elapsed 11m 9s (remain 2m 12s) Loss: 0.0001(0.0073) Grad: 361.9676  LR: 0.000014  \n","Epoch: [2][2500/2877] Elapsed 11m 37s (remain 1m 44s) Loss: 0.0041(0.0074) Grad: 10355.7109  LR: 0.000014  \n","Epoch: [2][2600/2877] Elapsed 12m 5s (remain 1m 16s) Loss: 0.0003(0.0073) Grad: 2845.5334  LR: 0.000014  \n","Epoch: [2][2700/2877] Elapsed 12m 32s (remain 0m 49s) Loss: 0.0009(0.0074) Grad: 3288.0107  LR: 0.000014  \n","Epoch: [2][2800/2877] Elapsed 13m 0s (remain 0m 21s) Loss: 0.0078(0.0073) Grad: 9725.1104  LR: 0.000013  \n","Epoch: [2][2876/2877] Elapsed 13m 21s (remain 0m 0s) Loss: 0.0304(0.0073) Grad: 28583.5430  LR: 0.000013  \n","EVAL: [0/698] Elapsed 0m 0s (remain 6m 0s) Loss: 0.0010(0.0010) \n","EVAL: [100/698] Elapsed 0m 18s (remain 1m 47s) Loss: 0.0009(0.0058) \n","EVAL: [200/698] Elapsed 0m 35s (remain 1m 28s) Loss: 0.0001(0.0080) \n","EVAL: [300/698] Elapsed 0m 53s (remain 1m 10s) Loss: 0.0003(0.0076) \n","EVAL: [400/698] Elapsed 1m 11s (remain 0m 52s) Loss: 0.0272(0.0081) \n","EVAL: [500/698] Elapsed 1m 28s (remain 0m 34s) Loss: 0.0065(0.0083) \n","EVAL: [600/698] Elapsed 1m 46s (remain 0m 17s) Loss: 0.0033(0.0079) \n","EVAL: [697/698] Elapsed 2m 3s (remain 0m 0s) Loss: 0.0000(0.0077) \n","Epoch 2 - avg_train_loss: 0.0073  avg_val_loss: 0.0077  time: 929s\n","Epoch 2 - Score: 0.8757\n","Epoch 2 - Save Best Score: 0.8757 Model\n","Epoch: [3][0/2877] Elapsed 0m 0s (remain 31m 4s) Loss: 0.0114(0.0114) Grad: 13376.2979  LR: 0.000013  \n","Epoch: [3][100/2877] Elapsed 0m 30s (remain 13m 57s) Loss: 0.0001(0.0064) Grad: 221.2897  LR: 0.000013  \n","Epoch: [3][200/2877] Elapsed 0m 59s (remain 13m 15s) Loss: 0.0047(0.0056) Grad: 12213.4639  LR: 0.000013  \n","Epoch: [3][300/2877] Elapsed 1m 27s (remain 12m 25s) Loss: 0.0023(0.0056) Grad: 4098.2671  LR: 0.000013  \n","Epoch: [3][400/2877] Elapsed 1m 54s (remain 11m 47s) Loss: 0.0448(0.0061) Grad: 69902.6719  LR: 0.000013  \n","Epoch: [3][500/2877] Elapsed 2m 22s (remain 11m 13s) Loss: 0.0028(0.0064) Grad: 4223.7329  LR: 0.000013  \n","Epoch: [3][600/2877] Elapsed 2m 49s (remain 10m 42s) Loss: 0.0000(0.0062) Grad: 31.2028  LR: 0.000012  \n","Epoch: [3][700/2877] Elapsed 3m 17s (remain 10m 12s) Loss: 0.0033(0.0061) Grad: 13742.8994  LR: 0.000012  \n","Epoch: [3][800/2877] Elapsed 3m 44s (remain 9m 42s) Loss: 0.0015(0.0060) Grad: 7406.5664  LR: 0.000012  \n","Epoch: [3][900/2877] Elapsed 4m 12s (remain 9m 13s) Loss: 0.1457(0.0062) Grad: 109795.0156  LR: 0.000012  \n","Epoch: [3][1000/2877] Elapsed 4m 39s (remain 8m 44s) Loss: 0.0001(0.0060) Grad: 881.4645  LR: 0.000012  \n","Epoch: [3][1100/2877] Elapsed 5m 7s (remain 8m 15s) Loss: 0.0005(0.0059) Grad: 1601.1338  LR: 0.000012  \n","Epoch: [3][1200/2877] Elapsed 5m 34s (remain 7m 47s) Loss: 0.0080(0.0058) Grad: 14978.8604  LR: 0.000011  \n","Epoch: [3][1300/2877] Elapsed 6m 2s (remain 7m 19s) Loss: 0.0208(0.0060) Grad: 47822.5742  LR: 0.000011  \n","Epoch: [3][1400/2877] Elapsed 6m 29s (remain 6m 50s) Loss: 0.0006(0.0060) Grad: 12469.2578  LR: 0.000011  \n","Epoch: [3][1500/2877] Elapsed 6m 57s (remain 6m 22s) Loss: 0.0000(0.0060) Grad: 174.4790  LR: 0.000011  \n","Epoch: [3][1600/2877] Elapsed 7m 25s (remain 5m 54s) Loss: 0.0019(0.0059) Grad: 3513.4363  LR: 0.000011  \n","Epoch: [3][1700/2877] Elapsed 7m 52s (remain 5m 26s) Loss: 0.0000(0.0059) Grad: 85.0208  LR: 0.000011  \n","Epoch: [3][1800/2877] Elapsed 8m 20s (remain 4m 59s) Loss: 0.0118(0.0060) Grad: 17296.1387  LR: 0.000011  \n","Epoch: [3][1900/2877] Elapsed 8m 48s (remain 4m 31s) Loss: 0.0023(0.0060) Grad: 9804.6914  LR: 0.000010  \n","Epoch: [3][2000/2877] Elapsed 9m 15s (remain 4m 3s) Loss: 0.0008(0.0059) Grad: 8872.3857  LR: 0.000010  \n","Epoch: [3][2100/2877] Elapsed 9m 43s (remain 3m 35s) Loss: 0.0074(0.0059) Grad: 99460.7188  LR: 0.000010  \n","Epoch: [3][2200/2877] Elapsed 10m 10s (remain 3m 7s) Loss: 0.0004(0.0059) Grad: 8207.4541  LR: 0.000010  \n","Epoch: [3][2300/2877] Elapsed 10m 38s (remain 2m 39s) Loss: 0.0019(0.0059) Grad: 6172.9761  LR: 0.000010  \n","Epoch: [3][2400/2877] Elapsed 11m 6s (remain 2m 12s) Loss: 0.0019(0.0060) Grad: 5689.3838  LR: 0.000010  \n","Epoch: [3][2500/2877] Elapsed 11m 33s (remain 1m 44s) Loss: 0.0003(0.0060) Grad: 7491.3218  LR: 0.000009  \n","Epoch: [3][2600/2877] Elapsed 12m 0s (remain 1m 16s) Loss: 0.0020(0.0060) Grad: 10741.9375  LR: 0.000009  \n","Epoch: [3][2700/2877] Elapsed 12m 28s (remain 0m 48s) Loss: 0.0003(0.0060) Grad: 965.1929  LR: 0.000009  \n","Epoch: [3][2800/2877] Elapsed 12m 55s (remain 0m 21s) Loss: 0.0059(0.0059) Grad: 274752.2500  LR: 0.000009  \n","Epoch: [3][2876/2877] Elapsed 13m 16s (remain 0m 0s) Loss: 0.0010(0.0060) Grad: 5983.8276  LR: 0.000009  \n","EVAL: [0/698] Elapsed 0m 0s (remain 5m 58s) Loss: 0.0005(0.0005) \n","EVAL: [100/698] Elapsed 0m 18s (remain 1m 48s) Loss: 0.0016(0.0064) \n","EVAL: [200/698] Elapsed 0m 36s (remain 1m 29s) Loss: 0.0009(0.0091) \n","EVAL: [300/698] Elapsed 0m 53s (remain 1m 10s) Loss: 0.0006(0.0095) \n","EVAL: [400/698] Elapsed 1m 11s (remain 0m 52s) Loss: 0.0231(0.0099) \n","EVAL: [500/698] Elapsed 1m 28s (remain 0m 34s) Loss: 0.0019(0.0096) \n","EVAL: [600/698] Elapsed 1m 46s (remain 0m 17s) Loss: 0.0048(0.0092) \n","EVAL: [697/698] Elapsed 2m 3s (remain 0m 0s) Loss: 0.0000(0.0090) \n","Epoch 3 - avg_train_loss: 0.0060  avg_val_loss: 0.0090  time: 924s\n","Epoch 3 - Score: 0.8814\n","Epoch 3 - Save Best Score: 0.8814 Model\n","Epoch: [4][0/2877] Elapsed 0m 0s (remain 31m 41s) Loss: 0.0343(0.0343) Grad: 284511.3750  LR: 0.000009  \n","Epoch: [4][100/2877] Elapsed 0m 29s (remain 13m 39s) Loss: 0.0025(0.0060) Grad: 13307.8682  LR: 0.000009  \n","Epoch: [4][200/2877] Elapsed 0m 59s (remain 13m 8s) Loss: 0.0111(0.0055) Grad: 144750.7344  LR: 0.000009  \n","Epoch: [4][300/2877] Elapsed 1m 26s (remain 12m 23s) Loss: 0.0055(0.0059) Grad: 6636.8467  LR: 0.000008  \n","Epoch: [4][400/2877] Elapsed 1m 54s (remain 11m 47s) Loss: 0.0000(0.0053) Grad: 72.2530  LR: 0.000008  \n","Epoch: [4][500/2877] Elapsed 2m 22s (remain 11m 14s) Loss: 0.0008(0.0052) Grad: 12863.6807  LR: 0.000008  \n","Epoch: [4][600/2877] Elapsed 2m 49s (remain 10m 42s) Loss: 0.0071(0.0053) Grad: 12673.0547  LR: 0.000008  \n","Epoch: [4][700/2877] Elapsed 3m 17s (remain 10m 12s) Loss: 0.0016(0.0053) Grad: 3305.1487  LR: 0.000008  \n","Epoch: [4][800/2877] Elapsed 3m 44s (remain 9m 42s) Loss: 0.0000(0.0049) Grad: 59.3440  LR: 0.000008  \n","Epoch: [4][900/2877] Elapsed 4m 12s (remain 9m 13s) Loss: 0.0038(0.0050) Grad: 13644.4707  LR: 0.000007  \n","Epoch: [4][1000/2877] Elapsed 4m 39s (remain 8m 44s) Loss: 0.0000(0.0051) Grad: 41.2860  LR: 0.000007  \n","Epoch: [4][1100/2877] Elapsed 5m 7s (remain 8m 15s) Loss: 0.0051(0.0051) Grad: 17661.4199  LR: 0.000007  \n","Epoch: [4][1200/2877] Elapsed 5m 34s (remain 7m 47s) Loss: 0.0001(0.0051) Grad: 622.3662  LR: 0.000007  \n","Epoch: [4][1300/2877] Elapsed 6m 2s (remain 7m 18s) Loss: 0.0001(0.0051) Grad: 1203.7272  LR: 0.000007  \n","Epoch: [4][1400/2877] Elapsed 6m 29s (remain 6m 50s) Loss: 0.0000(0.0051) Grad: 63.4987  LR: 0.000007  \n","Epoch: [4][1500/2877] Elapsed 6m 56s (remain 6m 22s) Loss: 0.0012(0.0050) Grad: 14372.7490  LR: 0.000007  \n","Epoch: [4][1600/2877] Elapsed 7m 24s (remain 5m 54s) Loss: 0.0009(0.0049) Grad: 9473.6641  LR: 0.000006  \n","Epoch: [4][1700/2877] Elapsed 7m 51s (remain 5m 26s) Loss: 0.0005(0.0049) Grad: 1618.8849  LR: 0.000006  \n","Epoch: [4][1800/2877] Elapsed 8m 19s (remain 4m 58s) Loss: 0.0001(0.0049) Grad: 538.1614  LR: 0.000006  \n","Epoch: [4][1900/2877] Elapsed 8m 46s (remain 4m 30s) Loss: 0.0003(0.0048) Grad: 2466.4229  LR: 0.000006  \n","Epoch: [4][2000/2877] Elapsed 9m 14s (remain 4m 2s) Loss: 0.0018(0.0048) Grad: 9759.5957  LR: 0.000006  \n","Epoch: [4][2100/2877] Elapsed 9m 41s (remain 3m 34s) Loss: 0.0000(0.0048) Grad: 16.6465  LR: 0.000006  \n","Epoch: [4][2200/2877] Elapsed 10m 9s (remain 3m 7s) Loss: 0.0000(0.0047) Grad: 28.8449  LR: 0.000005  \n","Epoch: [4][2300/2877] Elapsed 10m 36s (remain 2m 39s) Loss: 0.0051(0.0048) Grad: 5752.1523  LR: 0.000005  \n","Epoch: [4][2400/2877] Elapsed 11m 4s (remain 2m 11s) Loss: 0.0012(0.0048) Grad: 5141.7778  LR: 0.000005  \n","Epoch: [4][2500/2877] Elapsed 11m 31s (remain 1m 44s) Loss: 0.0192(0.0048) Grad: 79112.1016  LR: 0.000005  \n","Epoch: [4][2600/2877] Elapsed 11m 59s (remain 1m 16s) Loss: 0.0211(0.0048) Grad: 51812.2930  LR: 0.000005  \n","Epoch: [4][2700/2877] Elapsed 12m 26s (remain 0m 48s) Loss: 0.0122(0.0048) Grad: 43138.7344  LR: 0.000005  \n","Epoch: [4][2800/2877] Elapsed 12m 54s (remain 0m 21s) Loss: 0.0003(0.0048) Grad: 5445.0928  LR: 0.000005  \n","Epoch: [4][2876/2877] Elapsed 13m 15s (remain 0m 0s) Loss: 0.0035(0.0048) Grad: 3396.8562  LR: 0.000004  \n","EVAL: [0/698] Elapsed 0m 0s (remain 5m 52s) Loss: 0.0005(0.0005) \n","EVAL: [100/698] Elapsed 0m 18s (remain 1m 47s) Loss: 0.0007(0.0070) \n","EVAL: [200/698] Elapsed 0m 35s (remain 1m 28s) Loss: 0.0005(0.0095) \n","EVAL: [300/698] Elapsed 0m 53s (remain 1m 10s) Loss: 0.0007(0.0100) \n","EVAL: [400/698] Elapsed 1m 11s (remain 0m 52s) Loss: 0.0185(0.0098) \n","EVAL: [500/698] Elapsed 1m 28s (remain 0m 34s) Loss: 0.0021(0.0096) \n","EVAL: [600/698] Elapsed 1m 46s (remain 0m 17s) Loss: 0.0015(0.0091) \n","EVAL: [697/698] Elapsed 2m 3s (remain 0m 0s) Loss: 0.0000(0.0088) \n","Epoch 4 - avg_train_loss: 0.0048  avg_val_loss: 0.0088  time: 923s\n","Epoch 4 - Score: 0.8833\n","Epoch 4 - Save Best Score: 0.8833 Model\n","Epoch: [5][0/2877] Elapsed 0m 0s (remain 30m 41s) Loss: 0.0002(0.0002) Grad: 1046.0504  LR: 0.000004  \n","Epoch: [5][100/2877] Elapsed 0m 30s (remain 14m 0s) Loss: 0.0028(0.0030) Grad: 10943.9199  LR: 0.000004  \n","Epoch: [5][200/2877] Elapsed 0m 59s (remain 13m 15s) Loss: 0.0065(0.0040) Grad: 8764.8398  LR: 0.000004  \n","Epoch: [5][300/2877] Elapsed 1m 27s (remain 12m 26s) Loss: 0.0004(0.0036) Grad: 3994.8882  LR: 0.000004  \n","Epoch: [5][400/2877] Elapsed 1m 54s (remain 11m 48s) Loss: 0.0042(0.0034) Grad: 13395.1396  LR: 0.000004  \n","Epoch: [5][500/2877] Elapsed 2m 22s (remain 11m 14s) Loss: 0.0000(0.0036) Grad: 84.0787  LR: 0.000004  \n","Epoch: [5][600/2877] Elapsed 2m 49s (remain 10m 42s) Loss: 0.0001(0.0036) Grad: 959.3223  LR: 0.000004  \n","Epoch: [5][700/2877] Elapsed 3m 17s (remain 10m 12s) Loss: 0.0000(0.0037) Grad: 11.2746  LR: 0.000003  \n","Epoch: [5][800/2877] Elapsed 3m 44s (remain 9m 42s) Loss: 0.0000(0.0037) Grad: 26.4085  LR: 0.000003  \n","Epoch: [5][900/2877] Elapsed 4m 12s (remain 9m 13s) Loss: 0.0001(0.0039) Grad: 725.2084  LR: 0.000003  \n","Epoch: [5][1000/2877] Elapsed 4m 39s (remain 8m 44s) Loss: 0.0005(0.0039) Grad: 3670.2021  LR: 0.000003  \n","Epoch: [5][1100/2877] Elapsed 5m 7s (remain 8m 16s) Loss: 0.0005(0.0039) Grad: 4888.8994  LR: 0.000003  \n","Epoch: [5][1200/2877] Elapsed 5m 35s (remain 7m 47s) Loss: 0.0000(0.0038) Grad: 15.5581  LR: 0.000003  \n","Epoch: [5][1300/2877] Elapsed 6m 2s (remain 7m 19s) Loss: 0.0073(0.0039) Grad: 15043.5928  LR: 0.000002  \n","Epoch: [5][1400/2877] Elapsed 6m 30s (remain 6m 51s) Loss: 0.0000(0.0039) Grad: 128.2912  LR: 0.000002  \n","Epoch: [5][1500/2877] Elapsed 6m 58s (remain 6m 23s) Loss: 0.0018(0.0038) Grad: 16054.7197  LR: 0.000002  \n","Epoch: [5][1600/2877] Elapsed 7m 25s (remain 5m 55s) Loss: 0.0018(0.0038) Grad: 17278.4082  LR: 0.000002  \n","Epoch: [5][1700/2877] Elapsed 7m 53s (remain 5m 27s) Loss: 0.0033(0.0039) Grad: 11228.4473  LR: 0.000002  \n","Epoch: [5][1800/2877] Elapsed 8m 20s (remain 4m 59s) Loss: 0.0000(0.0038) Grad: 142.2821  LR: 0.000002  \n","Epoch: [5][1900/2877] Elapsed 8m 48s (remain 4m 31s) Loss: 0.0001(0.0038) Grad: 920.5491  LR: 0.000002  \n","Epoch: [5][2000/2877] Elapsed 9m 16s (remain 4m 3s) Loss: 0.0035(0.0039) Grad: 10883.2246  LR: 0.000001  \n","Epoch: [5][2100/2877] Elapsed 9m 43s (remain 3m 35s) Loss: 0.0000(0.0039) Grad: 170.9987  LR: 0.000001  \n","Epoch: [5][2200/2877] Elapsed 10m 10s (remain 3m 7s) Loss: 0.0119(0.0039) Grad: 74643.6484  LR: 0.000001  \n","Epoch: [5][2300/2877] Elapsed 10m 38s (remain 2m 39s) Loss: 0.0058(0.0039) Grad: 7614.6499  LR: 0.000001  \n","Epoch: [5][2400/2877] Elapsed 11m 5s (remain 2m 11s) Loss: 0.0001(0.0039) Grad: 580.5884  LR: 0.000001  \n","Epoch: [5][2500/2877] Elapsed 11m 33s (remain 1m 44s) Loss: 0.0045(0.0038) Grad: 7306.9834  LR: 0.000001  \n","Epoch: [5][2600/2877] Elapsed 12m 0s (remain 1m 16s) Loss: 0.0001(0.0039) Grad: 491.8984  LR: 0.000000  \n","Epoch: [5][2700/2877] Elapsed 12m 27s (remain 0m 48s) Loss: 0.0000(0.0039) Grad: 93.9507  LR: 0.000000  \n","Epoch: [5][2800/2877] Elapsed 12m 55s (remain 0m 21s) Loss: 0.0000(0.0039) Grad: 118.6194  LR: 0.000000  \n","Epoch: [5][2876/2877] Elapsed 13m 16s (remain 0m 0s) Loss: 0.0000(0.0039) Grad: 98.5320  LR: 0.000000  \n","EVAL: [0/698] Elapsed 0m 0s (remain 5m 49s) Loss: 0.0002(0.0002) \n","EVAL: [100/698] Elapsed 0m 18s (remain 1m 46s) Loss: 0.0013(0.0078) \n","EVAL: [200/698] Elapsed 0m 35s (remain 1m 28s) Loss: 0.0001(0.0103) \n","EVAL: [300/698] Elapsed 0m 53s (remain 1m 10s) Loss: 0.0007(0.0109) \n","EVAL: [400/698] Elapsed 1m 10s (remain 0m 52s) Loss: 0.0212(0.0110) \n","EVAL: [500/698] Elapsed 1m 28s (remain 0m 34s) Loss: 0.0021(0.0107) \n","EVAL: [600/698] Elapsed 1m 45s (remain 0m 17s) Loss: 0.0019(0.0102) \n","EVAL: [697/698] Elapsed 2m 2s (remain 0m 0s) Loss: 0.0000(0.0099) \n","Epoch 5 - avg_train_loss: 0.0039  avg_val_loss: 0.0099  time: 923s\n","Epoch 5 - Score: 0.8879\n","Epoch 5 - Save Best Score: 0.8879 Model\n","========== fold: 4 training ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n","Epoch: [1][0/2850] Elapsed 0m 0s (remain 40m 10s) Loss: 0.5280(0.5280) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/2850] Elapsed 0m 32s (remain 14m 36s) Loss: 0.2652(0.4031) Grad: 27202.2207  LR: 0.000001  \n","Epoch: [1][200/2850] Elapsed 0m 59s (remain 13m 6s) Loss: 0.0476(0.2439) Grad: 860.2441  LR: 0.000003  \n","Epoch: [1][300/2850] Elapsed 1m 27s (remain 12m 17s) Loss: 0.0210(0.1757) Grad: 1670.5734  LR: 0.000004  \n","Epoch: [1][400/2850] Elapsed 1m 54s (remain 11m 39s) Loss: 0.0081(0.1375) Grad: 890.0565  LR: 0.000006  \n","Epoch: [1][500/2850] Elapsed 2m 22s (remain 11m 6s) Loss: 0.0228(0.1137) Grad: 3680.1699  LR: 0.000007  \n","Epoch: [1][600/2850] Elapsed 2m 50s (remain 10m 38s) Loss: 0.0536(0.0975) Grad: 16532.2793  LR: 0.000008  \n","Epoch: [1][700/2850] Elapsed 3m 18s (remain 10m 7s) Loss: 0.0077(0.0856) Grad: 1974.8944  LR: 0.000010  \n","Epoch: [1][800/2850] Elapsed 3m 45s (remain 9m 37s) Loss: 0.0009(0.0764) Grad: 346.7089  LR: 0.000011  \n","Epoch: [1][900/2850] Elapsed 4m 13s (remain 9m 7s) Loss: 0.0061(0.0694) Grad: 5228.8828  LR: 0.000013  \n","Epoch: [1][1000/2850] Elapsed 4m 40s (remain 8m 37s) Loss: 0.0718(0.0637) Grad: 5946.0439  LR: 0.000014  \n","Epoch: [1][1100/2850] Elapsed 5m 7s (remain 8m 8s) Loss: 0.0280(0.0592) Grad: 6430.0669  LR: 0.000015  \n","Epoch: [1][1200/2850] Elapsed 5m 34s (remain 7m 39s) Loss: 0.0013(0.0552) Grad: 996.1161  LR: 0.000017  \n","Epoch: [1][1300/2850] Elapsed 6m 2s (remain 7m 11s) Loss: 0.0108(0.0520) Grad: 1054.3981  LR: 0.000018  \n","Epoch: [1][1400/2850] Elapsed 6m 29s (remain 6m 43s) Loss: 0.0017(0.0490) Grad: 344.5188  LR: 0.000020  \n","Epoch: [1][1500/2850] Elapsed 6m 57s (remain 6m 14s) Loss: 0.0082(0.0466) Grad: 3243.7969  LR: 0.000020  \n","Epoch: [1][1600/2850] Elapsed 7m 24s (remain 5m 46s) Loss: 0.0078(0.0445) Grad: 1006.9011  LR: 0.000020  \n","Epoch: [1][1700/2850] Elapsed 7m 51s (remain 5m 18s) Loss: 0.0092(0.0426) Grad: 1042.1406  LR: 0.000020  \n","Epoch: [1][1800/2850] Elapsed 8m 19s (remain 4m 50s) Loss: 0.0061(0.0408) Grad: 613.8064  LR: 0.000019  \n","Epoch: [1][1900/2850] Elapsed 8m 46s (remain 4m 22s) Loss: 0.0038(0.0391) Grad: 739.7705  LR: 0.000019  \n","Epoch: [1][2000/2850] Elapsed 9m 14s (remain 3m 55s) Loss: 0.0085(0.0377) Grad: 1831.2495  LR: 0.000019  \n","Epoch: [1][2100/2850] Elapsed 9m 41s (remain 3m 27s) Loss: 0.0006(0.0364) Grad: 172.4205  LR: 0.000019  \n","Epoch: [1][2200/2850] Elapsed 10m 8s (remain 2m 59s) Loss: 0.0038(0.0351) Grad: 800.0054  LR: 0.000019  \n","Epoch: [1][2300/2850] Elapsed 10m 36s (remain 2m 31s) Loss: 0.0026(0.0340) Grad: 618.3486  LR: 0.000019  \n","Epoch: [1][2400/2850] Elapsed 11m 3s (remain 2m 4s) Loss: 0.0094(0.0329) Grad: 2081.2942  LR: 0.000018  \n","Epoch: [1][2500/2850] Elapsed 11m 31s (remain 1m 36s) Loss: 0.0047(0.0321) Grad: 713.7958  LR: 0.000018  \n","Epoch: [1][2600/2850] Elapsed 11m 58s (remain 1m 8s) Loss: 0.0051(0.0311) Grad: 2576.9919  LR: 0.000018  \n","Epoch: [1][2700/2850] Elapsed 12m 26s (remain 0m 41s) Loss: 0.0065(0.0303) Grad: 1688.3245  LR: 0.000018  \n","Epoch: [1][2800/2850] Elapsed 12m 53s (remain 0m 13s) Loss: 0.0084(0.0295) Grad: 642.7167  LR: 0.000018  \n","Epoch: [1][2849/2850] Elapsed 13m 7s (remain 0m 0s) Loss: 0.0020(0.0292) Grad: 393.3969  LR: 0.000018  \n","EVAL: [0/725] Elapsed 0m 0s (remain 6m 46s) Loss: 0.0088(0.0088) \n","EVAL: [100/725] Elapsed 0m 18s (remain 1m 51s) Loss: 0.0023(0.0083) \n","EVAL: [200/725] Elapsed 0m 35s (remain 1m 33s) Loss: 0.0115(0.0089) \n","EVAL: [300/725] Elapsed 0m 53s (remain 1m 14s) Loss: 0.0024(0.0085) \n","EVAL: [400/725] Elapsed 1m 10s (remain 0m 57s) Loss: 0.0536(0.0092) \n","EVAL: [500/725] Elapsed 1m 28s (remain 0m 39s) Loss: 0.0096(0.0092) \n","EVAL: [600/725] Elapsed 1m 45s (remain 0m 21s) Loss: 0.0031(0.0091) \n","EVAL: [700/725] Elapsed 2m 3s (remain 0m 4s) Loss: 0.0035(0.0087) \n","EVAL: [724/725] Elapsed 2m 7s (remain 0m 0s) Loss: 0.0129(0.0085) \n","Epoch 1 - avg_train_loss: 0.0292  avg_val_loss: 0.0085  time: 919s\n","Epoch 1 - Score: 0.8374\n","Epoch 1 - Save Best Score: 0.8374 Model\n","Epoch: [2][0/2850] Elapsed 0m 0s (remain 32m 9s) Loss: 0.0031(0.0031) Grad: 4248.5708  LR: 0.000018  \n","Epoch: [2][100/2850] Elapsed 0m 30s (remain 13m 55s) Loss: 0.0298(0.0067) Grad: 15875.5576  LR: 0.000018  \n","Epoch: [2][200/2850] Elapsed 1m 0s (remain 13m 11s) Loss: 0.0030(0.0078) Grad: 13669.1426  LR: 0.000017  \n","Epoch: [2][300/2850] Elapsed 1m 27s (remain 12m 20s) Loss: 0.0065(0.0081) Grad: 4246.7461  LR: 0.000017  \n","Epoch: [2][400/2850] Elapsed 1m 55s (remain 11m 42s) Loss: 0.0009(0.0080) Grad: 4347.5259  LR: 0.000017  \n","Epoch: [2][500/2850] Elapsed 2m 22s (remain 11m 8s) Loss: 0.0003(0.0076) Grad: 1943.9061  LR: 0.000017  \n","Epoch: [2][600/2850] Elapsed 2m 49s (remain 10m 35s) Loss: 0.0072(0.0075) Grad: 23994.7227  LR: 0.000017  \n","Epoch: [2][700/2850] Elapsed 3m 17s (remain 10m 4s) Loss: 0.0067(0.0077) Grad: 5213.6343  LR: 0.000017  \n","Epoch: [2][800/2850] Elapsed 3m 44s (remain 9m 34s) Loss: 0.0218(0.0077) Grad: 49181.1797  LR: 0.000017  \n","Epoch: [2][900/2850] Elapsed 4m 11s (remain 9m 4s) Loss: 0.0003(0.0076) Grad: 746.1102  LR: 0.000016  \n","Epoch: [2][1000/2850] Elapsed 4m 39s (remain 8m 36s) Loss: 0.0426(0.0076) Grad: 51541.3711  LR: 0.000016  \n","Epoch: [2][1100/2850] Elapsed 5m 6s (remain 8m 7s) Loss: 0.0055(0.0074) Grad: 12874.3232  LR: 0.000016  \n","Epoch: [2][1200/2850] Elapsed 5m 34s (remain 7m 39s) Loss: 0.0006(0.0075) Grad: 6164.1499  LR: 0.000016  \n","Epoch: [2][1300/2850] Elapsed 6m 1s (remain 7m 10s) Loss: 0.0006(0.0075) Grad: 2900.6475  LR: 0.000016  \n","Epoch: [2][1400/2850] Elapsed 6m 29s (remain 6m 42s) Loss: 0.0013(0.0075) Grad: 8026.8418  LR: 0.000016  \n","Epoch: [2][1500/2850] Elapsed 6m 56s (remain 6m 14s) Loss: 0.0112(0.0075) Grad: 32020.5723  LR: 0.000015  \n","Epoch: [2][1600/2850] Elapsed 7m 24s (remain 5m 46s) Loss: 0.0001(0.0074) Grad: 394.6251  LR: 0.000015  \n","Epoch: [2][1700/2850] Elapsed 7m 52s (remain 5m 18s) Loss: 0.0000(0.0073) Grad: 151.3383  LR: 0.000015  \n","Epoch: [2][1800/2850] Elapsed 8m 19s (remain 4m 50s) Loss: 0.0151(0.0073) Grad: 39559.3047  LR: 0.000015  \n","Epoch: [2][1900/2850] Elapsed 8m 46s (remain 4m 23s) Loss: 0.0015(0.0072) Grad: 9627.7852  LR: 0.000015  \n","Epoch: [2][2000/2850] Elapsed 9m 14s (remain 3m 55s) Loss: 0.0047(0.0072) Grad: 54615.6562  LR: 0.000015  \n","Epoch: [2][2100/2850] Elapsed 9m 41s (remain 3m 27s) Loss: 0.0037(0.0072) Grad: 8748.1396  LR: 0.000015  \n","Epoch: [2][2200/2850] Elapsed 10m 8s (remain 2m 59s) Loss: 0.0000(0.0072) Grad: 39.6347  LR: 0.000014  \n","Epoch: [2][2300/2850] Elapsed 10m 36s (remain 2m 31s) Loss: 0.0040(0.0071) Grad: 10379.3691  LR: 0.000014  \n","Epoch: [2][2400/2850] Elapsed 11m 3s (remain 2m 4s) Loss: 0.0013(0.0071) Grad: 15858.8867  LR: 0.000014  \n","Epoch: [2][2500/2850] Elapsed 11m 31s (remain 1m 36s) Loss: 0.0065(0.0071) Grad: 205487.9062  LR: 0.000014  \n","Epoch: [2][2600/2850] Elapsed 11m 58s (remain 1m 8s) Loss: 0.0038(0.0071) Grad: 10912.8936  LR: 0.000014  \n","Epoch: [2][2700/2850] Elapsed 12m 25s (remain 0m 41s) Loss: 0.0000(0.0071) Grad: 62.5492  LR: 0.000014  \n","Epoch: [2][2800/2850] Elapsed 12m 53s (remain 0m 13s) Loss: 0.0081(0.0071) Grad: 13804.2314  LR: 0.000013  \n","Epoch: [2][2849/2850] Elapsed 13m 6s (remain 0m 0s) Loss: 0.0139(0.0071) Grad: 24651.8984  LR: 0.000013  \n","EVAL: [0/725] Elapsed 0m 0s (remain 6m 0s) Loss: 0.0136(0.0136) \n","EVAL: [100/725] Elapsed 0m 18s (remain 1m 51s) Loss: 0.0027(0.0100) \n","EVAL: [200/725] Elapsed 0m 35s (remain 1m 33s) Loss: 0.0204(0.0107) \n","EVAL: [300/725] Elapsed 0m 53s (remain 1m 14s) Loss: 0.0056(0.0095) \n","EVAL: [400/725] Elapsed 1m 10s (remain 0m 57s) Loss: 0.0761(0.0106) \n","EVAL: [500/725] Elapsed 1m 28s (remain 0m 39s) Loss: 0.0149(0.0107) \n","EVAL: [600/725] Elapsed 1m 45s (remain 0m 21s) Loss: 0.0109(0.0103) \n","EVAL: [700/725] Elapsed 2m 3s (remain 0m 4s) Loss: 0.0002(0.0095) \n","EVAL: [724/725] Elapsed 2m 7s (remain 0m 0s) Loss: 0.0153(0.0093) \n","Epoch 2 - avg_train_loss: 0.0071  avg_val_loss: 0.0093  time: 918s\n","Epoch 2 - Score: 0.8711\n","Epoch 2 - Save Best Score: 0.8711 Model\n","Epoch: [3][0/2850] Elapsed 0m 0s (remain 30m 54s) Loss: 0.0063(0.0063) Grad: 26899.3340  LR: 0.000013  \n","Epoch: [3][100/2850] Elapsed 0m 30s (remain 13m 58s) Loss: 0.0046(0.0057) Grad: 11380.1592  LR: 0.000013  \n","Epoch: [3][200/2850] Elapsed 1m 0s (remain 13m 11s) Loss: 0.0055(0.0057) Grad: 7654.0513  LR: 0.000013  \n","Epoch: [3][300/2850] Elapsed 1m 27s (remain 12m 21s) Loss: 0.0003(0.0059) Grad: 923.7378  LR: 0.000013  \n","Epoch: [3][400/2850] Elapsed 1m 55s (remain 11m 42s) Loss: 0.0233(0.0060) Grad: 38281.9805  LR: 0.000013  \n","Epoch: [3][500/2850] Elapsed 2m 22s (remain 11m 7s) Loss: 0.0000(0.0056) Grad: 33.6890  LR: 0.000013  \n","Epoch: [3][600/2850] Elapsed 2m 49s (remain 10m 35s) Loss: 0.0020(0.0055) Grad: 4443.3638  LR: 0.000012  \n","Epoch: [3][700/2850] Elapsed 3m 17s (remain 10m 4s) Loss: 0.0008(0.0056) Grad: 8738.8408  LR: 0.000012  \n","Epoch: [3][800/2850] Elapsed 3m 44s (remain 9m 34s) Loss: 0.0001(0.0059) Grad: 673.8903  LR: 0.000012  \n","Epoch: [3][900/2850] Elapsed 4m 12s (remain 9m 5s) Loss: 0.0001(0.0057) Grad: 614.1266  LR: 0.000012  \n","Epoch: [3][1000/2850] Elapsed 4m 39s (remain 8m 36s) Loss: 0.0006(0.0057) Grad: 1658.8357  LR: 0.000012  \n","Epoch: [3][1100/2850] Elapsed 5m 6s (remain 8m 7s) Loss: 0.0002(0.0056) Grad: 807.3504  LR: 0.000012  \n","Epoch: [3][1200/2850] Elapsed 5m 34s (remain 7m 39s) Loss: 0.0044(0.0056) Grad: 8577.2617  LR: 0.000011  \n","Epoch: [3][1300/2850] Elapsed 6m 2s (remain 7m 11s) Loss: 0.0054(0.0055) Grad: 21460.8594  LR: 0.000011  \n","Epoch: [3][1400/2850] Elapsed 6m 29s (remain 6m 42s) Loss: 0.0001(0.0055) Grad: 326.6442  LR: 0.000011  \n","Epoch: [3][1500/2850] Elapsed 6m 57s (remain 6m 14s) Loss: 0.0035(0.0055) Grad: 23685.5430  LR: 0.000011  \n","Epoch: [3][1600/2850] Elapsed 7m 24s (remain 5m 46s) Loss: 0.0018(0.0056) Grad: 15775.8730  LR: 0.000011  \n","Epoch: [3][1700/2850] Elapsed 7m 51s (remain 5m 18s) Loss: 0.0042(0.0055) Grad: 10820.4512  LR: 0.000011  \n","Epoch: [3][1800/2850] Elapsed 8m 19s (remain 4m 50s) Loss: 0.0393(0.0056) Grad: 111583.2188  LR: 0.000011  \n","Epoch: [3][1900/2850] Elapsed 8m 47s (remain 4m 23s) Loss: 0.0001(0.0056) Grad: 517.9861  LR: 0.000010  \n","Epoch: [3][2000/2850] Elapsed 9m 14s (remain 3m 55s) Loss: 0.0001(0.0056) Grad: 310.0208  LR: 0.000010  \n","Epoch: [3][2100/2850] Elapsed 9m 42s (remain 3m 27s) Loss: 0.0058(0.0056) Grad: 11714.4414  LR: 0.000010  \n","Epoch: [3][2200/2850] Elapsed 10m 10s (remain 2m 59s) Loss: 0.0004(0.0057) Grad: 4840.0645  LR: 0.000010  \n","Epoch: [3][2300/2850] Elapsed 10m 37s (remain 2m 32s) Loss: 0.0202(0.0058) Grad: 78151.4531  LR: 0.000010  \n","Epoch: [3][2400/2850] Elapsed 11m 5s (remain 2m 4s) Loss: 0.0227(0.0058) Grad: 28983.0508  LR: 0.000010  \n","Epoch: [3][2500/2850] Elapsed 11m 32s (remain 1m 36s) Loss: 0.0107(0.0058) Grad: 43337.6875  LR: 0.000009  \n","Epoch: [3][2600/2850] Elapsed 12m 0s (remain 1m 8s) Loss: 0.0218(0.0058) Grad: 50287.9883  LR: 0.000009  \n","Epoch: [3][2700/2850] Elapsed 12m 27s (remain 0m 41s) Loss: 0.0040(0.0058) Grad: 8455.7295  LR: 0.000009  \n","Epoch: [3][2800/2850] Elapsed 12m 55s (remain 0m 13s) Loss: 0.0019(0.0058) Grad: 9065.6768  LR: 0.000009  \n","Epoch: [3][2849/2850] Elapsed 13m 8s (remain 0m 0s) Loss: 0.0003(0.0058) Grad: 6257.9790  LR: 0.000009  \n","EVAL: [0/725] Elapsed 0m 0s (remain 6m 1s) Loss: 0.0192(0.0192) \n","EVAL: [100/725] Elapsed 0m 17s (remain 1m 50s) Loss: 0.0002(0.0095) \n","EVAL: [200/725] Elapsed 0m 35s (remain 1m 32s) Loss: 0.0182(0.0107) \n","EVAL: [300/725] Elapsed 0m 52s (remain 1m 14s) Loss: 0.0012(0.0094) \n","EVAL: [400/725] Elapsed 1m 10s (remain 0m 56s) Loss: 0.0624(0.0105) \n","EVAL: [500/725] Elapsed 1m 27s (remain 0m 39s) Loss: 0.0183(0.0103) \n","EVAL: [600/725] Elapsed 1m 45s (remain 0m 21s) Loss: 0.0098(0.0102) \n","EVAL: [700/725] Elapsed 2m 3s (remain 0m 4s) Loss: 0.0002(0.0094) \n","EVAL: [724/725] Elapsed 2m 7s (remain 0m 0s) Loss: 0.0163(0.0092) \n","Epoch 3 - avg_train_loss: 0.0058  avg_val_loss: 0.0092  time: 920s\n","Epoch 3 - Score: 0.8713\n","Epoch 3 - Save Best Score: 0.8713 Model\n","Epoch: [4][0/2850] Elapsed 0m 0s (remain 28m 9s) Loss: 0.0000(0.0000) Grad: 142.1559  LR: 0.000009  \n","Epoch: [4][100/2850] Elapsed 0m 30s (remain 13m 47s) Loss: 0.0033(0.0062) Grad: 10608.0068  LR: 0.000009  \n","Epoch: [4][200/2850] Elapsed 0m 59s (remain 13m 4s) Loss: 0.0050(0.0057) Grad: 18324.2891  LR: 0.000009  \n","Epoch: [4][300/2850] Elapsed 1m 26s (remain 12m 16s) Loss: 0.0000(0.0050) Grad: 241.2713  LR: 0.000008  \n","Epoch: [4][400/2850] Elapsed 1m 54s (remain 11m 38s) Loss: 0.0031(0.0052) Grad: 17935.0000  LR: 0.000008  \n","Epoch: [4][500/2850] Elapsed 2m 21s (remain 11m 4s) Loss: 0.0001(0.0051) Grad: 833.4195  LR: 0.000008  \n","Epoch: [4][600/2850] Elapsed 2m 49s (remain 10m 33s) Loss: 0.0011(0.0049) Grad: 8511.7520  LR: 0.000008  \n","Epoch: [4][700/2850] Elapsed 3m 16s (remain 10m 2s) Loss: 0.0016(0.0050) Grad: 7573.1685  LR: 0.000008  \n","Epoch: [4][800/2850] Elapsed 3m 43s (remain 9m 32s) Loss: 0.0053(0.0049) Grad: 7621.7231  LR: 0.000008  \n","Epoch: [4][900/2850] Elapsed 4m 11s (remain 9m 3s) Loss: 0.0024(0.0048) Grad: 16218.9277  LR: 0.000007  \n","Epoch: [4][1000/2850] Elapsed 4m 38s (remain 8m 34s) Loss: 0.0050(0.0048) Grad: 18160.8770  LR: 0.000007  \n","Epoch: [4][1100/2850] Elapsed 5m 6s (remain 8m 6s) Loss: 0.0012(0.0049) Grad: 5255.8545  LR: 0.000007  \n","Epoch: [4][1200/2850] Elapsed 5m 33s (remain 7m 38s) Loss: 0.0004(0.0049) Grad: 3194.4614  LR: 0.000007  \n","Epoch: [4][1300/2850] Elapsed 6m 1s (remain 7m 9s) Loss: 0.0000(0.0047) Grad: 233.3343  LR: 0.000007  \n","Epoch: [4][1400/2850] Elapsed 6m 28s (remain 6m 41s) Loss: 0.0000(0.0046) Grad: 32.0381  LR: 0.000007  \n","Epoch: [4][1500/2850] Elapsed 6m 55s (remain 6m 13s) Loss: 0.0001(0.0047) Grad: 266.8103  LR: 0.000007  \n","Epoch: [4][1600/2850] Elapsed 7m 23s (remain 5m 45s) Loss: 0.0001(0.0048) Grad: 649.5535  LR: 0.000006  \n","Epoch: [4][1700/2850] Elapsed 7m 50s (remain 5m 17s) Loss: 0.0000(0.0047) Grad: 24.4067  LR: 0.000006  \n","Epoch: [4][1800/2850] Elapsed 8m 17s (remain 4m 50s) Loss: 0.0073(0.0047) Grad: 15609.4326  LR: 0.000006  \n","Epoch: [4][1900/2850] Elapsed 8m 45s (remain 4m 22s) Loss: 0.0026(0.0046) Grad: 7188.8545  LR: 0.000006  \n","Epoch: [4][2000/2850] Elapsed 9m 12s (remain 3m 54s) Loss: 0.0060(0.0047) Grad: 11415.8330  LR: 0.000006  \n","Epoch: [4][2100/2850] Elapsed 9m 40s (remain 3m 26s) Loss: 0.0143(0.0046) Grad: 143848.9062  LR: 0.000006  \n","Epoch: [4][2200/2850] Elapsed 10m 7s (remain 2m 59s) Loss: 0.0000(0.0046) Grad: 27.3793  LR: 0.000005  \n","Epoch: [4][2300/2850] Elapsed 10m 34s (remain 2m 31s) Loss: 0.0053(0.0047) Grad: 57095.3633  LR: 0.000005  \n","Epoch: [4][2400/2850] Elapsed 11m 2s (remain 2m 3s) Loss: 0.0031(0.0047) Grad: 16679.3652  LR: 0.000005  \n","Epoch: [4][2500/2850] Elapsed 11m 30s (remain 1m 36s) Loss: 0.0029(0.0047) Grad: 10501.0391  LR: 0.000005  \n","Epoch: [4][2600/2850] Elapsed 11m 57s (remain 1m 8s) Loss: 0.0018(0.0047) Grad: 10767.1133  LR: 0.000005  \n","Epoch: [4][2700/2850] Elapsed 12m 24s (remain 0m 41s) Loss: 0.0000(0.0047) Grad: 34.1560  LR: 0.000005  \n","Epoch: [4][2800/2850] Elapsed 12m 52s (remain 0m 13s) Loss: 0.0085(0.0047) Grad: 26090.4609  LR: 0.000005  \n","Epoch: [4][2849/2850] Elapsed 13m 5s (remain 0m 0s) Loss: 0.0000(0.0047) Grad: 996.4031  LR: 0.000004  \n","EVAL: [0/725] Elapsed 0m 0s (remain 5m 38s) Loss: 0.0250(0.0250) \n","EVAL: [100/725] Elapsed 0m 18s (remain 1m 52s) Loss: 0.0029(0.0105) \n","EVAL: [200/725] Elapsed 0m 35s (remain 1m 33s) Loss: 0.0120(0.0119) \n","EVAL: [300/725] Elapsed 0m 53s (remain 1m 15s) Loss: 0.0061(0.0102) \n","EVAL: [400/725] Elapsed 1m 10s (remain 0m 57s) Loss: 0.0481(0.0110) \n","EVAL: [500/725] Elapsed 1m 28s (remain 0m 39s) Loss: 0.0133(0.0110) \n","EVAL: [600/725] Elapsed 1m 46s (remain 0m 21s) Loss: 0.0111(0.0108) \n","EVAL: [700/725] Elapsed 2m 3s (remain 0m 4s) Loss: 0.0002(0.0098) \n","EVAL: [724/725] Elapsed 2m 7s (remain 0m 0s) Loss: 0.0169(0.0096) \n","Epoch 4 - avg_train_loss: 0.0047  avg_val_loss: 0.0096  time: 917s\n","Epoch 4 - Score: 0.8752\n","Epoch 4 - Save Best Score: 0.8752 Model\n","Epoch: [5][0/2850] Elapsed 0m 0s (remain 31m 2s) Loss: 0.0011(0.0011) Grad: 4020.7595  LR: 0.000004  \n","Epoch: [5][100/2850] Elapsed 0m 30s (remain 13m 46s) Loss: 0.0117(0.0027) Grad: 66016.7734  LR: 0.000004  \n","Epoch: [5][200/2850] Elapsed 0m 59s (remain 13m 2s) Loss: 0.0018(0.0025) Grad: 5680.8735  LR: 0.000004  \n","Epoch: [5][300/2850] Elapsed 1m 27s (remain 12m 17s) Loss: 0.0071(0.0027) Grad: 15829.0244  LR: 0.000004  \n","Epoch: [5][400/2850] Elapsed 1m 54s (remain 11m 38s) Loss: 0.0006(0.0031) Grad: 6400.5605  LR: 0.000004  \n","Epoch: [5][500/2850] Elapsed 2m 21s (remain 11m 4s) Loss: 0.0024(0.0032) Grad: 13102.5869  LR: 0.000004  \n","Epoch: [5][600/2850] Elapsed 2m 48s (remain 10m 32s) Loss: 0.0065(0.0033) Grad: 7405.3730  LR: 0.000004  \n","Epoch: [5][700/2850] Elapsed 3m 16s (remain 10m 1s) Loss: 0.0000(0.0036) Grad: 144.8436  LR: 0.000003  \n","Epoch: [5][800/2850] Elapsed 3m 43s (remain 9m 31s) Loss: 0.0000(0.0039) Grad: 98.8047  LR: 0.000003  \n","Epoch: [5][900/2850] Elapsed 4m 10s (remain 9m 2s) Loss: 0.0000(0.0039) Grad: 44.5454  LR: 0.000003  \n","Epoch: [5][1000/2850] Elapsed 4m 38s (remain 8m 33s) Loss: 0.0000(0.0039) Grad: 56.7720  LR: 0.000003  \n","Epoch: [5][1100/2850] Elapsed 5m 5s (remain 8m 5s) Loss: 0.0000(0.0038) Grad: 1677.5737  LR: 0.000003  \n","Epoch: [5][1200/2850] Elapsed 5m 32s (remain 7m 36s) Loss: 0.0026(0.0038) Grad: 6859.3062  LR: 0.000003  \n","Epoch: [5][1300/2850] Elapsed 6m 0s (remain 7m 8s) Loss: 0.0000(0.0038) Grad: 29.5373  LR: 0.000002  \n","Epoch: [5][1400/2850] Elapsed 6m 27s (remain 6m 40s) Loss: 0.0017(0.0040) Grad: 38235.8828  LR: 0.000002  \n","Epoch: [5][1500/2850] Elapsed 6m 54s (remain 6m 12s) Loss: 0.0000(0.0039) Grad: 33.8833  LR: 0.000002  \n","Epoch: [5][1600/2850] Elapsed 7m 22s (remain 5m 44s) Loss: 0.0126(0.0040) Grad: 24134.5742  LR: 0.000002  \n","Epoch: [5][1700/2850] Elapsed 7m 49s (remain 5m 17s) Loss: 0.0290(0.0041) Grad: 97854.1406  LR: 0.000002  \n","Epoch: [5][1800/2850] Elapsed 8m 16s (remain 4m 49s) Loss: 0.0083(0.0041) Grad: 49080.5898  LR: 0.000002  \n","Epoch: [5][1900/2850] Elapsed 8m 44s (remain 4m 21s) Loss: 0.0256(0.0042) Grad: 11517.7246  LR: 0.000001  \n","Epoch: [5][2000/2850] Elapsed 9m 11s (remain 3m 53s) Loss: 0.0000(0.0042) Grad: 145.9364  LR: 0.000001  \n","Epoch: [5][2100/2850] Elapsed 9m 38s (remain 3m 26s) Loss: 0.0000(0.0041) Grad: 43.4991  LR: 0.000001  \n","Epoch: [5][2200/2850] Elapsed 10m 6s (remain 2m 58s) Loss: 0.0000(0.0041) Grad: 65.7735  LR: 0.000001  \n","Epoch: [5][2300/2850] Elapsed 10m 33s (remain 2m 31s) Loss: 0.0002(0.0041) Grad: 2400.1392  LR: 0.000001  \n","Epoch: [5][2400/2850] Elapsed 11m 1s (remain 2m 3s) Loss: 0.0000(0.0041) Grad: 24.5527  LR: 0.000001  \n","Epoch: [5][2500/2850] Elapsed 11m 28s (remain 1m 36s) Loss: 0.0245(0.0040) Grad: 17617.2754  LR: 0.000001  \n","Epoch: [5][2600/2850] Elapsed 11m 55s (remain 1m 8s) Loss: 0.0003(0.0040) Grad: 1850.9912  LR: 0.000000  \n","Epoch: [5][2700/2850] Elapsed 12m 23s (remain 0m 41s) Loss: 0.0000(0.0040) Grad: 17.1558  LR: 0.000000  \n","Epoch: [5][2800/2850] Elapsed 12m 50s (remain 0m 13s) Loss: 0.0006(0.0040) Grad: 6645.9355  LR: 0.000000  \n","Epoch: [5][2849/2850] Elapsed 13m 3s (remain 0m 0s) Loss: 0.0021(0.0040) Grad: 51997.6523  LR: 0.000000  \n","EVAL: [0/725] Elapsed 0m 0s (remain 6m 4s) Loss: 0.0089(0.0089) \n","EVAL: [100/725] Elapsed 0m 17s (remain 1m 51s) Loss: 0.0023(0.0115) \n","EVAL: [200/725] Elapsed 0m 35s (remain 1m 32s) Loss: 0.0150(0.0131) \n","EVAL: [300/725] Elapsed 0m 52s (remain 1m 14s) Loss: 0.0042(0.0114) \n","EVAL: [400/725] Elapsed 1m 10s (remain 0m 56s) Loss: 0.0680(0.0123) \n","EVAL: [500/725] Elapsed 1m 27s (remain 0m 39s) Loss: 0.0189(0.0122) \n","EVAL: [600/725] Elapsed 1m 45s (remain 0m 21s) Loss: 0.0133(0.0121) \n","EVAL: [700/725] Elapsed 2m 2s (remain 0m 4s) Loss: 0.0000(0.0111) \n","EVAL: [724/725] Elapsed 2m 7s (remain 0m 0s) Loss: 0.0173(0.0109) \n","Epoch 5 - avg_train_loss: 0.0040  avg_val_loss: 0.0109  time: 915s\n","Epoch 5 - Score: 0.8751\n","Best thres: 0.5, Score: 0.8806\n","Best thres: 0.4666015625, Score: 0.8809\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"22f43d6b8c994773a844003a8381204d","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"103908b1f9c44f74a236034bc467e3ed","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a2b21e18492f4e45a4b210b07beb8934","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e98dfc73998740af9770e614420913f5","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from pretrained\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"040168b615644c17bc1eced828f836c6","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","\n"]}],"source":["if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","name":"nbme-exp021.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"papermill":{"default_parameters":{},"duration":641.572862,"end_time":"2022-02-27T11:39:50.972497","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-02-27T11:29:09.399635","version":"2.3.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9771bf3c1a0549f582d6382e402a6eda":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a6f56248343d4f949e74dfc6ac6255e8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f06dbf572a3a4f5b8aaf3ac42993c730","IPY_MODEL_53596cda0a8e4d5a80c8205230d21dcf","IPY_MODEL_8ee0c2578d41408c991cb8fb58068a8c"]}},"a6f56248343d4f949e74dfc6ac6255e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f06dbf572a3a4f5b8aaf3ac42993c730":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_67a63039cc734afc97c10f0b745adef3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_82ea708e5be245f88eee8dfc8bba268e"}},"53596cda0a8e4d5a80c8205230d21dcf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_eb67a778ba0046f0ab0d009f1043bba4","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":42146,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":42146,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_566ab2dc971b41189a8ab3e67ca2ca7b"}},"8ee0c2578d41408c991cb8fb58068a8c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7ce61b097fa745f780ce3272c8f25d19","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 42146/42146 [00:34&lt;00:00, 1937.53it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5277b94b3f9b4d81919118f3c6f17a88"}},"67a63039cc734afc97c10f0b745adef3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"82ea708e5be245f88eee8dfc8bba268e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"eb67a778ba0046f0ab0d009f1043bba4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"566ab2dc971b41189a8ab3e67ca2ca7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7ce61b097fa745f780ce3272c8f25d19":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5277b94b3f9b4d81919118f3c6f17a88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e6e0aab637d54747808a3b9e6ae839fb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_374e075cea454b6d85582ae160c08145","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_fefee9c4c04e4d318082e61d13f59bf1","IPY_MODEL_9bc87f90b3954f38b1b047209bb5b6a8","IPY_MODEL_1a4b56e532e148aea31e14aeb5b12789"]}},"374e075cea454b6d85582ae160c08145":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fefee9c4c04e4d318082e61d13f59bf1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_664689527d8b4021b63563511be585ae","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_622e4ebad3f3438b92d9608b02e7cea2"}},"9bc87f90b3954f38b1b047209bb5b6a8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_efa38883d7c4458d9f5d36e33ebc33d8","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":143,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":143,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_17c350bd64084c33baaee5c14fd78461"}},"1a4b56e532e148aea31e14aeb5b12789":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b59cb54c6d204b0c9a76d8346681a468","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 143/143 [00:00&lt;00:00, 2707.41it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dee6e951eae74d859829e4fd4902ef87"}},"664689527d8b4021b63563511be585ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"622e4ebad3f3438b92d9608b02e7cea2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"efa38883d7c4458d9f5d36e33ebc33d8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"17c350bd64084c33baaee5c14fd78461":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b59cb54c6d204b0c9a76d8346681a468":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"dee6e951eae74d859829e4fd4902ef87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2d624ee031534bcda734e02496078a16":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_07dd4e926a6a4902a296e1f49d960653","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0025679f30f14eae8cf1dc5ba7a3d0cf","IPY_MODEL_9b5704ce56db4c0fad11e555cc31c138","IPY_MODEL_a37c75cc4bcd4d1496439cab94d346fa"]}},"07dd4e926a6a4902a296e1f49d960653":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0025679f30f14eae8cf1dc5ba7a3d0cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_74b1db0352d642de89931603066de877","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5d539ad71f6f4fbcb972361d06f12ba7"}},"9b5704ce56db4c0fad11e555cc31c138":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_70b07b299ef848a0b1128ad5b9312a5d","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":873673253,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":873673253,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9ec1a62a7fc542f2a580850d4f11784e"}},"a37c75cc4bcd4d1496439cab94d346fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8941f98fdce5478c85a3071871b52299","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 833M/833M [00:16&lt;00:00, 56.2MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_32d65665b0ff4bd6bfed10e0aaaf4f49"}},"74b1db0352d642de89931603066de877":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5d539ad71f6f4fbcb972361d06f12ba7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"70b07b299ef848a0b1128ad5b9312a5d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9ec1a62a7fc542f2a580850d4f11784e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8941f98fdce5478c85a3071871b52299":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"32d65665b0ff4bd6bfed10e0aaaf4f49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"22f43d6b8c994773a844003a8381204d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c203051f4ade4dcb92facf6ea808af94","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_60d5cf602a6a409cb6e9c3305284526b","IPY_MODEL_b0b7de67fe9c487ea1f77dcba4b73aaf","IPY_MODEL_9bba4c8c91f74a9895404db4a405bfde"]}},"c203051f4ade4dcb92facf6ea808af94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"60d5cf602a6a409cb6e9c3305284526b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_28ccab34aa244e02bf17242eed9a5d1b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6013524181d44d038c2b8db40d11c36b"}},"b0b7de67fe9c487ea1f77dcba4b73aaf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_da4d29e8f5c144b18d7f510257cc8d23","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_146fa233e89643edb1fcc0ce5b7d91f6"}},"9bba4c8c91f74a9895404db4a405bfde":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d0191b7be9804b7bb89c86c0ba70c6d4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2/2 [00:01&lt;00:00,  1.25s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_96973aa8207c4502a0fd2475a3ef7df7"}},"28ccab34aa244e02bf17242eed9a5d1b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6013524181d44d038c2b8db40d11c36b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"da4d29e8f5c144b18d7f510257cc8d23":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"146fa233e89643edb1fcc0ce5b7d91f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d0191b7be9804b7bb89c86c0ba70c6d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"96973aa8207c4502a0fd2475a3ef7df7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"103908b1f9c44f74a236034bc467e3ed":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_974cfaef615e4c1eb712d202eaaadcc4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e387f43ccfa44a88b5c86347c6a2e81a","IPY_MODEL_7e3ef616abae415297b2e6f6d04eff04","IPY_MODEL_2ef885e81bd5458ba9c1de718ddb8583"]}},"974cfaef615e4c1eb712d202eaaadcc4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e387f43ccfa44a88b5c86347c6a2e81a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_dd8fe517cbc847129e37f3b7b6420d65","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_63c765ac577f41bbb80f181e7e7f3a09"}},"7e3ef616abae415297b2e6f6d04eff04":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c7b33dfc0955442bbce76fe0fe67302e","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f78d14ec18a4465baac5c0e238420b94"}},"2ef885e81bd5458ba9c1de718ddb8583":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_506c780410ce4660b1d03bf775bc0fcb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2/2 [00:01&lt;00:00,  1.29s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3d54f9bbdbc54794a537c3b43f3e191e"}},"dd8fe517cbc847129e37f3b7b6420d65":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"63c765ac577f41bbb80f181e7e7f3a09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c7b33dfc0955442bbce76fe0fe67302e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f78d14ec18a4465baac5c0e238420b94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"506c780410ce4660b1d03bf775bc0fcb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3d54f9bbdbc54794a537c3b43f3e191e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a2b21e18492f4e45a4b210b07beb8934":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_27c7ff08010e4199941161a8a5d75387","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_314f01b476f841faac8b43edc281bdf1","IPY_MODEL_af2b16b279e641a581b6c29353868205","IPY_MODEL_f2674a6b545e40f49fe4a1a58deabe93"]}},"27c7ff08010e4199941161a8a5d75387":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"314f01b476f841faac8b43edc281bdf1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_01268b4c5ef34a7eb9fa26afbea0d45c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7f571ce5ef3e4ad2a5bc4bcbf66e7532"}},"af2b16b279e641a581b6c29353868205":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8129bd51909d45d4a088b7edecd0b959","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_636522976ea3430e8ef91a5fe9b2cf8a"}},"f2674a6b545e40f49fe4a1a58deabe93":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1c03d37eba784be4bee78e53b3bdb7b5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2/2 [00:01&lt;00:00,  1.47s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_92bc6917133148cbbf87db00e7e40805"}},"01268b4c5ef34a7eb9fa26afbea0d45c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7f571ce5ef3e4ad2a5bc4bcbf66e7532":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8129bd51909d45d4a088b7edecd0b959":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"636522976ea3430e8ef91a5fe9b2cf8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1c03d37eba784be4bee78e53b3bdb7b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"92bc6917133148cbbf87db00e7e40805":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e98dfc73998740af9770e614420913f5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_73c2cf6cb2a74a4fb7a3b03e7cc3a9a7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e174ad484db142cfaf911a730cd97625","IPY_MODEL_066594bdb7c24c068613869d69aef40a","IPY_MODEL_99bfd37198fd43ffb6cc9b44341f6df8"]}},"73c2cf6cb2a74a4fb7a3b03e7cc3a9a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e174ad484db142cfaf911a730cd97625":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ae14cd4fbd614aaabc433aef06177d11","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f813077fe25d47909de1b563ea50573a"}},"066594bdb7c24c068613869d69aef40a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e46367582d194a96ac46adef6789dd09","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ada34b7955d04a21a02ba2d70c011d53"}},"99bfd37198fd43ffb6cc9b44341f6df8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8ec840ed29b843df975c6ffb4dab03cc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2/2 [00:02&lt;00:00,  1.70s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2519d45e725c48caae5d98f27136b1cc"}},"ae14cd4fbd614aaabc433aef06177d11":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f813077fe25d47909de1b563ea50573a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e46367582d194a96ac46adef6789dd09":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ada34b7955d04a21a02ba2d70c011d53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8ec840ed29b843df975c6ffb4dab03cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2519d45e725c48caae5d98f27136b1cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"040168b615644c17bc1eced828f836c6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4ce2358e15f047008f91bfe22e2178eb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_91d28013a9ff4c10af50749abb28af80","IPY_MODEL_ef4efd34557f42349460d46e9708deaa","IPY_MODEL_4f1a70d9da3542fbb64d346136eee3b6"]}},"4ce2358e15f047008f91bfe22e2178eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"91d28013a9ff4c10af50749abb28af80":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2d6a562dbdf64f528cc21a7e47077a86","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1de8e9089c8d4e62b34404544a604106"}},"ef4efd34557f42349460d46e9708deaa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ab88b315c4cf4b76956fb04662bf2531","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6fcdcd5071ee4411bcdc92fb2b95d99d"}},"4f1a70d9da3542fbb64d346136eee3b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_73860debd7834167b72d6902d4bebdf5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2/2 [00:02&lt;00:00,  2.04s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c591425dbc1447a988024edc28ae5071"}},"2d6a562dbdf64f528cc21a7e47077a86":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1de8e9089c8d4e62b34404544a604106":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ab88b315c4cf4b76956fb04662bf2531":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6fcdcd5071ee4411bcdc92fb2b95d99d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"73860debd7834167b72d6902d4bebdf5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c591425dbc1447a988024edc28ae5071":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":5}