{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "racial-reputation",
   "metadata": {
    "id": "national-fancy"
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-premium",
   "metadata": {
    "id": "copyrighted-centre"
   },
   "source": [
    "- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-specific",
   "metadata": {
    "id": "imported-offset"
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stylish-circuit",
   "metadata": {
    "id": "complimentary-wyoming"
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"nbme-exp057\"\n",
    "ENV = \"local\"\n",
    "DEBUG_MODE = False\n",
    "SUBMISSION_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "electoral-joint",
   "metadata": {
    "id": "allied-circuit"
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    env=ENV\n",
    "    exp_name=EXP_NAME\n",
    "    debug=DEBUG_MODE\n",
    "    submission=SUBMISSION_MODE\n",
    "    apex=True\n",
    "    input_dir=None\n",
    "    output_dir=None\n",
    "    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n",
    "    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n",
    "    competition_name=\"nbme-score-clinical-patient-notes\"\n",
    "    id_col=\"id\"\n",
    "    target_col=\"location\"\n",
    "    pretrained_model_name=\"microsoft/deberta-large\"\n",
    "    tokenizer=None\n",
    "    max_len=None\n",
    "    max_char_len=None\n",
    "    output_dim=1\n",
    "    dropout=0.2\n",
    "    num_workers=4\n",
    "    batch_size=3\n",
    "    lr=2e-5\n",
    "    betas=(0.9, 0.98)\n",
    "    weight_decay=0.1\n",
    "    num_warmup_steps_rate=0.1\n",
    "    batch_scheduler=True\n",
    "    epochs=5\n",
    "    n_fold=4\n",
    "    train_fold=[0, 1, 2, 3]\n",
    "    seed=71\n",
    "    gradient_accumulation_steps=2\n",
    "    max_grad_norm=1000\n",
    "    print_freq=100\n",
    "    train=True\n",
    "    inference=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "distinct-beverage",
   "metadata": {
    "id": "geographic-hindu"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.train_fold = [0, 1]\n",
    "\n",
    "if CFG.submission:\n",
    "    CFG.train = False\n",
    "    CFG.inference = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-position",
   "metadata": {
    "id": "confident-fifth"
   },
   "source": [
    "## Directory Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "automotive-insertion",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "miniature-greeting",
    "outputId": "03fb9617-56d5-4312-f5c1-1bceaf425898"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "print(CFG.env)\n",
    "if CFG.env == \"colab\":\n",
    "    # colab環境\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "    # install packages\n",
    "    !pip install transformers==4.16.2\n",
    "\n",
    "elif CFG.env == \"local\":\n",
    "    # ローカルサーバ\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n",
    "    if not CFG.output_dir.exists():\n",
    "        CFG.output_dir.mkdir()\n",
    "\n",
    "elif CFG.env == \"kaggle\":\n",
    "    # kaggle環境\n",
    "    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n",
    "    CFG.output_dir = Path(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "little-kazakhstan",
   "metadata": {
    "id": "guilty-filename"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import BartModel,BertModel,BertTokenizer\n",
    "from transformers import DebertaModel,DebertaTokenizer\n",
    "from transformers import RobertaModel,RobertaTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-reality",
   "metadata": {
    "id": "cubic-designation"
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "centered-sullivan",
   "metadata": {
    "id": "opposite-plasma"
   },
   "outputs": [],
   "source": [
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "literary-notebook",
   "metadata": {
    "id": "multiple-poland"
   },
   "outputs": [],
   "source": [
    "def create_labels_for_scoring(df):\n",
    "    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n",
    "    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, \"location\"]\n",
    "        if lst:\n",
    "            new_lst = \";\".join(lst)\n",
    "            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n",
    "\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df[\"location_for_create_labels\"].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, token_probs, tokenizer):\n",
    "    res = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n",
    "        encoded = tokenizer(\n",
    "            text=text,\n",
    "            max_length=CFG.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n",
    "            start, end = offset_mapping\n",
    "            res[i][start:end] = pred\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_predicted_location_str(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        # result = np.where(char_prob >= th)[0] + 1\n",
    "        result = np.where(char_prob >= th)[0]\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        # result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = [f\"{min(r)} {max(r) + 1}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def scoring(df, th=0.5, use_token_prob=True):\n",
    "    labels = create_labels_for_scoring(df)\n",
    "\n",
    "    if use_token_prob:\n",
    "        token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n",
    "        char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n",
    "    else:\n",
    "        char_probs = df[[str(i) for i in range(CFG.max_char_len)]].values\n",
    "        char_probs = [char_probs[i] for i in range(len(char_probs))]\n",
    "\n",
    "    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n",
    "    preds = get_predictions(predicted_location_str)\n",
    "\n",
    "    score = get_score(labels, preds)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_best_thres(oof_df):\n",
    "    def f1_opt(x):\n",
    "        return -1 * scoring(oof_df, th=x)\n",
    "\n",
    "    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n",
    "    return best_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "incoming-rally",
   "metadata": {
    "id": "seventh-fighter"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "polish-figure",
   "metadata": {
    "id": "fifty-boundary"
   },
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-catalog",
   "metadata": {
    "id": "unlimited-hotel"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "rough-snapshot",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "classical-machine",
    "outputId": "30445311-52c3-44d2-d180-408a3e077f39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 6), (143, 3), (42146, 3), (5, 4))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(CFG.input_dir / \"train.csv\")\n",
    "features = pd.read_csv(CFG.input_dir / \"features.csv\")\n",
    "patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n",
    "test = pd.read_csv(CFG.input_dir / \"test.csv\")\n",
    "\n",
    "train.shape, features.shape, patient_notes.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "forty-territory",
   "metadata": {
    "id": "vanilla-iceland"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n",
    "    print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-seafood",
   "metadata": {
    "id": "convenient-plant"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "indonesian-threat",
   "metadata": {
    "id": "convertible-thunder"
   },
   "outputs": [],
   "source": [
    "def preprocess_features(features):\n",
    "    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "\n",
    "\n",
    "features = preprocess_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "classical-biology",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "charitable-memphis",
    "outputId": "ee7ca20b-3e34-42c6-d5de-6981eca4c190"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14300, 8), (5, 6))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "educated-hotel",
   "metadata": {
    "id": "governing-election"
   },
   "outputs": [],
   "source": [
    "train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n",
    "train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "considerable-soundtrack",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "negative-provincial",
    "outputId": "96208ed1-8a21-4dbe-c61d-ab4cc6da8cb7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4399\n",
       "1    8181\n",
       "2    1296\n",
       "3     287\n",
       "4      99\n",
       "5      27\n",
       "6       9\n",
       "7       1\n",
       "8       1\n",
       "Name: annotation_length, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n",
    "display(train['annotation_length'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-travel",
   "metadata": {
    "id": "arbitrary-beatles"
   },
   "source": [
    "## CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "hired-alias",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "important-murray",
    "outputId": "ba3cb2b6-0bf9-421c-ee6d-cccd960b54dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    3575\n",
       "1    3575\n",
       "2    3575\n",
       "3    3575\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Fold = GroupKFold(n_splits=CFG.n_fold)\n",
    "groups = train['pn_num'].values\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-questionnaire",
   "metadata": {
    "id": "configured-chemistry"
   },
   "source": [
    "## Setup tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "supreme-mexican",
   "metadata": {
    "id": "hindu-contest"
   },
   "outputs": [],
   "source": [
    "if CFG.submission:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG.pretrained_model_name)\n",
    "    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n",
    "\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-wesley",
   "metadata": {
    "id": "alleged-protein"
   },
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "advisory-animation",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "d2031e5f3510429f977a8accb73623ad",
      "0f7365a2a54e4f3289867fc78fa5d8f0",
      "4ae813d03aee4dbc9ea70b16e867b8d7",
      "0acd16bc1b774b27b8a1d0027251caf2",
      "6441d4d15bb14c6798ef59080d3efd5b",
      "f59fe1bf5c8644d5a41d476dffa39d61",
      "91205e4ebe844c0c88f5d1b6731ff233",
      "ef1a0ffadb9e438fb08d33c0e093388f",
      "177a811a8f0d4c3abd9c5b034bd33556",
      "3ec95d8a55984afb8eb9efd55b921d80",
      "77726cf0c093411085c59646885d2b28"
     ]
    },
    "id": "composed-stroke",
    "outputId": "7cf0d5f9-4635-47d5-e099-53cc19801385"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8710658add9477bb6cbc75c7c3db2c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 433\n"
     ]
    }
   ],
   "source": [
    "pn_history_lengths = []\n",
    "tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    pn_history_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(pn_history_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "neutral-nation",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "ac2475c1acb4499eab3945dfa5c56779",
      "c7e5b695ef7c4acea0b1eb9bd2725a98",
      "0ba1e9bd83154395a37575d2e4a724f9",
      "a9ad179331184ee18e5ef2f6a09dc967",
      "a2a872834ce548658aa1f85dcfc27d48",
      "1a6e09e641e84d919d0ea487e738a6a9",
      "16ec8142da5f457490909b5d5ecc5a2c",
      "a0a0ea9a68804c459463520533a6ddb2",
      "243f2cea17124685b7275199d0197947",
      "9f0eeaa852284fae82e92f09f611c5f6",
      "b33cf6ce0b314dc1a403827b3c2f5125"
     ]
    },
    "id": "emotional-region",
    "outputId": "18a794fa-8050-4c31-fda3-d45fef59b785"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e058da8f11448db1cc829cb64348d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 30\n"
     ]
    }
   ],
   "source": [
    "feature_text_lengths = []\n",
    "tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    feature_text_lengths.append(length)\n",
    "\n",
    "print(\"max length:\", np.max(feature_text_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "informational-neighbor",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wrong-leisure",
    "outputId": "485a6e51-cb3f-480d-91ec-a10c185d0a55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 466\n"
     ]
    }
   ],
   "source": [
    "CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n",
    "\n",
    "print(\"max length:\", CFG.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sized-manufacturer",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "b7530e27be384f3fbdc650a521b80962",
      "7d615121ce48438084f98f9dc0c5b663",
      "44edeaff57294f3182ba2f00b3d379a1",
      "046b59dfab82487eaf5e1268f996b018",
      "9059fb0eb80642afbc8c4a6625c853df",
      "1e3f145cc1564614aba29374a43dfe45",
      "03f973838c924885b7600241d729d183",
      "752ddda179d24fd287fee23f274a1206",
      "e77baa3975c2484595060dbdcad867d4",
      "96f479f1c36143b5a0c16de2651c5ecb",
      "df31fc87a42b4f5199b5f4b5d02dd64c"
     ]
    },
    "id": "convenient-gospel",
    "outputId": "217cf962-28cd-4913-8566-56d3edefa9b4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c26d0b7aede42fe876ffe46299ae2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 950\n"
     ]
    }
   ],
   "source": [
    "pn_history_lengths = []\n",
    "tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n",
    "for text in tk0:\n",
    "    length = len(text)\n",
    "    pn_history_lengths.append(length)\n",
    "\n",
    "CFG.max_char_len = max(pn_history_lengths)\n",
    "\n",
    "print(\"max length:\", CFG.max_char_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "lightweight-attitude",
   "metadata": {
    "id": "representative-contributor"
   },
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.max_char_len = self.cfg.max_char_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "        self.annotation_lengths = self.df[\"annotation_length\"].values\n",
    "        self.locations = self.df[\"location\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def _create_mapping_from_token_to_char(self, pn_history):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        mapping_from_token_to_char = np.zeros(self.max_char_len)\n",
    "        offset_mapping = encoded[\"offset_mapping\"]\n",
    "        for i, offset in enumerate(offset_mapping):\n",
    "            start_idx, end_idx = offset\n",
    "            mapping_from_token_to_char[start_idx:end_idx] = i\n",
    "        return torch.tensor(mapping_from_token_to_char, dtype=torch.long)\n",
    "\n",
    "    def _create_label(self, pn_history, annotation_length, location_list):\n",
    "        label = np.zeros(self.max_char_len)\n",
    "        label[len(pn_history):] = -1\n",
    "        if annotation_length > 0:\n",
    "            for location in location_list:\n",
    "                for loc in [s.split() for s in location.split(\";\")]:\n",
    "                    start, end = int(loc[0]), int(loc[1])\n",
    "                    label[start:end] = 1\n",
    "        return torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n",
    "        mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n",
    "        return input_, label, mapping_from_token_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "institutional-spirit",
   "metadata": {
    "id": "decent-johnson"
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.max_len = self.cfg.max_len\n",
    "        self.feature_texts = self.df[\"feature_text\"].values\n",
    "        self.pn_historys = self.df[\"pn_history\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _create_input(self, pn_history, feature_text):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            text_pair=feature_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in encoded.items():\n",
    "            encoded[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "    def _create_mapping_from_token_to_char(self, pn_history):\n",
    "        encoded = self.tokenizer(\n",
    "            text=pn_history,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        mapping_from_token_to_char = np.zeros(self.max_char_len)\n",
    "        offset_mapping = encoded[\"offset_mapping\"]\n",
    "        for i, offset in enumerate(offset_mapping):\n",
    "            start_idx, end_idx = offset\n",
    "            mapping_from_token_to_char[start_idx:end_idx] = i\n",
    "        return torch.tensor(mapping_from_token_to_char, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n",
    "        mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n",
    "        return input_, mapping_from_token_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-operation",
   "metadata": {
    "id": "arctic-joint"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "unique-siemens",
   "metadata": {
    "id": "UtM7nYFm333y"
   },
   "outputs": [],
   "source": [
    "class Exp054Model(nn.Module):\n",
    "    def __init__(self, cfg, model_config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if model_config_path is None:\n",
    "            self.model_config = AutoConfig.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "        else:\n",
    "            self.model_config = torch.load(model_config_path)\n",
    "\n",
    "        if pretrained:\n",
    "            self.backbone = AutoModel.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                config=self.model_config,\n",
    "            )\n",
    "            print(f\"Load weight from pretrained\")\n",
    "        else:\n",
    "            #self.backbone = AutoModel.from_config(self.model_config)\n",
    "            itpt = AutoModelForMaskedLM.from_config(self.model_config)\n",
    "            # path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp010/checkpoint-130170/pytorch_model.bin\")\n",
    "            path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n",
    "            state_dict = torch.load(path)\n",
    "            itpt.load_state_dict(state_dict)\n",
    "            self.backbone = itpt.deberta\n",
    "            print(f\"Load weight from {path}\")\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(self.cfg.dropout),\n",
    "            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h = self.backbone(**inputs)[\"last_hidden_state\"]\n",
    "        output = self.fc(h)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "engaged-typing",
   "metadata": {
    "id": "alternative-malawi"
   },
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, model_config_path=None, pretrained=False, i_fold=None):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if model_config_path is None:\n",
    "            self.model_config = AutoConfig.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "        else:\n",
    "            self.model_config = torch.load(model_config_path)\n",
    "\n",
    "        if pretrained:\n",
    "            self.backbone = AutoModel.from_pretrained(\n",
    "                self.cfg.pretrained_model_name,\n",
    "                config=self.model_config,\n",
    "            )\n",
    "            print(f\"Load weight from pretrained\")\n",
    "        else:\n",
    "            #self.backbone = AutoModel.from_config(self.model_config)\n",
    "\n",
    "            model = Exp054Model(cfg, model_config_path=None, pretrained=False)\n",
    "            # path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp054\" /  f\"fold{i_fold}_best.pth\")\n",
    "            path = str(Path(\"../output\") / CFG.competition_name /  \"nbme-exp054\" /  f\"fold{i_fold}_best.pth\")\n",
    "            state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "            model.load_state_dict(state[\"model\"])\n",
    "            self.backbone = model.backbone\n",
    "            print(f\"Load weight from {path}\")\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.model_config.hidden_size,\n",
    "            bidirectional=True,\n",
    "            hidden_size=self.model_config.hidden_size // 2,\n",
    "            num_layers=2,\n",
    "            dropout=self.cfg.dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(self.cfg.dropout),\n",
    "            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, mappings_from_token_to_char):\n",
    "        h = self.backbone(**inputs)[\"last_hidden_state\"]  # [batch, seq_len, d_model]\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.unsqueeze(2).expand(-1, -1, self.model_config.hidden_size)\n",
    "        h = torch.gather(h, 1, mappings_from_token_to_char)    # [batch, seq_len, d_model]\n",
    "        h, _ = self.lstm(h)\n",
    "        output = self.fc(h)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-civilization",
   "metadata": {
    "id": "therapeutic-assembly"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "psychological-savage",
   "metadata": {
    "id": "going-conversion"
   },
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    train_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    epoch,\n",
    "    scheduler,\n",
    "    device,\n",
    "):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels, mappings_from_token_to_char) in enumerate(train_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device) \n",
    "        batch_size = labels.size(0)\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            output = model(inputs, mappings_from_token_to_char)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if CFG.batch_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n",
    "            print(\n",
    "                \"Epoch: [{0}][{1}/{2}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                \"Grad: {grad_norm:.4f}  \"\n",
    "                \"LR: {lr:.6f}  \"\n",
    "                .format(\n",
    "                    epoch+1,\n",
    "                    step,\n",
    "                    len(train_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n",
    "                    loss=losses,\n",
    "                     grad_norm=grad_norm,\n",
    "                     lr=scheduler.get_lr()[0],\n",
    "                )\n",
    "            )\n",
    "\n",
    "    del output, loss, scaler, grad_norm; gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "innocent-surfing",
   "metadata": {
    "id": "alleged-commonwealth"
   },
   "outputs": [],
   "source": [
    "def valid_fn(\n",
    "    val_dataloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    device,\n",
    "):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    losses = AverageMeter()\n",
    "    start = time.time()\n",
    "    for step, (inputs, labels, mappings_from_token_to_char) in enumerate(val_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device) \n",
    "        batch_size = labels.size(0)\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            output = model(inputs, mappings_from_token_to_char)\n",
    "\n",
    "        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n",
    "        loss = loss.mean()\n",
    "    \n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n",
    "            print(\n",
    "                \"EVAL: [{0}/{1}] \"\n",
    "                \"Elapsed {remain:s} \"\n",
    "                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                .format(\n",
    "                    step, len(val_dataloader),\n",
    "                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n",
    "                    loss=losses,\n",
    "                )\n",
    "            )\n",
    "    preds = np.concatenate(preds)\n",
    "    return losses.avg, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "correct-overall",
   "metadata": {
    "id": "middle-determination"
   },
   "outputs": [],
   "source": [
    "def inference_fn(test_dataloader, model, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preds = []\n",
    "    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n",
    "    for (inputs, mappings_from_token_to_char) in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs, mappings_from_token_to_char)\n",
    "        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "authentic-activation",
   "metadata": {
    "id": "familiar-participation"
   },
   "outputs": [],
   "source": [
    "def train_loop(df, i_fold, device):\n",
    "    print(f\"========== fold: {i_fold} training ==========\")\n",
    "    train_idx = df[df[\"fold\"] != i_fold].index\n",
    "    val_idx = df[df[\"fold\"] == i_fold].index\n",
    "\n",
    "    train_folds = df.loc[train_idx].reset_index(drop=True)\n",
    "    val_folds = df.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = TrainingDataset(CFG, train_folds)\n",
    "    val_dataset = TrainingDataset(CFG, val_folds)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "    model = CustomModel(CFG, model_config_path=None, pretrained=False, i_fold=i_fold)   # itptを使うため\n",
    "    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n",
    "    model.to(device)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in param_optimizer if not any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n",
    "        {\"params\": [p for n, p in param_optimizer if any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=CFG.lr,\n",
    "        betas=CFG.betas,\n",
    "        weight_decay=CFG.weight_decay,\n",
    "    )\n",
    "    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n",
    "    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_train_optimization_steps,\n",
    "    )\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    best_score = -1 * np.inf\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "        avg_loss = train_fn(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            scheduler,\n",
    "            device,\n",
    "        )\n",
    "        avg_val_loss, val_preds = valid_fn(\n",
    "            val_dataloader,\n",
    "            model,\n",
    "            criterion,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "\n",
    "        # scoring\n",
    "        val_folds[[str(i) for i in range(CFG.max_char_len)]] = val_preds\n",
    "        score = scoring(val_folds, th=0.5, use_token_prob=False)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n",
    "        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"predictions\": val_preds,\n",
    "                },\n",
    "                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "            )\n",
    "\n",
    "    predictions = torch.load(\n",
    "        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )[\"predictions\"]\n",
    "    val_folds[[str(i) for i in range(CFG.max_char_len)]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return val_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-congo",
   "metadata": {
    "id": "coated-cameroon"
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bacterial-leader",
   "metadata": {
    "id": "quality-expansion"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for i_fold in range(CFG.n_fold):\n",
    "            if i_fold in CFG.train_fold:\n",
    "                _oof_df = train_loop(train, i_fold, device)\n",
    "                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n",
    "        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    if CFG.submission:\n",
    "        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n",
    "    else:\n",
    "        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n",
    "\n",
    "    score = scoring(oof_df, th=0.5, use_token_prob=False)\n",
    "    print(f\"Best thres: 0.5, Score: {score:.4f}\")\n",
    "    best_thres = get_best_thres(oof_df)\n",
    "    score = scoring(oof_df, th=best_thres, use_token_prob=False)\n",
    "    print(f\"Best thres: {best_thres}, Score: {score:.4f}\")\n",
    "\n",
    "    if CFG.inference:\n",
    "        test_dataset = TestDataset(CFG, test)\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=CFG.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=CFG.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        predictions = []\n",
    "        for i_fold in CFG.train_fold:\n",
    "            if CFG.submission:\n",
    "                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n",
    "                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n",
    "            else:\n",
    "                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n",
    "                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n",
    "\n",
    "            state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "            model.load_state_dict(state[\"model\"])\n",
    "            test_token_probs = inference_fn(test_dataloader, model, device)\n",
    "            test[[f\"fold{i_fold}_{i}\" for i in range(CFG.max_len)]] = test_token_probs\n",
    "            test_char_probs = get_char_probs(test[\"pn_history\"].values, test_token_probs, CFG.tokenizer)\n",
    "            predictions.append(test_char_probs)\n",
    "\n",
    "            del state, test_token_probs, model; gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        predictions = np.mean(predictions, axis=0)\n",
    "        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n",
    "        test[CFG.target_col] = predicted_location_str\n",
    "        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n",
    "        test[[CFG.id_col, CFG.target_col]].to_csv(\n",
    "            CFG.output_dir / \"submission.csv\", index=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "impossible-polymer",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "proprietary-civilian",
    "outputId": "14f4e231-9d3a-4547-82a1-aa9c03b642c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp054/fold0_best.pth\n",
      "Epoch: [1][0/3575] Elapsed 0m 1s (remain 61m 32s) Loss: 0.3467(0.3467) Grad: 91276.5000  LR: 0.000000  \n",
      "Epoch: [1][100/3575] Elapsed 1m 2s (remain 35m 34s) Loss: 0.2995(0.3315) Grad: 82714.9844  LR: 0.000001  \n",
      "Epoch: [1][200/3575] Elapsed 2m 2s (remain 34m 9s) Loss: 0.1878(0.2883) Grad: 56592.0039  LR: 0.000002  \n",
      "Epoch: [1][300/3575] Elapsed 3m 2s (remain 33m 0s) Loss: 0.0811(0.2354) Grad: 31200.6582  LR: 0.000003  \n",
      "Epoch: [1][400/3575] Elapsed 4m 3s (remain 32m 4s) Loss: 0.0289(0.1902) Grad: 13131.7627  LR: 0.000004  \n",
      "Epoch: [1][500/3575] Elapsed 5m 3s (remain 30m 59s) Loss: 0.0099(0.1561) Grad: 4199.2188  LR: 0.000006  \n",
      "Epoch: [1][600/3575] Elapsed 6m 3s (remain 29m 57s) Loss: 0.0048(0.1315) Grad: 2136.0283  LR: 0.000007  \n",
      "Epoch: [1][700/3575] Elapsed 7m 3s (remain 28m 57s) Loss: 0.0018(0.1136) Grad: 760.1451  LR: 0.000008  \n",
      "Epoch: [1][800/3575] Elapsed 8m 4s (remain 27m 58s) Loss: 0.0008(0.1001) Grad: 472.2552  LR: 0.000009  \n",
      "Epoch: [1][900/3575] Elapsed 9m 4s (remain 26m 55s) Loss: 0.0033(0.0895) Grad: 899.1988  LR: 0.000010  \n",
      "Epoch: [1][1000/3575] Elapsed 10m 3s (remain 25m 51s) Loss: 0.0009(0.0809) Grad: 1306.3774  LR: 0.000011  \n",
      "Epoch: [1][1100/3575] Elapsed 11m 3s (remain 24m 50s) Loss: 0.0027(0.0740) Grad: 77582.3359  LR: 0.000012  \n",
      "Epoch: [1][1200/3575] Elapsed 12m 2s (remain 23m 48s) Loss: 0.0005(0.0682) Grad: 577.1127  LR: 0.000013  \n",
      "Epoch: [1][1300/3575] Elapsed 13m 2s (remain 22m 47s) Loss: 0.0013(0.0632) Grad: 24248.7598  LR: 0.000015  \n",
      "Epoch: [1][1400/3575] Elapsed 14m 2s (remain 21m 48s) Loss: 0.0000(0.0591) Grad: 42.5429  LR: 0.000016  \n",
      "Epoch: [1][1500/3575] Elapsed 15m 2s (remain 20m 47s) Loss: 0.0005(0.0556) Grad: 728.3043  LR: 0.000017  \n",
      "Epoch: [1][1600/3575] Elapsed 16m 3s (remain 19m 47s) Loss: 0.0002(0.0523) Grad: 105.7123  LR: 0.000018  \n",
      "Epoch: [1][1700/3575] Elapsed 17m 4s (remain 18m 48s) Loss: 0.0267(0.0496) Grad: 76295.0625  LR: 0.000019  \n",
      "Epoch: [1][1800/3575] Elapsed 18m 2s (remain 17m 46s) Loss: 0.0095(0.0471) Grad: 15786.0801  LR: 0.000020  \n",
      "Epoch: [1][1900/3575] Elapsed 19m 0s (remain 16m 44s) Loss: 0.0863(0.0449) Grad: 139610.9375  LR: 0.000020  \n",
      "Epoch: [1][2000/3575] Elapsed 19m 58s (remain 15m 42s) Loss: 0.0001(0.0429) Grad: 27.1687  LR: 0.000020  \n",
      "Epoch: [1][2100/3575] Elapsed 20m 56s (remain 14m 41s) Loss: 0.0011(0.0410) Grad: 4009.3760  LR: 0.000020  \n",
      "Epoch: [1][2200/3575] Elapsed 21m 54s (remain 13m 40s) Loss: 0.0078(0.0394) Grad: 6730.9702  LR: 0.000019  \n",
      "Epoch: [1][2300/3575] Elapsed 22m 52s (remain 12m 39s) Loss: 0.0008(0.0379) Grad: 11109.8564  LR: 0.000019  \n",
      "Epoch: [1][2400/3575] Elapsed 23m 50s (remain 11m 39s) Loss: 0.0000(0.0365) Grad: 15.7854  LR: 0.000019  \n",
      "Epoch: [1][2500/3575] Elapsed 24m 48s (remain 10m 39s) Loss: 0.0002(0.0352) Grad: 861.5886  LR: 0.000019  \n",
      "Epoch: [1][2600/3575] Elapsed 25m 48s (remain 9m 39s) Loss: 0.0003(0.0340) Grad: 501.0568  LR: 0.000019  \n",
      "Epoch: [1][2700/3575] Elapsed 26m 48s (remain 8m 40s) Loss: 0.0002(0.0329) Grad: 173.2725  LR: 0.000019  \n",
      "Epoch: [1][2800/3575] Elapsed 27m 48s (remain 7m 41s) Loss: 0.0207(0.0319) Grad: 38826.3633  LR: 0.000019  \n",
      "Epoch: [1][2900/3575] Elapsed 28m 49s (remain 6m 41s) Loss: 0.0002(0.0311) Grad: 93.6648  LR: 0.000019  \n",
      "Epoch: [1][3000/3575] Elapsed 29m 50s (remain 5m 42s) Loss: 0.0001(0.0302) Grad: 40.3885  LR: 0.000018  \n",
      "Epoch: [1][3100/3575] Elapsed 30m 49s (remain 4m 42s) Loss: 0.0008(0.0294) Grad: 977.6445  LR: 0.000018  \n",
      "Epoch: [1][3200/3575] Elapsed 31m 49s (remain 3m 43s) Loss: 0.0000(0.0286) Grad: 11.6082  LR: 0.000018  \n",
      "Epoch: [1][3300/3575] Elapsed 32m 47s (remain 2m 43s) Loss: 0.0072(0.0279) Grad: 2277.1111  LR: 0.000018  \n",
      "Epoch: [1][3400/3575] Elapsed 33m 45s (remain 1m 43s) Loss: 0.0034(0.0273) Grad: 46242.1406  LR: 0.000018  \n",
      "Epoch: [1][3500/3575] Elapsed 34m 43s (remain 0m 44s) Loss: 0.0047(0.0266) Grad: 7124.9058  LR: 0.000018  \n",
      "Epoch: [1][3574/3575] Elapsed 35m 27s (remain 0m 0s) Loss: 0.0005(0.0262) Grad: 127.5856  LR: 0.000018  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 10m 34s) Loss: 0.0003(0.0003) \n",
      "EVAL: [100/1192] Elapsed 0m 19s (remain 3m 34s) Loss: 0.0361(0.0086) \n",
      "EVAL: [200/1192] Elapsed 0m 38s (remain 3m 9s) Loss: 0.0197(0.0085) \n",
      "EVAL: [300/1192] Elapsed 0m 56s (remain 2m 48s) Loss: 0.0116(0.0087) \n",
      "EVAL: [400/1192] Elapsed 1m 15s (remain 2m 28s) Loss: 0.0044(0.0088) \n",
      "EVAL: [500/1192] Elapsed 1m 34s (remain 2m 9s) Loss: 0.0138(0.0083) \n",
      "EVAL: [600/1192] Elapsed 1m 53s (remain 1m 51s) Loss: 0.0053(0.0087) \n",
      "EVAL: [700/1192] Elapsed 2m 12s (remain 1m 32s) Loss: 0.1014(0.0106) \n",
      "EVAL: [800/1192] Elapsed 2m 30s (remain 1m 13s) Loss: 0.0051(0.0110) \n",
      "EVAL: [900/1192] Elapsed 2m 49s (remain 0m 54s) Loss: 0.0075(0.0109) \n",
      "EVAL: [1000/1192] Elapsed 3m 8s (remain 0m 35s) Loss: 0.0000(0.0110) \n",
      "EVAL: [1100/1192] Elapsed 3m 27s (remain 0m 17s) Loss: 0.0055(0.0106) \n",
      "EVAL: [1191/1192] Elapsed 3m 44s (remain 0m 0s) Loss: 0.0001(0.0103) \n",
      "Epoch 1 - avg_train_loss: 0.0262  avg_val_loss: 0.0103  time: 2356s\n",
      "Epoch 1 - Score: 0.8669\n",
      "Epoch 1 - Save Best Score: 0.8669 Model\n",
      "Epoch: [2][0/3575] Elapsed 0m 1s (remain 61m 44s) Loss: 0.0048(0.0048) Grad: 52526.2930  LR: 0.000018  \n",
      "Epoch: [2][100/3575] Elapsed 1m 0s (remain 34m 50s) Loss: 0.0425(0.0046) Grad: 12748.5693  LR: 0.000018  \n",
      "Epoch: [2][200/3575] Elapsed 2m 0s (remain 33m 42s) Loss: 0.0001(0.0040) Grad: 371.5692  LR: 0.000018  \n",
      "Epoch: [2][300/3575] Elapsed 2m 58s (remain 32m 25s) Loss: 0.0001(0.0041) Grad: 67.2624  LR: 0.000017  \n",
      "Epoch: [2][400/3575] Elapsed 3m 57s (remain 31m 16s) Loss: 0.0007(0.0041) Grad: 5067.9805  LR: 0.000017  \n",
      "Epoch: [2][500/3575] Elapsed 4m 55s (remain 30m 13s) Loss: 0.0220(0.0041) Grad: 42571.5664  LR: 0.000017  \n",
      "Epoch: [2][600/3575] Elapsed 5m 53s (remain 29m 11s) Loss: 0.0094(0.0041) Grad: 8549.6191  LR: 0.000017  \n",
      "Epoch: [2][700/3575] Elapsed 6m 52s (remain 28m 12s) Loss: 0.0001(0.0038) Grad: 89.1031  LR: 0.000017  \n",
      "Epoch: [2][800/3575] Elapsed 7m 52s (remain 27m 15s) Loss: 0.0149(0.0039) Grad: 10691.3281  LR: 0.000017  \n",
      "Epoch: [2][900/3575] Elapsed 8m 51s (remain 26m 18s) Loss: 0.0001(0.0039) Grad: 955.6281  LR: 0.000017  \n",
      "Epoch: [2][1000/3575] Elapsed 9m 50s (remain 25m 19s) Loss: 0.0066(0.0038) Grad: 89595.8047  LR: 0.000017  \n",
      "Epoch: [2][1100/3575] Elapsed 10m 49s (remain 24m 18s) Loss: 0.0165(0.0038) Grad: 63982.2969  LR: 0.000016  \n",
      "Epoch: [2][1200/3575] Elapsed 11m 47s (remain 23m 19s) Loss: 0.0001(0.0038) Grad: 72.6964  LR: 0.000016  \n",
      "Epoch: [2][1300/3575] Elapsed 12m 46s (remain 22m 19s) Loss: 0.0001(0.0039) Grad: 191.6036  LR: 0.000016  \n",
      "Epoch: [2][1400/3575] Elapsed 13m 45s (remain 21m 21s) Loss: 0.0001(0.0038) Grad: 250.9737  LR: 0.000016  \n",
      "Epoch: [2][1500/3575] Elapsed 14m 44s (remain 20m 21s) Loss: 0.0000(0.0038) Grad: 21.0965  LR: 0.000016  \n",
      "Epoch: [2][1600/3575] Elapsed 15m 42s (remain 19m 22s) Loss: 0.0001(0.0038) Grad: 3526.8135  LR: 0.000016  \n",
      "Epoch: [2][1700/3575] Elapsed 16m 42s (remain 18m 24s) Loss: 0.0000(0.0037) Grad: 54.4954  LR: 0.000016  \n",
      "Epoch: [2][1800/3575] Elapsed 17m 41s (remain 17m 25s) Loss: 0.0000(0.0038) Grad: 32.5330  LR: 0.000016  \n",
      "Epoch: [2][1900/3575] Elapsed 18m 40s (remain 16m 26s) Loss: 0.0001(0.0038) Grad: 63.7915  LR: 0.000015  \n",
      "Epoch: [2][2000/3575] Elapsed 19m 38s (remain 15m 26s) Loss: 0.0000(0.0039) Grad: 26.4677  LR: 0.000015  \n",
      "Epoch: [2][2100/3575] Elapsed 20m 38s (remain 14m 28s) Loss: 0.0017(0.0039) Grad: 11993.8516  LR: 0.000015  \n",
      "Epoch: [2][2200/3575] Elapsed 21m 36s (remain 13m 29s) Loss: 0.0010(0.0038) Grad: 4155.4531  LR: 0.000015  \n",
      "Epoch: [2][2300/3575] Elapsed 22m 35s (remain 12m 30s) Loss: 0.0135(0.0039) Grad: 107399.2188  LR: 0.000015  \n",
      "Epoch: [2][2400/3575] Elapsed 23m 33s (remain 11m 31s) Loss: 0.0000(0.0038) Grad: 92.4898  LR: 0.000015  \n",
      "Epoch: [2][2500/3575] Elapsed 24m 32s (remain 10m 32s) Loss: 0.0000(0.0038) Grad: 54.5206  LR: 0.000015  \n",
      "Epoch: [2][2600/3575] Elapsed 25m 30s (remain 9m 33s) Loss: 0.0186(0.0039) Grad: 56405.2930  LR: 0.000015  \n",
      "Epoch: [2][2700/3575] Elapsed 26m 30s (remain 8m 34s) Loss: 0.0004(0.0039) Grad: 4715.4863  LR: 0.000014  \n",
      "Epoch: [2][2800/3575] Elapsed 27m 28s (remain 7m 35s) Loss: 0.0004(0.0039) Grad: 2669.6355  LR: 0.000014  \n",
      "Epoch: [2][2900/3575] Elapsed 28m 26s (remain 6m 36s) Loss: 0.0000(0.0038) Grad: 11.5564  LR: 0.000014  \n",
      "Epoch: [2][3000/3575] Elapsed 29m 24s (remain 5m 37s) Loss: 0.0000(0.0038) Grad: 129.9689  LR: 0.000014  \n",
      "Epoch: [2][3100/3575] Elapsed 30m 22s (remain 4m 38s) Loss: 0.0005(0.0038) Grad: 1847.8730  LR: 0.000014  \n",
      "Epoch: [2][3200/3575] Elapsed 31m 21s (remain 3m 39s) Loss: 0.0003(0.0038) Grad: 2084.7671  LR: 0.000014  \n",
      "Epoch: [2][3300/3575] Elapsed 32m 21s (remain 2m 41s) Loss: 0.0264(0.0037) Grad: 123399.3906  LR: 0.000014  \n",
      "Epoch: [2][3400/3575] Elapsed 33m 19s (remain 1m 42s) Loss: 0.0000(0.0037) Grad: 42.8915  LR: 0.000014  \n",
      "Epoch: [2][3500/3575] Elapsed 34m 17s (remain 0m 43s) Loss: 0.0001(0.0038) Grad: 517.8484  LR: 0.000013  \n",
      "Epoch: [2][3574/3575] Elapsed 35m 0s (remain 0m 0s) Loss: 0.0011(0.0038) Grad: 13889.8535  LR: 0.000013  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 12m 13s) Loss: 0.0001(0.0001) \n",
      "EVAL: [100/1192] Elapsed 0m 20s (remain 3m 36s) Loss: 0.0248(0.0068) \n",
      "EVAL: [200/1192] Elapsed 0m 39s (remain 3m 12s) Loss: 0.0069(0.0080) \n",
      "EVAL: [300/1192] Elapsed 0m 58s (remain 2m 52s) Loss: 0.0033(0.0082) \n",
      "EVAL: [400/1192] Elapsed 1m 17s (remain 2m 32s) Loss: 0.0043(0.0082) \n",
      "EVAL: [500/1192] Elapsed 1m 35s (remain 2m 12s) Loss: 0.0090(0.0079) \n",
      "EVAL: [600/1192] Elapsed 1m 54s (remain 1m 52s) Loss: 0.0057(0.0080) \n",
      "EVAL: [700/1192] Elapsed 2m 13s (remain 1m 33s) Loss: 0.0811(0.0099) \n",
      "EVAL: [800/1192] Elapsed 2m 32s (remain 1m 14s) Loss: 0.0007(0.0102) \n",
      "EVAL: [900/1192] Elapsed 2m 51s (remain 0m 55s) Loss: 0.0007(0.0102) \n",
      "EVAL: [1000/1192] Elapsed 3m 10s (remain 0m 36s) Loss: 0.0000(0.0101) \n",
      "EVAL: [1100/1192] Elapsed 3m 30s (remain 0m 17s) Loss: 0.0005(0.0097) \n",
      "EVAL: [1191/1192] Elapsed 3m 47s (remain 0m 0s) Loss: 0.0000(0.0093) \n",
      "Epoch 2 - avg_train_loss: 0.0038  avg_val_loss: 0.0093  time: 2333s\n",
      "Epoch 2 - Score: 0.8774\n",
      "Epoch 2 - Save Best Score: 0.8774 Model\n",
      "Epoch: [3][0/3575] Elapsed 0m 1s (remain 61m 32s) Loss: 0.0001(0.0001) Grad: 1004.0559  LR: 0.000013  \n",
      "Epoch: [3][100/3575] Elapsed 0m 59s (remain 33m 51s) Loss: 0.0000(0.0016) Grad: 28.4557  LR: 0.000013  \n",
      "Epoch: [3][200/3575] Elapsed 1m 57s (remain 32m 45s) Loss: 0.0521(0.0022) Grad: 211200.9375  LR: 0.000013  \n",
      "Epoch: [3][300/3575] Elapsed 2m 56s (remain 32m 2s) Loss: 0.0035(0.0026) Grad: 3537.3440  LR: 0.000013  \n",
      "Epoch: [3][400/3575] Elapsed 3m 55s (remain 31m 6s) Loss: 0.0000(0.0027) Grad: 11.1619  LR: 0.000013  \n",
      "Epoch: [3][500/3575] Elapsed 4m 55s (remain 30m 10s) Loss: 0.0037(0.0026) Grad: 14893.1309  LR: 0.000013  \n",
      "Epoch: [3][600/3575] Elapsed 5m 54s (remain 29m 12s) Loss: 0.0022(0.0028) Grad: 27159.2539  LR: 0.000013  \n",
      "Epoch: [3][700/3575] Elapsed 6m 52s (remain 28m 11s) Loss: 0.0003(0.0027) Grad: 386.6911  LR: 0.000012  \n",
      "Epoch: [3][800/3575] Elapsed 7m 51s (remain 27m 13s) Loss: 0.0000(0.0026) Grad: 4.9305  LR: 0.000012  \n",
      "Epoch: [3][900/3575] Elapsed 8m 51s (remain 26m 16s) Loss: 0.0000(0.0026) Grad: 52.3809  LR: 0.000012  \n",
      "Epoch: [3][1000/3575] Elapsed 9m 50s (remain 25m 17s) Loss: 0.0000(0.0026) Grad: 59.0269  LR: 0.000012  \n",
      "Epoch: [3][1100/3575] Elapsed 10m 47s (remain 24m 16s) Loss: 0.0004(0.0025) Grad: 1256.6793  LR: 0.000012  \n",
      "Epoch: [3][1200/3575] Elapsed 11m 46s (remain 23m 15s) Loss: 0.0000(0.0026) Grad: 13.9102  LR: 0.000012  \n",
      "Epoch: [3][1300/3575] Elapsed 12m 45s (remain 22m 17s) Loss: 0.0000(0.0026) Grad: 2.5400  LR: 0.000012  \n",
      "Epoch: [3][1400/3575] Elapsed 13m 43s (remain 21m 17s) Loss: 0.0000(0.0026) Grad: 10.3554  LR: 0.000012  \n",
      "Epoch: [3][1500/3575] Elapsed 14m 41s (remain 20m 18s) Loss: 0.0007(0.0026) Grad: 15843.4707  LR: 0.000011  \n",
      "Epoch: [3][1600/3575] Elapsed 15m 41s (remain 19m 21s) Loss: 0.0000(0.0026) Grad: 12.5291  LR: 0.000011  \n",
      "Epoch: [3][1700/3575] Elapsed 16m 40s (remain 18m 22s) Loss: 0.0000(0.0026) Grad: 78.6032  LR: 0.000011  \n",
      "Epoch: [3][1800/3575] Elapsed 17m 38s (remain 17m 22s) Loss: 0.0000(0.0026) Grad: 49.7078  LR: 0.000011  \n",
      "Epoch: [3][1900/3575] Elapsed 18m 36s (remain 16m 23s) Loss: 0.0000(0.0026) Grad: 18.4292  LR: 0.000011  \n",
      "Epoch: [3][2000/3575] Elapsed 19m 35s (remain 15m 24s) Loss: 0.0001(0.0027) Grad: 221.9262  LR: 0.000011  \n",
      "Epoch: [3][2100/3575] Elapsed 20m 34s (remain 14m 26s) Loss: 0.0000(0.0026) Grad: 10.5906  LR: 0.000011  \n",
      "Epoch: [3][2200/3575] Elapsed 21m 33s (remain 13m 27s) Loss: 0.0018(0.0026) Grad: 88767.3203  LR: 0.000011  \n",
      "Epoch: [3][2300/3575] Elapsed 22m 31s (remain 12m 28s) Loss: 0.0000(0.0026) Grad: 5.7304  LR: 0.000010  \n",
      "Epoch: [3][2400/3575] Elapsed 23m 28s (remain 11m 28s) Loss: 0.0008(0.0026) Grad: 10972.3213  LR: 0.000010  \n",
      "Epoch: [3][2500/3575] Elapsed 24m 27s (remain 10m 30s) Loss: 0.0001(0.0026) Grad: 2772.0320  LR: 0.000010  \n",
      "Epoch: [3][2600/3575] Elapsed 25m 25s (remain 9m 31s) Loss: 0.0006(0.0026) Grad: 2246.1562  LR: 0.000010  \n",
      "Epoch: [3][2700/3575] Elapsed 26m 23s (remain 8m 32s) Loss: 0.0000(0.0026) Grad: 17.4210  LR: 0.000010  \n",
      "Epoch: [3][2800/3575] Elapsed 27m 21s (remain 7m 33s) Loss: 0.0000(0.0025) Grad: 11.8414  LR: 0.000010  \n",
      "Epoch: [3][2900/3575] Elapsed 28m 19s (remain 6m 34s) Loss: 0.0000(0.0026) Grad: 21.6131  LR: 0.000010  \n",
      "Epoch: [3][3000/3575] Elapsed 29m 18s (remain 5m 36s) Loss: 0.0111(0.0026) Grad: 39599.1094  LR: 0.000010  \n",
      "Epoch: [3][3100/3575] Elapsed 30m 17s (remain 4m 37s) Loss: 0.0000(0.0026) Grad: 12.4755  LR: 0.000009  \n",
      "Epoch: [3][3200/3575] Elapsed 31m 15s (remain 3m 39s) Loss: 0.0000(0.0026) Grad: 85.3924  LR: 0.000009  \n",
      "Epoch: [3][3300/3575] Elapsed 32m 15s (remain 2m 40s) Loss: 0.0000(0.0027) Grad: 47.9645  LR: 0.000009  \n",
      "Epoch: [3][3400/3575] Elapsed 33m 14s (remain 1m 42s) Loss: 0.0007(0.0027) Grad: 4025.0408  LR: 0.000009  \n",
      "Epoch: [3][3500/3575] Elapsed 34m 12s (remain 0m 43s) Loss: 0.0000(0.0027) Grad: 93.1783  LR: 0.000009  \n",
      "Epoch: [3][3574/3575] Elapsed 34m 55s (remain 0m 0s) Loss: 0.0001(0.0027) Grad: 2575.3020  LR: 0.000009  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 12m 53s) Loss: 0.0067(0.0067) \n",
      "EVAL: [100/1192] Elapsed 0m 19s (remain 3m 33s) Loss: 0.0199(0.0100) \n",
      "EVAL: [200/1192] Elapsed 0m 39s (remain 3m 13s) Loss: 0.0050(0.0101) \n",
      "EVAL: [300/1192] Elapsed 0m 58s (remain 2m 52s) Loss: 0.0016(0.0104) \n",
      "EVAL: [400/1192] Elapsed 1m 16s (remain 2m 31s) Loss: 0.0061(0.0102) \n",
      "EVAL: [500/1192] Elapsed 1m 35s (remain 2m 12s) Loss: 0.0098(0.0097) \n",
      "EVAL: [600/1192] Elapsed 1m 54s (remain 1m 52s) Loss: 0.0196(0.0099) \n",
      "EVAL: [700/1192] Elapsed 2m 13s (remain 1m 33s) Loss: 0.1065(0.0120) \n",
      "EVAL: [800/1192] Elapsed 2m 32s (remain 1m 14s) Loss: 0.0012(0.0124) \n",
      "EVAL: [900/1192] Elapsed 2m 51s (remain 0m 55s) Loss: 0.0164(0.0124) \n",
      "EVAL: [1000/1192] Elapsed 3m 10s (remain 0m 36s) Loss: 0.0000(0.0123) \n",
      "EVAL: [1100/1192] Elapsed 3m 29s (remain 0m 17s) Loss: 0.0001(0.0118) \n",
      "EVAL: [1191/1192] Elapsed 3m 46s (remain 0m 0s) Loss: 0.0000(0.0115) \n",
      "Epoch 3 - avg_train_loss: 0.0027  avg_val_loss: 0.0115  time: 2326s\n",
      "Epoch 3 - Score: 0.8773\n",
      "Epoch: [4][0/3575] Elapsed 0m 0s (remain 57m 3s) Loss: 0.0000(0.0000) Grad: 1001.6453  LR: 0.000009  \n",
      "Epoch: [4][100/3575] Elapsed 0m 59s (remain 34m 21s) Loss: 0.0000(0.0013) Grad: 10.3118  LR: 0.000009  \n",
      "Epoch: [4][200/3575] Elapsed 1m 59s (remain 33m 23s) Loss: 0.0000(0.0011) Grad: 54.7829  LR: 0.000009  \n",
      "Epoch: [4][300/3575] Elapsed 2m 58s (remain 32m 25s) Loss: 0.0000(0.0014) Grad: 22.5678  LR: 0.000009  \n",
      "Epoch: [4][400/3575] Elapsed 3m 57s (remain 31m 19s) Loss: 0.0000(0.0013) Grad: 37.0983  LR: 0.000008  \n",
      "Epoch: [4][500/3575] Elapsed 4m 55s (remain 30m 13s) Loss: 0.0008(0.0014) Grad: 2573.0000  LR: 0.000008  \n",
      "Epoch: [4][600/3575] Elapsed 5m 52s (remain 29m 5s) Loss: 0.0000(0.0015) Grad: 8.5624  LR: 0.000008  \n",
      "Epoch: [4][700/3575] Elapsed 6m 51s (remain 28m 5s) Loss: 0.0026(0.0015) Grad: 9358.4609  LR: 0.000008  \n",
      "Epoch: [4][800/3575] Elapsed 7m 49s (remain 27m 5s) Loss: 0.0000(0.0017) Grad: 21.8700  LR: 0.000008  \n",
      "Epoch: [4][900/3575] Elapsed 8m 48s (remain 26m 8s) Loss: 0.0000(0.0017) Grad: 8.7715  LR: 0.000008  \n",
      "Epoch: [4][1000/3575] Elapsed 9m 47s (remain 25m 10s) Loss: 0.0038(0.0017) Grad: 8885.2354  LR: 0.000008  \n",
      "Epoch: [4][1100/3575] Elapsed 10m 45s (remain 24m 11s) Loss: 0.0000(0.0017) Grad: 39.4455  LR: 0.000008  \n",
      "Epoch: [4][1200/3575] Elapsed 11m 44s (remain 23m 12s) Loss: 0.0001(0.0017) Grad: 3733.2686  LR: 0.000007  \n",
      "Epoch: [4][1300/3575] Elapsed 12m 43s (remain 22m 14s) Loss: 0.0000(0.0017) Grad: 29.5757  LR: 0.000007  \n",
      "Epoch: [4][1400/3575] Elapsed 13m 42s (remain 21m 16s) Loss: 0.0000(0.0017) Grad: 0.8229  LR: 0.000007  \n",
      "Epoch: [4][1500/3575] Elapsed 14m 41s (remain 20m 18s) Loss: 0.0116(0.0018) Grad: 104655.7344  LR: 0.000007  \n",
      "Epoch: [4][1600/3575] Elapsed 15m 40s (remain 19m 19s) Loss: 0.0000(0.0018) Grad: 8.0117  LR: 0.000007  \n",
      "Epoch: [4][1700/3575] Elapsed 16m 39s (remain 18m 20s) Loss: 0.0001(0.0019) Grad: 1397.6505  LR: 0.000007  \n",
      "Epoch: [4][1800/3575] Elapsed 17m 37s (remain 17m 21s) Loss: 0.0001(0.0019) Grad: 923.0382  LR: 0.000007  \n",
      "Epoch: [4][1900/3575] Elapsed 18m 36s (remain 16m 22s) Loss: 0.0142(0.0018) Grad: 31448.5488  LR: 0.000007  \n",
      "Epoch: [4][2000/3575] Elapsed 19m 34s (remain 15m 23s) Loss: 0.0086(0.0019) Grad: 14951.1084  LR: 0.000006  \n",
      "Epoch: [4][2100/3575] Elapsed 20m 32s (remain 14m 24s) Loss: 0.0000(0.0018) Grad: 31.9603  LR: 0.000006  \n",
      "Epoch: [4][2200/3575] Elapsed 21m 30s (remain 13m 25s) Loss: 0.0000(0.0018) Grad: 26.3634  LR: 0.000006  \n",
      "Epoch: [4][2300/3575] Elapsed 22m 29s (remain 12m 27s) Loss: 0.0092(0.0018) Grad: 3220.0735  LR: 0.000006  \n",
      "Epoch: [4][2400/3575] Elapsed 23m 28s (remain 11m 28s) Loss: 0.0000(0.0018) Grad: 10.6211  LR: 0.000006  \n",
      "Epoch: [4][2500/3575] Elapsed 24m 28s (remain 10m 30s) Loss: 0.0000(0.0018) Grad: 8.1064  LR: 0.000006  \n",
      "Epoch: [4][2600/3575] Elapsed 25m 27s (remain 9m 31s) Loss: 0.0000(0.0018) Grad: 6.0617  LR: 0.000006  \n",
      "Epoch: [4][2700/3575] Elapsed 26m 25s (remain 8m 33s) Loss: 0.0001(0.0018) Grad: 2420.7256  LR: 0.000006  \n",
      "Epoch: [4][2800/3575] Elapsed 27m 24s (remain 7m 34s) Loss: 0.0000(0.0018) Grad: 151.5218  LR: 0.000005  \n",
      "Epoch: [4][2900/3575] Elapsed 28m 23s (remain 6m 35s) Loss: 0.0000(0.0019) Grad: 10.4623  LR: 0.000005  \n",
      "Epoch: [4][3000/3575] Elapsed 29m 20s (remain 5m 36s) Loss: 0.0000(0.0019) Grad: 5.0242  LR: 0.000005  \n",
      "Epoch: [4][3100/3575] Elapsed 30m 19s (remain 4m 38s) Loss: 0.0009(0.0019) Grad: 20306.6289  LR: 0.000005  \n",
      "Epoch: [4][3200/3575] Elapsed 31m 18s (remain 3m 39s) Loss: 0.0036(0.0019) Grad: 29513.7109  LR: 0.000005  \n",
      "Epoch: [4][3300/3575] Elapsed 32m 16s (remain 2m 40s) Loss: 0.0001(0.0019) Grad: 300.1296  LR: 0.000005  \n",
      "Epoch: [4][3400/3575] Elapsed 33m 15s (remain 1m 42s) Loss: 0.0000(0.0019) Grad: 15.5946  LR: 0.000005  \n",
      "Epoch: [4][3500/3575] Elapsed 34m 14s (remain 0m 43s) Loss: 0.0000(0.0019) Grad: 355.3137  LR: 0.000005  \n",
      "Epoch: [4][3574/3575] Elapsed 34m 58s (remain 0m 0s) Loss: 0.0000(0.0018) Grad: 11.4247  LR: 0.000004  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 13m 1s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 20s (remain 3m 39s) Loss: 0.0462(0.0096) \n",
      "EVAL: [200/1192] Elapsed 0m 39s (remain 3m 13s) Loss: 0.0093(0.0110) \n",
      "EVAL: [300/1192] Elapsed 0m 58s (remain 2m 53s) Loss: 0.0026(0.0113) \n",
      "EVAL: [400/1192] Elapsed 1m 17s (remain 2m 32s) Loss: 0.0079(0.0110) \n",
      "EVAL: [500/1192] Elapsed 1m 36s (remain 2m 13s) Loss: 0.0123(0.0105) \n",
      "EVAL: [600/1192] Elapsed 1m 55s (remain 1m 53s) Loss: 0.0328(0.0108) \n",
      "EVAL: [700/1192] Elapsed 2m 14s (remain 1m 34s) Loss: 0.1027(0.0130) \n",
      "EVAL: [800/1192] Elapsed 2m 33s (remain 1m 14s) Loss: 0.0013(0.0132) \n",
      "EVAL: [900/1192] Elapsed 2m 52s (remain 0m 55s) Loss: 0.0067(0.0134) \n",
      "EVAL: [1000/1192] Elapsed 3m 11s (remain 0m 36s) Loss: 0.0000(0.0131) \n",
      "EVAL: [1100/1192] Elapsed 3m 30s (remain 0m 17s) Loss: 0.0001(0.0126) \n",
      "EVAL: [1191/1192] Elapsed 3m 47s (remain 0m 0s) Loss: 0.0000(0.0122) \n",
      "Epoch 4 - avg_train_loss: 0.0018  avg_val_loss: 0.0122  time: 2329s\n",
      "Epoch 4 - Score: 0.8785\n",
      "Epoch 4 - Save Best Score: 0.8785 Model\n",
      "Epoch: [5][0/3575] Elapsed 0m 1s (remain 60m 8s) Loss: 0.0000(0.0000) Grad: 17.4901  LR: 0.000004  \n",
      "Epoch: [5][100/3575] Elapsed 0m 59s (remain 34m 22s) Loss: 0.0001(0.0004) Grad: 2544.8545  LR: 0.000004  \n",
      "Epoch: [5][200/3575] Elapsed 1m 58s (remain 33m 12s) Loss: 0.0000(0.0008) Grad: 16.7224  LR: 0.000004  \n",
      "Epoch: [5][300/3575] Elapsed 2m 56s (remain 32m 2s) Loss: 0.0001(0.0010) Grad: 146.9127  LR: 0.000004  \n",
      "Epoch: [5][400/3575] Elapsed 3m 54s (remain 30m 55s) Loss: 0.0000(0.0011) Grad: 6.2334  LR: 0.000004  \n",
      "Epoch: [5][500/3575] Elapsed 4m 53s (remain 30m 0s) Loss: 0.0000(0.0011) Grad: 29.2172  LR: 0.000004  \n",
      "Epoch: [5][600/3575] Elapsed 5m 51s (remain 29m 1s) Loss: 0.0034(0.0012) Grad: 7880.2612  LR: 0.000004  \n",
      "Epoch: [5][700/3575] Elapsed 6m 49s (remain 27m 59s) Loss: 0.0000(0.0013) Grad: 228.7559  LR: 0.000004  \n",
      "Epoch: [5][800/3575] Elapsed 7m 47s (remain 27m 0s) Loss: 0.0000(0.0013) Grad: 14.6186  LR: 0.000003  \n",
      "Epoch: [5][900/3575] Elapsed 8m 46s (remain 26m 3s) Loss: 0.0000(0.0014) Grad: 55.9771  LR: 0.000003  \n",
      "Epoch: [5][1000/3575] Elapsed 9m 45s (remain 25m 5s) Loss: 0.0000(0.0013) Grad: 16.3678  LR: 0.000003  \n",
      "Epoch: [5][1100/3575] Elapsed 10m 45s (remain 24m 9s) Loss: 0.0000(0.0013) Grad: 13.3622  LR: 0.000003  \n",
      "Epoch: [5][1200/3575] Elapsed 11m 44s (remain 23m 12s) Loss: 0.0000(0.0013) Grad: 166.9955  LR: 0.000003  \n",
      "Epoch: [5][1300/3575] Elapsed 12m 42s (remain 22m 12s) Loss: 0.0000(0.0013) Grad: 13.3478  LR: 0.000003  \n",
      "Epoch: [5][1400/3575] Elapsed 13m 39s (remain 21m 12s) Loss: 0.0000(0.0014) Grad: 18.5094  LR: 0.000003  \n",
      "Epoch: [5][1500/3575] Elapsed 14m 39s (remain 20m 14s) Loss: 0.0000(0.0014) Grad: 63.7573  LR: 0.000003  \n",
      "Epoch: [5][1600/3575] Elapsed 15m 37s (remain 19m 15s) Loss: 0.0000(0.0014) Grad: 5.8825  LR: 0.000002  \n",
      "Epoch: [5][1700/3575] Elapsed 16m 36s (remain 18m 17s) Loss: 0.0051(0.0013) Grad: 25432.7148  LR: 0.000002  \n",
      "Epoch: [5][1800/3575] Elapsed 17m 35s (remain 17m 19s) Loss: 0.0000(0.0014) Grad: 6.4170  LR: 0.000002  \n",
      "Epoch: [5][1900/3575] Elapsed 18m 34s (remain 16m 21s) Loss: 0.0001(0.0014) Grad: 7393.9243  LR: 0.000002  \n",
      "Epoch: [5][2000/3575] Elapsed 19m 31s (remain 15m 21s) Loss: 0.0000(0.0013) Grad: 110.7725  LR: 0.000002  \n",
      "Epoch: [5][2100/3575] Elapsed 20m 29s (remain 14m 22s) Loss: 0.0000(0.0013) Grad: 293.1086  LR: 0.000002  \n",
      "Epoch: [5][2200/3575] Elapsed 21m 28s (remain 13m 24s) Loss: 0.0004(0.0014) Grad: 3949.6343  LR: 0.000002  \n",
      "Epoch: [5][2300/3575] Elapsed 22m 26s (remain 12m 25s) Loss: 0.0000(0.0014) Grad: 15.6545  LR: 0.000002  \n",
      "Epoch: [5][2400/3575] Elapsed 23m 24s (remain 11m 26s) Loss: 0.0000(0.0013) Grad: 116.7052  LR: 0.000001  \n",
      "Epoch: [5][2500/3575] Elapsed 24m 23s (remain 10m 28s) Loss: 0.0000(0.0013) Grad: 5.6929  LR: 0.000001  \n",
      "Epoch: [5][2600/3575] Elapsed 25m 22s (remain 9m 29s) Loss: 0.0000(0.0013) Grad: 23.0118  LR: 0.000001  \n",
      "Epoch: [5][2700/3575] Elapsed 26m 20s (remain 8m 31s) Loss: 0.0000(0.0013) Grad: 63.3018  LR: 0.000001  \n",
      "Epoch: [5][2800/3575] Elapsed 27m 19s (remain 7m 32s) Loss: 0.0039(0.0013) Grad: 39955.6055  LR: 0.000001  \n",
      "Epoch: [5][2900/3575] Elapsed 28m 18s (remain 6m 34s) Loss: 0.0001(0.0013) Grad: 783.9932  LR: 0.000001  \n",
      "Epoch: [5][3000/3575] Elapsed 29m 15s (remain 5m 35s) Loss: 0.0000(0.0013) Grad: 8.0169  LR: 0.000001  \n",
      "Epoch: [5][3100/3575] Elapsed 30m 14s (remain 4m 37s) Loss: 0.0000(0.0013) Grad: 1.6524  LR: 0.000001  \n",
      "Epoch: [5][3200/3575] Elapsed 31m 13s (remain 3m 38s) Loss: 0.0000(0.0013) Grad: 6.8589  LR: 0.000000  \n",
      "Epoch: [5][3300/3575] Elapsed 32m 11s (remain 2m 40s) Loss: 0.0000(0.0013) Grad: 6.8816  LR: 0.000000  \n",
      "Epoch: [5][3400/3575] Elapsed 33m 10s (remain 1m 41s) Loss: 0.0000(0.0013) Grad: 18.5305  LR: 0.000000  \n",
      "Epoch: [5][3500/3575] Elapsed 34m 8s (remain 0m 43s) Loss: 0.0000(0.0013) Grad: 4.0574  LR: 0.000000  \n",
      "Epoch: [5][3574/3575] Elapsed 34m 51s (remain 0m 0s) Loss: 0.0001(0.0013) Grad: 186.8625  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 13m 0s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 19s (remain 3m 29s) Loss: 0.0567(0.0104) \n",
      "EVAL: [200/1192] Elapsed 0m 37s (remain 3m 7s) Loss: 0.0018(0.0116) \n",
      "EVAL: [300/1192] Elapsed 0m 57s (remain 2m 49s) Loss: 0.0016(0.0118) \n",
      "EVAL: [400/1192] Elapsed 1m 16s (remain 2m 30s) Loss: 0.0071(0.0117) \n",
      "EVAL: [500/1192] Elapsed 1m 34s (remain 2m 10s) Loss: 0.0111(0.0109) \n",
      "EVAL: [600/1192] Elapsed 1m 54s (remain 1m 52s) Loss: 0.0283(0.0112) \n",
      "EVAL: [700/1192] Elapsed 2m 13s (remain 1m 33s) Loss: 0.1161(0.0138) \n",
      "EVAL: [800/1192] Elapsed 2m 32s (remain 1m 14s) Loss: 0.0036(0.0141) \n",
      "EVAL: [900/1192] Elapsed 2m 51s (remain 0m 55s) Loss: 0.0032(0.0142) \n",
      "EVAL: [1000/1192] Elapsed 3m 10s (remain 0m 36s) Loss: 0.0000(0.0140) \n",
      "EVAL: [1100/1192] Elapsed 3m 29s (remain 0m 17s) Loss: 0.0000(0.0134) \n",
      "EVAL: [1191/1192] Elapsed 3m 47s (remain 0m 0s) Loss: 0.0000(0.0129) \n",
      "Epoch 5 - avg_train_loss: 0.0013  avg_val_loss: 0.0129  time: 2322s\n",
      "Epoch 5 - Score: 0.8803\n",
      "Epoch 5 - Save Best Score: 0.8803 Model\n",
      "========== fold: 1 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp054/fold1_best.pth\n",
      "Epoch: [1][0/3575] Elapsed 0m 1s (remain 63m 35s) Loss: 0.3231(0.3231) Grad: 83407.2109  LR: 0.000000  \n",
      "Epoch: [1][100/3575] Elapsed 1m 0s (remain 34m 33s) Loss: 0.2792(0.3081) Grad: 79978.7266  LR: 0.000001  \n",
      "Epoch: [1][200/3575] Elapsed 2m 0s (remain 33m 39s) Loss: 0.1693(0.2678) Grad: 53895.1836  LR: 0.000002  \n",
      "Epoch: [1][300/3575] Elapsed 2m 59s (remain 32m 32s) Loss: 0.0822(0.2192) Grad: 29815.2422  LR: 0.000003  \n",
      "Epoch: [1][400/3575] Elapsed 3m 59s (remain 31m 32s) Loss: 0.0293(0.1774) Grad: 12902.0000  LR: 0.000004  \n",
      "Epoch: [1][500/3575] Elapsed 4m 57s (remain 30m 26s) Loss: 0.0092(0.1459) Grad: 4458.5029  LR: 0.000006  \n",
      "Epoch: [1][600/3575] Elapsed 5m 57s (remain 29m 27s) Loss: 0.0063(0.1230) Grad: 2011.9006  LR: 0.000007  \n",
      "Epoch: [1][700/3575] Elapsed 6m 58s (remain 28m 36s) Loss: 0.0033(0.1063) Grad: 1109.4064  LR: 0.000008  \n",
      "Epoch: [1][800/3575] Elapsed 7m 56s (remain 27m 30s) Loss: 0.0120(0.0936) Grad: 111861.4062  LR: 0.000009  \n",
      "Epoch: [1][900/3575] Elapsed 8m 52s (remain 26m 21s) Loss: 0.0026(0.0837) Grad: 529.7045  LR: 0.000010  \n",
      "Epoch: [1][1000/3575] Elapsed 9m 49s (remain 25m 16s) Loss: 0.0052(0.0757) Grad: 62583.7656  LR: 0.000011  \n",
      "Epoch: [1][1100/3575] Elapsed 10m 46s (remain 24m 12s) Loss: 0.0003(0.0692) Grad: 127.9015  LR: 0.000012  \n",
      "Epoch: [1][1200/3575] Elapsed 11m 44s (remain 23m 12s) Loss: 0.0100(0.0638) Grad: 285490.0000  LR: 0.000013  \n",
      "Epoch: [1][1300/3575] Elapsed 12m 43s (remain 22m 14s) Loss: 0.0002(0.0592) Grad: 80.3779  LR: 0.000015  \n",
      "Epoch: [1][1400/3575] Elapsed 13m 42s (remain 21m 16s) Loss: 0.0002(0.0552) Grad: 75.8831  LR: 0.000016  \n",
      "Epoch: [1][1500/3575] Elapsed 14m 42s (remain 20m 19s) Loss: 0.0033(0.0518) Grad: 545.0467  LR: 0.000017  \n",
      "Epoch: [1][1600/3575] Elapsed 15m 42s (remain 19m 21s) Loss: 0.0003(0.0489) Grad: 134.0524  LR: 0.000018  \n",
      "Epoch: [1][1700/3575] Elapsed 16m 41s (remain 18m 23s) Loss: 0.0010(0.0463) Grad: 16549.9473  LR: 0.000019  \n",
      "Epoch: [1][1800/3575] Elapsed 17m 41s (remain 17m 25s) Loss: 0.0003(0.0439) Grad: 164.1334  LR: 0.000020  \n",
      "Epoch: [1][1900/3575] Elapsed 18m 40s (remain 16m 26s) Loss: 0.0002(0.0419) Grad: 160.4177  LR: 0.000020  \n",
      "Epoch: [1][2000/3575] Elapsed 19m 38s (remain 15m 27s) Loss: 0.0001(0.0401) Grad: 47.9198  LR: 0.000020  \n",
      "Epoch: [1][2100/3575] Elapsed 20m 38s (remain 14m 28s) Loss: 0.0227(0.0383) Grad: 5159.2524  LR: 0.000020  \n",
      "Epoch: [1][2200/3575] Elapsed 21m 37s (remain 13m 29s) Loss: 0.0253(0.0368) Grad: 4943.0737  LR: 0.000019  \n",
      "Epoch: [1][2300/3575] Elapsed 22m 38s (remain 12m 32s) Loss: 0.0001(0.0354) Grad: 45.3010  LR: 0.000019  \n",
      "Epoch: [1][2400/3575] Elapsed 23m 39s (remain 11m 33s) Loss: 0.0031(0.0341) Grad: 7299.5933  LR: 0.000019  \n",
      "Epoch: [1][2500/3575] Elapsed 24m 39s (remain 10m 35s) Loss: 0.0008(0.0329) Grad: 2294.5842  LR: 0.000019  \n",
      "Epoch: [1][2600/3575] Elapsed 25m 36s (remain 9m 35s) Loss: 0.0015(0.0319) Grad: 112272.3516  LR: 0.000019  \n",
      "Epoch: [1][2700/3575] Elapsed 26m 32s (remain 8m 35s) Loss: 0.0060(0.0308) Grad: 11462.3613  LR: 0.000019  \n",
      "Epoch: [1][2800/3575] Elapsed 27m 28s (remain 7m 35s) Loss: 0.0001(0.0299) Grad: 31.7935  LR: 0.000019  \n",
      "Epoch: [1][2900/3575] Elapsed 28m 25s (remain 6m 36s) Loss: 0.0023(0.0290) Grad: 12249.6211  LR: 0.000019  \n",
      "Epoch: [1][3000/3575] Elapsed 29m 21s (remain 5m 36s) Loss: 0.0000(0.0282) Grad: 25.5634  LR: 0.000018  \n",
      "Epoch: [1][3100/3575] Elapsed 30m 18s (remain 4m 38s) Loss: 0.0116(0.0275) Grad: 56392.7305  LR: 0.000018  \n",
      "Epoch: [1][3200/3575] Elapsed 31m 16s (remain 3m 39s) Loss: 0.0002(0.0268) Grad: 78.6911  LR: 0.000018  \n",
      "Epoch: [1][3300/3575] Elapsed 32m 14s (remain 2m 40s) Loss: 0.0159(0.0261) Grad: 44078.7930  LR: 0.000018  \n",
      "Epoch: [1][3400/3575] Elapsed 33m 13s (remain 1m 42s) Loss: 0.0002(0.0255) Grad: 1424.4478  LR: 0.000018  \n",
      "Epoch: [1][3500/3575] Elapsed 34m 11s (remain 0m 43s) Loss: 0.0001(0.0249) Grad: 89.3274  LR: 0.000018  \n",
      "Epoch: [1][3574/3575] Elapsed 34m 53s (remain 0m 0s) Loss: 0.0046(0.0245) Grad: 63393.3203  LR: 0.000018  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 14m 21s) Loss: 0.0004(0.0004) \n",
      "EVAL: [100/1192] Elapsed 0m 18s (remain 3m 23s) Loss: 0.0002(0.0066) \n",
      "EVAL: [200/1192] Elapsed 0m 36s (remain 3m 1s) Loss: 0.0002(0.0076) \n",
      "EVAL: [300/1192] Elapsed 0m 54s (remain 2m 42s) Loss: 0.0059(0.0113) \n",
      "EVAL: [400/1192] Elapsed 1m 12s (remain 2m 23s) Loss: 0.0282(0.0113) \n",
      "EVAL: [500/1192] Elapsed 1m 31s (remain 2m 6s) Loss: 0.0294(0.0103) \n",
      "EVAL: [600/1192] Elapsed 1m 50s (remain 1m 48s) Loss: 0.1331(0.0106) \n",
      "EVAL: [700/1192] Elapsed 2m 9s (remain 1m 30s) Loss: 0.0020(0.0115) \n",
      "EVAL: [800/1192] Elapsed 2m 27s (remain 1m 12s) Loss: 0.0040(0.0112) \n",
      "EVAL: [900/1192] Elapsed 2m 46s (remain 0m 53s) Loss: 0.0045(0.0109) \n",
      "EVAL: [1000/1192] Elapsed 3m 5s (remain 0m 35s) Loss: 0.0001(0.0104) \n",
      "EVAL: [1100/1192] Elapsed 3m 24s (remain 0m 16s) Loss: 0.0042(0.0100) \n",
      "EVAL: [1191/1192] Elapsed 3m 42s (remain 0m 0s) Loss: 0.0076(0.0094) \n",
      "Epoch 1 - avg_train_loss: 0.0245  avg_val_loss: 0.0094  time: 2321s\n",
      "Epoch 1 - Score: 0.8700\n",
      "Epoch 1 - Save Best Score: 0.8700 Model\n",
      "Epoch: [2][0/3575] Elapsed 0m 1s (remain 63m 36s) Loss: 0.0010(0.0010) Grad: 8415.6270  LR: 0.000018  \n",
      "Epoch: [2][100/3575] Elapsed 1m 0s (remain 34m 47s) Loss: 0.0000(0.0025) Grad: 99.0601  LR: 0.000018  \n",
      "Epoch: [2][200/3575] Elapsed 1m 59s (remain 33m 31s) Loss: 0.0001(0.0041) Grad: 368.6800  LR: 0.000018  \n",
      "Epoch: [2][300/3575] Elapsed 2m 57s (remain 32m 6s) Loss: 0.0007(0.0036) Grad: 261.4191  LR: 0.000017  \n",
      "Epoch: [2][400/3575] Elapsed 3m 56s (remain 31m 12s) Loss: 0.0003(0.0038) Grad: 13447.8672  LR: 0.000017  \n",
      "Epoch: [2][500/3575] Elapsed 4m 54s (remain 30m 8s) Loss: 0.0131(0.0039) Grad: 2076.8430  LR: 0.000017  \n",
      "Epoch: [2][600/3575] Elapsed 5m 52s (remain 29m 2s) Loss: 0.0001(0.0039) Grad: 34.2202  LR: 0.000017  \n",
      "Epoch: [2][700/3575] Elapsed 6m 49s (remain 27m 57s) Loss: 0.0007(0.0039) Grad: 1487.1160  LR: 0.000017  \n",
      "Epoch: [2][800/3575] Elapsed 7m 46s (remain 26m 54s) Loss: 0.0000(0.0038) Grad: 29.1668  LR: 0.000017  \n",
      "Epoch: [2][900/3575] Elapsed 8m 45s (remain 26m 0s) Loss: 0.0017(0.0037) Grad: 3527.7261  LR: 0.000017  \n",
      "Epoch: [2][1000/3575] Elapsed 9m 45s (remain 25m 5s) Loss: 0.0152(0.0037) Grad: 24422.2324  LR: 0.000017  \n",
      "Epoch: [2][1100/3575] Elapsed 10m 42s (remain 24m 3s) Loss: 0.0003(0.0038) Grad: 373.7537  LR: 0.000016  \n",
      "Epoch: [2][1200/3575] Elapsed 11m 39s (remain 23m 2s) Loss: 0.0007(0.0038) Grad: 269.2963  LR: 0.000016  \n",
      "Epoch: [2][1300/3575] Elapsed 12m 37s (remain 22m 3s) Loss: 0.0000(0.0037) Grad: 4.2830  LR: 0.000016  \n",
      "Epoch: [2][1400/3575] Elapsed 13m 36s (remain 21m 6s) Loss: 0.0084(0.0038) Grad: 10032.4424  LR: 0.000016  \n",
      "Epoch: [2][1500/3575] Elapsed 14m 33s (remain 20m 7s) Loss: 0.0004(0.0037) Grad: 1257.8674  LR: 0.000016  \n",
      "Epoch: [2][1600/3575] Elapsed 15m 30s (remain 19m 7s) Loss: 0.0001(0.0037) Grad: 473.7236  LR: 0.000016  \n",
      "Epoch: [2][1700/3575] Elapsed 16m 27s (remain 18m 8s) Loss: 0.0001(0.0036) Grad: 32.1220  LR: 0.000016  \n",
      "Epoch: [2][1800/3575] Elapsed 17m 27s (remain 17m 11s) Loss: 0.0002(0.0038) Grad: 56.9211  LR: 0.000016  \n",
      "Epoch: [2][1900/3575] Elapsed 18m 25s (remain 16m 13s) Loss: 0.0004(0.0037) Grad: 3182.0432  LR: 0.000015  \n",
      "Epoch: [2][2000/3575] Elapsed 19m 22s (remain 15m 14s) Loss: 0.0004(0.0037) Grad: 2132.7510  LR: 0.000015  \n",
      "Epoch: [2][2100/3575] Elapsed 20m 20s (remain 14m 16s) Loss: 0.0002(0.0037) Grad: 535.0548  LR: 0.000015  \n",
      "Epoch: [2][2200/3575] Elapsed 21m 19s (remain 13m 19s) Loss: 0.0002(0.0037) Grad: 261.9370  LR: 0.000015  \n",
      "Epoch: [2][2300/3575] Elapsed 22m 16s (remain 12m 20s) Loss: 0.0004(0.0037) Grad: 1439.1030  LR: 0.000015  \n",
      "Epoch: [2][2400/3575] Elapsed 23m 14s (remain 11m 21s) Loss: 0.0011(0.0036) Grad: 598.4796  LR: 0.000015  \n",
      "Epoch: [2][2500/3575] Elapsed 24m 12s (remain 10m 23s) Loss: 0.0001(0.0037) Grad: 90.0709  LR: 0.000015  \n",
      "Epoch: [2][2600/3575] Elapsed 25m 8s (remain 9m 24s) Loss: 0.0304(0.0037) Grad: 71371.7734  LR: 0.000015  \n",
      "Epoch: [2][2700/3575] Elapsed 26m 7s (remain 8m 27s) Loss: 0.0216(0.0037) Grad: 35683.8516  LR: 0.000014  \n",
      "Epoch: [2][2800/3575] Elapsed 27m 4s (remain 7m 29s) Loss: 0.0009(0.0037) Grad: 940.2205  LR: 0.000014  \n",
      "Epoch: [2][2900/3575] Elapsed 28m 2s (remain 6m 30s) Loss: 0.0001(0.0036) Grad: 20.9164  LR: 0.000014  \n",
      "Epoch: [2][3000/3575] Elapsed 29m 0s (remain 5m 32s) Loss: 0.0000(0.0037) Grad: 11.6555  LR: 0.000014  \n",
      "Epoch: [2][3100/3575] Elapsed 29m 58s (remain 4m 34s) Loss: 0.0031(0.0037) Grad: 6772.2603  LR: 0.000014  \n",
      "Epoch: [2][3200/3575] Elapsed 30m 56s (remain 3m 36s) Loss: 0.0003(0.0037) Grad: 514.2415  LR: 0.000014  \n",
      "Epoch: [2][3300/3575] Elapsed 31m 54s (remain 2m 38s) Loss: 0.0268(0.0036) Grad: 26387.4551  LR: 0.000014  \n",
      "Epoch: [2][3400/3575] Elapsed 32m 51s (remain 1m 40s) Loss: 0.0013(0.0037) Grad: 13650.5488  LR: 0.000014  \n",
      "Epoch: [2][3500/3575] Elapsed 33m 48s (remain 0m 42s) Loss: 0.0006(0.0036) Grad: 693.5202  LR: 0.000013  \n",
      "Epoch: [2][3574/3575] Elapsed 34m 30s (remain 0m 0s) Loss: 0.0078(0.0036) Grad: 8678.3125  LR: 0.000013  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 15m 22s) Loss: 0.0001(0.0001) \n",
      "EVAL: [100/1192] Elapsed 0m 21s (remain 3m 47s) Loss: 0.0001(0.0057) \n",
      "EVAL: [200/1192] Elapsed 0m 39s (remain 3m 16s) Loss: 0.0002(0.0074) \n",
      "EVAL: [300/1192] Elapsed 0m 58s (remain 2m 52s) Loss: 0.0016(0.0107) \n",
      "EVAL: [400/1192] Elapsed 1m 16s (remain 2m 30s) Loss: 0.0203(0.0108) \n",
      "EVAL: [500/1192] Elapsed 1m 34s (remain 2m 10s) Loss: 0.0269(0.0099) \n",
      "EVAL: [600/1192] Elapsed 1m 54s (remain 1m 52s) Loss: 0.1069(0.0102) \n",
      "EVAL: [700/1192] Elapsed 2m 12s (remain 1m 33s) Loss: 0.0055(0.0118) \n",
      "EVAL: [800/1192] Elapsed 2m 30s (remain 1m 13s) Loss: 0.0106(0.0114) \n",
      "EVAL: [900/1192] Elapsed 2m 48s (remain 0m 54s) Loss: 0.0012(0.0110) \n",
      "EVAL: [1000/1192] Elapsed 3m 7s (remain 0m 35s) Loss: 0.0002(0.0106) \n",
      "EVAL: [1100/1192] Elapsed 3m 26s (remain 0m 17s) Loss: 0.0053(0.0101) \n",
      "EVAL: [1191/1192] Elapsed 3m 43s (remain 0m 0s) Loss: 0.0166(0.0096) \n",
      "Epoch 2 - avg_train_loss: 0.0036  avg_val_loss: 0.0096  time: 2299s\n",
      "Epoch 2 - Score: 0.8693\n",
      "Epoch: [3][0/3575] Elapsed 0m 1s (remain 60m 58s) Loss: 0.0128(0.0128) Grad: 3724.3110  LR: 0.000013  \n",
      "Epoch: [3][100/3575] Elapsed 0m 59s (remain 33m 58s) Loss: 0.0029(0.0017) Grad: 15245.8164  LR: 0.000013  \n",
      "Epoch: [3][200/3575] Elapsed 1m 56s (remain 32m 39s) Loss: 0.0003(0.0022) Grad: 1015.8881  LR: 0.000013  \n",
      "Epoch: [3][300/3575] Elapsed 2m 53s (remain 31m 30s) Loss: 0.0002(0.0025) Grad: 219.4245  LR: 0.000013  \n",
      "Epoch: [3][400/3575] Elapsed 3m 51s (remain 30m 31s) Loss: 0.0000(0.0027) Grad: 29.0804  LR: 0.000013  \n",
      "Epoch: [3][500/3575] Elapsed 4m 50s (remain 29m 44s) Loss: 0.0000(0.0025) Grad: 23.5896  LR: 0.000013  \n",
      "Epoch: [3][600/3575] Elapsed 5m 53s (remain 29m 10s) Loss: 0.0029(0.0026) Grad: 2264.5386  LR: 0.000013  \n",
      "Epoch: [3][700/3575] Elapsed 6m 56s (remain 28m 29s) Loss: 0.0001(0.0026) Grad: 64.8998  LR: 0.000012  \n",
      "Epoch: [3][800/3575] Elapsed 7m 56s (remain 27m 31s) Loss: 0.0014(0.0026) Grad: 1280.2949  LR: 0.000012  \n",
      "Epoch: [3][900/3575] Elapsed 8m 54s (remain 26m 25s) Loss: 0.0000(0.0026) Grad: 291.7602  LR: 0.000012  \n",
      "Epoch: [3][1000/3575] Elapsed 9m 51s (remain 25m 20s) Loss: 0.0000(0.0026) Grad: 70.4876  LR: 0.000012  \n",
      "Epoch: [3][1100/3575] Elapsed 10m 49s (remain 24m 19s) Loss: 0.0153(0.0026) Grad: 34064.4688  LR: 0.000012  \n",
      "Epoch: [3][1200/3575] Elapsed 11m 47s (remain 23m 17s) Loss: 0.0000(0.0026) Grad: 21.7073  LR: 0.000012  \n",
      "Epoch: [3][1300/3575] Elapsed 12m 44s (remain 22m 15s) Loss: 0.0051(0.0026) Grad: 162560.9688  LR: 0.000012  \n",
      "Epoch: [3][1400/3575] Elapsed 13m 40s (remain 21m 13s) Loss: 0.0000(0.0026) Grad: 8.7998  LR: 0.000012  \n",
      "Epoch: [3][1500/3575] Elapsed 14m 39s (remain 20m 15s) Loss: 0.0001(0.0026) Grad: 987.3045  LR: 0.000011  \n",
      "Epoch: [3][1600/3575] Elapsed 15m 36s (remain 19m 14s) Loss: 0.0000(0.0027) Grad: 127.3384  LR: 0.000011  \n",
      "Epoch: [3][1700/3575] Elapsed 16m 33s (remain 18m 15s) Loss: 0.0001(0.0026) Grad: 59.6621  LR: 0.000011  \n",
      "Epoch: [3][1800/3575] Elapsed 17m 33s (remain 17m 18s) Loss: 0.0000(0.0026) Grad: 35.6894  LR: 0.000011  \n",
      "Epoch: [3][1900/3575] Elapsed 18m 35s (remain 16m 22s) Loss: 0.0002(0.0026) Grad: 154.8147  LR: 0.000011  \n",
      "Epoch: [3][2000/3575] Elapsed 19m 38s (remain 15m 27s) Loss: 0.0085(0.0026) Grad: 42077.8477  LR: 0.000011  \n",
      "Epoch: [3][2100/3575] Elapsed 20m 41s (remain 14m 30s) Loss: 0.0003(0.0026) Grad: 3395.3086  LR: 0.000011  \n",
      "Epoch: [3][2200/3575] Elapsed 21m 39s (remain 13m 31s) Loss: 0.0015(0.0026) Grad: 19090.5156  LR: 0.000011  \n",
      "Epoch: [3][2300/3575] Elapsed 22m 36s (remain 12m 30s) Loss: 0.0000(0.0026) Grad: 18.4546  LR: 0.000010  \n",
      "Epoch: [3][2400/3575] Elapsed 23m 35s (remain 11m 31s) Loss: 0.0000(0.0026) Grad: 63.8081  LR: 0.000010  \n",
      "Epoch: [3][2500/3575] Elapsed 24m 33s (remain 10m 32s) Loss: 0.0041(0.0026) Grad: 2484.9485  LR: 0.000010  \n",
      "Epoch: [3][2600/3575] Elapsed 25m 30s (remain 9m 33s) Loss: 0.0000(0.0026) Grad: 144.8956  LR: 0.000010  \n",
      "Epoch: [3][2700/3575] Elapsed 26m 27s (remain 8m 33s) Loss: 0.0001(0.0026) Grad: 133.0922  LR: 0.000010  \n",
      "Epoch: [3][2800/3575] Elapsed 27m 28s (remain 7m 35s) Loss: 0.0000(0.0026) Grad: 14.1254  LR: 0.000010  \n",
      "Epoch: [3][2900/3575] Elapsed 28m 25s (remain 6m 36s) Loss: 0.0000(0.0026) Grad: 60.1954  LR: 0.000010  \n",
      "Epoch: [3][3000/3575] Elapsed 29m 22s (remain 5m 37s) Loss: 0.0000(0.0025) Grad: 170.1983  LR: 0.000010  \n",
      "Epoch: [3][3100/3575] Elapsed 30m 18s (remain 4m 37s) Loss: 0.0000(0.0025) Grad: 44.4306  LR: 0.000009  \n",
      "Epoch: [3][3200/3575] Elapsed 31m 14s (remain 3m 39s) Loss: 0.0001(0.0025) Grad: 592.8459  LR: 0.000009  \n",
      "Epoch: [3][3300/3575] Elapsed 32m 14s (remain 2m 40s) Loss: 0.0000(0.0026) Grad: 528.8619  LR: 0.000009  \n",
      "Epoch: [3][3400/3575] Elapsed 33m 13s (remain 1m 42s) Loss: 0.0244(0.0026) Grad: 77816.1016  LR: 0.000009  \n",
      "Epoch: [3][3500/3575] Elapsed 34m 10s (remain 0m 43s) Loss: 0.0000(0.0026) Grad: 12.5975  LR: 0.000009  \n",
      "Epoch: [3][3574/3575] Elapsed 34m 53s (remain 0m 0s) Loss: 0.0017(0.0026) Grad: 2210.1685  LR: 0.000009  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 15m 43s) Loss: 0.0001(0.0001) \n",
      "EVAL: [100/1192] Elapsed 0m 19s (remain 3m 26s) Loss: 0.0000(0.0089) \n",
      "EVAL: [200/1192] Elapsed 0m 37s (remain 3m 4s) Loss: 0.0000(0.0103) \n",
      "EVAL: [300/1192] Elapsed 0m 57s (remain 2m 51s) Loss: 0.0014(0.0149) \n",
      "EVAL: [400/1192] Elapsed 1m 17s (remain 2m 32s) Loss: 0.0266(0.0151) \n",
      "EVAL: [500/1192] Elapsed 1m 35s (remain 2m 11s) Loss: 0.0375(0.0138) \n",
      "EVAL: [600/1192] Elapsed 1m 53s (remain 1m 51s) Loss: 0.2143(0.0140) \n",
      "EVAL: [700/1192] Elapsed 2m 11s (remain 1m 32s) Loss: 0.0060(0.0158) \n",
      "EVAL: [800/1192] Elapsed 2m 31s (remain 1m 13s) Loss: 0.0055(0.0154) \n",
      "EVAL: [900/1192] Elapsed 2m 51s (remain 0m 55s) Loss: 0.0002(0.0148) \n",
      "EVAL: [1000/1192] Elapsed 3m 9s (remain 0m 36s) Loss: 0.0000(0.0142) \n",
      "EVAL: [1100/1192] Elapsed 3m 27s (remain 0m 17s) Loss: 0.0067(0.0135) \n",
      "EVAL: [1191/1192] Elapsed 3m 44s (remain 0m 0s) Loss: 0.0109(0.0128) \n",
      "Epoch 3 - avg_train_loss: 0.0026  avg_val_loss: 0.0128  time: 2322s\n",
      "Epoch 3 - Score: 0.8800\n",
      "Epoch 3 - Save Best Score: 0.8800 Model\n",
      "Epoch: [4][0/3575] Elapsed 0m 1s (remain 62m 9s) Loss: 0.0000(0.0000) Grad: 993.8469  LR: 0.000009  \n",
      "Epoch: [4][100/3575] Elapsed 0m 58s (remain 33m 28s) Loss: 0.0000(0.0013) Grad: 12.0352  LR: 0.000009  \n",
      "Epoch: [4][200/3575] Elapsed 1m 56s (remain 32m 38s) Loss: 0.0046(0.0015) Grad: 18504.5000  LR: 0.000009  \n",
      "Epoch: [4][300/3575] Elapsed 2m 56s (remain 31m 57s) Loss: 0.0000(0.0019) Grad: 154.0266  LR: 0.000009  \n",
      "Epoch: [4][400/3575] Elapsed 3m 53s (remain 30m 47s) Loss: 0.0101(0.0018) Grad: 32982.0117  LR: 0.000008  \n",
      "Epoch: [4][500/3575] Elapsed 4m 51s (remain 29m 47s) Loss: 0.0000(0.0019) Grad: 7.7473  LR: 0.000008  \n",
      "Epoch: [4][600/3575] Elapsed 5m 49s (remain 28m 51s) Loss: 0.0001(0.0017) Grad: 110.4870  LR: 0.000008  \n",
      "Epoch: [4][700/3575] Elapsed 6m 47s (remain 27m 48s) Loss: 0.0001(0.0017) Grad: 264.1685  LR: 0.000008  \n",
      "Epoch: [4][800/3575] Elapsed 7m 44s (remain 26m 47s) Loss: 0.0000(0.0017) Grad: 11.7109  LR: 0.000008  \n",
      "Epoch: [4][900/3575] Elapsed 8m 42s (remain 25m 51s) Loss: 0.0000(0.0017) Grad: 13.6544  LR: 0.000008  \n",
      "Epoch: [4][1000/3575] Elapsed 9m 41s (remain 24m 54s) Loss: 0.0010(0.0016) Grad: 288.2091  LR: 0.000008  \n",
      "Epoch: [4][1100/3575] Elapsed 10m 38s (remain 23m 54s) Loss: 0.0000(0.0016) Grad: 80.3955  LR: 0.000008  \n",
      "Epoch: [4][1200/3575] Elapsed 11m 35s (remain 22m 54s) Loss: 0.0000(0.0017) Grad: 110.1130  LR: 0.000007  \n",
      "Epoch: [4][1300/3575] Elapsed 12m 33s (remain 21m 57s) Loss: 0.0005(0.0017) Grad: 1306.8066  LR: 0.000007  \n",
      "Epoch: [4][1400/3575] Elapsed 13m 35s (remain 21m 5s) Loss: 0.0000(0.0017) Grad: 23.4398  LR: 0.000007  \n",
      "Epoch: [4][1500/3575] Elapsed 14m 32s (remain 20m 6s) Loss: 0.0000(0.0016) Grad: 358.2920  LR: 0.000007  \n",
      "Epoch: [4][1600/3575] Elapsed 15m 29s (remain 19m 6s) Loss: 0.0000(0.0017) Grad: 42.7149  LR: 0.000007  \n",
      "Epoch: [4][1700/3575] Elapsed 16m 26s (remain 18m 7s) Loss: 0.0000(0.0017) Grad: 100.0735  LR: 0.000007  \n",
      "Epoch: [4][1800/3575] Elapsed 17m 23s (remain 17m 8s) Loss: 0.0001(0.0017) Grad: 1243.4117  LR: 0.000007  \n",
      "Epoch: [4][1900/3575] Elapsed 18m 20s (remain 16m 9s) Loss: 0.0000(0.0018) Grad: 332.0379  LR: 0.000007  \n",
      "Epoch: [4][2000/3575] Elapsed 19m 18s (remain 15m 11s) Loss: 0.0000(0.0018) Grad: 9.6140  LR: 0.000006  \n",
      "Epoch: [4][2100/3575] Elapsed 20m 17s (remain 14m 14s) Loss: 0.0000(0.0018) Grad: 27.5731  LR: 0.000006  \n",
      "Epoch: [4][2200/3575] Elapsed 21m 17s (remain 13m 17s) Loss: 0.0000(0.0018) Grad: 5.2144  LR: 0.000006  \n",
      "Epoch: [4][2300/3575] Elapsed 22m 14s (remain 12m 18s) Loss: 0.0097(0.0018) Grad: 13087.5244  LR: 0.000006  \n",
      "Epoch: [4][2400/3575] Elapsed 23m 11s (remain 11m 20s) Loss: 0.0000(0.0018) Grad: 10.0539  LR: 0.000006  \n",
      "Epoch: [4][2500/3575] Elapsed 24m 8s (remain 10m 21s) Loss: 0.0001(0.0018) Grad: 426.0870  LR: 0.000006  \n",
      "Epoch: [4][2600/3575] Elapsed 25m 6s (remain 9m 24s) Loss: 0.0000(0.0018) Grad: 9.9716  LR: 0.000006  \n",
      "Epoch: [4][2700/3575] Elapsed 26m 7s (remain 8m 27s) Loss: 0.0002(0.0018) Grad: 454.4629  LR: 0.000006  \n",
      "Epoch: [4][2800/3575] Elapsed 27m 5s (remain 7m 29s) Loss: 0.0004(0.0018) Grad: 1980.1499  LR: 0.000005  \n",
      "Epoch: [4][2900/3575] Elapsed 28m 3s (remain 6m 31s) Loss: 0.0000(0.0019) Grad: 7.5241  LR: 0.000005  \n",
      "Epoch: [4][3000/3575] Elapsed 29m 5s (remain 5m 33s) Loss: 0.0000(0.0018) Grad: 42.4806  LR: 0.000005  \n",
      "Epoch: [4][3100/3575] Elapsed 30m 5s (remain 4m 35s) Loss: 0.0001(0.0018) Grad: 633.2270  LR: 0.000005  \n",
      "Epoch: [4][3200/3575] Elapsed 31m 2s (remain 3m 37s) Loss: 0.0001(0.0018) Grad: 1014.0817  LR: 0.000005  \n",
      "Epoch: [4][3300/3575] Elapsed 31m 59s (remain 2m 39s) Loss: 0.0000(0.0018) Grad: 514.4816  LR: 0.000005  \n",
      "Epoch: [4][3400/3575] Elapsed 32m 55s (remain 1m 41s) Loss: 0.0000(0.0018) Grad: 322.8015  LR: 0.000005  \n",
      "Epoch: [4][3500/3575] Elapsed 33m 54s (remain 0m 43s) Loss: 0.0001(0.0018) Grad: 213.2268  LR: 0.000005  \n",
      "Epoch: [4][3574/3575] Elapsed 34m 39s (remain 0m 0s) Loss: 0.0000(0.0018) Grad: 270.4482  LR: 0.000004  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 14m 25s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 18s (remain 3m 24s) Loss: 0.0000(0.0091) \n",
      "EVAL: [200/1192] Elapsed 0m 37s (remain 3m 6s) Loss: 0.0000(0.0097) \n",
      "EVAL: [300/1192] Elapsed 0m 56s (remain 2m 48s) Loss: 0.0014(0.0137) \n",
      "EVAL: [400/1192] Elapsed 1m 15s (remain 2m 29s) Loss: 0.0274(0.0139) \n",
      "EVAL: [500/1192] Elapsed 1m 33s (remain 2m 9s) Loss: 0.0315(0.0128) \n",
      "EVAL: [600/1192] Elapsed 1m 51s (remain 1m 50s) Loss: 0.1412(0.0129) \n",
      "EVAL: [700/1192] Elapsed 2m 9s (remain 1m 31s) Loss: 0.0065(0.0146) \n",
      "EVAL: [800/1192] Elapsed 2m 28s (remain 1m 12s) Loss: 0.0048(0.0143) \n",
      "EVAL: [900/1192] Elapsed 2m 47s (remain 0m 54s) Loss: 0.0167(0.0139) \n",
      "EVAL: [1000/1192] Elapsed 3m 6s (remain 0m 35s) Loss: 0.0000(0.0134) \n",
      "EVAL: [1100/1192] Elapsed 3m 24s (remain 0m 16s) Loss: 0.0068(0.0127) \n",
      "EVAL: [1191/1192] Elapsed 3m 40s (remain 0m 0s) Loss: 0.0110(0.0121) \n",
      "Epoch 4 - avg_train_loss: 0.0018  avg_val_loss: 0.0121  time: 2304s\n",
      "Epoch 4 - Score: 0.8797\n",
      "Epoch: [5][0/3575] Elapsed 0m 1s (remain 64m 20s) Loss: 0.0027(0.0027) Grad: 5402.8325  LR: 0.000004  \n",
      "Epoch: [5][100/3575] Elapsed 0m 58s (remain 33m 17s) Loss: 0.0419(0.0026) Grad: 147768.0156  LR: 0.000004  \n",
      "Epoch: [5][200/3575] Elapsed 1m 55s (remain 32m 13s) Loss: 0.0000(0.0018) Grad: 72.4645  LR: 0.000004  \n",
      "Epoch: [5][300/3575] Elapsed 2m 52s (remain 31m 12s) Loss: 0.0000(0.0016) Grad: 15.9051  LR: 0.000004  \n",
      "Epoch: [5][400/3575] Elapsed 3m 49s (remain 30m 17s) Loss: 0.0040(0.0016) Grad: 18900.9141  LR: 0.000004  \n",
      "Epoch: [5][500/3575] Elapsed 4m 47s (remain 29m 26s) Loss: 0.0000(0.0016) Grad: 10.8002  LR: 0.000004  \n",
      "Epoch: [5][600/3575] Elapsed 5m 45s (remain 28m 27s) Loss: 0.0000(0.0015) Grad: 44.6754  LR: 0.000004  \n",
      "Epoch: [5][700/3575] Elapsed 6m 42s (remain 27m 28s) Loss: 0.0000(0.0015) Grad: 67.7376  LR: 0.000004  \n",
      "Epoch: [5][800/3575] Elapsed 7m 39s (remain 26m 30s) Loss: 0.0000(0.0013) Grad: 4.3584  LR: 0.000003  \n",
      "Epoch: [5][900/3575] Elapsed 8m 36s (remain 25m 34s) Loss: 0.0000(0.0013) Grad: 32.1188  LR: 0.000003  \n",
      "Epoch: [5][1000/3575] Elapsed 9m 33s (remain 24m 35s) Loss: 0.0000(0.0013) Grad: 7.6116  LR: 0.000003  \n",
      "Epoch: [5][1100/3575] Elapsed 10m 31s (remain 23m 39s) Loss: 0.0000(0.0013) Grad: 3.5096  LR: 0.000003  \n",
      "Epoch: [5][1200/3575] Elapsed 11m 28s (remain 22m 40s) Loss: 0.0000(0.0013) Grad: 25.6000  LR: 0.000003  \n",
      "Epoch: [5][1300/3575] Elapsed 12m 25s (remain 21m 42s) Loss: 0.0000(0.0013) Grad: 9.5301  LR: 0.000003  \n",
      "Epoch: [5][1400/3575] Elapsed 13m 21s (remain 20m 44s) Loss: 0.0000(0.0013) Grad: 38.7182  LR: 0.000003  \n",
      "Epoch: [5][1500/3575] Elapsed 14m 19s (remain 19m 47s) Loss: 0.0000(0.0013) Grad: 11.7575  LR: 0.000003  \n",
      "Epoch: [5][1600/3575] Elapsed 15m 17s (remain 18m 51s) Loss: 0.0000(0.0013) Grad: 10.1304  LR: 0.000002  \n",
      "Epoch: [5][1700/3575] Elapsed 16m 14s (remain 17m 53s) Loss: 0.0000(0.0013) Grad: 3.8194  LR: 0.000002  \n",
      "Epoch: [5][1800/3575] Elapsed 17m 11s (remain 16m 55s) Loss: 0.0000(0.0013) Grad: 5.9832  LR: 0.000002  \n",
      "Epoch: [5][1900/3575] Elapsed 18m 11s (remain 16m 0s) Loss: 0.0000(0.0013) Grad: 21.1027  LR: 0.000002  \n",
      "Epoch: [5][2000/3575] Elapsed 19m 13s (remain 15m 7s) Loss: 0.0000(0.0013) Grad: 33.9177  LR: 0.000002  \n",
      "Epoch: [5][2100/3575] Elapsed 20m 11s (remain 14m 9s) Loss: 0.0000(0.0013) Grad: 7.7923  LR: 0.000002  \n",
      "Epoch: [5][2200/3575] Elapsed 21m 11s (remain 13m 13s) Loss: 0.0156(0.0013) Grad: 16998.0488  LR: 0.000002  \n",
      "Epoch: [5][2300/3575] Elapsed 22m 8s (remain 12m 15s) Loss: 0.0000(0.0013) Grad: 55.0266  LR: 0.000002  \n",
      "Epoch: [5][2400/3575] Elapsed 23m 5s (remain 11m 17s) Loss: 0.0000(0.0013) Grad: 4.4037  LR: 0.000001  \n",
      "Epoch: [5][2500/3575] Elapsed 24m 2s (remain 10m 19s) Loss: 0.0000(0.0013) Grad: 29.6141  LR: 0.000001  \n",
      "Epoch: [5][2600/3575] Elapsed 24m 59s (remain 9m 21s) Loss: 0.0021(0.0013) Grad: 23033.1660  LR: 0.000001  \n",
      "Epoch: [5][2700/3575] Elapsed 25m 57s (remain 8m 24s) Loss: 0.0000(0.0013) Grad: 67.1544  LR: 0.000001  \n",
      "Epoch: [5][2800/3575] Elapsed 26m 56s (remain 7m 26s) Loss: 0.0000(0.0013) Grad: 3.1398  LR: 0.000001  \n",
      "Epoch: [5][2900/3575] Elapsed 27m 53s (remain 6m 28s) Loss: 0.0001(0.0013) Grad: 377.9231  LR: 0.000001  \n",
      "Epoch: [5][3000/3575] Elapsed 28m 51s (remain 5m 31s) Loss: 0.0002(0.0013) Grad: 632.5532  LR: 0.000001  \n",
      "Epoch: [5][3100/3575] Elapsed 29m 48s (remain 4m 33s) Loss: 0.0111(0.0013) Grad: 13867.2412  LR: 0.000001  \n",
      "Epoch: [5][3200/3575] Elapsed 30m 44s (remain 3m 35s) Loss: 0.0000(0.0013) Grad: 8.1814  LR: 0.000000  \n",
      "Epoch: [5][3300/3575] Elapsed 31m 42s (remain 2m 37s) Loss: 0.0000(0.0013) Grad: 9.6070  LR: 0.000000  \n",
      "Epoch: [5][3400/3575] Elapsed 32m 40s (remain 1m 40s) Loss: 0.0000(0.0013) Grad: 25.3495  LR: 0.000000  \n",
      "Epoch: [5][3500/3575] Elapsed 33m 38s (remain 0m 42s) Loss: 0.0013(0.0013) Grad: 252.8786  LR: 0.000000  \n",
      "Epoch: [5][3574/3575] Elapsed 34m 21s (remain 0m 0s) Loss: 0.0000(0.0013) Grad: 18.6967  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 13m 13s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 18s (remain 3m 23s) Loss: 0.0000(0.0098) \n",
      "EVAL: [200/1192] Elapsed 0m 37s (remain 3m 4s) Loss: 0.0000(0.0105) \n",
      "EVAL: [300/1192] Elapsed 0m 56s (remain 2m 46s) Loss: 0.0014(0.0151) \n",
      "EVAL: [400/1192] Elapsed 1m 14s (remain 2m 26s) Loss: 0.0328(0.0154) \n",
      "EVAL: [500/1192] Elapsed 1m 32s (remain 2m 7s) Loss: 0.0364(0.0142) \n",
      "EVAL: [600/1192] Elapsed 1m 50s (remain 1m 48s) Loss: 0.1625(0.0143) \n",
      "EVAL: [700/1192] Elapsed 2m 8s (remain 1m 30s) Loss: 0.0070(0.0161) \n",
      "EVAL: [800/1192] Elapsed 2m 28s (remain 1m 12s) Loss: 0.0144(0.0157) \n",
      "EVAL: [900/1192] Elapsed 2m 51s (remain 0m 55s) Loss: 0.0141(0.0153) \n",
      "EVAL: [1000/1192] Elapsed 3m 11s (remain 0m 36s) Loss: 0.0000(0.0148) \n",
      "EVAL: [1100/1192] Elapsed 3m 30s (remain 0m 17s) Loss: 0.0072(0.0141) \n",
      "EVAL: [1191/1192] Elapsed 3m 47s (remain 0m 0s) Loss: 0.0118(0.0134) \n",
      "Epoch 5 - avg_train_loss: 0.0013  avg_val_loss: 0.0134  time: 2292s\n",
      "Epoch 5 - Score: 0.8798\n",
      "========== fold: 2 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp054/fold2_best.pth\n",
      "Epoch: [1][0/3575] Elapsed 0m 0s (remain 58m 30s) Loss: 0.3723(0.3723) Grad: 99857.8203  LR: 0.000000  \n",
      "Epoch: [1][100/3575] Elapsed 0m 58s (remain 33m 48s) Loss: 0.3214(0.3555) Grad: 89358.7344  LR: 0.000001  \n",
      "Epoch: [1][200/3575] Elapsed 1m 55s (remain 32m 26s) Loss: 0.1904(0.3074) Grad: 58857.8672  LR: 0.000002  \n",
      "Epoch: [1][300/3575] Elapsed 2m 53s (remain 31m 24s) Loss: 0.0923(0.2503) Grad: 31693.8359  LR: 0.000003  \n",
      "Epoch: [1][400/3575] Elapsed 3m 50s (remain 30m 21s) Loss: 0.0352(0.2023) Grad: 14219.8027  LR: 0.000004  \n",
      "Epoch: [1][500/3575] Elapsed 4m 47s (remain 29m 22s) Loss: 0.0110(0.1663) Grad: 5055.6118  LR: 0.000006  \n",
      "Epoch: [1][600/3575] Elapsed 5m 45s (remain 28m 30s) Loss: 0.0073(0.1402) Grad: 58265.3828  LR: 0.000007  \n",
      "Epoch: [1][700/3575] Elapsed 6m 43s (remain 27m 35s) Loss: 0.0019(0.1210) Grad: 736.8088  LR: 0.000008  \n",
      "Epoch: [1][800/3575] Elapsed 7m 44s (remain 26m 47s) Loss: 0.0015(0.1064) Grad: 502.7080  LR: 0.000009  \n",
      "Epoch: [1][900/3575] Elapsed 8m 42s (remain 25m 49s) Loss: 0.0019(0.0953) Grad: 674.3247  LR: 0.000010  \n",
      "Epoch: [1][1000/3575] Elapsed 9m 38s (remain 24m 48s) Loss: 0.0030(0.0863) Grad: 8531.6855  LR: 0.000011  \n",
      "Epoch: [1][1100/3575] Elapsed 10m 35s (remain 23m 46s) Loss: 0.0012(0.0789) Grad: 240.3217  LR: 0.000012  \n",
      "Epoch: [1][1200/3575] Elapsed 11m 32s (remain 22m 49s) Loss: 0.0018(0.0728) Grad: 9870.0801  LR: 0.000013  \n",
      "Epoch: [1][1300/3575] Elapsed 12m 29s (remain 21m 50s) Loss: 0.0005(0.0676) Grad: 191.2703  LR: 0.000015  \n",
      "Epoch: [1][1400/3575] Elapsed 13m 27s (remain 20m 53s) Loss: 0.0001(0.0631) Grad: 95.3337  LR: 0.000016  \n",
      "Epoch: [1][1500/3575] Elapsed 14m 25s (remain 19m 56s) Loss: 0.0008(0.0592) Grad: 985.6401  LR: 0.000017  \n",
      "Epoch: [1][1600/3575] Elapsed 15m 24s (remain 19m 0s) Loss: 0.0005(0.0559) Grad: 279.7676  LR: 0.000018  \n",
      "Epoch: [1][1700/3575] Elapsed 16m 21s (remain 18m 1s) Loss: 0.0005(0.0528) Grad: 530.7003  LR: 0.000019  \n",
      "Epoch: [1][1800/3575] Elapsed 17m 19s (remain 17m 3s) Loss: 0.0001(0.0503) Grad: 172.8887  LR: 0.000020  \n",
      "Epoch: [1][1900/3575] Elapsed 18m 18s (remain 16m 7s) Loss: 0.0003(0.0479) Grad: 769.8271  LR: 0.000020  \n",
      "Epoch: [1][2000/3575] Elapsed 19m 16s (remain 15m 9s) Loss: 0.0002(0.0458) Grad: 600.7005  LR: 0.000020  \n",
      "Epoch: [1][2100/3575] Elapsed 20m 13s (remain 14m 11s) Loss: 0.0019(0.0439) Grad: 470.9549  LR: 0.000020  \n",
      "Epoch: [1][2200/3575] Elapsed 21m 10s (remain 13m 13s) Loss: 0.0004(0.0420) Grad: 6971.7739  LR: 0.000019  \n",
      "Epoch: [1][2300/3575] Elapsed 22m 8s (remain 12m 15s) Loss: 0.0001(0.0404) Grad: 311.2230  LR: 0.000019  \n",
      "Epoch: [1][2400/3575] Elapsed 23m 5s (remain 11m 17s) Loss: 0.0206(0.0390) Grad: 37711.4102  LR: 0.000019  \n",
      "Epoch: [1][2500/3575] Elapsed 24m 2s (remain 10m 19s) Loss: 0.0118(0.0376) Grad: 25077.2012  LR: 0.000019  \n",
      "Epoch: [1][2600/3575] Elapsed 25m 3s (remain 9m 22s) Loss: 0.0104(0.0364) Grad: 15714.6738  LR: 0.000019  \n",
      "Epoch: [1][2700/3575] Elapsed 26m 0s (remain 8m 24s) Loss: 0.0007(0.0352) Grad: 4882.2051  LR: 0.000019  \n",
      "Epoch: [1][2800/3575] Elapsed 26m 58s (remain 7m 27s) Loss: 0.0001(0.0342) Grad: 45.2630  LR: 0.000019  \n",
      "Epoch: [1][2900/3575] Elapsed 27m 55s (remain 6m 29s) Loss: 0.0029(0.0331) Grad: 5690.8223  LR: 0.000019  \n",
      "Epoch: [1][3000/3575] Elapsed 28m 54s (remain 5m 31s) Loss: 0.0105(0.0322) Grad: 40161.7539  LR: 0.000018  \n",
      "Epoch: [1][3100/3575] Elapsed 29m 50s (remain 4m 33s) Loss: 0.0059(0.0313) Grad: 86498.2031  LR: 0.000018  \n",
      "Epoch: [1][3200/3575] Elapsed 30m 47s (remain 3m 35s) Loss: 0.0001(0.0304) Grad: 51.3975  LR: 0.000018  \n",
      "Epoch: [1][3300/3575] Elapsed 31m 44s (remain 2m 38s) Loss: 0.0002(0.0297) Grad: 67.8126  LR: 0.000018  \n",
      "Epoch: [1][3400/3575] Elapsed 32m 41s (remain 1m 40s) Loss: 0.0051(0.0289) Grad: 123884.8672  LR: 0.000018  \n",
      "Epoch: [1][3500/3575] Elapsed 33m 38s (remain 0m 42s) Loss: 0.0039(0.0282) Grad: 3474.4551  LR: 0.000018  \n",
      "Epoch: [1][3574/3575] Elapsed 34m 21s (remain 0m 0s) Loss: 0.0001(0.0277) Grad: 73.0844  LR: 0.000018  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 13m 42s) Loss: 0.0009(0.0009) \n",
      "EVAL: [100/1192] Elapsed 0m 18s (remain 3m 22s) Loss: 0.0337(0.0100) \n",
      "EVAL: [200/1192] Elapsed 0m 36s (remain 3m 1s) Loss: 0.0035(0.0093) \n",
      "EVAL: [300/1192] Elapsed 0m 55s (remain 2m 44s) Loss: 0.0233(0.0088) \n",
      "EVAL: [400/1192] Elapsed 1m 15s (remain 2m 28s) Loss: 0.0002(0.0094) \n",
      "EVAL: [500/1192] Elapsed 1m 34s (remain 2m 9s) Loss: 0.0103(0.0088) \n",
      "EVAL: [600/1192] Elapsed 1m 52s (remain 1m 50s) Loss: 0.0084(0.0092) \n",
      "EVAL: [700/1192] Elapsed 2m 10s (remain 1m 31s) Loss: 0.0122(0.0101) \n",
      "EVAL: [800/1192] Elapsed 2m 28s (remain 1m 12s) Loss: 0.0000(0.0102) \n",
      "EVAL: [900/1192] Elapsed 2m 47s (remain 0m 54s) Loss: 0.0242(0.0102) \n",
      "EVAL: [1000/1192] Elapsed 3m 7s (remain 0m 35s) Loss: 0.0182(0.0099) \n",
      "EVAL: [1100/1192] Elapsed 3m 27s (remain 0m 17s) Loss: 0.0487(0.0094) \n",
      "EVAL: [1191/1192] Elapsed 3m 43s (remain 0m 0s) Loss: 0.0001(0.0090) \n",
      "Epoch 1 - avg_train_loss: 0.0277  avg_val_loss: 0.0090  time: 2290s\n",
      "Epoch 1 - Score: 0.8762\n",
      "Epoch 1 - Save Best Score: 0.8762 Model\n",
      "Epoch: [2][0/3575] Elapsed 0m 1s (remain 65m 47s) Loss: 0.0094(0.0094) Grad: 17264.8809  LR: 0.000018  \n",
      "Epoch: [2][100/3575] Elapsed 0m 58s (remain 33m 48s) Loss: 0.0001(0.0030) Grad: 93.9184  LR: 0.000018  \n",
      "Epoch: [2][200/3575] Elapsed 1m 57s (remain 32m 49s) Loss: 0.0103(0.0040) Grad: 10746.4902  LR: 0.000018  \n",
      "Epoch: [2][300/3575] Elapsed 2m 55s (remain 31m 51s) Loss: 0.0001(0.0037) Grad: 106.9762  LR: 0.000017  \n",
      "Epoch: [2][400/3575] Elapsed 3m 53s (remain 30m 50s) Loss: 0.0181(0.0039) Grad: 29128.0742  LR: 0.000017  \n",
      "Epoch: [2][500/3575] Elapsed 4m 50s (remain 29m 44s) Loss: 0.0002(0.0043) Grad: 356.8685  LR: 0.000017  \n",
      "Epoch: [2][600/3575] Elapsed 5m 47s (remain 28m 40s) Loss: 0.0004(0.0040) Grad: 916.4368  LR: 0.000017  \n",
      "Epoch: [2][700/3575] Elapsed 6m 45s (remain 27m 43s) Loss: 0.0003(0.0043) Grad: 691.8815  LR: 0.000017  \n",
      "Epoch: [2][800/3575] Elapsed 7m 45s (remain 26m 52s) Loss: 0.0001(0.0046) Grad: 245.9678  LR: 0.000017  \n",
      "Epoch: [2][900/3575] Elapsed 8m 43s (remain 25m 54s) Loss: 0.0001(0.0045) Grad: 213.8287  LR: 0.000017  \n",
      "Epoch: [2][1000/3575] Elapsed 9m 40s (remain 24m 53s) Loss: 0.0006(0.0044) Grad: 6181.7241  LR: 0.000017  \n",
      "Epoch: [2][1100/3575] Elapsed 10m 38s (remain 23m 55s) Loss: 0.0071(0.0044) Grad: 23092.6484  LR: 0.000016  \n",
      "Epoch: [2][1200/3575] Elapsed 11m 37s (remain 22m 58s) Loss: 0.0001(0.0043) Grad: 117.5609  LR: 0.000016  \n",
      "Epoch: [2][1300/3575] Elapsed 12m 34s (remain 21m 58s) Loss: 0.0012(0.0043) Grad: 4855.0850  LR: 0.000016  \n",
      "Epoch: [2][1400/3575] Elapsed 13m 33s (remain 21m 1s) Loss: 0.0001(0.0043) Grad: 50.8903  LR: 0.000016  \n",
      "Epoch: [2][1500/3575] Elapsed 14m 34s (remain 20m 8s) Loss: 0.0001(0.0043) Grad: 34.0529  LR: 0.000016  \n",
      "Epoch: [2][1600/3575] Elapsed 15m 32s (remain 19m 9s) Loss: 0.0089(0.0042) Grad: 53281.3633  LR: 0.000016  \n",
      "Epoch: [2][1700/3575] Elapsed 16m 28s (remain 18m 8s) Loss: 0.0001(0.0042) Grad: 404.9503  LR: 0.000016  \n",
      "Epoch: [2][1800/3575] Elapsed 17m 23s (remain 17m 8s) Loss: 0.0014(0.0042) Grad: 11809.5645  LR: 0.000016  \n",
      "Epoch: [2][1900/3575] Elapsed 18m 19s (remain 16m 8s) Loss: 0.0070(0.0043) Grad: 32942.9961  LR: 0.000015  \n",
      "Epoch: [2][2000/3575] Elapsed 19m 16s (remain 15m 9s) Loss: 0.0032(0.0042) Grad: 1606.1094  LR: 0.000015  \n",
      "Epoch: [2][2100/3575] Elapsed 20m 14s (remain 14m 12s) Loss: 0.0015(0.0042) Grad: 9719.9971  LR: 0.000015  \n",
      "Epoch: [2][2200/3575] Elapsed 21m 12s (remain 13m 14s) Loss: 0.0000(0.0042) Grad: 24.5779  LR: 0.000015  \n",
      "Epoch: [2][2300/3575] Elapsed 22m 9s (remain 12m 16s) Loss: 0.0001(0.0042) Grad: 362.9265  LR: 0.000015  \n",
      "Epoch: [2][2400/3575] Elapsed 23m 8s (remain 11m 18s) Loss: 0.0151(0.0042) Grad: 10640.4971  LR: 0.000015  \n",
      "Epoch: [2][2500/3575] Elapsed 24m 7s (remain 10m 21s) Loss: 0.0286(0.0043) Grad: 11148.6748  LR: 0.000015  \n",
      "Epoch: [2][2600/3575] Elapsed 25m 5s (remain 9m 23s) Loss: 0.0000(0.0043) Grad: 49.4831  LR: 0.000015  \n",
      "Epoch: [2][2700/3575] Elapsed 26m 2s (remain 8m 25s) Loss: 0.0044(0.0042) Grad: 23009.9219  LR: 0.000014  \n",
      "Epoch: [2][2800/3575] Elapsed 26m 59s (remain 7m 27s) Loss: 0.0072(0.0042) Grad: 22123.9551  LR: 0.000014  \n",
      "Epoch: [2][2900/3575] Elapsed 27m 56s (remain 6m 29s) Loss: 0.0001(0.0042) Grad: 426.8051  LR: 0.000014  \n",
      "Epoch: [2][3000/3575] Elapsed 28m 54s (remain 5m 31s) Loss: 0.0000(0.0042) Grad: 46.7429  LR: 0.000014  \n",
      "Epoch: [2][3100/3575] Elapsed 29m 53s (remain 4m 34s) Loss: 0.0000(0.0041) Grad: 28.5629  LR: 0.000014  \n",
      "Epoch: [2][3200/3575] Elapsed 30m 50s (remain 3m 36s) Loss: 0.0001(0.0041) Grad: 313.6877  LR: 0.000014  \n",
      "Epoch: [2][3300/3575] Elapsed 31m 46s (remain 2m 38s) Loss: 0.0001(0.0042) Grad: 485.6711  LR: 0.000014  \n",
      "Epoch: [2][3400/3575] Elapsed 32m 47s (remain 1m 40s) Loss: 0.0017(0.0042) Grad: 6090.7476  LR: 0.000014  \n",
      "Epoch: [2][3500/3575] Elapsed 33m 45s (remain 0m 42s) Loss: 0.0049(0.0042) Grad: 19200.2520  LR: 0.000013  \n",
      "Epoch: [2][3574/3575] Elapsed 34m 27s (remain 0m 0s) Loss: 0.0148(0.0042) Grad: 22414.2793  LR: 0.000013  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 12m 23s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 18s (remain 3m 22s) Loss: 0.0378(0.0101) \n",
      "EVAL: [200/1192] Elapsed 0m 36s (remain 3m 1s) Loss: 0.0092(0.0097) \n",
      "EVAL: [300/1192] Elapsed 0m 54s (remain 2m 41s) Loss: 0.0007(0.0096) \n",
      "EVAL: [400/1192] Elapsed 1m 12s (remain 2m 23s) Loss: 0.0001(0.0099) \n",
      "EVAL: [500/1192] Elapsed 1m 30s (remain 2m 4s) Loss: 0.0001(0.0092) \n",
      "EVAL: [600/1192] Elapsed 1m 49s (remain 1m 47s) Loss: 0.0119(0.0093) \n",
      "EVAL: [700/1192] Elapsed 2m 7s (remain 1m 29s) Loss: 0.0127(0.0102) \n",
      "EVAL: [800/1192] Elapsed 2m 26s (remain 1m 11s) Loss: 0.0000(0.0105) \n",
      "EVAL: [900/1192] Elapsed 2m 48s (remain 0m 54s) Loss: 0.0050(0.0106) \n",
      "EVAL: [1000/1192] Elapsed 3m 7s (remain 0m 35s) Loss: 0.0142(0.0105) \n",
      "EVAL: [1100/1192] Elapsed 3m 25s (remain 0m 16s) Loss: 0.0566(0.0100) \n",
      "EVAL: [1191/1192] Elapsed 3m 41s (remain 0m 0s) Loss: 0.0000(0.0096) \n",
      "Epoch 2 - avg_train_loss: 0.0042  avg_val_loss: 0.0096  time: 2293s\n",
      "Epoch 2 - Score: 0.8826\n",
      "Epoch 2 - Save Best Score: 0.8826 Model\n",
      "Epoch: [3][0/3575] Elapsed 0m 1s (remain 65m 6s) Loss: 0.0026(0.0026) Grad: 8657.8086  LR: 0.000013  \n",
      "Epoch: [3][100/3575] Elapsed 0m 58s (remain 33m 40s) Loss: 0.0000(0.0025) Grad: 15.8918  LR: 0.000013  \n",
      "Epoch: [3][200/3575] Elapsed 1m 56s (remain 32m 34s) Loss: 0.0001(0.0035) Grad: 82.1913  LR: 0.000013  \n",
      "Epoch: [3][300/3575] Elapsed 2m 54s (remain 31m 38s) Loss: 0.0005(0.0030) Grad: 479.5554  LR: 0.000013  \n",
      "Epoch: [3][400/3575] Elapsed 3m 51s (remain 30m 28s) Loss: 0.0000(0.0031) Grad: 46.8648  LR: 0.000013  \n",
      "Epoch: [3][500/3575] Elapsed 4m 47s (remain 29m 25s) Loss: 0.0019(0.0034) Grad: 1997.1172  LR: 0.000013  \n",
      "Epoch: [3][600/3575] Elapsed 5m 44s (remain 28m 22s) Loss: 0.0000(0.0032) Grad: 13.1894  LR: 0.000013  \n",
      "Epoch: [3][700/3575] Elapsed 6m 40s (remain 27m 22s) Loss: 0.0000(0.0032) Grad: 43.6344  LR: 0.000012  \n",
      "Epoch: [3][800/3575] Elapsed 7m 38s (remain 26m 27s) Loss: 0.0001(0.0034) Grad: 518.6006  LR: 0.000012  \n",
      "Epoch: [3][900/3575] Elapsed 8m 35s (remain 25m 28s) Loss: 0.0078(0.0034) Grad: 49682.3438  LR: 0.000012  \n",
      "Epoch: [3][1000/3575] Elapsed 9m 31s (remain 24m 29s) Loss: 0.0000(0.0033) Grad: 7.5479  LR: 0.000012  \n",
      "Epoch: [3][1100/3575] Elapsed 10m 29s (remain 23m 33s) Loss: 0.0014(0.0034) Grad: 13370.1260  LR: 0.000012  \n",
      "Epoch: [3][1200/3575] Elapsed 11m 27s (remain 22m 38s) Loss: 0.0000(0.0033) Grad: 31.6692  LR: 0.000012  \n",
      "Epoch: [3][1300/3575] Elapsed 12m 26s (remain 21m 44s) Loss: 0.0001(0.0032) Grad: 262.3392  LR: 0.000012  \n",
      "Epoch: [3][1400/3575] Elapsed 13m 25s (remain 20m 49s) Loss: 0.0001(0.0032) Grad: 960.7001  LR: 0.000012  \n",
      "Epoch: [3][1500/3575] Elapsed 14m 25s (remain 19m 56s) Loss: 0.0004(0.0032) Grad: 2816.1326  LR: 0.000011  \n",
      "Epoch: [3][1600/3575] Elapsed 15m 23s (remain 18m 58s) Loss: 0.0000(0.0032) Grad: 130.3731  LR: 0.000011  \n",
      "Epoch: [3][1700/3575] Elapsed 16m 19s (remain 17m 59s) Loss: 0.0014(0.0031) Grad: 11624.6465  LR: 0.000011  \n",
      "Epoch: [3][1800/3575] Elapsed 17m 17s (remain 17m 1s) Loss: 0.0019(0.0032) Grad: 12549.1377  LR: 0.000011  \n",
      "Epoch: [3][1900/3575] Elapsed 18m 16s (remain 16m 5s) Loss: 0.0002(0.0032) Grad: 195.0625  LR: 0.000011  \n",
      "Epoch: [3][2000/3575] Elapsed 19m 12s (remain 15m 6s) Loss: 0.0005(0.0032) Grad: 6326.3208  LR: 0.000011  \n",
      "Epoch: [3][2100/3575] Elapsed 20m 9s (remain 14m 8s) Loss: 0.0000(0.0032) Grad: 33.5416  LR: 0.000011  \n",
      "Epoch: [3][2200/3575] Elapsed 21m 8s (remain 13m 11s) Loss: 0.0125(0.0032) Grad: 184222.7500  LR: 0.000011  \n",
      "Epoch: [3][2300/3575] Elapsed 22m 5s (remain 12m 14s) Loss: 0.0521(0.0032) Grad: 144301.3438  LR: 0.000010  \n",
      "Epoch: [3][2400/3575] Elapsed 23m 2s (remain 11m 15s) Loss: 0.0000(0.0032) Grad: 13.4100  LR: 0.000010  \n",
      "Epoch: [3][2500/3575] Elapsed 23m 59s (remain 10m 17s) Loss: 0.0001(0.0032) Grad: 590.1830  LR: 0.000010  \n",
      "Epoch: [3][2600/3575] Elapsed 24m 55s (remain 9m 20s) Loss: 0.0016(0.0032) Grad: 5809.4990  LR: 0.000010  \n",
      "Epoch: [3][2700/3575] Elapsed 25m 53s (remain 8m 22s) Loss: 0.0003(0.0032) Grad: 5427.7393  LR: 0.000010  \n",
      "Epoch: [3][2800/3575] Elapsed 26m 55s (remain 7m 26s) Loss: 0.0000(0.0032) Grad: 209.0914  LR: 0.000010  \n",
      "Epoch: [3][2900/3575] Elapsed 27m 52s (remain 6m 28s) Loss: 0.0035(0.0031) Grad: 26939.3145  LR: 0.000010  \n",
      "Epoch: [3][3000/3575] Elapsed 28m 50s (remain 5m 30s) Loss: 0.0001(0.0031) Grad: 150.6588  LR: 0.000010  \n",
      "Epoch: [3][3100/3575] Elapsed 29m 46s (remain 4m 33s) Loss: 0.0001(0.0031) Grad: 217.5155  LR: 0.000009  \n",
      "Epoch: [3][3200/3575] Elapsed 30m 44s (remain 3m 35s) Loss: 0.0001(0.0031) Grad: 302.7231  LR: 0.000009  \n",
      "Epoch: [3][3300/3575] Elapsed 31m 43s (remain 2m 37s) Loss: 0.0002(0.0031) Grad: 2026.6703  LR: 0.000009  \n",
      "Epoch: [3][3400/3575] Elapsed 32m 40s (remain 1m 40s) Loss: 0.0000(0.0031) Grad: 47.2555  LR: 0.000009  \n",
      "Epoch: [3][3500/3575] Elapsed 33m 37s (remain 0m 42s) Loss: 0.0012(0.0031) Grad: 1453.3079  LR: 0.000009  \n",
      "Epoch: [3][3574/3575] Elapsed 34m 19s (remain 0m 0s) Loss: 0.0000(0.0031) Grad: 37.0518  LR: 0.000009  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 11m 45s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 18s (remain 3m 21s) Loss: 0.0395(0.0110) \n",
      "EVAL: [200/1192] Elapsed 0m 36s (remain 2m 59s) Loss: 0.0158(0.0099) \n",
      "EVAL: [300/1192] Elapsed 0m 54s (remain 2m 41s) Loss: 0.0190(0.0095) \n",
      "EVAL: [400/1192] Elapsed 1m 13s (remain 2m 25s) Loss: 0.0000(0.0099) \n",
      "EVAL: [500/1192] Elapsed 1m 32s (remain 2m 7s) Loss: 0.0198(0.0093) \n",
      "EVAL: [600/1192] Elapsed 1m 50s (remain 1m 48s) Loss: 0.0101(0.0093) \n",
      "EVAL: [700/1192] Elapsed 2m 8s (remain 1m 30s) Loss: 0.0132(0.0101) \n",
      "EVAL: [800/1192] Elapsed 2m 27s (remain 1m 12s) Loss: 0.0000(0.0102) \n",
      "EVAL: [900/1192] Elapsed 2m 46s (remain 0m 53s) Loss: 0.0201(0.0102) \n",
      "EVAL: [1000/1192] Elapsed 3m 4s (remain 0m 35s) Loss: 0.0071(0.0100) \n",
      "EVAL: [1100/1192] Elapsed 3m 23s (remain 0m 16s) Loss: 0.0616(0.0096) \n",
      "EVAL: [1191/1192] Elapsed 3m 39s (remain 0m 0s) Loss: 0.0000(0.0091) \n",
      "Epoch 3 - avg_train_loss: 0.0031  avg_val_loss: 0.0091  time: 2283s\n",
      "Epoch 3 - Score: 0.8866\n",
      "Epoch 3 - Save Best Score: 0.8866 Model\n",
      "Epoch: [4][0/3575] Elapsed 0m 1s (remain 62m 25s) Loss: 0.0000(0.0000) Grad: 73.2036  LR: 0.000009  \n",
      "Epoch: [4][100/3575] Elapsed 0m 58s (remain 33m 19s) Loss: 0.0000(0.0015) Grad: 87.3130  LR: 0.000009  \n",
      "Epoch: [4][200/3575] Elapsed 1m 55s (remain 32m 10s) Loss: 0.0005(0.0021) Grad: 1056.8168  LR: 0.000009  \n",
      "Epoch: [4][300/3575] Elapsed 2m 51s (remain 31m 9s) Loss: 0.0046(0.0021) Grad: 39608.8359  LR: 0.000009  \n",
      "Epoch: [4][400/3575] Elapsed 3m 49s (remain 30m 13s) Loss: 0.0000(0.0021) Grad: 52.4909  LR: 0.000008  \n",
      "Epoch: [4][500/3575] Elapsed 4m 47s (remain 29m 26s) Loss: 0.0005(0.0022) Grad: 1381.4825  LR: 0.000008  \n",
      "Epoch: [4][600/3575] Elapsed 5m 44s (remain 28m 24s) Loss: 0.0000(0.0021) Grad: 17.7812  LR: 0.000008  \n",
      "Epoch: [4][700/3575] Elapsed 6m 42s (remain 27m 29s) Loss: 0.0000(0.0021) Grad: 35.9917  LR: 0.000008  \n",
      "Epoch: [4][800/3575] Elapsed 7m 39s (remain 26m 29s) Loss: 0.0016(0.0020) Grad: 29472.1875  LR: 0.000008  \n",
      "Epoch: [4][900/3575] Elapsed 8m 35s (remain 25m 30s) Loss: 0.0000(0.0021) Grad: 7.8447  LR: 0.000008  \n",
      "Epoch: [4][1000/3575] Elapsed 9m 32s (remain 24m 31s) Loss: 0.0001(0.0023) Grad: 321.3863  LR: 0.000008  \n",
      "Epoch: [4][1100/3575] Elapsed 10m 29s (remain 23m 35s) Loss: 0.0013(0.0022) Grad: 18984.6836  LR: 0.000008  \n",
      "Epoch: [4][1200/3575] Elapsed 11m 28s (remain 22m 40s) Loss: 0.0034(0.0021) Grad: 31395.7285  LR: 0.000007  \n",
      "Epoch: [4][1300/3575] Elapsed 12m 27s (remain 21m 45s) Loss: 0.0000(0.0021) Grad: 10.7239  LR: 0.000007  \n",
      "Epoch: [4][1400/3575] Elapsed 13m 24s (remain 20m 47s) Loss: 0.0001(0.0021) Grad: 1315.9880  LR: 0.000007  \n",
      "Epoch: [4][1500/3575] Elapsed 14m 22s (remain 19m 51s) Loss: 0.0059(0.0021) Grad: 3054.9387  LR: 0.000007  \n",
      "Epoch: [4][1600/3575] Elapsed 15m 20s (remain 18m 54s) Loss: 0.0011(0.0021) Grad: 1962.9629  LR: 0.000007  \n",
      "Epoch: [4][1700/3575] Elapsed 16m 17s (remain 17m 56s) Loss: 0.0001(0.0021) Grad: 57.5229  LR: 0.000007  \n",
      "Epoch: [4][1800/3575] Elapsed 17m 14s (remain 16m 58s) Loss: 0.0000(0.0021) Grad: 8.6000  LR: 0.000007  \n",
      "Epoch: [4][1900/3575] Elapsed 18m 11s (remain 16m 1s) Loss: 0.0026(0.0021) Grad: 46100.1445  LR: 0.000007  \n",
      "Epoch: [4][2000/3575] Elapsed 19m 8s (remain 15m 3s) Loss: 0.0000(0.0022) Grad: 13.3879  LR: 0.000006  \n",
      "Epoch: [4][2100/3575] Elapsed 20m 6s (remain 14m 6s) Loss: 0.0060(0.0022) Grad: 239261.6406  LR: 0.000006  \n",
      "Epoch: [4][2200/3575] Elapsed 21m 3s (remain 13m 8s) Loss: 0.0000(0.0022) Grad: 27.7354  LR: 0.000006  \n",
      "Epoch: [4][2300/3575] Elapsed 22m 0s (remain 12m 11s) Loss: 0.0134(0.0022) Grad: 22335.2715  LR: 0.000006  \n",
      "Epoch: [4][2400/3575] Elapsed 22m 57s (remain 11m 13s) Loss: 0.0000(0.0022) Grad: 14.5795  LR: 0.000006  \n",
      "Epoch: [4][2500/3575] Elapsed 23m 55s (remain 10m 16s) Loss: 0.0000(0.0023) Grad: 20.3351  LR: 0.000006  \n",
      "Epoch: [4][2600/3575] Elapsed 24m 57s (remain 9m 20s) Loss: 0.0000(0.0023) Grad: 32.2318  LR: 0.000006  \n",
      "Epoch: [4][2700/3575] Elapsed 25m 56s (remain 8m 23s) Loss: 0.0000(0.0023) Grad: 126.7698  LR: 0.000006  \n",
      "Epoch: [4][2800/3575] Elapsed 26m 54s (remain 7m 26s) Loss: 0.0001(0.0023) Grad: 359.5059  LR: 0.000005  \n",
      "Epoch: [4][2900/3575] Elapsed 27m 50s (remain 6m 28s) Loss: 0.0000(0.0023) Grad: 23.0939  LR: 0.000005  \n",
      "Epoch: [4][3000/3575] Elapsed 28m 47s (remain 5m 30s) Loss: 0.0001(0.0023) Grad: 262.4086  LR: 0.000005  \n",
      "Epoch: [4][3100/3575] Elapsed 29m 44s (remain 4m 32s) Loss: 0.0000(0.0022) Grad: 29.7447  LR: 0.000005  \n",
      "Epoch: [4][3200/3575] Elapsed 30m 41s (remain 3m 35s) Loss: 0.0097(0.0022) Grad: 11114.4883  LR: 0.000005  \n",
      "Epoch: [4][3300/3575] Elapsed 31m 38s (remain 2m 37s) Loss: 0.0000(0.0022) Grad: 95.4282  LR: 0.000005  \n",
      "Epoch: [4][3400/3575] Elapsed 32m 35s (remain 1m 40s) Loss: 0.0000(0.0022) Grad: 4.5659  LR: 0.000005  \n",
      "Epoch: [4][3500/3575] Elapsed 33m 35s (remain 0m 42s) Loss: 0.0042(0.0022) Grad: 26871.4629  LR: 0.000005  \n",
      "Epoch: [4][3574/3575] Elapsed 34m 19s (remain 0m 0s) Loss: 0.0003(0.0022) Grad: 2151.6816  LR: 0.000004  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 12m 43s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 23s (remain 4m 8s) Loss: 0.0482(0.0130) \n",
      "EVAL: [200/1192] Elapsed 0m 42s (remain 3m 27s) Loss: 0.0175(0.0117) \n",
      "EVAL: [300/1192] Elapsed 1m 0s (remain 2m 58s) Loss: 0.0028(0.0112) \n",
      "EVAL: [400/1192] Elapsed 1m 18s (remain 2m 34s) Loss: 0.0000(0.0120) \n",
      "EVAL: [500/1192] Elapsed 1m 36s (remain 2m 13s) Loss: 0.0053(0.0112) \n",
      "EVAL: [600/1192] Elapsed 1m 54s (remain 1m 52s) Loss: 0.0170(0.0115) \n",
      "EVAL: [700/1192] Elapsed 2m 13s (remain 1m 33s) Loss: 0.0210(0.0126) \n",
      "EVAL: [800/1192] Elapsed 2m 32s (remain 1m 14s) Loss: 0.0000(0.0127) \n",
      "EVAL: [900/1192] Elapsed 2m 51s (remain 0m 55s) Loss: 0.0252(0.0129) \n",
      "EVAL: [1000/1192] Elapsed 3m 9s (remain 0m 36s) Loss: 0.0000(0.0127) \n",
      "EVAL: [1100/1192] Elapsed 3m 27s (remain 0m 17s) Loss: 0.0680(0.0120) \n",
      "EVAL: [1191/1192] Elapsed 3m 44s (remain 0m 0s) Loss: 0.0000(0.0114) \n",
      "Epoch 4 - avg_train_loss: 0.0022  avg_val_loss: 0.0114  time: 2288s\n",
      "Epoch 4 - Score: 0.8865\n",
      "Epoch: [5][0/3575] Elapsed 0m 0s (remain 57m 50s) Loss: 0.0000(0.0000) Grad: 1547.8685  LR: 0.000004  \n",
      "Epoch: [5][100/3575] Elapsed 0m 58s (remain 33m 22s) Loss: 0.0010(0.0011) Grad: 10088.7217  LR: 0.000004  \n",
      "Epoch: [5][200/3575] Elapsed 1m 54s (remain 32m 9s) Loss: 0.0001(0.0015) Grad: 70.4241  LR: 0.000004  \n",
      "Epoch: [5][300/3575] Elapsed 2m 52s (remain 31m 20s) Loss: 0.0000(0.0014) Grad: 155.6583  LR: 0.000004  \n",
      "Epoch: [5][400/3575] Elapsed 3m 50s (remain 30m 21s) Loss: 0.0006(0.0014) Grad: 1222.0133  LR: 0.000004  \n",
      "Epoch: [5][500/3575] Elapsed 4m 46s (remain 29m 19s) Loss: 0.0000(0.0015) Grad: 41.1514  LR: 0.000004  \n",
      "Epoch: [5][600/3575] Elapsed 5m 43s (remain 28m 18s) Loss: 0.0022(0.0015) Grad: 94293.0312  LR: 0.000004  \n",
      "Epoch: [5][700/3575] Elapsed 6m 39s (remain 27m 19s) Loss: 0.0058(0.0015) Grad: 13337.6748  LR: 0.000004  \n",
      "Epoch: [5][800/3575] Elapsed 7m 37s (remain 26m 24s) Loss: 0.0000(0.0015) Grad: 17.8369  LR: 0.000003  \n",
      "Epoch: [5][900/3575] Elapsed 8m 34s (remain 25m 26s) Loss: 0.0000(0.0014) Grad: 10.0951  LR: 0.000003  \n",
      "Epoch: [5][1000/3575] Elapsed 9m 31s (remain 24m 28s) Loss: 0.0000(0.0014) Grad: 3.6029  LR: 0.000003  \n",
      "Epoch: [5][1100/3575] Elapsed 10m 27s (remain 23m 30s) Loss: 0.0053(0.0014) Grad: 47195.1875  LR: 0.000003  \n",
      "Epoch: [5][1200/3575] Elapsed 11m 24s (remain 22m 33s) Loss: 0.0104(0.0014) Grad: 54548.8477  LR: 0.000003  \n",
      "Epoch: [5][1300/3575] Elapsed 12m 22s (remain 21m 37s) Loss: 0.0000(0.0014) Grad: 14.5155  LR: 0.000003  \n",
      "Epoch: [5][1400/3575] Elapsed 13m 18s (remain 20m 39s) Loss: 0.0038(0.0015) Grad: 30308.8320  LR: 0.000003  \n",
      "Epoch: [5][1500/3575] Elapsed 14m 15s (remain 19m 42s) Loss: 0.0000(0.0015) Grad: 60.5815  LR: 0.000003  \n",
      "Epoch: [5][1600/3575] Elapsed 15m 14s (remain 18m 46s) Loss: 0.0004(0.0015) Grad: 3456.9470  LR: 0.000002  \n",
      "Epoch: [5][1700/3575] Elapsed 16m 11s (remain 17m 49s) Loss: 0.0044(0.0016) Grad: 14999.7441  LR: 0.000002  \n",
      "Epoch: [5][1800/3575] Elapsed 17m 8s (remain 16m 52s) Loss: 0.0001(0.0015) Grad: 101.9276  LR: 0.000002  \n",
      "Epoch: [5][1900/3575] Elapsed 18m 4s (remain 15m 55s) Loss: 0.0073(0.0015) Grad: 32987.8633  LR: 0.000002  \n",
      "Epoch: [5][2000/3575] Elapsed 19m 2s (remain 14m 58s) Loss: 0.0001(0.0015) Grad: 132.8989  LR: 0.000002  \n",
      "Epoch: [5][2100/3575] Elapsed 19m 59s (remain 14m 1s) Loss: 0.0015(0.0015) Grad: 4010.5374  LR: 0.000002  \n",
      "Epoch: [5][2200/3575] Elapsed 20m 56s (remain 13m 4s) Loss: 0.0000(0.0015) Grad: 14.3159  LR: 0.000002  \n",
      "Epoch: [5][2300/3575] Elapsed 21m 53s (remain 12m 7s) Loss: 0.0001(0.0016) Grad: 483.0560  LR: 0.000002  \n",
      "Epoch: [5][2400/3575] Elapsed 22m 51s (remain 11m 10s) Loss: 0.0009(0.0016) Grad: 9708.7441  LR: 0.000001  \n",
      "Epoch: [5][2500/3575] Elapsed 23m 51s (remain 10m 14s) Loss: 0.0001(0.0016) Grad: 802.2864  LR: 0.000001  \n",
      "Epoch: [5][2600/3575] Elapsed 24m 49s (remain 9m 17s) Loss: 0.0054(0.0016) Grad: 8293.7607  LR: 0.000001  \n",
      "Epoch: [5][2700/3575] Elapsed 25m 46s (remain 8m 20s) Loss: 0.0000(0.0016) Grad: 4.7286  LR: 0.000001  \n",
      "Epoch: [5][2800/3575] Elapsed 26m 43s (remain 7m 23s) Loss: 0.0000(0.0016) Grad: 7.8535  LR: 0.000001  \n",
      "Epoch: [5][2900/3575] Elapsed 27m 40s (remain 6m 25s) Loss: 0.0000(0.0016) Grad: 0.4671  LR: 0.000001  \n",
      "Epoch: [5][3000/3575] Elapsed 28m 38s (remain 5m 28s) Loss: 0.0000(0.0016) Grad: 4.6907  LR: 0.000001  \n",
      "Epoch: [5][3100/3575] Elapsed 29m 34s (remain 4m 31s) Loss: 0.0001(0.0015) Grad: 738.9760  LR: 0.000001  \n",
      "Epoch: [5][3200/3575] Elapsed 30m 30s (remain 3m 33s) Loss: 0.0020(0.0015) Grad: 52469.9961  LR: 0.000000  \n",
      "Epoch: [5][3300/3575] Elapsed 31m 27s (remain 2m 36s) Loss: 0.0057(0.0015) Grad: 34647.5508  LR: 0.000000  \n",
      "Epoch: [5][3400/3575] Elapsed 32m 23s (remain 1m 39s) Loss: 0.0000(0.0016) Grad: 2.8078  LR: 0.000000  \n",
      "Epoch: [5][3500/3575] Elapsed 33m 21s (remain 0m 42s) Loss: 0.0000(0.0016) Grad: 22.6836  LR: 0.000000  \n",
      "Epoch: [5][3574/3575] Elapsed 34m 3s (remain 0m 0s) Loss: 0.0000(0.0015) Grad: 9.0150  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 13m 12s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 19s (remain 3m 27s) Loss: 0.0523(0.0137) \n",
      "EVAL: [200/1192] Elapsed 0m 38s (remain 3m 7s) Loss: 0.0214(0.0126) \n",
      "EVAL: [300/1192] Elapsed 0m 57s (remain 2m 49s) Loss: 0.0031(0.0120) \n",
      "EVAL: [400/1192] Elapsed 1m 15s (remain 2m 28s) Loss: 0.0000(0.0126) \n",
      "EVAL: [500/1192] Elapsed 1m 33s (remain 2m 8s) Loss: 0.0057(0.0118) \n",
      "EVAL: [600/1192] Elapsed 1m 51s (remain 1m 49s) Loss: 0.0282(0.0121) \n",
      "EVAL: [700/1192] Elapsed 2m 9s (remain 1m 31s) Loss: 0.0111(0.0132) \n",
      "EVAL: [800/1192] Elapsed 2m 28s (remain 1m 12s) Loss: 0.0000(0.0131) \n",
      "EVAL: [900/1192] Elapsed 2m 46s (remain 0m 53s) Loss: 0.0234(0.0133) \n",
      "EVAL: [1000/1192] Elapsed 3m 4s (remain 0m 35s) Loss: 0.0003(0.0131) \n",
      "EVAL: [1100/1192] Elapsed 3m 23s (remain 0m 16s) Loss: 0.0692(0.0124) \n",
      "EVAL: [1191/1192] Elapsed 3m 41s (remain 0m 0s) Loss: 0.0000(0.0118) \n",
      "Epoch 5 - avg_train_loss: 0.0015  avg_val_loss: 0.0118  time: 2269s\n",
      "Epoch 5 - Score: 0.8863\n",
      "========== fold: 3 training ==========\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n",
      "Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp054/fold3_best.pth\n",
      "Epoch: [1][0/3575] Elapsed 0m 0s (remain 56m 29s) Loss: 0.3281(0.3281) Grad: 85639.0703  LR: 0.000000  \n",
      "Epoch: [1][100/3575] Elapsed 1m 1s (remain 35m 1s) Loss: 0.2814(0.3129) Grad: 79145.0781  LR: 0.000001  \n",
      "Epoch: [1][200/3575] Elapsed 1m 58s (remain 33m 10s) Loss: 0.1681(0.2702) Grad: 52955.5469  LR: 0.000002  \n",
      "Epoch: [1][300/3575] Elapsed 2m 55s (remain 31m 53s) Loss: 0.0728(0.2200) Grad: 29718.5508  LR: 0.000003  \n",
      "Epoch: [1][400/3575] Elapsed 3m 53s (remain 30m 45s) Loss: 0.0330(0.1778) Grad: 15979.1865  LR: 0.000004  \n",
      "Epoch: [1][500/3575] Elapsed 4m 50s (remain 29m 40s) Loss: 0.0087(0.1464) Grad: 4188.5469  LR: 0.000006  \n",
      "Epoch: [1][600/3575] Elapsed 5m 49s (remain 28m 49s) Loss: 0.0052(0.1236) Grad: 1746.2355  LR: 0.000007  \n",
      "Epoch: [1][700/3575] Elapsed 6m 48s (remain 27m 52s) Loss: 0.0016(0.1067) Grad: 1720.3658  LR: 0.000008  \n",
      "Epoch: [1][800/3575] Elapsed 7m 45s (remain 26m 51s) Loss: 0.0015(0.0938) Grad: 483.6500  LR: 0.000009  \n",
      "Epoch: [1][900/3575] Elapsed 8m 42s (remain 25m 51s) Loss: 0.0009(0.0840) Grad: 365.6963  LR: 0.000010  \n",
      "Epoch: [1][1000/3575] Elapsed 9m 39s (remain 24m 50s) Loss: 0.0038(0.0760) Grad: 28632.9531  LR: 0.000011  \n",
      "Epoch: [1][1100/3575] Elapsed 10m 37s (remain 23m 51s) Loss: 0.0007(0.0695) Grad: 273.6757  LR: 0.000012  \n",
      "Epoch: [1][1200/3575] Elapsed 11m 35s (remain 22m 55s) Loss: 0.0004(0.0641) Grad: 149.1540  LR: 0.000013  \n",
      "Epoch: [1][1300/3575] Elapsed 12m 32s (remain 21m 55s) Loss: 0.0063(0.0595) Grad: 14228.8105  LR: 0.000015  \n",
      "Epoch: [1][1400/3575] Elapsed 13m 29s (remain 20m 55s) Loss: 0.0734(0.0556) Grad: 77927.1016  LR: 0.000016  \n",
      "Epoch: [1][1500/3575] Elapsed 14m 26s (remain 19m 57s) Loss: 0.0002(0.0522) Grad: 86.7342  LR: 0.000017  \n",
      "Epoch: [1][1600/3575] Elapsed 15m 27s (remain 19m 3s) Loss: 0.0028(0.0492) Grad: 47429.2344  LR: 0.000018  \n",
      "Epoch: [1][1700/3575] Elapsed 16m 25s (remain 18m 5s) Loss: 0.0002(0.0466) Grad: 76.6301  LR: 0.000019  \n",
      "Epoch: [1][1800/3575] Elapsed 17m 22s (remain 17m 6s) Loss: 0.0014(0.0442) Grad: 10714.0771  LR: 0.000020  \n",
      "Epoch: [1][1900/3575] Elapsed 18m 18s (remain 16m 7s) Loss: 0.0007(0.0422) Grad: 421.2305  LR: 0.000020  \n",
      "Epoch: [1][2000/3575] Elapsed 19m 15s (remain 15m 8s) Loss: 0.0014(0.0403) Grad: 150.0245  LR: 0.000020  \n",
      "Epoch: [1][2100/3575] Elapsed 20m 12s (remain 14m 10s) Loss: 0.0004(0.0387) Grad: 182.4495  LR: 0.000020  \n",
      "Epoch: [1][2200/3575] Elapsed 21m 11s (remain 13m 13s) Loss: 0.0007(0.0371) Grad: 1149.1737  LR: 0.000019  \n",
      "Epoch: [1][2300/3575] Elapsed 22m 7s (remain 12m 15s) Loss: 0.0000(0.0357) Grad: 17.3164  LR: 0.000019  \n",
      "Epoch: [1][2400/3575] Elapsed 23m 4s (remain 11m 16s) Loss: 0.0123(0.0344) Grad: 51736.5820  LR: 0.000019  \n",
      "Epoch: [1][2500/3575] Elapsed 24m 2s (remain 10m 19s) Loss: 0.0802(0.0333) Grad: 231732.3281  LR: 0.000019  \n",
      "Epoch: [1][2600/3575] Elapsed 25m 2s (remain 9m 22s) Loss: 0.0002(0.0323) Grad: 106.6898  LR: 0.000019  \n",
      "Epoch: [1][2700/3575] Elapsed 25m 58s (remain 8m 24s) Loss: 0.0001(0.0312) Grad: 43.0884  LR: 0.000019  \n",
      "Epoch: [1][2800/3575] Elapsed 26m 55s (remain 7m 26s) Loss: 0.0001(0.0303) Grad: 126.0017  LR: 0.000019  \n",
      "Epoch: [1][2900/3575] Elapsed 27m 52s (remain 6m 28s) Loss: 0.0006(0.0294) Grad: 2149.9072  LR: 0.000019  \n",
      "Epoch: [1][3000/3575] Elapsed 28m 49s (remain 5m 30s) Loss: 0.0001(0.0286) Grad: 107.4935  LR: 0.000018  \n",
      "Epoch: [1][3100/3575] Elapsed 29m 47s (remain 4m 33s) Loss: 0.0009(0.0278) Grad: 3002.5364  LR: 0.000018  \n",
      "Epoch: [1][3200/3575] Elapsed 30m 44s (remain 3m 35s) Loss: 0.0002(0.0270) Grad: 904.7946  LR: 0.000018  \n",
      "Epoch: [1][3300/3575] Elapsed 31m 41s (remain 2m 37s) Loss: 0.0001(0.0264) Grad: 299.7072  LR: 0.000018  \n",
      "Epoch: [1][3400/3575] Elapsed 32m 37s (remain 1m 40s) Loss: 0.0000(0.0257) Grad: 39.3473  LR: 0.000018  \n",
      "Epoch: [1][3500/3575] Elapsed 33m 34s (remain 0m 42s) Loss: 0.0001(0.0251) Grad: 248.8718  LR: 0.000018  \n",
      "Epoch: [1][3574/3575] Elapsed 34m 16s (remain 0m 0s) Loss: 0.0020(0.0247) Grad: 31377.2402  LR: 0.000018  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 11m 38s) Loss: 0.0001(0.0001) \n",
      "EVAL: [100/1192] Elapsed 0m 19s (remain 3m 35s) Loss: 0.0583(0.0089) \n",
      "EVAL: [200/1192] Elapsed 0m 38s (remain 3m 8s) Loss: 0.0111(0.0084) \n",
      "EVAL: [300/1192] Elapsed 0m 56s (remain 2m 46s) Loss: 0.0095(0.0101) \n",
      "EVAL: [400/1192] Elapsed 1m 14s (remain 2m 26s) Loss: 0.0000(0.0096) \n",
      "EVAL: [500/1192] Elapsed 1m 32s (remain 2m 7s) Loss: 0.0555(0.0092) \n",
      "EVAL: [600/1192] Elapsed 1m 51s (remain 1m 50s) Loss: 0.0120(0.0096) \n",
      "EVAL: [700/1192] Elapsed 2m 9s (remain 1m 31s) Loss: 0.0029(0.0106) \n",
      "EVAL: [800/1192] Elapsed 2m 27s (remain 1m 12s) Loss: 0.0199(0.0104) \n",
      "EVAL: [900/1192] Elapsed 2m 46s (remain 0m 53s) Loss: 0.0121(0.0107) \n",
      "EVAL: [1000/1192] Elapsed 3m 5s (remain 0m 35s) Loss: 0.0001(0.0104) \n",
      "EVAL: [1100/1192] Elapsed 3m 23s (remain 0m 16s) Loss: 0.0208(0.0101) \n",
      "EVAL: [1191/1192] Elapsed 3m 40s (remain 0m 0s) Loss: 0.0001(0.0098) \n",
      "Epoch 1 - avg_train_loss: 0.0247  avg_val_loss: 0.0098  time: 2281s\n",
      "Epoch 1 - Score: 0.8823\n",
      "Epoch 1 - Save Best Score: 0.8823 Model\n",
      "Epoch: [2][0/3575] Elapsed 0m 1s (remain 60m 50s) Loss: 0.0002(0.0002) Grad: 1008.4808  LR: 0.000018  \n",
      "Epoch: [2][100/3575] Elapsed 0m 58s (remain 33m 44s) Loss: 0.0001(0.0031) Grad: 269.8754  LR: 0.000018  \n",
      "Epoch: [2][200/3575] Elapsed 1m 55s (remain 32m 26s) Loss: 0.0001(0.0036) Grad: 67.8563  LR: 0.000018  \n",
      "Epoch: [2][300/3575] Elapsed 2m 52s (remain 31m 21s) Loss: 0.0014(0.0037) Grad: 4109.4644  LR: 0.000017  \n",
      "Epoch: [2][400/3575] Elapsed 3m 50s (remain 30m 20s) Loss: 0.0118(0.0035) Grad: 71533.6406  LR: 0.000017  \n",
      "Epoch: [2][500/3575] Elapsed 4m 47s (remain 29m 26s) Loss: 0.0001(0.0037) Grad: 181.4190  LR: 0.000017  \n",
      "Epoch: [2][600/3575] Elapsed 5m 47s (remain 28m 37s) Loss: 0.0068(0.0038) Grad: 10544.7598  LR: 0.000017  \n",
      "Epoch: [2][700/3575] Elapsed 6m 44s (remain 27m 39s) Loss: 0.0003(0.0037) Grad: 2767.0576  LR: 0.000017  \n",
      "Epoch: [2][800/3575] Elapsed 7m 41s (remain 26m 39s) Loss: 0.0001(0.0037) Grad: 172.4391  LR: 0.000017  \n",
      "Epoch: [2][900/3575] Elapsed 8m 38s (remain 25m 39s) Loss: 0.0003(0.0038) Grad: 5457.8633  LR: 0.000017  \n",
      "Epoch: [2][1000/3575] Elapsed 9m 36s (remain 24m 41s) Loss: 0.0001(0.0039) Grad: 47.8151  LR: 0.000017  \n",
      "Epoch: [2][1100/3575] Elapsed 10m 34s (remain 23m 45s) Loss: 0.0001(0.0039) Grad: 420.9106  LR: 0.000016  \n",
      "Epoch: [2][1200/3575] Elapsed 11m 32s (remain 22m 48s) Loss: 0.0016(0.0040) Grad: 12379.0596  LR: 0.000016  \n",
      "Epoch: [2][1300/3575] Elapsed 12m 29s (remain 21m 49s) Loss: 0.0000(0.0039) Grad: 23.2156  LR: 0.000016  \n",
      "Epoch: [2][1400/3575] Elapsed 13m 25s (remain 20m 50s) Loss: 0.0001(0.0040) Grad: 70.8628  LR: 0.000016  \n",
      "Epoch: [2][1500/3575] Elapsed 14m 22s (remain 19m 51s) Loss: 0.0128(0.0040) Grad: 6754.9497  LR: 0.000016  \n",
      "Epoch: [2][1600/3575] Elapsed 15m 20s (remain 18m 54s) Loss: 0.0000(0.0040) Grad: 40.2840  LR: 0.000016  \n",
      "Epoch: [2][1700/3575] Elapsed 16m 18s (remain 17m 58s) Loss: 0.0000(0.0040) Grad: 28.5046  LR: 0.000016  \n",
      "Epoch: [2][1800/3575] Elapsed 17m 15s (remain 17m 0s) Loss: 0.0001(0.0040) Grad: 55.9709  LR: 0.000016  \n",
      "Epoch: [2][1900/3575] Elapsed 18m 12s (remain 16m 2s) Loss: 0.0000(0.0040) Grad: 14.8395  LR: 0.000015  \n",
      "Epoch: [2][2000/3575] Elapsed 19m 8s (remain 15m 3s) Loss: 0.0000(0.0039) Grad: 20.6955  LR: 0.000015  \n",
      "Epoch: [2][2100/3575] Elapsed 20m 5s (remain 14m 5s) Loss: 0.0001(0.0039) Grad: 97.4613  LR: 0.000015  \n",
      "Epoch: [2][2200/3575] Elapsed 21m 3s (remain 13m 8s) Loss: 0.0000(0.0039) Grad: 12.7578  LR: 0.000015  \n",
      "Epoch: [2][2300/3575] Elapsed 22m 2s (remain 12m 12s) Loss: 0.0066(0.0039) Grad: 22287.7363  LR: 0.000015  \n",
      "Epoch: [2][2400/3575] Elapsed 22m 59s (remain 11m 14s) Loss: 0.0001(0.0039) Grad: 955.5628  LR: 0.000015  \n",
      "Epoch: [2][2500/3575] Elapsed 23m 56s (remain 10m 16s) Loss: 0.0006(0.0039) Grad: 4111.3511  LR: 0.000015  \n",
      "Epoch: [2][2600/3575] Elapsed 24m 53s (remain 9m 19s) Loss: 0.0001(0.0039) Grad: 299.9605  LR: 0.000015  \n",
      "Epoch: [2][2700/3575] Elapsed 25m 51s (remain 8m 21s) Loss: 0.0000(0.0039) Grad: 7.9222  LR: 0.000014  \n",
      "Epoch: [2][2800/3575] Elapsed 26m 49s (remain 7m 24s) Loss: 0.0000(0.0038) Grad: 111.5518  LR: 0.000014  \n",
      "Epoch: [2][2900/3575] Elapsed 27m 46s (remain 6m 27s) Loss: 0.0000(0.0039) Grad: 2.3894  LR: 0.000014  \n",
      "Epoch: [2][3000/3575] Elapsed 28m 42s (remain 5m 29s) Loss: 0.0001(0.0039) Grad: 354.6302  LR: 0.000014  \n",
      "Epoch: [2][3100/3575] Elapsed 29m 39s (remain 4m 32s) Loss: 0.0002(0.0039) Grad: 1900.6219  LR: 0.000014  \n",
      "Epoch: [2][3200/3575] Elapsed 30m 35s (remain 3m 34s) Loss: 0.0284(0.0039) Grad: 33539.7500  LR: 0.000014  \n",
      "Epoch: [2][3300/3575] Elapsed 31m 32s (remain 2m 37s) Loss: 0.0001(0.0039) Grad: 215.4613  LR: 0.000014  \n",
      "Epoch: [2][3400/3575] Elapsed 32m 30s (remain 1m 39s) Loss: 0.0069(0.0039) Grad: 15706.5596  LR: 0.000014  \n",
      "Epoch: [2][3500/3575] Elapsed 33m 28s (remain 0m 42s) Loss: 0.0000(0.0039) Grad: 16.3907  LR: 0.000013  \n",
      "Epoch: [2][3574/3575] Elapsed 34m 10s (remain 0m 0s) Loss: 0.0000(0.0038) Grad: 12.3260  LR: 0.000013  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 11m 59s) Loss: 0.0224(0.0224) \n",
      "EVAL: [100/1192] Elapsed 0m 19s (remain 3m 27s) Loss: 0.0678(0.0162) \n",
      "EVAL: [200/1192] Elapsed 0m 38s (remain 3m 8s) Loss: 0.0320(0.0127) \n",
      "EVAL: [300/1192] Elapsed 0m 56s (remain 2m 48s) Loss: 0.0104(0.0127) \n",
      "EVAL: [400/1192] Elapsed 1m 15s (remain 2m 27s) Loss: 0.0000(0.0121) \n",
      "EVAL: [500/1192] Elapsed 1m 32s (remain 2m 8s) Loss: 0.0713(0.0115) \n",
      "EVAL: [600/1192] Elapsed 1m 50s (remain 1m 49s) Loss: 0.0214(0.0116) \n",
      "EVAL: [700/1192] Elapsed 2m 9s (remain 1m 30s) Loss: 0.0046(0.0128) \n",
      "EVAL: [800/1192] Elapsed 2m 27s (remain 1m 11s) Loss: 0.0271(0.0128) \n",
      "EVAL: [900/1192] Elapsed 2m 46s (remain 0m 53s) Loss: 0.0105(0.0133) \n",
      "EVAL: [1000/1192] Elapsed 3m 4s (remain 0m 35s) Loss: 0.0000(0.0127) \n",
      "EVAL: [1100/1192] Elapsed 3m 22s (remain 0m 16s) Loss: 0.0210(0.0122) \n",
      "EVAL: [1191/1192] Elapsed 3m 39s (remain 0m 0s) Loss: 0.0000(0.0117) \n",
      "Epoch 2 - avg_train_loss: 0.0038  avg_val_loss: 0.0117  time: 2274s\n",
      "Epoch 2 - Score: 0.8348\n",
      "Epoch: [3][0/3575] Elapsed 0m 0s (remain 56m 57s) Loss: 0.0001(0.0001) Grad: 787.1882  LR: 0.000013  \n",
      "Epoch: [3][100/3575] Elapsed 0m 57s (remain 32m 52s) Loss: 0.0000(0.0031) Grad: 63.9454  LR: 0.000013  \n",
      "Epoch: [3][200/3575] Elapsed 1m 57s (remain 32m 51s) Loss: 0.0000(0.0027) Grad: 21.2339  LR: 0.000013  \n",
      "Epoch: [3][300/3575] Elapsed 2m 55s (remain 31m 49s) Loss: 0.0004(0.0027) Grad: 371.6625  LR: 0.000013  \n",
      "Epoch: [3][400/3575] Elapsed 3m 51s (remain 30m 36s) Loss: 0.0002(0.0027) Grad: 1812.9375  LR: 0.000013  \n",
      "Epoch: [3][500/3575] Elapsed 4m 48s (remain 29m 30s) Loss: 0.0000(0.0027) Grad: 30.5316  LR: 0.000013  \n",
      "Epoch: [3][600/3575] Elapsed 5m 45s (remain 28m 28s) Loss: 0.0023(0.0029) Grad: 4859.8896  LR: 0.000013  \n",
      "Epoch: [3][700/3575] Elapsed 6m 46s (remain 27m 44s) Loss: 0.0005(0.0028) Grad: 3612.5413  LR: 0.000012  \n",
      "Epoch: [3][800/3575] Elapsed 7m 43s (remain 26m 45s) Loss: 0.0001(0.0027) Grad: 994.5164  LR: 0.000012  \n",
      "Epoch: [3][900/3575] Elapsed 8m 40s (remain 25m 43s) Loss: 0.0052(0.0028) Grad: 7728.3721  LR: 0.000012  \n",
      "Epoch: [3][1000/3575] Elapsed 9m 37s (remain 24m 44s) Loss: 0.0001(0.0028) Grad: 107.9902  LR: 0.000012  \n",
      "Epoch: [3][1100/3575] Elapsed 10m 34s (remain 23m 45s) Loss: 0.0000(0.0027) Grad: 17.1463  LR: 0.000012  \n",
      "Epoch: [3][1200/3575] Elapsed 11m 31s (remain 22m 46s) Loss: 0.0000(0.0027) Grad: 67.4662  LR: 0.000012  \n",
      "Epoch: [3][1300/3575] Elapsed 12m 28s (remain 21m 47s) Loss: 0.0001(0.0027) Grad: 218.9338  LR: 0.000012  \n",
      "Epoch: [3][1400/3575] Elapsed 13m 25s (remain 20m 50s) Loss: 0.0003(0.0027) Grad: 1146.6698  LR: 0.000012  \n",
      "Epoch: [3][1500/3575] Elapsed 14m 26s (remain 19m 56s) Loss: 0.0000(0.0026) Grad: 50.2486  LR: 0.000011  \n",
      "Epoch: [3][1600/3575] Elapsed 15m 24s (remain 18m 59s) Loss: 0.0002(0.0026) Grad: 5685.4790  LR: 0.000011  \n",
      "Epoch: [3][1700/3575] Elapsed 16m 21s (remain 18m 1s) Loss: 0.0001(0.0026) Grad: 4058.1079  LR: 0.000011  \n",
      "Epoch: [3][1800/3575] Elapsed 17m 17s (remain 17m 2s) Loss: 0.0001(0.0027) Grad: 1225.0177  LR: 0.000011  \n",
      "Epoch: [3][1900/3575] Elapsed 18m 14s (remain 16m 3s) Loss: 0.0001(0.0026) Grad: 276.5060  LR: 0.000011  \n",
      "Epoch: [3][2000/3575] Elapsed 19m 12s (remain 15m 6s) Loss: 0.0000(0.0026) Grad: 43.0027  LR: 0.000011  \n",
      "Epoch: [3][2100/3575] Elapsed 20m 8s (remain 14m 8s) Loss: 0.0000(0.0026) Grad: 34.6197  LR: 0.000011  \n",
      "Epoch: [3][2200/3575] Elapsed 21m 5s (remain 13m 10s) Loss: 0.0001(0.0026) Grad: 305.3775  LR: 0.000011  \n",
      "Epoch: [3][2300/3575] Elapsed 22m 3s (remain 12m 12s) Loss: 0.0098(0.0026) Grad: 62206.5898  LR: 0.000010  \n",
      "Epoch: [3][2400/3575] Elapsed 23m 0s (remain 11m 15s) Loss: 0.0000(0.0026) Grad: 49.0182  LR: 0.000010  \n",
      "Epoch: [3][2500/3575] Elapsed 23m 57s (remain 10m 17s) Loss: 0.0010(0.0027) Grad: 18956.2285  LR: 0.000010  \n",
      "Epoch: [3][2600/3575] Elapsed 24m 54s (remain 9m 19s) Loss: 0.0000(0.0027) Grad: 13.9298  LR: 0.000010  \n",
      "Epoch: [3][2700/3575] Elapsed 25m 54s (remain 8m 23s) Loss: 0.0007(0.0027) Grad: 1180.1635  LR: 0.000010  \n",
      "Epoch: [3][2800/3575] Elapsed 26m 51s (remain 7m 25s) Loss: 0.0000(0.0027) Grad: 91.5096  LR: 0.000010  \n",
      "Epoch: [3][2900/3575] Elapsed 27m 48s (remain 6m 27s) Loss: 0.0001(0.0027) Grad: 742.2537  LR: 0.000010  \n",
      "Epoch: [3][3000/3575] Elapsed 28m 45s (remain 5m 30s) Loss: 0.0079(0.0027) Grad: 5241.1758  LR: 0.000010  \n",
      "Epoch: [3][3100/3575] Elapsed 29m 42s (remain 4m 32s) Loss: 0.0000(0.0027) Grad: 1.0105  LR: 0.000009  \n",
      "Epoch: [3][3200/3575] Elapsed 30m 40s (remain 3m 35s) Loss: 0.0001(0.0027) Grad: 113.0813  LR: 0.000009  \n",
      "Epoch: [3][3300/3575] Elapsed 31m 37s (remain 2m 37s) Loss: 0.0000(0.0028) Grad: 105.6168  LR: 0.000009  \n",
      "Epoch: [3][3400/3575] Elapsed 32m 33s (remain 1m 39s) Loss: 0.0047(0.0027) Grad: 13568.8994  LR: 0.000009  \n",
      "Epoch: [3][3500/3575] Elapsed 33m 31s (remain 0m 42s) Loss: 0.0001(0.0027) Grad: 265.8893  LR: 0.000009  \n",
      "Epoch: [3][3574/3575] Elapsed 34m 16s (remain 0m 0s) Loss: 0.0000(0.0028) Grad: 6.9551  LR: 0.000009  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 12m 44s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 18s (remain 3m 23s) Loss: 0.0575(0.0115) \n",
      "EVAL: [200/1192] Elapsed 0m 36s (remain 3m 1s) Loss: 0.0143(0.0104) \n",
      "EVAL: [300/1192] Elapsed 0m 55s (remain 2m 44s) Loss: 0.0121(0.0110) \n",
      "EVAL: [400/1192] Elapsed 1m 14s (remain 2m 27s) Loss: 0.0000(0.0105) \n",
      "EVAL: [500/1192] Elapsed 1m 32s (remain 2m 8s) Loss: 0.0796(0.0103) \n",
      "EVAL: [600/1192] Elapsed 1m 51s (remain 1m 49s) Loss: 0.0127(0.0103) \n",
      "EVAL: [700/1192] Elapsed 2m 9s (remain 1m 30s) Loss: 0.0049(0.0115) \n",
      "EVAL: [800/1192] Elapsed 2m 28s (remain 1m 12s) Loss: 0.0205(0.0116) \n",
      "EVAL: [900/1192] Elapsed 2m 48s (remain 0m 54s) Loss: 0.0031(0.0119) \n",
      "EVAL: [1000/1192] Elapsed 3m 6s (remain 0m 35s) Loss: 0.0000(0.0114) \n",
      "EVAL: [1100/1192] Elapsed 3m 25s (remain 0m 16s) Loss: 0.0259(0.0111) \n",
      "EVAL: [1191/1192] Elapsed 3m 41s (remain 0m 0s) Loss: 0.0000(0.0107) \n",
      "Epoch 3 - avg_train_loss: 0.0028  avg_val_loss: 0.0107  time: 2282s\n",
      "Epoch 3 - Score: 0.8653\n",
      "Epoch: [4][0/3575] Elapsed 0m 0s (remain 54m 23s) Loss: 0.0000(0.0000) Grad: 11.6334  LR: 0.000009  \n",
      "Epoch: [4][100/3575] Elapsed 0m 58s (remain 33m 15s) Loss: 0.0000(0.0018) Grad: 35.7009  LR: 0.000009  \n",
      "Epoch: [4][200/3575] Elapsed 1m 55s (remain 32m 22s) Loss: 0.0002(0.0023) Grad: 4837.8154  LR: 0.000009  \n",
      "Epoch: [4][300/3575] Elapsed 2m 52s (remain 31m 14s) Loss: 0.0041(0.0022) Grad: 27307.9863  LR: 0.000009  \n",
      "Epoch: [4][400/3575] Elapsed 3m 49s (remain 30m 20s) Loss: 0.0179(0.0024) Grad: 43452.1758  LR: 0.000008  \n",
      "Epoch: [4][500/3575] Elapsed 4m 49s (remain 29m 36s) Loss: 0.0000(0.0024) Grad: 9.6488  LR: 0.000008  \n",
      "Epoch: [4][600/3575] Elapsed 5m 46s (remain 28m 33s) Loss: 0.0059(0.0025) Grad: 18352.4863  LR: 0.000008  \n",
      "Epoch: [4][700/3575] Elapsed 6m 42s (remain 27m 32s) Loss: 0.0000(0.0024) Grad: 176.6688  LR: 0.000008  \n",
      "Epoch: [4][800/3575] Elapsed 7m 39s (remain 26m 31s) Loss: 0.0000(0.0023) Grad: 420.2801  LR: 0.000008  \n",
      "Epoch: [4][900/3575] Elapsed 8m 35s (remain 25m 30s) Loss: 0.0001(0.0022) Grad: 78.2636  LR: 0.000008  \n",
      "Epoch: [4][1000/3575] Elapsed 9m 31s (remain 24m 30s) Loss: 0.0001(0.0022) Grad: 652.8635  LR: 0.000008  \n",
      "Epoch: [4][1100/3575] Elapsed 10m 30s (remain 23m 36s) Loss: 0.0000(0.0023) Grad: 11.9656  LR: 0.000008  \n",
      "Epoch: [4][1200/3575] Elapsed 11m 29s (remain 22m 42s) Loss: 0.0000(0.0024) Grad: 38.3847  LR: 0.000007  \n",
      "Epoch: [4][1300/3575] Elapsed 12m 25s (remain 21m 42s) Loss: 0.0035(0.0023) Grad: 76236.0000  LR: 0.000007  \n",
      "Epoch: [4][1400/3575] Elapsed 13m 21s (remain 20m 44s) Loss: 0.0000(0.0022) Grad: 62.7310  LR: 0.000007  \n",
      "Epoch: [4][1500/3575] Elapsed 14m 18s (remain 19m 46s) Loss: 0.0000(0.0022) Grad: 54.0747  LR: 0.000007  \n",
      "Epoch: [4][1600/3575] Elapsed 15m 19s (remain 18m 53s) Loss: 0.0000(0.0022) Grad: 24.3932  LR: 0.000007  \n",
      "Epoch: [4][1700/3575] Elapsed 16m 16s (remain 17m 55s) Loss: 0.0008(0.0022) Grad: 5442.8379  LR: 0.000007  \n",
      "Epoch: [4][1800/3575] Elapsed 17m 12s (remain 16m 57s) Loss: 0.0000(0.0021) Grad: 56.6707  LR: 0.000007  \n",
      "Epoch: [4][1900/3575] Elapsed 18m 9s (remain 15m 59s) Loss: 0.0000(0.0021) Grad: 168.5345  LR: 0.000007  \n",
      "Epoch: [4][2000/3575] Elapsed 19m 6s (remain 15m 1s) Loss: 0.0001(0.0021) Grad: 628.7474  LR: 0.000006  \n",
      "Epoch: [4][2100/3575] Elapsed 20m 5s (remain 14m 5s) Loss: 0.0325(0.0021) Grad: 151418.9375  LR: 0.000006  \n",
      "Epoch: [4][2200/3575] Elapsed 21m 1s (remain 13m 7s) Loss: 0.0000(0.0021) Grad: 35.1474  LR: 0.000006  \n",
      "Epoch: [4][2300/3575] Elapsed 21m 58s (remain 12m 9s) Loss: 0.0011(0.0021) Grad: 4483.0259  LR: 0.000006  \n",
      "Epoch: [4][2400/3575] Elapsed 22m 54s (remain 11m 12s) Loss: 0.0000(0.0020) Grad: 131.8155  LR: 0.000006  \n",
      "Epoch: [4][2500/3575] Elapsed 23m 50s (remain 10m 14s) Loss: 0.0000(0.0020) Grad: 17.1918  LR: 0.000006  \n",
      "Epoch: [4][2600/3575] Elapsed 24m 47s (remain 9m 17s) Loss: 0.0001(0.0020) Grad: 153.2518  LR: 0.000006  \n",
      "Epoch: [4][2700/3575] Elapsed 25m 46s (remain 8m 20s) Loss: 0.0000(0.0020) Grad: 0.9711  LR: 0.000006  \n",
      "Epoch: [4][2800/3575] Elapsed 26m 45s (remain 7m 23s) Loss: 0.0000(0.0020) Grad: 95.0285  LR: 0.000005  \n",
      "Epoch: [4][2900/3575] Elapsed 27m 41s (remain 6m 25s) Loss: 0.0000(0.0020) Grad: 17.7767  LR: 0.000005  \n",
      "Epoch: [4][3000/3575] Elapsed 28m 37s (remain 5m 28s) Loss: 0.0001(0.0020) Grad: 292.7040  LR: 0.000005  \n",
      "Epoch: [4][3100/3575] Elapsed 29m 34s (remain 4m 31s) Loss: 0.0015(0.0019) Grad: 67754.3047  LR: 0.000005  \n",
      "Epoch: [4][3200/3575] Elapsed 30m 33s (remain 3m 34s) Loss: 0.0000(0.0019) Grad: 236.1183  LR: 0.000005  \n",
      "Epoch: [4][3300/3575] Elapsed 31m 32s (remain 2m 37s) Loss: 0.0000(0.0019) Grad: 76.7764  LR: 0.000005  \n",
      "Epoch: [4][3400/3575] Elapsed 32m 29s (remain 1m 39s) Loss: 0.0089(0.0019) Grad: 6358.9341  LR: 0.000005  \n",
      "Epoch: [4][3500/3575] Elapsed 33m 25s (remain 0m 42s) Loss: 0.0001(0.0019) Grad: 173.1373  LR: 0.000005  \n",
      "Epoch: [4][3574/3575] Elapsed 34m 6s (remain 0m 0s) Loss: 0.0000(0.0019) Grad: 15.0222  LR: 0.000004  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 11m 52s) Loss: 0.0001(0.0001) \n",
      "EVAL: [100/1192] Elapsed 0m 18s (remain 3m 19s) Loss: 0.0485(0.0143) \n",
      "EVAL: [200/1192] Elapsed 0m 36s (remain 2m 59s) Loss: 0.0085(0.0119) \n",
      "EVAL: [300/1192] Elapsed 0m 55s (remain 2m 42s) Loss: 0.0119(0.0122) \n",
      "EVAL: [400/1192] Elapsed 1m 14s (remain 2m 26s) Loss: 0.0000(0.0116) \n",
      "EVAL: [500/1192] Elapsed 1m 33s (remain 2m 8s) Loss: 0.0787(0.0111) \n",
      "EVAL: [600/1192] Elapsed 1m 51s (remain 1m 49s) Loss: 0.0137(0.0114) \n",
      "EVAL: [700/1192] Elapsed 2m 9s (remain 1m 30s) Loss: 0.0051(0.0128) \n",
      "EVAL: [800/1192] Elapsed 2m 27s (remain 1m 11s) Loss: 0.0219(0.0128) \n",
      "EVAL: [900/1192] Elapsed 2m 45s (remain 0m 53s) Loss: 0.0107(0.0134) \n",
      "EVAL: [1000/1192] Elapsed 3m 3s (remain 0m 34s) Loss: 0.0000(0.0128) \n",
      "EVAL: [1100/1192] Elapsed 3m 21s (remain 0m 16s) Loss: 0.0312(0.0124) \n",
      "EVAL: [1191/1192] Elapsed 3m 38s (remain 0m 0s) Loss: 0.0000(0.0120) \n",
      "Epoch 4 - avg_train_loss: 0.0019  avg_val_loss: 0.0120  time: 2269s\n",
      "Epoch 4 - Score: 0.8618\n",
      "Epoch: [5][0/3575] Elapsed 0m 0s (remain 56m 2s) Loss: 0.0001(0.0001) Grad: 392.8836  LR: 0.000004  \n",
      "Epoch: [5][100/3575] Elapsed 0m 57s (remain 33m 0s) Loss: 0.0000(0.0012) Grad: 99.0323  LR: 0.000004  \n",
      "Epoch: [5][200/3575] Elapsed 1m 53s (remain 31m 52s) Loss: 0.0000(0.0014) Grad: 94.4467  LR: 0.000004  \n",
      "Epoch: [5][300/3575] Elapsed 2m 51s (remain 31m 9s) Loss: 0.0000(0.0014) Grad: 14.7082  LR: 0.000004  \n",
      "Epoch: [5][400/3575] Elapsed 3m 49s (remain 30m 19s) Loss: 0.0000(0.0013) Grad: 14.1165  LR: 0.000004  \n",
      "Epoch: [5][500/3575] Elapsed 4m 46s (remain 29m 17s) Loss: 0.0000(0.0014) Grad: 110.1296  LR: 0.000004  \n",
      "Epoch: [5][600/3575] Elapsed 5m 43s (remain 28m 18s) Loss: 0.0001(0.0015) Grad: 941.2321  LR: 0.000004  \n",
      "Epoch: [5][700/3575] Elapsed 6m 40s (remain 27m 20s) Loss: 0.0018(0.0016) Grad: 66346.5703  LR: 0.000004  \n",
      "Epoch: [5][800/3575] Elapsed 7m 37s (remain 26m 22s) Loss: 0.0000(0.0016) Grad: 14.1524  LR: 0.000003  \n",
      "Epoch: [5][900/3575] Elapsed 8m 35s (remain 25m 31s) Loss: 0.0000(0.0015) Grad: 6.9923  LR: 0.000003  \n",
      "Epoch: [5][1000/3575] Elapsed 9m 33s (remain 24m 35s) Loss: 0.0011(0.0016) Grad: 2737.6257  LR: 0.000003  \n",
      "Epoch: [5][1100/3575] Elapsed 10m 31s (remain 23m 38s) Loss: 0.0000(0.0015) Grad: 80.6845  LR: 0.000003  \n",
      "Epoch: [5][1200/3575] Elapsed 11m 31s (remain 22m 47s) Loss: 0.0000(0.0014) Grad: 59.5642  LR: 0.000003  \n",
      "Epoch: [5][1300/3575] Elapsed 12m 28s (remain 21m 48s) Loss: 0.0000(0.0015) Grad: 241.5344  LR: 0.000003  \n",
      "Epoch: [5][1400/3575] Elapsed 13m 27s (remain 20m 52s) Loss: 0.0177(0.0015) Grad: 19173.0625  LR: 0.000003  \n",
      "Epoch: [5][1500/3575] Elapsed 14m 24s (remain 19m 53s) Loss: 0.0000(0.0015) Grad: 15.7821  LR: 0.000003  \n",
      "Epoch: [5][1600/3575] Elapsed 15m 20s (remain 18m 55s) Loss: 0.0000(0.0015) Grad: 38.0219  LR: 0.000002  \n",
      "Epoch: [5][1700/3575] Elapsed 16m 17s (remain 17m 56s) Loss: 0.0000(0.0014) Grad: 0.4844  LR: 0.000002  \n",
      "Epoch: [5][1800/3575] Elapsed 17m 15s (remain 16m 59s) Loss: 0.0000(0.0015) Grad: 72.9394  LR: 0.000002  \n",
      "Epoch: [5][1900/3575] Elapsed 18m 11s (remain 16m 1s) Loss: 0.0000(0.0015) Grad: 24.8617  LR: 0.000002  \n",
      "Epoch: [5][2000/3575] Elapsed 19m 9s (remain 15m 4s) Loss: 0.0000(0.0015) Grad: 80.0116  LR: 0.000002  \n",
      "Epoch: [5][2100/3575] Elapsed 20m 8s (remain 14m 7s) Loss: 0.0000(0.0015) Grad: 37.1157  LR: 0.000002  \n",
      "Epoch: [5][2200/3575] Elapsed 21m 6s (remain 13m 10s) Loss: 0.0000(0.0016) Grad: 19.6546  LR: 0.000002  \n",
      "Epoch: [5][2300/3575] Elapsed 22m 2s (remain 12m 12s) Loss: 0.0000(0.0016) Grad: 22.3222  LR: 0.000002  \n",
      "Epoch: [5][2400/3575] Elapsed 22m 58s (remain 11m 14s) Loss: 0.0000(0.0016) Grad: 4.2055  LR: 0.000001  \n",
      "Epoch: [5][2500/3575] Elapsed 23m 55s (remain 10m 16s) Loss: 0.0000(0.0016) Grad: 595.0417  LR: 0.000001  \n",
      "Epoch: [5][2600/3575] Elapsed 24m 54s (remain 9m 19s) Loss: 0.0011(0.0016) Grad: 274.2318  LR: 0.000001  \n",
      "Epoch: [5][2700/3575] Elapsed 25m 53s (remain 8m 22s) Loss: 0.0078(0.0015) Grad: 22405.5312  LR: 0.000001  \n",
      "Epoch: [5][2800/3575] Elapsed 26m 52s (remain 7m 25s) Loss: 0.0001(0.0015) Grad: 2538.8591  LR: 0.000001  \n",
      "Epoch: [5][2900/3575] Elapsed 27m 49s (remain 6m 27s) Loss: 0.0001(0.0015) Grad: 103.2278  LR: 0.000001  \n",
      "Epoch: [5][3000/3575] Elapsed 28m 45s (remain 5m 30s) Loss: 0.0000(0.0015) Grad: 10.0569  LR: 0.000001  \n",
      "Epoch: [5][3100/3575] Elapsed 29m 43s (remain 4m 32s) Loss: 0.0000(0.0015) Grad: 32.2276  LR: 0.000001  \n",
      "Epoch: [5][3200/3575] Elapsed 30m 42s (remain 3m 35s) Loss: 0.0000(0.0015) Grad: 34.8395  LR: 0.000000  \n",
      "Epoch: [5][3300/3575] Elapsed 31m 40s (remain 2m 37s) Loss: 0.0000(0.0015) Grad: 11.0117  LR: 0.000000  \n",
      "Epoch: [5][3400/3575] Elapsed 32m 37s (remain 1m 40s) Loss: 0.0000(0.0015) Grad: 7.6050  LR: 0.000000  \n",
      "Epoch: [5][3500/3575] Elapsed 33m 34s (remain 0m 42s) Loss: 0.0001(0.0015) Grad: 190.8064  LR: 0.000000  \n",
      "Epoch: [5][3574/3575] Elapsed 34m 15s (remain 0m 0s) Loss: 0.0000(0.0015) Grad: 5.6102  LR: 0.000000  \n",
      "EVAL: [0/1192] Elapsed 0m 0s (remain 11m 56s) Loss: 0.0000(0.0000) \n",
      "EVAL: [100/1192] Elapsed 0m 18s (remain 3m 21s) Loss: 0.0699(0.0133) \n",
      "EVAL: [200/1192] Elapsed 0m 37s (remain 3m 3s) Loss: 0.0031(0.0116) \n",
      "EVAL: [300/1192] Elapsed 0m 55s (remain 2m 43s) Loss: 0.0129(0.0123) \n",
      "EVAL: [400/1192] Elapsed 1m 13s (remain 2m 24s) Loss: 0.0000(0.0118) \n",
      "EVAL: [500/1192] Elapsed 1m 31s (remain 2m 6s) Loss: 0.0798(0.0114) \n",
      "EVAL: [600/1192] Elapsed 1m 49s (remain 1m 47s) Loss: 0.0120(0.0116) \n",
      "EVAL: [700/1192] Elapsed 2m 8s (remain 1m 29s) Loss: 0.0060(0.0131) \n",
      "EVAL: [800/1192] Elapsed 2m 27s (remain 1m 12s) Loss: 0.0251(0.0133) \n",
      "EVAL: [900/1192] Elapsed 2m 45s (remain 0m 53s) Loss: 0.0127(0.0138) \n",
      "EVAL: [1000/1192] Elapsed 3m 3s (remain 0m 35s) Loss: 0.0000(0.0133) \n",
      "EVAL: [1100/1192] Elapsed 3m 21s (remain 0m 16s) Loss: 0.0339(0.0129) \n",
      "EVAL: [1191/1192] Elapsed 3m 39s (remain 0m 0s) Loss: 0.0000(0.0124) \n",
      "Epoch 5 - avg_train_loss: 0.0015  avg_val_loss: 0.0124  time: 2279s\n",
      "Epoch 5 - Score: 0.8812\n",
      "Best thres: 0.5, Score: 0.8823\n",
      "Best thres: 0.5406250000000001, Score: 0.8823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from pretrained\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2682a88bcbe6442f8e90945c216ba32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-23-8e89234e65ba>\", line 41, in __getitem__\n    mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n  File \"<ipython-input-23-8e89234e65ba>\", line 32, in _create_mapping_from_token_to_char\n    mapping_from_token_to_char = np.zeros(self.max_char_len)\nAttributeError: 'TestDataset' object has no attribute 'max_char_len'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-0f6b8dbff6ab>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mtest_token_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"fold{i_fold}_{i}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_token_probs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mtest_char_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_char_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pn_history\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_token_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-ef7a3b9ca97f>\u001b[0m in \u001b[0;36minference_fn\u001b[0;34m(test_dataloader, model, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtk0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappings_from_token_to_char\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtk0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-23-8e89234e65ba>\", line 41, in __getitem__\n    mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n  File \"<ipython-input-23-8e89234e65ba>\", line 32, in _create_mapping_from_token_to_char\n    mapping_from_token_to_char = np.zeros(self.max_char_len)\nAttributeError: 'TestDataset' object has no attribute 'max_char_len'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "name": "nbme-exp057.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 641.572862,
   "end_time": "2022-02-27T11:39:50.972497",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-27T11:29:09.399635",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03f973838c924885b7600241d729d183": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "046b59dfab82487eaf5e1268f996b018": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_96f479f1c36143b5a0c16de2651c5ecb",
      "placeholder": "​",
      "style": "IPY_MODEL_df31fc87a42b4f5199b5f4b5d02dd64c",
      "value": " 42146/42146 [00:00&lt;00:00, 519145.33it/s]"
     }
    },
    "0acd16bc1b774b27b8a1d0027251caf2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ec95d8a55984afb8eb9efd55b921d80",
      "placeholder": "​",
      "style": "IPY_MODEL_77726cf0c093411085c59646885d2b28",
      "value": " 42146/42146 [00:35&lt;00:00, 1916.52it/s]"
     }
    },
    "0ba1e9bd83154395a37575d2e4a724f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0a0ea9a68804c459463520533a6ddb2",
      "max": 143,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_243f2cea17124685b7275199d0197947",
      "value": 143
     }
    },
    "0f7365a2a54e4f3289867fc78fa5d8f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f59fe1bf5c8644d5a41d476dffa39d61",
      "placeholder": "​",
      "style": "IPY_MODEL_91205e4ebe844c0c88f5d1b6731ff233",
      "value": "100%"
     }
    },
    "16ec8142da5f457490909b5d5ecc5a2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "177a811a8f0d4c3abd9c5b034bd33556": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1a6e09e641e84d919d0ea487e738a6a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e3f145cc1564614aba29374a43dfe45": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "243f2cea17124685b7275199d0197947": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3ec95d8a55984afb8eb9efd55b921d80": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "44edeaff57294f3182ba2f00b3d379a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_752ddda179d24fd287fee23f274a1206",
      "max": 42146,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e77baa3975c2484595060dbdcad867d4",
      "value": 42146
     }
    },
    "4ae813d03aee4dbc9ea70b16e867b8d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ef1a0ffadb9e438fb08d33c0e093388f",
      "max": 42146,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_177a811a8f0d4c3abd9c5b034bd33556",
      "value": 42146
     }
    },
    "6441d4d15bb14c6798ef59080d3efd5b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "752ddda179d24fd287fee23f274a1206": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77726cf0c093411085c59646885d2b28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d615121ce48438084f98f9dc0c5b663": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e3f145cc1564614aba29374a43dfe45",
      "placeholder": "​",
      "style": "IPY_MODEL_03f973838c924885b7600241d729d183",
      "value": "100%"
     }
    },
    "9059fb0eb80642afbc8c4a6625c853df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91205e4ebe844c0c88f5d1b6731ff233": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "96f479f1c36143b5a0c16de2651c5ecb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f0eeaa852284fae82e92f09f611c5f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0a0ea9a68804c459463520533a6ddb2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2a872834ce548658aa1f85dcfc27d48": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9ad179331184ee18e5ef2f6a09dc967": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f0eeaa852284fae82e92f09f611c5f6",
      "placeholder": "​",
      "style": "IPY_MODEL_b33cf6ce0b314dc1a403827b3c2f5125",
      "value": " 143/143 [00:00&lt;00:00, 1994.02it/s]"
     }
    },
    "ac2475c1acb4499eab3945dfa5c56779": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c7e5b695ef7c4acea0b1eb9bd2725a98",
       "IPY_MODEL_0ba1e9bd83154395a37575d2e4a724f9",
       "IPY_MODEL_a9ad179331184ee18e5ef2f6a09dc967"
      ],
      "layout": "IPY_MODEL_a2a872834ce548658aa1f85dcfc27d48"
     }
    },
    "b33cf6ce0b314dc1a403827b3c2f5125": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b7530e27be384f3fbdc650a521b80962": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7d615121ce48438084f98f9dc0c5b663",
       "IPY_MODEL_44edeaff57294f3182ba2f00b3d379a1",
       "IPY_MODEL_046b59dfab82487eaf5e1268f996b018"
      ],
      "layout": "IPY_MODEL_9059fb0eb80642afbc8c4a6625c853df"
     }
    },
    "c7e5b695ef7c4acea0b1eb9bd2725a98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1a6e09e641e84d919d0ea487e738a6a9",
      "placeholder": "​",
      "style": "IPY_MODEL_16ec8142da5f457490909b5d5ecc5a2c",
      "value": "100%"
     }
    },
    "d2031e5f3510429f977a8accb73623ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0f7365a2a54e4f3289867fc78fa5d8f0",
       "IPY_MODEL_4ae813d03aee4dbc9ea70b16e867b8d7",
       "IPY_MODEL_0acd16bc1b774b27b8a1d0027251caf2"
      ],
      "layout": "IPY_MODEL_6441d4d15bb14c6798ef59080d3efd5b"
     }
    },
    "df31fc87a42b4f5199b5f4b5d02dd64c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e77baa3975c2484595060dbdcad867d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ef1a0ffadb9e438fb08d33c0e093388f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f59fe1bf5c8644d5a41d476dffa39d61": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
