nbdiff nbme-exp022.ipynb nbme-exp019.ipynb
--- nbme-exp022.ipynb  2022-03-06 23:46:18.787919
+++ nbme-exp019.ipynb  2022-03-04 13:03:50.885867
[34m## modified /cells/0/id:[0m
[31m-  particular-wayne
[32m+  blind-kingdom

[0m[34m## modified /cells/0/metadata/id:[0m
[31m-  incredible-principle
[32m+  aa1f8e80

[0m[34m## inserted before /cells/1:[0m
[32m+  markdown cell:
[32m+    metadata (unknown keys):
[32m+      id: c0138fac
[32m+    source:
[32m+      - https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train

[0m[34m## deleted /cells/1:[0m
[31m-  markdown cell:
[31m-    metadata (unknown keys):
[31m-      id: simplified-tract
[31m-    source:
[31m-      - https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train
[31m-      - https://www.kaggle.com/abebe9849/nbmeexp019?select=itpt.py
[31m-      - https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-itpt
[31m-      - https://www.kaggle.com/maunish/clrp-pytorch-roberta-pretrain

[0m[34m## modified /cells/2/id:[0m
[31m-  first-deployment
[32m+  bored-ministry

[0m[34m## modified /cells/2/metadata/id:[0m
[31m-  boolean-shame
[32m+  cf1dfda9

[0m[34m## modified /cells/3/id:[0m
[31m-  played-polymer
[32m+  deadly-confidence

[0m[34m## modified /cells/3/metadata/id:[0m
[31m-  needed-consistency
[32m+  a7a78d25

[0m[34m## modified /cells/3/source:[0m
[36m@@ -1,4 +1,4 @@[m
[31m-EXP_NAME = "nbme-exp022"[m
[32m+[m[32mEXP_NAME = "nbme-exp019"[m
 ENV = "local"[m
 DEBUG_MODE = False[m
 SUBMISSION_MODE = False[m
[m

[0m[34m## modified /cells/4/id:[0m
[31m-  steady-bubble
[32m+  aware-worcester

[0m[34m## modified /cells/4/metadata/id:[0m
[31m-  operational-trader
[32m+  4ecc4e4d

[0m[34m## modified /cells/4/source:[0m
[36m@@ -11,13 +11,13 @@[m [mclass CFG:[m
     competition_name="nbme-score-clinical-patient-notes"[m
     id_col="id"[m
     target_col="location"[m
[31m-    pretrained_model_name="microsoft/deberta-v3-large"[m
[32m+[m[32m    pretrained_model_name="microsoft/deberta-large"[m
     tokenizer=None[m
     max_len=None[m
     output_dim=1[m
     dropout=0.2[m
     num_workers=4[m
[31m-    batch_size=8[m
[32m+[m[32m    batch_size=4[m
     lr=2e-5[m
     betas=(0.9, 0.98)[m
     weight_decay=0.1[m
[36m@@ -27,7 +27,7 @@[m [mclass CFG:[m
     n_fold=5[m
     train_fold=[0, 1, 2, 3, 4][m
     seed=71[m
[31m-    gradient_accumulation_steps=1[m
[32m+[m[32m    gradient_accumulation_steps=2[m
     max_grad_norm=1000[m
     print_freq=100[m
     train=True[m

[0m[34m## deleted /cells/5:[0m
[31m-  code cell:
[31m-    execution_count: 3
[31m-    metadata (unknown keys):
[31m-      id: g6Cc8fDp2Nle
[31m-    source:
[31m-      class CFG_For_MLM:
[31m-          epochs = 15 # adjust
[31m-          learning_rate = 5e-05
[31m-          train_batch_size = 8
[31m-          gradient_accum_steps = 4
[31m-          eval_batch_size = 16
[31m-          eval_steps = 8678
[31m-          block_size = 354 # tokenizer„ÅÆmax_length
[31m-          mlm_prob = 0.40
[31m-          fp16 = True

[0m[34m## replaced /cells/6/execution_count:[0m
[31m-  4
[32m+  3

[0m[34m## modified /cells/6/id:[0m
[31m-  polished-citizenship
[32m+  personalized-death

[0m[34m## modified /cells/6/metadata/id:[0m
[31m-  seasonal-consistency
[32m+  3894c88b

[0m[34m## modified /cells/7/id:[0m
[31m-  tired-working
[32m+  cardiovascular-neutral

[0m[34m## modified /cells/7/metadata/id:[0m
[31m-  billion-composite
[32m+  31768c85

[0m[34m## replaced /cells/8/execution_count:[0m
[31m-  5
[32m+  4

[0m[34m## modified /cells/8/id:[0m
[31m-  noticed-identifier
[32m+  checked-boards

[0m[34m## replaced /cells/8/metadata/executionInfo/elapsed:[0m
[31m-  4861
[32m+  4693

[0m[34m## replaced /cells/8/metadata/executionInfo/timestamp:[0m
[31m-  1645861057787
[32m+  1646023773081

[0m[34m## modified /cells/8/metadata/id:[0m
[31m-  desperate-collect
[32m+  00e7d967

[0m[34m## modified /cells/8/metadata/outputId:[0m
[31m-  5fe1bfad-5d36-4201-a4a2-f84d8a7d809d
[32m+  d56a483d-9171-44e6-856a-a90dfe8e0ac3

[0m[34m## modified /cells/8/source:[0m
[36m@@ -13,7 +13,6 @@[m [mif CFG.env == "colab":[m
         CFG.output_dir.mkdir()[m
     # install packages[m
     !pip install transformers[m
[31m-    !pip install sentencepiece[m
 [m
 elif CFG.env == "local":[m
     # „É≠„Éº„Ç´„É´„Çµ„Éº„Éê[m

[0m[34m## deleted /cells/9:[0m
[31m-  code cell:
[31m-    execution_count: 6
[31m-    source:
[31m-      # The following is necessary if you want to use the fast tokenizer for deberta v2 or v3
[31m-      # This must be done before importing transformers
[31m-      import shutil
[31m-      from pathlib import Path
[31m-      
[31m-      if CFG.env == "colab":
[31m-          input_dir = Path("./drive/MyDrive/00.kaggle/input/deberta-v2-3-fast-tokenizer")
[31m-          transformers_path = Path("/usr/local/lib/python3.7/dist-packages/transformers")
[31m-      else:
[31m-          input_dir = Path("../input/deberta-v2-3-fast-tokenizer")
[31m-          transformers_path = Path("/opt/conda/lib/python3.7/site-packages/transformers")
[31m-      
[31m-      convert_file = input_dir / "convert_slow_tokenizer.py"
[31m-      conversion_path = transformers_path/convert_file.name
[31m-      
[31m-      if conversion_path.exists():
[31m-          conversion_path.unlink()
[31m-      
[31m-      shutil.copy(convert_file, transformers_path)
[31m-      deberta_v2_path = transformers_path / "models" / "deberta_v2"
[31m-      
[31m-      for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:
[31m-          filepath = deberta_v2_path/filename
[31m-          if filepath.exists():
[31m-              filepath.unlink()
[31m-      
[31m-          shutil.copy(input_dir/filename, filepath)
[31m-          
[31m-          
[31m-      from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast

[0m[34m## replaced /cells/10/execution_count:[0m
[31m-  7
[32m+  5

[0m[34m## modified /cells/10/id:[0m
[31m-  amended-moscow
[32m+  vital-mexico

[0m[34m## modified /cells/10/metadata/id:[0m
[31m-  acute-pregnancy
[32m+  d726b7d9

[0m[34m## modified /cells/10/source:[0m
[36m@@ -20,6 +20,7 @@[m [mimport torchvision.transforms as T[m
 from torchvision.io import read_image[m
 from torch.utils.data import DataLoader, Dataset[m
 [m
[32m+[m[32mfrom transformers import AutoModelForMaskedLM[m
 from transformers import BartModel,BertModel,BertTokenizer[m
 from transformers import DebertaModel,DebertaTokenizer[m
 from transformers import RobertaModel,RobertaTokenizer[m

[0m[34m## deleted /cells/11:[0m
[31m-  code cell:
[31m-    execution_count: 8
[31m-    metadata (unknown keys):
[31m-      id: IW4rCfxa1wyC
[31m-    source:
[31m-      from transformers import (AutoModel, 
[31m-                                AutoModelForMaskedLM,
[31m-                                AutoTokenizer,
[31m-                                AutoConfig,
[31m-                                AdamW,
[31m-                                LineByLineTextDataset,
[31m-                                DataCollatorForLanguageModeling,
[31m-                                Trainer,
[31m-                                TrainingArguments)

[0m[34m## modified /cells/12/id:[0m
[31m-  advisory-broad
[32m+  economic-ladder

[0m[34m## modified /cells/12/metadata/id:[0m
[31m-  generous-raleigh
[32m+  b6d82f71

[0m[34m## inserted before /cells/13:[0m
[32m+  code cell:
[32m+    execution_count: 6
[32m+    metadata (unknown keys):
[32m+      id: 95abbe2c
[32m+    source:
[32m+      def micro_f1(preds, truths):
[32m+          """
[32m+          Micro f1 on binary arrays.
[32m+      
[32m+          Args:
[32m+              preds (list of lists of ints): Predictions.
[32m+              truths (list of lists of ints): Ground truths.
[32m+      
[32m+          Returns:
[32m+              float: f1 score.
[32m+          """
[32m+          # Micro : aggregating over all instances
[32m+          preds = np.concatenate(preds)
[32m+          truths = np.concatenate(truths)
[32m+          return f1_score(truths, preds)
[32m+      
[32m+      
[32m+      def spans_to_binary(spans, length=None):
[32m+          """
[32m+          Converts spans to a binary array indicating whether each character is in the span.
[32m+      
[32m+          Args:
[32m+              spans (list of lists of two ints): Spans.
[32m+      
[32m+          Returns:
[32m+              np array [length]: Binarized spans.
[32m+          """
[32m+          length = np.max(spans) if length is None else length
[32m+          binary = np.zeros(length)
[32m+          for start, end in spans:
[32m+              binary[start:end] = 1
[32m+          return binary
[32m+      
[32m+      
[32m+      def span_micro_f1(preds, truths):
[32m+          """
[32m+          Micro f1 on spans.
[32m+      
[32m+          Args:
[32m+              preds (list of lists of two ints): Prediction spans.
[32m+              truths (list of lists of two ints): Ground truth spans.
[32m+      
[32m+          Returns:
[32m+              float: f1 score.
[32m+          """
[32m+          bin_preds = []
[32m+          bin_truths = []
[32m+          for pred, truth in zip(preds, truths):
[32m+              if not len(pred) and not len(truth):
[32m+                  continue
[32m+              length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)
[32m+              bin_preds.append(spans_to_binary(pred, length))
[32m+              bin_truths.append(spans_to_binary(truth, length))
[32m+          return micro_f1(bin_preds, bin_truths)
[32m+      
[32m+      
[32m+      def get_score(y_true, y_pred):
[32m+          score = span_micro_f1(y_true, y_pred)
[32m+          return score
[32m+  code cell:
[32m+    execution_count: 7
[32m+    metadata (unknown keys):
[32m+      id: 832ee36d
[32m+    source:
[32m+      def create_labels_for_scoring(df):
[32m+          # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]
[32m+          df["location_for_create_labels"] = [ast.literal_eval(f"[]")] * len(df)
[32m+          for i in range(len(df)):
[32m+              lst = df.loc[i, "location"]
[32m+              if lst:
[32m+                  new_lst = ";".join(lst)
[32m+                  df.loc[i, "location_for_create_labels"] = ast.literal_eval(f"[['{new_lst}']]")
[32m+      
[32m+          # create labels
[32m+          truths = []
[32m+          for location_list in df["location_for_create_labels"].values:
[32m+              truth = []
[32m+              if len(location_list) > 0:
[32m+                  location = location_list[0]
[32m+                  for loc in [s.split() for s in location.split(";")]:
[32m+                      start, end = int(loc[0]), int(loc[1])
[32m+                      truth.append([start, end])
[32m+              truths.append(truth)
[32m+      
[32m+          return truths
[32m+      
[32m+      
[32m+      def get_char_probs(texts, token_probs, tokenizer):
[32m+          res = [np.zeros(len(t)) for t in texts]
[32m+          for i, (text, prediction) in enumerate(zip(texts, token_probs)):
[32m+              encoded = tokenizer(
[32m+                  text=text,
[32m+                  max_length=CFG.max_len,
[32m+                  padding="max_length",
[32m+                  return_offsets_mapping=True,
[32m+              )
[32m+              for (offset_mapping, pred) in zip(encoded["offset_mapping"], prediction):
[32m+                  start, end = offset_mapping
[32m+                  res[i][start:end] = pred
[32m+          return res
[32m+      
[32m+      
[32m+      def get_predicted_location_str(char_probs, th=0.5):
[32m+          results = []
[32m+          for char_prob in char_probs:
[32m+              result = np.where(char_prob >= th)[0] + 1
[32m+              result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]
[32m+              result = [f"{min(r)} {max(r)}" for r in result]
[32m+              result = ";".join(result)
[32m+              results.append(result)
[32m+          return results
[32m+      
[32m+      
[32m+      def get_predictions(results):
[32m+          predictions = []
[32m+          for result in results:
[32m+              prediction = []
[32m+              if result != "":
[32m+                  for loc in [s.split() for s in result.split(";")]:
[32m+                      start, end = int(loc[0]), int(loc[1])
[32m+                      prediction.append([start, end])
[32m+              predictions.append(prediction)
[32m+          return predictions
[32m+      
[32m+      
[32m+      def scoring(df, th=0.5):
[32m+          labels = create_labels_for_scoring(df)
[32m+      
[32m+          token_probs = df[[str(i) for i in range(CFG.max_len)]].values
[32m+          char_probs = get_char_probs(df["pn_history"].values, token_probs, CFG.tokenizer)
[32m+          predicted_location_str = get_predicted_location_str(char_probs, th=th)
[32m+          preds = get_predictions(predicted_location_str)
[32m+      
[32m+          score = get_score(labels, preds)
[32m+          return score
[32m+      
[32m+      
[32m+      def get_best_thres(oof_df):
[32m+          def f1_opt(x):
[32m+              return -1 * scoring(oof_df, th=x)
[32m+      
[32m+          best_thres = minimize(f1_opt, x0=np.array([0.5]), method="Nelder-Mead")["x"].item()
[32m+          return best_thres
[32m+  code cell:
[32m+    execution_count: 8
[32m+    metadata (unknown keys):
[32m+      id: 918828a7
[32m+    source:
[32m+      class AverageMeter(object):
[32m+          """Computes and stores the average and current value"""
[32m+          def __init__(self):
[32m+              self.reset()
[32m+      
[32m+          def reset(self):
[32m+              self.val = 0
[32m+              self.avg = 0
[32m+              self.sum = 0
[32m+              self.count = 0
[32m+      
[32m+          def update(self, val, n=1):
[32m+              self.val = val
[32m+              self.sum += val * n
[32m+              self.count += n
[32m+              self.avg = self.sum / self.count
[32m+      
[32m+      
[32m+      def asMinutes(s):
[32m+          m = math.floor(s / 60)
[32m+          s -= m * 60
[32m+          return "%dm %ds" % (m, s)
[32m+      
[32m+      
[32m+      def timeSince(since, percent):
[32m+          now = time.time()
[32m+          s = now - since
[32m+          es = s / (percent)
[32m+          rs = es - s
[32m+          return "%s (remain %s)" % (asMinutes(s), asMinutes(rs))
[32m+      
[32m+      
[32m+      def seed_everything(seed=42):
[32m+          random.seed(seed)
[32m+          os.environ['PYTHONHASHSEED'] = str(seed)
[32m+          np.random.seed(seed)
[32m+          torch.manual_seed(seed)
[32m+          torch.cuda.manual_seed(seed)
[32m+          torch.backends.cudnn.deterministic = True

[0m[34m## deleted /cells/13:[0m
[31m-  code cell:
[31m-    execution_count: 9
[31m-    metadata (unknown keys):
[31m-      id: mmZHVaPkh0Qc
[31m-    source:
[31m-      def seed_everything(seed=42):
[31m-          random.seed(seed)
[31m-          os.environ['PYTHONHASHSEED'] = str(seed)
[31m-          np.random.seed(seed)
[31m-          torch.manual_seed(seed)
[31m-          torch.cuda.manual_seed(seed)
[31m-          torch.backends.cudnn.deterministic = True

[0m[34m## replaced /cells/14/execution_count:[0m
[31m-  10
[32m+  9

[0m[34m## modified /cells/14/id:[0m
[31m-  casual-arrow
[32m+  gorgeous-record

[0m[34m## modified /cells/14/metadata/id:[0m
[31m-  IydDnpFyh4PX
[32m+  d02a78e1

[0m[34m## modified /cells/15/id:[0m
[31m-  dangerous-disclosure
[32m+  frozen-africa

[0m[34m## modified /cells/15/metadata/id:[0m
[31m-  formed-handbook
[32m+  47266f39

[0m[34m## replaced /cells/16/execution_count:[0m
[31m-  11
[32m+  10

[0m[34m## modified /cells/16/id:[0m
[31m-  dressed-broadcasting
[32m+  shaped-metallic

[0m[34m## replaced /cells/16/metadata/executionInfo/elapsed:[0m
[31m-  420
[32m+  815

[0m[34m## replaced /cells/16/metadata/executionInfo/timestamp:[0m
[31m-  1645861061670
[32m+  1646023777557

[0m[34m## modified /cells/16/metadata/id:[0m
[31m-  vanilla-register
[32m+  20fed6da

[0m[34m## modified /cells/16/metadata/outputId:[0m
[31m-  21d382a3-c203-4855-f7fd-e0f08cfaf2f2
[32m+  64d3e7ad-0986-4799-f9df-f0242c1977a2

[0m[34m## replaced /cells/16/outputs/0/execution_count:[0m
[31m-  11
[32m+  10

[0m[34m## replaced /cells/17/execution_count:[0m
[31m-  12
[32m+  11

[0m[34m## modified /cells/17/id:[0m
[31m-  amazing-joint
[32m+  visible-australia

[0m[34m## modified /cells/17/metadata/id:[0m
[31m-  approximate-transmission
[32m+  e67d0132

[0m[34m## modified /cells/18/id:[0m
[31m-  recorded-dodge
[32m+  hydraulic-gibson

[0m[34m## modified /cells/18/metadata/id:[0m
[31m-  civic-advisory
[32m+  47bca11a

[0m[34m## inserted before /cells/19:[0m
[32m+  code cell:
[32m+    execution_count: 12
[32m+    metadata (unknown keys):
[32m+      id: d9c8e9ba
[32m+    source:
[32m+      def preprocess_features(features):
[32m+          features.loc[features["feature_text"] == "Last-Pap-smear-I-year-ago", "feature_text"] = "Last-Pap-smear-1-year-ago"
[32m+          return features
[32m+      
[32m+      
[32m+      features = preprocess_features(features)
[32m+  code cell:
[32m+    execution_count: 13
[32m+    metadata (unknown keys):
[32m+      colab:
[32m+        base_uri: https://localhost:8080/
[32m+      executionInfo:
[32m+        elapsed: 5
[32m+        status: ok
[32m+        timestamp: 1646023777558
[32m+        user:
[32m+          displayName: Shuhei Goda
[32m+          photoUrl: https://lh3.googleusercontent.com/a/default-user=s64
[32m+          userId: 08246931244224045522
[32m+        user_tz: -540
[32m+      id: 7ef41e18
[32m+      outputId: 31edaa7d-c088-495a-95ee-f0d56f97074c
[32m+    source:
[32m+      train = train.merge(features, on=["feature_num", "case_num"], how="left")
[32m+      train = train.merge(patient_notes, on=["pn_num", "case_num"], how="left")
[32m+      test = test.merge(features, on=["feature_num", "case_num"], how="left")
[32m+      test = test.merge(patient_notes, on=["pn_num", "case_num"], how="left")
[32m+      
[32m+      train.shape, test.shape
[32m+    outputs:
[32m+      output 0:
[32m+        output_type: execute_result
[32m+        execution_count: 13
[32m+        data:
[32m+          text/plain: ((14300, 8), (5, 6))
[32m+  code cell:
[32m+    execution_count: 14
[32m+    metadata (unknown keys):
[32m+      id: 8233df16
[32m+    source:
[32m+      train["annotation"] = train["annotation"].apply(ast.literal_eval)
[32m+      train["location"] = train["location"].apply(ast.literal_eval)
[32m+  code cell:
[32m+    execution_count: 15
[32m+    metadata (unknown keys):
[32m+      colab:
[32m+        base_uri: https://localhost:8080/
[32m+        height: 195
[32m+      executionInfo:
[32m+        elapsed: 4
[32m+        status: ok
[32m+        timestamp: 1646023778018
[32m+        user:
[32m+          displayName: Shuhei Goda
[32m+          photoUrl: https://lh3.googleusercontent.com/a/default-user=s64
[32m+          userId: 08246931244224045522
[32m+        user_tz: -540
[32m+      id: e9143e61
[32m+      outputId: cf45e2d7-5f66-4d96-c6e2-da79c888bcc8
[32m+    source:
[32m+      train["annotation_length"] = train["annotation"].apply(len)
[32m+      display(train['annotation_length'].value_counts().sort_index())
[32m+    outputs:
[32m+      output 0:
[32m+        output_type: display_data
[32m+        data:
[32m+          text/plain:
[32m+            0    4399
[32m+            1    8181
[32m+            2    1296
[32m+            3     287
[32m+            4      99
[32m+            5      27
[32m+            6       9
[32m+            7       1
[32m+            8       1
[32m+            Name: annotation_length, dtype: int64
[32m+  markdown cell:
[32m+    metadata (unknown keys):
[32m+      id: 6bdc7949
[32m+    source:
[32m+      ## CV split
[32m+  code cell:
[32m+    execution_count: 16
[32m+    metadata (unknown keys):
[32m+      id: c4acf61d
[32m+    source:
[32m+      def get_groupkfold(df, group_name):
[32m+          groups = df[group_name].unique()
[32m+      
[32m+          kf = KFold(
[32m+              n_splits=CFG.n_fold,
[32m+              shuffle=True,
[32m+              random_state=CFG.seed,
[32m+          )
[32m+          folds_ids = []
[32m+          for i_fold, (_, val_group_idx) in enumerate(kf.split(groups)):
[32m+              val_group = groups[val_group_idx]
[32m+              is_val = df[group_name].isin(val_group)
[32m+              val_idx = df[is_val].index
[32m+              df.loc[val_idx, "fold"] = int(i_fold)
[32m+      
[32m+          df["fold"] = df["fold"].astype(int)
[32m+          return df
[32m+  code cell:
[32m+    execution_count: 17
[32m+    metadata (unknown keys):
[32m+      colab:
[32m+        base_uri: https://localhost:8080/
[32m+        height: 142
[32m+      executionInfo:
[32m+        elapsed: 3
[32m+        status: ok
[32m+        timestamp: 1646023778018
[32m+        user:
[32m+          displayName: Shuhei Goda
[32m+          photoUrl: https://lh3.googleusercontent.com/a/default-user=s64
[32m+          userId: 08246931244224045522
[32m+        user_tz: -540
[32m+      id: 2ca0c08e
[32m+      outputId: cfc9c06e-e30c-4cb5-a072-d0cfcfa5fdc7
[32m+    source:
[32m+      train = get_groupkfold(train, "pn_num")
[32m+      display(train.groupby("fold").size())
[32m+    outputs:
[32m+      output 0:
[32m+        output_type: display_data
[32m+        data:
[32m+          text/plain:
[32m+            fold
[32m+            0    2902
[32m+            1    2894
[32m+            2    2813
[32m+            3    2791
[32m+            4    2900
[32m+            dtype: int64

[0m[34m## deleted /cells/19:[0m
[31m-  code cell:
[31m-    execution_count: 13
[31m-    metadata (unknown keys):
[31m-      colab:
[31m-        base_uri: https://localhost:8080/
[31m-      executionInfo:
[31m-        elapsed: 382
[31m-        status: ok
[31m-        timestamp: 1645861062051
[31m-        user:
[31m-          displayName: Shuhei Goda
[31m-          photoUrl: https://lh3.googleusercontent.com/a/default-user=s64
[31m-          userId: 08246931244224045522
[31m-        user_tz: -540
[31m-      id: dPHRME1x0a6j
[31m-      outputId: a33b6872-352f-4fd9-fe39-de92cdf40a84
[31m-    source:
[31m-      pretrain_texts = patient_notes["pn_history"].unique()
[31m-      print(len(pretrain_texts))
[31m-      
[31m-      
[31m-      with open(CFG.output_dir / "text.txt", "w") as f:
[31m-          texts  = "\n".join(pretrain_texts.tolist())
[31m-          f.write(texts)
[31m-    outputs:
[31m-      output 0:
[31m-        output_type: stream
[31m-        name: stdout
[31m-        text:
[31m-          42146

[0m[34m## modified /cells/20/id:[0m
[31m-  prescribed-bidding
[32m+  subjective-entrance

[0m[34m## modified /cells/20/metadata/id:[0m
[31m-  senior-wichita
[32m+  a8560070

[0m[34m## replaced /cells/21/execution_count:[0m
[31m-  14
[32m+  18

[0m[34m## modified /cells/21/id:[0m
[31m-  electric-waters
[32m+  dramatic-afghanistan

[0m[34m## modified /cells/21/metadata/id:[0m
[31m-  thrown-theology
[32m+  c316b13f

[0m[34m## deleted /cells/21/outputs/0:[0m
[31m-  output:
[31m-    output_type: stream
[31m-    name: stderr
[31m-    text:
[31m-      Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[31m-      Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

[0m[34m## modified /cells/21/source:[0m
[36m@@ -1,7 +1,7 @@[m
 if CFG.submission:[m
[31m-    tokenizer = DebertaV2TokenizerFast.from_pretrained(Path("../input/") / CFG.exp_name / "tokenizer/")[m
[32m+[m[32m    tokenizer = AutoTokenizer.from_pretrained(Path("../input/") / CFG.exp_name / "tokenizer/")[m
 else:[m
[31m-    tokenizer = DebertaV2TokenizerFast.from_pretrained(CFG.pretrained_model_name)[m
[32m+[m[32m    tokenizer = AutoTokenizer.from_pretrained(CFG.pretrained_model_name)[m
     tokenizer.save_pretrained(CFG.output_dir / "tokenizer/")[m
 [m
 CFG.tokenizer = tokenizer[m
[m

[0m[34m## modified /cells/22/id:[0m
[31m-  funded-updating
[32m+  divided-arrow

[0m[34m## modified /cells/22/metadata/id:[0m
[31m-  ZbXou2dp152b
[32m+  e689a7fc

[0m[34m## modified /cells/22/source:[0m
[31m-  ## Setup Dataset
[32m+  ## Create dataset

[0m[34m## inserted before /cells/23:[0m
[32m+  code cell:
[32m+    execution_count: 19
[32m+    metadata (unknown keys):
[32m+      colab:
[32m+        base_uri: https://localhost:8080/
[32m+        height: 67
[32m+        referenced_widgets:
[32m+          item[0]: a31c60ff4dab48e08d2ef9293d85df6d
[32m+          item[1]: c40d970496ff447a8c0b80d787b07a4d
[32m+          item[2]: 40e6583408c447199ff5b94d23601936
[32m+          item[3]: 1141ae38bc6f473aab89db14fa4eeacf
[32m+          item[4]: 47b8a7f3d0544d79b30ad02e4222082e
[32m+          item[5]: 0260998578564385a0b5b9425a0a5ca1
[32m+          item[6]: 428ca357bd284d199e2558b1f577d79a
[32m+          item[7]: 2e32fee744ef42e0aaa89a7b03e82427
[32m+          item[8]: d51d3aa414db4aa8b0ccae896e671152
[32m+          item[9]: ad49cbf6b6e84ccaab873458182f22a1
[32m+          item[10]: 5375de82ce3a41a8b5550e0a6b4316c1
[32m+      executionInfo:
[32m+        elapsed: 37449
[32m+        status: ok
[32m+        timestamp: 1646023819498
[32m+        user:
[32m+          displayName: Shuhei Goda
[32m+          photoUrl: https://lh3.googleusercontent.com/a/default-user=s64
[32m+          userId: 08246931244224045522
[32m+        user_tz: -540
[32m+      id: df31758e
[32m+      outputId: e3ee6910-2896-413b-9bb7-1e1dd630166c
[32m+    source:
[32m+      pn_history_lengths = []
[32m+      tk0 = tqdm(patient_notes["pn_history"].fillna("").values, total=len(patient_notes))
[32m+      for text in tk0:
[32m+          length = len(tokenizer(text, add_special_tokens=False)["input_ids"])
[32m+          pn_history_lengths.append(length)
[32m+      
[32m+      print("max length:", np.max(pn_history_lengths))
[32m+    outputs:
[32m+      output 0:
[32m+        output_type: display_data
[32m+        data:
[32m+          application/vnd.jupyter.widget-view+json:
[32m+            model_id: bb90d2fce9214a4fa1d020b160b2257d
[32m+            version_major: 2
[32m+            version_minor: 0
[32m+          text/plain:   0%|          | 0/42146 [00:00<?, ?it/s]
[32m+      output 1:
[32m+        output_type: stream
[32m+        name: stdout
[32m+        text:
[32m+          max length: 433
[32m+  code cell:
[32m+    execution_count: 20
[32m+    metadata (unknown keys):
[32m+      colab:
[32m+        base_uri: https://localhost:8080/
[32m+        height: 67
[32m+        referenced_widgets:
[32m+          item[0]: 8a503d1abd884514a1e23101e03c6781
[32m+          item[1]: e06e5e9eb0414b6fad63bdc99b44a313
[32m+          item[2]: 6b04b019813e458080f02bc9111433a6
[32m+          item[3]: 2e3818222bab4603a896be5976cb8409
[32m+          item[4]: eeb468dbb94943fcb30219d4dd98fcab
[32m+          item[5]: 5e66444e9c714134bd2765cb3b6d1f15
[32m+          item[6]: 220f78b6119042af8729543465e1234e
[32m+          item[7]: 7f1d7796e2174485a0d1b1e9a71d7ade
[32m+          item[8]: e72cad76f875451a8e2479e2df237575
[32m+          item[9]: 9754a5f1e61d49c8972df40ee9290375
[32m+          item[10]: 25bf78e432e641e0a435dc3626c3ee8a
[32m+      executionInfo:
[32m+        elapsed: 32
[32m+        status: ok
[32m+        timestamp: 1646023819500
[32m+        user:
[32m+          displayName: Shuhei Goda
[32m+          photoUrl: https://lh3.googleusercontent.com/a/default-user=s64
[32m+          userId: 08246931244224045522
[32m+        user_tz: -540
[32m+      id: 3caff24a
[32m+      outputId: 09841871-9f3a-4e70-a528-07122a0ebba2
[32m+    source:
[32m+      feature_text_lengths = []
[32m+      tk0 = tqdm(features["feature_text"].fillna("").values, total=len(features))
[32m+      for text in tk0:
[32m+          length = len(tokenizer(text, add_special_tokens=False)["input_ids"])
[32m+          feature_text_lengths.append(length)
[32m+      
[32m+      print("max length:", np.max(feature_text_lengths))
[32m+    outputs:
[32m+      output 0:
[32m+        output_type: display_data
[32m+        data:
[32m+          application/vnd.jupyter.widget-view+json:
[32m+            model_id: 63970e450fdd47adb63ee4d115cdb334
[32m+            version_major: 2
[32m+            version_minor: 0
[32m+          text/plain:   0%|          | 0/143 [00:00<?, ?it/s]
[32m+      output 1:
[32m+        output_type: stream
[32m+        name: stdout
[32m+        text:
[32m+          max length: 30
[32m+  code cell:
[32m+    execution_count: 21
[32m+    metadata (unknown keys):
[32m+      colab:
[32m+        base_uri: https://localhost:8080/
[32m+      executionInfo:
[32m+        elapsed: 27
[32m+        status: ok
[32m+        timestamp: 1646023819500
[32m+        user:
[32m+          displayName: Shuhei Goda
[32m+          photoUrl: https://lh3.googleusercontent.com/a/default-user=s64
[32m+          userId: 08246931244224045522
[32m+        user_tz: -540
[32m+      id: 756d83ff
[32m+      outputId: 02d1e748-4ce8-4d68-f2a2-175f08316e9d
[32m+    source:
[32m+      CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep
[32m+      
[32m+      print("max length:", CFG.max_len)
[32m+    outputs:
[32m+      output 0:
[32m+        output_type: stream
[32m+        name: stdout
[32m+        text:
[32m+          max length: 466
[32m+  code cell:
[32m+    execution_count: 22
[32m+    metadata (unknown keys):
[32m+      id: 054b899a
[32m+    source:
[32m+      class TrainingDataset(Dataset):
[32m+          def __init__(self, cfg, df):
[32m+              self.cfg = cfg
[32m+              self.df = df
[32m+              self.tokenizer = self.cfg.tokenizer
[32m+              self.max_len = self.cfg.max_len
[32m+              self.feature_texts = self.df["feature_text"].values
[32m+              self.pn_historys = self.df["pn_history"].values
[32m+              self.annotation_lengths = self.df["annotation_length"].values
[32m+              self.locations = self.df["location"].values
[32m+      
[32m+          def __len__(self):
[32m+              return len(self.df)
[32m+      
[32m+          def _create_input(self, pn_history, feature_text):
[32m+              encoded = self.tokenizer(
[32m+                  text=pn_history,
[32m+                  text_pair=feature_text,
[32m+                  max_length=self.max_len,
[32m+                  padding="max_length",
[32m+                  return_offsets_mapping=False,
[32m+              )
[32m+              for k, v in encoded.items():
[32m+                  encoded[k] = torch.tensor(v, dtype=torch.long)
[32m+              return encoded
[32m+      
[32m+          def _create_label(self, pn_history, annotation_length, location_list):
[32m+              encoded = self.tokenizer(
[32m+                  text=pn_history,
[32m+                  max_length=self.max_len,
[32m+                  padding="max_length",
[32m+                  return_offsets_mapping=True,
[32m+              )
[32m+              offset_mapping = encoded["offset_mapping"]
[32m+              ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]
[32m+              label = np.zeros(len(offset_mapping))
[32m+              label[ignore_idxes] = -1
[32m+      
[32m+              if annotation_length > 0:
[32m+                  for location in location_list:
[32m+                      for loc in [s.split() for s in location.split(";")]:
[32m+                          start, end = int(loc[0]), int(loc[1])
[32m+                          start_idx = -1
[32m+                          end_idx = -1
[32m+                          for idx in range(len(offset_mapping)):
[32m+                              if (start_idx == -1) & (start < offset_mapping[idx][0]):
[32m+                                  start_idx = idx - 1
[32m+                              if (end_idx == -1) & (end <= offset_mapping[idx][1]):
[32m+                                  end_idx = idx + 1
[32m+                          if start_idx == -1:
[32m+                              start_idx = end_idx
[32m+                          if (start_idx != -1) & (end_idx != -1):
[32m+                              label[start_idx:end_idx] = 1
[32m+      
[32m+              return torch.tensor(label, dtype=torch.float)
[32m+      
[32m+          def __getitem__(self, idx):
[32m+              input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])
[32m+              label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])
[32m+              return input_, label
[32m+  code cell:
[32m+    execution_count: 23
[32m+    metadata (unknown keys):
[32m+      id: 1d58367c
[32m+    source:
[32m+      class TestDataset(Dataset):
[32m+          def __init__(self, cfg, df):
[32m+              self.cfg = cfg
[32m+              self.df = df
[32m+              self.tokenizer = self.cfg.tokenizer
[32m+              self.max_len = self.cfg.max_len
[32m+              self.feature_texts = self.df["feature_text"].values
[32m+              self.pn_historys = self.df["pn_history"].values
[32m+      
[32m+          def __len__(self):
[32m+              return len(self.df)
[32m+      
[32m+          def _create_input(self, pn_history, feature_text):
[32m+              encoded = self.tokenizer(
[32m+                  text=pn_history,
[32m+                  text_pair=feature_text,
[32m+                  max_length=self.max_len,
[32m+                  padding="max_length",
[32m+                  return_offsets_mapping=False,
[32m+              )
[32m+              for k, v in encoded.items():
[32m+                  encoded[k] = torch.tensor(v, dtype=torch.long)
[32m+              return encoded
[32m+      
[32m+          def __getitem__(self, idx):
[32m+              input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])
[32m+              return input_

[0m[34m## deleted /cells/23-24:[0m
[31m-  code cell:
[31m-    execution_count: 15
[31m-    metadata (unknown keys):
[31m-      id: a-o-wrnz1n-a
[31m-    source:
[31m-      train_dataset = LineByLineTextDataset(
[31m-          tokenizer=CFG.tokenizer,
[31m-          file_path=CFG.output_dir / "text.txt",
[31m-          block_size=CFG_For_MLM.block_size)
[31m-      
[31m-      valid_dataset = LineByLineTextDataset(
[31m-          tokenizer=CFG.tokenizer,
[31m-          file_path=CFG.output_dir / "text.txt",
[31m-          block_size=CFG_For_MLM.block_size)
[31m-  code cell:
[31m-    execution_count: 16
[31m-    metadata (unknown keys):
[31m-      id: o-XhXXOx2z4d
[31m-    source:
[31m-      data_collator = DataCollatorForLanguageModeling(
[31m-          tokenizer=CFG.tokenizer, 
[31m-          mlm=True, 
[31m-          mlm_probability=CFG_For_MLM.mlm_prob)

[0m[34m## modified /cells/25/id:[0m
[31m-  reported-zoning
[32m+  chemical-lucas

[0m[34m## modified /cells/25/metadata/id:[0m
[31m-  JaGyEd0r3i0W
[32m+  8c57abef

[0m[34m## inserted before /cells/26:[0m
[32m+  code cell:
[32m+    execution_count: 24
[32m+    metadata (unknown keys):
[32m+      id: 54f92d89
[32m+    source:
[32m+      class CustomModel(nn.Module):
[32m+          def __init__(self, cfg, model_config_path=None, pretrained=False):
[32m+              super().__init__()
[32m+              self.cfg = cfg
[32m+      
[32m+              if model_config_path is None:
[32m+                  self.model_config = AutoConfig.from_pretrained(
[32m+                      self.cfg.pretrained_model_name,
[32m+                      output_hidden_states=True,
[32m+                  )
[32m+              else:
[32m+                  self.model_config = torch.load(model_config_path)
[32m+      
[32m+              if pretrained:
[32m+                  self.backbone = AutoModel.from_pretrained(
[32m+                      self.cfg.pretrained_model_name,
[32m+                      config=self.model_config,
[32m+                  )
[32m+                  print(f"Load weight from pretrained")
[32m+              else:
[32m+                  #self.backbone = AutoModel.from_config(self.model_config)
[32m+                  itpt = AutoModelForMaskedLM.from_config(self.model_config)
[32m+                  #path = str(Path("./drive/MyDrive/00.kaggle/output") / CFG.competition_name /  "nbme-exp009/checkpoint-129000/pytorch_model.bin")
[32m+                  path = "../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin"
[32m+                  state_dict = torch.load(path)
[32m+                  itpt.load_state_dict(state_dict)
[32m+                  self.backbone = itpt.deberta
[32m+                  print(f"Load weight from {path}")
[32m+      
[32m+              self.fc = nn.Sequential(
[32m+                  nn.Dropout(self.cfg.dropout),
[32m+                  nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),
[32m+              )
[32m+      
[32m+          def forward(self, inputs):
[32m+              h = self.backbone(**inputs)["last_hidden_state"]
[32m+              output = self.fc(h)
[32m+              return output

[0m[34m## deleted /cells/26:[0m
[31m-  code cell:
[31m-    execution_count: 17
[31m-    metadata (unknown keys):
[31m-      colab:
[31m-        base_uri: https://localhost:8080/
[31m-      executionInfo:
[31m-        elapsed: 1889
[31m-        status: ok
[31m-        timestamp: 1645861087570
[31m-        user:
[31m-          displayName: Shuhei Goda
[31m-          photoUrl: https://lh3.googleusercontent.com/a/default-user=s64
[31m-          userId: 08246931244224045522
[31m-        user_tz: -540
[31m-      id: l6v9UUtX3h8m
[31m-      outputId: fe951334-cb48-4681-eed2-c065e2691367
[31m-    source:
[31m-      model = AutoModelForMaskedLM.from_pretrained(CFG.pretrained_model_name)
[31m-    outputs:
[31m-      output 0:
[31m-        output_type: stream
[31m-        name: stderr
[31m-        text:
[31m-          Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight']
[31m-          - This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
[31m-          - This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[31m-          Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
[31m-          You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[0m[34m## modified /cells/27/id:[0m
[31m-  personalized-interview
[32m+  thorough-bristol

[0m[34m## modified /cells/27/metadata/id:[0m
[31m-  seventh-configuration
[32m+  91401041

[0m[34m## inserted before /cells/28:[0m
[32m+  code cell:
[32m+    execution_count: 25
[32m+    metadata (unknown keys):
[32m+      id: eda8175d
[32m+    source:
[32m+      def train_fn(
[32m+          train_dataloader,
[32m+          model,
[32m+          criterion,
[32m+          optimizer,
[32m+          epoch,
[32m+          scheduler,
[32m+          device,
[32m+      ):
[32m+          model.train()
[32m+          scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)
[32m+          losses = AverageMeter()
[32m+          start = time.time()
[32m+          for step, (inputs, labels) in enumerate(train_dataloader):
[32m+              for k, v in inputs.items():
[32m+                  inputs[k] = v.to(device)
[32m+              labels = labels.to(device)
[32m+              batch_size = labels.size(0)
[32m+      
[32m+              with torch.cuda.amp.autocast(enabled=CFG.apex):
[32m+                  output = model(inputs)
[32m+      
[32m+              loss = criterion(output.view(-1, 1), labels.view(-1, 1))
[32m+              loss = torch.masked_select(loss, labels.view(-1, 1) != -1)
[32m+      
[32m+              pos_nums = (labels == 1).sum(axis=1)
[32m+              pos_nums = torch.tensor([[pos_num] * CFG.max_len for pos_num in pos_nums]).view(-1, 1).to(device)
[32m+              pos_nums = torch.masked_select(pos_nums, labels.view(-1, 1) != -1)
[32m+              weight = []
[32m+              for pos_num in pos_nums:
[32m+                  if pos_num == 0:
[32m+                      weight.append(3.0)
[32m+                  else:
[32m+                      weight.append(1.0)
[32m+              weight = torch.tensor(weight).to(device)
[32m+              loss = loss * weight
[32m+      
[32m+              loss = loss.mean()
[32m+      
[32m+              if CFG.gradient_accumulation_steps > 1:
[32m+                  loss = loss / CFG.gradient_accumulation_steps
[32m+              losses.update(loss.item(), batch_size)
[32m+              scaler.scale(loss).backward()
[32m+              grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)
[32m+      
[32m+              if (step + 1) % CFG.gradient_accumulation_steps == 0:
[32m+                  scaler.step(optimizer)
[32m+                  scaler.update()
[32m+                  optimizer.zero_grad()
[32m+      
[32m+              if CFG.batch_scheduler:
[32m+                  scheduler.step()
[32m+      
[32m+              end = time.time()
[32m+              if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):
[32m+                  print(
[32m+                      "Epoch: [{0}][{1}/{2}] "
[32m+                      "Elapsed {remain:s} "
[32m+                      "Loss: {loss.val:.4f}({loss.avg:.4f}) "
[32m+                      "Grad: {grad_norm:.4f}  "
[32m+                      "LR: {lr:.6f}  "
[32m+                      .format(
[32m+                          epoch+1,
[32m+                          step,
[32m+                          len(train_dataloader),
[32m+                          remain=timeSince(start, float(step+1) / len(train_dataloader)),
[32m+                          loss=losses,
[32m+                           grad_norm=grad_norm,
[32m+                           lr=scheduler.get_lr()[0],
[32m+                      )
[32m+                  )
[32m+          return losses.avg
[32m+  code cell:
[32m+    execution_count: 26
[32m+    metadata (unknown keys):
[32m+      id: c44b63a7
[32m+    source:
[32m+      def valid_fn(
[32m+          val_dataloader,
[32m+          model,
[32m+          criterion,
[32m+          device,
[32m+      ):
[32m+          model.eval()
[32m+          preds = []
[32m+          losses = AverageMeter()
[32m+          start = time.time()
[32m+          for step, (inputs, labels) in enumerate(val_dataloader):
[32m+              for k, v in inputs.items():
[32m+                  inputs[k] = v.to(device)
[32m+              labels = labels.to(device)
[32m+              batch_size = labels.size(0)
[32m+      
[32m+              with torch.no_grad():
[32m+                  output = model(inputs)
[32m+      
[32m+              loss = criterion(output.view(-1, 1), labels.view(-1, 1))
[32m+              loss = torch.masked_select(loss, labels.view(-1, 1) != -1)
[32m+      
[32m+              pos_nums = (labels == 1).sum(axis=1)
[32m+              pos_nums = torch.tensor([[pos_num] * CFG.max_len for pos_num in pos_nums]).view(-1, 1).to(device)
[32m+              pos_nums = torch.masked_select(pos_nums, labels.view(-1, 1) != -1)
[32m+              weight = []
[32m+              for pos_num in pos_nums:
[32m+                  if pos_num == 0:
[32m+                      weight.append(3.0)
[32m+                  else:
[32m+                      weight.append(1.0)
[32m+              weight = torch.tensor(weight).to(device)
[32m+              loss = loss * weight
[32m+      
[32m+              loss = loss.mean()
[32m+      
[32m+              if CFG.gradient_accumulation_steps > 1:
[32m+                  loss = loss / CFG.gradient_accumulation_steps
[32m+              losses.update(loss.item(), batch_size)
[32m+              preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())
[32m+      
[32m+              end = time.time()
[32m+              if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):
[32m+                  print(
[32m+                      "EVAL: [{0}/{1}] "
[32m+                      "Elapsed {remain:s} "
[32m+                      "Loss: {loss.val:.4f}({loss.avg:.4f}) "
[32m+                      .format(
[32m+                          step, len(val_dataloader),
[32m+                          remain=timeSince(start, float(step+1) / len(val_dataloader)),
[32m+                          loss=losses,
[32m+                      )
[32m+                  )
[32m+          preds = np.concatenate(preds)
[32m+          return losses.avg, preds
[32m+  code cell:
[32m+    execution_count: 27
[32m+    metadata (unknown keys):
[32m+      id: 4219ac38
[32m+    source:
[32m+      def inference_fn(test_dataloader, model, device):
[32m+          model.eval()
[32m+          model.to(device)
[32m+          preds = []
[32m+          tk0 = tqdm(test_dataloader, total=len(test_dataloader))
[32m+          for inputs in tk0:
[32m+              for k, v in inputs.items():
[32m+                  inputs[k] = v.to(device)
[32m+              with torch.no_grad():
[32m+                  output = model(inputs)
[32m+              preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())
[32m+          preds = np.concatenate(preds)
[32m+          return preds
[32m+  code cell:
[32m+    execution_count: 28
[32m+    metadata (unknown keys):
[32m+      id: 014a76b7
[32m+    source:
[32m+      def train_loop(df, i_fold, device):
[32m+          print(f"========== fold: {i_fold} training ==========")
[32m+          train_idx = df[df["fold"] != i_fold].index
[32m+          val_idx = df[df["fold"] == i_fold].index
[32m+      
[32m+          train_folds = df.loc[train_idx].reset_index(drop=True)
[32m+          val_folds = df.loc[val_idx].reset_index(drop=True)
[32m+      
[32m+          train_dataset = TrainingDataset(CFG, train_folds)
[32m+          val_dataset = TrainingDataset(CFG, val_folds)
[32m+      
[32m+          train_dataloader = DataLoader(
[32m+              train_dataset,
[32m+              batch_size=CFG.batch_size,
[32m+              shuffle=True,
[32m+              num_workers=CFG.num_workers,
[32m+              pin_memory=True,
[32m+              drop_last=True,
[32m+          )
[32m+          val_dataloader = DataLoader(
[32m+              val_dataset,
[32m+              batch_size=CFG.batch_size,
[32m+              shuffle=False,
[32m+              num_workers=CFG.num_workers,
[32m+              pin_memory=True,
[32m+              drop_last=False,
[32m+          )
[32m+      
[32m+          # model = CustomModel(CFG, model_config_path=None, pretrained=True)
[32m+          model = CustomModel(CFG, model_config_path=None, pretrained=False)   # itpt„Çí‰Ωø„ÅÜ„Åü„ÇÅ
[32m+          torch.save(model.model_config, CFG.output_dir / "model_config.pth")
[32m+          model.to(device)
[32m+      
[32m+          param_optimizer = list(model.named_parameters())
[32m+          no_decay = ["bias", "LayerNorm.bias", "LayerNorm.weight"]
[32m+          optimizer_grouped_parameters = [
[32m+              {"params": [p for n, p in param_optimizer if not any(
[32m+                  nd in n for nd in no_decay)], "weight_decay": CFG.weight_decay},
[32m+              {"params": [p for n, p in param_optimizer if any(
[32m+                  nd in n for nd in no_decay)], "weight_decay": 0.0}
[32m+          ]
[32m+          optimizer = AdamW(
[32m+              optimizer_grouped_parameters,
[32m+              lr=CFG.lr,
[32m+              betas=CFG.betas,
[32m+              weight_decay=CFG.weight_decay,
[32m+          )
[32m+          num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)
[32m+          num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)
[32m+          scheduler = get_linear_schedule_with_warmup(
[32m+              optimizer,
[32m+              num_warmup_steps=num_warmup_steps,
[32m+              num_training_steps=num_train_optimization_steps,
[32m+          )
[32m+      
[32m+          criterion = nn.BCEWithLogitsLoss(reduction="none")
[32m+          best_score = -1 * np.inf
[32m+      
[32m+          for epoch in range(CFG.epochs):
[32m+              start_time = time.time()
[32m+              avg_loss = train_fn(
[32m+                  train_dataloader,
[32m+                  model,
[32m+                  criterion,
[32m+                  optimizer,
[32m+                  epoch,
[32m+                  scheduler,
[32m+                  device,
[32m+              )
[32m+              avg_val_loss, val_preds = valid_fn(
[32m+                  val_dataloader,
[32m+                  model,
[32m+                  criterion,
[32m+                  device,
[32m+              )
[32m+      
[32m+              if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):
[32m+                  scheduler.step()
[32m+      
[32m+              # scoring
[32m+              val_folds[[str(i) for i in range(CFG.max_len)]] = val_preds
[32m+              score = scoring(val_folds, th=0.5)
[32m+      
[32m+              elapsed = time.time() - start_time
[32m+      
[32m+              print(f"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s")
[32m+              print(f"Epoch {epoch+1} - Score: {score:.4f}")
[32m+              if score > best_score:
[32m+                  best_score = score
[32m+                  print(f"Epoch {epoch+1} - Save Best Score: {score:.4f} Model")
[32m+                  torch.save({
[32m+                      "model": model.state_dict(),
[32m+                      "predictions": val_preds,
[32m+                      },
[32m+                      CFG.output_dir / f"fold{i_fold}_best.pth",
[32m+                  )
[32m+      
[32m+          predictions = torch.load(
[32m+              CFG.output_dir / f"fold{i_fold}_best.pth",
[32m+              map_location=torch.device("cpu"),
[32m+          )["predictions"]
[32m+          val_folds[[str(i) for i in range(CFG.max_len)]] = predictions
[32m+      
[32m+          torch.cuda.empty_cache()
[32m+          gc.collect()
[32m+      
[32m+          return val_folds
[32m+  markdown cell:
[32m+    metadata (unknown keys):
[32m+      id: c38fb834
[32m+    source:
[32m+      ## Main
[32m+  code cell:
[32m+    execution_count: 29
[32m+    metadata (unknown keys):
[32m+      id: 62d677cd
[32m+    source:
[32m+      def main():
[32m+          device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
[32m+          if CFG.train:
[32m+              oof_df = pd.DataFrame()
[32m+              for i_fold in range(CFG.n_fold):
[32m+                  if i_fold in CFG.train_fold:
[32m+                      _oof_df = train_loop(train, i_fold, device)
[32m+                      oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)
[32m+              oof_df.to_pickle(CFG.output_dir / "oof_df.pkl")
[32m+      
[32m+          if CFG.submission:
[32m+              oof_df = pd.read_pickle(Path("../input/") / CFG.exp_name / "oof_df.pkl")
[32m+          else:
[32m+              oof_df = pd.read_pickle(CFG.output_dir / "oof_df.pkl")
[32m+      
[32m+          score = scoring(oof_df, th=0.5)
[32m+          print(f"Best thres: 0.5, Score: {score:.4f}")
[32m+          best_thres = get_best_thres(oof_df)
[32m+          score = scoring(oof_df, th=best_thres)
[32m+          print(f"Best thres: {best_thres}, Score: {score:.4f}")
[32m+      
[32m+          if CFG.inference:
[32m+              test_dataset = TestDataset(CFG, test)
[32m+              test_dataloader = DataLoader(
[32m+                  test_dataset,
[32m+                  batch_size=CFG.batch_size,
[32m+                  shuffle=False,
[32m+                  num_workers=CFG.num_workers,
[32m+                  pin_memory=True,
[32m+                  drop_last=False,
[32m+              )
[32m+              predictions = []
[32m+              for i_fold in CFG.train_fold:
[32m+                  if CFG.submission:
[32m+                      model = CustomModel(CFG, model_config_path=Path("../input/") / CFG.exp_name / "model_config.pth", pretrained=False)
[32m+                      path = Path("../input/") / CFG.exp_name / f"fold{i_fold}_best.pth"
[32m+                  else:
[32m+                      model = CustomModel(CFG, model_config_path=None, pretrained=True)
[32m+                      path = CFG.output_dir / f"fold{i_fold}_best.pth"
[32m+      
[32m+                  state = torch.load(path, map_location=torch.device("cpu"))
[32m+                  model.load_state_dict(state["model"])
[32m+                  test_token_probs = inference_fn(test_dataloader, model, device)
[32m+                  test[[f"fold{i_fold}_{i}" for i in range(CFG.max_len)]] = test_token_probs
[32m+                  test_char_probs = get_char_probs(test["pn_history"].values, test_token_probs, CFG.tokenizer)
[32m+                  predictions.append(test_char_probs)
[32m+      
[32m+                  del state, test_token_probs, model; gc.collect()
[32m+                  torch.cuda.empty_cache()
[32m+      
[32m+              predictions = np.mean(predictions, axis=0)
[32m+              predicted_location_str = get_predicted_location_str(predictions, th=best_thres)
[32m+              test[CFG.target_col] = predicted_location_str
[32m+              test.to_csv(CFG.output_dir / "raw_submission.csv", index=False)
[32m+              test[[CFG.id_col, CFG.target_col]].to_csv(
[32m+                  CFG.output_dir / "submission.csv", index=False
[32m+              )
[32m+  code cell:
[32m+    execution_count: 30
[32m+    metadata (unknown keys):
[32m+      colab:
[32m+        base_uri: https://localhost:8080/
[32m+        height: 1000
[32m+        referenced_widgets:
[32m+          item[0]: 5a00641beddc46eb8da430f2d9999490
[32m+          item[1]: 04966868d2974cb8b3215a50572c2c94
[32m+          item[2]: 5a5c41748cba4234a6a6f9aabddfa861
[32m+          item[3]: 72044a7dbe7d4b5f839f38f6e827ec63
[32m+          item[4]: bed6a691643a46d5bd25e03cdc5b73f7
[32m+          item[5]: b4d0bd0dea5341a9b03f0092fe3cba39
[32m+      executionInfo:
[32m+        elapsed: 315
[32m+        status: ok
[32m+        timestamp: 1646034258180
[32m+        user:
[32m+          displayName: Shuhei Goda
[32m+          photoUrl: https://lh3.googleusercontent.com/a/default-user=s64
[32m+          userId: 08246931244224045522
[32m+        user_tz: -540
[32m+      id: 1d4fcf7c
[32m+      outputId: 1362d223-3d70-4ba7-daa5-14b7300eef5c
[32m+    source:
[32m+      if __name__ == "__main__":
[32m+          main()
[32m+    outputs:
[32m+      output 0:
[32m+        output_type: stream
[32m+        name: stdout
[32m+        text:
[32m+          ========== fold: 0 training ==========
[32m+          Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin
[32m+          Epoch: [1][0/2849] Elapsed 0m 1s (remain 58m 45s) Loss: 0.4680(0.4680) Grad: inf  LR: 0.000000  
[32m+          Epoch: [1][100/2849] Elapsed 0m 50s (remain 23m 4s) Loss: 0.3190(0.4386) Grad: 54221.9375  LR: 0.000001  
[32m+          Epoch: [1][200/2849] Elapsed 1m 40s (remain 22m 5s) Loss: 0.0869(0.3411) Grad: 6981.6616  LR: 0.000003  
[32m+          Epoch: [1][300/2849] Elapsed 2m 29s (remain 21m 9s) Loss: 0.0374(0.2434) Grad: 852.8734  LR: 0.000004  
[32m+          Epoch: [1][400/2849] Elapsed 3m 20s (remain 20m 21s) Loss: 0.0816(0.1933) Grad: 1578.8778  LR: 0.000006  
[32m+          Epoch: [1][500/2849] Elapsed 4m 10s (remain 19m 32s) Loss: 0.0309(0.1619) Grad: 937.6583  LR: 0.000007  
[32m+          Epoch: [1][600/2849] Elapsed 5m 0s (remain 18m 42s) Loss: 0.0368(0.1387) Grad: 8138.3501  LR: 0.000008  
[32m+          Epoch: [1][700/2849] Elapsed 5m 50s (remain 17m 54s) Loss: 0.0699(0.1217) Grad: 6198.0088  LR: 0.000010  
[32m+          Epoch: [1][800/2849] Elapsed 6m 41s (remain 17m 5s) Loss: 0.0060(0.1084) Grad: 807.5145  LR: 0.000011  
[32m+          Epoch: [1][900/2849] Elapsed 7m 31s (remain 16m 16s) Loss: 0.0054(0.0979) Grad: 630.7219  LR: 0.000013  
[32m+          Epoch: [1][1000/2849] Elapsed 8m 21s (remain 15m 25s) Loss: 0.0082(0.0893) Grad: 1308.4385  LR: 0.000014  
[32m+          Epoch: [1][1100/2849] Elapsed 9m 10s (remain 14m 34s) Loss: 0.0092(0.0822) Grad: 2203.1423  LR: 0.000015  
[32m+          Epoch: [1][1200/2849] Elapsed 10m 0s (remain 13m 44s) Loss: 0.0054(0.0763) Grad: 599.2184  LR: 0.000017  
[32m+          Epoch: [1][1300/2849] Elapsed 10m 50s (remain 12m 54s) Loss: 0.0073(0.0711) Grad: 693.2661  LR: 0.000018  
[32m+          Epoch: [1][1400/2849] Elapsed 11m 41s (remain 12m 4s) Loss: 0.0132(0.0667) Grad: 1828.4723  LR: 0.000020  
[32m+          Epoch: [1][1500/2849] Elapsed 12m 30s (remain 11m 14s) Loss: 0.0039(0.0628) Grad: 363.3955  LR: 0.000020  
[32m+          Epoch: [1][1600/2849] Elapsed 13m 20s (remain 10m 23s) Loss: 0.0072(0.0596) Grad: 585.2568  LR: 0.000020  
[32m+          Epoch: [1][1700/2849] Elapsed 14m 10s (remain 9m 33s) Loss: 0.0067(0.0567) Grad: 813.5896  LR: 0.000020  
[32m+          Epoch: [1][1800/2849] Elapsed 14m 59s (remain 8m 43s) Loss: 0.0020(0.0539) Grad: 261.2776  LR: 0.000019  
[32m+          Epoch: [1][1900/2849] Elapsed 15m 49s (remain 7m 53s) Loss: 0.0001(0.0516) Grad: 16.2247  LR: 0.000019  
[32m+          Epoch: [1][2000/2849] Elapsed 16m 39s (remain 7m 3s) Loss: 0.0030(0.0494) Grad: 384.2924  LR: 0.000019  
[32m+          Epoch: [1][2100/2849] Elapsed 17m 29s (remain 6m 13s) Loss: 0.0137(0.0474) Grad: 1963.9025  LR: 0.000019  
[32m+          Epoch: [1][2200/2849] Elapsed 18m 19s (remain 5m 23s) Loss: 0.0001(0.0456) Grad: 25.7079  LR: 0.000019  
[32m+          Epoch: [1][2300/2849] Elapsed 19m 8s (remain 4m 33s) Loss: 0.0804(0.0439) Grad: 1369.2407  LR: 0.000019  
[32m+          Epoch: [1][2400/2849] Elapsed 19m 58s (remain 3m 43s) Loss: 0.0032(0.0424) Grad: 159.9443  LR: 0.000018  
[32m+          Epoch: [1][2500/2849] Elapsed 20m 47s (remain 2m 53s) Loss: 0.0031(0.0410) Grad: 228.3253  LR: 0.000018  
[32m+          Epoch: [1][2600/2849] Elapsed 21m 36s (remain 2m 3s) Loss: 0.0014(0.0397) Grad: 66.0907  LR: 0.000018  
[32m+          Epoch: [1][2700/2849] Elapsed 22m 25s (remain 1m 13s) Loss: 0.0039(0.0385) Grad: 280.1574  LR: 0.000018  
[32m+          Epoch: [1][2800/2849] Elapsed 23m 14s (remain 0m 23s) Loss: 0.0026(0.0375) Grad: 106.3157  LR: 0.000018  
[32m+          Epoch: [1][2848/2849] Elapsed 23m 38s (remain 0m 0s) Loss: 0.0049(0.0370) Grad: 402.2635  LR: 0.000018  
[32m+          EVAL: [0/726] Elapsed 0m 0s (remain 6m 5s) Loss: 0.0024(0.0024) 
[32m+          EVAL: [100/726] Elapsed 0m 27s (remain 2m 52s) Loss: 0.0030(0.0063) 
[32m+          EVAL: [200/726] Elapsed 0m 55s (remain 2m 24s) Loss: 0.0005(0.0071) 
[32m+          EVAL: [300/726] Elapsed 1m 22s (remain 1m 56s) Loss: 0.0002(0.0066) 
[32m+          EVAL: [400/726] Elapsed 1m 50s (remain 1m 29s) Loss: 0.0054(0.0082) 
[32m+          EVAL: [500/726] Elapsed 2m 17s (remain 1m 1s) Loss: 0.0139(0.0081) 
[32m+          EVAL: [600/726] Elapsed 2m 44s (remain 0m 34s) Loss: 0.0032(0.0076) 
[32m+          EVAL: [700/726] Elapsed 3m 12s (remain 0m 6s) Loss: 0.0021(0.0073) 
[32m+          EVAL: [725/726] Elapsed 3m 18s (remain 0m 0s) Loss: 0.0005(0.0072) 
[32m+          Epoch 1 - avg_train_loss: 0.0370  avg_val_loss: 0.0072  time: 1623s
[32m+          Epoch 1 - Score: 0.8601
[32m+          Epoch 1 - Save Best Score: 0.8601 Model
[32m+          Epoch: [2][0/2849] Elapsed 0m 0s (remain 36m 6s) Loss: 0.0065(0.0065) Grad: 5447.1030  LR: 0.000018  
[32m+          Epoch: [2][100/2849] Elapsed 0m 51s (remain 23m 27s) Loss: 0.0042(0.0068) Grad: 11439.7852  LR: 0.000018  
[32m+          Epoch: [2][200/2849] Elapsed 1m 41s (remain 22m 21s) Loss: 0.0004(0.0072) Grad: 1160.4572  LR: 0.000017  
[32m+          Epoch: [2][300/2849] Elapsed 2m 31s (remain 21m 25s) Loss: 0.0043(0.0065) Grad: 4548.4243  LR: 0.000017  
[32m+          Epoch: [2][400/2849] Elapsed 3m 21s (remain 20m 32s) Loss: 0.0074(0.0061) Grad: 34142.0664  LR: 0.000017  
[32m+          Epoch: [2][500/2849] Elapsed 4m 11s (remain 19m 40s) Loss: 0.0021(0.0061) Grad: 2389.0862  LR: 0.000017  
[32m+          Epoch: [2][600/2849] Elapsed 5m 2s (remain 18m 51s) Loss: 0.0006(0.0060) Grad: 917.8663  LR: 0.000017  
[32m+          Epoch: [2][700/2849] Elapsed 5m 52s (remain 18m 0s) Loss: 0.0000(0.0059) Grad: 156.5087  LR: 0.000017  
[32m+          Epoch: [2][800/2849] Elapsed 6m 42s (remain 17m 10s) Loss: 0.0010(0.0059) Grad: 12788.2539  LR: 0.000017  
[32m+          Epoch: [2][900/2849] Elapsed 7m 33s (remain 16m 19s) Loss: 0.0077(0.0060) Grad: 10908.5820  LR: 0.000016  
[32m+          Epoch: [2][1000/2849] Elapsed 8m 23s (remain 15m 29s) Loss: 0.0136(0.0060) Grad: 7590.4395  LR: 0.000016  
[32m+          Epoch: [2][1100/2849] Elapsed 9m 14s (remain 14m 39s) Loss: 0.0012(0.0059) Grad: 1929.1815  LR: 0.000016  
[32m+          Epoch: [2][1200/2849] Elapsed 10m 3s (remain 13m 48s) Loss: 0.0196(0.0059) Grad: 27364.0859  LR: 0.000016  
[32m+          Epoch: [2][1300/2849] Elapsed 10m 53s (remain 12m 58s) Loss: 0.0032(0.0060) Grad: 6095.2124  LR: 0.000016  
[32m+          Epoch: [2][1400/2849] Elapsed 11m 44s (remain 12m 7s) Loss: 0.0001(0.0060) Grad: 208.8000  LR: 0.000016  
[32m+          Epoch: [2][1500/2849] Elapsed 12m 34s (remain 11m 18s) Loss: 0.0030(0.0061) Grad: 2875.9353  LR: 0.000015  
[32m+          Epoch: [2][1600/2849] Elapsed 13m 25s (remain 10m 27s) Loss: 0.0294(0.0060) Grad: 61794.0781  LR: 0.000015  
[32m+          Epoch: [2][1700/2849] Elapsed 14m 15s (remain 9m 37s) Loss: 0.0049(0.0061) Grad: 4343.4277  LR: 0.000015  
[32m+          Epoch: [2][1800/2849] Elapsed 15m 6s (remain 8m 47s) Loss: 0.0044(0.0061) Grad: 4103.5620  LR: 0.000015  
[32m+          Epoch: [2][1900/2849] Elapsed 15m 56s (remain 7m 56s) Loss: 0.0017(0.0061) Grad: 2349.4270  LR: 0.000015  
[32m+          Epoch: [2][2000/2849] Elapsed 16m 46s (remain 7m 6s) Loss: 0.0016(0.0061) Grad: 1707.2762  LR: 0.000015  
[32m+          Epoch: [2][2100/2849] Elapsed 17m 37s (remain 6m 16s) Loss: 0.0004(0.0061) Grad: 669.1428  LR: 0.000014  
[32m+          Epoch: [2][2200/2849] Elapsed 18m 27s (remain 5m 26s) Loss: 0.0241(0.0060) Grad: 24350.5664  LR: 0.000014  
[32m+          Epoch: [2][2300/2849] Elapsed 19m 17s (remain 4m 35s) Loss: 0.0029(0.0060) Grad: 4060.5388  LR: 0.000014  
[32m+          Epoch: [2][2400/2849] Elapsed 20m 7s (remain 3m 45s) Loss: 0.0049(0.0060) Grad: 6143.8384  LR: 0.000014  
[32m+          Epoch: [2][2500/2849] Elapsed 20m 57s (remain 2m 55s) Loss: 0.0003(0.0060) Grad: 601.2382  LR: 0.000014  
[32m+          Epoch: [2][2600/2849] Elapsed 21m 48s (remain 2m 4s) Loss: 0.0209(0.0060) Grad: 13459.1348  LR: 0.000014  
[32m+          Epoch: [2][2700/2849] Elapsed 22m 39s (remain 1m 14s) Loss: 0.0027(0.0060) Grad: 2602.1848  LR: 0.000014  
[32m+          Epoch: [2][2800/2849] Elapsed 23m 30s (remain 0m 24s) Loss: 0.0002(0.0059) Grad: 553.4805  LR: 0.000013  
[32m+          Epoch: [2][2848/2849] Elapsed 23m 54s (remain 0m 0s) Loss: 0.0002(0.0060) Grad: 888.4745  LR: 0.000013  
[32m+          EVAL: [0/726] Elapsed 0m 0s (remain 5m 58s) Loss: 0.0013(0.0013) 
[32m+          EVAL: [100/726] Elapsed 0m 27s (remain 2m 50s) Loss: 0.0030(0.0081) 
[32m+          EVAL: [200/726] Elapsed 0m 55s (remain 2m 23s) Loss: 0.0001(0.0074) 
[32m+          EVAL: [300/726] Elapsed 1m 22s (remain 1m 55s) Loss: 0.0002(0.0071) 
[32m+          EVAL: [400/726] Elapsed 1m 49s (remain 1m 28s) Loss: 0.0040(0.0091) 
[32m+          EVAL: [500/726] Elapsed 2m 16s (remain 1m 1s) Loss: 0.0255(0.0091) 
[32m+          EVAL: [600/726] Elapsed 2m 44s (remain 0m 34s) Loss: 0.0010(0.0084) 
[32m+          EVAL: [700/726] Elapsed 3m 11s (remain 0m 6s) Loss: 0.0034(0.0082) 
[32m+          EVAL: [725/726] Elapsed 3m 17s (remain 0m 0s) Loss: 0.0000(0.0081) 
[32m+          Epoch 2 - avg_train_loss: 0.0060  avg_val_loss: 0.0081  time: 1638s
[32m+          Epoch 2 - Score: 0.8770
[32m+          Epoch 2 - Save Best Score: 0.8770 Model
[32m+          Epoch: [3][0/2849] Elapsed 0m 0s (remain 35m 41s) Loss: 0.0010(0.0010) Grad: 33392.8906  LR: 0.000013  
[32m+          Epoch: [3][100/2849] Elapsed 0m 51s (remain 23m 19s) Loss: 0.0002(0.0032) Grad: 860.1767  LR: 0.000013  
[32m+          Epoch: [3][200/2849] Elapsed 1m 41s (remain 22m 11s) Loss: 0.0103(0.0034) Grad: 6645.3472  LR: 0.000013  
[32m+          Epoch: [3][300/2849] Elapsed 2m 30s (remain 21m 17s) Loss: 0.0000(0.0036) Grad: 128.7362  LR: 0.000013  
[32m+          Epoch: [3][400/2849] Elapsed 3m 20s (remain 20m 25s) Loss: 0.0003(0.0038) Grad: 974.0513  LR: 0.000013  
[32m+          Epoch: [3][500/2849] Elapsed 4m 11s (remain 19m 36s) Loss: 0.0055(0.0037) Grad: 6263.3423  LR: 0.000013  
[32m+          Epoch: [3][600/2849] Elapsed 5m 0s (remain 18m 45s) Loss: 0.0301(0.0040) Grad: 9805.2236  LR: 0.000012  
[32m+          Epoch: [3][700/2849] Elapsed 5m 51s (remain 17m 57s) Loss: 0.0057(0.0042) Grad: 12790.1650  LR: 0.000012  
[32m+          Epoch: [3][800/2849] Elapsed 6m 41s (remain 17m 7s) Loss: 0.0000(0.0042) Grad: 151.9216  LR: 0.000012  
[32m+          Epoch: [3][900/2849] Elapsed 7m 31s (remain 16m 16s) Loss: 0.0023(0.0043) Grad: 3455.3315  LR: 0.000012  
[32m+          Epoch: [3][1000/2849] Elapsed 8m 21s (remain 15m 26s) Loss: 0.0050(0.0043) Grad: 4050.8269  LR: 0.000012  
[32m+          Epoch: [3][1100/2849] Elapsed 9m 11s (remain 14m 36s) Loss: 0.0051(0.0044) Grad: 7272.1284  LR: 0.000012  
[32m+          Epoch: [3][1200/2849] Elapsed 10m 2s (remain 13m 46s) Loss: 0.0000(0.0044) Grad: 13.0525  LR: 0.000011  
[32m+          Epoch: [3][1300/2849] Elapsed 10m 52s (remain 12m 56s) Loss: 0.0006(0.0046) Grad: 1267.9907  LR: 0.000011  
[32m+          Epoch: [3][1400/2849] Elapsed 11m 42s (remain 12m 6s) Loss: 0.0043(0.0047) Grad: 3445.0874  LR: 0.000011  
[32m+          Epoch: [3][1500/2849] Elapsed 12m 32s (remain 11m 15s) Loss: 0.0002(0.0046) Grad: 782.2497  LR: 0.000011  
[32m+          Epoch: [3][1600/2849] Elapsed 13m 21s (remain 10m 25s) Loss: 0.0020(0.0045) Grad: 3098.0647  LR: 0.000011  
[32m+          Epoch: [3][1700/2849] Elapsed 14m 12s (remain 9m 35s) Loss: 0.0127(0.0045) Grad: 9692.0342  LR: 0.000011  
[32m+          Epoch: [3][1800/2849] Elapsed 15m 1s (remain 8m 44s) Loss: 0.0016(0.0045) Grad: 10123.1787  LR: 0.000011  
[32m+          Epoch: [3][1900/2849] Elapsed 15m 52s (remain 7m 55s) Loss: 0.0022(0.0045) Grad: 2989.7847  LR: 0.000010  
[32m+          Epoch: [3][2000/2849] Elapsed 16m 42s (remain 7m 5s) Loss: 0.0000(0.0045) Grad: 37.9997  LR: 0.000010  
[32m+          Epoch: [3][2100/2849] Elapsed 17m 33s (remain 6m 14s) Loss: 0.0001(0.0044) Grad: 161.5843  LR: 0.000010  
[32m+          Epoch: [3][2200/2849] Elapsed 18m 22s (remain 5m 24s) Loss: 0.0000(0.0044) Grad: 71.3622  LR: 0.000010  
[32m+          Epoch: [3][2300/2849] Elapsed 19m 12s (remain 4m 34s) Loss: 0.0017(0.0045) Grad: 2258.5894  LR: 0.000010  
[32m+          Epoch: [3][2400/2849] Elapsed 20m 2s (remain 3m 44s) Loss: 0.0107(0.0045) Grad: 20604.5176  LR: 0.000010  
[32m+          Epoch: [3][2500/2849] Elapsed 20m 52s (remain 2m 54s) Loss: 0.0027(0.0045) Grad: 3330.4189  LR: 0.000009  
[32m+          Epoch: [3][2600/2849] Elapsed 21m 43s (remain 2m 4s) Loss: 0.0118(0.0045) Grad: 22601.0156  LR: 0.000009  
[32m+          Epoch: [3][2700/2849] Elapsed 22m 33s (remain 1m 14s) Loss: 0.0000(0.0045) Grad: 23.6585  LR: 0.000009  
[32m+          Epoch: [3][2800/2849] Elapsed 23m 23s (remain 0m 24s) Loss: 0.0160(0.0046) Grad: 7440.5093  LR: 0.000009  
[32m+          Epoch: [3][2848/2849] Elapsed 23m 47s (remain 0m 0s) Loss: 0.0000(0.0046) Grad: 64.8253  LR: 0.000009  
[32m+          EVAL: [0/726] Elapsed 0m 0s (remain 5m 28s) Loss: 0.0006(0.0006) 
[32m+          EVAL: [100/726] Elapsed 0m 27s (remain 2m 50s) Loss: 0.0022(0.0075) 
[32m+          EVAL: [200/726] Elapsed 0m 55s (remain 2m 23s) Loss: 0.0001(0.0072) 
[32m+          EVAL: [300/726] Elapsed 1m 22s (remain 1m 56s) Loss: 0.0000(0.0071) 
[32m+          EVAL: [400/726] Elapsed 1m 49s (remain 1m 29s) Loss: 0.0031(0.0089) 
[32m+          EVAL: [500/726] Elapsed 2m 17s (remain 1m 1s) Loss: 0.0200(0.0089) 
[32m+          EVAL: [600/726] Elapsed 2m 44s (remain 0m 34s) Loss: 0.0049(0.0082) 
[32m+          EVAL: [700/726] Elapsed 3m 11s (remain 0m 6s) Loss: 0.0042(0.0081) 
[32m+          EVAL: [725/726] Elapsed 3m 18s (remain 0m 0s) Loss: 0.0000(0.0079) 
[32m+          Epoch 3 - avg_train_loss: 0.0046  avg_val_loss: 0.0079  time: 1631s
[32m+          Epoch 3 - Score: 0.8827
[32m+          Epoch 3 - Save Best Score: 0.8827 Model
[32m+          Epoch: [4][0/2849] Elapsed 0m 0s (remain 36m 12s) Loss: 0.0010(0.0010) Grad: 3598.9495  LR: 0.000009  
[32m+          Epoch: [4][100/2849] Elapsed 0m 51s (remain 23m 17s) Loss: 0.0028(0.0029) Grad: 11888.0723  LR: 0.000009  
[32m+          Epoch: [4][200/2849] Elapsed 1m 41s (remain 22m 22s) Loss: 0.0067(0.0032) Grad: 12633.9785  LR: 0.000009  
[32m+          Epoch: [4][300/2849] Elapsed 2m 31s (remain 21m 26s) Loss: 0.0009(0.0031) Grad: 5256.6855  LR: 0.000008  
[32m+          Epoch: [4][400/2849] Elapsed 3m 22s (remain 20m 33s) Loss: 0.0011(0.0030) Grad: 3940.4138  LR: 0.000008  
[32m+          Epoch: [4][500/2849] Elapsed 4m 11s (remain 19m 39s) Loss: 0.0000(0.0031) Grad: 75.3200  LR: 0.000008  
[32m+          Epoch: [4][600/2849] Elapsed 5m 1s (remain 18m 49s) Loss: 0.0001(0.0036) Grad: 321.4038  LR: 0.000008  
[32m+          Epoch: [4][700/2849] Elapsed 5m 51s (remain 17m 58s) Loss: 0.0001(0.0035) Grad: 902.2421  LR: 0.000008  
[32m+          Epoch: [4][800/2849] Elapsed 6m 41s (remain 17m 6s) Loss: 0.0057(0.0034) Grad: 14935.1084  LR: 0.000008  
[32m+          Epoch: [4][900/2849] Elapsed 7m 30s (remain 16m 15s) Loss: 0.0063(0.0034) Grad: 31559.5137  LR: 0.000007  
[32m+          Epoch: [4][1000/2849] Elapsed 8m 21s (remain 15m 25s) Loss: 0.0395(0.0034) Grad: 43110.3555  LR: 0.000007  
[32m+          Epoch: [4][1100/2849] Elapsed 9m 11s (remain 14m 35s) Loss: 0.0016(0.0035) Grad: 11770.8662  LR: 0.000007  
[32m+          Epoch: [4][1200/2849] Elapsed 10m 1s (remain 13m 45s) Loss: 0.0019(0.0036) Grad: 19313.3711  LR: 0.000007  
[32m+          Epoch: [4][1300/2849] Elapsed 10m 52s (remain 12m 55s) Loss: 0.0001(0.0034) Grad: 388.3870  LR: 0.000007  
[32m+          Epoch: [4][1400/2849] Elapsed 11m 42s (remain 12m 6s) Loss: 0.0013(0.0035) Grad: 5937.6455  LR: 0.000007  
[32m+          Epoch: [4][1500/2849] Elapsed 12m 32s (remain 11m 15s) Loss: 0.0000(0.0036) Grad: 140.5688  LR: 0.000007  
[32m+          Epoch: [4][1600/2849] Elapsed 13m 22s (remain 10m 25s) Loss: 0.0004(0.0036) Grad: 2674.5085  LR: 0.000006  
[32m+          Epoch: [4][1700/2849] Elapsed 14m 12s (remain 9m 35s) Loss: 0.0050(0.0036) Grad: 13573.5459  LR: 0.000006  
[32m+          Epoch: [4][1800/2849] Elapsed 15m 2s (remain 8m 45s) Loss: 0.0002(0.0035) Grad: 1501.7291  LR: 0.000006  
[32m+          Epoch: [4][1900/2849] Elapsed 15m 53s (remain 7m 55s) Loss: 0.0001(0.0036) Grad: 387.2801  LR: 0.000006  
[32m+          Epoch: [4][2000/2849] Elapsed 16m 43s (remain 7m 5s) Loss: 0.0030(0.0036) Grad: 8117.0088  LR: 0.000006  
[32m+          Epoch: [4][2100/2849] Elapsed 17m 33s (remain 6m 14s) Loss: 0.0001(0.0036) Grad: 728.6862  LR: 0.000006  
[32m+          Epoch: [4][2200/2849] Elapsed 18m 23s (remain 5m 24s) Loss: 0.0001(0.0036) Grad: 299.7931  LR: 0.000005  
[32m+          Epoch: [4][2300/2849] Elapsed 19m 13s (remain 4m 34s) Loss: 0.0000(0.0036) Grad: 36.6309  LR: 0.000005  
[32m+          Epoch: [4][2400/2849] Elapsed 20m 3s (remain 3m 44s) Loss: 0.0273(0.0036) Grad: 43553.3711  LR: 0.000005  
[32m+          Epoch: [4][2500/2849] Elapsed 20m 53s (remain 2m 54s) Loss: 0.0000(0.0036) Grad: 70.9267  LR: 0.000005  
[32m+          Epoch: [4][2600/2849] Elapsed 21m 43s (remain 2m 4s) Loss: 0.0138(0.0036) Grad: 22952.7734  LR: 0.000005  
[32m+          Epoch: [4][2700/2849] Elapsed 22m 34s (remain 1m 14s) Loss: 0.0003(0.0036) Grad: 4143.7959  LR: 0.000005  
[32m+          Epoch: [4][2800/2849] Elapsed 23m 24s (remain 0m 24s) Loss: 0.0000(0.0036) Grad: 64.7554  LR: 0.000005  
[32m+          Epoch: [4][2848/2849] Elapsed 23m 48s (remain 0m 0s) Loss: 0.0000(0.0036) Grad: 53.9092  LR: 0.000004  
[32m+          EVAL: [0/726] Elapsed 0m 0s (remain 6m 6s) Loss: 0.0003(0.0003) 
[32m+          EVAL: [100/726] Elapsed 0m 27s (remain 2m 49s) Loss: 0.0034(0.0098) 
[32m+          EVAL: [200/726] Elapsed 0m 55s (remain 2m 23s) Loss: 0.0000(0.0091) 
[32m+          EVAL: [300/726] Elapsed 1m 22s (remain 1m 56s) Loss: 0.0000(0.0089) 
[32m+          EVAL: [400/726] Elapsed 1m 49s (remain 1m 28s) Loss: 0.0019(0.0113) 
[32m+          EVAL: [500/726] Elapsed 2m 17s (remain 1m 1s) Loss: 0.0298(0.0112) 
[32m+          EVAL: [600/726] Elapsed 2m 44s (remain 0m 34s) Loss: 0.0037(0.0102) 
[32m+          EVAL: [700/726] Elapsed 3m 11s (remain 0m 6s) Loss: 0.0049(0.0100) 
[32m+          EVAL: [725/726] Elapsed 3m 18s (remain 0m 0s) Loss: 0.0000(0.0099) 
[32m+          Epoch 4 - avg_train_loss: 0.0036  avg_val_loss: 0.0099  time: 1632s
[32m+          Epoch 4 - Score: 0.8898
[32m+          Epoch 4 - Save Best Score: 0.8898 Model
[32m+          Epoch: [5][0/2849] Elapsed 0m 0s (remain 35m 46s) Loss: 0.0000(0.0000) Grad: 56.9750  LR: 0.000004  
[32m+          Epoch: [5][100/2849] Elapsed 0m 50s (remain 23m 3s) Loss: 0.0002(0.0035) Grad: 1980.9324  LR: 0.000004  
[32m+          Epoch: [5][200/2849] Elapsed 1m 40s (remain 22m 8s) Loss: 0.0000(0.0032) Grad: 274.5029  LR: 0.000004  
[32m+          Epoch: [5][300/2849] Elapsed 2m 31s (remain 21m 24s) Loss: 0.0000(0.0033) Grad: 158.5166  LR: 0.000004  
[32m+          Epoch: [5][400/2849] Elapsed 3m 21s (remain 20m 31s) Loss: 0.0000(0.0029) Grad: 15.6304  LR: 0.000004  
[32m+          Epoch: [5][500/2849] Elapsed 4m 11s (remain 19m 37s) Loss: 0.0005(0.0026) Grad: 4241.6284  LR: 0.000004  
[32m+          Epoch: [5][600/2849] Elapsed 5m 0s (remain 18m 45s) Loss: 0.0000(0.0029) Grad: 12.8810  LR: 0.000004  
[32m+          Epoch: [5][700/2849] Elapsed 5m 51s (remain 17m 56s) Loss: 0.0042(0.0027) Grad: 9834.7197  LR: 0.000003  
[32m+          Epoch: [5][800/2849] Elapsed 6m 41s (remain 17m 7s) Loss: 0.0002(0.0027) Grad: 1659.0437  LR: 0.000003  
[32m+          Epoch: [5][900/2849] Elapsed 7m 32s (remain 16m 17s) Loss: 0.0078(0.0028) Grad: 35262.6367  LR: 0.000003  
[32m+          Epoch: [5][1000/2849] Elapsed 8m 21s (remain 15m 26s) Loss: 0.0082(0.0027) Grad: 16082.6084  LR: 0.000003  
[32m+          Epoch: [5][1100/2849] Elapsed 9m 11s (remain 14m 35s) Loss: 0.0002(0.0027) Grad: 806.6213  LR: 0.000003  
[32m+          Epoch: [5][1200/2849] Elapsed 10m 1s (remain 13m 45s) Loss: 0.1157(0.0028) Grad: 80812.9531  LR: 0.000003  
[32m+          Epoch: [5][1300/2849] Elapsed 10m 51s (remain 12m 55s) Loss: 0.0000(0.0028) Grad: 4.9433  LR: 0.000002  
[32m+          Epoch: [5][1400/2849] Elapsed 11m 41s (remain 12m 5s) Loss: 0.0005(0.0029) Grad: 4237.0693  LR: 0.000002  
[32m+          Epoch: [5][1500/2849] Elapsed 12m 31s (remain 11m 14s) Loss: 0.0072(0.0028) Grad: 34146.7422  LR: 0.000002  
[32m+          Epoch: [5][1600/2849] Elapsed 13m 20s (remain 10m 24s) Loss: 0.0504(0.0028) Grad: 51213.6641  LR: 0.000002  
[32m+          Epoch: [5][1700/2849] Elapsed 14m 10s (remain 9m 34s) Loss: 0.0000(0.0028) Grad: 53.9328  LR: 0.000002  
[32m+          Epoch: [5][1800/2849] Elapsed 15m 0s (remain 8m 44s) Loss: 0.0129(0.0028) Grad: 21291.2148  LR: 0.000002  
[32m+          Epoch: [5][1900/2849] Elapsed 15m 50s (remain 7m 54s) Loss: 0.0000(0.0029) Grad: 8.7777  LR: 0.000001  
[32m+          Epoch: [5][2000/2849] Elapsed 16m 41s (remain 7m 4s) Loss: 0.0000(0.0029) Grad: 19.9947  LR: 0.000001  
[32m+          Epoch: [5][2100/2849] Elapsed 17m 31s (remain 6m 14s) Loss: 0.0026(0.0029) Grad: 14780.4922  LR: 0.000001  
[32m+          Epoch: [5][2200/2849] Elapsed 18m 21s (remain 5m 24s) Loss: 0.0000(0.0030) Grad: 14.0863  LR: 0.000001  
[32m+          Epoch: [5][2300/2849] Elapsed 19m 11s (remain 4m 34s) Loss: 0.0002(0.0030) Grad: 3825.5044  LR: 0.000001  
[32m+          Epoch: [5][2400/2849] Elapsed 20m 0s (remain 3m 44s) Loss: 0.0005(0.0030) Grad: 3217.6472  LR: 0.000001  
[32m+          Epoch: [5][2500/2849] Elapsed 20m 50s (remain 2m 53s) Loss: 0.0000(0.0030) Grad: 21.1870  LR: 0.000001  
[32m+          Epoch: [5][2600/2849] Elapsed 21m 39s (remain 2m 3s) Loss: 0.0000(0.0030) Grad: 8.3170  LR: 0.000000  
[32m+          Epoch: [5][2700/2849] Elapsed 22m 30s (remain 1m 13s) Loss: 0.0000(0.0030) Grad: 7.9358  LR: 0.000000  
[32m+          Epoch: [5][2800/2849] Elapsed 23m 19s (remain 0m 23s) Loss: 0.0000(0.0029) Grad: 41.6104  LR: 0.000000  
[32m+          Epoch: [5][2848/2849] Elapsed 23m 43s (remain 0m 0s) Loss: 0.0001(0.0030) Grad: 702.7087  LR: 0.000000  
[32m+          EVAL: [0/726] Elapsed 0m 0s (remain 5m 51s) Loss: 0.0005(0.0005) 
[32m+          EVAL: [100/726] Elapsed 0m 27s (remain 2m 50s) Loss: 0.0044(0.0104) 
[32m+          EVAL: [200/726] Elapsed 0m 55s (remain 2m 24s) Loss: 0.0000(0.0097) 
[32m+          EVAL: [300/726] Elapsed 1m 22s (remain 1m 56s) Loss: 0.0000(0.0094) 
[32m+          EVAL: [400/726] Elapsed 1m 49s (remain 1m 28s) Loss: 0.0023(0.0117) 
[32m+          EVAL: [500/726] Elapsed 2m 17s (remain 1m 1s) Loss: 0.0300(0.0116) 
[32m+          EVAL: [600/726] Elapsed 2m 44s (remain 0m 34s) Loss: 0.0025(0.0105) 
[32m+          EVAL: [700/726] Elapsed 3m 11s (remain 0m 6s) Loss: 0.0043(0.0104) 
[32m+          EVAL: [725/726] Elapsed 3m 18s (remain 0m 0s) Loss: 0.0000(0.0102) 
[32m+          Epoch 5 - avg_train_loss: 0.0030  avg_val_loss: 0.0102  time: 1628s
[32m+          Epoch 5 - Score: 0.8904
[32m+          Epoch 5 - Save Best Score: 0.8904 Model
[32m+          ========== fold: 1 training ==========
[32m+          Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin
[32m+          Epoch: [1][0/2851] Elapsed 0m 0s (remain 32m 53s) Loss: 0.7075(0.7075) Grad: inf  LR: 0.000000  
[32m+          Epoch: [1][100/2851] Elapsed 0m 50s (remain 23m 4s) Loss: 0.5893(0.5217) Grad: 104263.4531  LR: 0.000001  
[32m+          Epoch: [1][200/2851] Elapsed 1m 41s (remain 22m 20s) Loss: 0.1478(0.4031) Grad: 13508.2773  LR: 0.000003  
[32m+          Epoch: [1][300/2851] Elapsed 2m 32s (remain 21m 27s) Loss: 0.0371(0.2913) Grad: 1257.5658  LR: 0.000004  
[32m+          Epoch: [1][400/2851] Elapsed 3m 21s (remain 20m 33s) Loss: 0.0694(0.2289) Grad: 2883.9587  LR: 0.000006  
[32m+          Epoch: [1][500/2851] Elapsed 4m 12s (remain 19m 43s) Loss: 0.0426(0.1906) Grad: 1430.2423  LR: 0.000007  
[32m+          Epoch: [1][600/2851] Elapsed 5m 2s (remain 18m 52s) Loss: 0.0193(0.1636) Grad: 4334.7568  LR: 0.000008  
[32m+          Epoch: [1][700/2851] Elapsed 5m 52s (remain 18m 1s) Loss: 0.0094(0.1427) Grad: 3571.9719  LR: 0.000010  
[32m+          Epoch: [1][800/2851] Elapsed 6m 42s (remain 17m 9s) Loss: 0.0049(0.1272) Grad: 2099.0239  LR: 0.000011  
[32m+          Epoch: [1][900/2851] Elapsed 7m 32s (remain 16m 19s) Loss: 0.0080(0.1146) Grad: 1668.4113  LR: 0.000013  
[32m+          Epoch: [1][1000/2851] Elapsed 8m 23s (remain 15m 29s) Loss: 0.0015(0.1045) Grad: 630.8103  LR: 0.000014  
[32m+          Epoch: [1][1100/2851] Elapsed 9m 13s (remain 14m 40s) Loss: 0.0030(0.0960) Grad: 1882.4629  LR: 0.000015  
[32m+          Epoch: [1][1200/2851] Elapsed 10m 3s (remain 13m 49s) Loss: 0.0210(0.0888) Grad: 2217.9731  LR: 0.000017  
[32m+          Epoch: [1][1300/2851] Elapsed 10m 53s (remain 12m 59s) Loss: 0.0011(0.0828) Grad: 316.9373  LR: 0.000018  
[32m+          Epoch: [1][1400/2851] Elapsed 11m 44s (remain 12m 9s) Loss: 0.0026(0.0778) Grad: 659.0336  LR: 0.000020  
[32m+          Epoch: [1][1500/2851] Elapsed 12m 34s (remain 11m 18s) Loss: 0.0034(0.0733) Grad: 1110.5559  LR: 0.000020  
[32m+          Epoch: [1][1600/2851] Elapsed 13m 24s (remain 10m 28s) Loss: 0.0092(0.0694) Grad: 2267.2603  LR: 0.000020  
[32m+          Epoch: [1][1700/2851] Elapsed 14m 14s (remain 9m 37s) Loss: 0.0036(0.0659) Grad: 2297.1348  LR: 0.000020  
[32m+          Epoch: [1][1800/2851] Elapsed 15m 4s (remain 8m 47s) Loss: 0.0097(0.0627) Grad: 3072.6497  LR: 0.000019  
[32m+          Epoch: [1][1900/2851] Elapsed 15m 54s (remain 7m 57s) Loss: 0.0075(0.0598) Grad: 3651.8984  LR: 0.000019  
[32m+          Epoch: [1][2000/2851] Elapsed 16m 45s (remain 7m 7s) Loss: 0.0026(0.0572) Grad: 461.3329  LR: 0.000019  
[32m+          Epoch: [1][2100/2851] Elapsed 17m 35s (remain 6m 16s) Loss: 0.0063(0.0549) Grad: 2005.7234  LR: 0.000019  
[32m+          Epoch: [1][2200/2851] Elapsed 18m 24s (remain 5m 26s) Loss: 0.0025(0.0528) Grad: 545.5583  LR: 0.000019  
[32m+          Epoch: [1][2300/2851] Elapsed 19m 14s (remain 4m 35s) Loss: 0.0045(0.0510) Grad: 2511.8735  LR: 0.000019  
[32m+          Epoch: [1][2400/2851] Elapsed 20m 5s (remain 3m 45s) Loss: 0.0119(0.0492) Grad: 1437.2697  LR: 0.000018  
[32m+          Epoch: [1][2500/2851] Elapsed 20m 54s (remain 2m 55s) Loss: 0.0013(0.0476) Grad: 260.0315  LR: 0.000018  
[32m+          Epoch: [1][2600/2851] Elapsed 21m 44s (remain 2m 5s) Loss: 0.0031(0.0460) Grad: 492.0229  LR: 0.000018  
[32m+          Epoch: [1][2700/2851] Elapsed 22m 34s (remain 1m 15s) Loss: 0.0069(0.0446) Grad: 1541.1666  LR: 0.000018  
[32m+          Epoch: [1][2800/2851] Elapsed 23m 23s (remain 0m 25s) Loss: 0.0088(0.0432) Grad: 1421.0416  LR: 0.000018  
[32m+          Epoch: [1][2850/2851] Elapsed 23m 48s (remain 0m 0s) Loss: 0.0235(0.0425) Grad: 8656.0527  LR: 0.000018  
[32m+          EVAL: [0/724] Elapsed 0m 0s (remain 6m 14s) Loss: 0.0012(0.0012) 
[32m+          EVAL: [100/724] Elapsed 0m 28s (remain 2m 53s) Loss: 0.0025(0.0069) 
[32m+          EVAL: [200/724] Elapsed 0m 56s (remain 2m 26s) Loss: 0.0005(0.0081) 
[32m+          EVAL: [300/724] Elapsed 1m 23s (remain 1m 57s) Loss: 0.0004(0.0085) 
[32m+          EVAL: [400/724] Elapsed 1m 50s (remain 1m 29s) Loss: 0.0001(0.0086) 
[32m+          EVAL: [500/724] Elapsed 2m 18s (remain 1m 1s) Loss: 0.0112(0.0096) 
[32m+          EVAL: [600/724] Elapsed 2m 45s (remain 0m 33s) Loss: 0.0034(0.0093) 
[32m+          EVAL: [700/724] Elapsed 3m 12s (remain 0m 6s) Loss: 0.0001(0.0088) 
[32m+          EVAL: [723/724] Elapsed 3m 18s (remain 0m 0s) Loss: 0.0006(0.0087) 
[32m+          Epoch 1 - avg_train_loss: 0.0425  avg_val_loss: 0.0087  time: 1633s
[32m+          Epoch 1 - Score: 0.8347
[32m+          Epoch 1 - Save Best Score: 0.8347 Model
[32m+          Epoch: [2][0/2851] Elapsed 0m 0s (remain 35m 30s) Loss: 0.0007(0.0007) Grad: 1690.1714  LR: 0.000018  
[32m+          Epoch: [2][100/2851] Elapsed 0m 51s (remain 23m 19s) Loss: 0.0033(0.0063) Grad: 5984.9937  LR: 0.000018  
[32m+          Epoch: [2][200/2851] Elapsed 1m 41s (remain 22m 17s) Loss: 0.0000(0.0061) Grad: 538.6644  LR: 0.000017  
[32m+          Epoch: [2][300/2851] Elapsed 2m 32s (remain 21m 27s) Loss: 0.0027(0.0067) Grad: 6439.3218  LR: 0.000017  
[32m+          Epoch: [2][400/2851] Elapsed 3m 23s (remain 20m 42s) Loss: 0.0007(0.0064) Grad: 2183.5029  LR: 0.000017  
[32m+          Epoch: [2][500/2851] Elapsed 4m 13s (remain 19m 50s) Loss: 0.0126(0.0067) Grad: 10765.1094  LR: 0.000017  
[32m+          Epoch: [2][600/2851] Elapsed 5m 4s (remain 18m 58s) Loss: 0.0016(0.0064) Grad: 4893.7197  LR: 0.000017  
[32m+          Epoch: [2][700/2851] Elapsed 5m 55s (remain 18m 10s) Loss: 0.0004(0.0065) Grad: 1387.1985  LR: 0.000017  
[32m+          Epoch: [2][800/2851] Elapsed 6m 46s (remain 17m 20s) Loss: 0.0001(0.0065) Grad: 335.3214  LR: 0.000017  
[32m+          Epoch: [2][900/2851] Elapsed 7m 37s (remain 16m 29s) Loss: 0.0003(0.0066) Grad: 1701.4608  LR: 0.000016  
[32m+          Epoch: [2][1000/2851] Elapsed 8m 28s (remain 15m 39s) Loss: 0.0018(0.0066) Grad: 7533.8789  LR: 0.000016  
[32m+          Epoch: [2][1100/2851] Elapsed 9m 18s (remain 14m 48s) Loss: 0.0003(0.0066) Grad: 2079.3167  LR: 0.000016  
[32m+          Epoch: [2][1200/2851] Elapsed 10m 9s (remain 13m 56s) Loss: 0.0025(0.0066) Grad: 10057.2139  LR: 0.000016  
[32m+          Epoch: [2][1300/2851] Elapsed 10m 59s (remain 13m 6s) Loss: 0.0001(0.0066) Grad: 1484.3993  LR: 0.000016  
[32m+          Epoch: [2][1400/2851] Elapsed 11m 51s (remain 12m 16s) Loss: 0.0114(0.0065) Grad: 33786.5938  LR: 0.000016  
[32m+          Epoch: [2][1500/2851] Elapsed 12m 42s (remain 11m 26s) Loss: 0.0004(0.0066) Grad: 1578.1797  LR: 0.000015  
[32m+          Epoch: [2][1600/2851] Elapsed 13m 33s (remain 10m 34s) Loss: 0.0006(0.0066) Grad: 2648.3494  LR: 0.000015  
[32m+          Epoch: [2][1700/2851] Elapsed 14m 23s (remain 9m 43s) Loss: 0.0005(0.0066) Grad: 1855.3726  LR: 0.000015  
[32m+          Epoch: [2][1800/2851] Elapsed 15m 14s (remain 8m 52s) Loss: 0.0001(0.0067) Grad: 371.4232  LR: 0.000015  
[32m+          Epoch: [2][1900/2851] Elapsed 16m 5s (remain 8m 2s) Loss: 0.0100(0.0066) Grad: 16884.4688  LR: 0.000015  
[32m+          Epoch: [2][2000/2851] Elapsed 16m 56s (remain 7m 11s) Loss: 0.0067(0.0066) Grad: 17567.6758  LR: 0.000015  
[32m+          Epoch: [2][2100/2851] Elapsed 17m 46s (remain 6m 20s) Loss: 0.0000(0.0064) Grad: 5.9588  LR: 0.000015  
[32m+          Epoch: [2][2200/2851] Elapsed 18m 37s (remain 5m 29s) Loss: 0.0018(0.0064) Grad: 6099.8071  LR: 0.000014  
[32m+          Epoch: [2][2300/2851] Elapsed 19m 27s (remain 4m 39s) Loss: 0.0027(0.0063) Grad: 6536.1191  LR: 0.000014  
[32m+          Epoch: [2][2400/2851] Elapsed 20m 17s (remain 3m 48s) Loss: 0.0000(0.0063) Grad: 41.7514  LR: 0.000014  
[32m+          Epoch: [2][2500/2851] Elapsed 21m 10s (remain 2m 57s) Loss: 0.0053(0.0064) Grad: 17725.9844  LR: 0.000014  
[32m+          Epoch: [2][2600/2851] Elapsed 22m 1s (remain 2m 6s) Loss: 0.0013(0.0064) Grad: 3329.3628  LR: 0.000014  
[32m+          Epoch: [2][2700/2851] Elapsed 22m 51s (remain 1m 16s) Loss: 0.0001(0.0064) Grad: 313.5630  LR: 0.000014  
[32m+          Epoch: [2][2800/2851] Elapsed 23m 41s (remain 0m 25s) Loss: 0.0008(0.0064) Grad: 2214.9656  LR: 0.000013  
[32m+          Epoch: [2][2850/2851] Elapsed 24m 6s (remain 0m 0s) Loss: 0.0000(0.0064) Grad: 18.5353  LR: 0.000013  
[32m+          EVAL: [0/724] Elapsed 0m 0s (remain 6m 59s) Loss: 0.0011(0.0011) 
[32m+          EVAL: [100/724] Elapsed 0m 28s (remain 2m 57s) Loss: 0.0025(0.0064) 
[32m+          EVAL: [200/724] Elapsed 0m 56s (remain 2m 28s) Loss: 0.0000(0.0081) 
[32m+          EVAL: [300/724] Elapsed 1m 24s (remain 1m 58s) Loss: 0.0002(0.0078) 
[32m+          EVAL: [400/724] Elapsed 1m 52s (remain 1m 30s) Loss: 0.0000(0.0076) 
[32m+          EVAL: [500/724] Elapsed 2m 19s (remain 1m 2s) Loss: 0.0122(0.0090) 
[32m+          EVAL: [600/724] Elapsed 2m 47s (remain 0m 34s) Loss: 0.0020(0.0086) 
[32m+          EVAL: [700/724] Elapsed 3m 14s (remain 0m 6s) Loss: 0.0000(0.0079) 
[32m+          EVAL: [723/724] Elapsed 3m 20s (remain 0m 0s) Loss: 0.0001(0.0078) 
[32m+          Epoch 2 - avg_train_loss: 0.0064  avg_val_loss: 0.0078  time: 1653s
[32m+          Epoch 2 - Score: 0.8694
[32m+          Epoch 2 - Save Best Score: 0.8694 Model
[32m+          Epoch: [3][0/2851] Elapsed 0m 0s (remain 36m 25s) Loss: 0.0001(0.0001) Grad: 1597.1453  LR: 0.000013  
[32m+          Epoch: [3][100/2851] Elapsed 0m 51s (remain 23m 22s) Loss: 0.0060(0.0048) Grad: 12846.5342  LR: 0.000013  
[32m+          Epoch: [3][200/2851] Elapsed 1m 42s (remain 22m 25s) Loss: 0.0009(0.0052) Grad: 4008.9458  LR: 0.000013  
[32m+          Epoch: [3][300/2851] Elapsed 2m 32s (remain 21m 31s) Loss: 0.0013(0.0054) Grad: 13390.3848  LR: 0.000013  
[32m+          Epoch: [3][400/2851] Elapsed 3m 22s (remain 20m 38s) Loss: 0.0000(0.0055) Grad: 11.1291  LR: 0.000013  
[32m+          Epoch: [3][500/2851] Elapsed 4m 14s (remain 19m 52s) Loss: 0.0005(0.0052) Grad: 1784.6052  LR: 0.000013  
[32m+          Epoch: [3][600/2851] Elapsed 5m 6s (remain 19m 8s) Loss: 0.0002(0.0050) Grad: 2755.0193  LR: 0.000012  
[32m+          Epoch: [3][700/2851] Elapsed 5m 57s (remain 18m 17s) Loss: 0.0000(0.0048) Grad: 403.3839  LR: 0.000012  
[32m+          Epoch: [3][800/2851] Elapsed 6m 47s (remain 17m 24s) Loss: 0.0000(0.0051) Grad: 51.5807  LR: 0.000012  
[32m+          Epoch: [3][900/2851] Elapsed 7m 38s (remain 16m 32s) Loss: 0.0000(0.0050) Grad: 34.0727  LR: 0.000012  
[32m+          Epoch: [3][1000/2851] Elapsed 8m 28s (remain 15m 40s) Loss: 0.0046(0.0049) Grad: 8113.7388  LR: 0.000012  
[32m+          Epoch: [3][1100/2851] Elapsed 9m 19s (remain 14m 48s) Loss: 0.0000(0.0050) Grad: 202.0167  LR: 0.000012  
[32m+          Epoch: [3][1200/2851] Elapsed 10m 9s (remain 13m 57s) Loss: 0.0000(0.0050) Grad: 13.7840  LR: 0.000011  
[32m+          Epoch: [3][1300/2851] Elapsed 11m 0s (remain 13m 6s) Loss: 0.0002(0.0050) Grad: 739.0372  LR: 0.000011  
[32m+          Epoch: [3][1400/2851] Elapsed 11m 51s (remain 12m 16s) Loss: 0.0109(0.0053) Grad: 13362.2988  LR: 0.000011  
[32m+          Epoch: [3][1500/2851] Elapsed 12m 41s (remain 11m 25s) Loss: 0.0049(0.0052) Grad: 5323.9927  LR: 0.000011  
[32m+          Epoch: [3][1600/2851] Elapsed 13m 31s (remain 10m 33s) Loss: 0.0000(0.0052) Grad: 140.4794  LR: 0.000011  
[32m+          Epoch: [3][1700/2851] Elapsed 14m 22s (remain 9m 43s) Loss: 0.0029(0.0052) Grad: 5037.5781  LR: 0.000011  
[32m+          Epoch: [3][1800/2851] Elapsed 15m 13s (remain 8m 52s) Loss: 0.0003(0.0052) Grad: 910.8983  LR: 0.000011  
[32m+          Epoch: [3][1900/2851] Elapsed 16m 3s (remain 8m 1s) Loss: 0.0268(0.0052) Grad: 28267.5879  LR: 0.000010  
[32m+          Epoch: [3][2000/2851] Elapsed 16m 54s (remain 7m 10s) Loss: 0.0002(0.0052) Grad: 624.1973  LR: 0.000010  
[32m+          Epoch: [3][2100/2851] Elapsed 17m 45s (remain 6m 20s) Loss: 0.0007(0.0052) Grad: 1402.9733  LR: 0.000010  
[32m+          Epoch: [3][2200/2851] Elapsed 18m 35s (remain 5m 29s) Loss: 0.0046(0.0052) Grad: 4654.6724  LR: 0.000010  
[32m+          Epoch: [3][2300/2851] Elapsed 19m 26s (remain 4m 38s) Loss: 0.0292(0.0051) Grad: 15358.8242  LR: 0.000010  
[32m+          Epoch: [3][2400/2851] Elapsed 20m 16s (remain 3m 47s) Loss: 0.0121(0.0052) Grad: 9499.5898  LR: 0.000010  
[32m+          Epoch: [3][2500/2851] Elapsed 21m 6s (remain 2m 57s) Loss: 0.0040(0.0051) Grad: 7021.1494  LR: 0.000009  
[32m+          Epoch: [3][2600/2851] Elapsed 21m 56s (remain 2m 6s) Loss: 0.0013(0.0050) Grad: 3909.9819  LR: 0.000009  
[32m+          Epoch: [3][2700/2851] Elapsed 22m 47s (remain 1m 15s) Loss: 0.0113(0.0050) Grad: 13237.9160  LR: 0.000009  
[32m+          Epoch: [3][2800/2851] Elapsed 23m 37s (remain 0m 25s) Loss: 0.0091(0.0049) Grad: 25642.4980  LR: 0.000009  
[32m+          Epoch: [3][2850/2851] Elapsed 24m 3s (remain 0m 0s) Loss: 0.0034(0.0050) Grad: 5090.4141  LR: 0.000009  
[32m+          EVAL: [0/724] Elapsed 0m 0s (remain 6m 39s) Loss: 0.0012(0.0012) 
[32m+          EVAL: [100/724] Elapsed 0m 28s (remain 2m 54s) Loss: 0.0047(0.0073) 
[32m+          EVAL: [200/724] Elapsed 0m 56s (remain 2m 26s) Loss: 0.0000(0.0084) 
[32m+          EVAL: [300/724] Elapsed 1m 23s (remain 1m 57s) Loss: 0.0002(0.0080) 
[32m+          EVAL: [400/724] Elapsed 1m 51s (remain 1m 29s) Loss: 0.0000(0.0077) 
[32m+          EVAL: [500/724] Elapsed 2m 20s (remain 1m 2s) Loss: 0.0109(0.0092) 
[32m+          EVAL: [600/724] Elapsed 2m 49s (remain 0m 34s) Loss: 0.0038(0.0088) 
[32m+          EVAL: [700/724] Elapsed 3m 16s (remain 0m 6s) Loss: 0.0000(0.0080) 
[32m+          EVAL: [723/724] Elapsed 3m 22s (remain 0m 0s) Loss: 0.0000(0.0079) 
[32m+          Epoch 3 - avg_train_loss: 0.0050  avg_val_loss: 0.0079  time: 1651s
[32m+          Epoch 3 - Score: 0.8815
[32m+          Epoch 3 - Save Best Score: 0.8815 Model
[32m+          Epoch: [4][0/2851] Elapsed 0m 0s (remain 40m 30s) Loss: 0.0005(0.0005) Grad: 2572.2542  LR: 0.000009  
[32m+          Epoch: [4][100/2851] Elapsed 0m 51s (remain 23m 12s) Loss: 0.0008(0.0036) Grad: 2288.5671  LR: 0.000009  
[32m+          Epoch: [4][200/2851] Elapsed 1m 41s (remain 22m 19s) Loss: 0.0059(0.0036) Grad: 11989.1855  LR: 0.000009  
[32m+          Epoch: [4][300/2851] Elapsed 2m 31s (remain 21m 27s) Loss: 0.0094(0.0044) Grad: 50097.4062  LR: 0.000008  
[32m+          Epoch: [4][400/2851] Elapsed 3m 23s (remain 20m 40s) Loss: 0.0160(0.0042) Grad: 20703.5078  LR: 0.000008  
[32m+          Epoch: [4][500/2851] Elapsed 4m 13s (remain 19m 49s) Loss: 0.0012(0.0044) Grad: 16409.6836  LR: 0.000008  
[32m+          Epoch: [4][600/2851] Elapsed 5m 3s (remain 18m 57s) Loss: 0.0019(0.0042) Grad: 9057.7217  LR: 0.000008  
[32m+          Epoch: [4][700/2851] Elapsed 5m 54s (remain 18m 8s) Loss: 0.0000(0.0044) Grad: 166.1057  LR: 0.000008  
[32m+          Epoch: [4][800/2851] Elapsed 6m 45s (remain 17m 18s) Loss: 0.0000(0.0042) Grad: 22.9834  LR: 0.000008  
[32m+          Epoch: [4][900/2851] Elapsed 7m 36s (remain 16m 27s) Loss: 0.0166(0.0041) Grad: 31913.6758  LR: 0.000007  
[32m+          Epoch: [4][1000/2851] Elapsed 8m 26s (remain 15m 35s) Loss: 0.0011(0.0043) Grad: 4219.4009  LR: 0.000007  
[32m+          Epoch: [4][1100/2851] Elapsed 9m 17s (remain 14m 46s) Loss: 0.0710(0.0043) Grad: 62166.7070  LR: 0.000007  
[32m+          Epoch: [4][1200/2851] Elapsed 10m 8s (remain 13m 56s) Loss: 0.0004(0.0043) Grad: 608.5483  LR: 0.000007  
[32m+          Epoch: [4][1300/2851] Elapsed 10m 58s (remain 13m 5s) Loss: 0.0027(0.0044) Grad: 5141.6489  LR: 0.000007  
[32m+          Epoch: [4][1400/2851] Elapsed 11m 49s (remain 12m 13s) Loss: 0.0014(0.0043) Grad: 3187.7854  LR: 0.000007  
[32m+          Epoch: [4][1500/2851] Elapsed 12m 39s (remain 11m 23s) Loss: 0.0000(0.0044) Grad: 16.2476  LR: 0.000007  
[32m+          Epoch: [4][1600/2851] Elapsed 13m 30s (remain 10m 32s) Loss: 0.0001(0.0043) Grad: 542.5617  LR: 0.000006  
[32m+          Epoch: [4][1700/2851] Elapsed 14m 22s (remain 9m 42s) Loss: 0.0000(0.0043) Grad: 190.0896  LR: 0.000006  
[32m+          Epoch: [4][1800/2851] Elapsed 15m 13s (remain 8m 52s) Loss: 0.0050(0.0042) Grad: 5550.5112  LR: 0.000006  
[32m+          Epoch: [4][1900/2851] Elapsed 16m 3s (remain 8m 1s) Loss: 0.0013(0.0044) Grad: 1856.1926  LR: 0.000006  
[32m+          Epoch: [4][2000/2851] Elapsed 16m 53s (remain 7m 10s) Loss: 0.0105(0.0043) Grad: 11378.7588  LR: 0.000006  
[32m+          Epoch: [4][2100/2851] Elapsed 17m 44s (remain 6m 19s) Loss: 0.0010(0.0043) Grad: 8675.0068  LR: 0.000006  
[32m+          Epoch: [4][2200/2851] Elapsed 18m 35s (remain 5m 29s) Loss: 0.0003(0.0042) Grad: 774.3781  LR: 0.000005  
[32m+          Epoch: [4][2300/2851] Elapsed 19m 27s (remain 4m 38s) Loss: 0.0001(0.0042) Grad: 114.5096  LR: 0.000005  
[32m+          Epoch: [4][2400/2851] Elapsed 20m 17s (remain 3m 48s) Loss: 0.0309(0.0041) Grad: 14782.2783  LR: 0.000005  
[32m+          Epoch: [4][2500/2851] Elapsed 21m 7s (remain 2m 57s) Loss: 0.0053(0.0041) Grad: 4317.2783  LR: 0.000005  
[32m+          Epoch: [4][2600/2851] Elapsed 21m 58s (remain 2m 6s) Loss: 0.0022(0.0041) Grad: 7185.4521  LR: 0.000005  
[32m+          Epoch: [4][2700/2851] Elapsed 22m 50s (remain 1m 16s) Loss: 0.0049(0.0041) Grad: 3779.1909  LR: 0.000005  
[32m+          Epoch: [4][2800/2851] Elapsed 23m 40s (remain 0m 25s) Loss: 0.0317(0.0040) Grad: 12976.2773  LR: 0.000005  
[32m+          Epoch: [4][2850/2851] Elapsed 24m 5s (remain 0m 0s) Loss: 0.0000(0.0041) Grad: 48.6180  LR: 0.000004  
[32m+          EVAL: [0/724] Elapsed 0m 0s (remain 7m 10s) Loss: 0.0006(0.0006) 
[32m+          EVAL: [100/724] Elapsed 0m 28s (remain 2m 55s) Loss: 0.0033(0.0076) 
[32m+          EVAL: [200/724] Elapsed 0m 56s (remain 2m 27s) Loss: 0.0000(0.0091) 
[32m+          EVAL: [300/724] Elapsed 1m 24s (remain 1m 58s) Loss: 0.0003(0.0088) 
[32m+          EVAL: [400/724] Elapsed 1m 52s (remain 1m 30s) Loss: 0.0000(0.0085) 
[32m+          EVAL: [500/724] Elapsed 2m 19s (remain 1m 2s) Loss: 0.0099(0.0101) 
[32m+          EVAL: [600/724] Elapsed 2m 48s (remain 0m 34s) Loss: 0.0028(0.0098) 
[32m+          EVAL: [700/724] Elapsed 3m 15s (remain 0m 6s) Loss: 0.0000(0.0089) 
[32m+          EVAL: [723/724] Elapsed 3m 21s (remain 0m 0s) Loss: 0.0000(0.0088) 
[32m+          Epoch 4 - avg_train_loss: 0.0041  avg_val_loss: 0.0088  time: 1652s
[32m+          Epoch 4 - Score: 0.8827
[32m+          Epoch 4 - Save Best Score: 0.8827 Model
[32m+          Epoch: [5][0/2851] Elapsed 0m 0s (remain 38m 24s) Loss: 0.0221(0.0221) Grad: 47884.9023  LR: 0.000004  
[32m+          Epoch: [5][100/2851] Elapsed 0m 51s (remain 23m 25s) Loss: 0.0039(0.0032) Grad: 32674.3828  LR: 0.000004  
[32m+          Epoch: [5][200/2851] Elapsed 1m 42s (remain 22m 26s) Loss: 0.0004(0.0029) Grad: 4350.4370  LR: 0.000004  
[32m+          Epoch: [5][300/2851] Elapsed 2m 32s (remain 21m 29s) Loss: 0.0000(0.0028) Grad: 149.8888  LR: 0.000004  
[32m+          Epoch: [5][400/2851] Elapsed 3m 22s (remain 20m 35s) Loss: 0.0146(0.0029) Grad: 19909.8613  LR: 0.000004  
[32m+          Epoch: [5][500/2851] Elapsed 4m 12s (remain 19m 43s) Loss: 0.0032(0.0029) Grad: 11601.5713  LR: 0.000004  
[32m+          Epoch: [5][600/2851] Elapsed 5m 2s (remain 18m 53s) Loss: 0.0001(0.0031) Grad: 593.8757  LR: 0.000004  
[32m+          Epoch: [5][700/2851] Elapsed 5m 53s (remain 18m 5s) Loss: 0.0161(0.0031) Grad: 28718.7871  LR: 0.000003  
[32m+          Epoch: [5][800/2851] Elapsed 6m 45s (remain 17m 17s) Loss: 0.0053(0.0031) Grad: 8800.4756  LR: 0.000003  
[32m+          Epoch: [5][900/2851] Elapsed 7m 35s (remain 16m 26s) Loss: 0.0010(0.0031) Grad: 4999.9932  LR: 0.000003  
[32m+          Epoch: [5][1000/2851] Elapsed 8m 25s (remain 15m 34s) Loss: 0.0005(0.0030) Grad: 3724.5991  LR: 0.000003  
[32m+          Epoch: [5][1100/2851] Elapsed 9m 15s (remain 14m 43s) Loss: 0.0056(0.0031) Grad: 19455.0586  LR: 0.000003  
[32m+          Epoch: [5][1200/2851] Elapsed 10m 6s (remain 13m 53s) Loss: 0.0021(0.0031) Grad: 7020.3262  LR: 0.000003  
[32m+          Epoch: [5][1300/2851] Elapsed 10m 57s (remain 13m 3s) Loss: 0.0000(0.0031) Grad: 63.0700  LR: 0.000002  
[32m+          Epoch: [5][1400/2851] Elapsed 11m 47s (remain 12m 12s) Loss: 0.0000(0.0031) Grad: 30.1744  LR: 0.000002  
[32m+          Epoch: [5][1500/2851] Elapsed 12m 38s (remain 11m 22s) Loss: 0.0000(0.0032) Grad: 173.2499  LR: 0.000002  
[32m+          Epoch: [5][1600/2851] Elapsed 13m 28s (remain 10m 31s) Loss: 0.0109(0.0032) Grad: 42712.0664  LR: 0.000002  
[32m+          Epoch: [5][1700/2851] Elapsed 14m 19s (remain 9m 41s) Loss: 0.0000(0.0032) Grad: 72.5420  LR: 0.000002  
[32m+          Epoch: [5][1800/2851] Elapsed 15m 10s (remain 8m 50s) Loss: 0.0000(0.0031) Grad: 31.5939  LR: 0.000002  
[32m+          Epoch: [5][1900/2851] Elapsed 16m 1s (remain 8m 0s) Loss: 0.0000(0.0032) Grad: 7.6905  LR: 0.000001  
[32m+          Epoch: [5][2000/2851] Elapsed 16m 51s (remain 7m 9s) Loss: 0.0001(0.0032) Grad: 966.7061  LR: 0.000001  
[32m+          Epoch: [5][2100/2851] Elapsed 17m 41s (remain 6m 19s) Loss: 0.0064(0.0032) Grad: 13028.7227  LR: 0.000001  
[32m+          Epoch: [5][2200/2851] Elapsed 18m 32s (remain 5m 28s) Loss: 0.0000(0.0032) Grad: 21.4458  LR: 0.000001  
[32m+          Epoch: [5][2300/2851] Elapsed 19m 23s (remain 4m 38s) Loss: 0.0059(0.0032) Grad: 18736.0938  LR: 0.000001  
[32m+          Epoch: [5][2400/2851] Elapsed 20m 13s (remain 3m 47s) Loss: 0.0001(0.0033) Grad: 850.0435  LR: 0.000001  
[32m+          Epoch: [5][2500/2851] Elapsed 21m 4s (remain 2m 56s) Loss: 0.0534(0.0033) Grad: 27415.8652  LR: 0.000001  
[32m+          Epoch: [5][2600/2851] Elapsed 21m 55s (remain 2m 6s) Loss: 0.0008(0.0033) Grad: 4680.2803  LR: 0.000000  
[32m+          Epoch: [5][2700/2851] Elapsed 22m 45s (remain 1m 15s) Loss: 0.0001(0.0033) Grad: 238.4537  LR: 0.000000  
[32m+          Epoch: [5][2800/2851] Elapsed 23m 36s (remain 0m 25s) Loss: 0.0000(0.0032) Grad: 105.2971  LR: 0.000000  
[32m+          Epoch: [5][2850/2851] Elapsed 24m 2s (remain 0m 0s) Loss: 0.0009(0.0032) Grad: 2238.9463  LR: 0.000000  
[32m+          EVAL: [0/724] Elapsed 0m 0s (remain 7m 16s) Loss: 0.0006(0.0006) 
[32m+          EVAL: [100/724] Elapsed 0m 28s (remain 2m 58s) Loss: 0.0036(0.0080) 
[32m+          EVAL: [200/724] Elapsed 0m 57s (remain 2m 29s) Loss: 0.0000(0.0096) 
[32m+          EVAL: [300/724] Elapsed 1m 24s (remain 1m 59s) Loss: 0.0006(0.0094) 
[32m+          EVAL: [400/724] Elapsed 1m 52s (remain 1m 30s) Loss: 0.0000(0.0094) 
[32m+          EVAL: [500/724] Elapsed 2m 20s (remain 1m 2s) Loss: 0.0135(0.0115) 
[32m+          EVAL: [600/724] Elapsed 2m 49s (remain 0m 34s) Loss: 0.0016(0.0111) 
[32m+          EVAL: [700/724] Elapsed 3m 16s (remain 0m 6s) Loss: 0.0000(0.0101) 
[32m+          EVAL: [723/724] Elapsed 3m 22s (remain 0m 0s) Loss: 0.0000(0.0100) 
[32m+          Epoch 5 - avg_train_loss: 0.0032  avg_val_loss: 0.0100  time: 1650s
[32m+          Epoch 5 - Score: 0.8856
[32m+          Epoch 5 - Save Best Score: 0.8856 Model
[32m+          ========== fold: 2 training ==========
[32m+          Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin
[32m+          Epoch: [1][0/2871] Elapsed 0m 0s (remain 40m 54s) Loss: 0.3458(0.3458) Grad: 204017.2969  LR: 0.000000  
[32m+          Epoch: [1][100/2871] Elapsed 0m 51s (remain 23m 21s) Loss: 0.2916(0.4959) Grad: 12092.4521  LR: 0.000001  
[32m+          Epoch: [1][200/2871] Elapsed 1m 41s (remain 22m 24s) Loss: 0.0691(0.3467) Grad: 4892.7920  LR: 0.000003  
[32m+          Epoch: [1][300/2871] Elapsed 2m 31s (remain 21m 33s) Loss: 0.0324(0.2452) Grad: 442.7823  LR: 0.000004  
[32m+          Epoch: [1][400/2871] Elapsed 3m 21s (remain 20m 41s) Loss: 0.0412(0.1932) Grad: 763.4689  LR: 0.000006  
[32m+          Epoch: [1][500/2871] Elapsed 4m 11s (remain 19m 51s) Loss: 0.0234(0.1626) Grad: 1279.8525  LR: 0.000007  
[32m+          Epoch: [1][600/2871] Elapsed 5m 1s (remain 18m 58s) Loss: 0.0251(0.1405) Grad: 1186.7655  LR: 0.000008  
[32m+          Epoch: [1][700/2871] Elapsed 5m 51s (remain 18m 8s) Loss: 0.0121(0.1231) Grad: 1494.7047  LR: 0.000010  
[32m+          Epoch: [1][800/2871] Elapsed 6m 41s (remain 17m 18s) Loss: 0.0103(0.1096) Grad: 602.7847  LR: 0.000011  
[32m+          Epoch: [1][900/2871] Elapsed 7m 31s (remain 16m 27s) Loss: 0.0116(0.0987) Grad: 1297.7050  LR: 0.000013  
[32m+          Epoch: [1][1000/2871] Elapsed 8m 21s (remain 15m 37s) Loss: 0.0095(0.0903) Grad: 2481.4092  LR: 0.000014  
[32m+          Epoch: [1][1100/2871] Elapsed 9m 12s (remain 14m 47s) Loss: 0.0137(0.0832) Grad: 1565.5156  LR: 0.000015  
[32m+          Epoch: [1][1200/2871] Elapsed 10m 2s (remain 13m 57s) Loss: 0.0063(0.0770) Grad: 1362.4957  LR: 0.000017  
[32m+          Epoch: [1][1300/2871] Elapsed 10m 52s (remain 13m 7s) Loss: 0.0060(0.0718) Grad: 762.4417  LR: 0.000018  
[32m+          Epoch: [1][1400/2871] Elapsed 11m 41s (remain 12m 16s) Loss: 0.0032(0.0675) Grad: 471.4208  LR: 0.000020  
[32m+          Epoch: [1][1500/2871] Elapsed 12m 32s (remain 11m 26s) Loss: 0.0080(0.0637) Grad: 749.4076  LR: 0.000020  
[32m+          Epoch: [1][1600/2871] Elapsed 13m 21s (remain 10m 35s) Loss: 0.0418(0.0603) Grad: 4767.1172  LR: 0.000020  
[32m+          Epoch: [1][1700/2871] Elapsed 14m 11s (remain 9m 45s) Loss: 0.0128(0.0573) Grad: 1187.6406  LR: 0.000020  
[32m+          Epoch: [1][1800/2871] Elapsed 15m 1s (remain 8m 55s) Loss: 0.0097(0.0546) Grad: 859.0907  LR: 0.000019  
[32m+          Epoch: [1][1900/2871] Elapsed 15m 51s (remain 8m 5s) Loss: 0.0035(0.0522) Grad: 382.2367  LR: 0.000019  
[32m+          Epoch: [1][2000/2871] Elapsed 16m 40s (remain 7m 15s) Loss: 0.0041(0.0500) Grad: 637.7184  LR: 0.000019  
[32m+          Epoch: [1][2100/2871] Elapsed 17m 30s (remain 6m 24s) Loss: 0.0025(0.0479) Grad: 583.0992  LR: 0.000019  
[32m+          Epoch: [1][2200/2871] Elapsed 18m 20s (remain 5m 34s) Loss: 0.0020(0.0462) Grad: 416.8811  LR: 0.000019  
[32m+          Epoch: [1][2300/2871] Elapsed 19m 10s (remain 4m 44s) Loss: 0.0022(0.0446) Grad: 834.4047  LR: 0.000019  
[32m+          Epoch: [1][2400/2871] Elapsed 19m 59s (remain 3m 54s) Loss: 0.0041(0.0431) Grad: 599.3624  LR: 0.000019  
[32m+          Epoch: [1][2500/2871] Elapsed 20m 48s (remain 3m 4s) Loss: 0.0021(0.0416) Grad: 517.1616  LR: 0.000018  
[32m+          Epoch: [1][2600/2871] Elapsed 21m 39s (remain 2m 14s) Loss: 0.0032(0.0403) Grad: 688.7821  LR: 0.000018  
[32m+          Epoch: [1][2700/2871] Elapsed 22m 29s (remain 1m 24s) Loss: 0.0010(0.0391) Grad: 117.5020  LR: 0.000018  
[32m+          Epoch: [1][2800/2871] Elapsed 23m 18s (remain 0m 34s) Loss: 0.0183(0.0380) Grad: 1686.4248  LR: 0.000018  
[32m+          Epoch: [1][2870/2871] Elapsed 23m 53s (remain 0m 0s) Loss: 0.0009(0.0373) Grad: 126.6934  LR: 0.000018  
[32m+          EVAL: [0/704] Elapsed 0m 0s (remain 6m 19s) Loss: 0.0024(0.0024) 
[32m+          EVAL: [100/704] Elapsed 0m 27s (remain 2m 47s) Loss: 0.0044(0.0072) 
[32m+          EVAL: [200/704] Elapsed 0m 56s (remain 2m 20s) Loss: 0.0001(0.0063) 
[32m+          EVAL: [300/704] Elapsed 1m 23s (remain 1m 51s) Loss: 0.0004(0.0062) 
[32m+          EVAL: [400/704] Elapsed 1m 50s (remain 1m 23s) Loss: 0.0103(0.0067) 
[32m+          EVAL: [500/704] Elapsed 2m 18s (remain 0m 56s) Loss: 0.0075(0.0072) 
[32m+          EVAL: [600/704] Elapsed 2m 46s (remain 0m 28s) Loss: 0.0004(0.0074) 
[32m+          EVAL: [700/704] Elapsed 3m 13s (remain 0m 0s) Loss: 0.0001(0.0071) 
[32m+          EVAL: [703/704] Elapsed 3m 14s (remain 0m 0s) Loss: 0.0000(0.0070) 
[32m+          Epoch 1 - avg_train_loss: 0.0373  avg_val_loss: 0.0070  time: 1633s
[32m+          Epoch 1 - Score: 0.8445
[32m+          Epoch 1 - Save Best Score: 0.8445 Model
[32m+          Epoch: [2][0/2871] Elapsed 0m 0s (remain 36m 11s) Loss: 0.0004(0.0004) Grad: 1206.9116  LR: 0.000018  
[32m+          Epoch: [2][100/2871] Elapsed 0m 50s (remain 23m 16s) Loss: 0.0049(0.0040) Grad: 7509.4180  LR: 0.000018  
[32m+          Epoch: [2][200/2871] Elapsed 1m 41s (remain 22m 29s) Loss: 0.0321(0.0049) Grad: 75782.8203  LR: 0.000017  
[32m+          Epoch: [2][300/2871] Elapsed 2m 32s (remain 21m 43s) Loss: 0.0028(0.0053) Grad: 3337.6436  LR: 0.000017  
[32m+          Epoch: [2][400/2871] Elapsed 3m 23s (remain 20m 54s) Loss: 0.0026(0.0053) Grad: 5143.5430  LR: 0.000017  
[32m+          Epoch: [2][500/2871] Elapsed 4m 14s (remain 20m 3s) Loss: 0.0038(0.0056) Grad: 5551.0630  LR: 0.000017  
[32m+          Epoch: [2][600/2871] Elapsed 5m 5s (remain 19m 13s) Loss: 0.0011(0.0055) Grad: 4326.1685  LR: 0.000017  
[32m+          Epoch: [2][700/2871] Elapsed 5m 56s (remain 18m 23s) Loss: 0.0013(0.0054) Grad: 3336.9136  LR: 0.000017  
[32m+          Epoch: [2][800/2871] Elapsed 6m 47s (remain 17m 32s) Loss: 0.0007(0.0054) Grad: 2389.9539  LR: 0.000017  
[32m+          Epoch: [2][900/2871] Elapsed 7m 37s (remain 16m 41s) Loss: 0.0050(0.0054) Grad: 35186.9414  LR: 0.000016  
[32m+          Epoch: [2][1000/2871] Elapsed 8m 28s (remain 15m 50s) Loss: 0.0001(0.0054) Grad: 1472.4436  LR: 0.000016  
[32m+          Epoch: [2][1100/2871] Elapsed 9m 19s (remain 14m 58s) Loss: 0.0078(0.0056) Grad: 9703.6689  LR: 0.000016  
[32m+          Epoch: [2][1200/2871] Elapsed 10m 10s (remain 14m 8s) Loss: 0.0041(0.0055) Grad: 10467.7305  LR: 0.000016  
[32m+          Epoch: [2][1300/2871] Elapsed 11m 1s (remain 13m 18s) Loss: 0.0109(0.0056) Grad: 36560.1523  LR: 0.000016  
[32m+          Epoch: [2][1400/2871] Elapsed 11m 54s (remain 12m 30s) Loss: 0.0054(0.0057) Grad: 5927.6113  LR: 0.000016  
[32m+          Epoch: [2][1500/2871] Elapsed 12m 45s (remain 11m 38s) Loss: 0.0140(0.0056) Grad: 17848.7969  LR: 0.000015  
[32m+          Epoch: [2][1600/2871] Elapsed 13m 36s (remain 10m 47s) Loss: 0.0004(0.0057) Grad: 2505.1995  LR: 0.000015  
[32m+          Epoch: [2][1700/2871] Elapsed 14m 26s (remain 9m 56s) Loss: 0.0366(0.0057) Grad: 24658.6934  LR: 0.000015  
[32m+          Epoch: [2][1800/2871] Elapsed 15m 17s (remain 9m 5s) Loss: 0.0005(0.0057) Grad: 2961.2915  LR: 0.000015  
[32m+          Epoch: [2][1900/2871] Elapsed 16m 9s (remain 8m 14s) Loss: 0.0042(0.0058) Grad: 8849.4502  LR: 0.000015  
[32m+          Epoch: [2][2000/2871] Elapsed 17m 0s (remain 7m 23s) Loss: 0.0106(0.0058) Grad: 13414.5537  LR: 0.000015  
[32m+          Epoch: [2][2100/2871] Elapsed 17m 50s (remain 6m 32s) Loss: 0.0003(0.0058) Grad: 1387.1072  LR: 0.000015  
[32m+          Epoch: [2][2200/2871] Elapsed 18m 41s (remain 5m 41s) Loss: 0.0060(0.0058) Grad: 30900.2285  LR: 0.000014  
[32m+          Epoch: [2][2300/2871] Elapsed 19m 31s (remain 4m 50s) Loss: 0.0554(0.0057) Grad: 75542.8047  LR: 0.000014  
[32m+          Epoch: [2][2400/2871] Elapsed 20m 22s (remain 3m 59s) Loss: 0.0001(0.0058) Grad: 365.1594  LR: 0.000014  
[32m+          Epoch: [2][2500/2871] Elapsed 21m 13s (remain 3m 8s) Loss: 0.0002(0.0059) Grad: 476.9738  LR: 0.000014  
[32m+          Epoch: [2][2600/2871] Elapsed 22m 4s (remain 2m 17s) Loss: 0.0022(0.0059) Grad: 5720.3760  LR: 0.000014  
[32m+          Epoch: [2][2700/2871] Elapsed 22m 54s (remain 1m 26s) Loss: 0.0000(0.0059) Grad: 150.1539  LR: 0.000014  
[32m+          Epoch: [2][2800/2871] Elapsed 23m 44s (remain 0m 35s) Loss: 0.0084(0.0059) Grad: 9511.7852  LR: 0.000013  
[32m+          Epoch: [2][2870/2871] Elapsed 24m 20s (remain 0m 0s) Loss: 0.0000(0.0059) Grad: 39.0048  LR: 0.000013  
[32m+          EVAL: [0/704] Elapsed 0m 0s (remain 7m 15s) Loss: 0.0007(0.0007) 
[32m+          EVAL: [100/704] Elapsed 0m 28s (remain 2m 50s) Loss: 0.0026(0.0084) 
[32m+          EVAL: [200/704] Elapsed 0m 56s (remain 2m 20s) Loss: 0.0000(0.0072) 
[32m+          EVAL: [300/704] Elapsed 1m 23s (remain 1m 52s) Loss: 0.0001(0.0069) 
[32m+          EVAL: [400/704] Elapsed 1m 51s (remain 1m 23s) Loss: 0.0094(0.0077) 
[32m+          EVAL: [500/704] Elapsed 2m 18s (remain 0m 56s) Loss: 0.0062(0.0083) 
[32m+          EVAL: [600/704] Elapsed 2m 47s (remain 0m 28s) Loss: 0.0000(0.0083) 
[32m+          EVAL: [700/704] Elapsed 3m 15s (remain 0m 0s) Loss: 0.0000(0.0078) 
[32m+          EVAL: [703/704] Elapsed 3m 16s (remain 0m 0s) Loss: 0.0000(0.0078) 
[32m+          Epoch 2 - avg_train_loss: 0.0059  avg_val_loss: 0.0078  time: 1662s
[32m+          Epoch 2 - Score: 0.8692
[32m+          Epoch 2 - Save Best Score: 0.8692 Model
[32m+          Epoch: [3][0/2871] Elapsed 0m 0s (remain 38m 56s) Loss: 0.0001(0.0001) Grad: 344.1596  LR: 0.000013  
[32m+          Epoch: [3][100/2871] Elapsed 0m 51s (remain 23m 39s) Loss: 0.0002(0.0035) Grad: 1207.9785  LR: 0.000013  
[32m+          Epoch: [3][200/2871] Elapsed 1m 42s (remain 22m 38s) Loss: 0.0003(0.0045) Grad: 1090.9712  LR: 0.000013  
[32m+          Epoch: [3][300/2871] Elapsed 2m 33s (remain 21m 50s) Loss: 0.0040(0.0045) Grad: 6820.6611  LR: 0.000013  
[32m+          Epoch: [3][400/2871] Elapsed 3m 26s (remain 21m 10s) Loss: 0.0000(0.0043) Grad: 193.3957  LR: 0.000013  
[32m+          Epoch: [3][500/2871] Elapsed 4m 17s (remain 20m 17s) Loss: 0.0001(0.0045) Grad: 236.9936  LR: 0.000013  
[32m+          Epoch: [3][600/2871] Elapsed 5m 8s (remain 19m 23s) Loss: 0.0000(0.0048) Grad: 142.8712  LR: 0.000012  
[32m+          Epoch: [3][700/2871] Elapsed 5m 58s (remain 18m 29s) Loss: 0.0108(0.0048) Grad: 45849.7227  LR: 0.000012  
[32m+          Epoch: [3][800/2871] Elapsed 6m 48s (remain 17m 36s) Loss: 0.0038(0.0046) Grad: 6870.4424  LR: 0.000012  
[32m+          Epoch: [3][900/2871] Elapsed 7m 39s (remain 16m 43s) Loss: 0.0088(0.0045) Grad: 57183.3477  LR: 0.000012  
[32m+          Epoch: [3][1000/2871] Elapsed 8m 29s (remain 15m 51s) Loss: 0.0109(0.0045) Grad: 25748.9570  LR: 0.000012  
[32m+          Epoch: [3][1100/2871] Elapsed 9m 19s (remain 14m 59s) Loss: 0.0000(0.0048) Grad: 82.3939  LR: 0.000012  
[32m+          Epoch: [3][1200/2871] Elapsed 10m 10s (remain 14m 9s) Loss: 0.0032(0.0047) Grad: 8581.9678  LR: 0.000011  
[32m+          Epoch: [3][1300/2871] Elapsed 11m 2s (remain 13m 19s) Loss: 0.0002(0.0047) Grad: 804.2525  LR: 0.000011  
[32m+          Epoch: [3][1400/2871] Elapsed 11m 52s (remain 12m 27s) Loss: 0.0070(0.0047) Grad: 28220.2383  LR: 0.000011  
[32m+          Epoch: [3][1500/2871] Elapsed 12m 43s (remain 11m 36s) Loss: 0.0058(0.0047) Grad: 14615.5938  LR: 0.000011  
[32m+          Epoch: [3][1600/2871] Elapsed 13m 35s (remain 10m 47s) Loss: 0.0020(0.0048) Grad: 5663.7529  LR: 0.000011  
[32m+          Epoch: [3][1700/2871] Elapsed 14m 26s (remain 9m 56s) Loss: 0.0033(0.0048) Grad: 60878.4453  LR: 0.000011  
[32m+          Epoch: [3][1800/2871] Elapsed 15m 16s (remain 9m 4s) Loss: 0.0001(0.0047) Grad: 320.3818  LR: 0.000011  
[32m+          Epoch: [3][1900/2871] Elapsed 16m 6s (remain 8m 13s) Loss: 0.0000(0.0047) Grad: 328.9787  LR: 0.000010  
[32m+          Epoch: [3][2000/2871] Elapsed 16m 56s (remain 7m 22s) Loss: 0.0002(0.0048) Grad: 1146.8951  LR: 0.000010  
[32m+          Epoch: [3][2100/2871] Elapsed 17m 47s (remain 6m 31s) Loss: 0.0074(0.0048) Grad: 32689.5371  LR: 0.000010  
[32m+          Epoch: [3][2200/2871] Elapsed 18m 40s (remain 5m 41s) Loss: 0.0019(0.0047) Grad: 5206.1538  LR: 0.000010  
[32m+          Epoch: [3][2300/2871] Elapsed 19m 33s (remain 4m 50s) Loss: 0.0000(0.0048) Grad: 100.6425  LR: 0.000010  
[32m+          Epoch: [3][2400/2871] Elapsed 20m 24s (remain 3m 59s) Loss: 0.0001(0.0048) Grad: 200.3809  LR: 0.000010  
[32m+          Epoch: [3][2500/2871] Elapsed 21m 14s (remain 3m 8s) Loss: 0.0123(0.0047) Grad: 16338.1191  LR: 0.000009  
[32m+          Epoch: [3][2600/2871] Elapsed 22m 4s (remain 2m 17s) Loss: 0.0005(0.0047) Grad: 3039.4075  LR: 0.000009  
[32m+          Epoch: [3][2700/2871] Elapsed 22m 55s (remain 1m 26s) Loss: 0.0072(0.0049) Grad: 11157.1270  LR: 0.000009  
[32m+          Epoch: [3][2800/2871] Elapsed 23m 46s (remain 0m 35s) Loss: 0.0000(0.0049) Grad: 123.3228  LR: 0.000009  
[32m+          Epoch: [3][2870/2871] Elapsed 24m 22s (remain 0m 0s) Loss: 0.0001(0.0049) Grad: 386.4916  LR: 0.000009  
[32m+          EVAL: [0/704] Elapsed 0m 0s (remain 7m 16s) Loss: 0.0007(0.0007) 
[32m+          EVAL: [100/704] Elapsed 0m 28s (remain 2m 47s) Loss: 0.0036(0.0080) 
[32m+          EVAL: [200/704] Elapsed 0m 55s (remain 2m 19s) Loss: 0.0000(0.0069) 
[32m+          EVAL: [300/704] Elapsed 1m 23s (remain 1m 51s) Loss: 0.0002(0.0065) 
[32m+          EVAL: [400/704] Elapsed 1m 51s (remain 1m 24s) Loss: 0.0090(0.0075) 
[32m+          EVAL: [500/704] Elapsed 2m 19s (remain 0m 56s) Loss: 0.0068(0.0083) 
[32m+          EVAL: [600/704] Elapsed 2m 48s (remain 0m 28s) Loss: 0.0001(0.0085) 
[32m+          EVAL: [700/704] Elapsed 3m 15s (remain 0m 0s) Loss: 0.0000(0.0080) 
[32m+          EVAL: [703/704] Elapsed 3m 16s (remain 0m 0s) Loss: 0.0000(0.0079) 
[32m+          Epoch 3 - avg_train_loss: 0.0049  avg_val_loss: 0.0079  time: 1664s
[32m+          Epoch 3 - Score: 0.8720
[32m+          Epoch 3 - Save Best Score: 0.8720 Model
[32m+          Epoch: [4][0/2871] Elapsed 0m 0s (remain 37m 49s) Loss: 0.0086(0.0086) Grad: 10954.8730  LR: 0.000009  
[32m+          Epoch: [4][100/2871] Elapsed 0m 50s (remain 23m 18s) Loss: 0.0000(0.0030) Grad: 40.8028  LR: 0.000009  
[32m+          Epoch: [4][200/2871] Elapsed 1m 41s (remain 22m 26s) Loss: 0.0000(0.0038) Grad: 49.1368  LR: 0.000009  
[32m+          Epoch: [4][300/2871] Elapsed 2m 32s (remain 21m 43s) Loss: 0.0001(0.0036) Grad: 583.5991  LR: 0.000008  
[32m+          Epoch: [4][400/2871] Elapsed 3m 23s (remain 20m 52s) Loss: 0.0042(0.0043) Grad: 39298.3008  LR: 0.000008  
[32m+          Epoch: [4][500/2871] Elapsed 4m 13s (remain 19m 59s) Loss: 0.0004(0.0043) Grad: 2873.6125  LR: 0.000008  
[32m+          Epoch: [4][600/2871] Elapsed 5m 4s (remain 19m 9s) Loss: 0.0097(0.0045) Grad: 27754.3652  LR: 0.000008  
[32m+          Epoch: [4][700/2871] Elapsed 5m 55s (remain 18m 19s) Loss: 0.0004(0.0043) Grad: 2206.4834  LR: 0.000008  
[32m+          Epoch: [4][800/2871] Elapsed 6m 45s (remain 17m 28s) Loss: 0.0002(0.0044) Grad: 1380.2655  LR: 0.000008  
[32m+          Epoch: [4][900/2871] Elapsed 7m 35s (remain 16m 36s) Loss: 0.0022(0.0044) Grad: 9399.9111  LR: 0.000007  
[32m+          Epoch: [4][1000/2871] Elapsed 8m 25s (remain 15m 45s) Loss: 0.0031(0.0043) Grad: 9055.0664  LR: 0.000007  
[32m+          Epoch: [4][1100/2871] Elapsed 9m 16s (remain 14m 53s) Loss: 0.0035(0.0042) Grad: 13052.0693  LR: 0.000007  
[32m+          Epoch: [4][1200/2871] Elapsed 10m 6s (remain 14m 2s) Loss: 0.0026(0.0041) Grad: 9108.1348  LR: 0.000007  
[32m+          Epoch: [4][1300/2871] Elapsed 10m 56s (remain 13m 11s) Loss: 0.0001(0.0042) Grad: 658.9839  LR: 0.000007  
[32m+          Epoch: [4][1400/2871] Elapsed 11m 46s (remain 12m 20s) Loss: 0.0026(0.0042) Grad: 5606.0996  LR: 0.000007  
[32m+          Epoch: [4][1500/2871] Elapsed 12m 36s (remain 11m 30s) Loss: 0.0001(0.0041) Grad: 239.7679  LR: 0.000007  
[32m+          Epoch: [4][1600/2871] Elapsed 13m 27s (remain 10m 40s) Loss: 0.0005(0.0043) Grad: 4930.3828  LR: 0.000006  
[32m+          Epoch: [4][1700/2871] Elapsed 14m 18s (remain 9m 50s) Loss: 0.0000(0.0043) Grad: 87.0953  LR: 0.000006  
[32m+          Epoch: [4][1800/2871] Elapsed 15m 8s (remain 8m 59s) Loss: 0.0019(0.0043) Grad: 13595.8320  LR: 0.000006  
[32m+          Epoch: [4][1900/2871] Elapsed 15m 59s (remain 8m 9s) Loss: 0.0000(0.0043) Grad: 14.0969  LR: 0.000006  
[32m+          Epoch: [4][2000/2871] Elapsed 16m 50s (remain 7m 19s) Loss: 0.0022(0.0043) Grad: 7094.6216  LR: 0.000006  
[32m+          Epoch: [4][2100/2871] Elapsed 17m 40s (remain 6m 28s) Loss: 0.0124(0.0042) Grad: 18846.8438  LR: 0.000006  
[32m+          Epoch: [4][2200/2871] Elapsed 18m 31s (remain 5m 38s) Loss: 0.0089(0.0042) Grad: 27357.0625  LR: 0.000005  
[32m+          Epoch: [4][2300/2871] Elapsed 19m 21s (remain 4m 47s) Loss: 0.0051(0.0041) Grad: 21531.9727  LR: 0.000005  
[32m+          Epoch: [4][2400/2871] Elapsed 20m 12s (remain 3m 57s) Loss: 0.0000(0.0041) Grad: 8.0218  LR: 0.000005  
[32m+          Epoch: [4][2500/2871] Elapsed 21m 4s (remain 3m 7s) Loss: 0.0000(0.0041) Grad: 23.0375  LR: 0.000005  
[32m+          Epoch: [4][2600/2871] Elapsed 21m 54s (remain 2m 16s) Loss: 0.0019(0.0041) Grad: 8396.6572  LR: 0.000005  
[32m+          Epoch: [4][2700/2871] Elapsed 22m 44s (remain 1m 25s) Loss: 0.0015(0.0041) Grad: 5460.3350  LR: 0.000005  
[32m+          Epoch: [4][2800/2871] Elapsed 23m 35s (remain 0m 35s) Loss: 0.0000(0.0040) Grad: 45.6337  LR: 0.000005  
[32m+          Epoch: [4][2870/2871] Elapsed 24m 11s (remain 0m 0s) Loss: 0.0001(0.0041) Grad: 231.8709  LR: 0.000004  
[32m+          EVAL: [0/704] Elapsed 0m 0s (remain 6m 57s) Loss: 0.0006(0.0006) 
[32m+          EVAL: [100/704] Elapsed 0m 28s (remain 2m 49s) Loss: 0.0069(0.0083) 
[32m+          EVAL: [200/704] Elapsed 0m 56s (remain 2m 20s) Loss: 0.0000(0.0074) 
[32m+          EVAL: [300/704] Elapsed 1m 23s (remain 1m 52s) Loss: 0.0002(0.0071) 
[32m+          EVAL: [400/704] Elapsed 1m 51s (remain 1m 24s) Loss: 0.0120(0.0084) 
[32m+          EVAL: [500/704] Elapsed 2m 19s (remain 0m 56s) Loss: 0.0088(0.0094) 
[32m+          EVAL: [600/704] Elapsed 2m 47s (remain 0m 28s) Loss: 0.0000(0.0098) 
[32m+          EVAL: [700/704] Elapsed 3m 15s (remain 0m 0s) Loss: 0.0000(0.0092) 
[32m+          EVAL: [703/704] Elapsed 3m 15s (remain 0m 0s) Loss: 0.0000(0.0092) 
[32m+          Epoch 4 - avg_train_loss: 0.0041  avg_val_loss: 0.0092  time: 1652s
[32m+          Epoch 4 - Score: 0.8773
[32m+          Epoch 4 - Save Best Score: 0.8773 Model
[32m+          Epoch: [5][0/2871] Elapsed 0m 0s (remain 40m 13s) Loss: 0.0000(0.0000) Grad: 259.6436  LR: 0.000004  
[32m+          Epoch: [5][100/2871] Elapsed 0m 50s (remain 23m 12s) Loss: 0.0000(0.0029) Grad: 147.0931  LR: 0.000004  
[32m+          Epoch: [5][200/2871] Elapsed 1m 41s (remain 22m 23s) Loss: 0.0000(0.0030) Grad: 16.5106  LR: 0.000004  
[32m+          Epoch: [5][300/2871] Elapsed 2m 32s (remain 21m 39s) Loss: 0.0008(0.0032) Grad: 6882.1934  LR: 0.000004  
[32m+          Epoch: [5][400/2871] Elapsed 3m 23s (remain 20m 52s) Loss: 0.0116(0.0032) Grad: 27571.9355  LR: 0.000004  
[32m+          Epoch: [5][500/2871] Elapsed 4m 13s (remain 19m 59s) Loss: 0.0055(0.0033) Grad: 9289.1367  LR: 0.000004  
[32m+          Epoch: [5][600/2871] Elapsed 5m 3s (remain 19m 7s) Loss: 0.0000(0.0032) Grad: 17.4889  LR: 0.000004  
[32m+          Epoch: [5][700/2871] Elapsed 5m 53s (remain 18m 15s) Loss: 0.0000(0.0031) Grad: 71.3143  LR: 0.000003  
[32m+          Epoch: [5][800/2871] Elapsed 6m 43s (remain 17m 23s) Loss: 0.0000(0.0033) Grad: 65.5152  LR: 0.000003  
[32m+          Epoch: [5][900/2871] Elapsed 7m 34s (remain 16m 34s) Loss: 0.0003(0.0033) Grad: 2271.9368  LR: 0.000003  
[32m+          Epoch: [5][1000/2871] Elapsed 8m 25s (remain 15m 44s) Loss: 0.0000(0.0033) Grad: 19.7658  LR: 0.000003  
[32m+          Epoch: [5][1100/2871] Elapsed 9m 15s (remain 14m 53s) Loss: 0.0001(0.0033) Grad: 1479.5841  LR: 0.000003  
[32m+          Epoch: [5][1200/2871] Elapsed 10m 6s (remain 14m 3s) Loss: 0.0001(0.0033) Grad: 535.8121  LR: 0.000003  
[32m+          Epoch: [5][1300/2871] Elapsed 10m 58s (remain 13m 14s) Loss: 0.0000(0.0033) Grad: 11.1757  LR: 0.000002  
[32m+          Epoch: [5][1400/2871] Elapsed 11m 49s (remain 12m 23s) Loss: 0.0003(0.0035) Grad: 4629.9443  LR: 0.000002  
[32m+          Epoch: [5][1500/2871] Elapsed 12m 39s (remain 11m 33s) Loss: 0.0002(0.0034) Grad: 652.8654  LR: 0.000002  
[32m+          Epoch: [5][1600/2871] Elapsed 13m 29s (remain 10m 42s) Loss: 0.0001(0.0034) Grad: 1433.0679  LR: 0.000002  
[32m+          Epoch: [5][1700/2871] Elapsed 14m 20s (remain 9m 52s) Loss: 0.0000(0.0036) Grad: 316.5952  LR: 0.000002  
[32m+          Epoch: [5][1800/2871] Elapsed 15m 13s (remain 9m 2s) Loss: 0.0007(0.0035) Grad: 4901.1968  LR: 0.000002  
[32m+          Epoch: [5][1900/2871] Elapsed 16m 5s (remain 8m 12s) Loss: 0.0001(0.0035) Grad: 957.6162  LR: 0.000002  
[32m+          Epoch: [5][2000/2871] Elapsed 16m 55s (remain 7m 21s) Loss: 0.0045(0.0034) Grad: 7000.3613  LR: 0.000001  
[32m+          Epoch: [5][2100/2871] Elapsed 17m 45s (remain 6m 30s) Loss: 0.0017(0.0034) Grad: 7138.7734  LR: 0.000001  
[32m+          Epoch: [5][2200/2871] Elapsed 18m 36s (remain 5m 39s) Loss: 0.0000(0.0035) Grad: 434.5886  LR: 0.000001  
[32m+          Epoch: [5][2300/2871] Elapsed 19m 27s (remain 4m 49s) Loss: 0.0000(0.0034) Grad: 15.3668  LR: 0.000001  
[32m+          Epoch: [5][2400/2871] Elapsed 20m 17s (remain 3m 58s) Loss: 0.0018(0.0034) Grad: 7024.5703  LR: 0.000001  
[32m+          Epoch: [5][2500/2871] Elapsed 21m 8s (remain 3m 7s) Loss: 0.0000(0.0034) Grad: 166.8158  LR: 0.000001  
[32m+          Epoch: [5][2600/2871] Elapsed 21m 58s (remain 2m 16s) Loss: 0.0044(0.0035) Grad: 4970.0835  LR: 0.000000  
[32m+          Epoch: [5][2700/2871] Elapsed 22m 48s (remain 1m 26s) Loss: 0.0002(0.0035) Grad: 1623.1411  LR: 0.000000  
[32m+          Epoch: [5][2800/2871] Elapsed 23m 38s (remain 0m 35s) Loss: 0.0001(0.0034) Grad: 1075.1428  LR: 0.000000  
[32m+          Epoch: [5][2870/2871] Elapsed 24m 13s (remain 0m 0s) Loss: 0.0085(0.0034) Grad: 88866.0391  LR: 0.000000  
[32m+          EVAL: [0/704] Elapsed 0m 0s (remain 7m 18s) Loss: 0.0003(0.0003) 
[32m+          EVAL: [100/704] Elapsed 0m 28s (remain 2m 48s) Loss: 0.0059(0.0091) 
[32m+          EVAL: [200/704] Elapsed 0m 57s (remain 2m 22s) Loss: 0.0000(0.0081) 
[32m+          EVAL: [300/704] Elapsed 1m 24s (remain 1m 53s) Loss: 0.0001(0.0080) 
[32m+          EVAL: [400/704] Elapsed 1m 52s (remain 1m 24s) Loss: 0.0116(0.0090) 
[32m+          EVAL: [500/704] Elapsed 2m 20s (remain 0m 56s) Loss: 0.0093(0.0099) 
[32m+          EVAL: [600/704] Elapsed 2m 48s (remain 0m 28s) Loss: 0.0000(0.0102) 
[32m+          EVAL: [700/704] Elapsed 3m 15s (remain 0m 0s) Loss: 0.0000(0.0096) 
[32m+          EVAL: [703/704] Elapsed 3m 16s (remain 0m 0s) Loss: 0.0000(0.0096) 
[32m+          Epoch 5 - avg_train_loss: 0.0034  avg_val_loss: 0.0096  time: 1655s
[32m+          Epoch 5 - Score: 0.8768
[32m+          ========== fold: 3 training ==========
[32m+          Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin
[32m+          Epoch: [1][0/2877] Elapsed 0m 0s (remain 38m 49s) Loss: 0.5607(0.5607) Grad: inf  LR: 0.000000  
[32m+          Epoch: [1][100/2877] Elapsed 0m 52s (remain 24m 7s) Loss: 0.5110(0.5243) Grad: inf  LR: 0.000001  
[32m+          Epoch: [1][200/2877] Elapsed 1m 43s (remain 22m 55s) Loss: 0.1174(0.4009) Grad: 9952.3496  LR: 0.000003  
[32m+          Epoch: [1][300/2877] Elapsed 2m 34s (remain 21m 58s) Loss: 0.0283(0.2855) Grad: 963.2571  LR: 0.000004  
[32m+          Epoch: [1][400/2877] Elapsed 3m 24s (remain 21m 2s) Loss: 0.0263(0.2241) Grad: 930.7817  LR: 0.000006  
[32m+          Epoch: [1][500/2877] Elapsed 4m 15s (remain 20m 9s) Loss: 0.0398(0.1870) Grad: 1483.2206  LR: 0.000007  
[32m+          Epoch: [1][600/2877] Elapsed 5m 5s (remain 19m 17s) Loss: 0.0343(0.1618) Grad: 1867.9656  LR: 0.000008  
[32m+          Epoch: [1][700/2877] Elapsed 5m 57s (remain 18m 29s) Loss: 0.0072(0.1423) Grad: 1573.2526  LR: 0.000010  
[32m+          Epoch: [1][800/2877] Elapsed 6m 48s (remain 17m 37s) Loss: 0.0020(0.1268) Grad: 571.1824  LR: 0.000011  
[32m+          Epoch: [1][900/2877] Elapsed 7m 38s (remain 16m 46s) Loss: 0.0034(0.1144) Grad: 2020.1418  LR: 0.000013  
[32m+          Epoch: [1][1000/2877] Elapsed 8m 29s (remain 15m 54s) Loss: 0.0069(0.1042) Grad: 979.9125  LR: 0.000014  
[32m+          Epoch: [1][1100/2877] Elapsed 9m 20s (remain 15m 4s) Loss: 0.0097(0.0957) Grad: 6133.9326  LR: 0.000015  
[32m+          Epoch: [1][1200/2877] Elapsed 10m 13s (remain 14m 15s) Loss: 0.0131(0.0888) Grad: 5120.4102  LR: 0.000017  
[32m+          Epoch: [1][1300/2877] Elapsed 11m 4s (remain 13m 24s) Loss: 0.0231(0.0828) Grad: 3223.8054  LR: 0.000018  
[32m+          Epoch: [1][1400/2877] Elapsed 11m 55s (remain 12m 33s) Loss: 0.0139(0.0777) Grad: 1501.4844  LR: 0.000019  
[32m+          Epoch: [1][1500/2877] Elapsed 12m 48s (remain 11m 44s) Loss: 0.0017(0.0733) Grad: 847.9063  LR: 0.000020  
[32m+          Epoch: [1][1600/2877] Elapsed 13m 38s (remain 10m 52s) Loss: 0.0016(0.0693) Grad: 590.2435  LR: 0.000020  
[32m+          Epoch: [1][1700/2877] Elapsed 14m 28s (remain 10m 0s) Loss: 0.0286(0.0660) Grad: 9871.6543  LR: 0.000020  
[32m+          Epoch: [1][1800/2877] Elapsed 15m 19s (remain 9m 9s) Loss: 0.0054(0.0628) Grad: 863.0071  LR: 0.000019  
[32m+          Epoch: [1][1900/2877] Elapsed 16m 11s (remain 8m 18s) Loss: 0.0046(0.0600) Grad: 930.6978  LR: 0.000019  
[32m+          Epoch: [1][2000/2877] Elapsed 17m 1s (remain 7m 27s) Loss: 0.0055(0.0574) Grad: 1220.1887  LR: 0.000019  
[32m+          Epoch: [1][2100/2877] Elapsed 17m 52s (remain 6m 36s) Loss: 0.0050(0.0551) Grad: 650.6840  LR: 0.000019  
[32m+          Epoch: [1][2200/2877] Elapsed 18m 42s (remain 5m 44s) Loss: 0.0015(0.0530) Grad: 431.3770  LR: 0.000019  
[32m+          Epoch: [1][2300/2877] Elapsed 19m 32s (remain 4m 53s) Loss: 0.0102(0.0510) Grad: 814.5349  LR: 0.000019  
[32m+          Epoch: [1][2400/2877] Elapsed 20m 22s (remain 4m 2s) Loss: 0.0052(0.0492) Grad: 1173.0642  LR: 0.000019  
[32m+          Epoch: [1][2500/2877] Elapsed 21m 13s (remain 3m 11s) Loss: 0.0062(0.0475) Grad: 917.0492  LR: 0.000018  
[32m+          Epoch: [1][2600/2877] Elapsed 22m 4s (remain 2m 20s) Loss: 0.0002(0.0460) Grad: 75.2281  LR: 0.000018  
[32m+          Epoch: [1][2700/2877] Elapsed 22m 56s (remain 1m 29s) Loss: 0.0037(0.0446) Grad: 728.2426  LR: 0.000018  
[32m+          Epoch: [1][2800/2877] Elapsed 23m 47s (remain 0m 38s) Loss: 0.0366(0.0433) Grad: 4123.6685  LR: 0.000018  
[32m+          Epoch: [1][2876/2877] Elapsed 24m 25s (remain 0m 0s) Loss: 0.0014(0.0423) Grad: 613.7996  LR: 0.000018  
[32m+          EVAL: [0/698] Elapsed 0m 0s (remain 6m 44s) Loss: 0.0019(0.0019) 
[32m+          EVAL: [100/698] Elapsed 0m 28s (remain 2m 50s) Loss: 0.0032(0.0062) 
[32m+          EVAL: [200/698] Elapsed 0m 56s (remain 2m 20s) Loss: 0.0181(0.0083) 
[32m+          EVAL: [300/698] Elapsed 1m 24s (remain 1m 51s) Loss: 0.0017(0.0081) 
[32m+          EVAL: [400/698] Elapsed 1m 51s (remain 1m 22s) Loss: 0.0379(0.0088) 
[32m+          EVAL: [500/698] Elapsed 2m 20s (remain 0m 55s) Loss: 0.0049(0.0089) 
[32m+          EVAL: [600/698] Elapsed 2m 48s (remain 0m 27s) Loss: 0.0090(0.0085) 
[32m+          EVAL: [697/698] Elapsed 3m 14s (remain 0m 0s) Loss: 0.0000(0.0084) 
[32m+          Epoch 1 - avg_train_loss: 0.0423  avg_val_loss: 0.0084  time: 1665s
[32m+          Epoch 1 - Score: 0.8548
[32m+          Epoch 1 - Save Best Score: 0.8548 Model
[32m+          Epoch: [2][0/2877] Elapsed 0m 0s (remain 37m 5s) Loss: 0.0053(0.0053) Grad: 16241.2305  LR: 0.000018  
[32m+          Epoch: [2][100/2877] Elapsed 0m 50s (remain 23m 20s) Loss: 0.0001(0.0072) Grad: 460.7592  LR: 0.000018  
[32m+          Epoch: [2][200/2877] Elapsed 1m 42s (remain 22m 47s) Loss: 0.0001(0.0073) Grad: 612.9837  LR: 0.000017  
[32m+          Epoch: [2][300/2877] Elapsed 2m 33s (remain 21m 49s) Loss: 0.0337(0.0068) Grad: 40249.2266  LR: 0.000017  
[32m+          Epoch: [2][400/2877] Elapsed 3m 23s (remain 20m 56s) Loss: 0.0000(0.0066) Grad: 188.9160  LR: 0.000017  
[32m+          Epoch: [2][500/2877] Elapsed 4m 14s (remain 20m 7s) Loss: 0.0097(0.0065) Grad: 27847.8145  LR: 0.000017  
[32m+          Epoch: [2][600/2877] Elapsed 5m 6s (remain 19m 20s) Loss: 0.0033(0.0064) Grad: 30354.4297  LR: 0.000017  
[32m+          Epoch: [2][700/2877] Elapsed 5m 56s (remain 18m 27s) Loss: 0.0001(0.0064) Grad: 152.3243  LR: 0.000017  
[32m+          Epoch: [2][800/2877] Elapsed 6m 48s (remain 17m 38s) Loss: 0.0027(0.0066) Grad: 12051.4229  LR: 0.000017  
[32m+          Epoch: [2][900/2877] Elapsed 7m 39s (remain 16m 46s) Loss: 0.0009(0.0065) Grad: 2161.2358  LR: 0.000016  
[32m+          Epoch: [2][1000/2877] Elapsed 8m 29s (remain 15m 55s) Loss: 0.0533(0.0067) Grad: 51625.3594  LR: 0.000016  
[32m+          Epoch: [2][1100/2877] Elapsed 9m 20s (remain 15m 3s) Loss: 0.0005(0.0066) Grad: 1543.3909  LR: 0.000016  
[32m+          Epoch: [2][1200/2877] Elapsed 10m 10s (remain 14m 11s) Loss: 0.0048(0.0065) Grad: 6419.2075  LR: 0.000016  
[32m+          Epoch: [2][1300/2877] Elapsed 11m 1s (remain 13m 21s) Loss: 0.0012(0.0066) Grad: 3837.9482  LR: 0.000016  
[32m+          Epoch: [2][1400/2877] Elapsed 11m 52s (remain 12m 30s) Loss: 0.0195(0.0065) Grad: 36810.3594  LR: 0.000016  
[32m+          Epoch: [2][1500/2877] Elapsed 12m 43s (remain 11m 39s) Loss: 0.0065(0.0065) Grad: 35222.0938  LR: 0.000015  
[32m+          Epoch: [2][1600/2877] Elapsed 13m 33s (remain 10m 48s) Loss: 0.0039(0.0066) Grad: 3454.9653  LR: 0.000015  
[32m+          Epoch: [2][1700/2877] Elapsed 14m 24s (remain 9m 57s) Loss: 0.0118(0.0065) Grad: 14328.8877  LR: 0.000015  
[32m+          Epoch: [2][1800/2877] Elapsed 15m 17s (remain 9m 7s) Loss: 0.0003(0.0065) Grad: 1201.9280  LR: 0.000015  
[32m+          Epoch: [2][1900/2877] Elapsed 16m 7s (remain 8m 16s) Loss: 0.0185(0.0064) Grad: 32378.8730  LR: 0.000015  
[32m+          Epoch: [2][2000/2877] Elapsed 16m 58s (remain 7m 25s) Loss: 0.0008(0.0064) Grad: 2701.4209  LR: 0.000015  
[32m+          Epoch: [2][2100/2877] Elapsed 17m 49s (remain 6m 35s) Loss: 0.0087(0.0065) Grad: 20859.5801  LR: 0.000015  
[32m+          Epoch: [2][2200/2877] Elapsed 18m 40s (remain 5m 44s) Loss: 0.0003(0.0065) Grad: 1703.3076  LR: 0.000014  
[32m+          Epoch: [2][2300/2877] Elapsed 19m 30s (remain 4m 53s) Loss: 0.0021(0.0064) Grad: 6274.3799  LR: 0.000014  
[32m+          Epoch: [2][2400/2877] Elapsed 20m 21s (remain 4m 2s) Loss: 0.0003(0.0064) Grad: 1708.5969  LR: 0.000014  
[32m+          Epoch: [2][2500/2877] Elapsed 21m 11s (remain 3m 11s) Loss: 0.0000(0.0064) Grad: 54.5295  LR: 0.000014  
[32m+          Epoch: [2][2600/2877] Elapsed 22m 2s (remain 2m 20s) Loss: 0.0061(0.0064) Grad: 20670.4180  LR: 0.000014  
[32m+          Epoch: [2][2700/2877] Elapsed 22m 53s (remain 1m 29s) Loss: 0.0080(0.0064) Grad: 22621.3047  LR: 0.000014  
[32m+          Epoch: [2][2800/2877] Elapsed 23m 44s (remain 0m 38s) Loss: 0.0001(0.0064) Grad: 494.6836  LR: 0.000013  
[32m+          Epoch: [2][2876/2877] Elapsed 24m 22s (remain 0m 0s) Loss: 0.0259(0.0064) Grad: 32794.0898  LR: 0.000013  
[32m+          EVAL: [0/698] Elapsed 0m 0s (remain 6m 49s) Loss: 0.0009(0.0009) 
[32m+          EVAL: [100/698] Elapsed 0m 28s (remain 2m 47s) Loss: 0.0036(0.0055) 
[32m+          EVAL: [200/698] Elapsed 0m 55s (remain 2m 18s) Loss: 0.0008(0.0080) 
[32m+          EVAL: [300/698] Elapsed 1m 23s (remain 1m 50s) Loss: 0.0043(0.0077) 
[32m+          EVAL: [400/698] Elapsed 1m 51s (remain 1m 22s) Loss: 0.0448(0.0081) 
[32m+          EVAL: [500/698] Elapsed 2m 19s (remain 0m 54s) Loss: 0.0078(0.0080) 
[32m+          EVAL: [600/698] Elapsed 2m 47s (remain 0m 27s) Loss: 0.0060(0.0078) 
[32m+          EVAL: [697/698] Elapsed 3m 14s (remain 0m 0s) Loss: 0.0000(0.0076) 
[32m+          Epoch 2 - avg_train_loss: 0.0064  avg_val_loss: 0.0076  time: 1662s
[32m+          Epoch 2 - Score: 0.8736
[32m+          Epoch 2 - Save Best Score: 0.8736 Model
[32m+          Epoch: [3][0/2877] Elapsed 0m 0s (remain 39m 54s) Loss: 0.0030(0.0030) Grad: 9841.9434  LR: 0.000013  
[32m+          Epoch: [3][100/2877] Elapsed 0m 51s (remain 23m 46s) Loss: 0.0042(0.0044) Grad: 14938.5049  LR: 0.000013  
[32m+          Epoch: [3][200/2877] Elapsed 1m 42s (remain 22m 47s) Loss: 0.0028(0.0046) Grad: 4912.6660  LR: 0.000013  
[32m+          Epoch: [3][300/2877] Elapsed 2m 33s (remain 21m 50s) Loss: 0.0002(0.0049) Grad: 3663.0989  LR: 0.000013  
[32m+          Epoch: [3][400/2877] Elapsed 3m 23s (remain 20m 57s) Loss: 0.0000(0.0048) Grad: 26.6582  LR: 0.000013  
[32m+          Epoch: [3][500/2877] Elapsed 4m 14s (remain 20m 6s) Loss: 0.0066(0.0050) Grad: 85076.5156  LR: 0.000013  
[32m+          Epoch: [3][600/2877] Elapsed 5m 5s (remain 19m 16s) Loss: 0.0076(0.0052) Grad: 14014.5625  LR: 0.000012  
[32m+          Epoch: [3][700/2877] Elapsed 5m 56s (remain 18m 25s) Loss: 0.0016(0.0050) Grad: 3290.5049  LR: 0.000012  
[32m+          Epoch: [3][800/2877] Elapsed 6m 46s (remain 17m 33s) Loss: 0.0028(0.0050) Grad: 6034.7876  LR: 0.000012  
[32m+          Epoch: [3][900/2877] Elapsed 7m 37s (remain 16m 42s) Loss: 0.0051(0.0048) Grad: 18979.9473  LR: 0.000012  
[32m+          Epoch: [3][1000/2877] Elapsed 8m 28s (remain 15m 52s) Loss: 0.0002(0.0051) Grad: 1087.4966  LR: 0.000012  
[32m+          Epoch: [3][1100/2877] Elapsed 9m 19s (remain 15m 2s) Loss: 0.0001(0.0052) Grad: 615.2476  LR: 0.000012  
[32m+          Epoch: [3][1200/2877] Elapsed 10m 10s (remain 14m 11s) Loss: 0.0012(0.0051) Grad: 4130.2661  LR: 0.000011  
[32m+          Epoch: [3][1300/2877] Elapsed 11m 0s (remain 13m 20s) Loss: 0.0137(0.0049) Grad: 28058.6211  LR: 0.000011  
[32m+          Epoch: [3][1400/2877] Elapsed 11m 51s (remain 12m 29s) Loss: 0.0016(0.0050) Grad: 4546.5298  LR: 0.000011  
[32m+          Epoch: [3][1500/2877] Elapsed 12m 41s (remain 11m 38s) Loss: 0.0052(0.0051) Grad: 18052.7461  LR: 0.000011  
[32m+          Epoch: [3][1600/2877] Elapsed 13m 31s (remain 10m 47s) Loss: 0.0000(0.0051) Grad: 18.0587  LR: 0.000011  
[32m+          Epoch: [3][1700/2877] Elapsed 14m 24s (remain 9m 57s) Loss: 0.0022(0.0050) Grad: 7316.5767  LR: 0.000011  
[32m+          Epoch: [3][1800/2877] Elapsed 15m 15s (remain 9m 6s) Loss: 0.0193(0.0050) Grad: 36867.9961  LR: 0.000011  
[32m+          Epoch: [3][1900/2877] Elapsed 16m 5s (remain 8m 15s) Loss: 0.0006(0.0051) Grad: 1900.4257  LR: 0.000010  
[32m+          Epoch: [3][2000/2877] Elapsed 16m 56s (remain 7m 25s) Loss: 0.0000(0.0051) Grad: 40.6659  LR: 0.000010  
[32m+          Epoch: [3][2100/2877] Elapsed 17m 46s (remain 6m 34s) Loss: 0.0001(0.0052) Grad: 996.3578  LR: 0.000010  
[32m+          Epoch: [3][2200/2877] Elapsed 18m 37s (remain 5m 43s) Loss: 0.0169(0.0052) Grad: 53238.9648  LR: 0.000010  
[32m+          Epoch: [3][2300/2877] Elapsed 19m 27s (remain 4m 52s) Loss: 0.0225(0.0053) Grad: 32918.1562  LR: 0.000010  
[32m+          Epoch: [3][2400/2877] Elapsed 20m 18s (remain 4m 1s) Loss: 0.0401(0.0053) Grad: 53644.0039  LR: 0.000010  
[32m+          Epoch: [3][2500/2877] Elapsed 21m 9s (remain 3m 10s) Loss: 0.0057(0.0053) Grad: 15249.7305  LR: 0.000009  
[32m+          Epoch: [3][2600/2877] Elapsed 22m 0s (remain 2m 20s) Loss: 0.0066(0.0053) Grad: 8998.8906  LR: 0.000009  
[32m+          Epoch: [3][2700/2877] Elapsed 22m 50s (remain 1m 29s) Loss: 0.0110(0.0052) Grad: 70184.6250  LR: 0.000009  
[32m+          Epoch: [3][2800/2877] Elapsed 23m 40s (remain 0m 38s) Loss: 0.0150(0.0052) Grad: 26466.1953  LR: 0.000009  
[32m+          Epoch: [3][2876/2877] Elapsed 24m 19s (remain 0m 0s) Loss: 0.0016(0.0052) Grad: 4526.0464  LR: 0.000009  
[32m+          EVAL: [0/698] Elapsed 0m 0s (remain 7m 3s) Loss: 0.0028(0.0028) 
[32m+          EVAL: [100/698] Elapsed 0m 28s (remain 2m 47s) Loss: 0.0015(0.0050) 
[32m+          EVAL: [200/698] Elapsed 0m 55s (remain 2m 18s) Loss: 0.0008(0.0073) 
[32m+          EVAL: [300/698] Elapsed 1m 23s (remain 1m 50s) Loss: 0.0022(0.0072) 
[32m+          EVAL: [400/698] Elapsed 1m 50s (remain 1m 22s) Loss: 0.0268(0.0077) 
[32m+          EVAL: [500/698] Elapsed 2m 18s (remain 0m 54s) Loss: 0.0109(0.0077) 
[32m+          EVAL: [600/698] Elapsed 2m 46s (remain 0m 26s) Loss: 0.0024(0.0074) 
[32m+          EVAL: [697/698] Elapsed 3m 13s (remain 0m 0s) Loss: 0.0000(0.0071) 
[32m+          Epoch 3 - avg_train_loss: 0.0052  avg_val_loss: 0.0071  time: 1658s
[32m+          Epoch 3 - Score: 0.8856
[32m+          Epoch 3 - Save Best Score: 0.8856 Model
[32m+          Epoch: [4][0/2877] Elapsed 0m 0s (remain 41m 26s) Loss: 0.0028(0.0028) Grad: 23819.3301  LR: 0.000009  
[32m+          Epoch: [4][100/2877] Elapsed 0m 51s (remain 23m 25s) Loss: 0.0003(0.0043) Grad: 1693.4913  LR: 0.000009  
[32m+          Epoch: [4][200/2877] Elapsed 1m 41s (remain 22m 35s) Loss: 0.0079(0.0041) Grad: 13215.7178  LR: 0.000009  
[32m+          Epoch: [4][300/2877] Elapsed 2m 33s (remain 21m 49s) Loss: 0.0050(0.0039) Grad: 9824.8838  LR: 0.000008  
[32m+          Epoch: [4][400/2877] Elapsed 3m 23s (remain 20m 53s) Loss: 0.0009(0.0037) Grad: 2065.0957  LR: 0.000008  
[32m+          Epoch: [4][500/2877] Elapsed 4m 13s (remain 20m 0s) Loss: 0.0036(0.0038) Grad: 14078.8047  LR: 0.000008  
[32m+          Epoch: [4][600/2877] Elapsed 5m 3s (remain 19m 8s) Loss: 0.0071(0.0036) Grad: 27906.6523  LR: 0.000008  
[32m+          Epoch: [4][700/2877] Elapsed 5m 53s (remain 18m 18s) Loss: 0.0000(0.0038) Grad: 151.7593  LR: 0.000008  
[32m+          Epoch: [4][800/2877] Elapsed 6m 44s (remain 17m 28s) Loss: 0.0058(0.0039) Grad: 30304.4395  LR: 0.000008  
[32m+          Epoch: [4][900/2877] Elapsed 7m 35s (remain 16m 38s) Loss: 0.0002(0.0040) Grad: 1577.5884  LR: 0.000007  
[32m+          Epoch: [4][1000/2877] Elapsed 8m 25s (remain 15m 47s) Loss: 0.0402(0.0041) Grad: 55456.6953  LR: 0.000007  
[32m+          Epoch: [4][1100/2877] Elapsed 9m 15s (remain 14m 55s) Loss: 0.0066(0.0040) Grad: 7938.7144  LR: 0.000007  
[32m+          Epoch: [4][1200/2877] Elapsed 10m 5s (remain 14m 5s) Loss: 0.0000(0.0040) Grad: 676.1921  LR: 0.000007  
[32m+          Epoch: [4][1300/2877] Elapsed 10m 56s (remain 13m 15s) Loss: 0.0020(0.0042) Grad: 8729.7061  LR: 0.000007  
[32m+          Epoch: [4][1400/2877] Elapsed 11m 47s (remain 12m 25s) Loss: 0.0016(0.0041) Grad: 9418.7373  LR: 0.000007  
[32m+          Epoch: [4][1500/2877] Elapsed 12m 37s (remain 11m 34s) Loss: 0.0028(0.0041) Grad: 11725.2900  LR: 0.000007  
[32m+          Epoch: [4][1600/2877] Elapsed 13m 27s (remain 10m 43s) Loss: 0.0084(0.0040) Grad: 15975.3398  LR: 0.000006  
[32m+          Epoch: [4][1700/2877] Elapsed 14m 17s (remain 9m 53s) Loss: 0.0298(0.0041) Grad: 29421.3945  LR: 0.000006  
[32m+          Epoch: [4][1800/2877] Elapsed 15m 8s (remain 9m 2s) Loss: 0.0053(0.0041) Grad: 35263.1328  LR: 0.000006  
[32m+          Epoch: [4][1900/2877] Elapsed 15m 59s (remain 8m 12s) Loss: 0.0000(0.0041) Grad: 288.3994  LR: 0.000006  
[32m+          Epoch: [4][2000/2877] Elapsed 16m 50s (remain 7m 22s) Loss: 0.0366(0.0040) Grad: 38752.5781  LR: 0.000006  
[32m+          Epoch: [4][2100/2877] Elapsed 17m 41s (remain 6m 31s) Loss: 0.0001(0.0040) Grad: 923.2594  LR: 0.000006  
[32m+          Epoch: [4][2200/2877] Elapsed 18m 31s (remain 5m 41s) Loss: 0.0221(0.0040) Grad: 60205.0117  LR: 0.000005  
[32m+          Epoch: [4][2300/2877] Elapsed 19m 21s (remain 4m 50s) Loss: 0.0000(0.0041) Grad: 201.7835  LR: 0.000005  
[32m+          Epoch: [4][2400/2877] Elapsed 20m 12s (remain 4m 0s) Loss: 0.0577(0.0040) Grad: 35795.2734  LR: 0.000005  
[32m+          Epoch: [4][2500/2877] Elapsed 21m 4s (remain 3m 10s) Loss: 0.0021(0.0041) Grad: 5532.6479  LR: 0.000005  
[32m+          Epoch: [4][2600/2877] Elapsed 21m 54s (remain 2m 19s) Loss: 0.0002(0.0042) Grad: 1272.7646  LR: 0.000005  
[32m+          Epoch: [4][2700/2877] Elapsed 22m 45s (remain 1m 28s) Loss: 0.0062(0.0042) Grad: 14316.6523  LR: 0.000005  
[32m+          Epoch: [4][2800/2877] Elapsed 23m 37s (remain 0m 38s) Loss: 0.0027(0.0042) Grad: 11630.4893  LR: 0.000005  
[32m+          Epoch: [4][2876/2877] Elapsed 24m 16s (remain 0m 0s) Loss: 0.0041(0.0042) Grad: 7951.6177  LR: 0.000004  
[32m+          EVAL: [0/698] Elapsed 0m 0s (remain 6m 49s) Loss: 0.0008(0.0008) 
[32m+          EVAL: [100/698] Elapsed 0m 28s (remain 2m 49s) Loss: 0.0010(0.0057) 
[32m+          EVAL: [200/698] Elapsed 0m 57s (remain 2m 22s) Loss: 0.0015(0.0083) 
[32m+          EVAL: [300/698] Elapsed 1m 25s (remain 1m 52s) Loss: 0.0033(0.0081) 
[32m+          EVAL: [400/698] Elapsed 1m 52s (remain 1m 23s) Loss: 0.0277(0.0085) 
[32m+          EVAL: [500/698] Elapsed 2m 20s (remain 0m 55s) Loss: 0.0069(0.0085) 
[32m+          EVAL: [600/698] Elapsed 2m 47s (remain 0m 27s) Loss: 0.0050(0.0081) 
[32m+          EVAL: [697/698] Elapsed 3m 14s (remain 0m 0s) Loss: 0.0000(0.0078) 
[32m+          Epoch 4 - avg_train_loss: 0.0042  avg_val_loss: 0.0078  time: 1656s
[32m+          Epoch 4 - Score: 0.8888
[32m+          Epoch 4 - Save Best Score: 0.8888 Model
[32m+          Epoch: [5][0/2877] Elapsed 0m 0s (remain 40m 48s) Loss: 0.0104(0.0104) Grad: 15109.4023  LR: 0.000004  
[32m+          Epoch: [5][100/2877] Elapsed 0m 51s (remain 23m 30s) Loss: 0.0073(0.0028) Grad: 19063.9648  LR: 0.000004  
[32m+          Epoch: [5][200/2877] Elapsed 1m 42s (remain 22m 46s) Loss: 0.0015(0.0026) Grad: 6817.6006  LR: 0.000004  
[32m+          Epoch: [5][300/2877] Elapsed 2m 32s (remain 21m 48s) Loss: 0.0029(0.0028) Grad: 15684.6182  LR: 0.000004  
[32m+          Epoch: [5][400/2877] Elapsed 3m 23s (remain 20m 54s) Loss: 0.0003(0.0028) Grad: 1850.5190  LR: 0.000004  
[32m+          Epoch: [5][500/2877] Elapsed 4m 13s (remain 20m 0s) Loss: 0.0001(0.0029) Grad: 305.9181  LR: 0.000004  
[32m+          Epoch: [5][600/2877] Elapsed 5m 3s (remain 19m 10s) Loss: 0.0021(0.0031) Grad: 13189.6318  LR: 0.000004  
[32m+          Epoch: [5][700/2877] Elapsed 5m 55s (remain 18m 22s) Loss: 0.0000(0.0033) Grad: 130.9665  LR: 0.000003  
[32m+          Epoch: [5][800/2877] Elapsed 6m 45s (remain 17m 30s) Loss: 0.0006(0.0032) Grad: 4167.7832  LR: 0.000003  
[32m+          Epoch: [5][900/2877] Elapsed 7m 35s (remain 16m 38s) Loss: 0.0017(0.0032) Grad: 12338.0938  LR: 0.000003  
[32m+          Epoch: [5][1000/2877] Elapsed 8m 25s (remain 15m 48s) Loss: 0.0008(0.0032) Grad: 4419.8623  LR: 0.000003  
[32m+          Epoch: [5][1100/2877] Elapsed 9m 17s (remain 14m 58s) Loss: 0.0074(0.0032) Grad: 21177.0918  LR: 0.000003  
[32m+          Epoch: [5][1200/2877] Elapsed 10m 8s (remain 14m 8s) Loss: 0.0000(0.0033) Grad: 24.0843  LR: 0.000003  
[32m+          Epoch: [5][1300/2877] Elapsed 10m 58s (remain 13m 18s) Loss: 0.0514(0.0033) Grad: 113921.0156  LR: 0.000002  
[32m+          Epoch: [5][1400/2877] Elapsed 11m 49s (remain 12m 27s) Loss: 0.0016(0.0032) Grad: 10036.4971  LR: 0.000002  
[32m+          Epoch: [5][1500/2877] Elapsed 12m 39s (remain 11m 36s) Loss: 0.0001(0.0034) Grad: 483.0412  LR: 0.000002  
[32m+          Epoch: [5][1600/2877] Elapsed 13m 29s (remain 10m 45s) Loss: 0.0004(0.0033) Grad: 2611.1257  LR: 0.000002  
[32m+          Epoch: [5][1700/2877] Elapsed 14m 19s (remain 9m 54s) Loss: 0.0000(0.0034) Grad: 12.0651  LR: 0.000002  
[32m+          Epoch: [5][1800/2877] Elapsed 15m 10s (remain 9m 4s) Loss: 0.0000(0.0035) Grad: 234.2557  LR: 0.000002  
[32m+          Epoch: [5][1900/2877] Elapsed 16m 0s (remain 8m 13s) Loss: 0.0002(0.0035) Grad: 920.3159  LR: 0.000002  
[32m+          Epoch: [5][2000/2877] Elapsed 16m 51s (remain 7m 22s) Loss: 0.0064(0.0035) Grad: 41153.6641  LR: 0.000001  
[32m+          Epoch: [5][2100/2877] Elapsed 17m 41s (remain 6m 32s) Loss: 0.0000(0.0035) Grad: 48.0338  LR: 0.000001  
[32m+          Epoch: [5][2200/2877] Elapsed 18m 32s (remain 5m 41s) Loss: 0.0003(0.0035) Grad: 2449.4160  LR: 0.000001  
[32m+          Epoch: [5][2300/2877] Elapsed 19m 24s (remain 4m 51s) Loss: 0.0030(0.0035) Grad: 5301.1196  LR: 0.000001  
[32m+          Epoch: [5][2400/2877] Elapsed 20m 14s (remain 4m 0s) Loss: 0.0010(0.0034) Grad: 8207.2383  LR: 0.000001  
[32m+          Epoch: [5][2500/2877] Elapsed 21m 4s (remain 3m 10s) Loss: 0.0000(0.0034) Grad: 18.7437  LR: 0.000001  
[32m+          Epoch: [5][2600/2877] Elapsed 21m 55s (remain 2m 19s) Loss: 0.0000(0.0034) Grad: 396.6389  LR: 0.000000  
[32m+          Epoch: [5][2700/2877] Elapsed 22m 46s (remain 1m 29s) Loss: 0.0025(0.0034) Grad: 9535.9629  LR: 0.000000  
[32m+          Epoch: [5][2800/2877] Elapsed 23m 36s (remain 0m 38s) Loss: 0.0000(0.0034) Grad: 52.6685  LR: 0.000000  
[32m+          Epoch: [5][2876/2877] Elapsed 24m 14s (remain 0m 0s) Loss: 0.0032(0.0034) Grad: 7059.4419  LR: 0.000000  
[32m+          EVAL: [0/698] Elapsed 0m 0s (remain 6m 29s) Loss: 0.0013(0.0013) 
[32m+          EVAL: [100/698] Elapsed 0m 28s (remain 2m 46s) Loss: 0.0013(0.0066) 
[32m+          EVAL: [200/698] Elapsed 0m 55s (remain 2m 17s) Loss: 0.0004(0.0094) 
[32m+          EVAL: [300/698] Elapsed 1m 23s (remain 1m 50s) Loss: 0.0052(0.0092) 
[32m+          EVAL: [400/698] Elapsed 1m 51s (remain 1m 22s) Loss: 0.0276(0.0095) 
[32m+          EVAL: [500/698] Elapsed 2m 19s (remain 0m 54s) Loss: 0.0061(0.0095) 
[32m+          EVAL: [600/698] Elapsed 2m 47s (remain 0m 27s) Loss: 0.0041(0.0090) 
[32m+          EVAL: [697/698] Elapsed 3m 13s (remain 0m 0s) Loss: 0.0000(0.0087) 
[32m+          Epoch 5 - avg_train_loss: 0.0034  avg_val_loss: 0.0087  time: 1653s
[32m+          Epoch 5 - Score: 0.8905
[32m+          Epoch 5 - Save Best Score: 0.8905 Model
[32m+          ========== fold: 4 training ==========
[32m+          Load weight from ../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin
[32m+          Epoch: [1][0/2850] Elapsed 0m 0s (remain 40m 13s) Loss: 0.6354(0.6354) Grad: inf  LR: 0.000000  
[32m+          Epoch: [1][100/2850] Elapsed 0m 50s (remain 23m 7s) Loss: 0.3413(0.4965) Grad: 14548.1875  LR: 0.000001  
[32m+          Epoch: [1][200/2850] Elapsed 1m 41s (remain 22m 20s) Loss: 0.1000(0.3509) Grad: 6933.5356  LR: 0.000003  
[32m+          Epoch: [1][300/2850] Elapsed 2m 32s (remain 21m 34s) Loss: 0.0404(0.2501) Grad: 562.3069  LR: 0.000004  
[32m+          Epoch: [1][400/2850] Elapsed 3m 22s (remain 20m 38s) Loss: 0.0430(0.1973) Grad: 6325.8765  LR: 0.000006  
[32m+          Epoch: [1][500/2850] Elapsed 4m 12s (remain 19m 45s) Loss: 0.0500(0.1655) Grad: 1009.2373  LR: 0.000007  
[32m+          Epoch: [1][600/2850] Elapsed 5m 3s (remain 18m 55s) Loss: 0.0404(0.1435) Grad: 7714.2036  LR: 0.000008  
[32m+          Epoch: [1][700/2850] Elapsed 5m 54s (remain 18m 7s) Loss: 0.0314(0.1256) Grad: 4029.9856  LR: 0.000010  
[32m+          Epoch: [1][800/2850] Elapsed 6m 45s (remain 17m 18s) Loss: 0.0151(0.1120) Grad: 2186.8123  LR: 0.000011  
[32m+          Epoch: [1][900/2850] Elapsed 7m 36s (remain 16m 26s) Loss: 0.0007(0.1011) Grad: 213.4743  LR: 0.000013  
[32m+          Epoch: [1][1000/2850] Elapsed 8m 26s (remain 15m 35s) Loss: 0.0040(0.0922) Grad: 423.4719  LR: 0.000014  
[32m+          Epoch: [1][1100/2850] Elapsed 9m 17s (remain 14m 45s) Loss: 0.0035(0.0849) Grad: 445.7491  LR: 0.000015  
[32m+          Epoch: [1][1200/2850] Elapsed 10m 8s (remain 13m 56s) Loss: 0.0039(0.0786) Grad: 746.6478  LR: 0.000017  
[32m+          Epoch: [1][1300/2850] Elapsed 10m 59s (remain 13m 5s) Loss: 0.0043(0.0732) Grad: 614.5972  LR: 0.000018  
[32m+          Epoch: [1][1400/2850] Elapsed 11m 50s (remain 12m 14s) Loss: 0.0052(0.0688) Grad: 711.1902  LR: 0.000020  
[32m+          Epoch: [1][1500/2850] Elapsed 12m 40s (remain 11m 23s) Loss: 0.0402(0.0647) Grad: 4944.9575  LR: 0.000020  
[32m+          Epoch: [1][1600/2850] Elapsed 13m 31s (remain 10m 33s) Loss: 0.0190(0.0614) Grad: 1398.6908  LR: 0.000020  
[32m+          Epoch: [1][1700/2850] Elapsed 14m 21s (remain 9m 42s) Loss: 0.0042(0.0584) Grad: 463.7542  LR: 0.000020  
[32m+          Epoch: [1][1800/2850] Elapsed 15m 11s (remain 8m 50s) Loss: 0.0001(0.0557) Grad: 49.5836  LR: 0.000019  
[32m+          Epoch: [1][1900/2850] Elapsed 16m 1s (remain 7m 59s) Loss: 0.0026(0.0532) Grad: 289.1130  LR: 0.000019  
[32m+          Epoch: [1][2000/2850] Elapsed 16m 50s (remain 7m 8s) Loss: 0.0087(0.0510) Grad: 777.7863  LR: 0.000019  
[32m+          Epoch: [1][2100/2850] Elapsed 17m 41s (remain 6m 18s) Loss: 0.0004(0.0489) Grad: 55.2993  LR: 0.000019  
[32m+          Epoch: [1][2200/2850] Elapsed 18m 31s (remain 5m 27s) Loss: 0.0040(0.0471) Grad: 305.7004  LR: 0.000019  
[32m+          Epoch: [1][2300/2850] Elapsed 19m 21s (remain 4m 37s) Loss: 0.0035(0.0454) Grad: 460.0138  LR: 0.000019  
[32m+          Epoch: [1][2400/2850] Elapsed 20m 11s (remain 3m 46s) Loss: 0.0096(0.0438) Grad: 2734.4172  LR: 0.000018  
[32m+          Epoch: [1][2500/2850] Elapsed 21m 0s (remain 2m 55s) Loss: 0.0057(0.0423) Grad: 986.0294  LR: 0.000018  
[32m+          Epoch: [1][2600/2850] Elapsed 21m 50s (remain 2m 5s) Loss: 0.0145(0.0411) Grad: 992.2158  LR: 0.000018  
[32m+          Epoch: [1][2700/2850] Elapsed 22m 40s (remain 1m 15s) Loss: 0.0095(0.0398) Grad: 573.0438  LR: 0.000018  
[32m+          Epoch: [1][2800/2850] Elapsed 23m 30s (remain 0m 24s) Loss: 0.0030(0.0386) Grad: 221.7810  LR: 0.000018  
[32m+          Epoch: [1][2849/2850] Elapsed 23m 54s (remain 0m 0s) Loss: 0.0018(0.0381) Grad: 671.1459  LR: 0.000018  
[32m+          EVAL: [0/725] Elapsed 0m 0s (remain 6m 49s) Loss: 0.0080(0.0080) 
[32m+          EVAL: [100/725] Elapsed 0m 28s (remain 2m 53s) Loss: 0.0007(0.0074) 
[32m+          EVAL: [200/725] Elapsed 0m 55s (remain 2m 25s) Loss: 0.0124(0.0088) 
[32m+          EVAL: [300/725] Elapsed 1m 23s (remain 1m 57s) Loss: 0.0019(0.0077) 
[32m+          EVAL: [400/725] Elapsed 1m 51s (remain 1m 29s) Loss: 0.1149(0.0090) 
[32m+          EVAL: [500/725] Elapsed 2m 18s (remain 1m 2s) Loss: 0.0168(0.0090) 
[32m+          EVAL: [600/725] Elapsed 2m 47s (remain 0m 34s) Loss: 0.0023(0.0089) 
[32m+          EVAL: [700/725] Elapsed 3m 14s (remain 0m 6s) Loss: 0.0006(0.0081) 
[32m+          EVAL: [724/725] Elapsed 3m 21s (remain 0m 0s) Loss: 0.0109(0.0080) 
[32m+          Epoch 1 - avg_train_loss: 0.0381  avg_val_loss: 0.0080  time: 1642s
[32m+          Epoch 1 - Score: 0.8309
[32m+          Epoch 1 - Save Best Score: 0.8309 Model
[32m+          Epoch: [2][0/2850] Elapsed 0m 0s (remain 34m 39s) Loss: 0.0059(0.0059) Grad: 9034.2490  LR: 0.000018  
[32m+          Epoch: [2][100/2850] Elapsed 0m 50s (remain 23m 5s) Loss: 0.0003(0.0057) Grad: 1818.1257  LR: 0.000018  
[32m+          Epoch: [2][200/2850] Elapsed 1m 41s (remain 22m 13s) Loss: 0.0011(0.0061) Grad: 3810.9412  LR: 0.000017  
[32m+          Epoch: [2][300/2850] Elapsed 2m 31s (remain 21m 23s) Loss: 0.0016(0.0062) Grad: 4967.6812  LR: 0.000017  
[32m+          Epoch: [2][400/2850] Elapsed 3m 21s (remain 20m 32s) Loss: 0.0049(0.0063) Grad: 22412.5547  LR: 0.000017  
[32m+          Epoch: [2][500/2850] Elapsed 4m 12s (remain 19m 45s) Loss: 0.0013(0.0060) Grad: 6453.1597  LR: 0.000017  
[32m+          Epoch: [2][600/2850] Elapsed 5m 3s (remain 18m 56s) Loss: 0.0031(0.0057) Grad: 12118.7393  LR: 0.000017  
[32m+          Epoch: [2][700/2850] Elapsed 5m 53s (remain 18m 4s) Loss: 0.0072(0.0060) Grad: 17244.4473  LR: 0.000017  
[32m+          Epoch: [2][800/2850] Elapsed 6m 44s (remain 17m 13s) Loss: 0.0113(0.0059) Grad: 54225.5234  LR: 0.000017  
[32m+          Epoch: [2][900/2850] Elapsed 7m 34s (remain 16m 22s) Loss: 0.0001(0.0061) Grad: 224.1047  LR: 0.000016  
[32m+          Epoch: [2][1000/2850] Elapsed 8m 25s (remain 15m 34s) Loss: 0.0015(0.0060) Grad: 2497.2166  LR: 0.000016  
[32m+          Epoch: [2][1100/2850] Elapsed 9m 16s (remain 14m 44s) Loss: 0.0002(0.0059) Grad: 520.7672  LR: 0.000016  
[32m+          Epoch: [2][1200/2850] Elapsed 10m 7s (remain 13m 53s) Loss: 0.0023(0.0061) Grad: 4986.8379  LR: 0.000016  
[32m+          Epoch: [2][1300/2850] Elapsed 10m 58s (remain 13m 3s) Loss: 0.0066(0.0060) Grad: 4187.3857  LR: 0.000016  
[32m+          Epoch: [2][1400/2850] Elapsed 11m 49s (remain 12m 13s) Loss: 0.0011(0.0060) Grad: 3582.4417  LR: 0.000016  
[32m+          Epoch: [2][1500/2850] Elapsed 12m 40s (remain 11m 23s) Loss: 0.0001(0.0059) Grad: 303.5482  LR: 0.000015  
[32m+          Epoch: [2][1600/2850] Elapsed 13m 30s (remain 10m 32s) Loss: 0.0157(0.0059) Grad: 21130.9258  LR: 0.000015  
[32m+          Epoch: [2][1700/2850] Elapsed 14m 21s (remain 9m 41s) Loss: 0.0803(0.0058) Grad: 143131.5938  LR: 0.000015  
[32m+          Epoch: [2][1800/2850] Elapsed 15m 12s (remain 8m 51s) Loss: 0.0229(0.0059) Grad: 79574.5000  LR: 0.000015  
[32m+          Epoch: [2][1900/2850] Elapsed 16m 2s (remain 8m 0s) Loss: 0.0001(0.0058) Grad: 247.6488  LR: 0.000015  
[32m+          Epoch: [2][2000/2850] Elapsed 16m 52s (remain 7m 9s) Loss: 0.0459(0.0059) Grad: 105378.6328  LR: 0.000015  
[32m+          Epoch: [2][2100/2850] Elapsed 17m 42s (remain 6m 18s) Loss: 0.0012(0.0060) Grad: 1173.8699  LR: 0.000015  
[32m+          Epoch: [2][2200/2850] Elapsed 18m 34s (remain 5m 28s) Loss: 0.0015(0.0060) Grad: 3207.1975  LR: 0.000014  
[32m+          Epoch: [2][2300/2850] Elapsed 19m 24s (remain 4m 37s) Loss: 0.0008(0.0059) Grad: 1257.5015  LR: 0.000014  
[32m+          Epoch: [2][2400/2850] Elapsed 20m 14s (remain 3m 47s) Loss: 0.0001(0.0059) Grad: 256.8921  LR: 0.000014  
[32m+          Epoch: [2][2500/2850] Elapsed 21m 5s (remain 2m 56s) Loss: 0.0014(0.0059) Grad: 1531.1594  LR: 0.000014  
[32m+          Epoch: [2][2600/2850] Elapsed 21m 56s (remain 2m 5s) Loss: 0.0188(0.0059) Grad: 6752.0854  LR: 0.000014  
[32m+          Epoch: [2][2700/2850] Elapsed 22m 46s (remain 1m 15s) Loss: 0.0063(0.0059) Grad: 4901.4395  LR: 0.000014  
[32m+          Epoch: [2][2800/2850] Elapsed 23m 36s (remain 0m 24s) Loss: 0.0301(0.0060) Grad: 12417.9434  LR: 0.000013  
[32m+          Epoch: [2][2849/2850] Elapsed 24m 1s (remain 0m 0s) Loss: 0.0100(0.0059) Grad: 24412.5781  LR: 0.000013  
[32m+          EVAL: [0/725] Elapsed 0m 0s (remain 6m 52s) Loss: 0.0056(0.0056) 
[32m+          EVAL: [100/725] Elapsed 0m 28s (remain 2m 56s) Loss: 0.0016(0.0070) 
[32m+          EVAL: [200/725] Elapsed 0m 57s (remain 2m 28s) Loss: 0.0100(0.0086) 
[32m+          EVAL: [300/725] Elapsed 1m 24s (remain 1m 59s) Loss: 0.0006(0.0073) 
[32m+          EVAL: [400/725] Elapsed 1m 52s (remain 1m 30s) Loss: 0.0860(0.0081) 
[32m+          EVAL: [500/725] Elapsed 2m 19s (remain 1m 2s) Loss: 0.0071(0.0082) 
[32m+          EVAL: [600/725] Elapsed 2m 48s (remain 0m 34s) Loss: 0.0035(0.0079) 
[32m+          EVAL: [700/725] Elapsed 3m 15s (remain 0m 6s) Loss: 0.0005(0.0072) 
[32m+          EVAL: [724/725] Elapsed 3m 22s (remain 0m 0s) Loss: 0.0140(0.0072) 
[32m+          Epoch 2 - avg_train_loss: 0.0059  avg_val_loss: 0.0072  time: 1649s
[32m+          Epoch 2 - Score: 0.8787
[32m+          Epoch 2 - Save Best Score: 0.8787 Model
[32m+          Epoch: [3][0/2850] Elapsed 0m 0s (remain 39m 39s) Loss: 0.0004(0.0004) Grad: 1869.4890  LR: 0.000013  
[32m+          Epoch: [3][100/2850] Elapsed 0m 51s (remain 23m 31s) Loss: 0.0111(0.0034) Grad: 27653.4199  LR: 0.000013  
[32m+          Epoch: [3][200/2850] Elapsed 1m 42s (remain 22m 29s) Loss: 0.0001(0.0042) Grad: 1467.1466  LR: 0.000013  
[32m+          Epoch: [3][300/2850] Elapsed 2m 32s (remain 21m 33s) Loss: 0.0000(0.0044) Grad: 64.3674  LR: 0.000013  
[32m+          Epoch: [3][400/2850] Elapsed 3m 23s (remain 20m 41s) Loss: 0.0023(0.0044) Grad: 12159.6035  LR: 0.000013  
[32m+          Epoch: [3][500/2850] Elapsed 4m 14s (remain 19m 51s) Loss: 0.0049(0.0049) Grad: 9838.8154  LR: 0.000013  
[32m+          Epoch: [3][600/2850] Elapsed 5m 4s (remain 18m 59s) Loss: 0.0000(0.0049) Grad: 210.8261  LR: 0.000012  
[32m+          Epoch: [3][700/2850] Elapsed 5m 55s (remain 18m 8s) Loss: 0.0001(0.0048) Grad: 507.1217  LR: 0.000012  
[32m+          Epoch: [3][800/2850] Elapsed 6m 45s (remain 17m 16s) Loss: 0.0002(0.0049) Grad: 688.4974  LR: 0.000012  
[32m+          Epoch: [3][900/2850] Elapsed 7m 35s (remain 16m 24s) Loss: 0.0000(0.0047) Grad: 153.9189  LR: 0.000012  
[32m+          Epoch: [3][1000/2850] Elapsed 8m 25s (remain 15m 33s) Loss: 0.0001(0.0049) Grad: 304.1748  LR: 0.000012  
[32m+          Epoch: [3][1100/2850] Elapsed 9m 16s (remain 14m 43s) Loss: 0.0046(0.0048) Grad: 16499.4219  LR: 0.000012  
[32m+          Epoch: [3][1200/2850] Elapsed 10m 7s (remain 13m 53s) Loss: 0.0007(0.0048) Grad: 2027.3512  LR: 0.000011  
[32m+          Epoch: [3][1300/2850] Elapsed 10m 57s (remain 13m 3s) Loss: 0.0021(0.0048) Grad: 2167.1323  LR: 0.000011  
[32m+          Epoch: [3][1400/2850] Elapsed 11m 47s (remain 12m 12s) Loss: 0.0014(0.0049) Grad: 7632.9697  LR: 0.000011  
[32m+          Epoch: [3][1500/2850] Elapsed 12m 37s (remain 11m 21s) Loss: 0.0024(0.0049) Grad: 9110.4971  LR: 0.000011  
[32m+          Epoch: [3][1600/2850] Elapsed 13m 28s (remain 10m 30s) Loss: 0.0201(0.0050) Grad: 17783.0273  LR: 0.000011  
[32m+          Epoch: [3][1700/2850] Elapsed 14m 20s (remain 9m 41s) Loss: 0.0000(0.0050) Grad: 63.3644  LR: 0.000011  
[32m+          Epoch: [3][1800/2850] Elapsed 15m 11s (remain 8m 50s) Loss: 0.0000(0.0050) Grad: 34.8903  LR: 0.000011  
[32m+          Epoch: [3][1900/2850] Elapsed 16m 1s (remain 7m 59s) Loss: 0.0340(0.0051) Grad: 39170.3906  LR: 0.000010  
[32m+          Epoch: [3][2000/2850] Elapsed 16m 51s (remain 7m 9s) Loss: 0.0013(0.0051) Grad: 6373.8276  LR: 0.000010  
[32m+          Epoch: [3][2100/2850] Elapsed 17m 42s (remain 6m 18s) Loss: 0.0000(0.0051) Grad: 235.0192  LR: 0.000010  
[32m+          Epoch: [3][2200/2850] Elapsed 18m 33s (remain 5m 28s) Loss: 0.0000(0.0050) Grad: 149.8196  LR: 0.000010  
[32m+          Epoch: [3][2300/2850] Elapsed 19m 24s (remain 4m 37s) Loss: 0.0004(0.0050) Grad: 1857.5316  LR: 0.000010  
[32m+          Epoch: [3][2400/2850] Elapsed 20m 15s (remain 3m 47s) Loss: 0.0014(0.0050) Grad: 4780.6846  LR: 0.000010  
[32m+          Epoch: [3][2500/2850] Elapsed 21m 5s (remain 2m 56s) Loss: 0.0001(0.0049) Grad: 340.4219  LR: 0.000009  
[32m+          Epoch: [3][2600/2850] Elapsed 21m 56s (remain 2m 6s) Loss: 0.0010(0.0049) Grad: 2878.0281  LR: 0.000009  
[32m+          Epoch: [3][2700/2850] Elapsed 22m 46s (remain 1m 15s) Loss: 0.0000(0.0049) Grad: 28.6621  LR: 0.000009  
[32m+          Epoch: [3][2800/2850] Elapsed 23m 36s (remain 0m 24s) Loss: 0.0024(0.0048) Grad: 6564.8130  LR: 0.000009  
[32m+          Epoch: [3][2849/2850] Elapsed 24m 1s (remain 0m 0s) Loss: 0.0019(0.0048) Grad: 4979.6113  LR: 0.000009  
[32m+          EVAL: [0/725] Elapsed 0m 0s (remain 6m 49s) Loss: 0.0156(0.0156) 
[32m+          EVAL: [100/725] Elapsed 0m 28s (remain 2m 54s) Loss: 0.0011(0.0081) 
[32m+          EVAL: [200/725] Elapsed 0m 55s (remain 2m 25s) Loss: 0.0053(0.0103) 
[32m+          EVAL: [300/725] Elapsed 1m 23s (remain 1m 58s) Loss: 0.0168(0.0089) 
[32m+          EVAL: [400/725] Elapsed 1m 51s (remain 1m 30s) Loss: 0.1402(0.0100) 
[32m+          EVAL: [500/725] Elapsed 2m 20s (remain 1m 2s) Loss: 0.0166(0.0101) 
[32m+          EVAL: [600/725] Elapsed 2m 49s (remain 0m 34s) Loss: 0.0134(0.0098) 
[32m+          EVAL: [700/725] Elapsed 3m 16s (remain 0m 6s) Loss: 0.0000(0.0090) 
[32m+          EVAL: [724/725] Elapsed 3m 23s (remain 0m 0s) Loss: 0.0144(0.0089) 
[32m+          Epoch 3 - avg_train_loss: 0.0048  avg_val_loss: 0.0089  time: 1650s
[32m+          Epoch 3 - Score: 0.8831
[32m+          Epoch 3 - Save Best Score: 0.8831 Model
[32m+          Epoch: [4][0/2850] Elapsed 0m 0s (remain 38m 26s) Loss: 0.0006(0.0006) Grad: 6059.7480  LR: 0.000009  
[32m+          Epoch: [4][100/2850] Elapsed 0m 50s (remain 23m 7s) Loss: 0.0010(0.0047) Grad: 2338.2275  LR: 0.000009  
[32m+          Epoch: [4][200/2850] Elapsed 1m 40s (remain 22m 9s) Loss: 0.0103(0.0039) Grad: 5476.4751  LR: 0.000009  
[32m+          Epoch: [4][300/2850] Elapsed 2m 31s (remain 21m 20s) Loss: 0.0003(0.0041) Grad: 1034.6365  LR: 0.000008  
[32m+          Epoch: [4][400/2850] Elapsed 3m 21s (remain 20m 32s) Loss: 0.0043(0.0039) Grad: 3442.7576  LR: 0.000008  
[32m+          Epoch: [4][500/2850] Elapsed 4m 12s (remain 19m 45s) Loss: 0.0002(0.0041) Grad: 665.4832  LR: 0.000008  
[32m+          Epoch: [4][600/2850] Elapsed 5m 5s (remain 19m 4s) Loss: 0.0000(0.0038) Grad: 18.1574  LR: 0.000008  
[32m+          Epoch: [4][700/2850] Elapsed 5m 57s (remain 18m 15s) Loss: 0.0000(0.0040) Grad: 57.7918  LR: 0.000008  
[32m+          Epoch: [4][800/2850] Elapsed 6m 47s (remain 17m 22s) Loss: 0.0002(0.0039) Grad: 655.9120  LR: 0.000008  
[32m+          Epoch: [4][900/2850] Elapsed 7m 38s (remain 16m 31s) Loss: 0.0188(0.0039) Grad: 16272.4180  LR: 0.000007  
[32m+          Epoch: [4][1000/2850] Elapsed 8m 28s (remain 15m 40s) Loss: 0.0007(0.0038) Grad: 2891.6875  LR: 0.000007  
[32m+          Epoch: [4][1100/2850] Elapsed 9m 19s (remain 14m 48s) Loss: 0.0009(0.0038) Grad: 5141.8452  LR: 0.000007  
[32m+          Epoch: [4][1200/2850] Elapsed 10m 9s (remain 13m 56s) Loss: 0.0060(0.0038) Grad: 10544.7666  LR: 0.000007  
[32m+          Epoch: [4][1300/2850] Elapsed 11m 0s (remain 13m 6s) Loss: 0.0025(0.0038) Grad: 2800.5515  LR: 0.000007  
[32m+          Epoch: [4][1400/2850] Elapsed 11m 50s (remain 12m 14s) Loss: 0.0002(0.0039) Grad: 866.1207  LR: 0.000007  
[32m+          Epoch: [4][1500/2850] Elapsed 12m 40s (remain 11m 23s) Loss: 0.0188(0.0039) Grad: 38984.4414  LR: 0.000007  
[32m+          Epoch: [4][1600/2850] Elapsed 13m 32s (remain 10m 33s) Loss: 0.0010(0.0039) Grad: 3051.2607  LR: 0.000006  
[32m+          Epoch: [4][1700/2850] Elapsed 14m 22s (remain 9m 42s) Loss: 0.0000(0.0038) Grad: 100.1427  LR: 0.000006  
[32m+          Epoch: [4][1800/2850] Elapsed 15m 12s (remain 8m 51s) Loss: 0.0000(0.0038) Grad: 43.5047  LR: 0.000006  
[32m+          Epoch: [4][1900/2850] Elapsed 16m 3s (remain 8m 0s) Loss: 0.0132(0.0038) Grad: 12808.9209  LR: 0.000006  
[32m+          Epoch: [4][2000/2850] Elapsed 16m 53s (remain 7m 9s) Loss: 0.0006(0.0038) Grad: 1345.8685  LR: 0.000006  
[32m+          Epoch: [4][2100/2850] Elapsed 17m 43s (remain 6m 19s) Loss: 0.0008(0.0038) Grad: 1494.9817  LR: 0.000006  
[32m+          Epoch: [4][2200/2850] Elapsed 18m 33s (remain 5m 28s) Loss: 0.0136(0.0038) Grad: 31107.8066  LR: 0.000005  
[32m+          Epoch: [4][2300/2850] Elapsed 19m 23s (remain 4m 37s) Loss: 0.0105(0.0038) Grad: 10428.6807  LR: 0.000005  
[32m+          Epoch: [4][2400/2850] Elapsed 20m 13s (remain 3m 46s) Loss: 0.0000(0.0038) Grad: 9.9520  LR: 0.000005  
[32m+          Epoch: [4][2500/2850] Elapsed 21m 3s (remain 2m 56s) Loss: 0.0001(0.0038) Grad: 299.3885  LR: 0.000005  
[32m+          Epoch: [4][2600/2850] Elapsed 21m 54s (remain 2m 5s) Loss: 0.0002(0.0038) Grad: 475.8388  LR: 0.000005  
[32m+          Epoch: [4][2700/2850] Elapsed 22m 45s (remain 1m 15s) Loss: 0.0002(0.0038) Grad: 1433.3439  LR: 0.000005  
[32m+          Epoch: [4][2800/2850] Elapsed 23m 36s (remain 0m 24s) Loss: 0.0001(0.0038) Grad: 373.4178  LR: 0.000005  
[32m+          Epoch: [4][2849/2850] Elapsed 24m 0s (remain 0m 0s) Loss: 0.0016(0.0038) Grad: 7127.2524  LR: 0.000004  
[32m+          EVAL: [0/725] Elapsed 0m 0s (remain 6m 44s) Loss: 0.0084(0.0084) 
[32m+          EVAL: [100/725] Elapsed 0m 28s (remain 2m 53s) Loss: 0.0005(0.0075) 
[32m+          EVAL: [200/725] Elapsed 0m 56s (remain 2m 26s) Loss: 0.0046(0.0097) 
[32m+          EVAL: [300/725] Elapsed 1m 24s (remain 1m 58s) Loss: 0.0201(0.0084) 
[32m+          EVAL: [400/725] Elapsed 1m 53s (remain 1m 31s) Loss: 0.0661(0.0093) 
[32m+          EVAL: [500/725] Elapsed 2m 21s (remain 1m 3s) Loss: 0.0144(0.0096) 
[32m+          EVAL: [600/725] Elapsed 2m 49s (remain 0m 34s) Loss: 0.0124(0.0093) 
[32m+          EVAL: [700/725] Elapsed 3m 17s (remain 0m 6s) Loss: 0.0002(0.0085) 
[32m+          EVAL: [724/725] Elapsed 3m 23s (remain 0m 0s) Loss: 0.0141(0.0085) 
[32m+          Epoch 4 - avg_train_loss: 0.0038  avg_val_loss: 0.0085  time: 1650s
[32m+          Epoch 4 - Score: 0.8816
[32m+          Epoch: [5][0/2850] Elapsed 0m 0s (remain 37m 37s) Loss: 0.0053(0.0053) Grad: 34856.2305  LR: 0.000004  
[32m+          Epoch: [5][100/2850] Elapsed 0m 51s (remain 23m 8s) Loss: 0.0013(0.0025) Grad: 59431.3047  LR: 0.000004  
[32m+          Epoch: [5][200/2850] Elapsed 1m 40s (remain 22m 9s) Loss: 0.0000(0.0040) Grad: 23.8994  LR: 0.000004  
[32m+          Epoch: [5][300/2850] Elapsed 2m 31s (remain 21m 26s) Loss: 0.0000(0.0032) Grad: 31.3428  LR: 0.000004  
[32m+          Epoch: [5][400/2850] Elapsed 3m 22s (remain 20m 34s) Loss: 0.0034(0.0036) Grad: 34040.6797  LR: 0.000004  
[32m+          Epoch: [5][500/2850] Elapsed 4m 12s (remain 19m 44s) Loss: 0.0000(0.0035) Grad: 155.9857  LR: 0.000004  
[32m+          Epoch: [5][600/2850] Elapsed 5m 3s (remain 18m 56s) Loss: 0.0000(0.0035) Grad: 332.5297  LR: 0.000004  
[32m+          Epoch: [5][700/2850] Elapsed 5m 54s (remain 18m 7s) Loss: 0.0000(0.0034) Grad: 81.8794  LR: 0.000003  
[32m+          Epoch: [5][800/2850] Elapsed 6m 45s (remain 17m 16s) Loss: 0.0002(0.0034) Grad: 1138.2903  LR: 0.000003  
[32m+          Epoch: [5][900/2850] Elapsed 7m 35s (remain 16m 25s) Loss: 0.0015(0.0035) Grad: 3940.5059  LR: 0.000003  
[32m+          Epoch: [5][1000/2850] Elapsed 8m 26s (remain 15m 34s) Loss: 0.0008(0.0034) Grad: 14787.9463  LR: 0.000003  
[32m+          Epoch: [5][1100/2850] Elapsed 9m 18s (remain 14m 47s) Loss: 0.0039(0.0034) Grad: 23742.0547  LR: 0.000003  
[32m+          Epoch: [5][1200/2850] Elapsed 10m 8s (remain 13m 55s) Loss: 0.0025(0.0034) Grad: 3625.0488  LR: 0.000003  
[32m+          Epoch: [5][1300/2850] Elapsed 10m 58s (remain 13m 4s) Loss: 0.0003(0.0034) Grad: 2250.7585  LR: 0.000002  
[32m+          Epoch: [5][1400/2850] Elapsed 11m 48s (remain 12m 12s) Loss: 0.0023(0.0033) Grad: 5886.4883  LR: 0.000002  
[32m+          Epoch: [5][1500/2850] Elapsed 12m 38s (remain 11m 22s) Loss: 0.0027(0.0032) Grad: 11695.7510  LR: 0.000002  
[32m+          Epoch: [5][1600/2850] Elapsed 13m 29s (remain 10m 31s) Loss: 0.0002(0.0033) Grad: 1347.4526  LR: 0.000002  
[32m+          Epoch: [5][1700/2850] Elapsed 14m 20s (remain 9m 41s) Loss: 0.0000(0.0032) Grad: 6.5174  LR: 0.000002  
[32m+          Epoch: [5][1800/2850] Elapsed 15m 11s (remain 8m 50s) Loss: 0.0001(0.0033) Grad: 942.2531  LR: 0.000002  
[32m+          Epoch: [5][1900/2850] Elapsed 16m 1s (remain 7m 59s) Loss: 0.0002(0.0032) Grad: 2280.7012  LR: 0.000001  
[32m+          Epoch: [5][2000/2850] Elapsed 16m 51s (remain 7m 9s) Loss: 0.0002(0.0032) Grad: 2130.5713  LR: 0.000001  
[32m+          Epoch: [5][2100/2850] Elapsed 17m 41s (remain 6m 18s) Loss: 0.0008(0.0032) Grad: 2253.9983  LR: 0.000001  
[32m+          Epoch: [5][2200/2850] Elapsed 18m 32s (remain 5m 28s) Loss: 0.0000(0.0031) Grad: 11.6168  LR: 0.000001  
[32m+          Epoch: [5][2300/2850] Elapsed 19m 22s (remain 4m 37s) Loss: 0.0571(0.0032) Grad: 70703.7266  LR: 0.000001  
[32m+          Epoch: [5][2400/2850] Elapsed 20m 13s (remain 3m 46s) Loss: 0.0026(0.0031) Grad: 10517.9336  LR: 0.000001  
[32m+          Epoch: [5][2500/2850] Elapsed 21m 3s (remain 2m 56s) Loss: 0.0001(0.0031) Grad: 564.6112  LR: 0.000001  
[32m+          Epoch: [5][2600/2850] Elapsed 21m 53s (remain 2m 5s) Loss: 0.0071(0.0031) Grad: 27441.5957  LR: 0.000000  
[32m+          Epoch: [5][2700/2850] Elapsed 22m 45s (remain 1m 15s) Loss: 0.0000(0.0031) Grad: 178.2028  LR: 0.000000  
[32m+          Epoch: [5][2800/2850] Elapsed 23m 36s (remain 0m 24s) Loss: 0.0000(0.0031) Grad: 321.9850  LR: 0.000000  
[32m+          Epoch: [5][2849/2850] Elapsed 24m 0s (remain 0m 0s) Loss: 0.0060(0.0030) Grad: 28835.6445  LR: 0.000000  
[32m+          EVAL: [0/725] Elapsed 0m 0s (remain 7m 7s) Loss: 0.0214(0.0214) 
[32m+          EVAL: [100/725] Elapsed 0m 28s (remain 2m 54s) Loss: 0.0010(0.0090) 
[32m+          EVAL: [200/725] Elapsed 0m 56s (remain 2m 26s) Loss: 0.0035(0.0113) 
[32m+          EVAL: [300/725] Elapsed 1m 23s (remain 1m 58s) Loss: 0.0266(0.0098) 
[32m+          EVAL: [400/725] Elapsed 1m 51s (remain 1m 30s) Loss: 0.0649(0.0108) 
[32m+          EVAL: [500/725] Elapsed 2m 20s (remain 1m 3s) Loss: 0.0153(0.0112) 
[32m+          EVAL: [600/725] Elapsed 2m 49s (remain 0m 34s) Loss: 0.0158(0.0108) 
[32m+          EVAL: [700/725] Elapsed 3m 16s (remain 0m 6s) Loss: 0.0001(0.0099) 
[32m+          EVAL: [724/725] Elapsed 3m 23s (remain 0m 0s) Loss: 0.0155(0.0099) 
[32m+          Epoch 5 - avg_train_loss: 0.0030  avg_val_loss: 0.0099  time: 1649s
[32m+          Epoch 5 - Score: 0.8822
[32m+          Best thres: 0.5, Score: 0.8853
[32m+          Best thres: 0.47812499999999997, Score: 0.8856
[32m+      output 1:
[32m+        output_type: stream
[32m+        name: stderr
[32m+        text:
[32m+          Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias']
[32m+          - This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
[32m+          - This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[32m+      output 2:
[32m+        output_type: stream
[32m+        name: stdout
[32m+        text:
[32m+          Load weight from pretrained
[32m+      output 3:
[32m+        output_type: display_data
[32m+        data:
[32m+          application/vnd.jupyter.widget-view+json:
[32m+            model_id: 88ffb670579e4942b8b82021729e615f
[32m+            version_major: 2
[32m+            version_minor: 0
[32m+          text/plain:   0%|          | 0/2 [00:00<?, ?it/s]
[32m+      output 4:
[32m+        output_type: stream
[32m+        name: stderr
[32m+        text:
[32m+          Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias']
[32m+          - This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
[32m+          - This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[32m+      output 5:
[32m+        output_type: stream
[32m+        name: stdout
[32m+        text:
[32m+          Load weight from pretrained
[32m+      output 6:
[32m+        output_type: display_data
[32m+        data:
[32m+          application/vnd.jupyter.widget-view+json:
[32m+            model_id: d6321657400641ab94ef87d4311e4485
[32m+            version_major: 2
[32m+            version_minor: 0
[32m+          text/plain:   0%|          | 0/2 [00:00<?, ?it/s]
[32m+      output 7:
[32m+        output_type: stream
[32m+        name: stderr
[32m+        text:
[32m+          Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias']
[32m+          - This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
[32m+          - This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[32m+      output 8:
[32m+        output_type: stream
[32m+        name: stdout
[32m+        text:
[32m+          Load weight from pretrained
[32m+      output 9:
[32m+        output_type: display_data
[32m+        data:
[32m+          application/vnd.jupyter.widget-view+json:
[32m+            model_id: 36848173d6ce43ca890e53383da8edf9
[32m+            version_major: 2
[32m+            version_minor: 0
[32m+          text/plain:   0%|          | 0/2 [00:00<?, ?it/s]
[32m+      output 10:
[32m+        output_type: stream
[32m+        name: stderr
[32m+        text:
[32m+          Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias']
[32m+          - This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
[32m+          - This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[32m+      output 11:
[32m+        output_type: stream
[32m+        name: stdout
[32m+        text:
[32m+          Load weight from pretrained
[32m+      output 12:
[32m+        output_type: display_data
[32m+        data:
[32m+          application/vnd.jupyter.widget-view+json:
[32m+            model_id: e67ba1b1a0094142be6c9ca32c8df5da
[32m+            version_major: 2
[32m+            version_minor: 0
[32m+          text/plain:   0%|          | 0/2 [00:00<?, ?it/s]
[32m+      output 13:
[32m+        output_type: stream
[32m+        name: stderr
[32m+        text:
[32m+          Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias']
[32m+          - This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
[32m+          - This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[32m+      output 14:
[32m+        output_type: stream
[32m+        name: stdout
[32m+        text:
[32m+          Load weight from pretrained
[32m+      output 15:
[32m+        output_type: display_data
[32m+        data:
[32m+          application/vnd.jupyter.widget-view+json:
[32m+            model_id: 8665d1fd4a664c8f88c960630debd76b
[32m+            version_major: 2
[32m+            version_minor: 0
[32m+          text/plain:   0%|          | 0/2 [00:00<?, ?it/s]

[0m[34m## deleted /cells/28-29:[0m
[31m-  code cell:
[31m-    execution_count: 18
[31m-    metadata (unknown keys):
[31m-      colab:
[31m-        base_uri: https://localhost:8080/
[31m-      executionInfo:
[31m-        elapsed: 5222
[31m-        status: ok
[31m-        timestamp: 1645861092785
[31m-        user:
[31m-          displayName: Shuhei Goda
[31m-          photoUrl: https://lh3.googleusercontent.com/a/default-user=s64
[31m-          userId: 08246931244224045522
[31m-        user_tz: -540
[31m-      id: rocky-lexington
[31m-      outputId: 226f954d-8f15-43d4-dcc2-a96035789085
[31m-    source:
[31m-      training_args = TrainingArguments(
[31m-          output_dir=CFG.output_dir,
[31m-          overwrite_output_dir=True,
[31m-          num_train_epochs=CFG_For_MLM.epochs,
[31m-          per_device_train_batch_size=CFG_For_MLM.train_batch_size,
[31m-          per_device_eval_batch_size=CFG_For_MLM.eval_batch_size,
[31m-          learning_rate=CFG_For_MLM.learning_rate,
[31m-          gradient_accumulation_steps=CFG_For_MLM.gradient_accum_steps,
[31m-          fp16=CFG_For_MLM.fp16,
[31m-          eval_steps=CFG_For_MLM.eval_steps,
[31m-          save_steps=CFG_For_MLM.eval_steps,
[31m-          evaluation_strategy="steps",
[31m-          save_total_limit=2,
[31m-          metric_for_best_model="eval_loss",
[31m-          greater_is_better=False,
[31m-          load_best_model_at_end=True,
[31m-          prediction_loss_only=True,
[31m-          report_to="none",
[31m-      )
[31m-      trainer = Trainer(
[31m-          model=model,
[31m-          args=training_args,
[31m-          data_collator=data_collator,
[31m-          train_dataset=train_dataset,
[31m-          eval_dataset=valid_dataset,
[31m-      )
[31m-    outputs:
[31m-      output 0:
[31m-        output_type: stream
[31m-        name: stderr
[31m-        text:
[31m-          Using amp half precision backend
[31m-  code cell:
[31m-    execution_count: 19
[31m-    metadata (unknown keys):
[31m-      colab:
[31m-        base_uri: https://localhost:8080/
[31m-        height: 1000
[31m-      executionInfo:
[31m-        elapsed: 24631372
[31m-        status: ok
[31m-        timestamp: 1645913902114
[31m-        user:
[31m-          displayName: Shuhei Goda
[31m-          photoUrl: https://lh3.googleusercontent.com/a/default-user=s64
[31m-          userId: 08246931244224045522
[31m-        user_tz: -540
[31m-      id: LmU2Kx_p3qGh
[31m-      outputId: 00f1762a-aab6-42f5-c255-aa13468f0daa
[31m-    source:
[31m-      trainer.train()
[31m-      trainer.save_model(CFG.output_dir)
[31m-    outputs:
[31m-      output 0:
[31m-        output_type: stream
[31m-        name: stderr
[31m-        text:
[31m-          ***** Running training *****
[31m-            Num examples = 277742
[31m-            Num Epochs = 15
[31m-            Instantaneous batch size per device = 8
[31m-            Total train batch size (w. parallel, distributed & accumulation) = 32
[31m-            Gradient Accumulation steps = 4
[31m-            Total optimization steps = 130185
[31m-      output 1:
[31m-        output_type: display_data
[31m-        data:
[31m-          text/html:
[31m-            
[31m-                <div>
[31m-                  
[31m-                  <progress value='130185' max='130185' style='width:300px; height:20px; vertical-align: middle;'></progress>
[31m-                  [130185/130185 45:13:29, Epoch 14/15]
[31m-                </div>
[31m-                <table border="1" class="dataframe">
[31m-              <thead>
[31m-             <tr style="text-align: left;">
[31m-                  <th>Step</th>
[31m-                  <th>Training Loss</th>
[31m-                  <th>Validation Loss</th>
[31m-                </tr>
[31m-              </thead>
[31m-              <tbody>
[31m-                <tr>
[31m-                  <td>8678</td>
[31m-                  <td>1.745300</td>
[31m-                  <td>1.655970</td>
[31m-                </tr>
[31m-                <tr>
[31m-                  <td>17356</td>
[31m-                  <td>1.529400</td>
[31m-                  <td>1.463020</td>
[31m-                </tr>
[31m-                <tr>
[31m-                  <td>26034</td>
[31m-                  <td>1.436000</td>
[31m-                  <td>1.369631</td>
[31m-                </tr>
[31m-                <tr>
[31m-                  <td>34712</td>
[31m-                  <td>1.392100</td>
[31m-                  <td>1.343328</td>
[31m-                </tr>
[31m-                <tr>
[31m-                  <td>43390</td>
[31m-                  <td>1.312800</td>
[31m-                  <td>1.262357</td>
[31m-                </tr>
[31m-                <tr>
[31m-                  <td>52068</td>
[31m-                  <td>1.273000</td>
[31m-                  <td>1.228980</td>
[31m-                </tr>
[31m-                <tr>
[31m-                  <td>60746</td>
[31m-                  <td>1.253400</td>
[31m-                  <td>1.188702</td>
[31m-                </tr>
[31m-                <tr>
[31m-                  <td>69424</td>
[31m-                  <td>1.212900</td>
[31m-                  <td>1.163085</td>
[31m-                </tr>
[31m-                <tr>
[31m-                  <td>78102</td>
[31m-                  <td>1.178900</td>
[31m-                  <td>1.137680</td>
[31m-                </tr>
[31m-                <tr>
[31m-                  <td>86780</td>
[31m-                  <td>1.166900</td>
[31m-                  <td>1.111447</td>
[31m-                </tr>
[31m-                <tr>
[31m-                  <td>95458</td>
[31m-                  <td>1.134100</td>
[31m-                  <td>1.092211</td>
[31m-                </tr>
[31m-                <tr>
[31m-                  <td>104136</td>
[31m-                  <td>1.126800</td>
[31m-                  <td>1.072779</td>
[31m-                </tr>
[31m-                <tr>
[31m-                  <td>112814</td>
[31m-                  <td>1.122600</td>
[31m-                  <td>1.057548</td>
[31m-                </tr>
[31m-                <tr>
[31m-                  <td>121492</td>
[31m-                  <td>1.075300</td>
[31m-                  <td>1.039983</td>
[31m-                </tr>
[31m-                <tr>
[31m-                  <td>130170</td>
[31m-                  <td>1.081400</td>
[31m-                  <td>1.037232</td>
[31m-                </tr>
[31m-              </tbody>
[31m-            </table><p>
[31m-          text/plain: <IPython.core.display.HTML object>
[31m-      output 2:
[31m-        output_type: stream
[31m-        name: stderr
[31m-        text:
[31m-          ***** Running Evaluation *****
[31m-            Num examples = 277742
[31m-            Batch size = 16
[31m-          Saving model checkpoint to ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-8678
[31m-          Configuration saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-8678/config.json
[31m-          Model weights saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-8678/pytorch_model.bin
[31m-          ***** Running Evaluation *****
[31m-            Num examples = 277742
[31m-            Batch size = 16
[31m-          Saving model checkpoint to ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-17356
[31m-          Configuration saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-17356/config.json
[31m-          Model weights saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-17356/pytorch_model.bin
[31m-          ***** Running Evaluation *****
[31m-            Num examples = 277742
[31m-            Batch size = 16
[31m-          Saving model checkpoint to ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-26034
[31m-          Configuration saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-26034/config.json
[31m-          Model weights saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-26034/pytorch_model.bin
[31m-          Deleting older checkpoint [../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-8678] due to args.save_total_limit
[31m-          ***** Running Evaluation *****
[31m-            Num examples = 277742
[31m-            Batch size = 16
[31m-          Saving model checkpoint to ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-34712
[31m-          Configuration saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-34712/config.json
[31m-          Model weights saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-34712/pytorch_model.bin
[31m-          Deleting older checkpoint [../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-17356] due to args.save_total_limit
[31m-          ***** Running Evaluation *****
[31m-            Num examples = 277742
[31m-            Batch size = 16
[31m-          Saving model checkpoint to ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-43390
[31m-          Configuration saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-43390/config.json
[31m-          Model weights saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-43390/pytorch_model.bin
[31m-          Deleting older checkpoint [../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-26034] due to args.save_total_limit
[31m-          ***** Running Evaluation *****
[31m-            Num examples = 277742
[31m-            Batch size = 16
[31m-          Saving model checkpoint to ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-52068
[31m-          Configuration saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-52068/config.json
[31m-          Model weights saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-52068/pytorch_model.bin
[31m-          Deleting older checkpoint [../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-34712] due to args.save_total_limit
[31m-          ***** Running Evaluation *****
[31m-            Num examples = 277742
[31m-            Batch size = 16
[31m-          Saving model checkpoint to ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-60746
[31m-          Configuration saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-60746/config.json
[31m-          Model weights saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-60746/pytorch_model.bin
[31m-          Deleting older checkpoint [../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-43390] due to args.save_total_limit
[31m-          ***** Running Evaluation *****
[31m-            Num examples = 277742
[31m-            Batch size = 16
[31m-          Saving model checkpoint to ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-69424
[31m-          Configuration saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-69424/config.json
[31m-          Model weights saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-69424/pytorch_model.bin
[31m-          Deleting older checkpoint [../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-52068] due to args.save_total_limit
[31m-          ***** Running Evaluation *****
[31m-            Num examples = 277742
[31m-            Batch size = 16
[31m-          Saving model checkpoint to ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-78102
[31m-          Configuration saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-78102/config.json
[31m-          Model weights saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-78102/pytorch_model.bin
[31m-          Deleting older checkpoint [../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-60746] due to args.save_total_limit
[31m-          ***** Running Evaluation *****
[31m-            Num examples = 277742
[31m-            Batch size = 16
[31m-          Saving model checkpoint to ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-86780
[31m-          Configuration saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-86780/config.json
[31m-          Model weights saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-86780/pytorch_model.bin
[31m-          Deleting older checkpoint [../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-69424] due to args.save_total_limit
[31m-          ***** Running Evaluation *****
[31m-            Num examples = 277742
[31m-            Batch size = 16
[31m-          Saving model checkpoint to ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-95458
[31m-          Configuration saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-95458/config.json
[31m-          Model weights saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-95458/pytorch_model.bin
[31m-          Deleting older checkpoint [../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-78102] due to args.save_total_limit
[31m-          ***** Running Evaluation *****
[31m-            Num examples = 277742
[31m-            Batch size = 16
[31m-          Saving model checkpoint to ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-104136
[31m-          Configuration saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-104136/config.json
[31m-          Model weights saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-104136/pytorch_model.bin
[31m-          Deleting older checkpoint [../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-86780] due to args.save_total_limit
[31m-          ***** Running Evaluation *****
[31m-            Num examples = 277742
[31m-            Batch size = 16
[31m-          Saving model checkpoint to ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-112814
[31m-          Configuration saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-112814/config.json
[31m-          Model weights saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-112814/pytorch_model.bin
[31m-          Deleting older checkpoint [../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-95458] due to args.save_total_limit
[31m-          ***** Running Evaluation *****
[31m-            Num examples = 277742
[31m-            Batch size = 16
[31m-          Saving model checkpoint to ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-121492
[31m-          Configuration saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-121492/config.json
[31m-          Model weights saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-121492/pytorch_model.bin
[31m-          Deleting older checkpoint [../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-104136] due to args.save_total_limit
[31m-          ***** Running Evaluation *****
[31m-            Num examples = 277742
[31m-            Batch size = 16
[31m-          Saving model checkpoint to ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-130170
[31m-          Configuration saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-130170/config.json
[31m-          Model weights saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-130170/pytorch_model.bin
[31m-          Deleting older checkpoint [../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-112814] due to args.save_total_limit
[31m-          
[31m-          
[31m-          Training completed. Do not forget to share your model on huggingface.co/models =)
[31m-          
[31m-          
[31m-          Loading best model from ../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-130170 (score: 1.0372323989868164).
[31m-          Saving model checkpoint to ../output/nbme-score-clinical-patient-notes/nbme-exp022
[31m-          Configuration saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/config.json
[31m-          Model weights saved in ../output/nbme-score-clinical-patient-notes/nbme-exp022/pytorch_model.bin

[0m[34m## deleted /metadata/colab/collapsed_sections:[0m
[31m-  []

[0m[34m## modified /metadata/colab/name:[0m
[31m-  nbme-exp004.ipynb
[32m+  nbme-exp011.ipynb

[0m[34m## deleted /metadata/colab/provenance/0:[0m
[31m-  item[0]:
[31m-    file_id: 1k6U1erE6sYu9U7bfGdYhvEovwiTN0ehD
[31m-    timestamp: 1645625636482

[0m[34m## added /metadata/papermill:[0m
[32m+  default_parameters:
[32m+  duration: 641.572862
[32m+  end_time: 2022-02-27T11:39:50.972497
[32m+  environment_variables:
[32m+  exception: None
[32m+  input_path: __notebook__.ipynb
[32m+  output_path: __notebook__.ipynb
[32m+  parameters:
[32m+  start_time: 2022-02-27T11:29:09.399635
[32m+  version: 2.3.3

[0m[34m## added /metadata/widgets:[0m
[32m+  application/vnd.jupyter.widget-state+json:
[32m+    0260998578564385a0b5b9425a0a5ca1:
[32m+      model_module: @jupyter-widgets/base
[32m+      model_module_version: 1.2.0
[32m+      model_name: LayoutModel
[32m+      state:
[32m+        _model_module: @jupyter-widgets/base
[32m+        _model_module_version: 1.2.0
[32m+        _model_name: LayoutModel
[32m+        _view_count: None
[32m+        _view_module: @jupyter-widgets/base
[32m+        _view_module_version: 1.2.0
[32m+        _view_name: LayoutView
[32m+        align_content: None
[32m+        align_items: None
[32m+        align_self: None
[32m+        border: None
[32m+        bottom: None
[32m+        display: None
[32m+        flex: None
[32m+        flex_flow: None
[32m+        grid_area: None
[32m+        grid_auto_columns: None
[32m+        grid_auto_flow: None
[32m+        grid_auto_rows: None
[32m+        grid_column: None
[32m+        grid_gap: None
[32m+        grid_row: None
[32m+        grid_template_areas: None
[32m+        grid_template_columns: None
[32m+        grid_template_rows: None
[32m+        height: None
[32m+        justify_content: None
[32m+        justify_items: None
[32m+        left: None
[32m+        margin: None
[32m+        max_height: None
[32m+        max_width: None
[32m+        min_height: None
[32m+        min_width: None
[32m+        object_fit: None
[32m+        object_position: None
[32m+        order: None
[32m+        overflow: None
[32m+        overflow_x: None
[32m+        overflow_y: None
[32m+        padding: None
[32m+        right: None
[32m+        top: None
[32m+        visibility: None
[32m+        width: None
[32m+    1141ae38bc6f473aab89db14fa4eeacf:
[32m+      model_module: @jupyter-widgets/controls
[32m+      model_module_version: 1.5.0
[32m+      model_name: HTMLModel
[32m+      state:
[32m+        _dom_classes:
[32m+          []
[32m+        _model_module: @jupyter-widgets/controls
[32m+        _model_module_version: 1.5.0
[32m+        _model_name: HTMLModel
[32m+        _view_count: None
[32m+        _view_module: @jupyter-widgets/controls
[32m+        _view_module_version: 1.5.0
[32m+        _view_name: HTMLView
[32m+        description: 
[32m+        description_tooltip: None
[32m+        layout: IPY_MODEL_ad49cbf6b6e84ccaab873458182f22a1
[32m+        placeholder: ‚Äã
[32m+        style: IPY_MODEL_5375de82ce3a41a8b5550e0a6b4316c1
[32m+        value:  42146/42146 [00:36&lt;00:00, 2019.05it/s]
[32m+    220f78b6119042af8729543465e1234e:
[32m+      model_module: @jupyter-widgets/controls
[32m+      model_module_version: 1.5.0
[32m+      model_name: DescriptionStyleModel
[32m+      state:
[32m+        _model_module: @jupyter-widgets/controls
[32m+        _model_module_version: 1.5.0
[32m+        _model_name: DescriptionStyleModel
[32m+        _view_count: None
[32m+        _view_module: @jupyter-widgets/base
[32m+        _view_module_version: 1.2.0
[32m+        _view_name: StyleView
[32m+        description_width: 
[32m+    25bf78e432e641e0a435dc3626c3ee8a:
[32m+      model_module: @jupyter-widgets/controls
[32m+      model_module_version: 1.5.0
[32m+      model_name: DescriptionStyleModel
[32m+      state:
[32m+        _model_module: @jupyter-widgets/controls
[32m+        _model_module_version: 1.5.0
[32m+        _model_name: DescriptionStyleModel
[32m+        _view_count: None
[32m+        _view_module: @jupyter-widgets/base
[32m+        _view_module_version: 1.2.0
[32m+        _view_name: StyleView
[32m+        description_width: 
[32m+    2e32fee744ef42e0aaa89a7b03e82427:
[32m+      model_module: @jupyter-widgets/base
[32m+      model_module_version: 1.2.0
[32m+      model_name: LayoutModel
[32m+      state:
[32m+        _model_module: @jupyter-widgets/base
[32m+        _model_module_version: 1.2.0
[32m+        _model_name: LayoutModel
[32m+        _view_count: None
[32m+        _view_module: @jupyter-widgets/base
[32m+        _view_module_version: 1.2.0
[32m+        _view_name: LayoutView
[32m+        align_content: None
[32m+        align_items: None
[32m+        align_self: None
[32m+        border: None
[32m+        bottom: None
[32m+        display: None
[32m+        flex: None
[32m+        flex_flow: None
[32m+        grid_area: None
[32m+        grid_auto_columns: None
[32m+        grid_auto_flow: None
[32m+        grid_auto_rows: None
[32m+        grid_column: None
[32m+        grid_gap: None
[32m+        grid_row: None
[32m+        grid_template_areas: None
[32m+        grid_template_columns: None
[32m+        grid_template_rows: None
[32m+        height: None
[32m+        justify_content: None
[32m+        justify_items: None
[32m+        left: None
[32m+        margin: None
[32m+        max_height: None
[32m+        max_width: None
[32m+        min_height: None
[32m+        min_width: None
[32m+        object_fit: None
[32m+        object_position: None
[32m+        order: None
[32m+        overflow: None
[32m+        overflow_x: None
[32m+        overflow_y: None
[32m+        padding: None
[32m+        right: None
[32m+        top: None
[32m+        visibility: None
[32m+        width: None
[32m+    2e3818222bab4603a896be5976cb8409:
[32m+      model_module: @jupyter-widgets/controls
[32m+      model_module_version: 1.5.0
[32m+      model_name: HTMLModel
[32m+      state:
[32m+        _dom_classes:
[32m+          []
[32m+        _model_module: @jupyter-widgets/controls
[32m+        _model_module_version: 1.5.0
[32m+        _model_name: HTMLModel
[32m+        _view_count: None
[32m+        _view_module: @jupyter-widgets/controls
[32m+        _view_module_version: 1.5.0
[32m+        _view_name: HTMLView
[32m+        description: 
[32m+        description_tooltip: None
[32m+        layout: IPY_MODEL_9754a5f1e61d49c8972df40ee9290375
[32m+        placeholder: ‚Äã
[32m+        style: IPY_MODEL_25bf78e432e641e0a435dc3626c3ee8a
[32m+        value:  143/143 [00:00&lt;00:00, 2166.88it/s]
[32m+    40e6583408c447199ff5b94d23601936:
[32m+      model_module: @jupyter-widgets/controls
[32m+      model_module_version: 1.5.0
[32m+      model_name: FloatProgressModel
[32m+      state:
[32m+        _dom_classes:
[32m+          []
[32m+        _model_module: @jupyter-widgets/controls
[32m+        _model_module_version: 1.5.0
[32m+        _model_name: FloatProgressModel
[32m+        _view_count: None
[32m+        _view_module: @jupyter-widgets/controls
[32m+        _view_module_version: 1.5.0
[32m+        _view_name: ProgressView
[32m+        bar_style: success
[32m+        description: 
[32m+        description_tooltip: None
[32m+        layout: IPY_MODEL_2e32fee744ef42e0aaa89a7b03e82427
[32m+        max: 42146
[32m+        min: 0
[32m+        orientation: horizontal
[32m+        style: IPY_MODEL_d51d3aa414db4aa8b0ccae896e671152
[32m+        value: 42146
[32m+    428ca357bd284d199e2558b1f577d79a:
[32m+      model_module: @jupyter-widgets/controls
[32m+      model_module_version: 1.5.0
[32m+      model_name: DescriptionStyleModel
[32m+      state:
[32m+        _model_module: @jupyter-widgets/controls
[32m+        _model_module_version: 1.5.0
[32m+        _model_name: DescriptionStyleModel
[32m+        _view_count: None
[32m+        _view_module: @jupyter-widgets/base
[32m+        _view_module_version: 1.2.0
[32m+        _view_name: StyleView
[32m+        description_width: 
[32m+    47b8a7f3d0544d79b30ad02e4222082e:
[32m+      model_module: @jupyter-widgets/base
[32m+      model_module_version: 1.2.0
[32m+      model_name: LayoutModel
[32m+      state:
[32m+        _model_module: @jupyter-widgets/base
[32m+        _model_module_version: 1.2.0
[32m+        _model_name: LayoutModel
[32m+        _view_count: None
[32m+        _view_module: @jupyter-widgets/base
[32m+        _view_module_version: 1.2.0
[32m+        _view_name: LayoutView
[32m+        align_content: None
[32m+        align_items: None
[32m+        align_self: None
[32m+        border: None
[32m+        bottom: None
[32m+        display: None
[32m+        flex: None
[32m+        flex_flow: None
[32m+        grid_area: None
[32m+        grid_auto_columns: None
[32m+        grid_auto_flow: None
[32m+        grid_auto_rows: None
[32m+        grid_column: None
[32m+        grid_gap: None
[32m+        grid_row: None
[32m+        grid_template_areas: None
[32m+        grid_template_columns: None
[32m+        grid_template_rows: None
[32m+        height: None
[32m+        justify_content: None
[32m+        justify_items: None
[32m+        left: None
[32m+        margin: None
[32m+        max_height: None
[32m+        max_width: None
[32m+        min_height: None
[32m+        min_width: None
[32m+        object_fit: None
[32m+        object_position: None
[32m+        order: None
[32m+        overflow: None
[32m+        overflow_x: None
[32m+        overflow_y: None
[32m+        padding: None
[32m+        right: None
[32m+        top: None
[32m+        visibility: None
[32m+        width: None
[32m+    5375de82ce3a41a8b5550e0a6b4316c1:
[32m+      model_module: @jupyter-widgets/controls
[32m+      model_module_version: 1.5.0
[32m+      model_name: DescriptionStyleModel
[32m+      state:
[32m+        _model_module: @jupyter-widgets/controls
[32m+        _model_module_version: 1.5.0
[32m+        _model_name: DescriptionStyleModel
[32m+        _view_count: None
[32m+        _view_module: @jupyter-widgets/base
[32m+        _view_module_version: 1.2.0
[32m+        _view_name: StyleView
[32m+        description_width: 
[32m+    5e66444e9c714134bd2765cb3b6d1f15:
[32m+      model_module: @jupyter-widgets/base
[32m+      model_module_version: 1.2.0
[32m+      model_name: LayoutModel
[32m+      state:
[32m+        _model_module: @jupyter-widgets/base
[32m+        _model_module_version: 1.2.0
[32m+        _model_name: LayoutModel
[32m+        _view_count: None
[32m+        _view_module: @jupyter-widgets/base
[32m+        _view_module_version: 1.2.0
[32m+        _view_name: LayoutView
[32m+        align_content: None
[32m+        align_items: None
[32m+        align_self: None
[32m+        border: None
[32m+        bottom: None
[32m+        display: None
[32m+        flex: None
[32m+        flex_flow: None
[32m+        grid_area: None
[32m+        grid_auto_columns: None
[32m+        grid_auto_flow: None
[32m+        grid_auto_rows: None
[32m+        grid_column: None
[32m+        grid_gap: None
[32m+        grid_row: None
[32m+        grid_template_areas: None
[32m+        grid_template_columns: None
[32m+        grid_template_rows: None
[32m+        height: None
[32m+        justify_content: None
[32m+        justify_items: None
[32m+        left: None
[32m+        margin: None
[32m+        max_height: None
[32m+        max_width: None
[32m+        min_height: None
[32m+        min_width: None
[32m+        object_fit: None
[32m+        object_position: None
[32m+        order: None
[32m+        overflow: None
[32m+        overflow_x: None
[32m+        overflow_y: None
[32m+        padding: None
[32m+        right: None
[32m+        top: None
[32m+        visibility: None
[32m+        width: None
[32m+    6b04b019813e458080f02bc9111433a6:
[32m+      model_module: @jupyter-widgets/controls
[32m+      model_module_version: 1.5.0
[32m+      model_name: FloatProgressModel
[32m+      state:
[32m+        _dom_classes:
[32m+          []
[32m+        _model_module: @jupyter-widgets/controls
[32m+        _model_module_version: 1.5.0
[32m+        _model_name: FloatProgressModel
[32m+        _view_count: None
[32m+        _view_module: @jupyter-widgets/controls
[32m+        _view_module_version: 1.5.0
[32m+        _view_name: ProgressView
[32m+        bar_style: success
[32m+        description: 
[32m+        description_tooltip: None
[32m+        layout: IPY_MODEL_7f1d7796e2174485a0d1b1e9a71d7ade
[32m+        max: 143
[32m+        min: 0
[32m+        orientation: horizontal
[32m+        style: IPY_MODEL_e72cad76f875451a8e2479e2df237575
[32m+        value: 143
[32m+    7f1d7796e2174485a0d1b1e9a71d7ade:
[32m+      model_module: @jupyter-widgets/base
[32m+      model_module_version: 1.2.0
[32m+      model_name: LayoutModel
[32m+      state:
[32m+        _model_module: @jupyter-widgets/base
[32m+        _model_module_version: 1.2.0
[32m+        _model_name: LayoutModel
[32m+        _view_count: None
[32m+        _view_module: @jupyter-widgets/base
[32m+        _view_module_version: 1.2.0
[32m+        _view_name: LayoutView
[32m+        align_content: None
[32m+        align_items: None
[32m+        align_self: None
[32m+        border: None
[32m+        bottom: None
[32m+        display: None
[32m+        flex: None
[32m+        flex_flow: None
[32m+        grid_area: None
[32m+        grid_auto_columns: None
[32m+        grid_auto_flow: None
[32m+        grid_auto_rows: None
[32m+        grid_column: None
[32m+        grid_gap: None
[32m+        grid_row: None
[32m+        grid_template_areas: None
[32m+        grid_template_columns: None
[32m+        grid_template_rows: None
[32m+        height: None
[32m+        justify_content: None
[32m+        justify_items: None
[32m+        left: None
[32m+        margin: None
[32m+        max_height: None
[32m+        max_width: None
[32m+        min_height: None
[32m+        min_width: None
[32m+        object_fit: None
[32m+        object_position: None
[32m+        order: None
[32m+        overflow: None
[32m+        overflow_x: None
[32m+        overflow_y: None
[32m+        padding: None
[32m+        right: None
[32m+        top: None
[32m+        visibility: None
[32m+        width: None
[32m+    8a503d1abd884514a1e23101e03c6781:
[32m+      model_module: @jupyter-widgets/controls
[32m+      model_module_version: 1.5.0
[32m+      model_name: HBoxModel
[32m+      state:
[32m+        _dom_classes:
[32m+          []
[32m+        _model_module: @jupyter-widgets/controls
[32m+        _model_module_version: 1.5.0
[32m+        _model_name: HBoxModel
[32m+        _view_count: None
[32m+        _view_module: @jupyter-widgets/controls
[32m+        _view_module_version: 1.5.0
[32m+        _view_name: HBoxView
[32m+        box_style: 
[32m+        children:
[32m+          item[0]: IPY_MODEL_e06e5e9eb0414b6fad63bdc99b44a313
[32m+          item[1]: IPY_MODEL_6b04b019813e458080f02bc9111433a6
[32m+          item[2]: IPY_MODEL_2e3818222bab4603a896be5976cb8409
[32m+        layout: IPY_MODEL_eeb468dbb94943fcb30219d4dd98fcab
[32m+    9754a5f1e61d49c8972df40ee9290375:
[32m+      model_module: @jupyter-widgets/base
[32m+      model_module_version: 1.2.0
[32m+      model_name: LayoutModel
[32m+      state:
[32m+        _model_module: @jupyter-widgets/base
[32m+        _model_module_version: 1.2.0
[32m+        _model_name: LayoutModel
[32m+        _view_count: None
[32m+        _view_module: @jupyter-widgets/base
[32m+        _view_module_version: 1.2.0
[32m+        _view_name: LayoutView
[32m+        align_content: None
[32m+        align_items: None
[32m+        align_self: None
[32m+        border: None
[32m+        bottom: None
[32m+        display: None
[32m+        flex: None
[32m+        flex_flow: None
[32m+        grid_area: None
[32m+        grid_auto_columns: None
[32m+        grid_auto_flow: None
[32m+        grid_auto_rows: None
[32m+        grid_column: None
[32m+        grid_gap: None
[32m+        grid_row: None
[32m+        grid_template_areas: None
[32m+        grid_template_columns: None
[32m+        grid_template_rows: None
[32m+        height: None
[32m+        justify_content: None
[32m+        justify_items: None
[32m+        left: None
[32m+        margin: None
[32m+        max_height: None
[32m+        max_width: None
[32m+        min_height: None
[32m+        min_width: None
[32m+        object_fit: None
[32m+        object_position: None
[32m+        order: None
[32m+        overflow: None
[32m+        overflow_x: None
[32m+        overflow_y: None
[32m+        padding: None
[32m+        right: None
[32m+        top: None
[32m+        visibility: None
[32m+        width: None
[32m+    a31c60ff4dab48e08d2ef9293d85df6d:
[32m+      model_module: @jupyter-widgets/controls
[32m+      model_module_version: 1.5.0
[32m+      model_name: HBoxModel
[32m+      state:
[32m+        _dom_classes:
[32m+          []
[32m+        _model_module: @jupyter-widgets/controls
[32m+        _model_module_version: 1.5.0
[32m+        _model_name: HBoxModel
[32m+        _view_count: None
[32m+        _view_module: @jupyter-widgets/controls
[32m+        _view_module_version: 1.5.0
[32m+        _view_name: HBoxView
[32m+        box_style: 
[32m+        children:
[32m+          item[0]: IPY_MODEL_c40d970496ff447a8c0b80d787b07a4d
[32m+          item[1]: IPY_MODEL_40e6583408c447199ff5b94d23601936
[32m+          item[2]: IPY_MODEL_1141ae38bc6f473aab89db14fa4eeacf
[32m+        layout: IPY_MODEL_47b8a7f3d0544d79b30ad02e4222082e
[32m+    ad49cbf6b6e84ccaab873458182f22a1:
[32m+      model_module: @jupyter-widgets/base
[32m+      model_module_version: 1.2.0
[32m+      model_name: LayoutModel
[32m+      state:
[32m+        _model_module: @jupyter-widgets/base
[32m+        _model_module_version: 1.2.0
[32m+        _model_name: LayoutModel
[32m+        _view_count: None
[32m+        _view_module: @jupyter-widgets/base
[32m+        _view_module_version: 1.2.0
[32m+        _view_name: LayoutView
[32m+        align_content: None
[32m+        align_items: None
[32m+        align_self: None
[32m+        border: None
[32m+        bottom: None
[32m+        display: None
[32m+        flex: None
[32m+        flex_flow: None
[32m+        grid_area: None
[32m+        grid_auto_columns: None
[32m+        grid_auto_flow: None
[32m+        grid_auto_rows: None
[32m+        grid_column: None
[32m+        grid_gap: None
[32m+        grid_row: None
[32m+        grid_template_areas: None
[32m+        grid_template_columns: None
[32m+        grid_template_rows: None
[32m+        height: None
[32m+        justify_content: None
[32m+        justify_items: None
[32m+        left: None
[32m+        margin: None
[32m+        max_height: None
[32m+        max_width: None
[32m+        min_height: None
[32m+        min_width: None
[32m+        object_fit: None
[32m+        object_position: None
[32m+        order: None
[32m+        overflow: None
[32m+        overflow_x: None
[32m+        overflow_y: None
[32m+        padding: None
[32m+        right: None
[32m+        top: None
[32m+        visibility: None
[32m+        width: None
[32m+    c40d970496ff447a8c0b80d787b07a4d:
[32m+      model_module: @jupyter-widgets/controls
[32m+      model_module_version: 1.5.0
[32m+      model_name: HTMLModel
[32m+      state:
[32m+        _dom_classes:
[32m+          []
[32m+        _model_module: @jupyter-widgets/controls
[32m+        _model_module_version: 1.5.0
[32m+        _model_name: HTMLModel
[32m+        _view_count: None
[32m+        _view_module: @jupyter-widgets/controls
[32m+        _view_module_version: 1.5.0
[32m+        _view_name: HTMLView
[32m+        description: 
[32m+        description_tooltip: None
[32m+        layout: IPY_MODEL_0260998578564385a0b5b9425a0a5ca1
[32m+        placeholder: ‚Äã
[32m+        style: IPY_MODEL_428ca357bd284d199e2558b1f577d79a
[32m+        value: 100%
[32m+    d51d3aa414db4aa8b0ccae896e671152:
[32m+      model_module: @jupyter-widgets/controls
[32m+      model_module_version: 1.5.0
[32m+      model_name: ProgressStyleModel
[32m+      state:
[32m+        _model_module: @jupyter-widgets/controls
[32m+        _model_module_version: 1.5.0
[32m+        _model_name: ProgressStyleModel
[32m+        _view_count: None
[32m+        _view_module: @jupyter-widgets/base
[32m+        _view_module_version: 1.2.0
[32m+        _view_name: StyleView
[32m+        bar_color: None
[32m+        description_width: 
[32m+    e06e5e9eb0414b6fad63bdc99b44a313:
[32m+      model_module: @jupyter-widgets/controls
[32m+      model_module_version: 1.5.0
[32m+      model_name: HTMLModel
[32m+      state:
[32m+        _dom_classes:
[32m+          []
[32m+        _model_module: @jupyter-widgets/controls
[32m+        _model_module_version: 1.5.0
[32m+        _model_name: HTMLModel
[32m+        _view_count: None
[32m+        _view_module: @jupyter-widgets/controls
[32m+        _view_module_version: 1.5.0
[32m+        _view_name: HTMLView
[32m+        description: 
[32m+        description_tooltip: None
[32m+        layout: IPY_MODEL_5e66444e9c714134bd2765cb3b6d1f15
[32m+        placeholder: ‚Äã
[32m+        style: IPY_MODEL_220f78b6119042af8729543465e1234e
[32m+        value: 100%
[32m+    e72cad76f875451a8e2479e2df237575:
[32m+      model_module: @jupyter-widgets/controls
[32m+      model_module_version: 1.5.0
[32m+      model_name: ProgressStyleModel
[32m+      state:
[32m+        _model_module: @jupyter-widgets/controls
[32m+        _model_module_version: 1.5.0
[32m+        _model_name: ProgressStyleModel
[32m+        _view_count: None
[32m+        _view_module: @jupyter-widgets/base
[32m+        _view_module_version: 1.2.0
[32m+        _view_name: StyleView
[32m+        bar_color: None
[32m+        description_width: 
[32m+    eeb468dbb94943fcb30219d4dd98fcab:
[32m+      model_module: @jupyter-widgets/base
[32m+      model_module_version: 1.2.0
[32m+      model_name: LayoutModel
[32m+      state:
[32m+        _model_module: @jupyter-widgets/base
[32m+        _model_module_version: 1.2.0
[32m+        _model_name: LayoutModel
[32m+        _view_count: None
[32m+        _view_module: @jupyter-widgets/base
[32m+        _view_module_version: 1.2.0
[32m+        _view_name: LayoutView
[32m+        align_content: None
[32m+        align_items: None
[32m+        align_self: None
[32m+        border: None
[32m+        bottom: None
[32m+        display: None
[32m+        flex: None
[32m+        flex_flow: None
[32m+        grid_area: None
[32m+        grid_auto_columns: None
[32m+        grid_auto_flow: None
[32m+        grid_auto_rows: None
[32m+        grid_column: None
[32m+        grid_gap: None
[32m+        grid_row: None
[32m+        grid_template_areas: None
[32m+        grid_template_columns: None
[32m+        grid_template_rows: None
[32m+        height: None
[32m+        justify_content: None
[32m+        justify_items: None
[32m+        left: None
[32m+        margin: None
[32m+        max_height: None
[32m+        max_width: None
[32m+        min_height: None
[32m+        min_width: None
[32m+        object_fit: None
[32m+        object_position: None
[32m+        order: None
[32m+        overflow: None
[32m+        overflow_x: None
[32m+        overflow_y: None
[32m+        padding: None
[32m+        right: None
[32m+        top: None
[32m+        visibility: None
[32m+        width: None

[0m