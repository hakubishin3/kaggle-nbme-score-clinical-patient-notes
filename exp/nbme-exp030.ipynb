{"cells":[{"cell_type":"markdown","id":"clinical-cement","metadata":{"id":"clinical-cement"},"source":["## References"]},{"cell_type":"markdown","id":"exterior-exploration","metadata":{"id":"exterior-exploration"},"source":["- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"]},{"cell_type":"markdown","id":"recent-annex","metadata":{"id":"recent-annex"},"source":["## Configurations"]},{"cell_type":"code","execution_count":1,"id":"three-principle","metadata":{"id":"three-principle","executionInfo":{"status":"ok","timestamp":1646744067281,"user_tz":-540,"elapsed":10,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["EXP_NAME = \"nbme-exp030\"\n","ENV = \"colab\"\n","DEBUG_MODE = False\n","SUBMISSION_MODE = False"]},{"cell_type":"code","execution_count":2,"id":"curious-gambling","metadata":{"id":"curious-gambling","executionInfo":{"status":"ok","timestamp":1646744067282,"user_tz":-540,"elapsed":10,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class CFG:\n","    env=ENV\n","    exp_name=EXP_NAME\n","    debug=DEBUG_MODE\n","    submission=SUBMISSION_MODE\n","    apex=True\n","    input_dir=None\n","    output_dir=None\n","    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n","    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n","    competition_name=\"nbme-score-clinical-patient-notes\"\n","    id_col=\"id\"\n","    target_col=\"location\"\n","    pretrained_model_name=\"microsoft/deberta-v3-large\"\n","    tokenizer=None\n","    max_len=None\n","    output_dim=1\n","    dropout=0.2\n","    num_workers=4\n","    batch_size=4\n","    lr=2e-5\n","    betas=(0.9, 0.98)\n","    weight_decay=0.1\n","    num_warmup_steps_rate=0.1\n","    batch_scheduler=True\n","    epochs=5\n","    n_fold=5\n","    train_fold=[0, 1, 2, 3, 4]\n","    seed=71\n","    gradient_accumulation_steps=2\n","    max_grad_norm=1000\n","    print_freq=100\n","    train=True\n","    inference=True"]},{"cell_type":"code","execution_count":3,"id":"spectacular-circular","metadata":{"id":"spectacular-circular","executionInfo":{"status":"ok","timestamp":1646744067282,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.train_fold = [0, 1]\n","\n","if CFG.submission:\n","    CFG.train = False\n","    CFG.inference = True"]},{"cell_type":"markdown","id":"different-director","metadata":{"id":"different-director"},"source":["## Directory Settings"]},{"cell_type":"code","execution_count":4,"id":"architectural-landing","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"architectural-landing","outputId":"d4e78c13-3094-47a2-8617-419320f667c9","executionInfo":{"status":"ok","timestamp":1646744108056,"user_tz":-540,"elapsed":40783,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["colab\n","Mounted at /content/drive\n","Collecting transformers\n","  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 4.3 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 6.2 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 48.1 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 54.6 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 36.2 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.17.0\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 4.3 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n"]}],"source":["import sys\n","from pathlib import Path\n","\n","\n","print(CFG.env)\n","if CFG.env == \"colab\":\n","    # colab環境\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","    # install packages\n","    !pip install transformers\n","    !pip install sentencepiece\n","\n","elif CFG.env == \"local\":\n","    # ローカルサーバ\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","\n","elif CFG.env == \"kaggle\":\n","    # kaggle環境\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./\")"]},{"cell_type":"code","execution_count":5,"id":"disciplinary-recall","metadata":{"id":"disciplinary-recall","executionInfo":{"status":"ok","timestamp":1646744118029,"user_tz":-540,"elapsed":9980,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n","# This must be done before importing transformers\n","import shutil\n","from pathlib import Path\n","\n","if CFG.env == \"colab\":\n","    input_dir = Path(\"./drive/MyDrive/00.kaggle/input/deberta-v2-3-fast-tokenizer\")\n","    transformers_path = Path(\"/usr/local/lib/python3.7/dist-packages/transformers\")\n","else:\n","    input_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n","    transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n","\n","convert_file = input_dir / \"convert_slow_tokenizer.py\"\n","conversion_path = transformers_path/convert_file.name\n","\n","if conversion_path.exists():\n","    conversion_path.unlink()\n","\n","shutil.copy(convert_file, transformers_path)\n","deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n","\n","for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n","    filepath = deberta_v2_path/filename\n","    if filepath.exists():\n","        filepath.unlink()\n","\n","    shutil.copy(input_dir/filename, filepath)\n","    \n","    \n","from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast"]},{"cell_type":"code","execution_count":6,"id":"systematic-vehicle","metadata":{"id":"systematic-vehicle","executionInfo":{"status":"ok","timestamp":1646744121186,"user_tz":-540,"elapsed":3161,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["import gc\n","import os\n","import ast\n","import time\n","import math\n","import random\n","import itertools\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","from scipy.optimize import minimize\n","from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as T\n","from torchvision.io import read_image\n","from torch.utils.data import DataLoader, Dataset\n","\n","from transformers import AutoModelForMaskedLM\n","from transformers import BartModel,BertModel,BertTokenizer\n","from transformers import DebertaModel,DebertaTokenizer\n","from transformers import RobertaModel,RobertaTokenizer\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","id":"earned-anatomy","metadata":{"id":"earned-anatomy"},"source":["## Utilities"]},{"cell_type":"code","execution_count":7,"id":"consistent-rescue","metadata":{"id":"consistent-rescue","executionInfo":{"status":"ok","timestamp":1646744121186,"user_tz":-540,"elapsed":4,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on binary arrays.\n","\n","    Args:\n","        preds (list of lists of ints): Predictions.\n","        truths (list of lists of ints): Ground truths.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    # Micro : aggregating over all instances\n","    preds = np.concatenate(preds)\n","    truths = np.concatenate(truths)\n","    return f1_score(truths, preds)\n","\n","\n","def spans_to_binary(spans, length=None):\n","    \"\"\"\n","    Converts spans to a binary array indicating whether each character is in the span.\n","\n","    Args:\n","        spans (list of lists of two ints): Spans.\n","\n","    Returns:\n","        np array [length]: Binarized spans.\n","    \"\"\"\n","    length = np.max(spans) if length is None else length\n","    binary = np.zeros(length)\n","    for start, end in spans:\n","        binary[start:end] = 1\n","    return binary\n","\n","\n","def span_micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on spans.\n","\n","    Args:\n","        preds (list of lists of two ints): Prediction spans.\n","        truths (list of lists of two ints): Ground truth spans.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    bin_preds = []\n","    bin_truths = []\n","    for pred, truth in zip(preds, truths):\n","        if not len(pred) and not len(truth):\n","            continue\n","        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n","        bin_preds.append(spans_to_binary(pred, length))\n","        bin_truths.append(spans_to_binary(truth, length))\n","    return micro_f1(bin_preds, bin_truths)\n","\n","\n","def get_score(y_true, y_pred):\n","    score = span_micro_f1(y_true, y_pred)\n","    return score"]},{"cell_type":"code","execution_count":8,"id":"reflected-radio","metadata":{"id":"reflected-radio","executionInfo":{"status":"ok","timestamp":1646744121549,"user_tz":-540,"elapsed":366,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def create_labels_for_scoring(df):\n","    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n","    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n","    for i in range(len(df)):\n","        lst = df.loc[i, \"location\"]\n","        if lst:\n","            new_lst = \";\".join(lst)\n","            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n","\n","    # create labels\n","    truths = []\n","    for location_list in df[\"location_for_create_labels\"].values:\n","        truth = []\n","        if len(location_list) > 0:\n","            location = location_list[0]\n","            for loc in [s.split() for s in location.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                truth.append([start, end])\n","        truths.append(truth)\n","\n","    return truths\n","\n","\n","def get_char_probs(texts, token_probs, tokenizer):\n","    res = [np.zeros(len(t)) for t in texts]\n","    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n","        encoded = tokenizer(\n","            text=text,\n","            max_length=CFG.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n","            start, end = offset_mapping\n","            res[i][start:end] = pred\n","    return res\n","\n","\n","def get_predicted_location_str(char_probs, th=0.5):\n","    results = []\n","    for char_prob in char_probs:\n","        result = np.where(char_prob >= th)[0] + 1\n","        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n","        result = [f\"{min(r)} {max(r)}\" for r in result]\n","        result = \";\".join(result)\n","        results.append(result)\n","    return results\n","\n","\n","def get_predictions(results):\n","    predictions = []\n","    for result in results:\n","        prediction = []\n","        if result != \"\":\n","            for loc in [s.split() for s in result.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                prediction.append([start, end])\n","        predictions.append(prediction)\n","    return predictions\n","\n","\n","def scoring(df, th=0.5):\n","    labels = create_labels_for_scoring(df)\n","\n","    token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n","    char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n","    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n","    preds = get_predictions(predicted_location_str)\n","\n","    score = get_score(labels, preds)\n","    return score\n","\n","\n","def get_best_thres(oof_df):\n","    def f1_opt(x):\n","        return -1 * scoring(oof_df, th=x)\n","\n","    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n","    return best_thres"]},{"cell_type":"code","execution_count":9,"id":"noted-eating","metadata":{"id":"noted-eating","executionInfo":{"status":"ok","timestamp":1646744121550,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return \"%dm %ds\" % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":10,"id":"innocent-america","metadata":{"id":"innocent-america","executionInfo":{"status":"ok","timestamp":1646744121551,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["seed_everything()"]},{"cell_type":"markdown","id":"chubby-jefferson","metadata":{"id":"chubby-jefferson"},"source":["## Data Loading"]},{"cell_type":"code","execution_count":11,"id":"aerial-valley","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aerial-valley","outputId":"544cc8d3-6cf3-422f-f4a5-fbf20818499b","executionInfo":{"status":"ok","timestamp":1646744124954,"user_tz":-540,"elapsed":3408,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14300, 6), (143, 3), (42146, 3), (5, 4))"]},"metadata":{},"execution_count":11}],"source":["train = pd.read_csv(CFG.input_dir / \"train.csv\")\n","features = pd.read_csv(CFG.input_dir / \"features.csv\")\n","patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n","test = pd.read_csv(CFG.input_dir / \"test.csv\")\n","\n","train.shape, features.shape, patient_notes.shape, test.shape"]},{"cell_type":"code","execution_count":12,"id":"present-legislation","metadata":{"id":"present-legislation","executionInfo":{"status":"ok","timestamp":1646744124955,"user_tz":-540,"elapsed":5,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["if CFG.debug:\n","    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n","    print(train.shape)"]},{"cell_type":"markdown","id":"continued-notebook","metadata":{"id":"continued-notebook"},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":13,"id":"pending-rotation","metadata":{"id":"pending-rotation","executionInfo":{"status":"ok","timestamp":1646744124955,"user_tz":-540,"elapsed":4,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def preprocess_features(features):\n","    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n","    return features\n","\n","\n","features = preprocess_features(features)"]},{"cell_type":"code","execution_count":14,"id":"raising-anaheim","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"raising-anaheim","outputId":"2a79e889-bbdc-4f20-c0d4-f79d8a655149","executionInfo":{"status":"ok","timestamp":1646744125475,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14300, 8), (5, 6))"]},"metadata":{},"execution_count":14}],"source":["train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","\n","train.shape, test.shape"]},{"cell_type":"code","execution_count":15,"id":"legislative-partition","metadata":{"id":"legislative-partition","executionInfo":{"status":"ok","timestamp":1646744125860,"user_tz":-540,"elapsed":390,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n","train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"]},{"cell_type":"code","execution_count":16,"id":"indoor-humidity","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"id":"indoor-humidity","outputId":"e6d658af-911c-41d7-8132-234526e60b4d","executionInfo":{"status":"ok","timestamp":1646744125861,"user_tz":-540,"elapsed":13,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["0    4399\n","1    8181\n","2    1296\n","3     287\n","4      99\n","5      27\n","6       9\n","7       1\n","8       1\n","Name: annotation_length, dtype: int64"]},"metadata":{}}],"source":["train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n","display(train['annotation_length'].value_counts().sort_index())"]},{"cell_type":"markdown","id":"fresh-saskatchewan","metadata":{"id":"fresh-saskatchewan"},"source":["## CV split"]},{"cell_type":"code","execution_count":17,"id":"assumed-variation","metadata":{"id":"assumed-variation","executionInfo":{"status":"ok","timestamp":1646744125861,"user_tz":-540,"elapsed":11,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def get_groupkfold(df, group_name):\n","    groups = df[group_name].unique()\n","\n","    kf = KFold(\n","        n_splits=CFG.n_fold,\n","        shuffle=True,\n","        random_state=CFG.seed,\n","    )\n","    folds_ids = []\n","    for i_fold, (_, val_group_idx) in enumerate(kf.split(groups)):\n","        val_group = groups[val_group_idx]\n","        is_val = df[group_name].isin(val_group)\n","        val_idx = df[is_val].index\n","        df.loc[val_idx, \"fold\"] = int(i_fold)\n","\n","    df[\"fold\"] = df[\"fold\"].astype(int)\n","    return df"]},{"cell_type":"code","execution_count":18,"id":"embedded-wheat","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":142},"id":"embedded-wheat","outputId":"4b2ee751-2947-4453-e2b6-c4b53dc9e708","executionInfo":{"status":"ok","timestamp":1646744125862,"user_tz":-540,"elapsed":11,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["fold\n","0    2902\n","1    2894\n","2    2813\n","3    2791\n","4    2900\n","dtype: int64"]},"metadata":{}}],"source":["train = get_groupkfold(train, \"pn_num\")\n","display(train.groupby(\"fold\").size())"]},{"cell_type":"markdown","id":"single-gnome","metadata":{"id":"single-gnome"},"source":["## Setup tokenizer"]},{"cell_type":"code","execution_count":19,"id":"international-junior","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":148,"referenced_widgets":["4b7d943344f243b98e7a7d8ec4fa4e07","0ca6df7edd2b428db49cb300593f32f8","ab79466679dd40428cb7b1d4455b794d","34a057ee78bb45629f6bf81808d5f301","4882ff75a58b4919b0d6c9faedab896b","556850faa8ab498484f277c55ab4157c","fc7867840e4b431a8a75e235b25d931d","cfa36ac2690c47429743d9511338b9f0","a8823f24ffa046caa5fb2a88025c641b","7ca7337b55094343b4ad3a30c703af50","206e7187463c4681bb085de8c1919e1a","9dcc2a0264fb40bfbc19c614e9ad5612","a08cef401044437f8a7c92969e359296","aed60a6f514a48468173b02441920989","68a9117fa12c4410996ce5364065ad24","8a5e5faea7b84463a139555dc9ab5c30","de95c0127e334178a79b42330cc97e65","d46fd266de0d43bdb0b40e83fc7a420b","d7572660715c4963babc376fdeec9083","3860c619873a4c73b6f7f5acdf1469c4","2ef35c30fc5f4d08b0ec65a8954e3907","6dc8c533b0cc4187b00959ab75347b09","fb1512ff22f242faaef107942928e711","c2b975a94d7a439bb84dba22a14bb280","b6adad27b75d4fdd8117941993532b83","afa8a789d28540499c1c9d5d2b9831e9","2dace65f8dd045e59ce5c8ef0037b947","c53743cc418e4cb0b8c5db3ffc035323","9a601becadb7429eadb73cf88cad5620","c829bf7571f24690a7651fd1a67331a3","8006419d65564fae8a9a1a16010cfaa3","670d34a6a17d43c294f2eb9def9d950e","f9d70075da324627bead6ea379c8815e"]},"id":"international-junior","outputId":"edacfca2-bb8c-4749-a74d-f86dc25fe0ce","executionInfo":{"status":"ok","timestamp":1646744135445,"user_tz":-540,"elapsed":9593,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4b7d943344f243b98e7a7d8ec4fa4e07","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/2.35M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9dcc2a0264fb40bfbc19c614e9ad5612","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fb1512ff22f242faaef107942928e711","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/580 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["if CFG.submission:\n","    tokenizer = DebertaV2TokenizerFast.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n","else:\n","    tokenizer = DebertaV2TokenizerFast.from_pretrained(CFG.pretrained_model_name)\n","    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n","\n","CFG.tokenizer = tokenizer"]},{"cell_type":"markdown","id":"growing-power","metadata":{"id":"growing-power"},"source":["## Create dataset"]},{"cell_type":"code","execution_count":20,"id":"czech-remark","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["377f0d897ff048c29f71d21ceaa99c1f","66bb82a3ec6b4eada537b0e101d1d108","01a78948725e448fad47715fcd391201","947f049c16384e99bd31ffcde43f3f4c","69a1d2bea79948c781260c26f3087f70","8c16f35b39a64cefae61f7fbcbe62e53","237b7a26b3f04b53a97cd1efd70f043f","c3708ee7654f4173b713d8d0f72514ff","d676c966f5fe466ea6c6e3245f6ac87b","4a307a7c7270442181704f656e1885b6","67373530dac043a48164a37d707a4b29"]},"id":"czech-remark","outputId":"db08f299-e5f1-41a5-f3c3-23a66c5d36ed","executionInfo":{"status":"ok","timestamp":1646744164434,"user_tz":-540,"elapsed":29008,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"377f0d897ff048c29f71d21ceaa99c1f","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/42146 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 323\n"]}],"source":["pn_history_lengths = []\n","tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    pn_history_lengths.append(length)\n","\n","print(\"max length:\", np.max(pn_history_lengths))"]},{"cell_type":"code","execution_count":21,"id":"compressed-adoption","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["79b46d9ce7374a6f86d3b4dc913c3d09","d1d804e434f14b38ba2cd869aede2c5f","ad56311a7c4d44389f8b36e84263fa9a","4802446c32774733ad4f30e5fdf41ad7","f2f9a25ac0f545a38d7cd5b3735e16da","ccc40ad82b9742138d2a358929ee28f3","9bd48c1054c043c891c639ddba4bcc27","7e3a957050ee4ff0b9646bf48a48e9d0","90e215dc04664fccb690daec60bfd19e","b3743c6ae2ac4d228263b521bb623451","a1466613a2ae4654ab8a95b2f92638d9"]},"id":"compressed-adoption","outputId":"5b673e1f-4bfa-46e0-f33a-08833de8ed97","executionInfo":{"status":"ok","timestamp":1646744164972,"user_tz":-540,"elapsed":556,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"79b46d9ce7374a6f86d3b4dc913c3d09","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/143 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 28\n"]}],"source":["feature_text_lengths = []\n","tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    feature_text_lengths.append(length)\n","\n","print(\"max length:\", np.max(feature_text_lengths))"]},{"cell_type":"code","execution_count":22,"id":"human-housing","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"human-housing","outputId":"b7fd1531-20c0-441f-ff09-4be1eba70948","executionInfo":{"status":"ok","timestamp":1646744164973,"user_tz":-540,"elapsed":12,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["max length: 354\n"]}],"source":["CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n","\n","print(\"max length:\", CFG.max_len)"]},{"cell_type":"code","execution_count":23,"id":"least-africa","metadata":{"id":"least-africa","executionInfo":{"status":"ok","timestamp":1646744164973,"user_tz":-540,"elapsed":11,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class TrainingDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","        self.annotation_lengths = self.df[\"annotation_length\"].values\n","        self.locations = self.df[\"location\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def _create_label(self, pn_history, annotation_length, location_list):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        offset_mapping = encoded[\"offset_mapping\"]\n","        ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n","        label = np.zeros(len(offset_mapping))\n","        label[ignore_idxes] = -1\n","\n","        if annotation_length > 0:\n","            for location in location_list:\n","                for loc in [s.split() for s in location.split(\";\")]:\n","                    start, end = int(loc[0]), int(loc[1])\n","                    start_idx = -1\n","                    end_idx = -1\n","                    for idx in range(len(offset_mapping)):\n","                        if (start_idx == -1) & (start < offset_mapping[idx][0]):\n","                            start_idx = idx - 1\n","                        if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n","                            end_idx = idx + 1\n","                    if start_idx == -1:\n","                        start_idx = end_idx\n","                    if (start_idx != -1) & (end_idx != -1):\n","                        label[start_idx:end_idx] = 1\n","\n","        return torch.tensor(label, dtype=torch.float)\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n","        return input_, label"]},{"cell_type":"code","execution_count":24,"id":"dedicated-boundary","metadata":{"id":"dedicated-boundary","executionInfo":{"status":"ok","timestamp":1646744164974,"user_tz":-540,"elapsed":11,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class TestDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        return input_"]},{"cell_type":"markdown","id":"urban-democracy","metadata":{"id":"urban-democracy"},"source":["## Model"]},{"cell_type":"code","execution_count":25,"id":"chicken-activation","metadata":{"id":"chicken-activation","executionInfo":{"status":"ok","timestamp":1646744164974,"user_tz":-540,"elapsed":11,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class CustomModel(nn.Module):\n","    def __init__(self, cfg, model_config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        if model_config_path is None:\n","            self.model_config = AutoConfig.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                output_hidden_states=True,\n","            )\n","        else:\n","            self.model_config = torch.load(model_config_path)\n","\n","        if pretrained:\n","            self.backbone = AutoModel.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                config=self.model_config,\n","            )\n","            print(f\"Load weight from pretrained\")\n","        else:\n","            #self.backbone = AutoModel.from_config(self.model_config)\n","            itpt = AutoModelForMaskedLM.from_config(self.model_config)\n","            path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp027/checkpoint-52068/pytorch_model.bin\")\n","            #path = \"../output/nbme-score-clinical-patient-notes/nbme-exp022/checkpoint-130170/pytorch_model.bin\"\n","            state_dict = torch.load(path)\n","            itpt.load_state_dict(state_dict)\n","            self.backbone = itpt.deberta\n","            print(f\"Load weight from {path}\")\n","\n","        self.fc = nn.Sequential(\n","            nn.Dropout(self.cfg.dropout),\n","            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n","        )\n","\n","    def forward(self, inputs):\n","        h = self.backbone(**inputs)[\"last_hidden_state\"]\n","        output = self.fc(h)\n","        return output"]},{"cell_type":"markdown","id":"apart-strand","metadata":{"id":"apart-strand"},"source":["## Training"]},{"cell_type":"code","execution_count":26,"id":"different-narrow","metadata":{"id":"different-narrow","executionInfo":{"status":"ok","timestamp":1646744164975,"user_tz":-540,"elapsed":12,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class FocalLoss(nn.Module):\n","    \"\"\"https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/65938\n","    \"\"\"\n","    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n","        super(FocalLoss, self).__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.logits = logits\n","        self.reduce = reduce\n","\n","    def forward(self, inputs, targets):\n","        if self.logits:\n","            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n","        else:\n","            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n","        pt = torch.exp(-BCE_loss)\n","        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n","\n","        if self.reduce:\n","            return torch.mean(F_loss)\n","        else:\n","            return F_loss"]},{"cell_type":"code","execution_count":27,"id":"olive-pavilion","metadata":{"id":"olive-pavilion","executionInfo":{"status":"ok","timestamp":1646744164975,"user_tz":-540,"elapsed":11,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def train_fn(\n","    train_dataloader,\n","    model,\n","    criterion,\n","    optimizer,\n","    epoch,\n","    scheduler,\n","    device,\n","):\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(train_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","\n","        pos_nums = (labels == 1).sum(axis=1)\n","        pos_nums = torch.tensor([[pos_num] * CFG.max_len for pos_num in pos_nums]).view(-1, 1).to(device)\n","        pos_nums = torch.masked_select(pos_nums, labels.view(-1, 1) != -1)\n","        weight = []\n","        for pos_num in pos_nums:\n","            if pos_num == 0:\n","                weight.append(3.0)\n","            else:\n","                weight.append(1.0)\n","        weight = torch.tensor(weight).to(device)\n","        loss = loss * weight\n","\n","        loss = loss.mean()\n","\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","\n","        if CFG.batch_scheduler:\n","            scheduler.step()\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n","            print(\n","                \"Epoch: [{0}][{1}/{2}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                \"Grad: {grad_norm:.4f}  \"\n","                \"LR: {lr:.6f}  \"\n","                .format(\n","                    epoch+1,\n","                    step,\n","                    len(train_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n","                    loss=losses,\n","                     grad_norm=grad_norm,\n","                     lr=scheduler.get_lr()[0],\n","                )\n","            )\n","    return losses.avg"]},{"cell_type":"code","execution_count":28,"id":"crazy-oklahoma","metadata":{"id":"crazy-oklahoma","executionInfo":{"status":"ok","timestamp":1646744164975,"user_tz":-540,"elapsed":11,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def valid_fn(\n","    val_dataloader,\n","    model,\n","    criterion,\n","    device,\n","):\n","    model.eval()\n","    preds = []\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(val_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.no_grad():\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","\n","        pos_nums = (labels == 1).sum(axis=1)\n","        pos_nums = torch.tensor([[pos_num] * CFG.max_len for pos_num in pos_nums]).view(-1, 1).to(device)\n","        pos_nums = torch.masked_select(pos_nums, labels.view(-1, 1) != -1)\n","        weight = []\n","        for pos_num in pos_nums:\n","            if pos_num == 0:\n","                weight.append(3.0)\n","            else:\n","                weight.append(1.0)\n","        weight = torch.tensor(weight).to(device)\n","        loss = loss * weight\n","\n","        loss = loss.mean()\n","\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n","            print(\n","                \"EVAL: [{0}/{1}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                .format(\n","                    step, len(val_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n","                    loss=losses,\n","                )\n","            )\n","    preds = np.concatenate(preds)\n","    return losses.avg, preds"]},{"cell_type":"code","execution_count":29,"id":"clinical-story","metadata":{"id":"clinical-story","executionInfo":{"status":"ok","timestamp":1646744164976,"user_tz":-540,"elapsed":12,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def inference_fn(test_dataloader, model, device):\n","    model.eval()\n","    model.to(device)\n","    preds = []\n","    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n","    for inputs in tk0:\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        with torch.no_grad():\n","            output = model(inputs)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","    preds = np.concatenate(preds)\n","    return preds"]},{"cell_type":"code","execution_count":30,"id":"other-wrapping","metadata":{"id":"other-wrapping","executionInfo":{"status":"ok","timestamp":1646744164976,"user_tz":-540,"elapsed":11,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def train_loop(df, i_fold, device):\n","    print(f\"========== fold: {i_fold} training ==========\")\n","    train_idx = df[df[\"fold\"] != i_fold].index\n","    val_idx = df[df[\"fold\"] == i_fold].index\n","\n","    train_folds = df.loc[train_idx].reset_index(drop=True)\n","    val_folds = df.loc[val_idx].reset_index(drop=True)\n","\n","    train_dataset = TrainingDataset(CFG, train_folds)\n","    val_dataset = TrainingDataset(CFG, val_folds)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=True,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=True,\n","    )\n","    val_dataloader = DataLoader(\n","        val_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=False,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=False,\n","    )\n","\n","    #model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","    model = CustomModel(CFG, model_config_path=None, pretrained=False)\n","    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n","    model.to(device)\n","\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\"params\": [p for n, p in param_optimizer if not any(\n","            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n","        {\"params\": [p for n, p in param_optimizer if any(\n","            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n","    ]\n","    optimizer = AdamW(\n","        optimizer_grouped_parameters,\n","        lr=CFG.lr,\n","        betas=CFG.betas,\n","        weight_decay=CFG.weight_decay,\n","    )\n","    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n","    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        num_training_steps=num_train_optimization_steps,\n","    )\n","\n","    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    #criterion = FocalLoss(reduce=False)\n","    best_score = -1 * np.inf\n","\n","    for epoch in range(CFG.epochs):\n","        start_time = time.time()\n","        avg_loss = train_fn(\n","            train_dataloader,\n","            model,\n","            criterion,\n","            optimizer,\n","            epoch,\n","            scheduler,\n","            device,\n","        )\n","        avg_val_loss, val_preds = valid_fn(\n","            val_dataloader,\n","            model,\n","            criterion,\n","            device,\n","        )\n","\n","        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n","            scheduler.step()\n","\n","        # scoring\n","        val_folds[[str(i) for i in range(CFG.max_len)]] = val_preds\n","        score = scoring(val_folds, th=0.5)\n","\n","        elapsed = time.time() - start_time\n","\n","        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n","        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n","        if score > best_score:\n","            best_score = score\n","            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n","            torch.save({\n","                \"model\": model.state_dict(),\n","                \"predictions\": val_preds,\n","                },\n","                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","            )\n","\n","    predictions = torch.load(\n","        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","        map_location=torch.device(\"cpu\"),\n","    )[\"predictions\"]\n","    val_folds[[str(i) for i in range(CFG.max_len)]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return val_folds"]},{"cell_type":"markdown","id":"surgical-diesel","metadata":{"id":"surgical-diesel"},"source":["## Main"]},{"cell_type":"code","execution_count":31,"id":"logical-regular","metadata":{"id":"logical-regular","executionInfo":{"status":"ok","timestamp":1646744164977,"user_tz":-540,"elapsed":11,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def main():\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    if CFG.train:\n","        oof_df = pd.DataFrame()\n","        for i_fold in range(CFG.n_fold):\n","            if i_fold in CFG.train_fold:\n","                _oof_df = train_loop(train, i_fold, device)\n","                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n","        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    if CFG.submission:\n","        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n","    else:\n","        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    score = scoring(oof_df, th=0.5)\n","    print(f\"Best thres: 0.5, Score: {score:.4f}\")\n","    best_thres = get_best_thres(oof_df)\n","    score = scoring(oof_df, th=best_thres)\n","    print(f\"Best thres: {best_thres}, Score: {score:.4f}\")\n","\n","    if CFG.inference:\n","        test_dataset = TestDataset(CFG, test)\n","        test_dataloader = DataLoader(\n","            test_dataset,\n","            batch_size=CFG.batch_size,\n","            shuffle=False,\n","            num_workers=CFG.num_workers,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","        predictions = []\n","        for i_fold in CFG.train_fold:\n","            if CFG.submission:\n","                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n","                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n","            else:\n","                model = CustomModel(CFG, model_config_path=None, pretrained=False)\n","                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n","\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            model.load_state_dict(state[\"model\"])\n","            test_token_probs = inference_fn(test_dataloader, model, device)\n","            test[[f\"fold{i_fold}_{i}\" for i in range(CFG.max_len)]] = test_token_probs\n","            test_char_probs = get_char_probs(test[\"pn_history\"].values, test_token_probs, CFG.tokenizer)\n","            predictions.append(test_char_probs)\n","\n","            del state, test_token_probs, model; gc.collect()\n","            torch.cuda.empty_cache()\n","\n","        predictions = np.mean(predictions, axis=0)\n","        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n","        test[CFG.target_col] = predicted_location_str\n","        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n","        test[[CFG.id_col, CFG.target_col]].to_csv(\n","            CFG.output_dir / \"submission.csv\", index=False\n","        )"]},{"cell_type":"code","execution_count":32,"id":"honest-brother","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"honest-brother","outputId":"0de96d4f-9697-4b91-ba07-f3e37775da3f","executionInfo":{"status":"error","timestamp":1646754157443,"user_tz":-540,"elapsed":9992476,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["========== fold: 0 training ==========\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp027/checkpoint-52068/pytorch_model.bin\n","Epoch: [1][0/2849] Elapsed 0m 0s (remain 45m 58s) Loss: 0.7089(0.7089) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/2849] Elapsed 0m 28s (remain 12m 57s) Loss: 0.8321(0.6408) Grad: 84504.9141  LR: 0.000001  \n","Epoch: [1][200/2849] Elapsed 0m 56s (remain 12m 20s) Loss: 0.1947(0.5031) Grad: 21004.0879  LR: 0.000003  \n","Epoch: [1][300/2849] Elapsed 1m 24s (remain 11m 51s) Loss: 0.0311(0.3675) Grad: 692.9359  LR: 0.000004  \n","Epoch: [1][400/2849] Elapsed 1m 51s (remain 11m 21s) Loss: 0.0690(0.2870) Grad: 1605.7024  LR: 0.000006  \n","Epoch: [1][500/2849] Elapsed 2m 19s (remain 10m 52s) Loss: 0.0717(0.2379) Grad: 2525.6658  LR: 0.000007  \n","Epoch: [1][600/2849] Elapsed 2m 47s (remain 10m 27s) Loss: 0.0415(0.2047) Grad: 1099.3674  LR: 0.000008  \n","Epoch: [1][700/2849] Elapsed 3m 15s (remain 9m 59s) Loss: 0.0105(0.1812) Grad: 1503.3105  LR: 0.000010  \n","Epoch: [1][800/2849] Elapsed 3m 43s (remain 9m 30s) Loss: 0.0504(0.1629) Grad: 5656.5894  LR: 0.000011  \n","Epoch: [1][900/2849] Elapsed 4m 10s (remain 9m 2s) Loss: 0.0360(0.1473) Grad: 5863.5083  LR: 0.000013  \n","Epoch: [1][1000/2849] Elapsed 4m 38s (remain 8m 34s) Loss: 0.0121(0.1347) Grad: 2009.3945  LR: 0.000014  \n","Epoch: [1][1100/2849] Elapsed 5m 6s (remain 8m 5s) Loss: 0.0084(0.1240) Grad: 2097.5820  LR: 0.000015  \n","Epoch: [1][1200/2849] Elapsed 5m 33s (remain 7m 37s) Loss: 0.0218(0.1149) Grad: 2266.9199  LR: 0.000017  \n","Epoch: [1][1300/2849] Elapsed 6m 1s (remain 7m 9s) Loss: 0.0193(0.1074) Grad: 3133.1643  LR: 0.000018  \n","Epoch: [1][1400/2849] Elapsed 6m 29s (remain 6m 42s) Loss: 0.0089(0.1007) Grad: 2176.7268  LR: 0.000020  \n","Epoch: [1][1500/2849] Elapsed 6m 56s (remain 6m 13s) Loss: 0.0019(0.0948) Grad: 435.4048  LR: 0.000020  \n","Epoch: [1][1600/2849] Elapsed 7m 23s (remain 5m 45s) Loss: 0.0203(0.0896) Grad: 3509.1128  LR: 0.000020  \n","Epoch: [1][1700/2849] Elapsed 7m 50s (remain 5m 17s) Loss: 0.0083(0.0851) Grad: 2949.3774  LR: 0.000020  \n","Epoch: [1][1800/2849] Elapsed 8m 17s (remain 4m 49s) Loss: 0.0017(0.0811) Grad: 371.6054  LR: 0.000019  \n","Epoch: [1][1900/2849] Elapsed 8m 45s (remain 4m 21s) Loss: 0.0110(0.0774) Grad: 2382.9128  LR: 0.000019  \n","Epoch: [1][2000/2849] Elapsed 9m 12s (remain 3m 54s) Loss: 0.0017(0.0741) Grad: 435.1270  LR: 0.000019  \n","Epoch: [1][2100/2849] Elapsed 9m 39s (remain 3m 26s) Loss: 0.0130(0.0711) Grad: 3564.7000  LR: 0.000019  \n","Epoch: [1][2200/2849] Elapsed 10m 6s (remain 2m 58s) Loss: 0.0489(0.0683) Grad: 11184.0459  LR: 0.000019  \n","Epoch: [1][2300/2849] Elapsed 10m 33s (remain 2m 30s) Loss: 0.0015(0.0657) Grad: 358.8795  LR: 0.000019  \n","Epoch: [1][2400/2849] Elapsed 11m 1s (remain 2m 3s) Loss: 0.0042(0.0633) Grad: 1824.2463  LR: 0.000018  \n","Epoch: [1][2500/2849] Elapsed 11m 28s (remain 1m 35s) Loss: 0.0609(0.0612) Grad: 7829.3921  LR: 0.000018  \n","Epoch: [1][2600/2849] Elapsed 11m 55s (remain 1m 8s) Loss: 0.0004(0.0591) Grad: 350.0309  LR: 0.000018  \n","Epoch: [1][2700/2849] Elapsed 12m 22s (remain 0m 40s) Loss: 0.0005(0.0573) Grad: 197.6949  LR: 0.000018  \n","Epoch: [1][2800/2849] Elapsed 12m 50s (remain 0m 13s) Loss: 0.0061(0.0556) Grad: 1066.5474  LR: 0.000018  \n","Epoch: [1][2848/2849] Elapsed 13m 3s (remain 0m 0s) Loss: 0.0152(0.0548) Grad: 2150.9146  LR: 0.000018  \n","EVAL: [0/726] Elapsed 0m 0s (remain 5m 40s) Loss: 0.0025(0.0025) \n","EVAL: [100/726] Elapsed 0m 17s (remain 1m 51s) Loss: 0.0025(0.0092) \n","EVAL: [200/726] Elapsed 0m 35s (remain 1m 32s) Loss: 0.0037(0.0097) \n","EVAL: [300/726] Elapsed 0m 52s (remain 1m 14s) Loss: 0.0001(0.0091) \n","EVAL: [400/726] Elapsed 1m 10s (remain 0m 56s) Loss: 0.0037(0.0110) \n","EVAL: [500/726] Elapsed 1m 27s (remain 0m 39s) Loss: 0.0366(0.0110) \n","EVAL: [600/726] Elapsed 1m 45s (remain 0m 21s) Loss: 0.0063(0.0105) \n","EVAL: [700/726] Elapsed 2m 2s (remain 0m 4s) Loss: 0.0027(0.0099) \n","EVAL: [725/726] Elapsed 2m 6s (remain 0m 0s) Loss: 0.0002(0.0097) \n","Epoch 1 - avg_train_loss: 0.0548  avg_val_loss: 0.0097  time: 914s\n","Epoch 1 - Score: 0.8487\n","Epoch 1 - Save Best Score: 0.8487 Model\n","Epoch: [2][0/2849] Elapsed 0m 0s (remain 25m 23s) Loss: 0.0006(0.0006) Grad: 4961.8086  LR: 0.000018  \n","Epoch: [2][100/2849] Elapsed 0m 29s (remain 13m 31s) Loss: 0.0062(0.0066) Grad: 10923.8730  LR: 0.000018  \n","Epoch: [2][200/2849] Elapsed 0m 58s (remain 12m 54s) Loss: 0.0217(0.0068) Grad: 45185.7070  LR: 0.000017  \n","Epoch: [2][300/2849] Elapsed 1m 26s (remain 12m 9s) Loss: 0.0022(0.0068) Grad: 7356.3569  LR: 0.000017  \n","Epoch: [2][400/2849] Elapsed 1m 53s (remain 11m 32s) Loss: 0.0001(0.0067) Grad: 240.1267  LR: 0.000017  \n","Epoch: [2][500/2849] Elapsed 2m 20s (remain 11m 0s) Loss: 0.0326(0.0069) Grad: 91727.6172  LR: 0.000017  \n","Epoch: [2][600/2849] Elapsed 2m 48s (remain 10m 29s) Loss: 0.0087(0.0069) Grad: 36643.7109  LR: 0.000017  \n","Epoch: [2][700/2849] Elapsed 3m 15s (remain 9m 58s) Loss: 0.0000(0.0070) Grad: 65.2809  LR: 0.000017  \n","Epoch: [2][800/2849] Elapsed 3m 42s (remain 9m 29s) Loss: 0.0010(0.0072) Grad: 2566.8066  LR: 0.000017  \n","Epoch: [2][900/2849] Elapsed 4m 9s (remain 9m 0s) Loss: 0.0213(0.0076) Grad: 14025.2334  LR: 0.000016  \n","Epoch: [2][1000/2849] Elapsed 4m 37s (remain 8m 32s) Loss: 0.0001(0.0073) Grad: 1151.5579  LR: 0.000016  \n","Epoch: [2][1100/2849] Elapsed 5m 4s (remain 8m 3s) Loss: 0.0017(0.0073) Grad: 3442.8447  LR: 0.000016  \n","Epoch: [2][1200/2849] Elapsed 5m 31s (remain 7m 35s) Loss: 0.0006(0.0072) Grad: 6048.7563  LR: 0.000016  \n","Epoch: [2][1300/2849] Elapsed 5m 59s (remain 7m 7s) Loss: 0.0001(0.0073) Grad: 532.4488  LR: 0.000016  \n","Epoch: [2][1400/2849] Elapsed 6m 26s (remain 6m 39s) Loss: 0.0424(0.0074) Grad: 28491.2324  LR: 0.000016  \n","Epoch: [2][1500/2849] Elapsed 6m 54s (remain 6m 11s) Loss: 0.0007(0.0076) Grad: 3221.0769  LR: 0.000015  \n","Epoch: [2][1600/2849] Elapsed 7m 21s (remain 5m 44s) Loss: 0.0007(0.0076) Grad: 1670.2815  LR: 0.000015  \n","Epoch: [2][1700/2849] Elapsed 7m 49s (remain 5m 16s) Loss: 0.0004(0.0076) Grad: 2609.1758  LR: 0.000015  \n","Epoch: [2][1800/2849] Elapsed 8m 16s (remain 4m 48s) Loss: 0.0017(0.0076) Grad: 5690.0327  LR: 0.000015  \n","Epoch: [2][1900/2849] Elapsed 8m 43s (remain 4m 21s) Loss: 0.0020(0.0075) Grad: 3912.3118  LR: 0.000015  \n","Epoch: [2][2000/2849] Elapsed 9m 11s (remain 3m 53s) Loss: 0.0154(0.0076) Grad: 34410.1211  LR: 0.000015  \n","Epoch: [2][2100/2849] Elapsed 9m 38s (remain 3m 25s) Loss: 0.0014(0.0075) Grad: 3007.4995  LR: 0.000014  \n","Epoch: [2][2200/2849] Elapsed 10m 6s (remain 2m 58s) Loss: 0.0041(0.0074) Grad: 8447.7354  LR: 0.000014  \n","Epoch: [2][2300/2849] Elapsed 10m 33s (remain 2m 30s) Loss: 0.0000(0.0074) Grad: 206.9908  LR: 0.000014  \n","Epoch: [2][2400/2849] Elapsed 11m 1s (remain 2m 3s) Loss: 0.0045(0.0075) Grad: 8868.9717  LR: 0.000014  \n","Epoch: [2][2500/2849] Elapsed 11m 28s (remain 1m 35s) Loss: 0.0001(0.0074) Grad: 335.6499  LR: 0.000014  \n","Epoch: [2][2600/2849] Elapsed 11m 56s (remain 1m 8s) Loss: 0.0018(0.0074) Grad: 9445.3799  LR: 0.000014  \n","Epoch: [2][2700/2849] Elapsed 12m 23s (remain 0m 40s) Loss: 0.0001(0.0074) Grad: 3639.3503  LR: 0.000014  \n","Epoch: [2][2800/2849] Elapsed 12m 51s (remain 0m 13s) Loss: 0.0001(0.0075) Grad: 230.3932  LR: 0.000013  \n","Epoch: [2][2848/2849] Elapsed 13m 4s (remain 0m 0s) Loss: 0.0014(0.0075) Grad: 8271.1699  LR: 0.000013  \n","EVAL: [0/726] Elapsed 0m 0s (remain 5m 54s) Loss: 0.0004(0.0004) \n","EVAL: [100/726] Elapsed 0m 18s (remain 1m 51s) Loss: 0.0031(0.0087) \n","EVAL: [200/726] Elapsed 0m 35s (remain 1m 33s) Loss: 0.0002(0.0092) \n","EVAL: [300/726] Elapsed 0m 53s (remain 1m 15s) Loss: 0.0000(0.0087) \n","EVAL: [400/726] Elapsed 1m 10s (remain 0m 57s) Loss: 0.0048(0.0105) \n","EVAL: [500/726] Elapsed 1m 28s (remain 0m 39s) Loss: 0.0204(0.0105) \n","EVAL: [600/726] Elapsed 1m 45s (remain 0m 21s) Loss: 0.0014(0.0100) \n","EVAL: [700/726] Elapsed 2m 3s (remain 0m 4s) Loss: 0.0025(0.0095) \n","EVAL: [725/726] Elapsed 2m 7s (remain 0m 0s) Loss: 0.0001(0.0093) \n","Epoch 2 - avg_train_loss: 0.0075  avg_val_loss: 0.0093  time: 916s\n","Epoch 2 - Score: 0.8658\n","Epoch 2 - Save Best Score: 0.8658 Model\n","Epoch: [3][0/2849] Elapsed 0m 0s (remain 28m 28s) Loss: 0.0091(0.0091) Grad: 8735.6338  LR: 0.000013  \n","Epoch: [3][100/2849] Elapsed 0m 30s (remain 14m 0s) Loss: 0.0158(0.0058) Grad: 62912.1289  LR: 0.000013  \n","Epoch: [3][200/2849] Elapsed 1m 0s (remain 13m 17s) Loss: 0.0004(0.0057) Grad: 1722.6288  LR: 0.000013  \n","Epoch: [3][300/2849] Elapsed 1m 28s (remain 12m 27s) Loss: 0.0054(0.0055) Grad: 55832.5391  LR: 0.000013  \n","Epoch: [3][400/2849] Elapsed 1m 56s (remain 11m 48s) Loss: 0.0002(0.0058) Grad: 974.3403  LR: 0.000013  \n","Epoch: [3][500/2849] Elapsed 2m 23s (remain 11m 13s) Loss: 0.0468(0.0057) Grad: 75157.6797  LR: 0.000013  \n","Epoch: [3][600/2849] Elapsed 2m 51s (remain 10m 40s) Loss: 0.0001(0.0054) Grad: 286.6241  LR: 0.000012  \n","Epoch: [3][700/2849] Elapsed 3m 19s (remain 10m 10s) Loss: 0.0075(0.0056) Grad: 20056.4453  LR: 0.000012  \n","Epoch: [3][800/2849] Elapsed 3m 46s (remain 9m 40s) Loss: 0.0012(0.0061) Grad: 2897.7197  LR: 0.000012  \n","Epoch: [3][900/2849] Elapsed 4m 14s (remain 9m 10s) Loss: 0.0162(0.0062) Grad: 31674.1348  LR: 0.000012  \n","Epoch: [3][1000/2849] Elapsed 4m 42s (remain 8m 41s) Loss: 0.0073(0.0063) Grad: 11915.1748  LR: 0.000012  \n","Epoch: [3][1100/2849] Elapsed 5m 10s (remain 8m 12s) Loss: 0.0001(0.0062) Grad: 247.1496  LR: 0.000012  \n","Epoch: [3][1200/2849] Elapsed 5m 37s (remain 7m 43s) Loss: 0.0055(0.0061) Grad: 16446.5938  LR: 0.000011  \n","Epoch: [3][1300/2849] Elapsed 6m 5s (remain 7m 15s) Loss: 0.0001(0.0060) Grad: 2416.1514  LR: 0.000011  \n","Epoch: [3][1400/2849] Elapsed 6m 33s (remain 6m 46s) Loss: 0.0000(0.0061) Grad: 157.9792  LR: 0.000011  \n","Epoch: [3][1500/2849] Elapsed 7m 1s (remain 6m 18s) Loss: 0.0031(0.0060) Grad: 3864.5852  LR: 0.000011  \n","Epoch: [3][1600/2849] Elapsed 7m 29s (remain 5m 50s) Loss: 0.0000(0.0061) Grad: 59.4243  LR: 0.000011  \n","Epoch: [3][1700/2849] Elapsed 7m 56s (remain 5m 21s) Loss: 0.0004(0.0060) Grad: 2842.6418  LR: 0.000011  \n","Epoch: [3][1800/2849] Elapsed 8m 24s (remain 4m 53s) Loss: 0.0110(0.0061) Grad: 152355.0156  LR: 0.000011  \n","Epoch: [3][1900/2849] Elapsed 8m 52s (remain 4m 25s) Loss: 0.0033(0.0060) Grad: 9925.8652  LR: 0.000010  \n","Epoch: [3][2000/2849] Elapsed 9m 19s (remain 3m 57s) Loss: 0.0024(0.0061) Grad: 3310.5876  LR: 0.000010  \n","Epoch: [3][2100/2849] Elapsed 9m 47s (remain 3m 29s) Loss: 0.0018(0.0060) Grad: 3148.9856  LR: 0.000010  \n","Epoch: [3][2200/2849] Elapsed 10m 15s (remain 3m 1s) Loss: 0.0000(0.0059) Grad: 39.6663  LR: 0.000010  \n","Epoch: [3][2300/2849] Elapsed 10m 42s (remain 2m 33s) Loss: 0.0004(0.0059) Grad: 1275.7246  LR: 0.000010  \n","Epoch: [3][2400/2849] Elapsed 11m 10s (remain 2m 5s) Loss: 0.0130(0.0059) Grad: 109035.1562  LR: 0.000010  \n","Epoch: [3][2500/2849] Elapsed 11m 38s (remain 1m 37s) Loss: 0.0004(0.0059) Grad: 1984.1467  LR: 0.000009  \n","Epoch: [3][2600/2849] Elapsed 12m 6s (remain 1m 9s) Loss: 0.0000(0.0060) Grad: 127.8021  LR: 0.000009  \n","Epoch: [3][2700/2849] Elapsed 12m 34s (remain 0m 41s) Loss: 0.0268(0.0060) Grad: 29518.4395  LR: 0.000009  \n","Epoch: [3][2800/2849] Elapsed 13m 1s (remain 0m 13s) Loss: 0.0082(0.0060) Grad: 14808.8369  LR: 0.000009  \n","Epoch: [3][2848/2849] Elapsed 13m 15s (remain 0m 0s) Loss: 0.0003(0.0060) Grad: 888.1178  LR: 0.000009  \n","EVAL: [0/726] Elapsed 0m 0s (remain 5m 56s) Loss: 0.0005(0.0005) \n","EVAL: [100/726] Elapsed 0m 18s (remain 1m 53s) Loss: 0.0040(0.0106) \n","EVAL: [200/726] Elapsed 0m 36s (remain 1m 34s) Loss: 0.0001(0.0106) \n","EVAL: [300/726] Elapsed 0m 54s (remain 1m 16s) Loss: 0.0000(0.0099) \n","EVAL: [400/726] Elapsed 1m 11s (remain 0m 58s) Loss: 0.0015(0.0116) \n","EVAL: [500/726] Elapsed 1m 29s (remain 0m 40s) Loss: 0.0142(0.0114) \n","EVAL: [600/726] Elapsed 1m 47s (remain 0m 22s) Loss: 0.0045(0.0108) \n","EVAL: [700/726] Elapsed 2m 5s (remain 0m 4s) Loss: 0.0035(0.0103) \n","EVAL: [725/726] Elapsed 2m 9s (remain 0m 0s) Loss: 0.0000(0.0101) \n","Epoch 3 - avg_train_loss: 0.0060  avg_val_loss: 0.0101  time: 930s\n","Epoch 3 - Score: 0.8671\n","Epoch 3 - Save Best Score: 0.8671 Model\n","Epoch: [4][0/2849] Elapsed 0m 0s (remain 28m 3s) Loss: 0.0053(0.0053) Grad: 14199.7637  LR: 0.000009  \n","Epoch: [4][100/2849] Elapsed 0m 30s (remain 13m 47s) Loss: 0.0000(0.0039) Grad: 237.1416  LR: 0.000009  \n","Epoch: [4][200/2849] Elapsed 0m 59s (remain 13m 7s) Loss: 0.0007(0.0046) Grad: 2284.0342  LR: 0.000009  \n","Epoch: [4][300/2849] Elapsed 1m 27s (remain 12m 19s) Loss: 0.0002(0.0054) Grad: 1545.5687  LR: 0.000008  \n","Epoch: [4][400/2849] Elapsed 1m 55s (remain 11m 42s) Loss: 0.0000(0.0051) Grad: 35.3536  LR: 0.000008  \n","Epoch: [4][500/2849] Elapsed 2m 22s (remain 11m 9s) Loss: 0.0002(0.0050) Grad: 5669.0576  LR: 0.000008  \n","Epoch: [4][600/2849] Elapsed 2m 50s (remain 10m 37s) Loss: 0.0033(0.0049) Grad: 3229.1509  LR: 0.000008  \n","Epoch: [4][700/2849] Elapsed 3m 18s (remain 10m 7s) Loss: 0.0043(0.0050) Grad: 19750.9023  LR: 0.000008  \n","Epoch: [4][800/2849] Elapsed 3m 45s (remain 9m 37s) Loss: 0.0011(0.0049) Grad: 22658.2617  LR: 0.000008  \n","Epoch: [4][900/2849] Elapsed 4m 13s (remain 9m 7s) Loss: 0.0130(0.0048) Grad: 31511.1816  LR: 0.000007  \n","Epoch: [4][1000/2849] Elapsed 4m 41s (remain 8m 38s) Loss: 0.0001(0.0047) Grad: 1461.9547  LR: 0.000007  \n","Epoch: [4][1100/2849] Elapsed 5m 8s (remain 8m 10s) Loss: 0.0107(0.0047) Grad: 110820.1562  LR: 0.000007  \n","Epoch: [4][1200/2849] Elapsed 5m 36s (remain 7m 41s) Loss: 0.0000(0.0047) Grad: 69.8829  LR: 0.000007  \n","Epoch: [4][1300/2849] Elapsed 6m 4s (remain 7m 13s) Loss: 0.0043(0.0047) Grad: 14036.8506  LR: 0.000007  \n","Epoch: [4][1400/2849] Elapsed 6m 31s (remain 6m 44s) Loss: 0.0001(0.0048) Grad: 1889.1024  LR: 0.000007  \n","Epoch: [4][1500/2849] Elapsed 6m 59s (remain 6m 16s) Loss: 0.0000(0.0049) Grad: 21.7777  LR: 0.000007  \n","Epoch: [4][1600/2849] Elapsed 7m 27s (remain 5m 48s) Loss: 0.0001(0.0048) Grad: 458.7038  LR: 0.000006  \n","Epoch: [4][1700/2849] Elapsed 7m 54s (remain 5m 20s) Loss: 0.0009(0.0050) Grad: 3866.8274  LR: 0.000006  \n","Epoch: [4][1800/2849] Elapsed 8m 22s (remain 4m 52s) Loss: 0.0036(0.0049) Grad: 6395.8203  LR: 0.000006  \n","Epoch: [4][1900/2849] Elapsed 8m 50s (remain 4m 24s) Loss: 0.0005(0.0051) Grad: 22628.7305  LR: 0.000006  \n","Epoch: [4][2000/2849] Elapsed 9m 18s (remain 3m 56s) Loss: 0.0004(0.0050) Grad: 4025.7756  LR: 0.000006  \n","Epoch: [4][2100/2849] Elapsed 9m 46s (remain 3m 28s) Loss: 0.0006(0.0051) Grad: 6246.4692  LR: 0.000006  \n","Epoch: [4][2200/2849] Elapsed 10m 13s (remain 3m 0s) Loss: 0.0009(0.0051) Grad: 9593.5410  LR: 0.000005  \n","Epoch: [4][2300/2849] Elapsed 10m 41s (remain 2m 32s) Loss: 0.0001(0.0052) Grad: 1350.5336  LR: 0.000005  \n","Epoch: [4][2400/2849] Elapsed 11m 9s (remain 2m 4s) Loss: 0.0001(0.0051) Grad: 589.2350  LR: 0.000005  \n","Epoch: [4][2500/2849] Elapsed 11m 36s (remain 1m 36s) Loss: 0.0001(0.0051) Grad: 359.0416  LR: 0.000005  \n","Epoch: [4][2600/2849] Elapsed 12m 4s (remain 1m 9s) Loss: 0.0025(0.0050) Grad: 13666.3311  LR: 0.000005  \n","Epoch: [4][2700/2849] Elapsed 12m 32s (remain 0m 41s) Loss: 0.0019(0.0050) Grad: 7376.2075  LR: 0.000005  \n","Epoch: [4][2800/2849] Elapsed 13m 0s (remain 0m 13s) Loss: 0.0004(0.0050) Grad: 1810.3828  LR: 0.000005  \n","Epoch: [4][2848/2849] Elapsed 13m 13s (remain 0m 0s) Loss: 0.0432(0.0050) Grad: 47046.3438  LR: 0.000004  \n","EVAL: [0/726] Elapsed 0m 0s (remain 5m 35s) Loss: 0.0004(0.0004) \n","EVAL: [100/726] Elapsed 0m 18s (remain 1m 52s) Loss: 0.0042(0.0101) \n","EVAL: [200/726] Elapsed 0m 36s (remain 1m 34s) Loss: 0.0004(0.0102) \n","EVAL: [300/726] Elapsed 0m 53s (remain 1m 15s) Loss: 0.0000(0.0093) \n","EVAL: [400/726] Elapsed 1m 11s (remain 0m 58s) Loss: 0.0029(0.0113) \n","EVAL: [500/726] Elapsed 1m 29s (remain 0m 40s) Loss: 0.0268(0.0113) \n","EVAL: [600/726] Elapsed 1m 47s (remain 0m 22s) Loss: 0.0026(0.0106) \n","EVAL: [700/726] Elapsed 2m 4s (remain 0m 4s) Loss: 0.0026(0.0101) \n","EVAL: [725/726] Elapsed 2m 9s (remain 0m 0s) Loss: 0.0000(0.0099) \n","Epoch 4 - avg_train_loss: 0.0050  avg_val_loss: 0.0099  time: 927s\n","Epoch 4 - Score: 0.8735\n","Epoch 4 - Save Best Score: 0.8735 Model\n","Epoch: [5][0/2849] Elapsed 0m 0s (remain 29m 19s) Loss: 0.0003(0.0003) Grad: 2446.0342  LR: 0.000004  \n","Epoch: [5][100/2849] Elapsed 0m 31s (remain 14m 8s) Loss: 0.1101(0.0055) Grad: 35241.8711  LR: 0.000004  \n","Epoch: [5][200/2849] Elapsed 1m 0s (remain 13m 16s) Loss: 0.0739(0.0045) Grad: 74214.2812  LR: 0.000004  \n","Epoch: [5][300/2849] Elapsed 1m 28s (remain 12m 26s) Loss: 0.0026(0.0045) Grad: 12448.6367  LR: 0.000004  \n","Epoch: [5][400/2849] Elapsed 1m 55s (remain 11m 47s) Loss: 0.0035(0.0040) Grad: 22162.9707  LR: 0.000004  \n","Epoch: [5][500/2849] Elapsed 2m 23s (remain 11m 12s) Loss: 0.0015(0.0039) Grad: 6340.1270  LR: 0.000004  \n","Epoch: [5][600/2849] Elapsed 2m 51s (remain 10m 41s) Loss: 0.0004(0.0039) Grad: 4309.0391  LR: 0.000004  \n","Epoch: [5][700/2849] Elapsed 3m 19s (remain 10m 10s) Loss: 0.0000(0.0043) Grad: 139.5738  LR: 0.000003  \n","Epoch: [5][800/2849] Elapsed 3m 46s (remain 9m 39s) Loss: 0.0000(0.0042) Grad: 59.3139  LR: 0.000003  \n","Epoch: [5][900/2849] Elapsed 4m 14s (remain 9m 10s) Loss: 0.0046(0.0042) Grad: 8168.9731  LR: 0.000003  \n","Epoch: [5][1000/2849] Elapsed 4m 42s (remain 8m 40s) Loss: 0.0049(0.0041) Grad: 12120.3311  LR: 0.000003  \n","Epoch: [5][1100/2849] Elapsed 5m 9s (remain 8m 11s) Loss: 0.0066(0.0041) Grad: 23628.5371  LR: 0.000003  \n","Epoch: [5][1200/2849] Elapsed 5m 37s (remain 7m 43s) Loss: 0.0002(0.0041) Grad: 1005.6867  LR: 0.000003  \n","Epoch: [5][1300/2849] Elapsed 6m 5s (remain 7m 14s) Loss: 0.0038(0.0040) Grad: 3412.9988  LR: 0.000002  \n","Epoch: [5][1400/2849] Elapsed 6m 33s (remain 6m 46s) Loss: 0.0125(0.0041) Grad: 18870.5098  LR: 0.000002  \n","Epoch: [5][1500/2849] Elapsed 7m 0s (remain 6m 17s) Loss: 0.0036(0.0041) Grad: 9696.9365  LR: 0.000002  \n","Epoch: [5][1600/2849] Elapsed 7m 28s (remain 5m 49s) Loss: 0.0037(0.0043) Grad: 5130.8291  LR: 0.000002  \n","Epoch: [5][1700/2849] Elapsed 7m 56s (remain 5m 21s) Loss: 0.0000(0.0044) Grad: 13.5649  LR: 0.000002  \n","Epoch: [5][1800/2849] Elapsed 8m 23s (remain 4m 53s) Loss: 0.0000(0.0043) Grad: 33.1081  LR: 0.000002  \n","Epoch: [5][1900/2849] Elapsed 8m 51s (remain 4m 25s) Loss: 0.0000(0.0042) Grad: 434.1018  LR: 0.000001  \n","Epoch: [5][2000/2849] Elapsed 9m 19s (remain 3m 56s) Loss: 0.0000(0.0042) Grad: 98.7717  LR: 0.000001  \n","Epoch: [5][2100/2849] Elapsed 9m 46s (remain 3m 28s) Loss: 0.0001(0.0041) Grad: 226.8066  LR: 0.000001  \n","Epoch: [5][2200/2849] Elapsed 10m 14s (remain 3m 0s) Loss: 0.0179(0.0041) Grad: 25997.6562  LR: 0.000001  \n","Epoch: [5][2300/2849] Elapsed 10m 42s (remain 2m 32s) Loss: 0.0000(0.0042) Grad: 95.9373  LR: 0.000001  \n","Epoch: [5][2400/2849] Elapsed 11m 9s (remain 2m 5s) Loss: 0.0001(0.0043) Grad: 269.3735  LR: 0.000001  \n","Epoch: [5][2500/2849] Elapsed 11m 37s (remain 1m 37s) Loss: 0.0000(0.0042) Grad: 120.8055  LR: 0.000001  \n","Epoch: [5][2600/2849] Elapsed 12m 5s (remain 1m 9s) Loss: 0.0000(0.0042) Grad: 198.9951  LR: 0.000000  \n","Epoch: [5][2700/2849] Elapsed 12m 32s (remain 0m 41s) Loss: 0.0017(0.0042) Grad: 5176.4263  LR: 0.000000  \n","Epoch: [5][2800/2849] Elapsed 13m 1s (remain 0m 13s) Loss: 0.0006(0.0041) Grad: 1790.7335  LR: 0.000000  \n","Epoch: [5][2848/2849] Elapsed 13m 14s (remain 0m 0s) Loss: 0.0042(0.0041) Grad: 6651.5356  LR: 0.000000  \n","EVAL: [0/726] Elapsed 0m 0s (remain 6m 6s) Loss: 0.0002(0.0002) \n","EVAL: [100/726] Elapsed 0m 18s (remain 1m 53s) Loss: 0.0048(0.0111) \n","EVAL: [200/726] Elapsed 0m 36s (remain 1m 34s) Loss: 0.0000(0.0114) \n","EVAL: [300/726] Elapsed 0m 53s (remain 1m 16s) Loss: 0.0000(0.0104) \n","EVAL: [400/726] Elapsed 1m 11s (remain 0m 58s) Loss: 0.0024(0.0125) \n","EVAL: [500/726] Elapsed 1m 29s (remain 0m 40s) Loss: 0.0292(0.0125) \n","EVAL: [600/726] Elapsed 1m 47s (remain 0m 22s) Loss: 0.0014(0.0118) \n","EVAL: [700/726] Elapsed 2m 4s (remain 0m 4s) Loss: 0.0039(0.0112) \n","EVAL: [725/726] Elapsed 2m 9s (remain 0m 0s) Loss: 0.0000(0.0110) \n","Epoch 5 - avg_train_loss: 0.0041  avg_val_loss: 0.0110  time: 929s\n","Epoch 5 - Score: 0.8729\n","========== fold: 1 training ==========\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp027/checkpoint-52068/pytorch_model.bin\n","Epoch: [1][0/2851] Elapsed 0m 0s (remain 34m 9s) Loss: 0.9524(0.9524) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/2851] Elapsed 0m 30s (remain 13m 43s) Loss: 0.4901(0.5152) Grad: 53712.6797  LR: 0.000001  \n","Epoch: [1][200/2851] Elapsed 0m 59s (remain 13m 8s) Loss: 0.1515(0.3890) Grad: 18974.0449  LR: 0.000003  \n","Epoch: [1][300/2851] Elapsed 1m 29s (remain 12m 39s) Loss: 0.0431(0.2830) Grad: 1087.9005  LR: 0.000004  \n","Epoch: [1][400/2851] Elapsed 1m 59s (remain 12m 9s) Loss: 0.0469(0.2226) Grad: 1350.8285  LR: 0.000006  \n","Epoch: [1][500/2851] Elapsed 2m 28s (remain 11m 38s) Loss: 0.0424(0.1861) Grad: 1251.7567  LR: 0.000007  \n","Epoch: [1][600/2851] Elapsed 2m 58s (remain 11m 9s) Loss: 0.0804(0.1618) Grad: 2290.0781  LR: 0.000008  \n","Epoch: [1][700/2851] Elapsed 3m 28s (remain 10m 39s) Loss: 0.0342(0.1443) Grad: 1676.2291  LR: 0.000010  \n","Epoch: [1][800/2851] Elapsed 3m 58s (remain 10m 9s) Loss: 0.0264(0.1306) Grad: 1841.7612  LR: 0.000011  \n","Epoch: [1][900/2851] Elapsed 4m 27s (remain 9m 38s) Loss: 0.0224(0.1187) Grad: 3507.4158  LR: 0.000013  \n","Epoch: [1][1000/2851] Elapsed 4m 56s (remain 9m 7s) Loss: 0.0125(0.1087) Grad: 5103.3828  LR: 0.000014  \n","Epoch: [1][1100/2851] Elapsed 5m 25s (remain 8m 37s) Loss: 0.0021(0.1007) Grad: 386.8513  LR: 0.000015  \n","Epoch: [1][1200/2851] Elapsed 5m 54s (remain 8m 7s) Loss: 0.0078(0.0935) Grad: 2103.4670  LR: 0.000017  \n","Epoch: [1][1300/2851] Elapsed 6m 23s (remain 7m 36s) Loss: 0.0110(0.0874) Grad: 2684.1719  LR: 0.000018  \n","Epoch: [1][1400/2851] Elapsed 6m 52s (remain 7m 6s) Loss: 0.0175(0.0822) Grad: 6043.3354  LR: 0.000020  \n","Epoch: [1][1500/2851] Elapsed 7m 21s (remain 6m 37s) Loss: 0.0066(0.0777) Grad: 1474.5238  LR: 0.000020  \n","Epoch: [1][1600/2851] Elapsed 7m 50s (remain 6m 7s) Loss: 0.0006(0.0735) Grad: 256.8073  LR: 0.000020  \n","Epoch: [1][1700/2851] Elapsed 8m 19s (remain 5m 37s) Loss: 0.0303(0.0699) Grad: 9158.9209  LR: 0.000020  \n","Epoch: [1][1800/2851] Elapsed 8m 48s (remain 5m 8s) Loss: 0.0007(0.0666) Grad: 184.1782  LR: 0.000019  \n","Epoch: [1][1900/2851] Elapsed 9m 17s (remain 4m 38s) Loss: 0.0006(0.0637) Grad: 169.4911  LR: 0.000019  \n","Epoch: [1][2000/2851] Elapsed 9m 46s (remain 4m 9s) Loss: 0.0078(0.0610) Grad: 893.8190  LR: 0.000019  \n","Epoch: [1][2100/2851] Elapsed 10m 16s (remain 3m 39s) Loss: 0.0002(0.0586) Grad: 163.1774  LR: 0.000019  \n","Epoch: [1][2200/2851] Elapsed 10m 45s (remain 3m 10s) Loss: 0.0132(0.0564) Grad: 9380.0928  LR: 0.000019  \n","Epoch: [1][2300/2851] Elapsed 11m 14s (remain 2m 41s) Loss: 0.0056(0.0544) Grad: 1040.7025  LR: 0.000019  \n","Epoch: [1][2400/2851] Elapsed 11m 43s (remain 2m 11s) Loss: 0.0162(0.0525) Grad: 2015.4264  LR: 0.000018  \n","Epoch: [1][2500/2851] Elapsed 12m 12s (remain 1m 42s) Loss: 0.0162(0.0507) Grad: 1264.4215  LR: 0.000018  \n","Epoch: [1][2600/2851] Elapsed 12m 41s (remain 1m 13s) Loss: 0.0019(0.0491) Grad: 1091.2192  LR: 0.000018  \n","Epoch: [1][2700/2851] Elapsed 13m 10s (remain 0m 43s) Loss: 0.0030(0.0477) Grad: 538.8588  LR: 0.000018  \n","Epoch: [1][2800/2851] Elapsed 13m 39s (remain 0m 14s) Loss: 0.0066(0.0464) Grad: 2091.6147  LR: 0.000018  \n","Epoch: [1][2850/2851] Elapsed 13m 54s (remain 0m 0s) Loss: 0.0001(0.0457) Grad: 41.6506  LR: 0.000018  \n","EVAL: [0/724] Elapsed 0m 0s (remain 6m 26s) Loss: 0.0023(0.0023) \n","EVAL: [100/724] Elapsed 0m 19s (remain 2m 2s) Loss: 0.0115(0.0086) \n","EVAL: [200/724] Elapsed 0m 39s (remain 1m 42s) Loss: 0.0001(0.0100) \n","EVAL: [300/724] Elapsed 0m 58s (remain 1m 22s) Loss: 0.0003(0.0095) \n","EVAL: [400/724] Elapsed 1m 17s (remain 1m 2s) Loss: 0.0000(0.0099) \n","EVAL: [500/724] Elapsed 1m 36s (remain 0m 43s) Loss: 0.0155(0.0114) \n","EVAL: [600/724] Elapsed 1m 56s (remain 0m 23s) Loss: 0.0130(0.0109) \n","EVAL: [700/724] Elapsed 2m 15s (remain 0m 4s) Loss: 0.0001(0.0101) \n","EVAL: [723/724] Elapsed 2m 19s (remain 0m 0s) Loss: 0.0001(0.0100) \n","Epoch 1 - avg_train_loss: 0.0457  avg_val_loss: 0.0100  time: 985s\n","Epoch 1 - Score: 0.8509\n","Epoch 1 - Save Best Score: 0.8509 Model\n","Epoch: [2][0/2851] Elapsed 0m 0s (remain 30m 31s) Loss: 0.0033(0.0033) Grad: 20377.8438  LR: 0.000018  \n","Epoch: [2][100/2851] Elapsed 0m 30s (remain 13m 42s) Loss: 0.0050(0.0067) Grad: 36572.1680  LR: 0.000018  \n","Epoch: [2][200/2851] Elapsed 0m 59s (remain 13m 4s) Loss: 0.0028(0.0066) Grad: 8722.1494  LR: 0.000017  \n","Epoch: [2][300/2851] Elapsed 1m 27s (remain 12m 20s) Loss: 0.0003(0.0068) Grad: 1463.4779  LR: 0.000017  \n","Epoch: [2][400/2851] Elapsed 1m 55s (remain 11m 42s) Loss: 0.0001(0.0076) Grad: 233.2669  LR: 0.000017  \n","Epoch: [2][500/2851] Elapsed 2m 22s (remain 11m 9s) Loss: 0.0190(0.0078) Grad: 24389.8770  LR: 0.000017  \n","Epoch: [2][600/2851] Elapsed 2m 50s (remain 10m 37s) Loss: 0.0363(0.0079) Grad: 35687.9492  LR: 0.000017  \n","Epoch: [2][700/2851] Elapsed 3m 17s (remain 10m 7s) Loss: 0.0024(0.0082) Grad: 9265.9785  LR: 0.000017  \n","Epoch: [2][800/2851] Elapsed 3m 45s (remain 9m 37s) Loss: 0.0003(0.0081) Grad: 1399.2274  LR: 0.000017  \n","Epoch: [2][900/2851] Elapsed 4m 13s (remain 9m 8s) Loss: 0.0009(0.0081) Grad: 2781.4668  LR: 0.000016  \n","Epoch: [2][1000/2851] Elapsed 4m 40s (remain 8m 39s) Loss: 0.0001(0.0080) Grad: 204.2139  LR: 0.000016  \n","Epoch: [2][1100/2851] Elapsed 5m 8s (remain 8m 10s) Loss: 0.0073(0.0079) Grad: 43421.4648  LR: 0.000016  \n","Epoch: [2][1200/2851] Elapsed 5m 36s (remain 7m 41s) Loss: 0.0439(0.0078) Grad: 28991.9453  LR: 0.000016  \n","Epoch: [2][1300/2851] Elapsed 6m 3s (remain 7m 13s) Loss: 0.0003(0.0076) Grad: 1038.5441  LR: 0.000016  \n","Epoch: [2][1400/2851] Elapsed 6m 31s (remain 6m 45s) Loss: 0.0054(0.0075) Grad: 11816.5527  LR: 0.000016  \n","Epoch: [2][1500/2851] Elapsed 6m 59s (remain 6m 16s) Loss: 0.0085(0.0074) Grad: 10000.5352  LR: 0.000015  \n","Epoch: [2][1600/2851] Elapsed 7m 26s (remain 5m 48s) Loss: 0.0026(0.0074) Grad: 6675.4917  LR: 0.000015  \n","Epoch: [2][1700/2851] Elapsed 7m 54s (remain 5m 20s) Loss: 0.0341(0.0073) Grad: 27269.9355  LR: 0.000015  \n","Epoch: [2][1800/2851] Elapsed 8m 21s (remain 4m 52s) Loss: 0.0029(0.0072) Grad: 6104.0205  LR: 0.000015  \n","Epoch: [2][1900/2851] Elapsed 8m 49s (remain 4m 24s) Loss: 0.0017(0.0074) Grad: 2874.5129  LR: 0.000015  \n","Epoch: [2][2000/2851] Elapsed 9m 17s (remain 3m 56s) Loss: 0.0024(0.0075) Grad: 4719.1206  LR: 0.000015  \n","Epoch: [2][2100/2851] Elapsed 9m 45s (remain 3m 28s) Loss: 0.0070(0.0075) Grad: 16640.4121  LR: 0.000015  \n","Epoch: [2][2200/2851] Elapsed 10m 12s (remain 3m 0s) Loss: 0.0020(0.0075) Grad: 4399.4893  LR: 0.000014  \n","Epoch: [2][2300/2851] Elapsed 10m 40s (remain 2m 33s) Loss: 0.0017(0.0076) Grad: 9729.1396  LR: 0.000014  \n","Epoch: [2][2400/2851] Elapsed 11m 7s (remain 2m 5s) Loss: 0.0000(0.0076) Grad: 170.0871  LR: 0.000014  \n","Epoch: [2][2500/2851] Elapsed 11m 35s (remain 1m 37s) Loss: 0.0001(0.0075) Grad: 834.3123  LR: 0.000014  \n","Epoch: [2][2600/2851] Elapsed 12m 3s (remain 1m 9s) Loss: 0.0002(0.0075) Grad: 517.6609  LR: 0.000014  \n","Epoch: [2][2700/2851] Elapsed 12m 30s (remain 0m 41s) Loss: 0.0000(0.0076) Grad: 190.5095  LR: 0.000014  \n","Epoch: [2][2800/2851] Elapsed 12m 58s (remain 0m 13s) Loss: 0.0023(0.0075) Grad: 9178.5498  LR: 0.000013  \n","Epoch: [2][2850/2851] Elapsed 13m 12s (remain 0m 0s) Loss: 0.0019(0.0074) Grad: 2702.4844  LR: 0.000013  \n","EVAL: [0/724] Elapsed 0m 0s (remain 6m 5s) Loss: 0.0005(0.0005) \n","EVAL: [100/724] Elapsed 0m 18s (remain 1m 51s) Loss: 0.0134(0.0083) \n","EVAL: [200/724] Elapsed 0m 35s (remain 1m 33s) Loss: 0.0000(0.0096) \n","EVAL: [300/724] Elapsed 0m 53s (remain 1m 15s) Loss: 0.0002(0.0099) \n","EVAL: [400/724] Elapsed 1m 10s (remain 0m 57s) Loss: 0.0000(0.0096) \n","EVAL: [500/724] Elapsed 1m 28s (remain 0m 39s) Loss: 0.0210(0.0109) \n","EVAL: [600/724] Elapsed 1m 46s (remain 0m 21s) Loss: 0.0010(0.0106) \n","EVAL: [700/724] Elapsed 2m 3s (remain 0m 4s) Loss: 0.0000(0.0098) \n","EVAL: [723/724] Elapsed 2m 7s (remain 0m 0s) Loss: 0.0000(0.0097) \n","Epoch 2 - avg_train_loss: 0.0074  avg_val_loss: 0.0097  time: 924s\n","Epoch 2 - Score: 0.8582\n","Epoch 2 - Save Best Score: 0.8582 Model\n","Epoch: [3][0/2851] Elapsed 0m 0s (remain 29m 12s) Loss: 0.0021(0.0021) Grad: 13753.9316  LR: 0.000013  \n","Epoch: [3][100/2851] Elapsed 0m 30s (remain 13m 55s) Loss: 0.0000(0.0062) Grad: 145.4266  LR: 0.000013  \n","Epoch: [3][200/2851] Elapsed 0m 59s (remain 13m 7s) Loss: 0.0034(0.0056) Grad: 30381.8438  LR: 0.000013  \n","Epoch: [3][300/2851] Elapsed 1m 27s (remain 12m 19s) Loss: 0.0054(0.0056) Grad: 16647.8516  LR: 0.000013  \n","Epoch: [3][400/2851] Elapsed 1m 54s (remain 11m 40s) Loss: 0.0001(0.0052) Grad: 509.1555  LR: 0.000013  \n","Epoch: [3][500/2851] Elapsed 2m 22s (remain 11m 7s) Loss: 0.0212(0.0054) Grad: 47434.1250  LR: 0.000013  \n","Epoch: [3][600/2851] Elapsed 2m 49s (remain 10m 35s) Loss: 0.0028(0.0054) Grad: 6907.2500  LR: 0.000012  \n","Epoch: [3][700/2851] Elapsed 3m 17s (remain 10m 4s) Loss: 0.0015(0.0056) Grad: 10454.8789  LR: 0.000012  \n","Epoch: [3][800/2851] Elapsed 3m 44s (remain 9m 34s) Loss: 0.0288(0.0058) Grad: 25113.7852  LR: 0.000012  \n","Epoch: [3][900/2851] Elapsed 4m 12s (remain 9m 5s) Loss: 0.0001(0.0060) Grad: 323.7068  LR: 0.000012  \n","Epoch: [3][1000/2851] Elapsed 4m 39s (remain 8m 36s) Loss: 0.0130(0.0061) Grad: 125031.8359  LR: 0.000012  \n","Epoch: [3][1100/2851] Elapsed 5m 7s (remain 8m 8s) Loss: 0.0002(0.0059) Grad: 521.0079  LR: 0.000012  \n","Epoch: [3][1200/2851] Elapsed 5m 34s (remain 7m 39s) Loss: 0.0072(0.0060) Grad: 15202.5410  LR: 0.000011  \n","Epoch: [3][1300/2851] Elapsed 6m 2s (remain 7m 11s) Loss: 0.0001(0.0059) Grad: 328.7159  LR: 0.000011  \n","Epoch: [3][1400/2851] Elapsed 6m 29s (remain 6m 43s) Loss: 0.0032(0.0059) Grad: 10784.5381  LR: 0.000011  \n","Epoch: [3][1500/2851] Elapsed 6m 56s (remain 6m 15s) Loss: 0.0288(0.0060) Grad: 80430.1172  LR: 0.000011  \n","Epoch: [3][1600/2851] Elapsed 7m 24s (remain 5m 47s) Loss: 0.0083(0.0059) Grad: 26420.7930  LR: 0.000011  \n","Epoch: [3][1700/2851] Elapsed 7m 52s (remain 5m 19s) Loss: 0.0259(0.0059) Grad: 47409.2617  LR: 0.000011  \n","Epoch: [3][1800/2851] Elapsed 8m 19s (remain 4m 51s) Loss: 0.0003(0.0058) Grad: 2038.7853  LR: 0.000011  \n","Epoch: [3][1900/2851] Elapsed 8m 46s (remain 4m 23s) Loss: 0.0002(0.0058) Grad: 987.0793  LR: 0.000010  \n","Epoch: [3][2000/2851] Elapsed 9m 14s (remain 3m 55s) Loss: 0.0001(0.0059) Grad: 371.7654  LR: 0.000010  \n","Epoch: [3][2100/2851] Elapsed 9m 41s (remain 3m 27s) Loss: 0.0000(0.0060) Grad: 165.4255  LR: 0.000010  \n","Epoch: [3][2200/2851] Elapsed 10m 9s (remain 2m 59s) Loss: 0.0065(0.0060) Grad: 9053.8125  LR: 0.000010  \n","Epoch: [3][2300/2851] Elapsed 10m 37s (remain 2m 32s) Loss: 0.0116(0.0061) Grad: 10855.7441  LR: 0.000010  \n","Epoch: [3][2400/2851] Elapsed 11m 4s (remain 2m 4s) Loss: 0.0001(0.0061) Grad: 275.3572  LR: 0.000010  \n","Epoch: [3][2500/2851] Elapsed 11m 32s (remain 1m 36s) Loss: 0.0037(0.0061) Grad: 27989.9199  LR: 0.000009  \n","Epoch: [3][2600/2851] Elapsed 11m 59s (remain 1m 9s) Loss: 0.0178(0.0061) Grad: 17038.1836  LR: 0.000009  \n","Epoch: [3][2700/2851] Elapsed 12m 27s (remain 0m 41s) Loss: 0.0073(0.0061) Grad: 22400.9688  LR: 0.000009  \n","Epoch: [3][2800/2851] Elapsed 12m 55s (remain 0m 13s) Loss: 0.0121(0.0061) Grad: 44338.4062  LR: 0.000009  \n","Epoch: [3][2850/2851] Elapsed 13m 8s (remain 0m 0s) Loss: 0.0150(0.0061) Grad: 16377.2881  LR: 0.000009  \n","EVAL: [0/724] Elapsed 0m 0s (remain 5m 53s) Loss: 0.0005(0.0005) \n","EVAL: [100/724] Elapsed 0m 18s (remain 1m 51s) Loss: 0.0077(0.0075) \n","EVAL: [200/724] Elapsed 0m 35s (remain 1m 33s) Loss: 0.0000(0.0090) \n","EVAL: [300/724] Elapsed 0m 53s (remain 1m 14s) Loss: 0.0000(0.0089) \n","EVAL: [400/724] Elapsed 1m 10s (remain 0m 56s) Loss: 0.0000(0.0091) \n","EVAL: [500/724] Elapsed 1m 28s (remain 0m 39s) Loss: 0.0150(0.0109) \n","EVAL: [600/724] Elapsed 1m 45s (remain 0m 21s) Loss: 0.0014(0.0105) \n","EVAL: [700/724] Elapsed 2m 3s (remain 0m 4s) Loss: 0.0000(0.0096) \n","EVAL: [723/724] Elapsed 2m 6s (remain 0m 0s) Loss: 0.0000(0.0094) \n","Epoch 3 - avg_train_loss: 0.0061  avg_val_loss: 0.0094  time: 920s\n","Epoch 3 - Score: 0.8771\n","Epoch 3 - Save Best Score: 0.8771 Model\n","Epoch: [4][0/2851] Elapsed 0m 0s (remain 28m 54s) Loss: 0.0029(0.0029) Grad: 9765.5098  LR: 0.000009  \n","Epoch: [4][100/2851] Elapsed 0m 30s (remain 13m 47s) Loss: 0.0058(0.0046) Grad: 7605.9448  LR: 0.000009  \n","Epoch: [4][200/2851] Elapsed 0m 59s (remain 13m 9s) Loss: 0.0345(0.0054) Grad: 17088.2148  LR: 0.000009  \n","Epoch: [4][300/2851] Elapsed 1m 27s (remain 12m 21s) Loss: 0.0001(0.0053) Grad: 2748.6628  LR: 0.000008  \n","Epoch: [4][400/2851] Elapsed 1m 54s (remain 11m 42s) Loss: 0.0897(0.0054) Grad: 194458.3281  LR: 0.000008  \n","Epoch: [4][500/2851] Elapsed 2m 22s (remain 11m 7s) Loss: 0.0000(0.0057) Grad: 42.1938  LR: 0.000008  \n","Epoch: [4][600/2851] Elapsed 2m 49s (remain 10m 35s) Loss: 0.0042(0.0054) Grad: 5088.8247  LR: 0.000008  \n","Epoch: [4][700/2851] Elapsed 3m 17s (remain 10m 5s) Loss: 0.0023(0.0056) Grad: 11858.3984  LR: 0.000008  \n","Epoch: [4][800/2851] Elapsed 3m 44s (remain 9m 35s) Loss: 0.0147(0.0055) Grad: 23946.7500  LR: 0.000008  \n","Epoch: [4][900/2851] Elapsed 4m 12s (remain 9m 6s) Loss: 0.0065(0.0053) Grad: 14378.3896  LR: 0.000007  \n","Epoch: [4][1000/2851] Elapsed 4m 39s (remain 8m 37s) Loss: 0.0043(0.0052) Grad: 19601.9395  LR: 0.000007  \n","Epoch: [4][1100/2851] Elapsed 5m 7s (remain 8m 8s) Loss: 0.0472(0.0052) Grad: 114543.0703  LR: 0.000007  \n","Epoch: [4][1200/2851] Elapsed 5m 34s (remain 7m 40s) Loss: 0.0013(0.0051) Grad: 4207.0620  LR: 0.000007  \n","Epoch: [4][1300/2851] Elapsed 6m 2s (remain 7m 11s) Loss: 0.0057(0.0052) Grad: 103645.5000  LR: 0.000007  \n","Epoch: [4][1400/2851] Elapsed 6m 30s (remain 6m 43s) Loss: 0.0040(0.0052) Grad: 18240.6055  LR: 0.000007  \n","Epoch: [4][1500/2851] Elapsed 6m 57s (remain 6m 15s) Loss: 0.0002(0.0051) Grad: 1091.9108  LR: 0.000007  \n","Epoch: [4][1600/2851] Elapsed 7m 24s (remain 5m 47s) Loss: 0.0000(0.0051) Grad: 57.2197  LR: 0.000006  \n","Epoch: [4][1700/2851] Elapsed 7m 52s (remain 5m 19s) Loss: 0.0000(0.0051) Grad: 37.7031  LR: 0.000006  \n","Epoch: [4][1800/2851] Elapsed 8m 20s (remain 4m 51s) Loss: 0.0001(0.0051) Grad: 856.9548  LR: 0.000006  \n","Epoch: [4][1900/2851] Elapsed 8m 47s (remain 4m 23s) Loss: 0.0006(0.0051) Grad: 3166.4021  LR: 0.000006  \n","Epoch: [4][2000/2851] Elapsed 9m 15s (remain 3m 55s) Loss: 0.0006(0.0051) Grad: 7120.4678  LR: 0.000006  \n","Epoch: [4][2100/2851] Elapsed 9m 42s (remain 3m 28s) Loss: 0.0058(0.0051) Grad: 9200.6191  LR: 0.000006  \n","Epoch: [4][2200/2851] Elapsed 10m 10s (remain 3m 0s) Loss: 0.0022(0.0051) Grad: 5852.0835  LR: 0.000005  \n","Epoch: [4][2300/2851] Elapsed 10m 37s (remain 2m 32s) Loss: 0.0000(0.0051) Grad: 109.1858  LR: 0.000005  \n","Epoch: [4][2400/2851] Elapsed 11m 5s (remain 2m 4s) Loss: 0.0000(0.0051) Grad: 97.5289  LR: 0.000005  \n","Epoch: [4][2500/2851] Elapsed 11m 32s (remain 1m 36s) Loss: 0.0035(0.0051) Grad: 10408.0488  LR: 0.000005  \n","Epoch: [4][2600/2851] Elapsed 12m 0s (remain 1m 9s) Loss: 0.0005(0.0050) Grad: 2143.8376  LR: 0.000005  \n","Epoch: [4][2700/2851] Elapsed 12m 27s (remain 0m 41s) Loss: 0.0006(0.0050) Grad: 5506.1963  LR: 0.000005  \n","Epoch: [4][2800/2851] Elapsed 12m 55s (remain 0m 13s) Loss: 0.0000(0.0049) Grad: 52.9678  LR: 0.000005  \n","Epoch: [4][2850/2851] Elapsed 13m 9s (remain 0m 0s) Loss: 0.0005(0.0049) Grad: 1790.3232  LR: 0.000004  \n","EVAL: [0/724] Elapsed 0m 0s (remain 6m 3s) Loss: 0.0004(0.0004) \n","EVAL: [100/724] Elapsed 0m 18s (remain 1m 52s) Loss: 0.0026(0.0087) \n","EVAL: [200/724] Elapsed 0m 36s (remain 1m 33s) Loss: 0.0000(0.0100) \n","EVAL: [300/724] Elapsed 0m 53s (remain 1m 15s) Loss: 0.0000(0.0102) \n","EVAL: [400/724] Elapsed 1m 11s (remain 0m 57s) Loss: 0.0000(0.0100) \n","EVAL: [500/724] Elapsed 1m 29s (remain 0m 39s) Loss: 0.0203(0.0118) \n","EVAL: [600/724] Elapsed 1m 46s (remain 0m 21s) Loss: 0.0030(0.0113) \n","EVAL: [700/724] Elapsed 2m 4s (remain 0m 4s) Loss: 0.0000(0.0104) \n","EVAL: [723/724] Elapsed 2m 8s (remain 0m 0s) Loss: 0.0000(0.0104) \n","Epoch 4 - avg_train_loss: 0.0049  avg_val_loss: 0.0104  time: 922s\n","Epoch 4 - Score: 0.8786\n","Epoch 4 - Save Best Score: 0.8786 Model\n","Epoch: [5][0/2851] Elapsed 0m 0s (remain 29m 13s) Loss: 0.0399(0.0399) Grad: 43566.0781  LR: 0.000004  \n","Epoch: [5][100/2851] Elapsed 0m 29s (remain 13m 11s) Loss: 0.0047(0.0038) Grad: 8996.3037  LR: 0.000004  \n","Epoch: [5][200/2851] Elapsed 0m 57s (remain 12m 38s) Loss: 0.0032(0.0040) Grad: 17339.3379  LR: 0.000004  \n","Epoch: [5][300/2851] Elapsed 1m 25s (remain 12m 7s) Loss: 0.0001(0.0039) Grad: 903.3726  LR: 0.000004  \n","Epoch: [5][400/2851] Elapsed 1m 54s (remain 11m 36s) Loss: 0.0073(0.0039) Grad: 15491.0068  LR: 0.000004  \n","Epoch: [5][500/2851] Elapsed 2m 22s (remain 11m 9s) Loss: 0.0015(0.0041) Grad: 3359.8323  LR: 0.000004  \n","Epoch: [5][600/2851] Elapsed 2m 50s (remain 10m 38s) Loss: 0.0000(0.0041) Grad: 146.7607  LR: 0.000004  \n","Epoch: [5][700/2851] Elapsed 3m 18s (remain 10m 8s) Loss: 0.0620(0.0042) Grad: 41079.0664  LR: 0.000003  \n","Epoch: [5][800/2851] Elapsed 3m 45s (remain 9m 38s) Loss: 0.0052(0.0042) Grad: 13967.8223  LR: 0.000003  \n","Epoch: [5][900/2851] Elapsed 4m 13s (remain 9m 8s) Loss: 0.0039(0.0041) Grad: 7205.0420  LR: 0.000003  \n","Epoch: [5][1000/2851] Elapsed 4m 41s (remain 8m 39s) Loss: 0.0000(0.0041) Grad: 41.5706  LR: 0.000003  \n","Epoch: [5][1100/2851] Elapsed 5m 9s (remain 8m 11s) Loss: 0.0202(0.0041) Grad: 39421.8984  LR: 0.000003  \n","Epoch: [5][1200/2851] Elapsed 5m 36s (remain 7m 42s) Loss: 0.0000(0.0042) Grad: 60.7275  LR: 0.000003  \n","Epoch: [5][1300/2851] Elapsed 6m 4s (remain 7m 14s) Loss: 0.0001(0.0042) Grad: 607.4169  LR: 0.000002  \n","Epoch: [5][1400/2851] Elapsed 6m 32s (remain 6m 45s) Loss: 0.0039(0.0042) Grad: 23383.1465  LR: 0.000002  \n","Epoch: [5][1500/2851] Elapsed 6m 59s (remain 6m 17s) Loss: 0.0024(0.0043) Grad: 6779.8765  LR: 0.000002  \n","Epoch: [5][1600/2851] Elapsed 7m 27s (remain 5m 49s) Loss: 0.0043(0.0042) Grad: 2397.0898  LR: 0.000002  \n","Epoch: [5][1700/2851] Elapsed 7m 55s (remain 5m 21s) Loss: 0.0000(0.0042) Grad: 260.2547  LR: 0.000002  \n","Epoch: [5][1800/2851] Elapsed 8m 22s (remain 4m 53s) Loss: 0.0002(0.0042) Grad: 1135.0909  LR: 0.000002  \n","Epoch: [5][1900/2851] Elapsed 8m 50s (remain 4m 25s) Loss: 0.0015(0.0042) Grad: 5362.9697  LR: 0.000001  \n","Epoch: [5][2000/2851] Elapsed 9m 18s (remain 3m 57s) Loss: 0.0007(0.0042) Grad: 5343.2441  LR: 0.000001  \n","Epoch: [5][2100/2851] Elapsed 9m 45s (remain 3m 29s) Loss: 0.0327(0.0043) Grad: 19477.0078  LR: 0.000001  \n","Epoch: [5][2200/2851] Elapsed 10m 13s (remain 3m 1s) Loss: 0.0062(0.0043) Grad: 5653.7095  LR: 0.000001  \n","Epoch: [5][2300/2851] Elapsed 10m 41s (remain 2m 33s) Loss: 0.0000(0.0042) Grad: 22.6681  LR: 0.000001  \n","Epoch: [5][2400/2851] Elapsed 11m 8s (remain 2m 5s) Loss: 0.0005(0.0042) Grad: 3408.9851  LR: 0.000001  \n","Epoch: [5][2500/2851] Elapsed 11m 36s (remain 1m 37s) Loss: 0.0000(0.0042) Grad: 22.3128  LR: 0.000001  \n","Epoch: [5][2600/2851] Elapsed 12m 3s (remain 1m 9s) Loss: 0.0000(0.0042) Grad: 288.8801  LR: 0.000000  \n","Epoch: [5][2700/2851] Elapsed 12m 31s (remain 0m 41s) Loss: 0.0006(0.0042) Grad: 10521.9238  LR: 0.000000  \n","Epoch: [5][2800/2851] Elapsed 12m 59s (remain 0m 13s) Loss: 0.0046(0.0042) Grad: 26506.2266  LR: 0.000000  \n","Epoch: [5][2850/2851] Elapsed 13m 13s (remain 0m 0s) Loss: 0.0001(0.0042) Grad: 369.4055  LR: 0.000000  \n","EVAL: [0/724] Elapsed 0m 0s (remain 5m 59s) Loss: 0.0005(0.0005) \n","EVAL: [100/724] Elapsed 0m 18s (remain 1m 51s) Loss: 0.0051(0.0089) \n","EVAL: [200/724] Elapsed 0m 35s (remain 1m 33s) Loss: 0.0000(0.0104) \n","EVAL: [300/724] Elapsed 0m 53s (remain 1m 15s) Loss: 0.0001(0.0105) \n","EVAL: [400/724] Elapsed 1m 10s (remain 0m 57s) Loss: 0.0000(0.0104) \n","EVAL: [500/724] Elapsed 1m 28s (remain 0m 39s) Loss: 0.0201(0.0122) \n","EVAL: [600/724] Elapsed 1m 46s (remain 0m 21s) Loss: 0.0044(0.0116) \n","EVAL: [700/724] Elapsed 2m 3s (remain 0m 4s) Loss: 0.0000(0.0107) \n","EVAL: [723/724] Elapsed 2m 7s (remain 0m 0s) Loss: 0.0000(0.0106) \n","Epoch 5 - avg_train_loss: 0.0042  avg_val_loss: 0.0106  time: 925s\n","Epoch 5 - Score: 0.8806\n","Epoch 5 - Save Best Score: 0.8806 Model\n","========== fold: 2 training ==========\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp027/checkpoint-52068/pytorch_model.bin\n","Epoch: [1][0/2871] Elapsed 0m 0s (remain 35m 53s) Loss: 0.4390(0.4390) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/2871] Elapsed 0m 31s (remain 14m 12s) Loss: 0.2349(0.4439) Grad: 23080.2324  LR: 0.000001  \n","Epoch: [1][200/2871] Elapsed 0m 58s (remain 13m 0s) Loss: 0.0852(0.3331) Grad: 6294.9194  LR: 0.000003  \n","Epoch: [1][300/2871] Elapsed 1m 26s (remain 12m 17s) Loss: 0.0379(0.2423) Grad: 703.7405  LR: 0.000004  \n","Epoch: [1][400/2871] Elapsed 1m 54s (remain 11m 42s) Loss: 0.0650(0.1923) Grad: 1561.4142  LR: 0.000006  \n","Epoch: [1][500/2871] Elapsed 2m 21s (remain 11m 10s) Loss: 0.0439(0.1622) Grad: 1196.0491  LR: 0.000007  \n","Epoch: [1][600/2871] Elapsed 2m 49s (remain 10m 40s) Loss: 0.0277(0.1419) Grad: 1124.1367  LR: 0.000008  \n","Epoch: [1][700/2871] Elapsed 3m 17s (remain 10m 10s) Loss: 0.0576(0.1264) Grad: 3234.1377  LR: 0.000010  \n","Epoch: [1][800/2871] Elapsed 3m 44s (remain 9m 40s) Loss: 0.0296(0.1142) Grad: 3411.6050  LR: 0.000011  \n","Epoch: [1][900/2871] Elapsed 4m 12s (remain 9m 12s) Loss: 0.0140(0.1038) Grad: 6160.6138  LR: 0.000013  \n","Epoch: [1][1000/2871] Elapsed 4m 39s (remain 8m 43s) Loss: 0.0188(0.0951) Grad: 4645.3999  LR: 0.000014  \n","Epoch: [1][1100/2871] Elapsed 5m 7s (remain 8m 14s) Loss: 0.0280(0.0877) Grad: 7215.1641  LR: 0.000015  \n","Epoch: [1][1200/2871] Elapsed 5m 34s (remain 7m 45s) Loss: 0.0056(0.0817) Grad: 2761.6934  LR: 0.000017  \n","Epoch: [1][1300/2871] Elapsed 6m 1s (remain 7m 16s) Loss: 0.0412(0.0767) Grad: 11436.1279  LR: 0.000018  \n","Epoch: [1][1400/2871] Elapsed 6m 29s (remain 6m 48s) Loss: 0.0094(0.0723) Grad: 2300.2751  LR: 0.000020  \n","Epoch: [1][1500/2871] Elapsed 6m 56s (remain 6m 20s) Loss: 0.0232(0.0685) Grad: 3583.6069  LR: 0.000020  \n","Epoch: [1][1600/2871] Elapsed 7m 23s (remain 5m 52s) Loss: 0.0045(0.0650) Grad: 1111.6240  LR: 0.000020  \n","Epoch: [1][1700/2871] Elapsed 7m 51s (remain 5m 24s) Loss: 0.0109(0.0619) Grad: 2678.3564  LR: 0.000020  \n","Epoch: [1][1800/2871] Elapsed 8m 18s (remain 4m 56s) Loss: 0.0170(0.0592) Grad: 5813.1333  LR: 0.000019  \n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-31-98e5515dc0a9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi_fold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi_fold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_fold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                 \u001b[0m_oof_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                 \u001b[0moof_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moof_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_oof_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moof_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"oof_df.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-30-bf40143adb0a>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(df, i_fold, device)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         )\n\u001b[1;32m     71\u001b[0m         avg_val_loss, val_preds = valid_fn(\n","\u001b[0;32m<ipython-input-27-581e435ea304>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(train_dataloader, model, criterion, optimizer, epoch, scheduler, device)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mgrad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"name":"nbme-exp030.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"papermill":{"default_parameters":{},"duration":641.572862,"end_time":"2022-02-27T11:39:50.972497","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-02-27T11:29:09.399635","version":"2.3.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"4b7d943344f243b98e7a7d8ec4fa4e07":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0ca6df7edd2b428db49cb300593f32f8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ab79466679dd40428cb7b1d4455b794d","IPY_MODEL_34a057ee78bb45629f6bf81808d5f301","IPY_MODEL_4882ff75a58b4919b0d6c9faedab896b"]}},"0ca6df7edd2b428db49cb300593f32f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ab79466679dd40428cb7b1d4455b794d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_556850faa8ab498484f277c55ab4157c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fc7867840e4b431a8a75e235b25d931d"}},"34a057ee78bb45629f6bf81808d5f301":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_cfa36ac2690c47429743d9511338b9f0","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2464616,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2464616,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a8823f24ffa046caa5fb2a88025c641b"}},"4882ff75a58b4919b0d6c9faedab896b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7ca7337b55094343b4ad3a30c703af50","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2.35M/2.35M [00:00&lt;00:00, 6.60MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_206e7187463c4681bb085de8c1919e1a"}},"556850faa8ab498484f277c55ab4157c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fc7867840e4b431a8a75e235b25d931d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cfa36ac2690c47429743d9511338b9f0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a8823f24ffa046caa5fb2a88025c641b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7ca7337b55094343b4ad3a30c703af50":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"206e7187463c4681bb085de8c1919e1a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9dcc2a0264fb40bfbc19c614e9ad5612":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a08cef401044437f8a7c92969e359296","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_aed60a6f514a48468173b02441920989","IPY_MODEL_68a9117fa12c4410996ce5364065ad24","IPY_MODEL_8a5e5faea7b84463a139555dc9ab5c30"]}},"a08cef401044437f8a7c92969e359296":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aed60a6f514a48468173b02441920989":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_de95c0127e334178a79b42330cc97e65","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d46fd266de0d43bdb0b40e83fc7a420b"}},"68a9117fa12c4410996ce5364065ad24":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d7572660715c4963babc376fdeec9083","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":52,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":52,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3860c619873a4c73b6f7f5acdf1469c4"}},"8a5e5faea7b84463a139555dc9ab5c30":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2ef35c30fc5f4d08b0ec65a8954e3907","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 52.0/52.0 [00:00&lt;00:00, 315B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6dc8c533b0cc4187b00959ab75347b09"}},"de95c0127e334178a79b42330cc97e65":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d46fd266de0d43bdb0b40e83fc7a420b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d7572660715c4963babc376fdeec9083":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3860c619873a4c73b6f7f5acdf1469c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2ef35c30fc5f4d08b0ec65a8954e3907":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6dc8c533b0cc4187b00959ab75347b09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fb1512ff22f242faaef107942928e711":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c2b975a94d7a439bb84dba22a14bb280","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b6adad27b75d4fdd8117941993532b83","IPY_MODEL_afa8a789d28540499c1c9d5d2b9831e9","IPY_MODEL_2dace65f8dd045e59ce5c8ef0037b947"]}},"c2b975a94d7a439bb84dba22a14bb280":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b6adad27b75d4fdd8117941993532b83":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c53743cc418e4cb0b8c5db3ffc035323","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9a601becadb7429eadb73cf88cad5620"}},"afa8a789d28540499c1c9d5d2b9831e9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c829bf7571f24690a7651fd1a67331a3","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":580,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":580,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8006419d65564fae8a9a1a16010cfaa3"}},"2dace65f8dd045e59ce5c8ef0037b947":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_670d34a6a17d43c294f2eb9def9d950e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 580/580 [00:00&lt;00:00, 7.46kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f9d70075da324627bead6ea379c8815e"}},"c53743cc418e4cb0b8c5db3ffc035323":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9a601becadb7429eadb73cf88cad5620":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c829bf7571f24690a7651fd1a67331a3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8006419d65564fae8a9a1a16010cfaa3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"670d34a6a17d43c294f2eb9def9d950e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f9d70075da324627bead6ea379c8815e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"377f0d897ff048c29f71d21ceaa99c1f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_66bb82a3ec6b4eada537b0e101d1d108","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_01a78948725e448fad47715fcd391201","IPY_MODEL_947f049c16384e99bd31ffcde43f3f4c","IPY_MODEL_69a1d2bea79948c781260c26f3087f70"]}},"66bb82a3ec6b4eada537b0e101d1d108":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"01a78948725e448fad47715fcd391201":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8c16f35b39a64cefae61f7fbcbe62e53","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_237b7a26b3f04b53a97cd1efd70f043f"}},"947f049c16384e99bd31ffcde43f3f4c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c3708ee7654f4173b713d8d0f72514ff","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":42146,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":42146,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d676c966f5fe466ea6c6e3245f6ac87b"}},"69a1d2bea79948c781260c26f3087f70":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4a307a7c7270442181704f656e1885b6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 42146/42146 [00:29&lt;00:00, 1758.72it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_67373530dac043a48164a37d707a4b29"}},"8c16f35b39a64cefae61f7fbcbe62e53":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"237b7a26b3f04b53a97cd1efd70f043f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c3708ee7654f4173b713d8d0f72514ff":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d676c966f5fe466ea6c6e3245f6ac87b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4a307a7c7270442181704f656e1885b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"67373530dac043a48164a37d707a4b29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"79b46d9ce7374a6f86d3b4dc913c3d09":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d1d804e434f14b38ba2cd869aede2c5f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ad56311a7c4d44389f8b36e84263fa9a","IPY_MODEL_4802446c32774733ad4f30e5fdf41ad7","IPY_MODEL_f2f9a25ac0f545a38d7cd5b3735e16da"]}},"d1d804e434f14b38ba2cd869aede2c5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ad56311a7c4d44389f8b36e84263fa9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ccc40ad82b9742138d2a358929ee28f3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9bd48c1054c043c891c639ddba4bcc27"}},"4802446c32774733ad4f30e5fdf41ad7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7e3a957050ee4ff0b9646bf48a48e9d0","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":143,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":143,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_90e215dc04664fccb690daec60bfd19e"}},"f2f9a25ac0f545a38d7cd5b3735e16da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b3743c6ae2ac4d228263b521bb623451","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 143/143 [00:00&lt;00:00, 2464.43it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a1466613a2ae4654ab8a95b2f92638d9"}},"ccc40ad82b9742138d2a358929ee28f3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9bd48c1054c043c891c639ddba4bcc27":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7e3a957050ee4ff0b9646bf48a48e9d0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"90e215dc04664fccb690daec60bfd19e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b3743c6ae2ac4d228263b521bb623451":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a1466613a2ae4654ab8a95b2f92638d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":5}