{"cells":[{"cell_type":"markdown","metadata":{"id":"aa1f8e80"},"source":["## References"],"id":"aa1f8e80"},{"cell_type":"markdown","metadata":{"id":"c0138fac"},"source":["- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"],"id":"c0138fac"},{"cell_type":"markdown","metadata":{"id":"cf1dfda9"},"source":["## Configurations"],"id":"cf1dfda9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a7a78d25"},"outputs":[],"source":["EXP_NAME = \"nbme-exp018\"\n","ENV = \"colab\"\n","DEBUG_MODE = False\n","SUBMISSION_MODE = False"],"id":"a7a78d25"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ecc4e4d"},"outputs":[],"source":["class CFG:\n","    env=ENV\n","    exp_name=EXP_NAME\n","    debug=DEBUG_MODE\n","    submission=SUBMISSION_MODE\n","    apex=True\n","    input_dir=None\n","    output_dir=None\n","    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n","    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n","    competition_name=\"nbme-score-clinical-patient-notes\"\n","    id_col=\"id\"\n","    target_col=\"location\"\n","    pretrained_model_name=\"microsoft/deberta-base\"\n","    tokenizer=None\n","    max_len=None\n","    output_dim=1\n","    dropout=0.2\n","    num_workers=4\n","    batch_size=8\n","    lr=2e-5\n","    betas=(0.9, 0.98)\n","    weight_decay=0.1\n","    num_warmup_steps_rate=0.1\n","    batch_scheduler=True\n","    epochs=5\n","    n_fold=5\n","    train_fold=[0, 1, 2, 3, 4]\n","    seed=71\n","    gradient_accumulation_steps=1\n","    max_grad_norm=1000\n","    print_freq=100\n","    train=True\n","    inference=True"],"id":"4ecc4e4d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"3894c88b"},"outputs":[],"source":["if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.train_fold = [0, 1]\n","\n","if CFG.submission:\n","    CFG.train = False\n","    CFG.inference = True"],"id":"3894c88b"},{"cell_type":"markdown","metadata":{"id":"31768c85"},"source":["## Directory Settings"],"id":"31768c85"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28407,"status":"ok","timestamp":1646348934292,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"00e7d967","outputId":"a2f8f802-2b51-441e-8548-db6b020e5108"},"outputs":[{"output_type":"stream","name":"stdout","text":["colab\n","Mounted at /content/drive\n","Collecting transformers\n","  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 4.9 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 58.4 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 76.4 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 65.9 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 7.3 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.17.0\n"]}],"source":["import sys\n","from pathlib import Path\n","\n","\n","print(CFG.env)\n","if CFG.env == \"colab\":\n","    # colab環境\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","    # install packages\n","    !pip install transformers\n","\n","elif CFG.env == \"local\":\n","    # ローカルサーバ\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","\n","elif CFG.env == \"kaggle\":\n","    # kaggle環境\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./\")"],"id":"00e7d967"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d726b7d9"},"outputs":[],"source":["import gc\n","import os\n","import ast\n","import time\n","import math\n","import random\n","import itertools\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","from scipy.optimize import minimize\n","from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as T\n","from torchvision.io import read_image\n","from torch.utils.data import DataLoader, Dataset\n","\n","from transformers import AutoModelForMaskedLM\n","from transformers import BartModel,BertModel,BertTokenizer\n","from transformers import DebertaModel,DebertaTokenizer\n","from transformers import RobertaModel,RobertaTokenizer\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"id":"d726b7d9"},{"cell_type":"markdown","metadata":{"id":"b6d82f71"},"source":["## Utilities"],"id":"b6d82f71"},{"cell_type":"code","execution_count":null,"metadata":{"id":"95abbe2c"},"outputs":[],"source":["def micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on binary arrays.\n","\n","    Args:\n","        preds (list of lists of ints): Predictions.\n","        truths (list of lists of ints): Ground truths.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    # Micro : aggregating over all instances\n","    preds = np.concatenate(preds)\n","    truths = np.concatenate(truths)\n","    return f1_score(truths, preds)\n","\n","\n","def spans_to_binary(spans, length=None):\n","    \"\"\"\n","    Converts spans to a binary array indicating whether each character is in the span.\n","\n","    Args:\n","        spans (list of lists of two ints): Spans.\n","\n","    Returns:\n","        np array [length]: Binarized spans.\n","    \"\"\"\n","    length = np.max(spans) if length is None else length\n","    binary = np.zeros(length)\n","    for start, end in spans:\n","        binary[start:end] = 1\n","    return binary\n","\n","\n","def span_micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on spans.\n","\n","    Args:\n","        preds (list of lists of two ints): Prediction spans.\n","        truths (list of lists of two ints): Ground truth spans.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    bin_preds = []\n","    bin_truths = []\n","    for pred, truth in zip(preds, truths):\n","        if not len(pred) and not len(truth):\n","            continue\n","        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n","        bin_preds.append(spans_to_binary(pred, length))\n","        bin_truths.append(spans_to_binary(truth, length))\n","    return micro_f1(bin_preds, bin_truths)\n","\n","\n","def get_score(y_true, y_pred):\n","    score = span_micro_f1(y_true, y_pred)\n","    return score"],"id":"95abbe2c"},{"cell_type":"code","execution_count":null,"metadata":{"id":"832ee36d"},"outputs":[],"source":["def create_labels_for_scoring(df):\n","    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n","    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n","    for i in range(len(df)):\n","        lst = df.loc[i, \"location\"]\n","        if lst:\n","            new_lst = \";\".join(lst)\n","            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n","\n","    # create labels\n","    truths = []\n","    for location_list in df[\"location_for_create_labels\"].values:\n","        truth = []\n","        if len(location_list) > 0:\n","            location = location_list[0]\n","            for loc in [s.split() for s in location.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                truth.append([start, end])\n","        truths.append(truth)\n","\n","    return truths\n","\n","\n","def get_char_probs(texts, token_probs, tokenizer):\n","    res = [np.zeros(len(t)) for t in texts]\n","    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n","        encoded = tokenizer(\n","            text=text,\n","            max_length=CFG.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n","            start, end = offset_mapping\n","            res[i][start:end] = pred\n","    return res\n","\n","\n","def get_predicted_location_str(char_probs, th=0.5):\n","    results = []\n","    for char_prob in char_probs:\n","        result = np.where(char_prob >= th)[0] + 1\n","        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n","        result = [f\"{min(r)} {max(r)}\" for r in result]\n","        result = \";\".join(result)\n","        results.append(result)\n","    return results\n","\n","\n","def get_predictions(results):\n","    predictions = []\n","    for result in results:\n","        prediction = []\n","        if result != \"\":\n","            for loc in [s.split() for s in result.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                prediction.append([start, end])\n","        predictions.append(prediction)\n","    return predictions\n","\n","\n","def scoring(df, th=0.5):\n","    labels = create_labels_for_scoring(df)\n","\n","    token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n","    char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n","    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n","    preds = get_predictions(predicted_location_str)\n","\n","    score = get_score(labels, preds)\n","    return score\n","\n","\n","def get_best_thres(oof_df):\n","    def f1_opt(x):\n","        return -1 * scoring(oof_df, th=x)\n","\n","    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n","    return best_thres"],"id":"832ee36d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"918828a7"},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return \"%dm %ds\" % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"],"id":"918828a7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d02a78e1"},"outputs":[],"source":["seed_everything()"],"id":"d02a78e1"},{"cell_type":"markdown","metadata":{"id":"47266f39"},"source":["## Data Loading"],"id":"47266f39"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3924,"status":"ok","timestamp":1646314193954,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"20fed6da","outputId":"03de1791-a585-4fef-d767-7720b7fdd181"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14300, 6), (143, 3), (42146, 3), (5, 4))"]},"metadata":{},"execution_count":10}],"source":["train = pd.read_csv(CFG.input_dir / \"train.csv\")\n","features = pd.read_csv(CFG.input_dir / \"features.csv\")\n","patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n","test = pd.read_csv(CFG.input_dir / \"test.csv\")\n","\n","train.shape, features.shape, patient_notes.shape, test.shape"],"id":"20fed6da"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e67d0132"},"outputs":[],"source":["if CFG.debug:\n","    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n","    print(train.shape)"],"id":"e67d0132"},{"cell_type":"markdown","metadata":{"id":"47bca11a"},"source":["## Preprocessing"],"id":"47bca11a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d9c8e9ba"},"outputs":[],"source":["def preprocess_features(features):\n","    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n","    return features\n","\n","\n","features = preprocess_features(features)"],"id":"d9c8e9ba"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1646314193957,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"7ef41e18","outputId":"34bda720-43b4-4d69-a685-67aa07774ce5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14300, 8), (5, 6))"]},"metadata":{},"execution_count":13}],"source":["train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","\n","train.shape, test.shape"],"id":"7ef41e18"},{"cell_type":"code","execution_count":null,"metadata":{"id":"8233df16"},"outputs":[],"source":["train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n","train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"],"id":"8233df16"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1646314193957,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"e9143e61","outputId":"3e939bc7-203f-4792-8030-30cdcd11baeb"},"outputs":[{"output_type":"display_data","data":{"text/plain":["0    4399\n","1    8181\n","2    1296\n","3     287\n","4      99\n","5      27\n","6       9\n","7       1\n","8       1\n","Name: annotation_length, dtype: int64"]},"metadata":{}}],"source":["train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n","display(train['annotation_length'].value_counts().sort_index())"],"id":"e9143e61"},{"cell_type":"markdown","metadata":{"id":"6bdc7949"},"source":["## CV split"],"id":"6bdc7949"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c4acf61d"},"outputs":[],"source":["def get_groupkfold(df, group_name):\n","    groups = df[group_name].unique()\n","\n","    kf = KFold(\n","        n_splits=CFG.n_fold,\n","        shuffle=True,\n","        random_state=CFG.seed,\n","    )\n","    folds_ids = []\n","    for i_fold, (_, val_group_idx) in enumerate(kf.split(groups)):\n","        val_group = groups[val_group_idx]\n","        is_val = df[group_name].isin(val_group)\n","        val_idx = df[is_val].index\n","        df.loc[val_idx, \"fold\"] = int(i_fold)\n","\n","    df[\"fold\"] = df[\"fold\"].astype(int)\n","    return df"],"id":"c4acf61d"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"elapsed":415,"status":"ok","timestamp":1646314194365,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"2ca0c08e","outputId":"d387d025-6fab-41af-aecd-a3c600b092e6"},"outputs":[{"output_type":"display_data","data":{"text/plain":["fold\n","0    2902\n","1    2894\n","2    2813\n","3    2791\n","4    2900\n","dtype: int64"]},"metadata":{}}],"source":["train = get_groupkfold(train, \"pn_num\")\n","display(train.groupby(\"fold\").size())"],"id":"2ca0c08e"},{"cell_type":"markdown","metadata":{"id":"a8560070"},"source":["## Setup tokenizer"],"id":"a8560070"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c316b13f","colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["de6402d4aa614af7b1d58e0ce9703d29","0cb0dcb8b6c44532bf0f46c3b31cd403","a390e5c1f7fb4071b9ef40391f5899ea","be8f0e42e2974ee5847fcfee3961bfa8","34cffccd19ca4c4982d306d2e6539ec6","7cfd44dc72ed4c62aac45c29432080e8","c252519b399b42a2bf584999cd5b203c","f7f531db4e6343c5bf8abc525abcc00e","c1b0d317c72744f5bf6d2262433e69eb","ec6dda688bc844929d6b40bc18ce3721","c17a85f150184b4f8537415be182de63","4748eb6316f84473a53b593721fdad30","72288a2574c043bd936114bcedeb7a87","a00864cef7c34606802489ec9de53c82","f8e7a617a1c74e8c925fcd5d3a724e6a","876e1f875b274c3b814decf2b9123b44","3308cb4b493044e78cd50761e2c393f9","c775606a5e794b06a5d2fd889d814cc6","59f9d735d8c4493690f2c730b7b1293f","bfdce0a738884ff8950df0916f0279d6","20b5d6353577421bafeb5ad30edb4755","7ec72f88b4fa41808b509568a9bd6d22","4e0d3e0ba644490f81a4907b9f479977","e4cb579869b94901b10907352874d270","976c4c15e2ab4462998f17e956f62800","96d35ce8a6fc48b994b48bb6bafd6f13","7f2e537dda58431c83ee07ba6bab9996","dc2ca91cc3fc40d8b7d75c9fe25fa261","97ecf84fe68043c199f350fb5d7aecbf","b722d8c874d14a2e8fb162409ce78482","294c6af870b44f5a81cc050c2cbacc9a","3199b1d33b0b4816aaaafa6fdcb2a94b","212331f2abac46d7b6fd456c90d92555","46690d244b894522becd5b590e775047","672ab6fb002545fbb98df4b1bd00a2ad","ad16c9931ad748e0b440d41310939567","d2a17d161f5d4080a19b34fe67c00aec","fac192764b1141d0b5e0875d0508264b","e3256ddc0972490b9dd42eb06e77f056","8dfc300d95774958aa7ab2c31161e70b","99015149fe674971ba9c9fbf4404dc7c","32640c31b69c4ddb989a289b0a078b75","2753130aed024d97af79c1631dc3fb8c","95de06adf9584609a2d4c2414aace49e"]},"executionInfo":{"status":"ok","timestamp":1646349001809,"user_tz":-540,"elapsed":3364,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}},"outputId":"5abec21a-18d9-4ade-f544-82fc0d08d4ce"},"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"de6402d4aa614af7b1d58e0ce9703d29","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4748eb6316f84473a53b593721fdad30","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/474 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4e0d3e0ba644490f81a4907b9f479977","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"46690d244b894522becd5b590e775047","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"]},"metadata":{}}],"source":["if CFG.submission:\n","    tokenizer = AutoTokenizer.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n","else:\n","    tokenizer = AutoTokenizer.from_pretrained(CFG.pretrained_model_name)\n","    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n","\n","CFG.tokenizer = tokenizer"],"id":"c316b13f"},{"cell_type":"markdown","metadata":{"id":"e689a7fc"},"source":["## Create dataset"],"id":"e689a7fc"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["2603838b826d4a179cec09dda44ccc82","68fb52b1bdfa4885b624d9e1a599d321","7ee04e5716ae4c7080bf4471416be12b","452e67001b1344efb1d19b3d8c043861","8e73e68e7d0c4c10a91790b05191b9b2","705d0aed36f144c98c0e395d873d530f","0837d1ae78d14fcb93ef8040ec660720","296cff3e7b0d494b878e0c7c3e39d517","812e9ec39fb04642aed692895981f8db","2969834c57694d0eb6d85f9c4f795b53","dd743a29be3c4dbf981c3425ce4acd2c"]},"executionInfo":{"elapsed":31928,"status":"ok","timestamp":1646314233909,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"df31758e","outputId":"926aa414-60ba-416c-ef76-6c1e1f9ed7a6"},"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2603838b826d4a179cec09dda44ccc82","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/42146 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 433\n"]}],"source":["pn_history_lengths = []\n","tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    pn_history_lengths.append(length)\n","\n","print(\"max length:\", np.max(pn_history_lengths))"],"id":"df31758e"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["e969a23ed18f45239447045ba1375af7","d02045aaf05c4ade8be8e4d262daf052","e09471f7a148419597d69e9af4ccb481","3eefbd0b19d54dfdbd2f8baf37ac48b1","03c769b988784c7ba259be9730a62e09","d14c11443d3a47789fc37e4591f9162b","56a140221c2a45339fc63e10305a9904","ccfbf231a1ca4c50bf76498325e07b79","d90a5ddc207849698b7ded16468420e8","29aee77f64c0400cb6299104f51e7d6d","b96ee18df24941518b5670cd9c694575"]},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1646314233910,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"3caff24a","outputId":"2a79f219-4069-44bc-d7a4-3175d3d4ce5b"},"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e969a23ed18f45239447045ba1375af7","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/143 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 30\n"]}],"source":["feature_text_lengths = []\n","tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    feature_text_lengths.append(length)\n","\n","print(\"max length:\", np.max(feature_text_lengths))"],"id":"3caff24a"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1646314233910,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"756d83ff","outputId":"877cc924-5ccd-4e80-84f0-e0e2d811aad3"},"outputs":[{"output_type":"stream","name":"stdout","text":["max length: 466\n"]}],"source":["CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n","\n","print(\"max length:\", CFG.max_len)"],"id":"756d83ff"},{"cell_type":"code","execution_count":null,"metadata":{"id":"054b899a"},"outputs":[],"source":["class TrainingDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","        self.annotation_lengths = self.df[\"annotation_length\"].values\n","        self.locations = self.df[\"location\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def _create_label(self, pn_history, annotation_length, location_list):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        offset_mapping = encoded[\"offset_mapping\"]\n","        ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n","        label = np.zeros(len(offset_mapping))\n","        label[ignore_idxes] = -1\n","\n","        if annotation_length > 0:\n","            for location in location_list:\n","                for loc in [s.split() for s in location.split(\";\")]:\n","                    start, end = int(loc[0]), int(loc[1])\n","                    start_idx = -1\n","                    end_idx = -1\n","                    for idx in range(len(offset_mapping)):\n","                        if (start_idx == -1) & (start < offset_mapping[idx][0]):\n","                            start_idx = idx - 1\n","                        if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n","                            end_idx = idx + 1\n","                    if start_idx == -1:\n","                        start_idx = end_idx\n","                    if (start_idx != -1) & (end_idx != -1):\n","                        label[start_idx:end_idx] = 1\n","\n","        return torch.tensor(label, dtype=torch.float)\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n","        return input_, label"],"id":"054b899a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1d58367c"},"outputs":[],"source":["class TestDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        return input_"],"id":"1d58367c"},{"cell_type":"markdown","metadata":{"id":"8c57abef"},"source":["## Model"],"id":"8c57abef"},{"cell_type":"code","execution_count":null,"metadata":{"id":"54f92d89"},"outputs":[],"source":["class CustomModel(nn.Module):\n","    def __init__(self, cfg, model_config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        if model_config_path is None:\n","            self.model_config = AutoConfig.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                output_hidden_states=True,\n","            )\n","        else:\n","            self.model_config = torch.load(model_config_path)\n","\n","        if pretrained:\n","            self.backbone = AutoModel.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                config=self.model_config,\n","            )\n","            print(f\"Load weight from pretrained\")\n","        else:\n","            #self.backbone = AutoModel.from_config(self.model_config)\n","            itpt = AutoModelForMaskedLM.from_config(self.model_config)\n","            path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp009/checkpoint-129000/pytorch_model.bin\")\n","            state_dict = torch.load(path)\n","            itpt.load_state_dict(state_dict)\n","            self.backbone = itpt.deberta\n","            print(f\"Load weight from {path}\")\n","\n","        self.fc = nn.Sequential(\n","            nn.Dropout(self.cfg.dropout),\n","            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n","        )\n","\n","    def forward(self, inputs):\n","        h = self.backbone(**inputs)[\"last_hidden_state\"]\n","        output = self.fc(h)\n","        return output"],"id":"54f92d89"},{"cell_type":"markdown","metadata":{"id":"91401041"},"source":["## Training"],"id":"91401041"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5658,"status":"ok","timestamp":1646314240093,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"Vu6qpUwmAizr","outputId":"149eb149-b5a6-4cb7-842d-177d6beb34e0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 3.0,\n"," 1: 1.0,\n"," 2: 1.0,\n"," 3: 1.0,\n"," 4: 1.0,\n"," 5: 1.0,\n"," 6: 1.0,\n"," 7: 1.0,\n"," 8: 1.0,\n"," 9: 1.0,\n"," 10: 1.0,\n"," 11: 1.0,\n"," 12: 1.0,\n"," 13: 1.0,\n"," 14: 1.0,\n"," 15: 1.0,\n"," 16: 1.0,\n"," 17: 1.0,\n"," 18: 1.0,\n"," 19: 1.0,\n"," 20: 1.0,\n"," 21: 1.0,\n"," 22: 1.0,\n"," 23: 1.0,\n"," 24: 1.0,\n"," 25: 1.0,\n"," 26: 1.0,\n"," 27: 1.0,\n"," 28: 1.0,\n"," 29: 1.0,\n"," 30: 1.0,\n"," 31: 1.0,\n"," 32: 1.0,\n"," 33: 1.0,\n"," 34: 1.0,\n"," 35: 1.0,\n"," 36: 1.0,\n"," 37: 1.0,\n"," 38: 1.0,\n"," 39: 1.0,\n"," 40: 1.0,\n"," 41: 1.0,\n"," 42: 1.0,\n"," 43: 1.0,\n"," 44: 1.0,\n"," 45: 1.0,\n"," 46: 1.0,\n"," 47: 1.0,\n"," 48: 1.0,\n"," 49: 1.0,\n"," 50: 1.0,\n"," 51: 1.0,\n"," 52: 1.0,\n"," 53: 1.0,\n"," 54: 1.0,\n"," 55: 1.0,\n"," 56: 1.0,\n"," 57: 1.0,\n"," 58: 1.0,\n"," 59: 1.0,\n"," 60: 1.0,\n"," 61: 1.0,\n"," 62: 1.0,\n"," 63: 1.0,\n"," 64: 1.0,\n"," 65: 1.0,\n"," 66: 1.0,\n"," 67: 1.0,\n"," 68: 1.0,\n"," 69: 1.0,\n"," 70: 1.0,\n"," 71: 1.0,\n"," 72: 1.0,\n"," 73: 1.0,\n"," 74: 1.0,\n"," 75: 1.0,\n"," 76: 1.0,\n"," 77: 1.0,\n"," 78: 1.0,\n"," 79: 1.0,\n"," 80: 1.0,\n"," 81: 1.0,\n"," 82: 1.0,\n"," 83: 1.0,\n"," 84: 1.0,\n"," 85: 1.0,\n"," 86: 1.0,\n"," 87: 1.0,\n"," 88: 1.0,\n"," 89: 1.0,\n"," 90: 1.0,\n"," 91: 1.0,\n"," 92: 1.0,\n"," 93: 1.0,\n"," 94: 1.0,\n"," 95: 1.0,\n"," 96: 1.0,\n"," 97: 1.0,\n"," 98: 1.0,\n"," 99: 1.0,\n"," 100: 1.0,\n"," 101: 1.0,\n"," 102: 1.0,\n"," 103: 1.0,\n"," 104: 1.0,\n"," 106: 1.0,\n"," 107: 1.0,\n"," 109: 1.0,\n"," 110: 1.0,\n"," 111: 1.0,\n"," 112: 1.0,\n"," 114: 1.0,\n"," 115: 1.0,\n"," 116: 1.0,\n"," 117: 1.0,\n"," 118: 1.0,\n"," 120: 1.0,\n"," 121: 1.0,\n"," 122: 1.0,\n"," 124: 1.0,\n"," 125: 1.0,\n"," 126: 1.0,\n"," 127: 1.0,\n"," 128: 1.0,\n"," 130: 1.0,\n"," 131: 1.0,\n"," 132: 1.0,\n"," 135: 1.0,\n"," 136: 1.0,\n"," 137: 1.0,\n"," 139: 1.0,\n"," 143: 1.0,\n"," 151: 1.0,\n"," 166: 1.0,\n"," 167: 1.0,\n"," 198: 1.0,\n"," 200: 1.0,\n"," 205: 1.0,\n"," 212: 1.0,\n"," 257: 1.0,\n"," 260: 1.0,\n"," 276: 1.0}"]},"metadata":{},"execution_count":25}],"source":["labels = create_labels_for_scoring(train)\n","res = []\n","for i in labels:\n","    tot = 0\n","    for j in i:\n","        tot += j[1] - j[0]\n","    res.append(tot)\n","train[\"num_of_chars\"] = res\n","train[\"loss_weight\"] = np.log1p(train[\"num_of_chars\"] + 1) / np.log(train[\"num_of_chars\"] + 1).max() * 2\n","weights = train[[\"num_of_chars\", \"loss_weight\"]].drop_duplicates().set_index(\"num_of_chars\").to_dict()[\"loss_weight\"]\n","for key in weights.keys():\n","    weights[key] = 1.0\n","weights[0] = 3.0\n","weights"],"id":"Vu6qpUwmAizr"},{"cell_type":"code","execution_count":null,"metadata":{"id":"eda8175d"},"outputs":[],"source":["def train_fn(\n","    train_dataloader,\n","    model,\n","    criterion,\n","    optimizer,\n","    epoch,\n","    scheduler,\n","    device,\n","):\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(train_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","\n","        pos_nums = (labels == 1).sum(axis=1)\n","        pos_nums = torch.tensor([[pos_num] * CFG.max_len for pos_num in pos_nums]).view(-1, 1).to(device)\n","        pos_nums = torch.masked_select(pos_nums, labels.view(-1, 1) != -1)\n","        weight = []\n","        for pos_num in pos_nums:\n","            weight.append(weights[pos_num.item()])\n","        weight = torch.tensor(weight).to(device)\n","        loss = loss * weight\n","\n","        loss = loss.mean()\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","\n","        if CFG.batch_scheduler:\n","            scheduler.step()\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n","            print(\n","                \"Epoch: [{0}][{1}/{2}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                \"Grad: {grad_norm:.4f}  \"\n","                \"LR: {lr:.6f}  \"\n","                .format(\n","                    epoch+1,\n","                    step,\n","                    len(train_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n","                    loss=losses,\n","                     grad_norm=grad_norm,\n","                     lr=scheduler.get_lr()[0],\n","                )\n","            )\n","    return losses.avg"],"id":"eda8175d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c44b63a7"},"outputs":[],"source":["def valid_fn(\n","    val_dataloader,\n","    model,\n","    criterion,\n","    device,\n","):\n","    model.eval()\n","    preds = []\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(val_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.no_grad():\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","\n","        pos_nums = (labels == 1).sum(axis=1)\n","        pos_nums = torch.tensor([[pos_num] * CFG.max_len for pos_num in pos_nums]).view(-1, 1).to(device)\n","        pos_nums = torch.masked_select(pos_nums, labels.view(-1, 1) != -1)\n","        weight = []\n","        for pos_num in pos_nums:\n","            weight.append(weights[pos_num.item()])\n","        weight = torch.tensor(weight).to(device)\n","        loss = loss * weight\n","\n","        loss = loss.mean()\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(output.sigmoid().squeeze().detach().cpu().numpy())\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n","            print(\n","                \"EVAL: [{0}/{1}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                .format(\n","                    step, len(val_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n","                    loss=losses,\n","                )\n","            )\n","    preds = np.concatenate(preds)\n","    return losses.avg, preds"],"id":"c44b63a7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4219ac38"},"outputs":[],"source":["def inference_fn(test_dataloader, model, device):\n","    model.eval()\n","    model.to(device)\n","    preds = []\n","    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n","    for inputs in tk0:\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        with torch.no_grad():\n","            output = model(inputs)\n","        preds.append(output.sigmoid().squeeze().detach().cpu().numpy())\n","    preds = np.concatenate(preds)\n","    return preds"],"id":"4219ac38"},{"cell_type":"code","execution_count":null,"metadata":{"id":"014a76b7"},"outputs":[],"source":["def train_loop(df, i_fold, device):\n","    print(f\"========== fold: {i_fold} training ==========\")\n","    train_idx = df[df[\"fold\"] != i_fold].index\n","    val_idx = df[df[\"fold\"] == i_fold].index\n","\n","    train_folds = df.loc[train_idx].reset_index(drop=True)\n","    val_folds = df.loc[val_idx].reset_index(drop=True)\n","\n","    train_dataset = TrainingDataset(CFG, train_folds)\n","    val_dataset = TrainingDataset(CFG, val_folds)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=True,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=True,\n","    )\n","    val_dataloader = DataLoader(\n","        val_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=False,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=False,\n","    )\n","\n","    # model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","    model = CustomModel(CFG, model_config_path=None, pretrained=False)   # itptを使うため\n","    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n","    model.to(device)\n","\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\"params\": [p for n, p in param_optimizer if not any(\n","            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n","        {\"params\": [p for n, p in param_optimizer if any(\n","            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n","    ]\n","    optimizer = AdamW(\n","        optimizer_grouped_parameters,\n","        lr=CFG.lr,\n","        betas=CFG.betas,\n","        weight_decay=CFG.weight_decay,\n","    )\n","    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n","    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        num_training_steps=num_train_optimization_steps,\n","    )\n","\n","    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    best_score = -1 * np.inf\n","\n","    for epoch in range(CFG.epochs):\n","        start_time = time.time()\n","        avg_loss = train_fn(\n","            train_dataloader,\n","            model,\n","            criterion,\n","            optimizer,\n","            epoch,\n","            scheduler,\n","            device,\n","        )\n","        avg_val_loss, val_preds = valid_fn(\n","            val_dataloader,\n","            model,\n","            criterion,\n","            device,\n","        )\n","\n","        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n","            scheduler.step()\n","\n","        # scoring\n","        val_folds[[str(i) for i in range(CFG.max_len)]] = val_preds\n","        score = scoring(val_folds, th=0.5)\n","\n","        elapsed = time.time() - start_time\n","\n","        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n","        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n","        if score > best_score:\n","            best_score = score\n","            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n","            torch.save({\n","                \"model\": model.state_dict(),\n","                \"predictions\": val_preds,\n","                },\n","                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","            )\n","\n","    predictions = torch.load(\n","        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","        map_location=torch.device(\"cpu\"),\n","    )[\"predictions\"]\n","    val_folds[[str(i) for i in range(CFG.max_len)]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return val_folds"],"id":"014a76b7"},{"cell_type":"markdown","metadata":{"id":"c38fb834"},"source":["## Main"],"id":"c38fb834"},{"cell_type":"code","source":["CFG.max_len = 466"],"metadata":{"id":"27p0PDxJDEKK"},"id":"27p0PDxJDEKK","execution_count":null,"outputs":[]},{"cell_type":"code","source":["oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n","score = scoring(oof_df, th=0.5)\n","print(f\"Best thres: 0.5, Score: {score:.4f}\")\n","best_thres = get_best_thres(oof_df)\n","score = scoring(oof_df, th=best_thres)\n","print(f\"Best thres: {best_thres}, Score: {score:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lOTGoG9SCiVw","executionInfo":{"status":"ok","timestamp":1646349468829,"user_tz":-540,"elapsed":446243,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}},"outputId":"051ef501-d770-4fec-acce-dc9a9ea2a85a"},"id":"lOTGoG9SCiVw","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Best thres: 0.5, Score: 0.8769\n","Best thres: 0.5373046875, Score: 0.8774\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"62d677cd"},"outputs":[],"source":["def main():\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    if CFG.train:\n","        oof_df = pd.DataFrame()\n","        for i_fold in range(CFG.n_fold):\n","            if i_fold in CFG.train_fold:\n","                _oof_df = train_loop(train, i_fold, device)\n","                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n","        #oof_df.to_csv(CFG.output_dir / \"oof_df.csv\", index=False)\n","        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    if CFG.submission:\n","        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n","    else:\n","        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    score = scoring(oof_df, th=0.5)\n","    print(f\"Best thres: 0.5, Score: {score:.4f}\")\n","    best_thres = get_best_thres(oof_df)\n","    score = scoring(oof_df, th=best_thres)\n","    print(f\"Best thres: {best_thres}, Score: {score:.4f}\")\n","\n","    if CFG.inference:\n","        test_dataset = TestDataset(CFG, test)\n","        test_dataloader = DataLoader(\n","            test_dataset,\n","            batch_size=CFG.batch_size,\n","            shuffle=False,\n","            num_workers=CFG.num_workers,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","        predictions = []\n","        for i_fold in CFG.train_fold:\n","            if CFG.submission:\n","                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n","                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n","            else:\n","                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n","\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            model.load_state_dict(state[\"model\"])\n","            test_token_probs = inference_fn(test_dataloader, model, device)\n","            test[[f\"fold{i_fold}_{i}\" for i in range(CFG.max_len)]] = test_token_probs\n","            test_char_probs = get_char_probs(test[\"pn_history\"].values, test_token_probs, CFG.tokenizer)\n","            predictions.append(test_char_probs)\n","\n","            del state, test_token_probs, model; gc.collect()\n","            torch.cuda.empty_cache()\n","\n","        predictions = np.mean(predictions, axis=0)\n","        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n","        test[CFG.target_col] = predicted_location_str\n","        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n","        test[[CFG.id_col, CFG.target_col]].to_csv(\n","            CFG.output_dir / \"submission.csv\", index=False\n","        )"],"id":"62d677cd"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1d4fcf7c","outputId":"f97740cd-6882-4f38-eeec-6d9cdbc4a914"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["========== fold: 0 training ==========\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp009/checkpoint-129000/pytorch_model.bin\n","Epoch: [1][0/1424] Elapsed 0m 0s (remain 20m 31s) Loss: 1.0710(1.0710) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/1424] Elapsed 0m 30s (remain 6m 34s) Loss: 0.2345(0.8549) Grad: 16059.6514  LR: 0.000003  \n","Epoch: [1][200/1424] Elapsed 1m 0s (remain 6m 7s) Loss: 0.0856(0.4796) Grad: 951.6530  LR: 0.000006  \n","Epoch: [1][300/1424] Elapsed 1m 29s (remain 5m 33s) Loss: 0.1058(0.3460) Grad: 1977.3275  LR: 0.000008  \n","Epoch: [1][400/1424] Elapsed 1m 58s (remain 5m 2s) Loss: 0.0200(0.2742) Grad: 2150.7798  LR: 0.000011  \n","Epoch: [1][500/1424] Elapsed 2m 27s (remain 4m 32s) Loss: 0.0661(0.2265) Grad: 3728.8164  LR: 0.000014  \n","Epoch: [1][600/1424] Elapsed 2m 56s (remain 4m 2s) Loss: 0.0209(0.1936) Grad: 2739.8706  LR: 0.000017  \n","Epoch: [1][700/1424] Elapsed 3m 25s (remain 3m 32s) Loss: 0.0091(0.1694) Grad: 1458.4388  LR: 0.000020  \n","Epoch: [1][800/1424] Elapsed 3m 55s (remain 3m 2s) Loss: 0.0080(0.1512) Grad: 941.2450  LR: 0.000020  \n","Epoch: [1][900/1424] Elapsed 4m 24s (remain 2m 33s) Loss: 0.0187(0.1371) Grad: 3476.1213  LR: 0.000019  \n","Epoch: [1][1000/1424] Elapsed 4m 53s (remain 2m 3s) Loss: 0.0282(0.1254) Grad: 2100.5586  LR: 0.000019  \n","Epoch: [1][1100/1424] Elapsed 5m 22s (remain 1m 34s) Loss: 0.0090(0.1157) Grad: 3978.2659  LR: 0.000019  \n","Epoch: [1][1200/1424] Elapsed 5m 51s (remain 1m 5s) Loss: 0.0056(0.1077) Grad: 1297.0807  LR: 0.000018  \n","Epoch: [1][1300/1424] Elapsed 6m 19s (remain 0m 35s) Loss: 0.0111(0.1006) Grad: 1521.2900  LR: 0.000018  \n","Epoch: [1][1400/1424] Elapsed 6m 48s (remain 0m 6s) Loss: 0.0123(0.0947) Grad: 921.9125  LR: 0.000018  \n","Epoch: [1][1423/1424] Elapsed 6m 55s (remain 0m 0s) Loss: 0.0225(0.0934) Grad: 3896.2673  LR: 0.000018  \n","EVAL: [0/363] Elapsed 0m 0s (remain 3m 29s) Loss: 0.0217(0.0217) \n","EVAL: [100/363] Elapsed 0m 19s (remain 0m 49s) Loss: 0.0124(0.0159) \n","EVAL: [200/363] Elapsed 0m 37s (remain 0m 30s) Loss: 0.0356(0.0180) \n","EVAL: [300/363] Elapsed 0m 55s (remain 0m 11s) Loss: 0.0064(0.0171) \n","EVAL: [362/363] Elapsed 1m 7s (remain 0m 0s) Loss: 0.0054(0.0159) \n","Epoch 1 - avg_train_loss: 0.0934  avg_val_loss: 0.0159  time: 491s\n","Epoch 1 - Score: 0.8448\n","Epoch 1 - Save Best Score: 0.8448 Model\n","Epoch: [2][0/1424] Elapsed 0m 0s (remain 14m 35s) Loss: 0.0054(0.0054) Grad: 8699.9736  LR: 0.000018  \n","Epoch: [2][100/1424] Elapsed 0m 30s (remain 6m 34s) Loss: 0.0108(0.0137) Grad: 31673.7090  LR: 0.000017  \n","Epoch: [2][200/1424] Elapsed 0m 59s (remain 6m 0s) Loss: 0.0143(0.0128) Grad: 16188.7939  LR: 0.000017  \n","Epoch: [2][300/1424] Elapsed 1m 28s (remain 5m 29s) Loss: 0.0049(0.0122) Grad: 25607.1602  LR: 0.000017  \n","Epoch: [2][400/1424] Elapsed 1m 57s (remain 4m 58s) Loss: 0.0097(0.0127) Grad: 20359.2793  LR: 0.000017  \n","Epoch: [2][500/1424] Elapsed 2m 26s (remain 4m 29s) Loss: 0.0039(0.0129) Grad: 5854.1958  LR: 0.000016  \n","Epoch: [2][600/1424] Elapsed 2m 55s (remain 3m 59s) Loss: 0.0018(0.0128) Grad: 6086.7588  LR: 0.000016  \n","Epoch: [2][700/1424] Elapsed 3m 24s (remain 3m 30s) Loss: 0.0056(0.0130) Grad: 15330.5166  LR: 0.000016  \n","Epoch: [2][800/1424] Elapsed 3m 52s (remain 3m 1s) Loss: 0.0372(0.0128) Grad: 108813.9375  LR: 0.000015  \n","Epoch: [2][900/1424] Elapsed 4m 21s (remain 2m 32s) Loss: 0.0103(0.0127) Grad: 21202.9922  LR: 0.000015  \n","Epoch: [2][1000/1424] Elapsed 4m 50s (remain 2m 2s) Loss: 0.0059(0.0128) Grad: 11372.6289  LR: 0.000015  \n","Epoch: [2][1100/1424] Elapsed 5m 19s (remain 1m 33s) Loss: 0.0113(0.0128) Grad: 8920.1211  LR: 0.000014  \n","Epoch: [2][1200/1424] Elapsed 5m 48s (remain 1m 4s) Loss: 0.0382(0.0129) Grad: 30654.7500  LR: 0.000014  \n","Epoch: [2][1300/1424] Elapsed 6m 17s (remain 0m 35s) Loss: 0.0005(0.0130) Grad: 1785.4270  LR: 0.000014  \n","Epoch: [2][1400/1424] Elapsed 6m 46s (remain 0m 6s) Loss: 0.0068(0.0131) Grad: 40832.4141  LR: 0.000013  \n","Epoch: [2][1423/1424] Elapsed 6m 53s (remain 0m 0s) Loss: 0.0129(0.0131) Grad: 63955.7305  LR: 0.000013  \n","EVAL: [0/363] Elapsed 0m 0s (remain 2m 50s) Loss: 0.0113(0.0113) \n","EVAL: [100/363] Elapsed 0m 18s (remain 0m 48s) Loss: 0.0092(0.0150) \n","EVAL: [200/363] Elapsed 0m 37s (remain 0m 30s) Loss: 0.0384(0.0186) \n","EVAL: [300/363] Elapsed 0m 55s (remain 0m 11s) Loss: 0.0070(0.0171) \n","EVAL: [362/363] Elapsed 1m 6s (remain 0m 0s) Loss: 0.0052(0.0161) \n","Epoch 2 - avg_train_loss: 0.0131  avg_val_loss: 0.0161  time: 484s\n","Epoch 2 - Score: 0.8634\n","Epoch 2 - Save Best Score: 0.8634 Model\n","Epoch: [3][0/1424] Elapsed 0m 0s (remain 15m 36s) Loss: 0.0084(0.0084) Grad: 10786.5273  LR: 0.000013  \n","Epoch: [3][100/1424] Elapsed 0m 30s (remain 6m 36s) Loss: 0.0164(0.0093) Grad: 88863.8125  LR: 0.000013  \n","Epoch: [3][200/1424] Elapsed 0m 59s (remain 6m 0s) Loss: 0.0082(0.0102) Grad: 28552.8945  LR: 0.000013  \n","Epoch: [3][300/1424] Elapsed 1m 28s (remain 5m 30s) Loss: 0.0160(0.0106) Grad: 26292.3047  LR: 0.000012  \n","Epoch: [3][400/1424] Elapsed 1m 57s (remain 4m 59s) Loss: 0.0019(0.0108) Grad: 4916.7671  LR: 0.000012  \n","Epoch: [3][500/1424] Elapsed 2m 26s (remain 4m 29s) Loss: 0.0036(0.0109) Grad: 7091.7949  LR: 0.000012  \n","Epoch: [3][600/1424] Elapsed 2m 55s (remain 4m 0s) Loss: 0.0291(0.0107) Grad: 23808.3398  LR: 0.000011  \n","Epoch: [3][700/1424] Elapsed 3m 24s (remain 3m 30s) Loss: 0.0038(0.0108) Grad: 27794.6133  LR: 0.000011  \n","Epoch: [3][800/1424] Elapsed 3m 53s (remain 3m 1s) Loss: 0.0124(0.0107) Grad: 106371.5625  LR: 0.000011  \n","Epoch: [3][900/1424] Elapsed 4m 22s (remain 2m 32s) Loss: 0.0067(0.0105) Grad: 17113.6641  LR: 0.000011  \n","Epoch: [3][1000/1424] Elapsed 4m 51s (remain 2m 3s) Loss: 0.0019(0.0105) Grad: 5170.4170  LR: 0.000010  \n","Epoch: [3][1100/1424] Elapsed 5m 20s (remain 1m 33s) Loss: 0.0572(0.0104) Grad: 55607.8008  LR: 0.000010  \n","Epoch: [3][1200/1424] Elapsed 5m 49s (remain 1m 4s) Loss: 0.0014(0.0104) Grad: 4530.6978  LR: 0.000010  \n","Epoch: [3][1300/1424] Elapsed 6m 18s (remain 0m 35s) Loss: 0.0336(0.0103) Grad: 47207.3633  LR: 0.000009  \n","Epoch: [3][1400/1424] Elapsed 6m 47s (remain 0m 6s) Loss: 0.0034(0.0103) Grad: 7126.2607  LR: 0.000009  \n","Epoch: [3][1423/1424] Elapsed 6m 53s (remain 0m 0s) Loss: 0.0045(0.0103) Grad: 19294.6348  LR: 0.000009  \n","EVAL: [0/363] Elapsed 0m 0s (remain 2m 50s) Loss: 0.0095(0.0095) \n","EVAL: [100/363] Elapsed 0m 18s (remain 0m 48s) Loss: 0.0116(0.0143) \n","EVAL: [200/363] Elapsed 0m 37s (remain 0m 29s) Loss: 0.0151(0.0166) \n","EVAL: [300/363] Elapsed 0m 55s (remain 0m 11s) Loss: 0.0028(0.0152) \n","EVAL: [362/363] Elapsed 1m 6s (remain 0m 0s) Loss: 0.0040(0.0145) \n","Epoch 3 - avg_train_loss: 0.0103  avg_val_loss: 0.0145  time: 488s\n","Epoch 3 - Score: 0.8743\n","Epoch 3 - Save Best Score: 0.8743 Model\n","Epoch: [4][0/1424] Elapsed 0m 0s (remain 15m 12s) Loss: 0.0021(0.0021) Grad: 4921.3452  LR: 0.000009  \n","Epoch: [4][100/1424] Elapsed 0m 30s (remain 6m 36s) Loss: 0.0079(0.0097) Grad: 14977.4434  LR: 0.000009  \n","Epoch: [4][200/1424] Elapsed 0m 59s (remain 6m 0s) Loss: 0.0001(0.0087) Grad: 443.8838  LR: 0.000008  \n","Epoch: [4][300/1424] Elapsed 1m 28s (remain 5m 28s) Loss: 0.0065(0.0087) Grad: 18801.7266  LR: 0.000008  \n","Epoch: [4][400/1424] Elapsed 1m 56s (remain 4m 58s) Loss: 0.0029(0.0092) Grad: 4428.6401  LR: 0.000008  \n","Epoch: [4][500/1424] Elapsed 2m 25s (remain 4m 28s) Loss: 0.0072(0.0091) Grad: 32313.9668  LR: 0.000007  \n","Epoch: [4][600/1424] Elapsed 2m 54s (remain 3m 59s) Loss: 0.0004(0.0091) Grad: 917.3077  LR: 0.000007  \n","Epoch: [4][700/1424] Elapsed 3m 23s (remain 3m 30s) Loss: 0.0287(0.0091) Grad: 56027.0000  LR: 0.000007  \n","Epoch: [4][800/1424] Elapsed 3m 52s (remain 3m 0s) Loss: 0.0105(0.0090) Grad: 16693.4277  LR: 0.000006  \n","Epoch: [4][900/1424] Elapsed 4m 21s (remain 2m 31s) Loss: 0.1266(0.0090) Grad: 198480.4219  LR: 0.000006  \n","Epoch: [4][1000/1424] Elapsed 4m 50s (remain 2m 2s) Loss: 0.0002(0.0089) Grad: 1490.2510  LR: 0.000006  \n","Epoch: [4][1100/1424] Elapsed 5m 19s (remain 1m 33s) Loss: 0.0022(0.0088) Grad: 8338.6309  LR: 0.000005  \n","Epoch: [4][1200/1424] Elapsed 5m 48s (remain 1m 4s) Loss: 0.0054(0.0086) Grad: 17687.8906  LR: 0.000005  \n","Epoch: [4][1300/1424] Elapsed 6m 17s (remain 0m 35s) Loss: 0.0028(0.0086) Grad: 7049.5542  LR: 0.000005  \n","Epoch: [4][1400/1424] Elapsed 6m 46s (remain 0m 6s) Loss: 0.0001(0.0085) Grad: 292.6650  LR: 0.000005  \n","Epoch: [4][1423/1424] Elapsed 6m 52s (remain 0m 0s) Loss: 0.0018(0.0085) Grad: 16076.9844  LR: 0.000004  \n","EVAL: [0/363] Elapsed 0m 0s (remain 2m 49s) Loss: 0.0075(0.0075) \n","EVAL: [100/363] Elapsed 0m 18s (remain 0m 48s) Loss: 0.0132(0.0175) \n","EVAL: [200/363] Elapsed 0m 37s (remain 0m 29s) Loss: 0.0424(0.0194) \n","EVAL: [300/363] Elapsed 0m 55s (remain 0m 11s) Loss: 0.0015(0.0175) \n","EVAL: [362/363] Elapsed 1m 6s (remain 0m 0s) Loss: 0.0072(0.0166) \n","Epoch 4 - avg_train_loss: 0.0085  avg_val_loss: 0.0166  time: 487s\n","Epoch 4 - Score: 0.8749\n","Epoch 4 - Save Best Score: 0.8749 Model\n","Epoch: [5][0/1424] Elapsed 0m 0s (remain 15m 16s) Loss: 0.0085(0.0085) Grad: 18958.0000  LR: 0.000004  \n","Epoch: [5][100/1424] Elapsed 0m 30s (remain 6m 35s) Loss: 0.0016(0.0064) Grad: 6282.7070  LR: 0.000004  \n","Epoch: [5][200/1424] Elapsed 0m 59s (remain 6m 1s) Loss: 0.0007(0.0064) Grad: 5268.4004  LR: 0.000004  \n","Epoch: [5][300/1424] Elapsed 1m 28s (remain 5m 29s) Loss: 0.0029(0.0067) Grad: 9991.0127  LR: 0.000004  \n","Epoch: [5][400/1424] Elapsed 1m 57s (remain 4m 59s) Loss: 0.0122(0.0074) Grad: 28457.8496  LR: 0.000003  \n","Epoch: [5][500/1424] Elapsed 2m 26s (remain 4m 29s) Loss: 0.0002(0.0071) Grad: 1071.6887  LR: 0.000003  \n","Epoch: [5][600/1424] Elapsed 2m 55s (remain 4m 0s) Loss: 0.0031(0.0072) Grad: 5031.4155  LR: 0.000003  \n","Epoch: [5][700/1424] Elapsed 3m 24s (remain 3m 31s) Loss: 0.0007(0.0074) Grad: 9437.3301  LR: 0.000002  \n","Epoch: [5][800/1424] Elapsed 3m 53s (remain 3m 1s) Loss: 0.0001(0.0072) Grad: 462.6460  LR: 0.000002  \n","Epoch: [5][900/1424] Elapsed 4m 22s (remain 2m 32s) Loss: 0.0221(0.0070) Grad: 61973.2031  LR: 0.000002  \n","Epoch: [5][1000/1424] Elapsed 4m 51s (remain 2m 3s) Loss: 0.0221(0.0073) Grad: 38155.7500  LR: 0.000001  \n","Epoch: [5][1100/1424] Elapsed 5m 20s (remain 1m 34s) Loss: 0.0008(0.0073) Grad: 3335.6208  LR: 0.000001  \n","Epoch: [5][1200/1424] Elapsed 5m 50s (remain 1m 4s) Loss: 0.0163(0.0074) Grad: 88145.5625  LR: 0.000001  \n","Epoch: [5][1300/1424] Elapsed 6m 19s (remain 0m 35s) Loss: 0.0001(0.0073) Grad: 189.6748  LR: 0.000000  \n","Epoch: [5][1400/1424] Elapsed 6m 48s (remain 0m 6s) Loss: 0.0021(0.0073) Grad: 10594.4062  LR: 0.000000  \n","Epoch: [5][1423/1424] Elapsed 6m 54s (remain 0m 0s) Loss: 0.0006(0.0073) Grad: 5135.3296  LR: 0.000000  \n","EVAL: [0/363] Elapsed 0m 0s (remain 2m 57s) Loss: 0.0056(0.0056) \n","EVAL: [100/363] Elapsed 0m 18s (remain 0m 49s) Loss: 0.0165(0.0180) \n","EVAL: [200/363] Elapsed 0m 37s (remain 0m 30s) Loss: 0.0519(0.0201) \n","EVAL: [300/363] Elapsed 0m 55s (remain 0m 11s) Loss: 0.0039(0.0182) \n","EVAL: [362/363] Elapsed 1m 7s (remain 0m 0s) Loss: 0.0074(0.0174) \n","Epoch 5 - avg_train_loss: 0.0073  avg_val_loss: 0.0174  time: 490s\n","Epoch 5 - Score: 0.8772\n","Epoch 5 - Save Best Score: 0.8772 Model\n","========== fold: 1 training ==========\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp009/checkpoint-129000/pytorch_model.bin\n","Epoch: [1][0/1425] Elapsed 0m 0s (remain 14m 24s) Loss: 1.2250(1.2250) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/1425] Elapsed 0m 30s (remain 6m 36s) Loss: 0.2292(0.7223) Grad: 14736.0947  LR: 0.000003  \n","Epoch: [1][200/1425] Elapsed 0m 59s (remain 6m 0s) Loss: 0.0612(0.4148) Grad: 686.6940  LR: 0.000006  \n","Epoch: [1][300/1425] Elapsed 1m 28s (remain 5m 29s) Loss: 0.0661(0.3023) Grad: 731.0942  LR: 0.000008  \n","Epoch: [1][400/1425] Elapsed 1m 57s (remain 4m 59s) Loss: 0.0528(0.2424) Grad: 5669.6245  LR: 0.000011  \n","Epoch: [1][500/1425] Elapsed 2m 26s (remain 4m 30s) Loss: 0.0256(0.2021) Grad: 2806.9016  LR: 0.000014  \n","Epoch: [1][600/1425] Elapsed 2m 55s (remain 4m 0s) Loss: 0.0137(0.1733) Grad: 1022.4539  LR: 0.000017  \n","Epoch: [1][700/1425] Elapsed 3m 24s (remain 3m 31s) Loss: 0.0159(0.1518) Grad: 2417.7646  LR: 0.000020  \n","Epoch: [1][800/1425] Elapsed 3m 53s (remain 3m 2s) Loss: 0.0330(0.1362) Grad: 4871.6318  LR: 0.000020  \n","Epoch: [1][900/1425] Elapsed 4m 22s (remain 2m 32s) Loss: 0.0369(0.1234) Grad: 5684.6313  LR: 0.000019  \n","Epoch: [1][1000/1425] Elapsed 4m 52s (remain 2m 3s) Loss: 0.0217(0.1132) Grad: 1923.3710  LR: 0.000019  \n","Epoch: [1][1100/1425] Elapsed 5m 21s (remain 1m 34s) Loss: 0.0287(0.1047) Grad: 5107.0625  LR: 0.000019  \n","Epoch: [1][1200/1425] Elapsed 5m 50s (remain 1m 5s) Loss: 0.0068(0.0976) Grad: 770.9113  LR: 0.000018  \n","Epoch: [1][1300/1425] Elapsed 6m 19s (remain 0m 36s) Loss: 0.0057(0.0914) Grad: 578.3397  LR: 0.000018  \n","Epoch: [1][1400/1425] Elapsed 6m 48s (remain 0m 6s) Loss: 0.0021(0.0860) Grad: 440.1141  LR: 0.000018  \n","Epoch: [1][1424/1425] Elapsed 6m 55s (remain 0m 0s) Loss: 0.0119(0.0849) Grad: 2270.8428  LR: 0.000018  \n","EVAL: [0/362] Elapsed 0m 0s (remain 2m 43s) Loss: 0.0336(0.0336) \n","EVAL: [100/362] Elapsed 0m 19s (remain 0m 49s) Loss: 0.0046(0.0196) \n","EVAL: [200/362] Elapsed 0m 37s (remain 0m 30s) Loss: 0.0079(0.0183) \n","EVAL: [300/362] Elapsed 0m 56s (remain 0m 11s) Loss: 0.0099(0.0203) \n","EVAL: [361/362] Elapsed 1m 7s (remain 0m 0s) Loss: 0.0049(0.0185) \n","Epoch 1 - avg_train_loss: 0.0849  avg_val_loss: 0.0185  time: 487s\n","Epoch 1 - Score: 0.8336\n","Epoch 1 - Save Best Score: 0.8336 Model\n","Epoch: [2][0/1425] Elapsed 0m 0s (remain 15m 35s) Loss: 0.0040(0.0040) Grad: 11585.1367  LR: 0.000018  \n","Epoch: [2][100/1425] Elapsed 0m 30s (remain 6m 38s) Loss: 0.0030(0.0141) Grad: 5792.8989  LR: 0.000017  \n","Epoch: [2][200/1425] Elapsed 0m 59s (remain 6m 3s) Loss: 0.0157(0.0131) Grad: 44079.7344  LR: 0.000017  \n","Epoch: [2][300/1425] Elapsed 1m 28s (remain 5m 30s) Loss: 0.0091(0.0124) Grad: 26297.5469  LR: 0.000017  \n","Epoch: [2][400/1425] Elapsed 1m 57s (remain 5m 0s) Loss: 0.0131(0.0124) Grad: 33857.2891  LR: 0.000017  \n","Epoch: [2][500/1425] Elapsed 2m 26s (remain 4m 30s) Loss: 0.0179(0.0129) Grad: 33163.7227  LR: 0.000016  \n","Epoch: [2][600/1425] Elapsed 2m 55s (remain 4m 0s) Loss: 0.0892(0.0130) Grad: 98996.1172  LR: 0.000016  \n","Epoch: [2][700/1425] Elapsed 3m 24s (remain 3m 31s) Loss: 0.0089(0.0129) Grad: 14698.9492  LR: 0.000016  \n","Epoch: [2][800/1425] Elapsed 3m 53s (remain 3m 2s) Loss: 0.0237(0.0129) Grad: 24999.0488  LR: 0.000015  \n","Epoch: [2][900/1425] Elapsed 4m 22s (remain 2m 32s) Loss: 0.0102(0.0130) Grad: 19995.9980  LR: 0.000015  \n","Epoch: [2][1000/1425] Elapsed 4m 51s (remain 2m 3s) Loss: 0.0004(0.0130) Grad: 2228.7727  LR: 0.000015  \n","Epoch: [2][1100/1425] Elapsed 5m 20s (remain 1m 34s) Loss: 0.0670(0.0130) Grad: 66517.1797  LR: 0.000014  \n","Epoch: [2][1200/1425] Elapsed 5m 50s (remain 1m 5s) Loss: 0.0021(0.0131) Grad: 3486.0391  LR: 0.000014  \n","Epoch: [2][1300/1425] Elapsed 6m 18s (remain 0m 36s) Loss: 0.0298(0.0131) Grad: 46389.1133  LR: 0.000014  \n","Epoch: [2][1400/1425] Elapsed 6m 48s (remain 0m 6s) Loss: 0.0059(0.0132) Grad: 11461.2920  LR: 0.000013  \n","Epoch: [2][1424/1425] Elapsed 6m 55s (remain 0m 0s) Loss: 0.0074(0.0132) Grad: 24371.8105  LR: 0.000013  \n","EVAL: [0/362] Elapsed 0m 0s (remain 2m 52s) Loss: 0.0018(0.0018) \n","EVAL: [100/362] Elapsed 0m 19s (remain 0m 49s) Loss: 0.0019(0.0149) \n","EVAL: [200/362] Elapsed 0m 37s (remain 0m 30s) Loss: 0.0099(0.0142) \n","EVAL: [300/362] Elapsed 0m 56s (remain 0m 11s) Loss: 0.0060(0.0155) \n","EVAL: [361/362] Elapsed 1m 7s (remain 0m 0s) Loss: 0.0037(0.0139) \n","Epoch 2 - avg_train_loss: 0.0132  avg_val_loss: 0.0139  time: 489s\n","Epoch 2 - Score: 0.8697\n","Epoch 2 - Save Best Score: 0.8697 Model\n","Epoch: [3][0/1425] Elapsed 0m 0s (remain 15m 14s) Loss: 0.0009(0.0009) Grad: 4007.2776  LR: 0.000013  \n","Epoch: [3][100/1425] Elapsed 0m 30s (remain 6m 36s) Loss: 0.0048(0.0107) Grad: 5764.9932  LR: 0.000013  \n","Epoch: [3][200/1425] Elapsed 0m 59s (remain 6m 1s) Loss: 0.0016(0.0096) Grad: 6345.9888  LR: 0.000013  \n","Epoch: [3][300/1425] Elapsed 1m 28s (remain 5m 29s) Loss: 0.0006(0.0099) Grad: 2849.9268  LR: 0.000012  \n","Epoch: [3][400/1425] Elapsed 1m 57s (remain 5m 0s) Loss: 0.0215(0.0102) Grad: 36020.2930  LR: 0.000012  \n","Epoch: [3][500/1425] Elapsed 2m 26s (remain 4m 30s) Loss: 0.0027(0.0110) Grad: 12913.4424  LR: 0.000012  \n","Epoch: [3][600/1425] Elapsed 2m 55s (remain 4m 0s) Loss: 0.0050(0.0108) Grad: 8897.8691  LR: 0.000011  \n","Epoch: [3][700/1425] Elapsed 3m 24s (remain 3m 31s) Loss: 0.0025(0.0108) Grad: 14729.6494  LR: 0.000011  \n","Epoch: [3][800/1425] Elapsed 3m 53s (remain 3m 1s) Loss: 0.0194(0.0108) Grad: 44434.9336  LR: 0.000011  \n","Epoch: [3][900/1425] Elapsed 4m 22s (remain 2m 32s) Loss: 0.0079(0.0106) Grad: 11079.4121  LR: 0.000011  \n","Epoch: [3][1000/1425] Elapsed 4m 51s (remain 2m 3s) Loss: 0.0014(0.0106) Grad: 3890.9570  LR: 0.000010  \n","Epoch: [3][1100/1425] Elapsed 5m 20s (remain 1m 34s) Loss: 0.0065(0.0106) Grad: 9220.9766  LR: 0.000010  \n","Epoch: [3][1200/1425] Elapsed 5m 49s (remain 1m 5s) Loss: 0.0032(0.0106) Grad: 8939.9902  LR: 0.000010  \n","Epoch: [3][1300/1425] Elapsed 6m 18s (remain 0m 36s) Loss: 0.0125(0.0105) Grad: 10177.9766  LR: 0.000009  \n","Epoch: [3][1400/1425] Elapsed 6m 47s (remain 0m 6s) Loss: 0.0068(0.0104) Grad: 16770.4941  LR: 0.000009  \n","Epoch: [3][1424/1425] Elapsed 6m 54s (remain 0m 0s) Loss: 0.0134(0.0104) Grad: 17168.4902  LR: 0.000009  \n","EVAL: [0/362] Elapsed 0m 0s (remain 2m 40s) Loss: 0.0010(0.0010) \n","EVAL: [100/362] Elapsed 0m 19s (remain 0m 49s) Loss: 0.0015(0.0155) \n","EVAL: [200/362] Elapsed 0m 37s (remain 0m 29s) Loss: 0.0025(0.0151) \n","EVAL: [300/362] Elapsed 0m 55s (remain 0m 11s) Loss: 0.0029(0.0171) \n","EVAL: [361/362] Elapsed 1m 6s (remain 0m 0s) Loss: 0.0036(0.0153) \n","Epoch 3 - avg_train_loss: 0.0104  avg_val_loss: 0.0153  time: 488s\n","Epoch 3 - Score: 0.8793\n","Epoch 3 - Save Best Score: 0.8793 Model\n","Epoch: [4][0/1425] Elapsed 0m 0s (remain 15m 51s) Loss: 0.0070(0.0070) Grad: 22568.0137  LR: 0.000009  \n","Epoch: [4][100/1425] Elapsed 0m 30s (remain 6m 38s) Loss: 0.0036(0.0068) Grad: 9362.9551  LR: 0.000009  \n","Epoch: [4][200/1425] Elapsed 0m 59s (remain 6m 0s) Loss: 0.0148(0.0079) Grad: 46570.2109  LR: 0.000008  \n","Epoch: [4][300/1425] Elapsed 1m 28s (remain 5m 29s) Loss: 0.0767(0.0078) Grad: 117937.6172  LR: 0.000008  \n","Epoch: [4][400/1425] Elapsed 1m 56s (remain 4m 58s) Loss: 0.0174(0.0084) Grad: 22359.9102  LR: 0.000008  \n","Epoch: [4][500/1425] Elapsed 2m 25s (remain 4m 29s) Loss: 0.0091(0.0084) Grad: 13308.9355  LR: 0.000007  \n","Epoch: [4][600/1425] Elapsed 2m 54s (remain 3m 59s) Loss: 0.0004(0.0083) Grad: 1390.8446  LR: 0.000007  \n","Epoch: [4][700/1425] Elapsed 3m 23s (remain 3m 30s) Loss: 0.0120(0.0084) Grad: 14893.8965  LR: 0.000007  \n","Epoch: [4][800/1425] Elapsed 3m 52s (remain 3m 1s) Loss: 0.0005(0.0086) Grad: 835.8237  LR: 0.000006  \n","Epoch: [4][900/1425] Elapsed 4m 21s (remain 2m 32s) Loss: 0.0036(0.0084) Grad: 5674.2793  LR: 0.000006  \n","Epoch: [4][1000/1425] Elapsed 4m 50s (remain 2m 2s) Loss: 0.0003(0.0085) Grad: 877.6419  LR: 0.000006  \n","Epoch: [4][1100/1425] Elapsed 5m 19s (remain 1m 33s) Loss: 0.0031(0.0085) Grad: 2310.2185  LR: 0.000005  \n","Epoch: [4][1200/1425] Elapsed 5m 48s (remain 1m 4s) Loss: 0.0017(0.0084) Grad: 2335.3772  LR: 0.000005  \n","Epoch: [4][1300/1425] Elapsed 6m 17s (remain 0m 35s) Loss: 0.0032(0.0085) Grad: 4671.2314  LR: 0.000005  \n","Epoch: [4][1400/1425] Elapsed 6m 45s (remain 0m 6s) Loss: 0.0026(0.0084) Grad: 2938.9785  LR: 0.000005  \n","Epoch: [4][1424/1425] Elapsed 6m 52s (remain 0m 0s) Loss: 0.0042(0.0084) Grad: 10348.6338  LR: 0.000004  \n","EVAL: [0/362] Elapsed 0m 0s (remain 2m 36s) Loss: 0.0014(0.0014) \n","EVAL: [100/362] Elapsed 0m 18s (remain 0m 48s) Loss: 0.0013(0.0162) \n","EVAL: [200/362] Elapsed 0m 37s (remain 0m 29s) Loss: 0.0038(0.0157) \n","EVAL: [300/362] Elapsed 0m 55s (remain 0m 11s) Loss: 0.0014(0.0177) \n","EVAL: [361/362] Elapsed 1m 6s (remain 0m 0s) Loss: 0.0019(0.0159) \n","Epoch 4 - avg_train_loss: 0.0084  avg_val_loss: 0.0159  time: 486s\n","Epoch 4 - Score: 0.8801\n","Epoch 4 - Save Best Score: 0.8801 Model\n","Epoch: [5][0/1425] Elapsed 0m 0s (remain 15m 11s) Loss: 0.0336(0.0336) Grad: 131939.7812  LR: 0.000004  \n","Epoch: [5][100/1425] Elapsed 0m 30s (remain 6m 36s) Loss: 0.0028(0.0085) Grad: 7509.8848  LR: 0.000004  \n","Epoch: [5][200/1425] Elapsed 0m 59s (remain 6m 0s) Loss: 0.0071(0.0078) Grad: 27125.4551  LR: 0.000004  \n","Epoch: [5][300/1425] Elapsed 1m 28s (remain 5m 30s) Loss: 0.0030(0.0075) Grad: 19119.8027  LR: 0.000004  \n","Epoch: [5][400/1425] Elapsed 1m 57s (remain 4m 59s) Loss: 0.0246(0.0076) Grad: 85186.1641  LR: 0.000003  \n","Epoch: [5][500/1425] Elapsed 2m 26s (remain 4m 29s) Loss: 0.0247(0.0076) Grad: 81572.3672  LR: 0.000003  \n","Epoch: [5][600/1425] Elapsed 2m 55s (remain 4m 0s) Loss: 0.0040(0.0078) Grad: 15888.4307  LR: 0.000003  \n","Epoch: [5][700/1425] Elapsed 3m 24s (remain 3m 30s) Loss: 0.0021(0.0077) Grad: 6057.4526  LR: 0.000002  \n","Epoch: [5][800/1425] Elapsed 3m 53s (remain 3m 1s) Loss: 0.0025(0.0078) Grad: 7656.2100  LR: 0.000002  \n","Epoch: [5][900/1425] Elapsed 4m 22s (remain 2m 32s) Loss: 0.0012(0.0075) Grad: 2900.3970  LR: 0.000002  \n","Epoch: [5][1000/1425] Elapsed 4m 50s (remain 2m 3s) Loss: 0.0016(0.0075) Grad: 9775.7881  LR: 0.000001  \n","Epoch: [5][1100/1425] Elapsed 5m 19s (remain 1m 34s) Loss: 0.0068(0.0074) Grad: 15340.1719  LR: 0.000001  \n","Epoch: [5][1200/1425] Elapsed 5m 48s (remain 1m 5s) Loss: 0.0078(0.0074) Grad: 43781.2227  LR: 0.000001  \n","Epoch: [5][1300/1425] Elapsed 6m 17s (remain 0m 36s) Loss: 0.0345(0.0073) Grad: 45472.7773  LR: 0.000000  \n","Epoch: [5][1400/1425] Elapsed 6m 46s (remain 0m 6s) Loss: 0.0024(0.0072) Grad: 9992.4824  LR: 0.000000  \n","Epoch: [5][1424/1425] Elapsed 6m 53s (remain 0m 0s) Loss: 0.0044(0.0072) Grad: 12644.3193  LR: 0.000000  \n","EVAL: [0/362] Elapsed 0m 0s (remain 2m 40s) Loss: 0.0014(0.0014) \n","EVAL: [100/362] Elapsed 0m 18s (remain 0m 48s) Loss: 0.0009(0.0170) \n","EVAL: [200/362] Elapsed 0m 37s (remain 0m 29s) Loss: 0.0025(0.0166) \n","EVAL: [300/362] Elapsed 0m 55s (remain 0m 11s) Loss: 0.0019(0.0188) \n","EVAL: [361/362] Elapsed 1m 6s (remain 0m 0s) Loss: 0.0024(0.0168) \n","Epoch 5 - avg_train_loss: 0.0072  avg_val_loss: 0.0168  time: 487s\n","Epoch 5 - Score: 0.8821\n","Epoch 5 - Save Best Score: 0.8821 Model\n","========== fold: 2 training ==========\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp009/checkpoint-129000/pytorch_model.bin\n","Epoch: [1][0/1435] Elapsed 0m 0s (remain 15m 11s) Loss: 1.7494(1.7494) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/1435] Elapsed 0m 30s (remain 6m 39s) Loss: 0.3317(0.9520) Grad: 13221.7080  LR: 0.000003  \n","Epoch: [1][200/1435] Elapsed 0m 59s (remain 6m 3s) Loss: 0.1197(0.5263) Grad: 1107.0586  LR: 0.000006  \n","Epoch: [1][300/1435] Elapsed 1m 28s (remain 5m 31s) Loss: 0.0565(0.3779) Grad: 388.3168  LR: 0.000008  \n","Epoch: [1][400/1435] Elapsed 1m 56s (remain 5m 1s) Loss: 0.0461(0.2989) Grad: 1463.8914  LR: 0.000011  \n","Epoch: [1][500/1435] Elapsed 2m 26s (remain 4m 32s) Loss: 0.0460(0.2474) Grad: 2525.3435  LR: 0.000014  \n","Epoch: [1][600/1435] Elapsed 2m 55s (remain 4m 2s) Loss: 0.0376(0.2113) Grad: 1225.9791  LR: 0.000017  \n","Epoch: [1][700/1435] Elapsed 3m 23s (remain 3m 33s) Loss: 0.0235(0.1849) Grad: 837.9126  LR: 0.000020  \n","Epoch: [1][800/1435] Elapsed 3m 52s (remain 3m 4s) Loss: 0.0245(0.1643) Grad: 1027.3743  LR: 0.000020  \n","Epoch: [1][900/1435] Elapsed 4m 21s (remain 2m 35s) Loss: 0.0084(0.1482) Grad: 710.6930  LR: 0.000019  \n","Epoch: [1][1000/1435] Elapsed 4m 50s (remain 2m 6s) Loss: 0.0072(0.1352) Grad: 333.3278  LR: 0.000019  \n","Epoch: [1][1100/1435] Elapsed 5m 19s (remain 1m 36s) Loss: 0.0162(0.1246) Grad: 1405.2865  LR: 0.000019  \n","Epoch: [1][1200/1435] Elapsed 5m 48s (remain 1m 7s) Loss: 0.0192(0.1156) Grad: 995.1873  LR: 0.000019  \n","Epoch: [1][1300/1435] Elapsed 6m 17s (remain 0m 38s) Loss: 0.0236(0.1080) Grad: 1271.5005  LR: 0.000018  \n","Epoch: [1][1400/1435] Elapsed 6m 46s (remain 0m 9s) Loss: 0.0061(0.1016) Grad: 413.8855  LR: 0.000018  \n","Epoch: [1][1434/1435] Elapsed 6m 56s (remain 0m 0s) Loss: 0.0255(0.0996) Grad: 1226.3145  LR: 0.000018  \n","EVAL: [0/352] Elapsed 0m 0s (remain 2m 49s) Loss: 0.0146(0.0146) \n","EVAL: [100/352] Elapsed 0m 18s (remain 0m 46s) Loss: 0.0141(0.0143) \n","EVAL: [200/352] Elapsed 0m 37s (remain 0m 27s) Loss: 0.0264(0.0150) \n","EVAL: [300/352] Elapsed 0m 55s (remain 0m 9s) Loss: 0.0080(0.0165) \n","EVAL: [351/352] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0023(0.0159) \n","Epoch 1 - avg_train_loss: 0.0996  avg_val_loss: 0.0159  time: 486s\n","Epoch 1 - Score: 0.8269\n","Epoch 1 - Save Best Score: 0.8269 Model\n","Epoch: [2][0/1435] Elapsed 0m 0s (remain 16m 16s) Loss: 0.0155(0.0155) Grad: 20979.8945  LR: 0.000018  \n","Epoch: [2][100/1435] Elapsed 0m 30s (remain 6m 38s) Loss: 0.0646(0.0152) Grad: 122479.1797  LR: 0.000017  \n"]}],"source":["if __name__ == \"__main__\":\n","    main()"],"id":"1d4fcf7c"}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"name":"nbme-exp018.ipynb","provenance":[{"file_id":"1Yxwq9ayBJWEWbnW1MKCSK3wtmEBBqKA_","timestamp":1646313696795},{"file_id":"1rAF71UVXBqYZ5eQZD27ltsR4TaVfG6nX","timestamp":1646294862307}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":641.572862,"end_time":"2022-02-27T11:39:50.972497","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-02-27T11:29:09.399635","version":"2.3.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2603838b826d4a179cec09dda44ccc82":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_68fb52b1bdfa4885b624d9e1a599d321","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7ee04e5716ae4c7080bf4471416be12b","IPY_MODEL_452e67001b1344efb1d19b3d8c043861","IPY_MODEL_8e73e68e7d0c4c10a91790b05191b9b2"]}},"68fb52b1bdfa4885b624d9e1a599d321":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7ee04e5716ae4c7080bf4471416be12b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_705d0aed36f144c98c0e395d873d530f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0837d1ae78d14fcb93ef8040ec660720"}},"452e67001b1344efb1d19b3d8c043861":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_296cff3e7b0d494b878e0c7c3e39d517","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":42146,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":42146,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_812e9ec39fb04642aed692895981f8db"}},"8e73e68e7d0c4c10a91790b05191b9b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2969834c57694d0eb6d85f9c4f795b53","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 42146/42146 [00:31&lt;00:00, 1963.44it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dd743a29be3c4dbf981c3425ce4acd2c"}},"705d0aed36f144c98c0e395d873d530f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0837d1ae78d14fcb93ef8040ec660720":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"296cff3e7b0d494b878e0c7c3e39d517":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"812e9ec39fb04642aed692895981f8db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2969834c57694d0eb6d85f9c4f795b53":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"dd743a29be3c4dbf981c3425ce4acd2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e969a23ed18f45239447045ba1375af7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d02045aaf05c4ade8be8e4d262daf052","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e09471f7a148419597d69e9af4ccb481","IPY_MODEL_3eefbd0b19d54dfdbd2f8baf37ac48b1","IPY_MODEL_03c769b988784c7ba259be9730a62e09"]}},"d02045aaf05c4ade8be8e4d262daf052":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e09471f7a148419597d69e9af4ccb481":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d14c11443d3a47789fc37e4591f9162b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_56a140221c2a45339fc63e10305a9904"}},"3eefbd0b19d54dfdbd2f8baf37ac48b1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ccfbf231a1ca4c50bf76498325e07b79","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":143,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":143,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d90a5ddc207849698b7ded16468420e8"}},"03c769b988784c7ba259be9730a62e09":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_29aee77f64c0400cb6299104f51e7d6d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 143/143 [00:00&lt;00:00, 2003.22it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b96ee18df24941518b5670cd9c694575"}},"d14c11443d3a47789fc37e4591f9162b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"56a140221c2a45339fc63e10305a9904":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ccfbf231a1ca4c50bf76498325e07b79":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d90a5ddc207849698b7ded16468420e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"29aee77f64c0400cb6299104f51e7d6d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b96ee18df24941518b5670cd9c694575":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"de6402d4aa614af7b1d58e0ce9703d29":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0cb0dcb8b6c44532bf0f46c3b31cd403","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a390e5c1f7fb4071b9ef40391f5899ea","IPY_MODEL_be8f0e42e2974ee5847fcfee3961bfa8","IPY_MODEL_34cffccd19ca4c4982d306d2e6539ec6"]}},"0cb0dcb8b6c44532bf0f46c3b31cd403":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a390e5c1f7fb4071b9ef40391f5899ea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7cfd44dc72ed4c62aac45c29432080e8","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c252519b399b42a2bf584999cd5b203c"}},"be8f0e42e2974ee5847fcfee3961bfa8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f7f531db4e6343c5bf8abc525abcc00e","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":52,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":52,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c1b0d317c72744f5bf6d2262433e69eb"}},"34cffccd19ca4c4982d306d2e6539ec6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ec6dda688bc844929d6b40bc18ce3721","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 52.0/52.0 [00:00&lt;00:00, 1.89kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c17a85f150184b4f8537415be182de63"}},"7cfd44dc72ed4c62aac45c29432080e8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c252519b399b42a2bf584999cd5b203c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f7f531db4e6343c5bf8abc525abcc00e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c1b0d317c72744f5bf6d2262433e69eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ec6dda688bc844929d6b40bc18ce3721":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c17a85f150184b4f8537415be182de63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4748eb6316f84473a53b593721fdad30":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_72288a2574c043bd936114bcedeb7a87","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a00864cef7c34606802489ec9de53c82","IPY_MODEL_f8e7a617a1c74e8c925fcd5d3a724e6a","IPY_MODEL_876e1f875b274c3b814decf2b9123b44"]}},"72288a2574c043bd936114bcedeb7a87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a00864cef7c34606802489ec9de53c82":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3308cb4b493044e78cd50761e2c393f9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c775606a5e794b06a5d2fd889d814cc6"}},"f8e7a617a1c74e8c925fcd5d3a724e6a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_59f9d735d8c4493690f2c730b7b1293f","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":474,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":474,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bfdce0a738884ff8950df0916f0279d6"}},"876e1f875b274c3b814decf2b9123b44":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_20b5d6353577421bafeb5ad30edb4755","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 474/474 [00:00&lt;00:00, 19.5kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7ec72f88b4fa41808b509568a9bd6d22"}},"3308cb4b493044e78cd50761e2c393f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c775606a5e794b06a5d2fd889d814cc6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"59f9d735d8c4493690f2c730b7b1293f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"bfdce0a738884ff8950df0916f0279d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"20b5d6353577421bafeb5ad30edb4755":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7ec72f88b4fa41808b509568a9bd6d22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4e0d3e0ba644490f81a4907b9f479977":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e4cb579869b94901b10907352874d270","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_976c4c15e2ab4462998f17e956f62800","IPY_MODEL_96d35ce8a6fc48b994b48bb6bafd6f13","IPY_MODEL_7f2e537dda58431c83ee07ba6bab9996"]}},"e4cb579869b94901b10907352874d270":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"976c4c15e2ab4462998f17e956f62800":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_dc2ca91cc3fc40d8b7d75c9fe25fa261","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_97ecf84fe68043c199f350fb5d7aecbf"}},"96d35ce8a6fc48b994b48bb6bafd6f13":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b722d8c874d14a2e8fb162409ce78482","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":898825,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":898825,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_294c6af870b44f5a81cc050c2cbacc9a"}},"7f2e537dda58431c83ee07ba6bab9996":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3199b1d33b0b4816aaaafa6fdcb2a94b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 878k/878k [00:00&lt;00:00, 6.14MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_212331f2abac46d7b6fd456c90d92555"}},"dc2ca91cc3fc40d8b7d75c9fe25fa261":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"97ecf84fe68043c199f350fb5d7aecbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b722d8c874d14a2e8fb162409ce78482":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"294c6af870b44f5a81cc050c2cbacc9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3199b1d33b0b4816aaaafa6fdcb2a94b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"212331f2abac46d7b6fd456c90d92555":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"46690d244b894522becd5b590e775047":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_672ab6fb002545fbb98df4b1bd00a2ad","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ad16c9931ad748e0b440d41310939567","IPY_MODEL_d2a17d161f5d4080a19b34fe67c00aec","IPY_MODEL_fac192764b1141d0b5e0875d0508264b"]}},"672ab6fb002545fbb98df4b1bd00a2ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ad16c9931ad748e0b440d41310939567":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e3256ddc0972490b9dd42eb06e77f056","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8dfc300d95774958aa7ab2c31161e70b"}},"d2a17d161f5d4080a19b34fe67c00aec":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_99015149fe674971ba9c9fbf4404dc7c","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":456318,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":456318,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_32640c31b69c4ddb989a289b0a078b75"}},"fac192764b1141d0b5e0875d0508264b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2753130aed024d97af79c1631dc3fb8c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 446k/446k [00:00&lt;00:00, 4.70MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_95de06adf9584609a2d4c2414aace49e"}},"e3256ddc0972490b9dd42eb06e77f056":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8dfc300d95774958aa7ab2c31161e70b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"99015149fe674971ba9c9fbf4404dc7c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"32640c31b69c4ddb989a289b0a078b75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2753130aed024d97af79c1631dc3fb8c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"95de06adf9584609a2d4c2414aace49e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":5}