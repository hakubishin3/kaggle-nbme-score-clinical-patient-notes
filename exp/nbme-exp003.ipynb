{"cells":[{"cell_type":"markdown","metadata":{"id":"incredible-principle"},"source":["## References"]},{"cell_type":"markdown","metadata":{"id":"simplified-tract"},"source":["- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"]},{"cell_type":"markdown","metadata":{"id":"boolean-shame"},"source":["## Configurations"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1645625690811,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"needed-consistency"},"outputs":[],"source":["EXP_NAME = \"nbme-exp003\"\n","ENV = \"colab\"\n","DEBUG_MODE = False\n","SUBMISSION_MODE = False"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1645625690812,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"operational-trader"},"outputs":[],"source":["class CFG:\n","    env=ENV\n","    exp_name=EXP_NAME\n","    debug=DEBUG_MODE\n","    submission=SUBMISSION_MODE\n","    apex=True\n","    input_dir=None\n","    output_dir=None\n","    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n","    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n","    competition_name=\"nbme-score-clinical-patient-notes\"\n","    id_col=\"id\"\n","    target_col=\"location\"\n","    pretrained_model_name=\"microsoft/deberta-base\"\n","    tokenizer=None\n","    max_len=None\n","    output_dim=1\n","    dropout=0.2\n","    num_workers=4\n","    batch_size=8\n","    lr=2e-5\n","    betas=(0.9, 0.98)\n","    weight_decay=0.1\n","    num_warmup_steps_rate=0.1\n","    batch_scheduler=True\n","    epochs=5\n","    n_fold=5\n","    train_fold=[0, 1, 2, 3, 4]\n","    seed=71\n","    gradient_accumulation_steps=1\n","    max_grad_norm=1000\n","    print_freq=100\n","    train=True\n","    inference=True"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1645625690812,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"seasonal-consistency"},"outputs":[],"source":["if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.train_fold = [0, 1]\n","\n","if CFG.submission:\n","    CFG.train = False\n","    CFG.inference = True"]},{"cell_type":"markdown","metadata":{"id":"billion-composite"},"source":["## Directory Settings"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30309,"status":"ok","timestamp":1645625721113,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"desperate-collect","outputId":"c2d67c04-3582-414c-aae4-0c0bf1988f09"},"outputs":[{"name":"stdout","output_type":"stream","text":["colab\n","Mounted at /content/drive\n","Collecting transformers\n","  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 5.1 MB/s \n","\u001b[?25hCollecting pyyaml\u003e=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 80.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Collecting huggingface-hub\u003c1.0,\u003e=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 6.9 MB/s \n","\u001b[?25hRequirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 78.2 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.1)\n","Collecting tokenizers!=0.11.3,\u003e=0.10.1\n","  Downloading tokenizers-0.11.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n","\u001b[K     |████████████████████████████████| 6.8 MB 75.4 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.1.0-\u003etransformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=20.0-\u003etransformers) (3.0.7)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003etransformers) (3.7.0)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2.10)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2021.10.8)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (1.24.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers) (1.1.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.5 transformers-4.16.2\n"]}],"source":["import sys\n","from pathlib import Path\n","\n","\n","print(CFG.env)\n","if CFG.env == \"colab\":\n","    # colab環境\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","    # install packages\n","    !pip install transformers\n","\n","elif CFG.env == \"local\":\n","    # ローカルサーバ\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","\n","elif CFG.env == \"kaggle\":\n","    # kaggle環境\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./\")"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":8037,"status":"ok","timestamp":1645625729146,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"acute-pregnancy"},"outputs":[],"source":["import gc\n","import os\n","import ast\n","import time\n","import math\n","import random\n","import itertools\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","from scipy.optimize import minimize\n","from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as T\n","from torchvision.io import read_image\n","from torch.utils.data import DataLoader, Dataset\n","\n","from transformers import BartModel,BertModel,BertTokenizer\n","from transformers import DebertaModel,DebertaTokenizer\n","from transformers import RobertaModel,RobertaTokenizer\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"generous-raleigh"},"source":["## Utilities"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1645625729146,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"controlling-headset"},"outputs":[],"source":["def micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on binary arrays.\n","\n","    Args:\n","        preds (list of lists of ints): Predictions.\n","        truths (list of lists of ints): Ground truths.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    # Micro : aggregating over all instances\n","    preds = np.concatenate(preds)\n","    truths = np.concatenate(truths)\n","    return f1_score(truths, preds)\n","\n","\n","def spans_to_binary(spans, length=None):\n","    \"\"\"\n","    Converts spans to a binary array indicating whether each character is in the span.\n","\n","    Args:\n","        spans (list of lists of two ints): Spans.\n","\n","    Returns:\n","        np array [length]: Binarized spans.\n","    \"\"\"\n","    length = np.max(spans) if length is None else length\n","    binary = np.zeros(length)\n","    for start, end in spans:\n","        binary[start:end] = 1\n","    return binary\n","\n","\n","def span_micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on spans.\n","\n","    Args:\n","        preds (list of lists of two ints): Prediction spans.\n","        truths (list of lists of two ints): Ground truth spans.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    bin_preds = []\n","    bin_truths = []\n","    for pred, truth in zip(preds, truths):\n","        if not len(pred) and not len(truth):\n","            continue\n","        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n","        bin_preds.append(spans_to_binary(pred, length))\n","        bin_truths.append(spans_to_binary(truth, length))\n","    return micro_f1(bin_preds, bin_truths)\n","\n","\n","def get_score(y_true, y_pred):\n","    score = span_micro_f1(y_true, y_pred)\n","    return score"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1645625729146,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"UOscbQSt4Cqo"},"outputs":[],"source":["def create_labels_for_scoring(df):\n","    # example: ['48 61', '111 128'] -\u003e [[48, 61], [111, 128]]\n","    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n","    for i in range(len(df)):\n","        lst = df.loc[i, \"location\"]\n","        if lst:\n","            new_lst = \";\".join(lst)\n","            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n","\n","    # create labels\n","    truths = []\n","    for location_list in df[\"location_for_create_labels\"].values:\n","        truth = []\n","        if len(location_list) \u003e 0:\n","            location = location_list[0]\n","            for loc in [s.split() for s in location.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                truth.append([start, end])\n","        truths.append(truth)\n","\n","    return truths\n","\n","\n","def get_char_probs(texts, token_probs, tokenizer):\n","    res = [np.zeros(len(t)) for t in texts]\n","    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n","        encoded = tokenizer(\n","            text=text,\n","            max_length=CFG.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n","            start, end = offset_mapping\n","            res[i][start:end] = pred\n","    return res\n","\n","\n","def get_predicted_location_str(char_probs, th=0.5):\n","    results = []\n","    for char_prob in char_probs:\n","        result = np.where(char_prob \u003e= th)[0] + 1\n","        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n","        result = [f\"{min(r)} {max(r)}\" for r in result]\n","        result = \";\".join(result)\n","        results.append(result)\n","    return results\n","\n","\n","def get_predictions(results):\n","    predictions = []\n","    for result in results:\n","        prediction = []\n","        if result != \"\":\n","            for loc in [s.split() for s in result.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                prediction.append([start, end])\n","        predictions.append(prediction)\n","    return predictions\n","\n","\n","def scoring(df, th=0.5):\n","    labels = create_labels_for_scoring(df)\n","\n","    token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n","    char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n","    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n","    preds = get_predictions(predicted_location_str)\n","\n","    score = get_score(labels, preds)\n","    return score\n","\n","\n","def get_best_thres(oof_df):\n","    def f1_opt(x):\n","        return -1 * scoring(oof_df, th=x)\n","\n","    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n","    return best_thres"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1645625729146,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"mmZHVaPkh0Qc"},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return \"%dm %ds\" % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1645625729146,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"IydDnpFyh4PX"},"outputs":[],"source":["seed_everything()"]},{"cell_type":"markdown","metadata":{"id":"formed-handbook"},"source":["## Data Loading"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2454,"status":"ok","timestamp":1645625731592,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"vanilla-register","outputId":"ddf6ea25-10ab-44c7-cc42-450d38c46955"},"outputs":[{"data":{"text/plain":["((14300, 6), (143, 3), (42146, 3), (5, 4))"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["train = pd.read_csv(CFG.input_dir / \"train.csv\")\n","features = pd.read_csv(CFG.input_dir / \"features.csv\")\n","patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n","test = pd.read_csv(CFG.input_dir / \"test.csv\")\n","\n","train.shape, features.shape, patient_notes.shape, test.shape"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1645625731593,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"approximate-transmission"},"outputs":[],"source":["if CFG.debug:\n","    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n","    print(train.shape)"]},{"cell_type":"markdown","metadata":{"id":"civic-advisory"},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1645625731593,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"irish-nature"},"outputs":[],"source":["def preprocess_features(features):\n","    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n","    return features\n","\n","\n","features = preprocess_features(features)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1645625732029,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"parliamentary-stupid","outputId":"1c2552a9-f7e7-488a-a24f-b7ce7c1b0d9e"},"outputs":[{"data":{"text/plain":["((14300, 8), (5, 6))"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","\n","train.shape, test.shape"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1645625732029,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"vietnamese-spare"},"outputs":[],"source":["def fix_anno(df, target_id, annotation, location):\n","    idx = df[\"id\"] == target_id\n","    df.loc[idx, \"annotation\"] = annotation\n","    df.loc[idx, \"location\"] = location"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1645625732030,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"tutorial-soldier","outputId":"db300a42-33ff-41f0-a9c4-e1922f64b750"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nfix_anno(train, \"00669_000\", \"[\\'father heart attack\\']\", \"[\\'764 783\\']\")\\nfix_anno(train, \"01110_010\", \"[\\'for the last 2-3 months\\', \\'over the last 2 months\\']\", \"[\\'77 100\\', \\'398 420\\']\")\\nfix_anno(train, \"01146_005\", \"[\\'no heat intolerance\\', \\'no cold intolerance\\']\", \"[\\'285 292;301 312\\', \\'285 287;296 312\\']\")\\nfix_anno(train, \"02428_001\", \"[\\'mother thyroid problem\\']\", \"[\\'551 557;565 580\\']\")\\nfix_anno(train, \"02428_004\", \\'[\\'felt like he was going to \"pass out\"\\']\\', \"[\\'131 135;181 212\\']\")\\nfix_anno(train, \"10047_105\", \"[\\'stool , with no blood\\']\", \"[\\'259 280\\']\")\\nfix_anno(train, \"10196_105\", \"[\\'diarrhoe non blooody\\']\", \"[\\'176 184;201 212\\']\")\\nfix_anno(train, \"10206_103\", \"[\\'diarrhea for last 2-3 days\\']\", \"[\\'249 257;271 288\\']\")\\nfix_anno(train, \"10228_100\", \"[\\'no vaginal discharge\\']\", \"[\\'822 824;907 924\\']\")\\nfix_anno(train, \"10268_111\", \"[\\'started about 8-10 hours ago\\']\", \"[\\'101 129\\']\")\\nfix_anno(train, \"10459_105\", \"[\\'no blood in the stool\\']\", \"[\\'531 539;549 561\\']\")\\nfix_anno(train, \"10620_102\", \"[\\'last sexually active 9 months ago\\']\", \"[\\'540 560;581 593\\']\")\\nfix_anno(train, \"10646_107\", \"[\\'right lower quadrant pain\\']\", \"[\\'32 57\\']\")\\nfix_anno(train, \"10968_105\", \"[\\'diarrhoea no blood\\']\", \"[\\'308 317;376 384\\']\")\\nfix_anno(train, \"20747_214\", \"[\\'sweating\\']\", \"[\\'549 557\\']\")\\nfix_anno(\\n    train,\\n    \"21686_200\",\\n    \"[\\'previously as regular\\', \\'previously eveyr 28-29 days\\', \\'previously lasting 5 days\\', \\'previously regular flow\\']\",\\n    \"[\\'102 123\\', \\'102 112;125 141\\', \\'102 112;143 157\\', \\'102 112;159 171\\']\",\\n)\\nfix_anno(train, \"30437_309\", \"[\\'for 2 months\\']\", \"[\\'33 45\\']\")\\nfix_anno(train, \"32657_315\", \"[\\'35 year old\\']\", \"[\\'5 16\\']\")\\nfix_anno(train, \"32996_302\", \"[\\'darker brown stools\\']\", \"[\\'175 194\\']\")\\nfix_anno(train, \"33531_300\", \"[\\'uncle with peptic ulcer\\']\", \"[\\'700 723\\']\")\\nfix_anno(train, \"40974_406\", \"[\\'difficulty falling asleep\\']\", \"[\\'225 250\\']\")\\nfix_anno(train, \"41825_402\", \"[\\'helps to take care of aging mother and in-laws\\']\", \"[\\'197 218;236 260\\']\")\\nfix_anno(\\n    train,\\n    \"42625_400\",\\n    \"[\\'No hair changes\\', \\'No skin changes\\', \\'No GI changes\\', \\'No palpitations\\', \\'No excessive sweating\\']\",\\n    \"[\\'480 482;507 519\\', \\'480 482;499 503;512 519\\', \\'480 482;521 531\\', \\'480 482;533 545\\', \\'480 482;564 582\\']\",\\n)\\nfix_anno(\\n    train,\\n    \"43451_402\",\\n    \"[\\'stressed due to taking care of her mother\\', \\'stressed due to taking care of husbands parents\\']\",\\n    \"[\\'290 320;327 337\\', \\'290 320;342 358\\']\",\\n)\\nfix_anno(train, \"44958_402\", \"[\\'stressor taking care of many sick family members\\']\", \"[\\'288 296;324 363\\']\")\\nfix_anno(train, \"50574_514\", \"[\\'heart started racing and felt numbness for the 1st time in her finger tips\\']\", \"[\\'108 182\\']\")\\nfix_anno(train, \"52512_500\", \"[\\'first started 5 yrs\\']\", \"[\\'102 121\\']\")\\nfix_anno(train, \"60235_608\", \"[\\'No shortness of breath\\']\", \"[\\'481 483;533 552\\']\")\\nfix_anno(train, \"60469_603\", \"[\\'recent URI\\', \\'nasal stuffines, rhinorrhea, for 3-4 days\\']\", \"[\\'92 102\\', \\'123 164\\']\")\\nfix_anno(\\n    train,\\n    \"70255_702\",\\n    \"[\\'irregularity with her cycles\\', \\'heavier bleeding\\', \\'changes her pad every couple hours\\']\",\\n    \"[\\'89 117\\', \\'122 138\\', \\'368 402\\']\",\\n)\\nfix_anno(train, \"70412_701\", \"[\\'gaining 10-15 lbs\\']\", \"[\\'344 361\\']\")\\nfix_anno(train, \"72660_701\", \"[\\'weight gain\\', \\'gain of 10-16lbs\\']\", \"[\\'600 611\\', \\'607 623\\']\")\\nfix_anno(train, \"81856_813\", \"[\\'seeing her son knows are not real\\']\", \"[\\'386 400;443 461\\']\")\\nfix_anno(train, \"81985_813\", \"[\\'saw him once in the kitchen after he died\\']\", \"[\\'160 201\\']\")\\nfix_anno(train, \"83199_810\", \"[\\'tried Ambien but it didnt work\\']\", \"[\\'325 337;349 366\\']\")\\nfix_anno(train, \"83757_803\", \"[\\'heard what she described as a party later than evening these things did not actually happen\\']\", \"[\\'405 459;488 524\\']\")\\nfix_anno(train, \"83757_813\", \"[\\'experienced seeing her son at the kitchen table these things did not actually happen\\']\", \"[\\'353 400;488 524\\']\")\\nfix_anno(train, \"92224_909\", \"[\\'SCRACHY THROAT\\', \\'RUNNY NOSE\\']\", \"[\\'293 307\\', \\'321 331\\']\")\\nfix_anno(train, \"92385_900\", \"[\\'without improvement when taking tylenol\\', \\'without improvement when taking ibuprofen\\']\", \"[\\'182 221\\', \\'182 213;225 234\\']\")\\nfix_anno(train, \"92385_902\", \"[\\'yesterday\\', \\'yesterday\\']\", \"[\\'79 88\\', \\'409 418\\']\")\\nfix_anno(train, \"93988_904\", \"[\\'headache global\\', \\'headache throughout her head\\']\", \"[\\'86 94;230 236\\', \\'86 94;237 256\\']\")\\nfix_anno(train, \"94656_904\", \"[\\'headache generalized in her head\\']\", \"[\\'56 64;156 179\\']\")\\n'"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","fix_anno(train, \"00669_000\", \"['father heart attack']\", \"['764 783']\")\n","fix_anno(train, \"01110_010\", \"['for the last 2-3 months', 'over the last 2 months']\", \"['77 100', '398 420']\")\n","fix_anno(train, \"01146_005\", \"['no heat intolerance', 'no cold intolerance']\", \"['285 292;301 312', '285 287;296 312']\")\n","fix_anno(train, \"02428_001\", \"['mother thyroid problem']\", \"['551 557;565 580']\")\n","fix_anno(train, \"02428_004\", '[\\'felt like he was going to \"pass out\"\\']', \"['131 135;181 212']\")\n","fix_anno(train, \"10047_105\", \"['stool , with no blood']\", \"['259 280']\")\n","fix_anno(train, \"10196_105\", \"['diarrhoe non blooody']\", \"['176 184;201 212']\")\n","fix_anno(train, \"10206_103\", \"['diarrhea for last 2-3 days']\", \"['249 257;271 288']\")\n","fix_anno(train, \"10228_100\", \"['no vaginal discharge']\", \"['822 824;907 924']\")\n","fix_anno(train, \"10268_111\", \"['started about 8-10 hours ago']\", \"['101 129']\")\n","fix_anno(train, \"10459_105\", \"['no blood in the stool']\", \"['531 539;549 561']\")\n","fix_anno(train, \"10620_102\", \"['last sexually active 9 months ago']\", \"['540 560;581 593']\")\n","fix_anno(train, \"10646_107\", \"['right lower quadrant pain']\", \"['32 57']\")\n","fix_anno(train, \"10968_105\", \"['diarrhoea no blood']\", \"['308 317;376 384']\")\n","fix_anno(train, \"20747_214\", \"['sweating']\", \"['549 557']\")\n","fix_anno(\n","    train,\n","    \"21686_200\",\n","    \"['previously as regular', 'previously eveyr 28-29 days', 'previously lasting 5 days', 'previously regular flow']\",\n","    \"['102 123', '102 112;125 141', '102 112;143 157', '102 112;159 171']\",\n",")\n","fix_anno(train, \"30437_309\", \"['for 2 months']\", \"['33 45']\")\n","fix_anno(train, \"32657_315\", \"['35 year old']\", \"['5 16']\")\n","fix_anno(train, \"32996_302\", \"['darker brown stools']\", \"['175 194']\")\n","fix_anno(train, \"33531_300\", \"['uncle with peptic ulcer']\", \"['700 723']\")\n","fix_anno(train, \"40974_406\", \"['difficulty falling asleep']\", \"['225 250']\")\n","fix_anno(train, \"41825_402\", \"['helps to take care of aging mother and in-laws']\", \"['197 218;236 260']\")\n","fix_anno(\n","    train,\n","    \"42625_400\",\n","    \"['No hair changes', 'No skin changes', 'No GI changes', 'No palpitations', 'No excessive sweating']\",\n","    \"['480 482;507 519', '480 482;499 503;512 519', '480 482;521 531', '480 482;533 545', '480 482;564 582']\",\n",")\n","fix_anno(\n","    train,\n","    \"43451_402\",\n","    \"['stressed due to taking care of her mother', 'stressed due to taking care of husbands parents']\",\n","    \"['290 320;327 337', '290 320;342 358']\",\n",")\n","fix_anno(train, \"44958_402\", \"['stressor taking care of many sick family members']\", \"['288 296;324 363']\")\n","fix_anno(train, \"50574_514\", \"['heart started racing and felt numbness for the 1st time in her finger tips']\", \"['108 182']\")\n","fix_anno(train, \"52512_500\", \"['first started 5 yrs']\", \"['102 121']\")\n","fix_anno(train, \"60235_608\", \"['No shortness of breath']\", \"['481 483;533 552']\")\n","fix_anno(train, \"60469_603\", \"['recent URI', 'nasal stuffines, rhinorrhea, for 3-4 days']\", \"['92 102', '123 164']\")\n","fix_anno(\n","    train,\n","    \"70255_702\",\n","    \"['irregularity with her cycles', 'heavier bleeding', 'changes her pad every couple hours']\",\n","    \"['89 117', '122 138', '368 402']\",\n",")\n","fix_anno(train, \"70412_701\", \"['gaining 10-15 lbs']\", \"['344 361']\")\n","fix_anno(train, \"72660_701\", \"['weight gain', 'gain of 10-16lbs']\", \"['600 611', '607 623']\")\n","fix_anno(train, \"81856_813\", \"['seeing her son knows are not real']\", \"['386 400;443 461']\")\n","fix_anno(train, \"81985_813\", \"['saw him once in the kitchen after he died']\", \"['160 201']\")\n","fix_anno(train, \"83199_810\", \"['tried Ambien but it didnt work']\", \"['325 337;349 366']\")\n","fix_anno(train, \"83757_803\", \"['heard what she described as a party later than evening these things did not actually happen']\", \"['405 459;488 524']\")\n","fix_anno(train, \"83757_813\", \"['experienced seeing her son at the kitchen table these things did not actually happen']\", \"['353 400;488 524']\")\n","fix_anno(train, \"92224_909\", \"['SCRACHY THROAT', 'RUNNY NOSE']\", \"['293 307', '321 331']\")\n","fix_anno(train, \"92385_900\", \"['without improvement when taking tylenol', 'without improvement when taking ibuprofen']\", \"['182 221', '182 213;225 234']\")\n","fix_anno(train, \"92385_902\", \"['yesterday', 'yesterday']\", \"['79 88', '409 418']\")\n","fix_anno(train, \"93988_904\", \"['headache global', 'headache throughout her head']\", \"['86 94;230 236', '86 94;237 256']\")\n","fix_anno(train, \"94656_904\", \"['headache generalized in her head']\", \"['56 64;156 179']\")\n","\"\"\""]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1645625732030,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"imposed-encyclopedia"},"outputs":[],"source":["train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n","train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1645625732030,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"collected-princeton","outputId":"4410d6a9-9663-4648-a272-d6aca0b0db88"},"outputs":[{"data":{"text/plain":["0    4399\n","1    8181\n","2    1296\n","3     287\n","4      99\n","5      27\n","6       9\n","7       1\n","8       1\n","Name: annotation_length, dtype: int64"]},"metadata":{},"output_type":"display_data"}],"source":["train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n","display(train['annotation_length'].value_counts().sort_index())"]},{"cell_type":"markdown","metadata":{"id":"downtown-frame"},"source":["## CV split"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1645625732030,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"detailed-drive"},"outputs":[],"source":["def get_groupkfold(df, group_name):\n","    groups = df[group_name].unique()\n","\n","    kf = KFold(\n","        n_splits=CFG.n_fold,\n","        shuffle=True,\n","        random_state=CFG.seed,\n","    )\n","    folds_ids = []\n","    for i_fold, (_, val_group_idx) in enumerate(kf.split(groups)):\n","        val_group = groups[val_group_idx]\n","        is_val = df[group_name].isin(val_group)\n","        val_idx = df[is_val].index\n","        df.loc[val_idx, \"fold\"] = int(i_fold)\n","\n","    df[\"fold\"] = df[\"fold\"].astype(int)\n","    return df"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1645625732030,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"other-satisfaction","outputId":"5bcc96ed-688a-4f92-be01-2fe26c29ad9d"},"outputs":[{"data":{"text/plain":["fold\n","0    2902\n","1    2894\n","2    2813\n","3    2791\n","4    2900\n","dtype: int64"]},"metadata":{},"output_type":"display_data"}],"source":["train = get_groupkfold(train, \"pn_num\")\n","display(train.groupby(\"fold\").size())"]},{"cell_type":"markdown","metadata":{"id":"senior-wichita"},"source":["## Setup tokenizer"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145},"executionInfo":{"elapsed":1939,"status":"ok","timestamp":1645625733959,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"thrown-theology","outputId":"02ac3be1-5413-40cc-a03e-ac30f5650d45"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1240e074abae4cd7bee98a82b6acb493","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/52.0 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ef70a47928e241f69a9b9c114a6ed384","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/474 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8f0c338c4759472f9250d8f08b347d2e","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/878k [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b9f3c3b86db74db7bef8fc86f2d246ac","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/446k [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["if CFG.submission:\n","    tokenizer = AutoTokenizer.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n","else:\n","    tokenizer = AutoTokenizer.from_pretrained(CFG.pretrained_model_name)\n","    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n","\n","CFG.tokenizer = tokenizer"]},{"cell_type":"markdown","metadata":{"id":"varying-tourism"},"source":["## Create dataset"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"elapsed":21477,"status":"ok","timestamp":1645625755427,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"white-integral","outputId":"3608a2a1-43f7-40e3-b900-463179dcb231"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"23802b98bd184620bb7ee14fd4bb8556","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/42146 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["max length: 433\n"]}],"source":["pn_history_lengths = []\n","tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    pn_history_lengths.append(length)\n","\n","print(\"max length:\", np.max(pn_history_lengths))"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1645625755764,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"demonstrated-version","outputId":"f5296056-7b0c-44ea-92b8-bd7f3b9a6647"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c64066983b5c4aa688e22b41a2431341","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/143 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["max length: 30\n"]}],"source":["feature_text_lengths = []\n","tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    feature_text_lengths.append(length)\n","\n","print(\"max length:\", np.max(feature_text_lengths))"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1645625755765,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"posted-miami","outputId":"16b6ec3a-dde9-4981-ac74-439a6e3a9339"},"outputs":[{"name":"stdout","output_type":"stream","text":["max length: 466\n"]}],"source":["CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls \u0026 sep \u0026 sep\n","\n","print(\"max length:\", CFG.max_len)"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1645625755765,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"fossil-supply"},"outputs":[],"source":["class TrainingDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","        self.annotation_lengths = self.df[\"annotation_length\"].values\n","        self.locations = self.df[\"location\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def _create_label(self, pn_history, annotation_length, location_list):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        offset_mapping = encoded[\"offset_mapping\"]\n","        ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n","        label = np.zeros(len(offset_mapping))\n","        label[ignore_idxes] = -1\n","\n","        if annotation_length \u003e 0:\n","            for location in location_list:\n","                for loc in [s.split() for s in location.split(\";\")]:\n","                    start, end = int(loc[0]), int(loc[1])\n","                    start_idx = -1\n","                    end_idx = -1\n","                    for idx in range(len(offset_mapping)):\n","                        if (start_idx == -1) \u0026 (start \u003c offset_mapping[idx][0]):\n","                            start_idx = idx - 1\n","                        if (end_idx == -1) \u0026 (end \u003c= offset_mapping[idx][1]):\n","                            end_idx = idx + 1\n","                    if start_idx == -1:\n","                        start_idx = end_idx\n","                    if (start_idx != -1) \u0026 (end_idx != -1):\n","                        label[start_idx:end_idx] = 1\n","\n","        return torch.tensor(label, dtype=torch.float)\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n","        return input_, label"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1645625755765,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"apparent-norfolk"},"outputs":[],"source":["class TestDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        return input_"]},{"cell_type":"markdown","metadata":{"id":"motivated-bread"},"source":["## Model"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1645625755766,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"minute-virginia"},"outputs":[],"source":["class CustomModel(nn.Module):\n","    def __init__(self, cfg, model_config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        if model_config_path is None:\n","            self.model_config = AutoConfig.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                output_hidden_states=True,\n","            )\n","        else:\n","            self.model_config = torch.load(model_config_path)\n","\n","        if pretrained:\n","            self.backbone = AutoModel.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                config=self.model_config,\n","            )\n","        else:\n","            self.backbone = AutoModel.from_config(self.model_config)\n","\n","        self.fc = nn.Sequential(\n","            nn.Dropout(self.cfg.dropout),\n","            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n","        )\n","\n","    def forward(self, inputs):\n","        h = self.backbone(**inputs)[\"last_hidden_state\"]\n","        output = self.fc(h)\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"seventh-configuration"},"source":["## Training"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1645625755766,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"rocky-lexington"},"outputs":[],"source":["def train_fn(\n","    train_dataloader,\n","    model,\n","    criterion,\n","    optimizer,\n","    epoch,\n","    scheduler,\n","    device,\n","):\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(train_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n","        if CFG.gradient_accumulation_steps \u003e 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","\n","        if CFG.batch_scheduler:\n","            scheduler.step()\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n","            print(\n","                \"Epoch: [{0}][{1}/{2}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                \"Grad: {grad_norm:.4f}  \"\n","                \"LR: {lr:.6f}  \"\n","                .format(\n","                    epoch+1,\n","                    step,\n","                    len(train_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n","                    loss=losses,\n","                     grad_norm=grad_norm,\n","                     lr=scheduler.get_lr()[0],\n","                )\n","            )\n","    return losses.avg"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1645625755766,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"honest-programming"},"outputs":[],"source":["def valid_fn(\n","    val_dataloader,\n","    model,\n","    criterion,\n","    device,\n","):\n","    model.eval()\n","    preds = []\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels) in enumerate(val_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.no_grad():\n","            output = model(inputs)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n","        if CFG.gradient_accumulation_steps \u003e 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(output.sigmoid().squeeze().detach().cpu().numpy())\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n","            print(\n","                \"EVAL: [{0}/{1}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                .format(\n","                    step, len(val_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n","                    loss=losses,\n","                )\n","            )\n","    preds = np.concatenate(preds)\n","    return losses.avg, preds"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1645625755766,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"junior-international"},"outputs":[],"source":["def inference_fn(test_dataloader, model, device):\n","    model.eval()\n","    model.to(device)\n","    preds = []\n","    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n","    for inputs in tk0:\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        with torch.no_grad():\n","            output = model(inputs)\n","        preds.append(output.sigmoid().squeeze().detach().cpu().numpy())\n","    preds = np.concatenate(preds)\n","    return preds"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1645625755767,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"complicated-testament"},"outputs":[],"source":["def train_loop(df, i_fold, device):\n","    print(f\"========== fold: {i_fold} training ==========\")\n","    train_idx = df[df[\"fold\"] != i_fold].index\n","    val_idx = df[df[\"fold\"] == i_fold].index\n","\n","    train_folds = df.loc[train_idx].reset_index(drop=True)\n","    val_folds = df.loc[val_idx].reset_index(drop=True)\n","\n","    train_dataset = TrainingDataset(CFG, train_folds)\n","    val_dataset = TrainingDataset(CFG, val_folds)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=True,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=True,\n","    )\n","    val_dataloader = DataLoader(\n","        val_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=False,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=False,\n","    )\n","\n","    model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n","    model.to(device)\n","\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\"params\": [p for n, p in param_optimizer if not any(\n","            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n","        {\"params\": [p for n, p in param_optimizer if any(\n","            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n","    ]\n","    optimizer = AdamW(\n","        optimizer_grouped_parameters,\n","        lr=CFG.lr,\n","        betas=CFG.betas,\n","        weight_decay=CFG.weight_decay,\n","    )\n","    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n","    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        num_training_steps=num_train_optimization_steps,\n","    )\n","\n","    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    best_score = -1 * np.inf\n","\n","    for epoch in range(CFG.epochs):\n","        start_time = time.time()\n","        avg_loss = train_fn(\n","            train_dataloader,\n","            model,\n","            criterion,\n","            optimizer,\n","            epoch,\n","            scheduler,\n","            device,\n","        )\n","        avg_val_loss, val_preds = valid_fn(\n","            val_dataloader,\n","            model,\n","            criterion,\n","            device,\n","        )\n","\n","        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n","            scheduler.step()\n","\n","        # scoring\n","        val_folds[[str(i) for i in range(CFG.max_len)]] = val_preds\n","        score = scoring(val_folds, th=0.5)\n","\n","        elapsed = time.time() - start_time\n","\n","        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n","        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n","        if score \u003e best_score:\n","            best_score = score\n","            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n","            torch.save({\n","                \"model\": model.state_dict(),\n","                \"predictions\": val_preds,\n","                },\n","                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","            )\n","\n","    predictions = torch.load(\n","        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","        map_location=torch.device(\"cpu\"),\n","    )[\"predictions\"]\n","    val_folds[[str(i) for i in range(CFG.max_len)]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return val_folds"]},{"cell_type":"markdown","metadata":{"id":"returning-banner"},"source":["## Main"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":274,"status":"ok","timestamp":1645625756032,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"},"user_tz":-540},"id":"ongoing-budget"},"outputs":[],"source":["def main():\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    if CFG.train:\n","        oof_df = pd.DataFrame()\n","        for i_fold in range(CFG.n_fold):\n","            if i_fold in CFG.train_fold:\n","                _oof_df = train_loop(train, i_fold, device)\n","                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n","        #oof_df.to_csv(CFG.output_dir / \"oof_df.csv\", index=False)\n","        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    if CFG.submission:\n","        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n","    else:\n","        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    score = scoring(oof_df, th=0.5)\n","    print(f\"Best thres: 0.5, Score: {score:.4f}\")\n","    best_thres = get_best_thres(oof_df)\n","    score = scoring(oof_df, th=best_thres)\n","    print(f\"Best thres: {best_thres}, Score: {score:.4f}\")\n","\n","    if CFG.inference:\n","        test_dataset = TestDataset(CFG, test)\n","        test_dataloader = DataLoader(\n","            test_dataset,\n","            batch_size=CFG.batch_size,\n","            shuffle=False,\n","            num_workers=CFG.num_workers,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","        predictions = []\n","        for i_fold in CFG.train_fold:\n","            if CFG.submission:\n","                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n","                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n","            else:\n","                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n","\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            model.load_state_dict(state[\"model\"])\n","            test_token_probs = inference_fn(test_dataloader, model, device)\n","            test[[f\"fold{i_fold}_{i}\" for i in range(CFG.max_len)]] = test_token_probs\n","            test_char_probs = get_char_probs(test[\"pn_history\"].values, test_token_probs, CFG.tokenizer)\n","            predictions.append(test_char_probs)\n","\n","            del state, test_token_probs, model; gc.collect()\n","            torch.cuda.empty_cache()\n","\n","        predictions = np.mean(predictions, axis=0)\n","        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n","        test[CFG.target_col] = predicted_location_str\n","        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n","        test[[CFG.id_col, CFG.target_col]].to_csv(\n","            CFG.output_dir / \"submission.csv\", index=False\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"nearby-ultimate"},"outputs":[{"name":"stdout","output_type":"stream","text":["========== fold: 0 training ==========\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83267a4bd06940c3a06b4d61425c4490","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/533M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/1424] Elapsed 0m 0s (remain 15m 56s) Loss: 0.7735(0.7735) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/1424] Elapsed 0m 18s (remain 4m 1s) Loss: 0.1150(0.3785) Grad: 1952.8127  LR: 0.000003  \n","Epoch: [1][200/1424] Elapsed 0m 36s (remain 3m 39s) Loss: 0.0299(0.2216) Grad: 3285.1736  LR: 0.000006  \n","Epoch: [1][300/1424] Elapsed 0m 53s (remain 3m 20s) Loss: 0.0095(0.1617) Grad: 1272.7969  LR: 0.000008  \n","Epoch: [1][400/1424] Elapsed 1m 11s (remain 3m 2s) Loss: 0.0331(0.1298) Grad: 2999.4971  LR: 0.000011  \n","Epoch: [1][500/1424] Elapsed 1m 29s (remain 2m 44s) Loss: 0.0421(0.1097) Grad: 7896.0752  LR: 0.000014  \n","Epoch: [1][600/1424] Elapsed 1m 47s (remain 2m 26s) Loss: 0.0326(0.0954) Grad: 4370.9561  LR: 0.000017  \n","Epoch: [1][700/1424] Elapsed 2m 4s (remain 2m 8s) Loss: 0.0302(0.0852) Grad: 4100.8350  LR: 0.000020  \n","Epoch: [1][800/1424] Elapsed 2m 22s (remain 1m 50s) Loss: 0.0090(0.0774) Grad: 1856.0403  LR: 0.000020  \n","Epoch: [1][900/1424] Elapsed 2m 40s (remain 1m 32s) Loss: 0.0249(0.0713) Grad: 2390.4136  LR: 0.000019  \n","Epoch: [1][1000/1424] Elapsed 2m 57s (remain 1m 15s) Loss: 0.0470(0.0661) Grad: 5188.8433  LR: 0.000019  \n","Epoch: [1][1100/1424] Elapsed 3m 15s (remain 0m 57s) Loss: 0.0189(0.0620) Grad: 2664.1450  LR: 0.000019  \n","Epoch: [1][1200/1424] Elapsed 3m 33s (remain 0m 39s) Loss: 0.0219(0.0584) Grad: 5693.0117  LR: 0.000018  \n","Epoch: [1][1300/1424] Elapsed 3m 50s (remain 0m 21s) Loss: 0.0078(0.0552) Grad: 1776.6104  LR: 0.000018  \n","Epoch: [1][1400/1424] Elapsed 4m 8s (remain 0m 4s) Loss: 0.0048(0.0526) Grad: 1404.7909  LR: 0.000018  \n","Epoch: [1][1423/1424] Elapsed 4m 12s (remain 0m 0s) Loss: 0.0206(0.0521) Grad: 2184.6284  LR: 0.000018  \n","EVAL: [0/363] Elapsed 0m 0s (remain 1m 43s) Loss: 0.0099(0.0099) \n","EVAL: [100/363] Elapsed 0m 4s (remain 0m 12s) Loss: 0.0224(0.0139) \n","EVAL: [200/363] Elapsed 0m 8s (remain 0m 7s) Loss: 0.0167(0.0160) \n","EVAL: [300/363] Elapsed 0m 13s (remain 0m 2s) Loss: 0.0103(0.0154) \n","EVAL: [362/363] Elapsed 0m 16s (remain 0m 0s) Loss: 0.0031(0.0142) \n","Epoch 1 - avg_train_loss: 0.0521  avg_val_loss: 0.0142  time: 273s\n","Epoch 1 - Score: 0.8311\n","Epoch 1 - Save Best Score: 0.8311 Model\n","Epoch: [2][0/1424] Elapsed 0m 0s (remain 10m 35s) Loss: 0.0229(0.0229) Grad: 31192.1973  LR: 0.000018  \n","Epoch: [2][100/1424] Elapsed 0m 18s (remain 3m 58s) Loss: 0.0168(0.0141) Grad: 19811.4238  LR: 0.000017  \n","Epoch: [2][200/1424] Elapsed 0m 35s (remain 3m 38s) Loss: 0.0080(0.0134) Grad: 20304.6816  LR: 0.000017  \n","Epoch: [2][300/1424] Elapsed 0m 53s (remain 3m 20s) Loss: 0.0092(0.0128) Grad: 28629.5918  LR: 0.000017  \n","Epoch: [2][400/1424] Elapsed 1m 11s (remain 3m 1s) Loss: 0.0042(0.0122) Grad: 11211.4912  LR: 0.000017  \n","Epoch: [2][500/1424] Elapsed 1m 29s (remain 2m 44s) Loss: 0.0164(0.0125) Grad: 30664.2852  LR: 0.000016  \n","Epoch: [2][600/1424] Elapsed 1m 46s (remain 2m 26s) Loss: 0.0071(0.0124) Grad: 29300.5391  LR: 0.000016  \n","Epoch: [2][700/1424] Elapsed 2m 4s (remain 2m 8s) Loss: 0.0113(0.0124) Grad: 16301.3496  LR: 0.000016  \n","Epoch: [2][800/1424] Elapsed 2m 22s (remain 1m 50s) Loss: 0.0254(0.0122) Grad: 53857.3555  LR: 0.000015  \n","Epoch: [2][900/1424] Elapsed 2m 39s (remain 1m 32s) Loss: 0.0059(0.0122) Grad: 10551.6758  LR: 0.000015  \n","Epoch: [2][1000/1424] Elapsed 2m 57s (remain 1m 15s) Loss: 0.0064(0.0122) Grad: 9727.1758  LR: 0.000015  \n","Epoch: [2][1100/1424] Elapsed 3m 15s (remain 0m 57s) Loss: 0.0158(0.0124) Grad: 20047.7285  LR: 0.000014  \n","Epoch: [2][1200/1424] Elapsed 3m 33s (remain 0m 39s) Loss: 0.0123(0.0123) Grad: 34609.0391  LR: 0.000014  \n","Epoch: [2][1300/1424] Elapsed 3m 50s (remain 0m 21s) Loss: 0.0023(0.0122) Grad: 8038.8179  LR: 0.000014  \n","Epoch: [2][1400/1424] Elapsed 4m 8s (remain 0m 4s) Loss: 0.0013(0.0121) Grad: 5296.5317  LR: 0.000013  \n","Epoch: [2][1423/1424] Elapsed 4m 12s (remain 0m 0s) Loss: 0.0115(0.0122) Grad: 16938.5977  LR: 0.000013  \n","EVAL: [0/363] Elapsed 0m 0s (remain 1m 39s) Loss: 0.0041(0.0041) \n","EVAL: [100/363] Elapsed 0m 4s (remain 0m 12s) Loss: 0.0111(0.0124) \n","EVAL: [200/363] Elapsed 0m 8s (remain 0m 7s) Loss: 0.0096(0.0143) \n","EVAL: [300/363] Elapsed 0m 13s (remain 0m 2s) Loss: 0.0016(0.0136) \n","EVAL: [362/363] Elapsed 0m 16s (remain 0m 0s) Loss: 0.0014(0.0126) \n","Epoch 2 - avg_train_loss: 0.0122  avg_val_loss: 0.0126  time: 273s\n","Epoch 2 - Score: 0.8523\n","Epoch 2 - Save Best Score: 0.8523 Model\n","Epoch: [3][0/1424] Elapsed 0m 0s (remain 10m 52s) Loss: 0.0025(0.0025) Grad: 10587.3271  LR: 0.000013  \n","Epoch: [3][100/1424] Elapsed 0m 18s (remain 3m 59s) Loss: 0.0027(0.0075) Grad: 6113.6240  LR: 0.000013  \n","Epoch: [3][200/1424] Elapsed 0m 35s (remain 3m 38s) Loss: 0.0059(0.0082) Grad: 26562.5820  LR: 0.000013  \n","Epoch: [3][300/1424] Elapsed 0m 53s (remain 3m 20s) Loss: 0.0063(0.0087) Grad: 30021.8848  LR: 0.000012  \n","Epoch: [3][400/1424] Elapsed 1m 11s (remain 3m 2s) Loss: 0.0209(0.0089) Grad: 82957.1875  LR: 0.000012  \n","Epoch: [3][500/1424] Elapsed 1m 29s (remain 2m 44s) Loss: 0.0291(0.0094) Grad: 47282.5547  LR: 0.000012  \n","Epoch: [3][600/1424] Elapsed 1m 46s (remain 2m 26s) Loss: 0.0036(0.0095) Grad: 36899.2383  LR: 0.000011  \n","Epoch: [3][700/1424] Elapsed 2m 4s (remain 2m 8s) Loss: 0.0138(0.0093) Grad: 25939.7988  LR: 0.000011  \n","Epoch: [3][800/1424] Elapsed 2m 22s (remain 1m 50s) Loss: 0.0056(0.0093) Grad: 21255.2715  LR: 0.000011  \n","Epoch: [3][900/1424] Elapsed 2m 39s (remain 1m 32s) Loss: 0.0097(0.0095) Grad: 25803.7188  LR: 0.000011  \n","Epoch: [3][1000/1424] Elapsed 2m 57s (remain 1m 15s) Loss: 0.0062(0.0094) Grad: 29729.4824  LR: 0.000010  \n","Epoch: [3][1100/1424] Elapsed 3m 15s (remain 0m 57s) Loss: 0.0009(0.0093) Grad: 16202.9502  LR: 0.000010  \n","Epoch: [3][1200/1424] Elapsed 3m 32s (remain 0m 39s) Loss: 0.0013(0.0094) Grad: 5985.4917  LR: 0.000010  \n","Epoch: [3][1300/1424] Elapsed 3m 50s (remain 0m 21s) Loss: 0.0134(0.0094) Grad: 86887.3906  LR: 0.000009  \n","Epoch: [3][1400/1424] Elapsed 4m 8s (remain 0m 4s) Loss: 0.0078(0.0094) Grad: 48368.0898  LR: 0.000009  \n","Epoch: [3][1423/1424] Elapsed 4m 12s (remain 0m 0s) Loss: 0.0167(0.0094) Grad: 31508.3477  LR: 0.000009  \n","EVAL: [0/363] Elapsed 0m 0s (remain 1m 37s) Loss: 0.0025(0.0025) \n","EVAL: [100/363] Elapsed 0m 4s (remain 0m 11s) Loss: 0.0112(0.0126) \n","EVAL: [200/363] Elapsed 0m 8s (remain 0m 7s) Loss: 0.0045(0.0147) \n","EVAL: [300/363] Elapsed 0m 13s (remain 0m 2s) Loss: 0.0016(0.0141) \n","EVAL: [362/363] Elapsed 0m 16s (remain 0m 0s) Loss: 0.0030(0.0129) \n","Epoch 3 - avg_train_loss: 0.0094  avg_val_loss: 0.0129  time: 273s\n","Epoch 3 - Score: 0.8621\n","Epoch 3 - Save Best Score: 0.8621 Model\n","Epoch: [4][0/1424] Elapsed 0m 0s (remain 10m 30s) Loss: 0.0106(0.0106) Grad: 77771.5000  LR: 0.000009  \n","Epoch: [4][100/1424] Elapsed 0m 18s (remain 3m 58s) Loss: 0.0060(0.0065) Grad: 22186.8555  LR: 0.000009  \n","Epoch: [4][200/1424] Elapsed 0m 35s (remain 3m 38s) Loss: 0.0007(0.0075) Grad: 2691.5364  LR: 0.000008  \n","Epoch: [4][300/1424] Elapsed 0m 53s (remain 3m 20s) Loss: 0.0002(0.0074) Grad: 1782.3280  LR: 0.000008  \n","Epoch: [4][400/1424] Elapsed 1m 11s (remain 3m 2s) Loss: 0.0090(0.0074) Grad: 44809.2188  LR: 0.000008  \n","Epoch: [4][500/1424] Elapsed 1m 29s (remain 2m 44s) Loss: 0.0053(0.0077) Grad: 24070.3086  LR: 0.000007  \n","Epoch: [4][600/1424] Elapsed 1m 46s (remain 2m 26s) Loss: 0.0021(0.0078) Grad: 20806.0684  LR: 0.000007  \n","Epoch: [4][700/1424] Elapsed 2m 4s (remain 2m 8s) Loss: 0.0147(0.0079) Grad: 35700.7227  LR: 0.000007  \n","Epoch: [4][800/1424] Elapsed 2m 22s (remain 1m 50s) Loss: 0.0044(0.0079) Grad: 15548.4043  LR: 0.000006  \n","Epoch: [4][900/1424] Elapsed 2m 40s (remain 1m 32s) Loss: 0.0031(0.0078) Grad: 33336.9688  LR: 0.000006  \n","Epoch: [4][1000/1424] Elapsed 2m 57s (remain 1m 15s) Loss: 0.0033(0.0078) Grad: 10784.7617  LR: 0.000006  \n","Epoch: [4][1100/1424] Elapsed 3m 15s (remain 0m 57s) Loss: 0.0008(0.0078) Grad: 6503.6250  LR: 0.000005  \n","Epoch: [4][1200/1424] Elapsed 3m 33s (remain 0m 39s) Loss: 0.0021(0.0077) Grad: 10699.1943  LR: 0.000005  \n","Epoch: [4][1300/1424] Elapsed 3m 50s (remain 0m 21s) Loss: 0.0007(0.0076) Grad: 5260.8066  LR: 0.000005  \n","Epoch: [4][1400/1424] Elapsed 4m 8s (remain 0m 4s) Loss: 0.0046(0.0075) Grad: 15453.0752  LR: 0.000005  \n","Epoch: [4][1423/1424] Elapsed 4m 12s (remain 0m 0s) Loss: 0.0014(0.0075) Grad: 16196.3057  LR: 0.000004  \n","EVAL: [0/363] Elapsed 0m 0s (remain 1m 35s) Loss: 0.0104(0.0104) \n","EVAL: [100/363] Elapsed 0m 4s (remain 0m 11s) Loss: 0.0108(0.0143) \n","EVAL: [200/363] Elapsed 0m 8s (remain 0m 7s) Loss: 0.0023(0.0166) \n","EVAL: [300/363] Elapsed 0m 13s (remain 0m 2s) Loss: 0.0008(0.0156) \n","EVAL: [362/363] Elapsed 0m 16s (remain 0m 0s) Loss: 0.0016(0.0143) \n","Epoch 4 - avg_train_loss: 0.0075  avg_val_loss: 0.0143  time: 273s\n","Epoch 4 - Score: 0.8671\n","Epoch 4 - Save Best Score: 0.8671 Model\n","Epoch: [5][0/1424] Elapsed 0m 0s (remain 10m 23s) Loss: 0.0150(0.0150) Grad: 17710.8789  LR: 0.000004  \n","Epoch: [5][100/1424] Elapsed 0m 18s (remain 3m 58s) Loss: 0.0158(0.0056) Grad: 46811.6133  LR: 0.000004  \n","Epoch: [5][200/1424] Elapsed 0m 35s (remain 3m 38s) Loss: 0.0131(0.0059) Grad: 18315.6777  LR: 0.000004  \n","Epoch: [5][300/1424] Elapsed 0m 53s (remain 3m 20s) Loss: 0.0020(0.0063) Grad: 14914.8047  LR: 0.000004  \n","Epoch: [5][400/1424] Elapsed 1m 11s (remain 3m 2s) Loss: 0.0001(0.0066) Grad: 1247.3448  LR: 0.000003  \n","Epoch: [5][500/1424] Elapsed 1m 29s (remain 2m 44s) Loss: 0.0004(0.0064) Grad: 4962.4888  LR: 0.000003  \n","Epoch: [5][600/1424] Elapsed 1m 46s (remain 2m 26s) Loss: 0.0055(0.0063) Grad: 21723.6523  LR: 0.000003  \n","Epoch: [5][700/1424] Elapsed 2m 4s (remain 2m 8s) Loss: 0.0033(0.0062) Grad: 29014.7871  LR: 0.000002  \n","Epoch: [5][800/1424] Elapsed 2m 22s (remain 1m 50s) Loss: 0.0110(0.0062) Grad: 32499.4570  LR: 0.000002  \n","Epoch: [5][900/1424] Elapsed 2m 39s (remain 1m 32s) Loss: 0.0010(0.0062) Grad: 11665.4170  LR: 0.000002  \n","Epoch: [5][1000/1424] Elapsed 2m 57s (remain 1m 15s) Loss: 0.0019(0.0062) Grad: 20370.4844  LR: 0.000001  \n","Epoch: [5][1100/1424] Elapsed 3m 15s (remain 0m 57s) Loss: 0.0050(0.0061) Grad: 22464.9668  LR: 0.000001  \n","Epoch: [5][1200/1424] Elapsed 3m 33s (remain 0m 39s) Loss: 0.0033(0.0061) Grad: 21991.6699  LR: 0.000001  \n","Epoch: [5][1300/1424] Elapsed 3m 50s (remain 0m 21s) Loss: 0.0069(0.0061) Grad: 26581.2656  LR: 0.000000  \n","Epoch: [5][1400/1424] Elapsed 4m 8s (remain 0m 4s) Loss: 0.0093(0.0060) Grad: 19789.0918  LR: 0.000000  \n","Epoch: [5][1423/1424] Elapsed 4m 12s (remain 0m 0s) Loss: 0.0021(0.0061) Grad: 12435.7334  LR: 0.000000  \n","EVAL: [0/363] Elapsed 0m 0s (remain 1m 35s) Loss: 0.0050(0.0050) \n","EVAL: [100/363] Elapsed 0m 4s (remain 0m 11s) Loss: 0.0123(0.0150) \n","EVAL: [200/363] Elapsed 0m 8s (remain 0m 7s) Loss: 0.0023(0.0174) \n","EVAL: [300/363] Elapsed 0m 13s (remain 0m 2s) Loss: 0.0011(0.0163) \n","EVAL: [362/363] Elapsed 0m 16s (remain 0m 0s) Loss: 0.0013(0.0150) \n","Epoch 5 - avg_train_loss: 0.0061  avg_val_loss: 0.0150  time: 273s\n","Epoch 5 - Score: 0.8687\n","Epoch 5 - Save Best Score: 0.8687 Model\n","========== fold: 1 training ==========\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/1425] Elapsed 0m 0s (remain 9m 59s) Loss: 0.3876(0.3876) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/1425] Elapsed 0m 18s (remain 3m 58s) Loss: 0.1365(0.2115) Grad: 5167.9600  LR: 0.000003  \n","Epoch: [1][200/1425] Elapsed 0m 35s (remain 3m 38s) Loss: 0.0961(0.1378) Grad: 9342.2803  LR: 0.000006  \n","Epoch: [1][300/1425] Elapsed 0m 53s (remain 3m 20s) Loss: 0.0137(0.1052) Grad: 4183.5142  LR: 0.000008  \n","Epoch: [1][400/1425] Elapsed 1m 11s (remain 3m 2s) Loss: 0.0323(0.0886) Grad: 4946.7798  LR: 0.000011  \n","Epoch: [1][500/1425] Elapsed 1m 29s (remain 2m 44s) Loss: 0.0126(0.0766) Grad: 3647.0781  LR: 0.000014  \n","Epoch: [1][600/1425] Elapsed 1m 46s (remain 2m 26s) Loss: 0.0288(0.0681) Grad: 11679.2402  LR: 0.000017  \n","Epoch: [1][700/1425] Elapsed 2m 4s (remain 2m 8s) Loss: 0.0087(0.0622) Grad: 2100.1680  LR: 0.000020  \n","Epoch: [1][800/1425] Elapsed 2m 22s (remain 1m 50s) Loss: 0.0162(0.0573) Grad: 4123.7402  LR: 0.000020  \n","Epoch: [1][900/1425] Elapsed 2m 40s (remain 1m 33s) Loss: 0.0257(0.0534) Grad: 8842.1641  LR: 0.000019  \n","Epoch: [1][1000/1425] Elapsed 2m 57s (remain 1m 15s) Loss: 0.0655(0.0498) Grad: 20487.8848  LR: 0.000019  \n","Epoch: [1][1100/1425] Elapsed 3m 15s (remain 0m 57s) Loss: 0.0068(0.0474) Grad: 2370.4062  LR: 0.000019  \n","Epoch: [1][1200/1425] Elapsed 3m 33s (remain 0m 39s) Loss: 0.0162(0.0451) Grad: 5218.2598  LR: 0.000018  \n","Epoch: [1][1300/1425] Elapsed 3m 51s (remain 0m 22s) Loss: 0.0210(0.0430) Grad: 7391.9429  LR: 0.000018  \n","Epoch: [1][1400/1425] Elapsed 4m 8s (remain 0m 4s) Loss: 0.0087(0.0412) Grad: 3081.9475  LR: 0.000018  \n","Epoch: [1][1424/1425] Elapsed 4m 13s (remain 0m 0s) Loss: 0.0152(0.0408) Grad: 4205.0488  LR: 0.000018  \n","EVAL: [0/362] Elapsed 0m 0s (remain 1m 40s) Loss: 0.0053(0.0053) \n","EVAL: [100/362] Elapsed 0m 4s (remain 0m 11s) Loss: 0.0052(0.0160) \n","EVAL: [200/362] Elapsed 0m 8s (remain 0m 7s) Loss: 0.0468(0.0160) \n","EVAL: [300/362] Elapsed 0m 13s (remain 0m 2s) Loss: 0.0105(0.0158) \n","EVAL: [361/362] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0071(0.0146) \n","Epoch 1 - avg_train_loss: 0.0408  avg_val_loss: 0.0146  time: 273s\n","Epoch 1 - Score: 0.8247\n","Epoch 1 - Save Best Score: 0.8247 Model\n","Epoch: [2][0/1425] Elapsed 0m 0s (remain 10m 22s) Loss: 0.0105(0.0105) Grad: 19342.0195  LR: 0.000018  \n","Epoch: [2][100/1425] Elapsed 0m 18s (remain 3m 59s) Loss: 0.0092(0.0145) Grad: 19646.0645  LR: 0.000017  \n","Epoch: [2][200/1425] Elapsed 0m 35s (remain 3m 39s) Loss: 0.0088(0.0129) Grad: 28825.2578  LR: 0.000017  \n","Epoch: [2][300/1425] Elapsed 0m 53s (remain 3m 20s) Loss: 0.0076(0.0126) Grad: 38248.3672  LR: 0.000017  \n","Epoch: [2][400/1425] Elapsed 1m 11s (remain 3m 2s) Loss: 0.0105(0.0127) Grad: 25261.4141  LR: 0.000017  \n","Epoch: [2][500/1425] Elapsed 1m 29s (remain 2m 44s) Loss: 0.0329(0.0132) Grad: 63707.2305  LR: 0.000016  \n","Epoch: [2][600/1425] Elapsed 1m 46s (remain 2m 26s) Loss: 0.0049(0.0134) Grad: 13637.4990  LR: 0.000016  \n","Epoch: [2][700/1425] Elapsed 2m 4s (remain 2m 8s) Loss: 0.0029(0.0132) Grad: 14817.4121  LR: 0.000016  \n","Epoch: [2][800/1425] Elapsed 2m 22s (remain 1m 50s) Loss: 0.0090(0.0130) Grad: 24300.3711  LR: 0.000015  \n","Epoch: [2][900/1425] Elapsed 2m 40s (remain 1m 33s) Loss: 0.0728(0.0131) Grad: 181982.8438  LR: 0.000015  \n","Epoch: [2][1000/1425] Elapsed 2m 57s (remain 1m 15s) Loss: 0.0208(0.0131) Grad: 35029.5039  LR: 0.000015  \n","Epoch: [2][1100/1425] Elapsed 3m 15s (remain 0m 57s) Loss: 0.0098(0.0129) Grad: 50390.7227  LR: 0.000014  \n","Epoch: [2][1200/1425] Elapsed 3m 33s (remain 0m 39s) Loss: 0.0023(0.0128) Grad: 7190.5278  LR: 0.000014  \n","Epoch: [2][1300/1425] Elapsed 3m 51s (remain 0m 22s) Loss: 0.0035(0.0127) Grad: 16639.9434  LR: 0.000014  \n","Epoch: [2][1400/1425] Elapsed 4m 8s (remain 0m 4s) Loss: 0.0079(0.0126) Grad: 13433.0957  LR: 0.000013  \n","Epoch: [2][1424/1425] Elapsed 4m 13s (remain 0m 0s) Loss: 0.0189(0.0126) Grad: 61059.8984  LR: 0.000013  \n","EVAL: [0/362] Elapsed 0m 0s (remain 1m 38s) Loss: 0.0041(0.0041) \n","EVAL: [100/362] Elapsed 0m 4s (remain 0m 11s) Loss: 0.0033(0.0141) \n","EVAL: [200/362] Elapsed 0m 8s (remain 0m 7s) Loss: 0.0274(0.0143) \n","EVAL: [300/362] Elapsed 0m 13s (remain 0m 2s) Loss: 0.0082(0.0148) \n","EVAL: [361/362] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0063(0.0136) \n","Epoch 2 - avg_train_loss: 0.0126  avg_val_loss: 0.0136  time: 273s\n","Epoch 2 - Score: 0.8456\n","Epoch 2 - Save Best Score: 0.8456 Model\n","Epoch: [3][0/1425] Elapsed 0m 0s (remain 10m 39s) Loss: 0.0029(0.0029) Grad: 6301.6025  LR: 0.000013  \n","Epoch: [3][100/1425] Elapsed 0m 18s (remain 4m 0s) Loss: 0.0079(0.0102) Grad: 26253.5078  LR: 0.000013  \n","Epoch: [3][200/1425] Elapsed 0m 36s (remain 3m 39s) Loss: 0.0002(0.0100) Grad: 1318.5576  LR: 0.000013  \n","Epoch: [3][300/1425] Elapsed 0m 53s (remain 3m 20s) Loss: 0.0054(0.0105) Grad: 19665.8809  LR: 0.000012  \n","Epoch: [3][400/1425] Elapsed 1m 11s (remain 3m 2s) Loss: 0.0069(0.0101) Grad: 26244.0762  LR: 0.000012  \n","Epoch: [3][500/1425] Elapsed 1m 29s (remain 2m 44s) Loss: 0.0004(0.0098) Grad: 951.5090  LR: 0.000012  \n","Epoch: [3][600/1425] Elapsed 1m 46s (remain 2m 26s) Loss: 0.0149(0.0098) Grad: 28963.0820  LR: 0.000011  \n","Epoch: [3][700/1425] Elapsed 2m 4s (remain 2m 8s) Loss: 0.0161(0.0096) Grad: 59267.0859  LR: 0.000011  \n","Epoch: [3][800/1425] Elapsed 2m 22s (remain 1m 51s) Loss: 0.0043(0.0094) Grad: 23330.6230  LR: 0.000011  \n","Epoch: [3][900/1425] Elapsed 2m 40s (remain 1m 33s) Loss: 0.0019(0.0095) Grad: 8064.4805  LR: 0.000011  \n","Epoch: [3][1000/1425] Elapsed 2m 57s (remain 1m 15s) Loss: 0.0138(0.0094) Grad: 21919.2812  LR: 0.000010  \n","Epoch: [3][1100/1425] Elapsed 3m 15s (remain 0m 57s) Loss: 0.0089(0.0095) Grad: 23897.8242  LR: 0.000010  \n","Epoch: [3][1200/1425] Elapsed 3m 33s (remain 0m 39s) Loss: 0.0020(0.0096) Grad: 8747.1289  LR: 0.000010  \n","Epoch: [3][1300/1425] Elapsed 3m 51s (remain 0m 22s) Loss: 0.0086(0.0095) Grad: 24703.7129  LR: 0.000009  \n","Epoch: [3][1400/1425] Elapsed 4m 9s (remain 0m 4s) Loss: 0.0091(0.0094) Grad: 26967.2520  LR: 0.000009  \n","Epoch: [3][1424/1425] Elapsed 4m 13s (remain 0m 0s) Loss: 0.0178(0.0094) Grad: 44309.3320  LR: 0.000009  \n","EVAL: [0/362] Elapsed 0m 0s (remain 1m 40s) Loss: 0.0027(0.0027) \n","EVAL: [100/362] Elapsed 0m 4s (remain 0m 11s) Loss: 0.0017(0.0141) \n","EVAL: [200/362] Elapsed 0m 8s (remain 0m 7s) Loss: 0.0380(0.0140) \n","EVAL: [300/362] Elapsed 0m 13s (remain 0m 2s) Loss: 0.0069(0.0142) \n","EVAL: [361/362] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0039(0.0131) \n","Epoch 3 - avg_train_loss: 0.0094  avg_val_loss: 0.0131  time: 273s\n","Epoch 3 - Score: 0.8548\n","Epoch 3 - Save Best Score: 0.8548 Model\n","Epoch: [4][0/1425] Elapsed 0m 0s (remain 10m 40s) Loss: 0.0037(0.0037) Grad: 7191.7842  LR: 0.000009  \n","Epoch: [4][100/1425] Elapsed 0m 18s (remain 4m 0s) Loss: 0.0036(0.0057) Grad: 12011.4922  LR: 0.000009  \n","Epoch: [4][200/1425] Elapsed 0m 36s (remain 3m 39s) Loss: 0.0007(0.0064) Grad: 2745.5676  LR: 0.000008  \n","Epoch: [4][300/1425] Elapsed 0m 53s (remain 3m 21s) Loss: 0.0000(0.0067) Grad: 181.6205  LR: 0.000008  \n","Epoch: [4][400/1425] Elapsed 1m 11s (remain 3m 2s) Loss: 0.0005(0.0068) Grad: 15631.3379  LR: 0.000008  \n","Epoch: [4][500/1425] Elapsed 1m 29s (remain 2m 44s) Loss: 0.0010(0.0068) Grad: 5155.2017  LR: 0.000007  \n","Epoch: [4][600/1425] Elapsed 1m 46s (remain 2m 26s) Loss: 0.0032(0.0069) Grad: 15418.9404  LR: 0.000007  \n","Epoch: [4][700/1425] Elapsed 2m 4s (remain 2m 8s) Loss: 0.0001(0.0072) Grad: 526.5664  LR: 0.000007  \n","Epoch: [4][800/1425] Elapsed 2m 22s (remain 1m 50s) Loss: 0.0023(0.0074) Grad: 10720.8037  LR: 0.000006  \n","Epoch: [4][900/1425] Elapsed 2m 40s (remain 1m 33s) Loss: 0.0303(0.0074) Grad: 111641.0000  LR: 0.000006  \n","Epoch: [4][1000/1425] Elapsed 2m 57s (remain 1m 15s) Loss: 0.0038(0.0076) Grad: 55342.0039  LR: 0.000006  \n","Epoch: [4][1100/1425] Elapsed 3m 15s (remain 0m 57s) Loss: 0.0154(0.0076) Grad: 34823.1250  LR: 0.000005  \n","Epoch: [4][1200/1425] Elapsed 3m 33s (remain 0m 39s) Loss: 0.0178(0.0075) Grad: 62742.2383  LR: 0.000005  \n","Epoch: [4][1300/1425] Elapsed 3m 51s (remain 0m 22s) Loss: 0.0028(0.0076) Grad: 8489.5781  LR: 0.000005  \n","Epoch: [4][1400/1425] Elapsed 4m 8s (remain 0m 4s) Loss: 0.0084(0.0076) Grad: 27281.3633  LR: 0.000005  \n","Epoch: [4][1424/1425] Elapsed 4m 13s (remain 0m 0s) Loss: 0.0007(0.0076) Grad: 7111.8398  LR: 0.000004  \n","EVAL: [0/362] Elapsed 0m 0s (remain 1m 41s) Loss: 0.0032(0.0032) \n","EVAL: [100/362] Elapsed 0m 4s (remain 0m 11s) Loss: 0.0005(0.0149) \n","EVAL: [200/362] Elapsed 0m 8s (remain 0m 7s) Loss: 0.0521(0.0151) \n","EVAL: [300/362] Elapsed 0m 13s (remain 0m 2s) Loss: 0.0025(0.0155) \n","EVAL: [361/362] Elapsed 0m 16s (remain 0m 0s) Loss: 0.0057(0.0142) \n","Epoch 4 - avg_train_loss: 0.0076  avg_val_loss: 0.0142  time: 273s\n","Epoch 4 - Score: 0.8693\n","Epoch 4 - Save Best Score: 0.8693 Model\n","Epoch: [5][0/1425] Elapsed 0m 0s (remain 10m 26s) Loss: 0.0171(0.0171) Grad: 86088.5078  LR: 0.000004  \n","Epoch: [5][100/1425] Elapsed 0m 18s (remain 3m 59s) Loss: 0.0072(0.0064) Grad: 61764.0547  LR: 0.000004  \n","Epoch: [5][200/1425] Elapsed 0m 35s (remain 3m 39s) Loss: 0.0200(0.0059) Grad: 159904.9219  LR: 0.000004  \n","Epoch: [5][300/1425] Elapsed 0m 53s (remain 3m 20s) Loss: 0.0131(0.0063) Grad: 203465.6406  LR: 0.000004  \n","Epoch: [5][400/1425] Elapsed 1m 11s (remain 3m 2s) Loss: 0.0029(0.0063) Grad: 8292.8701  LR: 0.000003  \n","Epoch: [5][500/1425] Elapsed 1m 29s (remain 2m 44s) Loss: 0.0081(0.0063) Grad: 10289.8252  LR: 0.000003  \n","Epoch: [5][600/1425] Elapsed 1m 46s (remain 2m 26s) Loss: 0.0121(0.0062) Grad: 24625.1074  LR: 0.000003  \n","Epoch: [5][700/1425] Elapsed 2m 4s (remain 2m 8s) Loss: 0.0038(0.0062) Grad: 10841.8799  LR: 0.000002  \n","Epoch: [5][800/1425] Elapsed 2m 22s (remain 1m 50s) Loss: 0.0001(0.0060) Grad: 539.9357  LR: 0.000002  \n","Epoch: [5][900/1425] Elapsed 2m 40s (remain 1m 33s) Loss: 0.0079(0.0060) Grad: 40447.0391  LR: 0.000002  \n","Epoch: [5][1000/1425] Elapsed 2m 57s (remain 1m 15s) Loss: 0.0031(0.0059) Grad: 28208.3750  LR: 0.000001  \n","Epoch: [5][1100/1425] Elapsed 3m 15s (remain 0m 57s) Loss: 0.0009(0.0059) Grad: 10396.4180  LR: 0.000001  \n","Epoch: [5][1200/1425] Elapsed 3m 33s (remain 0m 39s) Loss: 0.0160(0.0060) Grad: 36301.1992  LR: 0.000001  \n","Epoch: [5][1300/1425] Elapsed 3m 51s (remain 0m 22s) Loss: 0.0182(0.0060) Grad: 38874.3516  LR: 0.000000  \n","Epoch: [5][1400/1425] Elapsed 4m 8s (remain 0m 4s) Loss: 0.0023(0.0059) Grad: 19721.0371  LR: 0.000000  \n","Epoch: [5][1424/1425] Elapsed 4m 13s (remain 0m 0s) Loss: 0.0088(0.0059) Grad: 17345.7207  LR: 0.000000  \n","EVAL: [0/362] Elapsed 0m 0s (remain 1m 40s) Loss: 0.0033(0.0033) \n","EVAL: [100/362] Elapsed 0m 4s (remain 0m 11s) Loss: 0.0002(0.0158) \n","EVAL: [200/362] Elapsed 0m 8s (remain 0m 7s) Loss: 0.0506(0.0161) \n","EVAL: [300/362] Elapsed 0m 13s (remain 0m 2s) Loss: 0.0070(0.0166) \n","EVAL: [361/362] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0066(0.0153) \n","Epoch 5 - avg_train_loss: 0.0059  avg_val_loss: 0.0153  time: 273s\n","Epoch 5 - Score: 0.8696\n","Epoch 5 - Save Best Score: 0.8696 Model\n","========== fold: 2 training ==========\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/1435] Elapsed 0m 0s (remain 10m 34s) Loss: 1.2502(1.2502) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/1435] Elapsed 0m 18s (remain 4m 0s) Loss: 0.0325(0.6277) Grad: 557.1497  LR: 0.000003  \n","Epoch: [1][200/1435] Elapsed 0m 35s (remain 3m 40s) Loss: 0.0201(0.3456) Grad: 1861.0237  LR: 0.000006  \n","Epoch: [1][300/1435] Elapsed 0m 53s (remain 3m 22s) Loss: 0.0284(0.2435) Grad: 2344.4683  LR: 0.000008  \n","Epoch: [1][400/1435] Elapsed 1m 11s (remain 3m 4s) Loss: 0.0088(0.1907) Grad: 1186.4371  LR: 0.000011  \n","Epoch: [1][500/1435] Elapsed 1m 29s (remain 2m 46s) Loss: 0.0223(0.1586) Grad: 2178.8765  LR: 0.000014  \n","Epoch: [1][600/1435] Elapsed 1m 46s (remain 2m 28s) Loss: 0.0257(0.1365) Grad: 2769.3625  LR: 0.000017  \n","Epoch: [1][700/1435] Elapsed 2m 4s (remain 2m 10s) Loss: 0.0509(0.1208) Grad: 4474.9648  LR: 0.000020  \n","Epoch: [1][800/1435] Elapsed 2m 22s (remain 1m 52s) Loss: 0.0215(0.1085) Grad: 1405.5791  LR: 0.000020  \n","Epoch: [1][900/1435] Elapsed 2m 40s (remain 1m 34s) Loss: 0.0048(0.0989) Grad: 1003.7446  LR: 0.000019  \n","Epoch: [1][1000/1435] Elapsed 2m 57s (remain 1m 17s) Loss: 0.0243(0.0911) Grad: 3130.9854  LR: 0.000019  \n","Epoch: [1][1100/1435] Elapsed 3m 15s (remain 0m 59s) Loss: 0.0089(0.0844) Grad: 2838.2454  LR: 0.000019  \n","Epoch: [1][1200/1435] Elapsed 3m 33s (remain 0m 41s) Loss: 0.0078(0.0788) Grad: 1033.3925  LR: 0.000019  \n","Epoch: [1][1300/1435] Elapsed 3m 51s (remain 0m 23s) Loss: 0.0417(0.0741) Grad: 5006.8965  LR: 0.000018  \n","Epoch: [1][1400/1435] Elapsed 4m 8s (remain 0m 6s) Loss: 0.0066(0.0700) Grad: 1407.5017  LR: 0.000018  \n","Epoch: [1][1434/1435] Elapsed 4m 14s (remain 0m 0s) Loss: 0.0507(0.0688) Grad: 3415.5027  LR: 0.000018  \n","EVAL: [0/352] Elapsed 0m 0s (remain 1m 38s) Loss: 0.0414(0.0414) \n","EVAL: [100/352] Elapsed 0m 4s (remain 0m 11s) Loss: 0.0171(0.0168) \n","EVAL: [200/352] Elapsed 0m 8s (remain 0m 6s) Loss: 0.0264(0.0188) \n","EVAL: [300/352] Elapsed 0m 13s (remain 0m 2s) Loss: 0.0109(0.0198) \n","EVAL: [351/352] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0074(0.0189) \n","Epoch 1 - avg_train_loss: 0.0688  avg_val_loss: 0.0189  time: 275s\n","Epoch 1 - Score: 0.7854\n","Epoch 1 - Save Best Score: 0.7854 Model\n","Epoch: [2][0/1435] Elapsed 0m 0s (remain 10m 33s) Loss: 0.0067(0.0067) Grad: 17881.1641  LR: 0.000018  \n","Epoch: [2][100/1435] Elapsed 0m 18s (remain 4m 1s) Loss: 0.0112(0.0131) Grad: 46754.6445  LR: 0.000017  \n","Epoch: [2][200/1435] Elapsed 0m 35s (remain 3m 40s) Loss: 0.0108(0.0122) Grad: 26737.8906  LR: 0.000017  \n","Epoch: [2][300/1435] Elapsed 0m 53s (remain 3m 22s) Loss: 0.0037(0.0130) Grad: 7563.0562  LR: 0.000017  \n","Epoch: [2][400/1435] Elapsed 1m 11s (remain 3m 4s) Loss: 0.0300(0.0131) Grad: 82656.4297  LR: 0.000017  \n","Epoch: [2][500/1435] Elapsed 1m 29s (remain 2m 46s) Loss: 0.0033(0.0132) Grad: 10901.1924  LR: 0.000016  \n","Epoch: [2][600/1435] Elapsed 1m 46s (remain 2m 28s) Loss: 0.0135(0.0132) Grad: 33278.1602  LR: 0.000016  \n","Epoch: [2][700/1435] Elapsed 2m 4s (remain 2m 10s) Loss: 0.0051(0.0129) Grad: 27549.5879  LR: 0.000016  \n","Epoch: [2][800/1435] Elapsed 2m 22s (remain 1m 52s) Loss: 0.0026(0.0129) Grad: 5076.0176  LR: 0.000015  \n","Epoch: [2][900/1435] Elapsed 2m 40s (remain 1m 34s) Loss: 0.0107(0.0127) Grad: 59314.9648  LR: 0.000015  \n","Epoch: [2][1000/1435] Elapsed 2m 57s (remain 1m 17s) Loss: 0.0037(0.0129) Grad: 13236.9678  LR: 0.000015  \n","Epoch: [2][1100/1435] Elapsed 3m 15s (remain 0m 59s) Loss: 0.0024(0.0126) Grad: 6948.0732  LR: 0.000014  \n","Epoch: [2][1200/1435] Elapsed 3m 33s (remain 0m 41s) Loss: 0.0055(0.0126) Grad: 9070.0801  LR: 0.000014  \n","Epoch: [2][1300/1435] Elapsed 3m 51s (remain 0m 23s) Loss: 0.0103(0.0126) Grad: 16335.9092  LR: 0.000014  \n","Epoch: [2][1400/1435] Elapsed 4m 8s (remain 0m 6s) Loss: 0.0057(0.0125) Grad: 13205.9521  LR: 0.000013  \n","Epoch: [2][1434/1435] Elapsed 4m 14s (remain 0m 0s) Loss: 0.0065(0.0125) Grad: 13614.8428  LR: 0.000013  \n","EVAL: [0/352] Elapsed 0m 0s (remain 1m 40s) Loss: 0.0307(0.0307) \n","EVAL: [100/352] Elapsed 0m 4s (remain 0m 11s) Loss: 0.0104(0.0114) \n","EVAL: [200/352] Elapsed 0m 8s (remain 0m 6s) Loss: 0.0363(0.0130) \n","EVAL: [300/352] Elapsed 0m 13s (remain 0m 2s) Loss: 0.0014(0.0139) \n","EVAL: [351/352] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0041(0.0131) \n","Epoch 2 - avg_train_loss: 0.0125  avg_val_loss: 0.0131  time: 274s\n","Epoch 2 - Score: 0.8363\n","Epoch 2 - Save Best Score: 0.8363 Model\n","Epoch: [3][0/1435] Elapsed 0m 0s (remain 10m 35s) Loss: 0.0121(0.0121) Grad: 56710.7930  LR: 0.000013  \n","Epoch: [3][100/1435] Elapsed 0m 18s (remain 4m 1s) Loss: 0.0002(0.0080) Grad: 569.7551  LR: 0.000013  \n","Epoch: [3][200/1435] Elapsed 0m 35s (remain 3m 40s) Loss: 0.0010(0.0088) Grad: 23997.2402  LR: 0.000013  \n","Epoch: [3][300/1435] Elapsed 0m 53s (remain 3m 22s) Loss: 0.0022(0.0090) Grad: 11730.0908  LR: 0.000012  \n","Epoch: [3][400/1435] Elapsed 1m 11s (remain 3m 4s) Loss: 0.0102(0.0094) Grad: 19726.5352  LR: 0.000012  \n","Epoch: [3][500/1435] Elapsed 1m 29s (remain 2m 46s) Loss: 0.0040(0.0098) Grad: 10939.9082  LR: 0.000012  \n","Epoch: [3][600/1435] Elapsed 1m 46s (remain 2m 28s) Loss: 0.0043(0.0097) Grad: 18942.0996  LR: 0.000011  \n","Epoch: [3][700/1435] Elapsed 2m 4s (remain 2m 10s) Loss: 0.0015(0.0096) Grad: 5257.2642  LR: 0.000011  \n","Epoch: [3][800/1435] Elapsed 2m 22s (remain 1m 52s) Loss: 0.0103(0.0094) Grad: 34814.2969  LR: 0.000011  \n","Epoch: [3][900/1435] Elapsed 2m 40s (remain 1m 34s) Loss: 0.0119(0.0093) Grad: 16974.1152  LR: 0.000011  \n","Epoch: [3][1000/1435] Elapsed 2m 57s (remain 1m 17s) Loss: 0.0112(0.0094) Grad: 33954.7461  LR: 0.000010  \n","Epoch: [3][1100/1435] Elapsed 3m 15s (remain 0m 59s) Loss: 0.0148(0.0094) Grad: 43258.9922  LR: 0.000010  \n","Epoch: [3][1200/1435] Elapsed 3m 33s (remain 0m 41s) Loss: 0.0035(0.0096) Grad: 17630.3828  LR: 0.000010  \n","Epoch: [3][1300/1435] Elapsed 3m 51s (remain 0m 23s) Loss: 0.0097(0.0096) Grad: 97572.5938  LR: 0.000009  \n","Epoch: [3][1400/1435] Elapsed 4m 8s (remain 0m 6s) Loss: 0.0118(0.0096) Grad: 28034.7812  LR: 0.000009  \n","Epoch: [3][1434/1435] Elapsed 4m 14s (remain 0m 0s) Loss: 0.0062(0.0096) Grad: 115863.7969  LR: 0.000009  \n","EVAL: [0/352] Elapsed 0m 0s (remain 1m 35s) Loss: 0.0103(0.0103) \n","EVAL: [100/352] Elapsed 0m 4s (remain 0m 11s) Loss: 0.0050(0.0114) \n","EVAL: [200/352] Elapsed 0m 8s (remain 0m 6s) Loss: 0.0302(0.0132) \n","EVAL: [300/352] Elapsed 0m 13s (remain 0m 2s) Loss: 0.0008(0.0142) \n","EVAL: [351/352] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0010(0.0132) \n","Epoch 3 - avg_train_loss: 0.0096  avg_val_loss: 0.0132  time: 275s\n","Epoch 3 - Score: 0.8512\n","Epoch 3 - Save Best Score: 0.8512 Model\n","Epoch: [4][0/1435] Elapsed 0m 0s (remain 10m 19s) Loss: 0.0019(0.0019) Grad: 16378.8906  LR: 0.000009  \n","Epoch: [4][100/1435] Elapsed 0m 18s (remain 4m 0s) Loss: 0.0311(0.0079) Grad: 63768.3242  LR: 0.000009  \n","Epoch: [4][200/1435] Elapsed 0m 35s (remain 3m 40s) Loss: 0.0331(0.0077) Grad: 73981.0547  LR: 0.000008  \n","Epoch: [4][300/1435] Elapsed 0m 53s (remain 3m 22s) Loss: 0.0067(0.0078) Grad: 18416.7422  LR: 0.000008  \n","Epoch: [4][400/1435] Elapsed 1m 11s (remain 3m 4s) Loss: 0.0125(0.0078) Grad: 36258.7617  LR: 0.000008  \n","Epoch: [4][500/1435] Elapsed 1m 29s (remain 2m 46s) Loss: 0.0051(0.0077) Grad: 45946.3906  LR: 0.000007  \n","Epoch: [4][600/1435] Elapsed 1m 46s (remain 2m 28s) Loss: 0.0018(0.0076) Grad: 7805.2275  LR: 0.000007  \n","Epoch: [4][700/1435] Elapsed 2m 4s (remain 2m 10s) Loss: 0.0004(0.0074) Grad: 2457.2793  LR: 0.000007  \n","Epoch: [4][800/1435] Elapsed 2m 22s (remain 1m 52s) Loss: 0.0008(0.0074) Grad: 2715.9805  LR: 0.000006  \n","Epoch: [4][900/1435] Elapsed 2m 40s (remain 1m 34s) Loss: 0.0054(0.0073) Grad: 38304.9648  LR: 0.000006  \n","Epoch: [4][1000/1435] Elapsed 2m 57s (remain 1m 17s) Loss: 0.0017(0.0072) Grad: 35258.8477  LR: 0.000006  \n","Epoch: [4][1100/1435] Elapsed 3m 15s (remain 0m 59s) Loss: 0.0031(0.0073) Grad: 7390.6548  LR: 0.000005  \n","Epoch: [4][1200/1435] Elapsed 3m 33s (remain 0m 41s) Loss: 0.0073(0.0072) Grad: 22370.1289  LR: 0.000005  \n","Epoch: [4][1300/1435] Elapsed 3m 51s (remain 0m 23s) Loss: 0.0021(0.0074) Grad: 8632.9932  LR: 0.000005  \n","Epoch: [4][1400/1435] Elapsed 4m 8s (remain 0m 6s) Loss: 0.0283(0.0074) Grad: 76602.0078  LR: 0.000005  \n","Epoch: [4][1434/1435] Elapsed 4m 14s (remain 0m 0s) Loss: 0.0176(0.0073) Grad: 34225.7734  LR: 0.000004  \n","EVAL: [0/352] Elapsed 0m 0s (remain 1m 37s) Loss: 0.0147(0.0147) \n","EVAL: [100/352] Elapsed 0m 4s (remain 0m 11s) Loss: 0.0074(0.0129) \n","EVAL: [200/352] Elapsed 0m 8s (remain 0m 6s) Loss: 0.0367(0.0145) \n","EVAL: [300/352] Elapsed 0m 13s (remain 0m 2s) Loss: 0.0004(0.0159) \n","EVAL: [351/352] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0006(0.0148) \n","Epoch 4 - avg_train_loss: 0.0073  avg_val_loss: 0.0148  time: 274s\n","Epoch 4 - Score: 0.8542\n","Epoch 4 - Save Best Score: 0.8542 Model\n","Epoch: [5][0/1435] Elapsed 0m 0s (remain 10m 39s) Loss: 0.0004(0.0004) Grad: 3434.6226  LR: 0.000004  \n","Epoch: [5][100/1435] Elapsed 0m 18s (remain 4m 1s) Loss: 0.0019(0.0054) Grad: 6331.3843  LR: 0.000004  \n","Epoch: [5][200/1435] Elapsed 0m 35s (remain 3m 40s) Loss: 0.0046(0.0056) Grad: 12593.2441  LR: 0.000004  \n","Epoch: [5][300/1435] Elapsed 0m 53s (remain 3m 22s) Loss: 0.0026(0.0061) Grad: 33718.3477  LR: 0.000004  \n","Epoch: [5][400/1435] Elapsed 1m 11s (remain 3m 4s) Loss: 0.0023(0.0060) Grad: 13284.4004  LR: 0.000003  \n","Epoch: [5][500/1435] Elapsed 1m 29s (remain 2m 46s) Loss: 0.0008(0.0060) Grad: 5401.8623  LR: 0.000003  \n","Epoch: [5][600/1435] Elapsed 1m 46s (remain 2m 28s) Loss: 0.0315(0.0062) Grad: 14263.8369  LR: 0.000003  \n","Epoch: [5][700/1435] Elapsed 2m 4s (remain 2m 10s) Loss: 0.0174(0.0062) Grad: 73618.5078  LR: 0.000002  \n","Epoch: [5][800/1435] Elapsed 2m 22s (remain 1m 52s) Loss: 0.0018(0.0062) Grad: 9939.4375  LR: 0.000002  \n","Epoch: [5][900/1435] Elapsed 2m 40s (remain 1m 34s) Loss: 0.0021(0.0061) Grad: 10852.4316  LR: 0.000002  \n","Epoch: [5][1000/1435] Elapsed 2m 57s (remain 1m 17s) Loss: 0.0067(0.0061) Grad: 21180.5293  LR: 0.000001  \n","Epoch: [5][1100/1435] Elapsed 3m 15s (remain 0m 59s) Loss: 0.0012(0.0060) Grad: 4122.1157  LR: 0.000001  \n","Epoch: [5][1200/1435] Elapsed 3m 33s (remain 0m 41s) Loss: 0.0054(0.0060) Grad: 41625.1875  LR: 0.000001  \n","Epoch: [5][1300/1435] Elapsed 3m 51s (remain 0m 23s) Loss: 0.0008(0.0060) Grad: 7455.4438  LR: 0.000000  \n","Epoch: [5][1400/1435] Elapsed 4m 8s (remain 0m 6s) Loss: 0.0008(0.0060) Grad: 4458.8047  LR: 0.000000  \n","Epoch: [5][1434/1435] Elapsed 4m 14s (remain 0m 0s) Loss: 0.0057(0.0060) Grad: 13245.4199  LR: 0.000000  \n","EVAL: [0/352] Elapsed 0m 0s (remain 1m 36s) Loss: 0.0114(0.0114) \n","EVAL: [100/352] Elapsed 0m 4s (remain 0m 11s) Loss: 0.0118(0.0133) \n","EVAL: [200/352] Elapsed 0m 8s (remain 0m 6s) Loss: 0.0405(0.0150) \n","EVAL: [300/352] Elapsed 0m 13s (remain 0m 2s) Loss: 0.0004(0.0165) \n","EVAL: [351/352] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0003(0.0153) \n","Epoch 5 - avg_train_loss: 0.0060  avg_val_loss: 0.0153  time: 274s\n","Epoch 5 - Score: 0.8576\n","Epoch 5 - Save Best Score: 0.8576 Model\n","========== fold: 3 training ==========\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/1438] Elapsed 0m 0s (remain 10m 14s) Loss: 1.0444(1.0444) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/1438] Elapsed 0m 18s (remain 4m 0s) Loss: 0.0637(0.5206) Grad: 700.3566  LR: 0.000003  \n","Epoch: [1][200/1438] Elapsed 0m 35s (remain 3m 41s) Loss: 0.0455(0.2940) Grad: 5665.3096  LR: 0.000006  \n","Epoch: [1][300/1438] Elapsed 0m 53s (remain 3m 22s) Loss: 0.0639(0.2104) Grad: 5775.4360  LR: 0.000008  \n","Epoch: [1][400/1438] Elapsed 1m 11s (remain 3m 4s) Loss: 0.0134(0.1660) Grad: 648.7770  LR: 0.000011  \n","Epoch: [1][500/1438] Elapsed 1m 29s (remain 2m 46s) Loss: 0.0138(0.1388) Grad: 1775.4066  LR: 0.000014  \n","Epoch: [1][600/1438] Elapsed 1m 46s (remain 2m 28s) Loss: 0.0155(0.1200) Grad: 2168.3206  LR: 0.000017  \n","Epoch: [1][700/1438] Elapsed 2m 4s (remain 2m 11s) Loss: 0.0092(0.1061) Grad: 917.2572  LR: 0.000019  \n","Epoch: [1][800/1438] Elapsed 2m 22s (remain 1m 53s) Loss: 0.0115(0.0957) Grad: 3067.3542  LR: 0.000020  \n","Epoch: [1][900/1438] Elapsed 2m 40s (remain 1m 35s) Loss: 0.0307(0.0873) Grad: 2901.3269  LR: 0.000019  \n","Epoch: [1][1000/1438] Elapsed 2m 57s (remain 1m 17s) Loss: 0.0098(0.0805) Grad: 1990.9252  LR: 0.000019  \n","Epoch: [1][1100/1438] Elapsed 3m 15s (remain 0m 59s) Loss: 0.0237(0.0749) Grad: 2253.0281  LR: 0.000019  \n","Epoch: [1][1200/1438] Elapsed 3m 33s (remain 0m 42s) Loss: 0.0088(0.0701) Grad: 1034.1626  LR: 0.000019  \n","Epoch: [1][1300/1438] Elapsed 3m 51s (remain 0m 24s) Loss: 0.0218(0.0662) Grad: 1886.4835  LR: 0.000018  \n","Epoch: [1][1400/1438] Elapsed 4m 8s (remain 0m 6s) Loss: 0.0014(0.0626) Grad: 337.4112  LR: 0.000018  \n","Epoch: [1][1437/1438] Elapsed 4m 15s (remain 0m 0s) Loss: 0.0070(0.0615) Grad: 803.9194  LR: 0.000018  \n","EVAL: [0/349] Elapsed 0m 0s (remain 1m 37s) Loss: 0.0075(0.0075) \n","EVAL: [100/349] Elapsed 0m 4s (remain 0m 11s) Loss: 0.0220(0.0144) \n","EVAL: [200/349] Elapsed 0m 8s (remain 0m 6s) Loss: 0.0135(0.0155) \n","EVAL: [300/349] Elapsed 0m 13s (remain 0m 2s) Loss: 0.0101(0.0154) \n","EVAL: [348/349] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0029(0.0146) \n","Epoch 1 - avg_train_loss: 0.0615  avg_val_loss: 0.0146  time: 275s\n","Epoch 1 - Score: 0.8210\n","Epoch 1 - Save Best Score: 0.8210 Model\n","Epoch: [2][0/1438] Elapsed 0m 0s (remain 10m 43s) Loss: 0.0073(0.0073) Grad: 13790.2041  LR: 0.000018  \n","Epoch: [2][100/1438] Elapsed 0m 18s (remain 4m 1s) Loss: 0.0042(0.0140) Grad: 17149.2422  LR: 0.000017  \n","Epoch: [2][200/1438] Elapsed 0m 35s (remain 3m 41s) Loss: 0.0234(0.0142) Grad: 55956.8047  LR: 0.000017  \n","Epoch: [2][300/1438] Elapsed 0m 53s (remain 3m 22s) Loss: 0.0013(0.0136) Grad: 11804.6689  LR: 0.000017  \n","Epoch: [2][400/1438] Elapsed 1m 11s (remain 3m 4s) Loss: 0.0196(0.0138) Grad: 88445.8125  LR: 0.000017  \n","Epoch: [2][500/1438] Elapsed 1m 29s (remain 2m 46s) Loss: 0.0089(0.0137) Grad: 21304.1016  LR: 0.000016  \n","Epoch: [2][600/1438] Elapsed 1m 46s (remain 2m 28s) Loss: 0.0022(0.0135) Grad: 37959.6992  LR: 0.000016  \n","Epoch: [2][700/1438] Elapsed 2m 4s (remain 2m 11s) Loss: 0.0254(0.0137) Grad: 51095.0391  LR: 0.000016  \n","Epoch: [2][800/1438] Elapsed 2m 22s (remain 1m 53s) Loss: 0.0054(0.0136) Grad: 14217.6104  LR: 0.000015  \n","Epoch: [2][900/1438] Elapsed 2m 40s (remain 1m 35s) Loss: 0.0071(0.0133) Grad: 16243.7939  LR: 0.000015  \n","Epoch: [2][1000/1438] Elapsed 2m 57s (remain 1m 17s) Loss: 0.0102(0.0131) Grad: 31739.7832  LR: 0.000015  \n","Epoch: [2][1100/1438] Elapsed 3m 15s (remain 0m 59s) Loss: 0.0085(0.0128) Grad: 19098.0781  LR: 0.000014  \n","Epoch: [2][1200/1438] Elapsed 3m 33s (remain 0m 42s) Loss: 0.0140(0.0126) Grad: 20679.3086  LR: 0.000014  \n","Epoch: [2][1300/1438] Elapsed 3m 51s (remain 0m 24s) Loss: 0.0349(0.0127) Grad: 84161.3125  LR: 0.000014  \n","Epoch: [2][1400/1438] Elapsed 4m 8s (remain 0m 6s) Loss: 0.0063(0.0126) Grad: 13650.8799  LR: 0.000013  \n","Epoch: [2][1437/1438] Elapsed 4m 15s (remain 0m 0s) Loss: 0.0216(0.0126) Grad: 34131.7109  LR: 0.000013  \n","EVAL: [0/349] Elapsed 0m 0s (remain 1m 32s) Loss: 0.0062(0.0062) \n","EVAL: [100/349] Elapsed 0m 4s (remain 0m 11s) Loss: 0.0101(0.0108) \n","EVAL: [200/349] Elapsed 0m 8s (remain 0m 6s) Loss: 0.0220(0.0118) \n","EVAL: [300/349] Elapsed 0m 13s (remain 0m 2s) Loss: 0.0093(0.0118) \n","EVAL: [348/349] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0005(0.0112) \n","Epoch 2 - avg_train_loss: 0.0126  avg_val_loss: 0.0112  time: 275s\n","Epoch 2 - Score: 0.8593\n","Epoch 2 - Save Best Score: 0.8593 Model\n","Epoch: [3][0/1438] Elapsed 0m 0s (remain 10m 54s) Loss: 0.0106(0.0106) Grad: 20714.9824  LR: 0.000013  \n","Epoch: [3][100/1438] Elapsed 0m 18s (remain 4m 2s) Loss: 0.0093(0.0107) Grad: 23234.4082  LR: 0.000013  \n","Epoch: [3][200/1438] Elapsed 0m 36s (remain 3m 41s) Loss: 0.0082(0.0107) Grad: 24602.6738  LR: 0.000013  \n","Epoch: [3][300/1438] Elapsed 0m 53s (remain 3m 23s) Loss: 0.0127(0.0101) Grad: 67238.3359  LR: 0.000012  \n","Epoch: [3][400/1438] Elapsed 1m 11s (remain 3m 4s) Loss: 0.0629(0.0096) Grad: 95919.4922  LR: 0.000012  \n","Epoch: [3][500/1438] Elapsed 1m 29s (remain 2m 46s) Loss: 0.0518(0.0095) Grad: 87141.7188  LR: 0.000012  \n","Epoch: [3][600/1438] Elapsed 1m 46s (remain 2m 28s) Loss: 0.0193(0.0096) Grad: 125763.6016  LR: 0.000011  \n","Epoch: [3][700/1438] Elapsed 2m 4s (remain 2m 11s) Loss: 0.0204(0.0094) Grad: 42318.1523  LR: 0.000011  \n","Epoch: [3][800/1438] Elapsed 2m 22s (remain 1m 53s) Loss: 0.0190(0.0093) Grad: 40941.1953  LR: 0.000011  \n","Epoch: [3][900/1438] Elapsed 2m 40s (remain 1m 35s) Loss: 0.0209(0.0093) Grad: 35040.1758  LR: 0.000011  \n","Epoch: [3][1000/1438] Elapsed 2m 57s (remain 1m 17s) Loss: 0.0027(0.0093) Grad: 54860.6328  LR: 0.000010  \n","Epoch: [3][1100/1438] Elapsed 3m 15s (remain 0m 59s) Loss: 0.0009(0.0092) Grad: 3484.5815  LR: 0.000010  \n","Epoch: [3][1200/1438] Elapsed 3m 33s (remain 0m 42s) Loss: 0.0014(0.0093) Grad: 5017.1514  LR: 0.000010  \n","Epoch: [3][1300/1438] Elapsed 3m 51s (remain 0m 24s) Loss: 0.0029(0.0094) Grad: 7559.9180  LR: 0.000009  \n","Epoch: [3][1400/1438] Elapsed 4m 8s (remain 0m 6s) Loss: 0.0037(0.0095) Grad: 9140.3564  LR: 0.000009  \n","Epoch: [3][1437/1438] Elapsed 4m 15s (remain 0m 0s) Loss: 0.0031(0.0094) Grad: 10654.1182  LR: 0.000009  \n","EVAL: [0/349] Elapsed 0m 0s (remain 1m 34s) Loss: 0.0032(0.0032) \n","EVAL: [100/349] Elapsed 0m 4s (remain 0m 11s) Loss: 0.0098(0.0118) \n","EVAL: [200/349] Elapsed 0m 8s (remain 0m 6s) Loss: 0.0178(0.0128) \n","EVAL: [300/349] Elapsed 0m 13s (remain 0m 2s) Loss: 0.0122(0.0130) \n","EVAL: [348/349] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0006(0.0123) \n","Epoch 3 - avg_train_loss: 0.0094  avg_val_loss: 0.0123  time: 275s\n","Epoch 3 - Score: 0.8600\n","Epoch 3 - Save Best Score: 0.8600 Model\n","Epoch: [4][0/1438] Elapsed 0m 0s (remain 10m 43s) Loss: 0.0044(0.0044) Grad: 12934.0391  LR: 0.000009  \n","Epoch: [4][100/1438] Elapsed 0m 18s (remain 4m 2s) Loss: 0.0107(0.0066) Grad: 37000.9609  LR: 0.000009  \n","Epoch: [4][200/1438] Elapsed 0m 36s (remain 3m 41s) Loss: 0.0080(0.0073) Grad: 25396.0684  LR: 0.000008  \n","Epoch: [4][300/1438] Elapsed 0m 53s (remain 3m 23s) Loss: 0.0011(0.0075) Grad: 5288.6743  LR: 0.000008  \n","Epoch: [4][400/1438] Elapsed 1m 11s (remain 3m 4s) Loss: 0.0009(0.0075) Grad: 6945.9092  LR: 0.000008  \n","Epoch: [4][500/1438] Elapsed 1m 29s (remain 2m 46s) Loss: 0.0286(0.0077) Grad: 115125.1797  LR: 0.000007  \n","Epoch: [4][600/1438] Elapsed 1m 47s (remain 2m 29s) Loss: 0.0023(0.0075) Grad: 10866.1895  LR: 0.000007  \n","Epoch: [4][700/1438] Elapsed 2m 4s (remain 2m 11s) Loss: 0.0075(0.0076) Grad: 35329.1484  LR: 0.000007  \n","Epoch: [4][800/1438] Elapsed 2m 22s (remain 1m 53s) Loss: 0.0052(0.0076) Grad: 28509.6953  LR: 0.000006  \n","Epoch: [4][900/1438] Elapsed 2m 40s (remain 1m 35s) Loss: 0.0132(0.0075) Grad: 23647.9062  LR: 0.000006  \n","Epoch: [4][1000/1438] Elapsed 2m 57s (remain 1m 17s) Loss: 0.0019(0.0074) Grad: 33963.3594  LR: 0.000006  \n","Epoch: [4][1100/1438] Elapsed 3m 15s (remain 0m 59s) Loss: 0.0004(0.0074) Grad: 4039.2070  LR: 0.000005  \n","Epoch: [4][1200/1438] Elapsed 3m 33s (remain 0m 42s) Loss: 0.0110(0.0076) Grad: 66810.3672  LR: 0.000005  \n","Epoch: [4][1300/1438] Elapsed 3m 51s (remain 0m 24s) Loss: 0.0122(0.0075) Grad: 19110.4902  LR: 0.000005  \n","Epoch: [4][1400/1438] Elapsed 4m 8s (remain 0m 6s) Loss: 0.0062(0.0074) Grad: 14672.6465  LR: 0.000005  \n","Epoch: [4][1437/1438] Elapsed 4m 15s (remain 0m 0s) Loss: 0.0003(0.0074) Grad: 2908.1404  LR: 0.000004  \n","EVAL: [0/349] Elapsed 0m 0s (remain 1m 35s) Loss: 0.0023(0.0023) \n","EVAL: [100/349] Elapsed 0m 4s (remain 0m 11s) Loss: 0.0146(0.0123) \n","EVAL: [200/349] Elapsed 0m 8s (remain 0m 6s) Loss: 0.0134(0.0131) \n","EVAL: [300/349] Elapsed 0m 13s (remain 0m 2s) Loss: 0.0078(0.0132) \n","EVAL: [348/349] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0005(0.0125) \n","Epoch 4 - avg_train_loss: 0.0074  avg_val_loss: 0.0125  time: 275s\n","Epoch 4 - Score: 0.8720\n","Epoch 4 - Save Best Score: 0.8720 Model\n","Epoch: [5][0/1438] Elapsed 0m 0s (remain 10m 41s) Loss: 0.0029(0.0029) Grad: 19274.2148  LR: 0.000004  \n","Epoch: [5][100/1438] Elapsed 0m 18s (remain 4m 0s) Loss: 0.0082(0.0040) Grad: 27358.8047  LR: 0.000004  \n","Epoch: [5][200/1438] Elapsed 0m 35s (remain 3m 41s) Loss: 0.0035(0.0055) Grad: 9559.0840  LR: 0.000004  \n","Epoch: [5][300/1438] Elapsed 0m 53s (remain 3m 22s) Loss: 0.0050(0.0059) Grad: 16522.4414  LR: 0.000004  \n","Epoch: [5][400/1438] Elapsed 1m 11s (remain 3m 4s) Loss: 0.0095(0.0059) Grad: 29702.2305  LR: 0.000003  \n","Epoch: [5][500/1438] Elapsed 1m 29s (remain 2m 46s) Loss: 0.0004(0.0061) Grad: 3087.5251  LR: 0.000003  \n","Epoch: [5][600/1438] Elapsed 1m 46s (remain 2m 28s) Loss: 0.0218(0.0062) Grad: 72359.2109  LR: 0.000003  \n","Epoch: [5][700/1438] Elapsed 2m 4s (remain 2m 10s) Loss: 0.0003(0.0061) Grad: 4854.3276  LR: 0.000002  \n","Epoch: [5][800/1438] Elapsed 2m 22s (remain 1m 53s) Loss: 0.0013(0.0062) Grad: 14510.9912  LR: 0.000002  \n","Epoch: [5][900/1438] Elapsed 2m 40s (remain 1m 35s) Loss: 0.0052(0.0062) Grad: 12402.0811  LR: 0.000002  \n","Epoch: [5][1000/1438] Elapsed 2m 57s (remain 1m 17s) Loss: 0.0012(0.0061) Grad: 21588.7969  LR: 0.000001  \n","Epoch: [5][1100/1438] Elapsed 3m 15s (remain 0m 59s) Loss: 0.0191(0.0062) Grad: 23012.9746  LR: 0.000001  \n","Epoch: [5][1200/1438] Elapsed 3m 33s (remain 0m 42s) Loss: 0.0008(0.0062) Grad: 11930.5273  LR: 0.000001  \n","Epoch: [5][1300/1438] Elapsed 3m 51s (remain 0m 24s) Loss: 0.0023(0.0061) Grad: 8419.0312  LR: 0.000000  \n","Epoch: [5][1400/1438] Elapsed 4m 8s (remain 0m 6s) Loss: 0.0024(0.0061) Grad: 9497.8564  LR: 0.000000  \n","Epoch: [5][1437/1438] Elapsed 4m 15s (remain 0m 0s) Loss: 0.0052(0.0061) Grad: 25576.0703  LR: 0.000000  \n","EVAL: [0/349] Elapsed 0m 0s (remain 1m 35s) Loss: 0.0026(0.0026) \n","EVAL: [100/349] Elapsed 0m 4s (remain 0m 11s) Loss: 0.0143(0.0137) \n","EVAL: [200/349] Elapsed 0m 8s (remain 0m 6s) Loss: 0.0122(0.0145) \n","EVAL: [300/349] Elapsed 0m 13s (remain 0m 2s) Loss: 0.0114(0.0148) \n","EVAL: [348/349] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0003(0.0140) \n","Epoch 5 - avg_train_loss: 0.0061  avg_val_loss: 0.0140  time: 275s\n","Epoch 5 - Score: 0.8712\n","========== fold: 4 training ==========\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/1425] Elapsed 0m 0s (remain 10m 2s) Loss: 1.0935(1.0935) Grad: inf  LR: 0.000000  \n","Epoch: [1][100/1425] Elapsed 0m 18s (remain 3m 57s) Loss: 0.0815(0.5538) Grad: 1167.9292  LR: 0.000003  \n","Epoch: [1][200/1425] Elapsed 0m 35s (remain 3m 38s) Loss: 0.0564(0.3066) Grad: 4817.5190  LR: 0.000006  \n","Epoch: [1][300/1425] Elapsed 0m 53s (remain 3m 20s) Loss: 0.0661(0.2189) Grad: 6738.8809  LR: 0.000008  \n","Epoch: [1][400/1425] Elapsed 1m 11s (remain 3m 2s) Loss: 0.0255(0.1729) Grad: 4087.9812  LR: 0.000011  \n","Epoch: [1][500/1425] Elapsed 1m 29s (remain 2m 44s) Loss: 0.0078(0.1441) Grad: 1090.0005  LR: 0.000014  \n","Epoch: [1][600/1425] Elapsed 1m 46s (remain 2m 26s) Loss: 0.0254(0.1245) Grad: 2647.0820  LR: 0.000017  \n","Epoch: [1][700/1425] Elapsed 2m 4s (remain 2m 8s) Loss: 0.0116(0.1100) Grad: 973.7100  LR: 0.000020  \n","Epoch: [1][800/1425] Elapsed 2m 22s (remain 1m 50s) Loss: 0.0037(0.0990) Grad: 763.0963  LR: 0.000020  \n","Epoch: [1][900/1425] Elapsed 2m 40s (remain 1m 33s) Loss: 0.0065(0.0905) Grad: 1787.2557  LR: 0.000019  \n","Epoch: [1][1000/1425] Elapsed 2m 57s (remain 1m 15s) Loss: 0.0214(0.0833) Grad: 2088.2593  LR: 0.000019  \n","Epoch: [1][1100/1425] Elapsed 3m 15s (remain 0m 57s) Loss: 0.0180(0.0775) Grad: 1405.1517  LR: 0.000019  \n","Epoch: [1][1200/1425] Elapsed 3m 33s (remain 0m 39s) Loss: 0.0273(0.0727) Grad: 3236.4966  LR: 0.000018  \n","Epoch: [1][1300/1425] Elapsed 3m 51s (remain 0m 22s) Loss: 0.0132(0.0685) Grad: 1112.4124  LR: 0.000018  \n","Epoch: [1][1400/1425] Elapsed 4m 8s (remain 0m 4s) Loss: 0.0541(0.0649) Grad: 24942.2715  LR: 0.000018  \n","Epoch: [1][1424/1425] Elapsed 4m 13s (remain 0m 0s) Loss: 0.0094(0.0640) Grad: 1707.3245  LR: 0.000018  \n","EVAL: [0/363] Elapsed 0m 0s (remain 1m 37s) Loss: 0.0200(0.0200) \n","EVAL: [100/363] Elapsed 0m 4s (remain 0m 11s) Loss: 0.0288(0.0164) \n","EVAL: [200/363] Elapsed 0m 8s (remain 0m 7s) Loss: 0.0766(0.0172) \n","EVAL: [300/363] Elapsed 0m 13s (remain 0m 2s) Loss: 0.0084(0.0170) \n","EVAL: [362/363] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0080(0.0158) \n","Epoch 1 - avg_train_loss: 0.0640  avg_val_loss: 0.0158  time: 273s\n","Epoch 1 - Score: 0.8109\n","Epoch 1 - Save Best Score: 0.8109 Model\n","Epoch: [2][0/1425] Elapsed 0m 0s (remain 10m 35s) Loss: 0.0186(0.0186) Grad: 65221.1797  LR: 0.000018  \n","Epoch: [2][100/1425] Elapsed 0m 18s (remain 4m 0s) Loss: 0.0164(0.0119) Grad: 22641.7090  LR: 0.000017  \n","Epoch: [2][200/1425] Elapsed 0m 36s (remain 3m 39s) Loss: 0.0047(0.0126) Grad: 20224.4941  LR: 0.000017  \n","Epoch: [2][300/1425] Elapsed 0m 53s (remain 3m 21s) Loss: 0.0093(0.0130) Grad: 12630.3770  LR: 0.000017  \n","Epoch: [2][400/1425] Elapsed 1m 11s (remain 3m 2s) Loss: 0.0340(0.0127) Grad: 50030.3477  LR: 0.000017  \n","Epoch: [2][500/1425] Elapsed 1m 29s (remain 2m 44s) Loss: 0.0082(0.0123) Grad: 45835.9805  LR: 0.000016  \n","Epoch: [2][600/1425] Elapsed 1m 47s (remain 2m 26s) Loss: 0.0119(0.0127) Grad: 64829.9766  LR: 0.000016  \n","Epoch: [2][700/1425] Elapsed 2m 4s (remain 2m 9s) Loss: 0.0400(0.0129) Grad: 76648.2812  LR: 0.000016  \n","Epoch: [2][800/1425] Elapsed 2m 22s (remain 1m 51s) Loss: 0.0039(0.0130) Grad: 10528.4941  LR: 0.000015  \n","Epoch: [2][900/1425] Elapsed 2m 40s (remain 1m 33s) Loss: 0.0081(0.0129) Grad: 21607.3652  LR: 0.000015  \n","Epoch: [2][1000/1425] Elapsed 2m 58s (remain 1m 15s) Loss: 0.0083(0.0127) Grad: 22587.8398  LR: 0.000015  \n","Epoch: [2][1100/1425] Elapsed 3m 15s (remain 0m 57s) Loss: 0.0163(0.0126) Grad: 125332.5156  LR: 0.000014  \n","Epoch: [2][1200/1425] Elapsed 3m 33s (remain 0m 39s) Loss: 0.0073(0.0125) Grad: 64892.3516  LR: 0.000014  \n","Epoch: [2][1300/1425] Elapsed 3m 51s (remain 0m 22s) Loss: 0.0098(0.0125) Grad: 22120.8145  LR: 0.000014  \n","Epoch: [2][1400/1425] Elapsed 4m 9s (remain 0m 4s) Loss: 0.0074(0.0125) Grad: 37999.1289  LR: 0.000013  \n","Epoch: [2][1424/1425] Elapsed 4m 13s (remain 0m 0s) Loss: 0.0181(0.0124) Grad: 97240.2266  LR: 0.000013  \n","EVAL: [0/363] Elapsed 0m 0s (remain 1m 36s) Loss: 0.0417(0.0417) \n","EVAL: [100/363] Elapsed 0m 4s (remain 0m 11s) Loss: 0.0082(0.0154) \n","EVAL: [200/363] Elapsed 0m 8s (remain 0m 7s) Loss: 0.1053(0.0158) \n","EVAL: [300/363] Elapsed 0m 13s (remain 0m 2s) Loss: 0.0045(0.0150) \n","EVAL: [362/363] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0076(0.0134) \n","Epoch 2 - avg_train_loss: 0.0124  avg_val_loss: 0.0134  time: 274s\n","Epoch 2 - Score: 0.8552\n","Epoch 2 - Save Best Score: 0.8552 Model\n","Epoch: [3][0/1425] Elapsed 0m 0s (remain 10m 46s) Loss: 0.0119(0.0119) Grad: 40320.5664  LR: 0.000013  \n","Epoch: [3][100/1425] Elapsed 0m 18s (remain 3m 59s) Loss: 0.0183(0.0106) Grad: 42618.6055  LR: 0.000013  \n","Epoch: [3][200/1425] Elapsed 0m 36s (remain 3m 39s) Loss: 0.0234(0.0101) Grad: 50094.3242  LR: 0.000013  \n","Epoch: [3][300/1425] Elapsed 0m 53s (remain 3m 20s) Loss: 0.0088(0.0098) Grad: 23957.0957  LR: 0.000012  \n","Epoch: [3][400/1425] Elapsed 1m 11s (remain 3m 2s) Loss: 0.0068(0.0094) Grad: 17153.8965  LR: 0.000012  \n","Epoch: [3][500/1425] Elapsed 1m 29s (remain 2m 44s) Loss: 0.0096(0.0096) Grad: 28255.6055  LR: 0.000012  \n","Epoch: [3][600/1425] Elapsed 1m 46s (remain 2m 26s) Loss: 0.0042(0.0095) Grad: 14796.1875  LR: 0.000011  \n","Epoch: [3][700/1425] Elapsed 2m 4s (remain 2m 8s) Loss: 0.0067(0.0096) Grad: 12175.7832  LR: 0.000011  \n","Epoch: [3][800/1425] Elapsed 2m 22s (remain 1m 50s) Loss: 0.0042(0.0095) Grad: 17737.5352  LR: 0.000011  \n","Epoch: [3][900/1425] Elapsed 2m 40s (remain 1m 33s) Loss: 0.0122(0.0094) Grad: 34004.9688  LR: 0.000011  \n","Epoch: [3][1000/1425] Elapsed 2m 57s (remain 1m 15s) Loss: 0.0090(0.0095) Grad: 18877.8105  LR: 0.000010  \n","Epoch: [3][1100/1425] Elapsed 3m 15s (remain 0m 57s) Loss: 0.0093(0.0094) Grad: 18105.7832  LR: 0.000010  \n","Epoch: [3][1200/1425] Elapsed 3m 33s (remain 0m 39s) Loss: 0.0100(0.0093) Grad: 61403.7305  LR: 0.000010  \n","Epoch: [3][1300/1425] Elapsed 3m 50s (remain 0m 22s) Loss: 0.0134(0.0094) Grad: 36931.6445  LR: 0.000009  \n","Epoch: [3][1400/1425] Elapsed 4m 8s (remain 0m 4s) Loss: 0.0034(0.0094) Grad: 10472.5381  LR: 0.000009  \n","Epoch: [3][1424/1425] Elapsed 4m 12s (remain 0m 0s) Loss: 0.0109(0.0094) Grad: 34969.1406  LR: 0.000009  \n","EVAL: [0/363] Elapsed 0m 0s (remain 1m 41s) Loss: 0.0287(0.0287) \n","EVAL: [100/363] Elapsed 0m 4s (remain 0m 12s) Loss: 0.0096(0.0148) \n","EVAL: [200/363] Elapsed 0m 8s (remain 0m 7s) Loss: 0.0790(0.0149) \n","EVAL: [300/363] Elapsed 0m 13s (remain 0m 2s) Loss: 0.0085(0.0145) \n","EVAL: [362/363] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0079(0.0131) \n","Epoch 3 - avg_train_loss: 0.0094  avg_val_loss: 0.0131  time: 273s\n","Epoch 3 - Score: 0.8561\n","Epoch 3 - Save Best Score: 0.8561 Model\n","Epoch: [4][0/1425] Elapsed 0m 0s (remain 10m 30s) Loss: 0.0091(0.0091) Grad: 27981.6816  LR: 0.000009  \n","Epoch: [4][100/1425] Elapsed 0m 18s (remain 3m 59s) Loss: 0.0082(0.0089) Grad: 31624.1758  LR: 0.000009  \n","Epoch: [4][200/1425] Elapsed 0m 35s (remain 3m 38s) Loss: 0.0014(0.0081) Grad: 7980.1616  LR: 0.000008  \n","Epoch: [4][300/1425] Elapsed 0m 53s (remain 3m 20s) Loss: 0.0222(0.0077) Grad: 21038.0566  LR: 0.000008  \n","Epoch: [4][400/1425] Elapsed 1m 11s (remain 3m 2s) Loss: 0.0126(0.0075) Grad: 42315.6094  LR: 0.000008  \n","Epoch: [4][500/1425] Elapsed 1m 29s (remain 2m 44s) Loss: 0.0127(0.0075) Grad: 16085.7002  LR: 0.000007  \n","Epoch: [4][600/1425] Elapsed 1m 46s (remain 2m 26s) Loss: 0.0028(0.0073) Grad: 12190.6768  LR: 0.000007  \n","Epoch: [4][700/1425] Elapsed 2m 4s (remain 2m 8s) Loss: 0.0006(0.0072) Grad: 5604.9116  LR: 0.000007  \n","Epoch: [4][800/1425] Elapsed 2m 22s (remain 1m 50s) Loss: 0.0003(0.0073) Grad: 2296.9939  LR: 0.000006  \n","Epoch: [4][900/1425] Elapsed 2m 40s (remain 1m 33s) Loss: 0.0031(0.0073) Grad: 31775.2188  LR: 0.000006  \n","Epoch: [4][1000/1425] Elapsed 2m 57s (remain 1m 15s) Loss: 0.0009(0.0073) Grad: 4556.0142  LR: 0.000006  \n","Epoch: [4][1100/1425] Elapsed 3m 15s (remain 0m 57s) Loss: 0.0085(0.0073) Grad: 65911.9062  LR: 0.000005  \n","Epoch: [4][1200/1425] Elapsed 3m 33s (remain 0m 39s) Loss: 0.0041(0.0074) Grad: 10720.5381  LR: 0.000005  \n","Epoch: [4][1300/1425] Elapsed 3m 51s (remain 0m 22s) Loss: 0.0007(0.0074) Grad: 4770.4263  LR: 0.000005  \n","Epoch: [4][1400/1425] Elapsed 4m 8s (remain 0m 4s) Loss: 0.0005(0.0074) Grad: 3779.8938  LR: 0.000005  \n","Epoch: [4][1424/1425] Elapsed 4m 13s (remain 0m 0s) Loss: 0.0072(0.0074) Grad: 19666.1797  LR: 0.000004  \n","EVAL: [0/363] Elapsed 0m 0s (remain 1m 40s) Loss: 0.0289(0.0289) \n","EVAL: [100/363] Elapsed 0m 4s (remain 0m 11s) Loss: 0.0095(0.0172) \n","EVAL: [200/363] Elapsed 0m 8s (remain 0m 7s) Loss: 0.0841(0.0162) \n","EVAL: [300/363] Elapsed 0m 13s (remain 0m 2s) Loss: 0.0084(0.0156) \n","EVAL: [362/363] Elapsed 0m 16s (remain 0m 0s) Loss: 0.0088(0.0140) \n","Epoch 4 - avg_train_loss: 0.0074  avg_val_loss: 0.0140  time: 273s\n","Epoch 4 - Score: 0.8590\n","Epoch 4 - Save Best Score: 0.8590 Model\n","Epoch: [5][0/1425] Elapsed 0m 0s (remain 10m 22s) Loss: 0.0001(0.0001) Grad: 395.3257  LR: 0.000004  \n","Epoch: [5][100/1425] Elapsed 0m 18s (remain 3m 58s) Loss: 0.0074(0.0062) Grad: 20231.0898  LR: 0.000004  \n","Epoch: [5][200/1425] Elapsed 0m 35s (remain 3m 38s) Loss: 0.0002(0.0058) Grad: 1316.9437  LR: 0.000004  \n","Epoch: [5][300/1425] Elapsed 0m 53s (remain 3m 20s) Loss: 0.0044(0.0059) Grad: 18526.1602  LR: 0.000004  \n","Epoch: [5][400/1425] Elapsed 1m 11s (remain 3m 2s) Loss: 0.0185(0.0059) Grad: 139433.1406  LR: 0.000003  \n","Epoch: [5][500/1425] Elapsed 1m 29s (remain 2m 44s) Loss: 0.0090(0.0060) Grad: 20145.3105  LR: 0.000003  \n","Epoch: [5][600/1425] Elapsed 1m 46s (remain 2m 26s) Loss: 0.0035(0.0060) Grad: 41614.4805  LR: 0.000003  \n","Epoch: [5][700/1425] Elapsed 2m 4s (remain 2m 8s) Loss: 0.0093(0.0060) Grad: 24343.8457  LR: 0.000002  \n","Epoch: [5][800/1425] Elapsed 2m 22s (remain 1m 50s) Loss: 0.0017(0.0061) Grad: 35989.6484  LR: 0.000002  \n","Epoch: [5][900/1425] Elapsed 2m 39s (remain 1m 33s) Loss: 0.0024(0.0060) Grad: 30769.7930  LR: 0.000002  \n","Epoch: [5][1000/1425] Elapsed 2m 57s (remain 1m 15s) Loss: 0.0073(0.0061) Grad: 17143.2578  LR: 0.000001  \n","Epoch: [5][1100/1425] Elapsed 3m 15s (remain 0m 57s) Loss: 0.0032(0.0061) Grad: 15553.2402  LR: 0.000001  \n","Epoch: [5][1200/1425] Elapsed 3m 33s (remain 0m 39s) Loss: 0.0091(0.0060) Grad: 22991.3418  LR: 0.000001  \n","Epoch: [5][1300/1425] Elapsed 3m 50s (remain 0m 22s) Loss: 0.0082(0.0060) Grad: 13669.7393  LR: 0.000000  \n","Epoch: [5][1400/1425] Elapsed 4m 8s (remain 0m 4s) Loss: 0.0007(0.0060) Grad: 2009.2950  LR: 0.000000  \n","Epoch: [5][1424/1425] Elapsed 4m 12s (remain 0m 0s) Loss: 0.0083(0.0060) Grad: 32571.1387  LR: 0.000000  \n","EVAL: [0/363] Elapsed 0m 0s (remain 1m 38s) Loss: 0.0402(0.0402) \n","EVAL: [100/363] Elapsed 0m 4s (remain 0m 11s) Loss: 0.0098(0.0183) \n","EVAL: [200/363] Elapsed 0m 8s (remain 0m 7s) Loss: 0.0905(0.0176) \n","EVAL: [300/363] Elapsed 0m 13s (remain 0m 2s) Loss: 0.0114(0.0168) \n","EVAL: [362/363] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0090(0.0152) \n","Epoch 5 - avg_train_loss: 0.0060  avg_val_loss: 0.0152  time: 273s\n","Epoch 5 - Score: 0.8578\n","Best thres: 0.5, Score: 0.8653\n","Best thres: 0.5128906249999999, Score: 0.8655\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6b47b9d05b104969a33df104f4302fea","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b080b5f0b9a448488c738a8f24583c83","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"64053d3b429743a0895f39f11635f797","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e1fd0e5bccd495d9d40570ec10f141a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a73b21b8c0f74940b20774521342898b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n"]}],"source":["if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"name":"nbme-exp003.ipynb","provenance":[{"file_id":"1k6U1erE6sYu9U7bfGdYhvEovwiTN0ehD","timestamp":1645625636482}],"version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"079b2574766243d8a23f92570dbd1533":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0934c9bd53b0406e9cf09f228d67c87e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_27b7dcd8fbc8450eb6c510b3b2313dc3","placeholder":"​","style":"IPY_MODEL_a8d954da9d4b40549108b9c288061a05","value":" 446k/446k [00:00\u0026lt;00:00, 4.73MB/s]"}},"094cb1ec381449fba8d0669bc35917f2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e1a0f5a4b01443c8602b850235bdcbf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc94ea4b3f8b4e3584da1f151cd0e103","placeholder":"​","style":"IPY_MODEL_a16d54cd03bb478f96da89474b8e0074","value":"100%"}},"0e721d1b98094e3fb391c77c74e06019":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_724392a8d4724dc98fd0b3c7273c693d","max":42146,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c9a771c7a74a4fc998b4d14db9a60100","value":42146}},"0f5b33bc213e47b39ea128bb3f2687ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f6a177c8f9d44e29c877783bd023862","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b2daba5168eb42048830cb6010d64d43","value":456318}},"121f0221a8a449388deebec0ead6ef58":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_079b2574766243d8a23f92570dbd1533","placeholder":"​","style":"IPY_MODEL_f5afd1b96eac4094b7a1d783cf9e6192","value":" 52.0/52.0 [00:00\u0026lt;00:00, 2.06kB/s]"}},"1240e074abae4cd7bee98a82b6acb493":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8d5785ea9e7140cc8e4144e1c714a548","IPY_MODEL_eca43aa1df0b4a4782bb253a3623a838","IPY_MODEL_121f0221a8a449388deebec0ead6ef58"],"layout":"IPY_MODEL_426455dc60f34b4a9bba3919ceabc78b"}},"13a13b1845cc4259acc074970cb3168e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"166073c76ef54ab0bfa7606bba547fa1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5267216ad0344df89010b0317bd3334","placeholder":"​","style":"IPY_MODEL_e6a75dff1d394befa5d623ef2e48f89d","value":"100%"}},"16dfaf81ac3f45b19a8ec4e7959a89d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb194c973adf479fab7d725f4d5efaff","placeholder":"​","style":"IPY_MODEL_28c54474f1d4498b8875825e5ca8bffa","value":" 878k/878k [00:00\u0026lt;00:00, 6.82MB/s]"}},"1c229d668fcc4097b1588a501934e619":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5ad1b8f2fb74dd5a56b1a935ebea106","max":898825,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2f101287c0464ad88e5ed3fa8f26f948","value":898825}},"1e7d3e75b4d149209c30aa744bd4fc6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c2ac13512d945c5821d461ba7b351ca","placeholder":"​","style":"IPY_MODEL_508c64fbb0624368a2796b165cc28837","value":" 474/474 [00:00\u0026lt;00:00, 19.6kB/s]"}},"23802b98bd184620bb7ee14fd4bb8556":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0e1a0f5a4b01443c8602b850235bdcbf","IPY_MODEL_0e721d1b98094e3fb391c77c74e06019","IPY_MODEL_26a9df7de1914dc4ad1997e803513aad"],"layout":"IPY_MODEL_dc4914db9bff475f947303c210f86731"}},"239cf276bdb947fea9b828055094e931":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"251a7e5a2c704ea38bcd9def4c0a1bbe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"264273174f4e4f6884a89dfe47402205":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"26a9df7de1914dc4ad1997e803513aad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_094cb1ec381449fba8d0669bc35917f2","placeholder":"​","style":"IPY_MODEL_7de976b196c04471bb4f8d53f73d9de6","value":" 42146/42146 [00:21\u0026lt;00:00, 2026.70it/s]"}},"27b7dcd8fbc8450eb6c510b3b2313dc3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28c54474f1d4498b8875825e5ca8bffa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2bb15fefe29448e3b61c9e7de0e1fed3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2f101287c0464ad88e5ed3fa8f26f948":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3456bcab5db4469f8cf2127fb6154d8a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e616a2fcd19e42a2a43e48d2b225c566","max":143,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c5c3416c69504775af0ff33e9101a8ad","value":143}},"3771f967cbd94395abce58ca3c3b4340":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3efb0e72f3fb4e9389f170c6ba25304a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"426455dc60f34b4a9bba3919ceabc78b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42b6d0dc088541b4bafb28520bb67a11":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_926e7f1cadb64f91b912d889d85f8bfe","placeholder":"​","style":"IPY_MODEL_dab2c4c56e3446578737b1efa0086ad3","value":"Downloading: 100%"}},"49ff5f982cb2446487ce5d0c7886dd00":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"508c64fbb0624368a2796b165cc28837":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"58dfc4d72c4b409899cbbcbf4d2adadd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b30914e5fe8d4a0898a9a180e3023593","placeholder":"​","style":"IPY_MODEL_749990b6bb24490b840efa59c6504336","value":" 533M/533M [00:10\u0026lt;00:00, 58.3MB/s]"}},"5c2ac13512d945c5821d461ba7b351ca":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63beac326cd4473098340ccec7fae7a8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65892b37358e4e8d844c3081fe47639a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6acb6389a35747fe80ae529923369dcc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce9dbfbda065477da4d7dafe3a389f03","max":558614189,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d5771c13f32846c4b2cf616c8493fe39","value":558614189}},"6f6a177c8f9d44e29c877783bd023862":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"724392a8d4724dc98fd0b3c7273c693d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"749990b6bb24490b840efa59c6504336":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"76b633fd8cb644a491d7d10ead4885ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a36709849a204bb998f11338d3c17098","placeholder":"​","style":"IPY_MODEL_93454c251e14468b96dce46bdea4fdd0","value":" 143/143 [00:00\u0026lt;00:00, 3169.53it/s]"}},"774d7cf77c1840f8b3c5dba91c057258":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7de976b196c04471bb4f8d53f73d9de6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"80d784b724f844dbb2416d49dbf7d280":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_13a13b1845cc4259acc074970cb3168e","max":474,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f7862193e22d4fdda484ae0f71414163","value":474}},"83267a4bd06940c3a06b4d61425c4490":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_42b6d0dc088541b4bafb28520bb67a11","IPY_MODEL_6acb6389a35747fe80ae529923369dcc","IPY_MODEL_58dfc4d72c4b409899cbbcbf4d2adadd"],"layout":"IPY_MODEL_63beac326cd4473098340ccec7fae7a8"}},"8d5785ea9e7140cc8e4144e1c714a548":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_239cf276bdb947fea9b828055094e931","placeholder":"​","style":"IPY_MODEL_e9e4a84d9114421d8a8ae6b3b026e842","value":"Downloading: 100%"}},"8f0c338c4759472f9250d8f08b347d2e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ae427a5b962e4b7b91668401a18e0dd5","IPY_MODEL_1c229d668fcc4097b1588a501934e619","IPY_MODEL_16dfaf81ac3f45b19a8ec4e7959a89d0"],"layout":"IPY_MODEL_65892b37358e4e8d844c3081fe47639a"}},"926e7f1cadb64f91b912d889d85f8bfe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93454c251e14468b96dce46bdea4fdd0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a16d54cd03bb478f96da89474b8e0074":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a36709849a204bb998f11338d3c17098":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5ad1b8f2fb74dd5a56b1a935ebea106":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8d954da9d4b40549108b9c288061a05":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a928ded64c5341e598412cf5309e558c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae427a5b962e4b7b91668401a18e0dd5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a928ded64c5341e598412cf5309e558c","placeholder":"​","style":"IPY_MODEL_264273174f4e4f6884a89dfe47402205","value":"Downloading: 100%"}},"b2daba5168eb42048830cb6010d64d43":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b30914e5fe8d4a0898a9a180e3023593":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b43b86df432c458f941c269460d19898":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9f3c3b86db74db7bef8fc86f2d246ac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e0a3ac7d83f44dd793039141a4485f52","IPY_MODEL_0f5b33bc213e47b39ea128bb3f2687ed","IPY_MODEL_0934c9bd53b0406e9cf09f228d67c87e"],"layout":"IPY_MODEL_49ff5f982cb2446487ce5d0c7886dd00"}},"bb194c973adf479fab7d725f4d5efaff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc94ea4b3f8b4e3584da1f151cd0e103":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5c3416c69504775af0ff33e9101a8ad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c64066983b5c4aa688e22b41a2431341":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_166073c76ef54ab0bfa7606bba547fa1","IPY_MODEL_3456bcab5db4469f8cf2127fb6154d8a","IPY_MODEL_76b633fd8cb644a491d7d10ead4885ef"],"layout":"IPY_MODEL_774d7cf77c1840f8b3c5dba91c057258"}},"c9a771c7a74a4fc998b4d14db9a60100":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ce9dbfbda065477da4d7dafe3a389f03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5771c13f32846c4b2cf616c8493fe39":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dab2c4c56e3446578737b1efa0086ad3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc4914db9bff475f947303c210f86731":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0a3ac7d83f44dd793039141a4485f52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b43b86df432c458f941c269460d19898","placeholder":"​","style":"IPY_MODEL_251a7e5a2c704ea38bcd9def4c0a1bbe","value":"Downloading: 100%"}},"e616a2fcd19e42a2a43e48d2b225c566":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6a75dff1d394befa5d623ef2e48f89d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e9e4a84d9114421d8a8ae6b3b026e842":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eace1904db414e0cbaa2344a3bb308f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3efb0e72f3fb4e9389f170c6ba25304a","placeholder":"​","style":"IPY_MODEL_f9e1f3b203bb404fb6cf04957f6f4f40","value":"Downloading: 100%"}},"ec2808cf5be240038c006801e0626fef":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eca43aa1df0b4a4782bb253a3623a838":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3771f967cbd94395abce58ca3c3b4340","max":52,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2bb15fefe29448e3b61c9e7de0e1fed3","value":52}},"ef70a47928e241f69a9b9c114a6ed384":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eace1904db414e0cbaa2344a3bb308f5","IPY_MODEL_80d784b724f844dbb2416d49dbf7d280","IPY_MODEL_1e7d3e75b4d149209c30aa744bd4fc6c"],"layout":"IPY_MODEL_ec2808cf5be240038c006801e0626fef"}},"f5267216ad0344df89010b0317bd3334":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5afd1b96eac4094b7a1d783cf9e6192":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7862193e22d4fdda484ae0f71414163":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f9e1f3b203bb404fb6cf04957f6f4f40":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}