{"cells":[{"cell_type":"markdown","id":"national-fancy","metadata":{"id":"national-fancy"},"source":["## References"]},{"cell_type":"markdown","id":"copyrighted-centre","metadata":{"id":"copyrighted-centre"},"source":["- https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train"]},{"cell_type":"markdown","id":"imported-offset","metadata":{"id":"imported-offset"},"source":["## Configurations"]},{"cell_type":"code","execution_count":1,"id":"complimentary-wyoming","metadata":{"id":"complimentary-wyoming","executionInfo":{"status":"ok","timestamp":1647830297362,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["EXP_NAME = \"nbme-exp057\"\n","ENV = \"colab\"\n","DEBUG_MODE = False\n","SUBMISSION_MODE = False"]},{"cell_type":"code","execution_count":2,"id":"allied-circuit","metadata":{"id":"allied-circuit","executionInfo":{"status":"ok","timestamp":1647830297362,"user_tz":-540,"elapsed":6,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class CFG:\n","    env=ENV\n","    exp_name=EXP_NAME\n","    debug=DEBUG_MODE\n","    submission=SUBMISSION_MODE\n","    apex=True\n","    input_dir=None\n","    output_dir=None\n","    library=\"pytorch\"  # [\"tf\", \"pytorch\"]\n","    device=\"GPU\"  # [\"GPU\", \"TPU\"]\n","    competition_name=\"nbme-score-clinical-patient-notes\"\n","    id_col=\"id\"\n","    target_col=\"location\"\n","    pretrained_model_name=\"microsoft/deberta-large\"\n","    tokenizer=None\n","    max_len=None\n","    max_char_len=None\n","    output_dim=1\n","    dropout=0.2\n","    num_workers=4\n","    batch_size=1\n","    lr=2e-5\n","    betas=(0.9, 0.98)\n","    weight_decay=0.1\n","    num_warmup_steps_rate=0.1\n","    batch_scheduler=True\n","    epochs=5\n","    n_fold=4\n","    train_fold=[0, 1, 2, 3]\n","    seed=71\n","    gradient_accumulation_steps=6\n","    max_grad_norm=1000\n","    print_freq=100\n","    train=True\n","    inference=True"]},{"cell_type":"code","execution_count":3,"id":"geographic-hindu","metadata":{"id":"geographic-hindu","executionInfo":{"status":"ok","timestamp":1647830297363,"user_tz":-540,"elapsed":7,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.train_fold = [0, 1]\n","\n","if CFG.submission:\n","    CFG.train = False\n","    CFG.inference = True"]},{"cell_type":"markdown","id":"confident-fifth","metadata":{"id":"confident-fifth"},"source":["## Directory Settings"]},{"cell_type":"code","execution_count":4,"id":"miniature-greeting","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"miniature-greeting","outputId":"03fb9617-56d5-4312-f5c1-1bceaf425898","executionInfo":{"status":"ok","timestamp":1647830304580,"user_tz":-540,"elapsed":7223,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["colab\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: transformers==4.16.2 in /usr/local/lib/python3.7/dist-packages (4.16.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (1.21.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2019.12.20)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (6.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (0.0.49)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.11.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.63.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (3.6.0)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (0.11.6)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (0.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.16.2) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.16.2) (3.7.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (1.15.0)\n"]}],"source":["import sys\n","from pathlib import Path\n","\n","\n","print(CFG.env)\n","if CFG.env == \"colab\":\n","    # colab環境\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","    CFG.input_dir = Path(\"./drive/MyDrive/00.kaggle/input\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","    # install packages\n","    !pip install transformers==4.16.2\n","\n","elif CFG.env == \"local\":\n","    # ローカルサーバ\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"../output/\") / CFG.competition_name / CFG.exp_name\n","    if not CFG.output_dir.exists():\n","        CFG.output_dir.mkdir()\n","\n","elif CFG.env == \"kaggle\":\n","    # kaggle環境\n","    CFG.input_dir = Path(\"../input/\") / CFG.competition_name\n","    CFG.output_dir = Path(\"./\")"]},{"cell_type":"code","execution_count":5,"id":"guilty-filename","metadata":{"id":"guilty-filename","executionInfo":{"status":"ok","timestamp":1647830314473,"user_tz":-540,"elapsed":9898,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["import gc\n","import os\n","import ast\n","import time\n","import math\n","import random\n","import itertools\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","from scipy.optimize import minimize\n","from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as T\n","from torchvision.io import read_image\n","from torch.utils.data import DataLoader, Dataset\n","\n","from transformers import AutoModelForMaskedLM\n","from transformers import BartModel,BertModel,BertTokenizer\n","from transformers import DebertaModel,DebertaTokenizer\n","from transformers import RobertaModel,RobertaTokenizer\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","id":"cubic-designation","metadata":{"id":"cubic-designation"},"source":["## Utilities"]},{"cell_type":"code","execution_count":6,"id":"opposite-plasma","metadata":{"id":"opposite-plasma","executionInfo":{"status":"ok","timestamp":1647830314474,"user_tz":-540,"elapsed":4,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on binary arrays.\n","\n","    Args:\n","        preds (list of lists of ints): Predictions.\n","        truths (list of lists of ints): Ground truths.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    # Micro : aggregating over all instances\n","    preds = np.concatenate(preds)\n","    truths = np.concatenate(truths)\n","    return f1_score(truths, preds)\n","\n","\n","def spans_to_binary(spans, length=None):\n","    \"\"\"\n","    Converts spans to a binary array indicating whether each character is in the span.\n","\n","    Args:\n","        spans (list of lists of two ints): Spans.\n","\n","    Returns:\n","        np array [length]: Binarized spans.\n","    \"\"\"\n","    length = np.max(spans) if length is None else length\n","    binary = np.zeros(length)\n","    for start, end in spans:\n","        binary[start:end] = 1\n","    return binary\n","\n","\n","def span_micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on spans.\n","\n","    Args:\n","        preds (list of lists of two ints): Prediction spans.\n","        truths (list of lists of two ints): Ground truth spans.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    bin_preds = []\n","    bin_truths = []\n","    for pred, truth in zip(preds, truths):\n","        if not len(pred) and not len(truth):\n","            continue\n","        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n","        bin_preds.append(spans_to_binary(pred, length))\n","        bin_truths.append(spans_to_binary(truth, length))\n","    return micro_f1(bin_preds, bin_truths)\n","\n","\n","def get_score(y_true, y_pred):\n","    score = span_micro_f1(y_true, y_pred)\n","    return score"]},{"cell_type":"code","execution_count":7,"id":"multiple-poland","metadata":{"id":"multiple-poland","executionInfo":{"status":"ok","timestamp":1647830314475,"user_tz":-540,"elapsed":4,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def create_labels_for_scoring(df):\n","    # example: ['48 61', '111 128'] -> [[48, 61], [111, 128]]\n","    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n","    for i in range(len(df)):\n","        lst = df.loc[i, \"location\"]\n","        if lst:\n","            new_lst = \";\".join(lst)\n","            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(f\"[['{new_lst}']]\")\n","\n","    # create labels\n","    truths = []\n","    for location_list in df[\"location_for_create_labels\"].values:\n","        truth = []\n","        if len(location_list) > 0:\n","            location = location_list[0]\n","            for loc in [s.split() for s in location.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                truth.append([start, end])\n","        truths.append(truth)\n","\n","    return truths\n","\n","\n","def get_char_probs(texts, token_probs, tokenizer):\n","    res = [np.zeros(len(t)) for t in texts]\n","    for i, (text, prediction) in enumerate(zip(texts, token_probs)):\n","        encoded = tokenizer(\n","            text=text,\n","            max_length=CFG.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        for (offset_mapping, pred) in zip(encoded[\"offset_mapping\"], prediction):\n","            start, end = offset_mapping\n","            res[i][start:end] = pred\n","    return res\n","\n","\n","def get_predicted_location_str(char_probs, th=0.5):\n","    results = []\n","    for char_prob in char_probs:\n","        # result = np.where(char_prob >= th)[0] + 1\n","        result = np.where(char_prob >= th)[0]\n","        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n","        # result = [f\"{min(r)} {max(r)}\" for r in result]\n","        result = [f\"{min(r)} {max(r) + 1}\" for r in result]\n","        result = \";\".join(result)\n","        results.append(result)\n","    return results\n","\n","\n","def get_predictions(results):\n","    predictions = []\n","    for result in results:\n","        prediction = []\n","        if result != \"\":\n","            for loc in [s.split() for s in result.split(\";\")]:\n","                start, end = int(loc[0]), int(loc[1])\n","                prediction.append([start, end])\n","        predictions.append(prediction)\n","    return predictions\n","\n","\n","def scoring(df, th=0.5, use_token_prob=True):\n","    labels = create_labels_for_scoring(df)\n","\n","    if use_token_prob:\n","        token_probs = df[[str(i) for i in range(CFG.max_len)]].values\n","        char_probs = get_char_probs(df[\"pn_history\"].values, token_probs, CFG.tokenizer)\n","    else:\n","        char_probs = df[[str(i) for i in range(CFG.max_char_len)]].values\n","        char_probs = [char_probs[i] for i in range(len(char_probs))]\n","\n","    predicted_location_str = get_predicted_location_str(char_probs, th=th)\n","    preds = get_predictions(predicted_location_str)\n","\n","    score = get_score(labels, preds)\n","    return score\n","\n","\n","def get_best_thres(oof_df):\n","    def f1_opt(x):\n","        return -1 * scoring(oof_df, th=x)\n","\n","    best_thres = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")[\"x\"].item()\n","    return best_thres"]},{"cell_type":"code","execution_count":8,"id":"seventh-fighter","metadata":{"id":"seventh-fighter","executionInfo":{"status":"ok","timestamp":1647830314475,"user_tz":-540,"elapsed":4,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return \"%dm %ds\" % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":9,"id":"fifty-boundary","metadata":{"id":"fifty-boundary","executionInfo":{"status":"ok","timestamp":1647830315023,"user_tz":-540,"elapsed":552,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["seed_everything()"]},{"cell_type":"markdown","id":"unlimited-hotel","metadata":{"id":"unlimited-hotel"},"source":["## Data Loading"]},{"cell_type":"code","execution_count":10,"id":"classical-machine","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"classical-machine","outputId":"30445311-52c3-44d2-d180-408a3e077f39","executionInfo":{"status":"ok","timestamp":1647830315487,"user_tz":-540,"elapsed":470,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14300, 6), (143, 3), (42146, 3), (5, 4))"]},"metadata":{},"execution_count":10}],"source":["train = pd.read_csv(CFG.input_dir / \"train.csv\")\n","features = pd.read_csv(CFG.input_dir / \"features.csv\")\n","patient_notes = pd.read_csv(CFG.input_dir / \"patient_notes.csv\")\n","test = pd.read_csv(CFG.input_dir / \"test.csv\")\n","\n","train.shape, features.shape, patient_notes.shape, test.shape"]},{"cell_type":"code","execution_count":11,"id":"vanilla-iceland","metadata":{"id":"vanilla-iceland","executionInfo":{"status":"ok","timestamp":1647830315488,"user_tz":-540,"elapsed":3,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["if CFG.debug:\n","    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n","    print(train.shape)"]},{"cell_type":"markdown","id":"convenient-plant","metadata":{"id":"convenient-plant"},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":12,"id":"convertible-thunder","metadata":{"id":"convertible-thunder","executionInfo":{"status":"ok","timestamp":1647830315797,"user_tz":-540,"elapsed":312,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def preprocess_features(features):\n","    features.loc[features[\"feature_text\"] == \"Last-Pap-smear-I-year-ago\", \"feature_text\"] = \"Last-Pap-smear-1-year-ago\"\n","    return features\n","\n","\n","features = preprocess_features(features)"]},{"cell_type":"code","execution_count":13,"id":"charitable-memphis","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"charitable-memphis","outputId":"ee7ca20b-3e34-42c6-d5de-6981eca4c190","executionInfo":{"status":"ok","timestamp":1647830315797,"user_tz":-540,"elapsed":10,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14300, 8), (5, 6))"]},"metadata":{},"execution_count":13}],"source":["train = train.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","train = train.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","test = test.merge(features, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","test = test.merge(patient_notes, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","\n","train.shape, test.shape"]},{"cell_type":"code","execution_count":14,"id":"governing-election","metadata":{"id":"governing-election","executionInfo":{"status":"ok","timestamp":1647830315798,"user_tz":-540,"elapsed":10,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["train[\"annotation\"] = train[\"annotation\"].apply(ast.literal_eval)\n","train[\"location\"] = train[\"location\"].apply(ast.literal_eval)"]},{"cell_type":"code","execution_count":15,"id":"negative-provincial","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"id":"negative-provincial","outputId":"96208ed1-8a21-4dbe-c61d-ab4cc6da8cb7","executionInfo":{"status":"ok","timestamp":1647830315798,"user_tz":-540,"elapsed":10,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["0    4399\n","1    8181\n","2    1296\n","3     287\n","4      99\n","5      27\n","6       9\n","7       1\n","8       1\n","Name: annotation_length, dtype: int64"]},"metadata":{}}],"source":["train[\"annotation_length\"] = train[\"annotation\"].apply(len)\n","display(train['annotation_length'].value_counts().sort_index())"]},{"cell_type":"markdown","id":"arbitrary-beatles","metadata":{"id":"arbitrary-beatles"},"source":["## CV split"]},{"cell_type":"code","execution_count":16,"id":"important-murray","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":124},"id":"important-murray","outputId":"ba3cb2b6-0bf9-421c-ee6d-cccd960b54dd","executionInfo":{"status":"ok","timestamp":1647830315799,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["fold\n","0    3575\n","1    3575\n","2    3575\n","3    3575\n","dtype: int64"]},"metadata":{}}],"source":["Fold = GroupKFold(n_splits=CFG.n_fold)\n","groups = train['pn_num'].values\n","for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n","    train.loc[val_index, 'fold'] = int(n)\n","train['fold'] = train['fold'].astype(int)\n","display(train.groupby('fold').size())"]},{"cell_type":"markdown","id":"configured-chemistry","metadata":{"id":"configured-chemistry"},"source":["## Setup tokenizer"]},{"cell_type":"code","execution_count":17,"id":"hindu-contest","metadata":{"id":"hindu-contest","executionInfo":{"status":"ok","timestamp":1647830320507,"user_tz":-540,"elapsed":4716,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["if CFG.submission:\n","    tokenizer = AutoTokenizer.from_pretrained(Path(\"../input/\") / CFG.exp_name / \"tokenizer/\")\n","else:\n","    tokenizer = AutoTokenizer.from_pretrained(CFG.pretrained_model_name)\n","    tokenizer.save_pretrained(CFG.output_dir / \"tokenizer/\")\n","\n","CFG.tokenizer = tokenizer"]},{"cell_type":"markdown","id":"alleged-protein","metadata":{"id":"alleged-protein"},"source":["## Create dataset"]},{"cell_type":"code","execution_count":18,"id":"composed-stroke","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["d2031e5f3510429f977a8accb73623ad","0f7365a2a54e4f3289867fc78fa5d8f0","4ae813d03aee4dbc9ea70b16e867b8d7","0acd16bc1b774b27b8a1d0027251caf2","6441d4d15bb14c6798ef59080d3efd5b","f59fe1bf5c8644d5a41d476dffa39d61","91205e4ebe844c0c88f5d1b6731ff233","ef1a0ffadb9e438fb08d33c0e093388f","177a811a8f0d4c3abd9c5b034bd33556","3ec95d8a55984afb8eb9efd55b921d80","77726cf0c093411085c59646885d2b28"]},"id":"composed-stroke","outputId":"7cf0d5f9-4635-47d5-e099-53cc19801385","executionInfo":{"status":"ok","timestamp":1647830356352,"user_tz":-540,"elapsed":35851,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/42146 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2031e5f3510429f977a8accb73623ad"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 433\n"]}],"source":["pn_history_lengths = []\n","tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    pn_history_lengths.append(length)\n","\n","print(\"max length:\", np.max(pn_history_lengths))"]},{"cell_type":"code","execution_count":19,"id":"emotional-region","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["ac2475c1acb4499eab3945dfa5c56779","c7e5b695ef7c4acea0b1eb9bd2725a98","0ba1e9bd83154395a37575d2e4a724f9","a9ad179331184ee18e5ef2f6a09dc967","a2a872834ce548658aa1f85dcfc27d48","1a6e09e641e84d919d0ea487e738a6a9","16ec8142da5f457490909b5d5ecc5a2c","a0a0ea9a68804c459463520533a6ddb2","243f2cea17124685b7275199d0197947","9f0eeaa852284fae82e92f09f611c5f6","b33cf6ce0b314dc1a403827b3c2f5125"]},"id":"emotional-region","outputId":"18a794fa-8050-4c31-fda3-d45fef59b785","executionInfo":{"status":"ok","timestamp":1647830356353,"user_tz":-540,"elapsed":26,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/143 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac2475c1acb4499eab3945dfa5c56779"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 30\n"]}],"source":["feature_text_lengths = []\n","tk0 = tqdm(features[\"feature_text\"].fillna(\"\").values, total=len(features))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n","    feature_text_lengths.append(length)\n","\n","print(\"max length:\", np.max(feature_text_lengths))"]},{"cell_type":"code","execution_count":20,"id":"wrong-leisure","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wrong-leisure","outputId":"485a6e51-cb3f-480d-91ec-a10c185d0a55","executionInfo":{"status":"ok","timestamp":1647830356353,"user_tz":-540,"elapsed":22,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["max length: 466\n"]}],"source":["CFG.max_len = max(pn_history_lengths) + max(feature_text_lengths) + 3   # cls & sep & sep\n","\n","print(\"max length:\", CFG.max_len)"]},{"cell_type":"code","execution_count":21,"id":"convenient-gospel","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["b7530e27be384f3fbdc650a521b80962","7d615121ce48438084f98f9dc0c5b663","44edeaff57294f3182ba2f00b3d379a1","046b59dfab82487eaf5e1268f996b018","9059fb0eb80642afbc8c4a6625c853df","1e3f145cc1564614aba29374a43dfe45","03f973838c924885b7600241d729d183","752ddda179d24fd287fee23f274a1206","e77baa3975c2484595060dbdcad867d4","96f479f1c36143b5a0c16de2651c5ecb","df31fc87a42b4f5199b5f4b5d02dd64c"]},"id":"convenient-gospel","executionInfo":{"status":"ok","timestamp":1647830356878,"user_tz":-540,"elapsed":545,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}},"outputId":"217cf962-28cd-4913-8566-56d3edefa9b4"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/42146 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7530e27be384f3fbdc650a521b80962"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["max length: 950\n"]}],"source":["pn_history_lengths = []\n","tk0 = tqdm(patient_notes[\"pn_history\"].fillna(\"\").values, total=len(patient_notes))\n","for text in tk0:\n","    length = len(text)\n","    pn_history_lengths.append(length)\n","\n","CFG.max_char_len = max(pn_history_lengths)\n","\n","print(\"max length:\", CFG.max_char_len)"]},{"cell_type":"code","execution_count":22,"id":"representative-contributor","metadata":{"id":"representative-contributor","executionInfo":{"status":"ok","timestamp":1647830356878,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class TrainingDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.max_char_len = self.cfg.max_char_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","        self.annotation_lengths = self.df[\"annotation_length\"].values\n","        self.locations = self.df[\"location\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def _create_mapping_from_token_to_char(self, pn_history):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        mapping_from_token_to_char = np.zeros(self.max_char_len)\n","        offset_mapping = encoded[\"offset_mapping\"]\n","        for i, offset in enumerate(offset_mapping):\n","            start_idx, end_idx = offset\n","            mapping_from_token_to_char[start_idx:end_idx] = i\n","        return torch.tensor(mapping_from_token_to_char, dtype=torch.long)\n","\n","    def _create_label(self, pn_history, annotation_length, location_list):\n","        label = np.zeros(self.max_char_len)\n","        label[len(pn_history):] = -1\n","        if annotation_length > 0:\n","            for location in location_list:\n","                for loc in [s.split() for s in location.split(\";\")]:\n","                    start, end = int(loc[0]), int(loc[1])\n","                    label[start:end] = 1\n","        return torch.tensor(label, dtype=torch.float)\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        label = self._create_label(self.pn_historys[idx], self.annotation_lengths[idx], self.locations[idx])\n","        mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n","        return input_, label, mapping_from_token_to_char"]},{"cell_type":"code","execution_count":23,"id":"decent-johnson","metadata":{"id":"decent-johnson","executionInfo":{"status":"ok","timestamp":1647830356879,"user_tz":-540,"elapsed":10,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class TestDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = self.cfg.tokenizer\n","        self.max_len = self.cfg.max_len\n","        self.feature_texts = self.df[\"feature_text\"].values\n","        self.pn_historys = self.df[\"pn_history\"].values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _create_input(self, pn_history, feature_text):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            text_pair=feature_text,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=False,\n","        )\n","        for k, v in encoded.items():\n","            encoded[k] = torch.tensor(v, dtype=torch.long)\n","        return encoded\n","\n","    def _create_mapping_from_token_to_char(self, pn_history):\n","        encoded = self.tokenizer(\n","            text=pn_history,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_offsets_mapping=True,\n","        )\n","        mapping_from_token_to_char = np.zeros(self.max_char_len)\n","        offset_mapping = encoded[\"offset_mapping\"]\n","        for i, offset in enumerate(offset_mapping):\n","            start_idx, end_idx = offset\n","            mapping_from_token_to_char[start_idx:end_idx] = i\n","        return torch.tensor(mapping_from_token_to_char, dtype=torch.long)\n","\n","    def __getitem__(self, idx):\n","        input_ = self._create_input(self.pn_historys[idx], self.feature_texts[idx])\n","        mapping_from_token_to_char = self._create_mapping_from_token_to_char(self.pn_historys[idx])\n","        return input_, mapping_from_token_to_char"]},{"cell_type":"markdown","id":"arctic-joint","metadata":{"id":"arctic-joint"},"source":["## Model"]},{"cell_type":"code","source":["class Exp054Model(nn.Module):\n","    def __init__(self, cfg, model_config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        if model_config_path is None:\n","            self.model_config = AutoConfig.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                output_hidden_states=True,\n","            )\n","        else:\n","            self.model_config = torch.load(model_config_path)\n","\n","        if pretrained:\n","            self.backbone = AutoModel.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                config=self.model_config,\n","            )\n","            print(f\"Load weight from pretrained\")\n","        else:\n","            #self.backbone = AutoModel.from_config(self.model_config)\n","            itpt = AutoModelForMaskedLM.from_config(self.model_config)\n","            path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp010/checkpoint-130170/pytorch_model.bin\")\n","            # path = \"../output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\"\n","            state_dict = torch.load(path)\n","            itpt.load_state_dict(state_dict)\n","            self.backbone = itpt.deberta\n","            print(f\"Load weight from {path}\")\n","\n","        self.fc = nn.Sequential(\n","            nn.Dropout(self.cfg.dropout),\n","            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n","        )\n","\n","    def forward(self, inputs):\n","        h = self.backbone(**inputs)[\"last_hidden_state\"]\n","        output = self.fc(h)\n","        return output"],"metadata":{"id":"UtM7nYFm333y","executionInfo":{"status":"ok","timestamp":1647830356879,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"id":"UtM7nYFm333y","execution_count":24,"outputs":[]},{"cell_type":"code","execution_count":25,"id":"alternative-malawi","metadata":{"id":"alternative-malawi","executionInfo":{"status":"ok","timestamp":1647830356879,"user_tz":-540,"elapsed":9,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["class CustomModel(nn.Module):\n","    def __init__(self, cfg, model_config_path=None, pretrained=False, i_fold=None):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        if model_config_path is None:\n","            self.model_config = AutoConfig.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                output_hidden_states=True,\n","            )\n","        else:\n","            self.model_config = torch.load(model_config_path)\n","\n","        if pretrained:\n","            self.backbone = AutoModel.from_pretrained(\n","                self.cfg.pretrained_model_name,\n","                config=self.model_config,\n","            )\n","            print(f\"Load weight from pretrained\")\n","        else:\n","            #self.backbone = AutoModel.from_config(self.model_config)\n","\n","            model = Exp054Model(cfg, model_config_path=None, pretrained=False)\n","            path = str(Path(\"./drive/MyDrive/00.kaggle/output\") / CFG.competition_name /  \"nbme-exp054\" /  f\"fold{i_fold}_best.pth\")\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            model.load_state_dict(state[\"model\"])\n","            self.backbone = model.backbone\n","            print(f\"Load weight from {path}\")\n","\n","        self.lstm = nn.LSTM(\n","            input_size=self.model_config.hidden_size,\n","            bidirectional=True,\n","            hidden_size=self.model_config.hidden_size // 2,\n","            num_layers=2,\n","            dropout=self.cfg.dropout,\n","            batch_first=True,\n","        )\n","        self.fc = nn.Sequential(\n","            nn.Dropout(self.cfg.dropout),\n","            nn.Linear(self.model_config.hidden_size, self.cfg.output_dim),\n","        )\n","\n","    def forward(self, inputs, mappings_from_token_to_char):\n","        h = self.backbone(**inputs)[\"last_hidden_state\"]  # [batch, seq_len, d_model]\n","        mappings_from_token_to_char = mappings_from_token_to_char.unsqueeze(2).expand(-1, -1, self.model_config.hidden_size)\n","        h = torch.gather(h, 1, mappings_from_token_to_char)    # [batch, seq_len, d_model]\n","        h, _ = self.lstm(h)\n","        output = self.fc(h)\n","\n","        return output"]},{"cell_type":"markdown","id":"therapeutic-assembly","metadata":{"id":"therapeutic-assembly"},"source":["## Training"]},{"cell_type":"code","execution_count":26,"id":"going-conversion","metadata":{"id":"going-conversion","executionInfo":{"status":"ok","timestamp":1647830356880,"user_tz":-540,"elapsed":10,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def train_fn(\n","    train_dataloader,\n","    model,\n","    criterion,\n","    optimizer,\n","    epoch,\n","    scheduler,\n","    device,\n","):\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels, mappings_from_token_to_char) in enumerate(train_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device) \n","        batch_size = labels.size(0)\n","        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            output = model(inputs, mappings_from_token_to_char)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","        loss = loss.mean()\n","\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","\n","        if CFG.batch_scheduler:\n","            scheduler.step()\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_dataloader)-1):\n","            print(\n","                \"Epoch: [{0}][{1}/{2}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                \"Grad: {grad_norm:.4f}  \"\n","                \"LR: {lr:.6f}  \"\n","                .format(\n","                    epoch+1,\n","                    step,\n","                    len(train_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(train_dataloader)),\n","                    loss=losses,\n","                     grad_norm=grad_norm,\n","                     lr=scheduler.get_lr()[0],\n","                )\n","            )\n","\n","    del output, loss, scaler, grad_norm; gc.collect()\n","    torch.cuda.empty_cache()\n","    return losses.avg"]},{"cell_type":"code","execution_count":27,"id":"alleged-commonwealth","metadata":{"id":"alleged-commonwealth","executionInfo":{"status":"ok","timestamp":1647830356880,"user_tz":-540,"elapsed":10,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def valid_fn(\n","    val_dataloader,\n","    model,\n","    criterion,\n","    device,\n","):\n","    model.eval()\n","    preds = []\n","    losses = AverageMeter()\n","    start = time.time()\n","    for step, (inputs, labels, mappings_from_token_to_char) in enumerate(val_dataloader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device) \n","        batch_size = labels.size(0)\n","        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            output = model(inputs, mappings_from_token_to_char)\n","\n","        loss = criterion(output.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1)\n","        loss = loss.mean()\n","    \n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(val_dataloader)-1):\n","            print(\n","                \"EVAL: [{0}/{1}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                .format(\n","                    step, len(val_dataloader),\n","                    remain=timeSince(start, float(step+1) / len(val_dataloader)),\n","                    loss=losses,\n","                )\n","            )\n","    preds = np.concatenate(preds)\n","    return losses.avg, preds"]},{"cell_type":"code","execution_count":28,"id":"middle-determination","metadata":{"id":"middle-determination","executionInfo":{"status":"ok","timestamp":1647830356881,"user_tz":-540,"elapsed":10,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def inference_fn(test_dataloader, model, device):\n","    model.eval()\n","    model.to(device)\n","    preds = []\n","    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n","    for (inputs, mappings_from_token_to_char) in tk0:\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        mappings_from_token_to_char = mappings_from_token_to_char.to(device)\n","\n","        with torch.no_grad():\n","            output = model(inputs, mappings_from_token_to_char)\n","        preds.append(output.sigmoid().squeeze(2).detach().cpu().numpy())\n","    preds = np.concatenate(preds)\n","    return preds"]},{"cell_type":"code","execution_count":29,"id":"familiar-participation","metadata":{"id":"familiar-participation","executionInfo":{"status":"ok","timestamp":1647830356881,"user_tz":-540,"elapsed":10,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def train_loop(df, i_fold, device):\n","    print(f\"========== fold: {i_fold} training ==========\")\n","    train_idx = df[df[\"fold\"] != i_fold].index\n","    val_idx = df[df[\"fold\"] == i_fold].index\n","\n","    train_folds = df.loc[train_idx].reset_index(drop=True)\n","    val_folds = df.loc[val_idx].reset_index(drop=True)\n","\n","    train_dataset = TrainingDataset(CFG, train_folds)\n","    val_dataset = TrainingDataset(CFG, val_folds)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=True,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=True,\n","    )\n","    val_dataloader = DataLoader(\n","        val_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=False,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=False,\n","    )\n","\n","    # model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","    model = CustomModel(CFG, model_config_path=None, pretrained=False, i_fold=i_fold)   # itptを使うため\n","    torch.save(model.model_config, CFG.output_dir / \"model_config.pth\")\n","    model.to(device)\n","\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\"params\": [p for n, p in param_optimizer if not any(\n","            nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n","        {\"params\": [p for n, p in param_optimizer if any(\n","            nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n","    ]\n","    optimizer = AdamW(\n","        optimizer_grouped_parameters,\n","        lr=CFG.lr,\n","        betas=CFG.betas,\n","        weight_decay=CFG.weight_decay,\n","    )\n","    num_train_optimization_steps = int(len(train_dataloader) * CFG.epochs)\n","    num_warmup_steps = int(num_train_optimization_steps * CFG.num_warmup_steps_rate)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        num_training_steps=num_train_optimization_steps,\n","    )\n","\n","    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    best_score = -1 * np.inf\n","\n","    for epoch in range(CFG.epochs):\n","        start_time = time.time()\n","        avg_loss = train_fn(\n","            train_dataloader,\n","            model,\n","            criterion,\n","            optimizer,\n","            epoch,\n","            scheduler,\n","            device,\n","        )\n","        avg_val_loss, val_preds = valid_fn(\n","            val_dataloader,\n","            model,\n","            criterion,\n","            device,\n","        )\n","\n","        if isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n","            scheduler.step()\n","\n","        # scoring\n","        val_folds[[str(i) for i in range(CFG.max_char_len)]] = val_preds\n","        score = scoring(val_folds, th=0.5, use_token_prob=False)\n","\n","        elapsed = time.time() - start_time\n","\n","        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\")\n","        print(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n","        if score > best_score:\n","            best_score = score\n","            print(f\"Epoch {epoch+1} - Save Best Score: {score:.4f} Model\")\n","            torch.save({\n","                \"model\": model.state_dict(),\n","                \"predictions\": val_preds,\n","                },\n","                CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","            )\n","\n","    predictions = torch.load(\n","        CFG.output_dir / f\"fold{i_fold}_best.pth\",\n","        map_location=torch.device(\"cpu\"),\n","    )[\"predictions\"]\n","    val_folds[[str(i) for i in range(CFG.max_char_len)]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return val_folds"]},{"cell_type":"markdown","id":"coated-cameroon","metadata":{"id":"coated-cameroon"},"source":["## Main"]},{"cell_type":"code","execution_count":30,"id":"quality-expansion","metadata":{"id":"quality-expansion","executionInfo":{"status":"ok","timestamp":1647830356882,"user_tz":-540,"elapsed":11,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[],"source":["def main():\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    if CFG.train:\n","        oof_df = pd.DataFrame()\n","        for i_fold in range(CFG.n_fold):\n","            if i_fold in CFG.train_fold:\n","                _oof_df = train_loop(train, i_fold, device)\n","                oof_df = pd.concat([oof_df, _oof_df], axis=0, ignore_index=True)\n","        oof_df.to_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    if CFG.submission:\n","        oof_df = pd.read_pickle(Path(\"../input/\") / CFG.exp_name / \"oof_df.pkl\")\n","    else:\n","        oof_df = pd.read_pickle(CFG.output_dir / \"oof_df.pkl\")\n","\n","    score = scoring(oof_df, th=0.5, use_token_prob=False)\n","    print(f\"Best thres: 0.5, Score: {score:.4f}\")\n","    best_thres = get_best_thres(oof_df)\n","    score = scoring(oof_df, th=best_thres, use_token_prob=False)\n","    print(f\"Best thres: {best_thres}, Score: {score:.4f}\")\n","\n","    if CFG.inference:\n","        test_dataset = TestDataset(CFG, test)\n","        test_dataloader = DataLoader(\n","            test_dataset,\n","            batch_size=CFG.batch_size,\n","            shuffle=False,\n","            num_workers=CFG.num_workers,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","        predictions = []\n","        for i_fold in CFG.train_fold:\n","            if CFG.submission:\n","                model = CustomModel(CFG, model_config_path=Path(\"../input/\") / CFG.exp_name / \"model_config.pth\", pretrained=False)\n","                path = Path(\"../input/\") / CFG.exp_name / f\"fold{i_fold}_best.pth\"\n","            else:\n","                model = CustomModel(CFG, model_config_path=None, pretrained=True)\n","                path = CFG.output_dir / f\"fold{i_fold}_best.pth\"\n","\n","            state = torch.load(path, map_location=torch.device(\"cpu\"))\n","            model.load_state_dict(state[\"model\"])\n","            test_token_probs = inference_fn(test_dataloader, model, device)\n","            test[[f\"fold{i_fold}_{i}\" for i in range(CFG.max_len)]] = test_token_probs\n","            test_char_probs = get_char_probs(test[\"pn_history\"].values, test_token_probs, CFG.tokenizer)\n","            predictions.append(test_char_probs)\n","\n","            del state, test_token_probs, model; gc.collect()\n","            torch.cuda.empty_cache()\n","\n","        predictions = np.mean(predictions, axis=0)\n","        predicted_location_str = get_predicted_location_str(predictions, th=best_thres)\n","        test[CFG.target_col] = predicted_location_str\n","        test.to_csv(CFG.output_dir / \"raw_submission.csv\", index=False)\n","        test[[CFG.id_col, CFG.target_col]].to_csv(\n","            CFG.output_dir / \"submission.csv\", index=False\n","        )"]},{"cell_type":"code","execution_count":31,"id":"proprietary-civilian","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"proprietary-civilian","outputId":"14f4e231-9d3a-4547-82a1-aa9c03b642c1","executionInfo":{"status":"error","timestamp":1647847795461,"user_tz":-540,"elapsed":17438589,"user":{"displayName":"Shuhei Goda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08246931244224045522"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["========== fold: 0 training ==========\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp010/checkpoint-130170/pytorch_model.bin\n","Load weight from drive/MyDrive/00.kaggle/output/nbme-score-clinical-patient-notes/nbme-exp054/fold0_best.pth\n","Epoch: [1][0/10725] Elapsed 0m 1s (remain 239m 55s) Loss: 0.1161(0.1161) Grad: 30909.2227  LR: 0.000000  \n","Epoch: [1][100/10725] Elapsed 0m 51s (remain 91m 1s) Loss: 0.1150(0.1167) Grad: 32377.5469  LR: 0.000000  \n","Epoch: [1][200/10725] Elapsed 1m 40s (remain 87m 35s) Loss: 0.1105(0.1150) Grad: 29931.4512  LR: 0.000001  \n","Epoch: [1][300/10725] Elapsed 2m 28s (remain 85m 59s) Loss: 0.1008(0.1121) Grad: 28085.7363  LR: 0.000001  \n","Epoch: [1][400/10725] Elapsed 3m 17s (remain 84m 45s) Loss: 0.0906(0.1081) Grad: 25989.3906  LR: 0.000001  \n","Epoch: [1][500/10725] Elapsed 4m 6s (remain 83m 53s) Loss: 0.0770(0.1032) Grad: 22761.2988  LR: 0.000002  \n","Epoch: [1][600/10725] Elapsed 4m 55s (remain 83m 3s) Loss: 0.0636(0.0978) Grad: 19785.4160  LR: 0.000002  \n","Epoch: [1][700/10725] Elapsed 5m 45s (remain 82m 13s) Loss: 0.0492(0.0919) Grad: 17326.0195  LR: 0.000003  \n","Epoch: [1][800/10725] Elapsed 6m 33s (remain 81m 19s) Loss: 0.0387(0.0860) Grad: 13884.6143  LR: 0.000003  \n","Epoch: [1][900/10725] Elapsed 7m 23s (remain 80m 30s) Loss: 0.0360(0.0802) Grad: 10490.3281  LR: 0.000003  \n","Epoch: [1][1000/10725] Elapsed 8m 12s (remain 79m 48s) Loss: 0.0204(0.0748) Grad: 9241.7334  LR: 0.000004  \n","Epoch: [1][1100/10725] Elapsed 9m 2s (remain 79m 6s) Loss: 0.0141(0.0697) Grad: 7386.9517  LR: 0.000004  \n","Epoch: [1][1200/10725] Elapsed 9m 52s (remain 78m 17s) Loss: 0.0162(0.0650) Grad: 5267.5581  LR: 0.000004  \n","Epoch: [1][1300/10725] Elapsed 10m 41s (remain 77m 29s) Loss: 0.0092(0.0608) Grad: 4475.9180  LR: 0.000005  \n","Epoch: [1][1400/10725] Elapsed 11m 31s (remain 76m 42s) Loss: 0.0069(0.0570) Grad: 3445.6714  LR: 0.000005  \n","Epoch: [1][1500/10725] Elapsed 12m 21s (remain 75m 54s) Loss: 0.0040(0.0535) Grad: 1713.4614  LR: 0.000006  \n","Epoch: [1][1600/10725] Elapsed 13m 10s (remain 75m 5s) Loss: 0.0022(0.0504) Grad: 2174.3074  LR: 0.000006  \n","Epoch: [1][1700/10725] Elapsed 14m 0s (remain 74m 17s) Loss: 0.0019(0.0476) Grad: 1739.0968  LR: 0.000006  \n","Epoch: [1][1800/10725] Elapsed 14m 49s (remain 73m 28s) Loss: 0.0024(0.0451) Grad: 656.9896  LR: 0.000007  \n","Epoch: [1][1900/10725] Elapsed 15m 39s (remain 72m 39s) Loss: 0.0026(0.0428) Grad: 1514.0327  LR: 0.000007  \n","Epoch: [1][2000/10725] Elapsed 16m 28s (remain 71m 49s) Loss: 0.0002(0.0408) Grad: 996.3198  LR: 0.000007  \n","Epoch: [1][2100/10725] Elapsed 17m 17s (remain 70m 59s) Loss: 0.0002(0.0389) Grad: 150.4773  LR: 0.000008  \n","Epoch: [1][2200/10725] Elapsed 18m 6s (remain 70m 9s) Loss: 0.0002(0.0372) Grad: 1035.7279  LR: 0.000008  \n","Epoch: [1][2300/10725] Elapsed 18m 56s (remain 69m 19s) Loss: 0.0005(0.0357) Grad: 665.4427  LR: 0.000009  \n","Epoch: [1][2400/10725] Elapsed 19m 45s (remain 68m 29s) Loss: 0.0006(0.0342) Grad: 203.7815  LR: 0.000009  \n","Epoch: [1][2500/10725] Elapsed 20m 34s (remain 67m 38s) Loss: 0.0005(0.0329) Grad: 1044.2139  LR: 0.000009  \n","Epoch: [1][2600/10725] Elapsed 21m 23s (remain 66m 48s) Loss: 0.0003(0.0317) Grad: 1005.3959  LR: 0.000010  \n","Epoch: [1][2700/10725] Elapsed 22m 12s (remain 65m 58s) Loss: 0.0005(0.0306) Grad: 155.8249  LR: 0.000010  \n","Epoch: [1][2800/10725] Elapsed 23m 1s (remain 65m 7s) Loss: 0.0006(0.0296) Grad: 795.2715  LR: 0.000010  \n","Epoch: [1][2900/10725] Elapsed 23m 49s (remain 64m 16s) Loss: 0.0004(0.0286) Grad: 983.6774  LR: 0.000011  \n","Epoch: [1][3000/10725] Elapsed 24m 39s (remain 63m 27s) Loss: 0.0003(0.0277) Grad: 149.5024  LR: 0.000011  \n","Epoch: [1][3100/10725] Elapsed 25m 28s (remain 62m 38s) Loss: 0.0000(0.0269) Grad: 1004.4054  LR: 0.000012  \n","Epoch: [1][3200/10725] Elapsed 26m 17s (remain 61m 48s) Loss: 0.0006(0.0261) Grad: 517.9987  LR: 0.000012  \n","Epoch: [1][3300/10725] Elapsed 27m 6s (remain 60m 58s) Loss: 0.0000(0.0253) Grad: 17.0737  LR: 0.000012  \n","Epoch: [1][3400/10725] Elapsed 27m 56s (remain 60m 9s) Loss: 0.0003(0.0247) Grad: 315.0734  LR: 0.000013  \n","Epoch: [1][3500/10725] Elapsed 28m 45s (remain 59m 20s) Loss: 0.0126(0.0240) Grad: 196450.0000  LR: 0.000013  \n","Epoch: [1][3600/10725] Elapsed 29m 34s (remain 58m 30s) Loss: 0.0000(0.0234) Grad: 13.1032  LR: 0.000013  \n","Epoch: [1][3700/10725] Elapsed 30m 23s (remain 57m 40s) Loss: 0.0000(0.0228) Grad: 304.8391  LR: 0.000014  \n","Epoch: [1][3800/10725] Elapsed 31m 12s (remain 56m 51s) Loss: 0.0001(0.0222) Grad: 430.0714  LR: 0.000014  \n","Epoch: [1][3900/10725] Elapsed 32m 2s (remain 56m 2s) Loss: 0.0001(0.0216) Grad: 59.0638  LR: 0.000015  \n","Epoch: [1][4000/10725] Elapsed 32m 51s (remain 55m 13s) Loss: 0.0062(0.0211) Grad: 4609.2432  LR: 0.000015  \n","Epoch: [1][4100/10725] Elapsed 33m 40s (remain 54m 23s) Loss: 0.0020(0.0207) Grad: 5627.7168  LR: 0.000015  \n","Epoch: [1][4200/10725] Elapsed 34m 30s (remain 53m 34s) Loss: 0.0002(0.0202) Grad: 1602.0341  LR: 0.000016  \n","Epoch: [1][4300/10725] Elapsed 35m 19s (remain 52m 45s) Loss: 0.0008(0.0198) Grad: 1066.5872  LR: 0.000016  \n","Epoch: [1][4400/10725] Elapsed 36m 8s (remain 51m 55s) Loss: 0.0000(0.0194) Grad: 999.7014  LR: 0.000016  \n","Epoch: [1][4500/10725] Elapsed 36m 57s (remain 51m 6s) Loss: 0.0001(0.0189) Grad: 87.8127  LR: 0.000017  \n","Epoch: [1][4600/10725] Elapsed 37m 46s (remain 50m 16s) Loss: 0.0000(0.0186) Grad: 1000.1595  LR: 0.000017  \n","Epoch: [1][4700/10725] Elapsed 38m 34s (remain 49m 26s) Loss: 0.0000(0.0182) Grad: 51.9278  LR: 0.000018  \n","Epoch: [1][4800/10725] Elapsed 39m 23s (remain 48m 36s) Loss: 0.0015(0.0179) Grad: 258.5461  LR: 0.000018  \n","Epoch: [1][4900/10725] Elapsed 40m 12s (remain 47m 46s) Loss: 0.0000(0.0176) Grad: 311.3730  LR: 0.000018  \n","Epoch: [1][5000/10725] Elapsed 41m 1s (remain 46m 57s) Loss: 0.0001(0.0172) Grad: 36.9888  LR: 0.000019  \n","Epoch: [1][5100/10725] Elapsed 41m 50s (remain 46m 7s) Loss: 0.0000(0.0169) Grad: 5.3339  LR: 0.000019  \n","Epoch: [1][5200/10725] Elapsed 42m 39s (remain 45m 18s) Loss: 0.0013(0.0167) Grad: 1445.6298  LR: 0.000019  \n","Epoch: [1][5300/10725] Elapsed 43m 28s (remain 44m 29s) Loss: 0.0001(0.0164) Grad: 1005.1044  LR: 0.000020  \n","Epoch: [1][5400/10725] Elapsed 44m 17s (remain 43m 39s) Loss: 0.0000(0.0161) Grad: 5.6514  LR: 0.000020  \n","Epoch: [1][5500/10725] Elapsed 45m 6s (remain 42m 50s) Loss: 0.0001(0.0159) Grad: 1000.5602  LR: 0.000020  \n","Epoch: [1][5600/10725] Elapsed 45m 55s (remain 42m 1s) Loss: 0.0000(0.0156) Grad: 58.7404  LR: 0.000020  \n","Epoch: [1][5700/10725] Elapsed 46m 45s (remain 41m 12s) Loss: 0.0001(0.0154) Grad: 423.0789  LR: 0.000020  \n","Epoch: [1][5800/10725] Elapsed 47m 34s (remain 40m 23s) Loss: 0.0004(0.0151) Grad: 1940.2902  LR: 0.000020  \n","Epoch: [1][5900/10725] Elapsed 48m 24s (remain 39m 34s) Loss: 0.0001(0.0149) Grad: 1001.4796  LR: 0.000020  \n","Epoch: [1][6000/10725] Elapsed 49m 13s (remain 38m 45s) Loss: 0.0001(0.0147) Grad: 48.0008  LR: 0.000020  \n","Epoch: [1][6100/10725] Elapsed 50m 3s (remain 37m 56s) Loss: 0.0001(0.0145) Grad: 998.5424  LR: 0.000020  \n","Epoch: [1][6200/10725] Elapsed 50m 52s (remain 37m 7s) Loss: 0.0109(0.0143) Grad: 21964.0117  LR: 0.000020  \n","Epoch: [1][6300/10725] Elapsed 51m 41s (remain 36m 17s) Loss: 0.0001(0.0141) Grad: 29.8759  LR: 0.000020  \n","Epoch: [1][6400/10725] Elapsed 52m 31s (remain 35m 28s) Loss: 0.0001(0.0139) Grad: 994.5593  LR: 0.000020  \n","Epoch: [1][6500/10725] Elapsed 53m 20s (remain 34m 39s) Loss: 0.0000(0.0137) Grad: 112.9324  LR: 0.000020  \n","Epoch: [1][6600/10725] Elapsed 54m 9s (remain 33m 50s) Loss: 0.0001(0.0135) Grad: 40.9906  LR: 0.000019  \n","Epoch: [1][6700/10725] Elapsed 54m 58s (remain 33m 0s) Loss: 0.0000(0.0134) Grad: 638.6564  LR: 0.000019  \n","Epoch: [1][6800/10725] Elapsed 55m 48s (remain 32m 11s) Loss: 0.0001(0.0132) Grad: 35.8009  LR: 0.000019  \n","Epoch: [1][6900/10725] Elapsed 56m 37s (remain 31m 22s) Loss: 0.0000(0.0130) Grad: 3.1671  LR: 0.000019  \n","Epoch: [1][7000/10725] Elapsed 57m 26s (remain 30m 33s) Loss: 0.0005(0.0129) Grad: 1020.8481  LR: 0.000019  \n","Epoch: [1][7100/10725] Elapsed 58m 15s (remain 29m 43s) Loss: 0.0000(0.0127) Grad: 44.5300  LR: 0.000019  \n","Epoch: [1][7200/10725] Elapsed 59m 4s (remain 28m 54s) Loss: 0.0000(0.0125) Grad: 81.4140  LR: 0.000019  \n","Epoch: [1][7300/10725] Elapsed 59m 53s (remain 28m 5s) Loss: 0.0001(0.0124) Grad: 198.7327  LR: 0.000019  \n","Epoch: [1][7400/10725] Elapsed 60m 42s (remain 27m 15s) Loss: 0.0166(0.0123) Grad: 25812.5410  LR: 0.000019  \n","Epoch: [1][7500/10725] Elapsed 61m 31s (remain 26m 26s) Loss: 0.0001(0.0121) Grad: 47.2297  LR: 0.000019  \n","Epoch: [1][7600/10725] Elapsed 62m 21s (remain 25m 37s) Loss: 0.0000(0.0120) Grad: 1000.0740  LR: 0.000019  \n","Epoch: [1][7700/10725] Elapsed 63m 10s (remain 24m 48s) Loss: 0.0070(0.0118) Grad: 6642.6699  LR: 0.000019  \n","Epoch: [1][7800/10725] Elapsed 63m 59s (remain 23m 59s) Loss: 0.0000(0.0117) Grad: 4.8401  LR: 0.000019  \n","Epoch: [1][7900/10725] Elapsed 64m 49s (remain 23m 10s) Loss: 0.0277(0.0116) Grad: 67336.6172  LR: 0.000019  \n","Epoch: [1][8000/10725] Elapsed 65m 38s (remain 22m 21s) Loss: 0.0000(0.0115) Grad: 996.1086  LR: 0.000019  \n","Epoch: [1][8100/10725] Elapsed 66m 28s (remain 21m 31s) Loss: 0.0000(0.0113) Grad: 26.5755  LR: 0.000019  \n","Epoch: [1][8200/10725] Elapsed 67m 16s (remain 20m 42s) Loss: 0.0038(0.0112) Grad: 1717.5225  LR: 0.000019  \n","Epoch: [1][8300/10725] Elapsed 68m 5s (remain 19m 53s) Loss: 0.0001(0.0111) Grad: 1002.2728  LR: 0.000019  \n","Epoch: [1][8400/10725] Elapsed 68m 54s (remain 19m 3s) Loss: 0.0000(0.0110) Grad: 21.0262  LR: 0.000019  \n","Epoch: [1][8500/10725] Elapsed 69m 43s (remain 18m 14s) Loss: 0.0000(0.0109) Grad: 194.1803  LR: 0.000019  \n","Epoch: [1][8600/10725] Elapsed 70m 32s (remain 17m 25s) Loss: 0.0000(0.0108) Grad: 24.5550  LR: 0.000019  \n","Epoch: [1][8700/10725] Elapsed 71m 21s (remain 16m 35s) Loss: 0.0000(0.0107) Grad: 1.9514  LR: 0.000019  \n","Epoch: [1][8800/10725] Elapsed 72m 10s (remain 15m 46s) Loss: 0.0000(0.0106) Grad: 999.8834  LR: 0.000019  \n","Epoch: [1][8900/10725] Elapsed 72m 59s (remain 14m 57s) Loss: 0.0000(0.0105) Grad: 967.4290  LR: 0.000019  \n","Epoch: [1][9000/10725] Elapsed 73m 48s (remain 14m 8s) Loss: 0.0000(0.0104) Grad: 2.7295  LR: 0.000018  \n","Epoch: [1][9100/10725] Elapsed 74m 37s (remain 13m 18s) Loss: 0.0000(0.0103) Grad: 1000.0411  LR: 0.000018  \n","Epoch: [1][9200/10725] Elapsed 75m 26s (remain 12m 29s) Loss: 0.0000(0.0102) Grad: 106.2957  LR: 0.000018  \n","Epoch: [1][9300/10725] Elapsed 76m 16s (remain 11m 40s) Loss: 0.0001(0.0101) Grad: 106.3481  LR: 0.000018  \n","Epoch: [1][9400/10725] Elapsed 77m 5s (remain 10m 51s) Loss: 0.0000(0.0100) Grad: 1001.0242  LR: 0.000018  \n","Epoch: [1][9500/10725] Elapsed 77m 54s (remain 10m 2s) Loss: 0.0000(0.0099) Grad: 1000.0970  LR: 0.000018  \n","Epoch: [1][9600/10725] Elapsed 78m 43s (remain 9m 12s) Loss: 0.0000(0.0098) Grad: 37.1681  LR: 0.000018  \n","Epoch: [1][9700/10725] Elapsed 79m 32s (remain 8m 23s) Loss: 0.0000(0.0097) Grad: 1000.1747  LR: 0.000018  \n","Epoch: [1][9800/10725] Elapsed 80m 21s (remain 7m 34s) Loss: 0.0000(0.0096) Grad: 999.9138  LR: 0.000018  \n","Epoch: [1][9900/10725] Elapsed 81m 10s (remain 6m 45s) Loss: 0.0170(0.0096) Grad: 24389.1973  LR: 0.000018  \n","Epoch: [1][10000/10725] Elapsed 81m 59s (remain 5m 56s) Loss: 0.0001(0.0095) Grad: 1005.1011  LR: 0.000018  \n","Epoch: [1][10100/10725] Elapsed 82m 48s (remain 5m 6s) Loss: 0.0000(0.0094) Grad: 43.7584  LR: 0.000018  \n","Epoch: [1][10200/10725] Elapsed 83m 37s (remain 4m 17s) Loss: 0.0000(0.0093) Grad: 3.4254  LR: 0.000018  \n","Epoch: [1][10300/10725] Elapsed 84m 26s (remain 3m 28s) Loss: 0.0000(0.0093) Grad: 999.4750  LR: 0.000018  \n","Epoch: [1][10400/10725] Elapsed 85m 16s (remain 2m 39s) Loss: 0.0001(0.0092) Grad: 402.8713  LR: 0.000018  \n","Epoch: [1][10500/10725] Elapsed 86m 5s (remain 1m 50s) Loss: 0.0000(0.0091) Grad: 5.1205  LR: 0.000018  \n","Epoch: [1][10600/10725] Elapsed 86m 54s (remain 1m 0s) Loss: 0.0000(0.0090) Grad: 15.4701  LR: 0.000018  \n","Epoch: [1][10700/10725] Elapsed 87m 43s (remain 0m 11s) Loss: 0.0000(0.0090) Grad: 93.7550  LR: 0.000018  \n","Epoch: [1][10724/10725] Elapsed 87m 55s (remain 0m 0s) Loss: 0.0144(0.0090) Grad: 67909.5469  LR: 0.000018  \n","EVAL: [0/3575] Elapsed 0m 0s (remain 40m 44s) Loss: 0.0000(0.0000) \n","EVAL: [100/3575] Elapsed 0m 21s (remain 12m 31s) Loss: 0.0000(0.0028) \n","EVAL: [200/3575] Elapsed 0m 42s (remain 11m 55s) Loss: 0.0000(0.0030) \n","EVAL: [300/3575] Elapsed 1m 3s (remain 11m 32s) Loss: 0.0026(0.0028) \n","EVAL: [400/3575] Elapsed 1m 24s (remain 11m 10s) Loss: 0.0002(0.0030) \n","EVAL: [500/3575] Elapsed 1m 45s (remain 10m 49s) Loss: 0.0000(0.0030) \n","EVAL: [600/3575] Elapsed 2m 6s (remain 10m 26s) Loss: 0.0000(0.0029) \n","EVAL: [700/3575] Elapsed 2m 27s (remain 10m 5s) Loss: 0.0000(0.0029) \n","EVAL: [800/3575] Elapsed 2m 48s (remain 9m 43s) Loss: 0.0008(0.0029) \n","EVAL: [900/3575] Elapsed 3m 9s (remain 9m 22s) Loss: 0.0051(0.0029) \n","EVAL: [1000/3575] Elapsed 3m 30s (remain 9m 1s) Loss: 0.0000(0.0029) \n","EVAL: [1100/3575] Elapsed 3m 51s (remain 8m 40s) Loss: 0.0000(0.0029) \n","EVAL: [1200/3575] Elapsed 4m 12s (remain 8m 19s) Loss: 0.0000(0.0028) \n","EVAL: [1300/3575] Elapsed 4m 33s (remain 7m 58s) Loss: 0.0000(0.0027) \n","EVAL: [1400/3575] Elapsed 4m 54s (remain 7m 37s) Loss: 0.0005(0.0027) \n","EVAL: [1500/3575] Elapsed 5m 15s (remain 7m 15s) Loss: 0.0000(0.0027) \n","EVAL: [1600/3575] Elapsed 5m 36s (remain 6m 54s) Loss: 0.0028(0.0027) \n","EVAL: [1700/3575] Elapsed 5m 57s (remain 6m 33s) Loss: 0.0000(0.0027) \n","EVAL: [1800/3575] Elapsed 6m 18s (remain 6m 12s) Loss: 0.0002(0.0028) \n","EVAL: [1900/3575] Elapsed 6m 39s (remain 5m 51s) Loss: 0.0136(0.0030) \n","EVAL: [2000/3575] Elapsed 7m 0s (remain 5m 30s) Loss: 0.0000(0.0032) \n","EVAL: [2100/3575] Elapsed 7m 21s (remain 5m 9s) Loss: 0.0193(0.0034) \n","EVAL: [2200/3575] Elapsed 7m 42s (remain 4m 48s) Loss: 0.0001(0.0036) \n","EVAL: [2300/3575] Elapsed 8m 3s (remain 4m 27s) Loss: 0.0000(0.0036) \n","EVAL: [2400/3575] Elapsed 8m 24s (remain 4m 6s) Loss: 0.0004(0.0036) \n","EVAL: [2500/3575] Elapsed 8m 45s (remain 3m 45s) Loss: 0.0002(0.0035) \n","EVAL: [2600/3575] Elapsed 9m 6s (remain 3m 24s) Loss: 0.0000(0.0035) \n","EVAL: [2700/3575] Elapsed 9m 27s (remain 3m 3s) Loss: 0.0000(0.0035) \n","EVAL: [2800/3575] Elapsed 9m 48s (remain 2m 42s) Loss: 0.0000(0.0035) \n","EVAL: [2900/3575] Elapsed 10m 9s (remain 2m 21s) Loss: 0.0001(0.0035) \n","EVAL: [3000/3575] Elapsed 10m 30s (remain 2m 0s) Loss: 0.0000(0.0035) \n","EVAL: [3100/3575] Elapsed 10m 51s (remain 1m 39s) Loss: 0.0000(0.0035) \n","EVAL: [3200/3575] Elapsed 11m 12s (remain 1m 18s) Loss: 0.0000(0.0034) \n","EVAL: [3300/3575] Elapsed 11m 33s (remain 0m 57s) Loss: 0.0047(0.0034) \n","EVAL: [3400/3575] Elapsed 11m 54s (remain 0m 36s) Loss: 0.0053(0.0033) \n","EVAL: [3500/3575] Elapsed 12m 15s (remain 0m 15s) Loss: 0.0000(0.0033) \n","EVAL: [3574/3575] Elapsed 12m 31s (remain 0m 0s) Loss: 0.0000(0.0033) \n","Epoch 1 - avg_train_loss: 0.0090  avg_val_loss: 0.0033  time: 6030s\n","Epoch 1 - Score: 0.7180\n","Epoch 1 - Save Best Score: 0.7180 Model\n","Epoch: [2][0/10725] Elapsed 0m 0s (remain 156m 32s) Loss: 0.0000(0.0000) Grad: 998.8074  LR: 0.000018  \n","Epoch: [2][100/10725] Elapsed 0m 55s (remain 97m 7s) Loss: 0.0000(0.0010) Grad: 999.9344  LR: 0.000018  \n","Epoch: [2][200/10725] Elapsed 1m 44s (remain 91m 33s) Loss: 0.0001(0.0010) Grad: 94.6938  LR: 0.000018  \n","Epoch: [2][300/10725] Elapsed 2m 34s (remain 89m 1s) Loss: 0.0001(0.0011) Grad: 134.7194  LR: 0.000018  \n","Epoch: [2][400/10725] Elapsed 3m 23s (remain 87m 21s) Loss: 0.0000(0.0011) Grad: 1000.1676  LR: 0.000018  \n","Epoch: [2][500/10725] Elapsed 4m 13s (remain 86m 6s) Loss: 0.0000(0.0011) Grad: 33.7737  LR: 0.000018  \n","Epoch: [2][600/10725] Elapsed 5m 2s (remain 84m 58s) Loss: 0.0000(0.0013) Grad: 1.4826  LR: 0.000018  \n","Epoch: [2][700/10725] Elapsed 5m 52s (remain 83m 55s) Loss: 0.0000(0.0013) Grad: 67.0617  LR: 0.000017  \n","Epoch: [2][800/10725] Elapsed 6m 42s (remain 83m 1s) Loss: 0.0002(0.0013) Grad: 516.0300  LR: 0.000017  \n","Epoch: [2][900/10725] Elapsed 7m 31s (remain 82m 3s) Loss: 0.0019(0.0013) Grad: 55430.0859  LR: 0.000017  \n","Epoch: [2][1000/10725] Elapsed 8m 21s (remain 81m 8s) Loss: 0.0000(0.0013) Grad: 991.7920  LR: 0.000017  \n","Epoch: [2][1100/10725] Elapsed 9m 10s (remain 80m 15s) Loss: 0.0001(0.0014) Grad: 997.5926  LR: 0.000017  \n","Epoch: [2][1200/10725] Elapsed 10m 0s (remain 79m 20s) Loss: 0.0003(0.0013) Grad: 2612.4983  LR: 0.000017  \n","Epoch: [2][1300/10725] Elapsed 10m 49s (remain 78m 27s) Loss: 0.0000(0.0014) Grad: 998.5325  LR: 0.000017  \n","Epoch: [2][1400/10725] Elapsed 11m 38s (remain 77m 31s) Loss: 0.0000(0.0014) Grad: 71.8604  LR: 0.000017  \n","Epoch: [2][1500/10725] Elapsed 12m 28s (remain 76m 39s) Loss: 0.0000(0.0014) Grad: 17.3987  LR: 0.000017  \n","Epoch: [2][1600/10725] Elapsed 13m 18s (remain 75m 48s) Loss: 0.1230(0.0014) Grad: 61612.1211  LR: 0.000017  \n","Epoch: [2][1700/10725] Elapsed 14m 7s (remain 74m 57s) Loss: 0.0094(0.0014) Grad: 1706.5331  LR: 0.000017  \n","Epoch: [2][1800/10725] Elapsed 14m 57s (remain 74m 6s) Loss: 0.0001(0.0014) Grad: 87.9228  LR: 0.000017  \n","Epoch: [2][1900/10725] Elapsed 15m 47s (remain 73m 16s) Loss: 0.0002(0.0014) Grad: 1103.1763  LR: 0.000017  \n","Epoch: [2][2000/10725] Elapsed 16m 36s (remain 72m 25s) Loss: 0.0000(0.0013) Grad: 1000.0233  LR: 0.000017  \n","Epoch: [2][2100/10725] Elapsed 17m 26s (remain 71m 35s) Loss: 0.0000(0.0014) Grad: 16.6338  LR: 0.000017  \n","Epoch: [2][2200/10725] Elapsed 18m 15s (remain 70m 43s) Loss: 0.0001(0.0014) Grad: 648.5220  LR: 0.000017  \n","Epoch: [2][2300/10725] Elapsed 19m 5s (remain 69m 53s) Loss: 0.0000(0.0014) Grad: 1002.1475  LR: 0.000017  \n","Epoch: [2][2400/10725] Elapsed 19m 55s (remain 69m 3s) Loss: 0.0000(0.0013) Grad: 1.2797  LR: 0.000017  \n","Epoch: [2][2500/10725] Elapsed 20m 44s (remain 68m 12s) Loss: 0.0000(0.0013) Grad: 29.1141  LR: 0.000017  \n","Epoch: [2][2600/10725] Elapsed 21m 34s (remain 67m 22s) Loss: 0.0000(0.0013) Grad: 999.8518  LR: 0.000017  \n","Epoch: [2][2700/10725] Elapsed 22m 23s (remain 66m 31s) Loss: 0.0008(0.0014) Grad: 12363.3662  LR: 0.000017  \n","Epoch: [2][2800/10725] Elapsed 23m 13s (remain 65m 41s) Loss: 0.0000(0.0013) Grad: 924.5532  LR: 0.000017  \n","Epoch: [2][2900/10725] Elapsed 24m 2s (remain 64m 50s) Loss: 0.0000(0.0013) Grad: 46.0469  LR: 0.000017  \n","Epoch: [2][3000/10725] Elapsed 24m 52s (remain 64m 0s) Loss: 0.0000(0.0013) Grad: 0.7761  LR: 0.000017  \n","Epoch: [2][3100/10725] Elapsed 25m 41s (remain 63m 8s) Loss: 0.0000(0.0014) Grad: 986.7637  LR: 0.000016  \n","Epoch: [2][3200/10725] Elapsed 26m 30s (remain 62m 19s) Loss: 0.0000(0.0013) Grad: 64.7483  LR: 0.000016  \n","Epoch: [2][3300/10725] Elapsed 27m 20s (remain 61m 29s) Loss: 0.0001(0.0014) Grad: 169.6540  LR: 0.000016  \n","Epoch: [2][3400/10725] Elapsed 28m 11s (remain 60m 43s) Loss: 0.0000(0.0014) Grad: 999.8698  LR: 0.000016  \n","Epoch: [2][3500/10725] Elapsed 29m 1s (remain 59m 53s) Loss: 0.0001(0.0014) Grad: 47.1800  LR: 0.000016  \n","Epoch: [2][3600/10725] Elapsed 29m 50s (remain 59m 2s) Loss: 0.0001(0.0014) Grad: 275.7415  LR: 0.000016  \n","Epoch: [2][3700/10725] Elapsed 30m 40s (remain 58m 12s) Loss: 0.0000(0.0013) Grad: 35.1277  LR: 0.000016  \n","Epoch: [2][3800/10725] Elapsed 31m 29s (remain 57m 21s) Loss: 0.0125(0.0014) Grad: 26194.5977  LR: 0.000016  \n","Epoch: [2][3900/10725] Elapsed 32m 18s (remain 56m 31s) Loss: 0.0000(0.0014) Grad: 10.1640  LR: 0.000016  \n","Epoch: [2][4000/10725] Elapsed 33m 7s (remain 55m 40s) Loss: 0.0081(0.0014) Grad: 30200.7871  LR: 0.000016  \n","Epoch: [2][4100/10725] Elapsed 33m 57s (remain 54m 50s) Loss: 0.0000(0.0014) Grad: 92.1967  LR: 0.000016  \n","Epoch: [2][4200/10725] Elapsed 34m 46s (remain 54m 0s) Loss: 0.0000(0.0013) Grad: 10.1469  LR: 0.000016  \n","Epoch: [2][4300/10725] Elapsed 35m 35s (remain 53m 10s) Loss: 0.0006(0.0014) Grad: 3496.1321  LR: 0.000016  \n","Epoch: [2][4400/10725] Elapsed 36m 25s (remain 52m 19s) Loss: 0.0001(0.0014) Grad: 1015.7667  LR: 0.000016  \n","Epoch: [2][4500/10725] Elapsed 37m 14s (remain 51m 30s) Loss: 0.0080(0.0013) Grad: 33410.3008  LR: 0.000016  \n","Epoch: [2][4600/10725] Elapsed 38m 4s (remain 50m 40s) Loss: 0.0001(0.0013) Grad: 1021.2009  LR: 0.000016  \n","Epoch: [2][4700/10725] Elapsed 38m 53s (remain 49m 50s) Loss: 0.0000(0.0013) Grad: 998.9543  LR: 0.000016  \n","Epoch: [2][4800/10725] Elapsed 39m 42s (remain 48m 59s) Loss: 0.0001(0.0013) Grad: 38.9175  LR: 0.000016  \n","Epoch: [2][4900/10725] Elapsed 40m 31s (remain 48m 9s) Loss: 0.0001(0.0013) Grad: 1001.2870  LR: 0.000016  \n","Epoch: [2][5000/10725] Elapsed 41m 20s (remain 47m 19s) Loss: 0.0008(0.0013) Grad: 2393.6125  LR: 0.000016  \n","Epoch: [2][5100/10725] Elapsed 42m 10s (remain 46m 29s) Loss: 0.0000(0.0013) Grad: 10.7554  LR: 0.000016  \n","Epoch: [2][5200/10725] Elapsed 42m 59s (remain 45m 39s) Loss: 0.0000(0.0013) Grad: 999.2894  LR: 0.000016  \n","Epoch: [2][5300/10725] Elapsed 43m 48s (remain 44m 49s) Loss: 0.0002(0.0013) Grad: 1171.6829  LR: 0.000016  \n","Epoch: [2][5400/10725] Elapsed 44m 38s (remain 44m 0s) Loss: 0.0078(0.0013) Grad: 17476.8008  LR: 0.000016  \n","Epoch: [2][5500/10725] Elapsed 45m 27s (remain 43m 10s) Loss: 0.0005(0.0013) Grad: 1074.0627  LR: 0.000015  \n","Epoch: [2][5600/10725] Elapsed 46m 16s (remain 42m 20s) Loss: 0.0000(0.0013) Grad: 492.4178  LR: 0.000015  \n","Epoch: [2][5700/10725] Elapsed 47m 6s (remain 41m 30s) Loss: 0.0001(0.0014) Grad: 118.3352  LR: 0.000015  \n","Epoch: [2][5800/10725] Elapsed 47m 55s (remain 40m 41s) Loss: 0.0000(0.0013) Grad: 284.3466  LR: 0.000015  \n","Epoch: [2][5900/10725] Elapsed 48m 45s (remain 39m 51s) Loss: 0.0001(0.0013) Grad: 1028.0867  LR: 0.000015  \n","Epoch: [2][6000/10725] Elapsed 49m 34s (remain 39m 1s) Loss: 0.0000(0.0013) Grad: 28.3565  LR: 0.000015  \n","Epoch: [2][6100/10725] Elapsed 50m 23s (remain 38m 11s) Loss: 0.0001(0.0013) Grad: 345.6464  LR: 0.000015  \n","Epoch: [2][6200/10725] Elapsed 51m 12s (remain 37m 21s) Loss: 0.0000(0.0014) Grad: 28.1496  LR: 0.000015  \n","Epoch: [2][6300/10725] Elapsed 52m 1s (remain 36m 31s) Loss: 0.0000(0.0013) Grad: 1.4078  LR: 0.000015  \n","Epoch: [2][6400/10725] Elapsed 52m 50s (remain 35m 41s) Loss: 0.0000(0.0013) Grad: 29.3774  LR: 0.000015  \n","Epoch: [2][6500/10725] Elapsed 53m 40s (remain 34m 52s) Loss: 0.0000(0.0013) Grad: 32.4403  LR: 0.000015  \n","Epoch: [2][6600/10725] Elapsed 54m 29s (remain 34m 2s) Loss: 0.0000(0.0013) Grad: 0.8436  LR: 0.000015  \n","Epoch: [2][6700/10725] Elapsed 55m 18s (remain 33m 12s) Loss: 0.0015(0.0014) Grad: 10240.7939  LR: 0.000015  \n","Epoch: [2][6800/10725] Elapsed 56m 8s (remain 32m 23s) Loss: 0.0001(0.0014) Grad: 118.8673  LR: 0.000015  \n","Epoch: [2][6900/10725] Elapsed 56m 57s (remain 31m 33s) Loss: 0.0001(0.0013) Grad: 120.5331  LR: 0.000015  \n","Epoch: [2][7000/10725] Elapsed 57m 46s (remain 30m 44s) Loss: 0.0035(0.0013) Grad: 1897.0439  LR: 0.000015  \n","Epoch: [2][7100/10725] Elapsed 58m 36s (remain 29m 54s) Loss: 0.0000(0.0013) Grad: 61.1078  LR: 0.000015  \n","Epoch: [2][7200/10725] Elapsed 59m 26s (remain 29m 5s) Loss: 0.0000(0.0013) Grad: 11.5321  LR: 0.000015  \n","Epoch: [2][7300/10725] Elapsed 60m 17s (remain 28m 16s) Loss: 0.0000(0.0013) Grad: 1000.2205  LR: 0.000015  \n","Epoch: [2][7400/10725] Elapsed 61m 6s (remain 27m 26s) Loss: 0.0000(0.0013) Grad: 1000.0463  LR: 0.000015  \n","Epoch: [2][7500/10725] Elapsed 61m 56s (remain 26m 37s) Loss: 0.0000(0.0013) Grad: 9.0730  LR: 0.000015  \n","Epoch: [2][7600/10725] Elapsed 62m 45s (remain 25m 47s) Loss: 0.0000(0.0013) Grad: 997.6983  LR: 0.000015  \n","Epoch: [2][7700/10725] Elapsed 63m 34s (remain 24m 57s) Loss: 0.0000(0.0013) Grad: 24.9525  LR: 0.000015  \n","Epoch: [2][7800/10725] Elapsed 64m 24s (remain 24m 8s) Loss: 0.0000(0.0013) Grad: 1.0163  LR: 0.000015  \n","Epoch: [2][7900/10725] Elapsed 65m 13s (remain 23m 18s) Loss: 0.0000(0.0013) Grad: 1001.2803  LR: 0.000015  \n","Epoch: [2][8000/10725] Elapsed 66m 2s (remain 22m 29s) Loss: 0.0018(0.0013) Grad: 27996.5176  LR: 0.000014  \n","Epoch: [2][8100/10725] Elapsed 66m 51s (remain 21m 39s) Loss: 0.0000(0.0013) Grad: 15.7148  LR: 0.000014  \n","Epoch: [2][8200/10725] Elapsed 67m 41s (remain 20m 49s) Loss: 0.0000(0.0013) Grad: 1000.0791  LR: 0.000014  \n","Epoch: [2][8300/10725] Elapsed 68m 30s (remain 20m 0s) Loss: 0.0000(0.0013) Grad: 166.0641  LR: 0.000014  \n","Epoch: [2][8400/10725] Elapsed 69m 20s (remain 19m 10s) Loss: 0.0000(0.0013) Grad: 1.7272  LR: 0.000014  \n","Epoch: [2][8500/10725] Elapsed 70m 10s (remain 18m 21s) Loss: 0.0000(0.0013) Grad: 1000.4684  LR: 0.000014  \n","Epoch: [2][8600/10725] Elapsed 70m 58s (remain 17m 31s) Loss: 0.0101(0.0013) Grad: 8416.7842  LR: 0.000014  \n","Epoch: [2][8700/10725] Elapsed 71m 48s (remain 16m 42s) Loss: 0.0098(0.0013) Grad: 9548.6553  LR: 0.000014  \n","Epoch: [2][8800/10725] Elapsed 72m 36s (remain 15m 52s) Loss: 0.0001(0.0013) Grad: 1001.3677  LR: 0.000014  \n","Epoch: [2][8900/10725] Elapsed 73m 25s (remain 15m 2s) Loss: 0.0000(0.0013) Grad: 39.3275  LR: 0.000014  \n","Epoch: [2][9000/10725] Elapsed 74m 14s (remain 14m 13s) Loss: 0.0020(0.0013) Grad: 48285.7656  LR: 0.000014  \n","Epoch: [2][9100/10725] Elapsed 75m 3s (remain 13m 23s) Loss: 0.0001(0.0013) Grad: 979.4730  LR: 0.000014  \n","Epoch: [2][9200/10725] Elapsed 75m 51s (remain 12m 33s) Loss: 0.0001(0.0013) Grad: 1001.6035  LR: 0.000014  \n","Epoch: [2][9300/10725] Elapsed 76m 41s (remain 11m 44s) Loss: 0.0000(0.0013) Grad: 18.9829  LR: 0.000014  \n","Epoch: [2][9400/10725] Elapsed 77m 29s (remain 10m 54s) Loss: 0.0107(0.0013) Grad: 38194.4570  LR: 0.000014  \n","Epoch: [2][9500/10725] Elapsed 78m 18s (remain 10m 5s) Loss: 0.0001(0.0013) Grad: 105.6975  LR: 0.000014  \n","Epoch: [2][9600/10725] Elapsed 79m 7s (remain 9m 15s) Loss: 0.0000(0.0013) Grad: 23.9190  LR: 0.000014  \n","Epoch: [2][9700/10725] Elapsed 79m 56s (remain 8m 26s) Loss: 0.0000(0.0013) Grad: 999.0270  LR: 0.000014  \n","Epoch: [2][9800/10725] Elapsed 80m 45s (remain 7m 36s) Loss: 0.0000(0.0013) Grad: 207.7619  LR: 0.000014  \n","Epoch: [2][9900/10725] Elapsed 81m 35s (remain 6m 47s) Loss: 0.0000(0.0013) Grad: 8.8427  LR: 0.000014  \n","Epoch: [2][10000/10725] Elapsed 82m 24s (remain 5m 57s) Loss: 0.0000(0.0013) Grad: 999.2355  LR: 0.000014  \n","Epoch: [2][10100/10725] Elapsed 83m 13s (remain 5m 8s) Loss: 0.0002(0.0013) Grad: 1055.2271  LR: 0.000014  \n","Epoch: [2][10200/10725] Elapsed 84m 2s (remain 4m 18s) Loss: 0.0000(0.0013) Grad: 16.5024  LR: 0.000014  \n","Epoch: [2][10300/10725] Elapsed 84m 50s (remain 3m 29s) Loss: 0.0000(0.0013) Grad: 997.0536  LR: 0.000014  \n","Epoch: [2][10400/10725] Elapsed 85m 39s (remain 2m 40s) Loss: 0.0000(0.0013) Grad: 992.9105  LR: 0.000013  \n","Epoch: [2][10500/10725] Elapsed 86m 28s (remain 1m 50s) Loss: 0.0014(0.0013) Grad: 402.9396  LR: 0.000013  \n","Epoch: [2][10600/10725] Elapsed 87m 17s (remain 1m 1s) Loss: 0.0001(0.0013) Grad: 1010.1453  LR: 0.000013  \n","Epoch: [2][10700/10725] Elapsed 88m 6s (remain 0m 11s) Loss: 0.0000(0.0013) Grad: 999.9325  LR: 0.000013  \n","Epoch: [2][10724/10725] Elapsed 88m 17s (remain 0m 0s) Loss: 0.0000(0.0013) Grad: 215.7927  LR: 0.000013  \n","EVAL: [0/3575] Elapsed 0m 0s (remain 34m 5s) Loss: 0.0000(0.0000) \n","EVAL: [100/3575] Elapsed 0m 21s (remain 12m 17s) Loss: 0.0000(0.0027) \n","EVAL: [200/3575] Elapsed 0m 42s (remain 11m 47s) Loss: 0.0000(0.0027) \n","EVAL: [300/3575] Elapsed 1m 2s (remain 11m 22s) Loss: 0.0037(0.0024) \n","EVAL: [400/3575] Elapsed 1m 23s (remain 10m 59s) Loss: 0.0001(0.0033) \n","EVAL: [500/3575] Elapsed 1m 44s (remain 10m 38s) Loss: 0.0000(0.0034) \n","EVAL: [600/3575] Elapsed 2m 4s (remain 10m 15s) Loss: 0.0008(0.0033) \n","EVAL: [700/3575] Elapsed 2m 25s (remain 9m 55s) Loss: 0.0000(0.0033) \n","EVAL: [800/3575] Elapsed 2m 45s (remain 9m 34s) Loss: 0.0011(0.0032) \n","EVAL: [900/3575] Elapsed 3m 6s (remain 9m 13s) Loss: 0.0078(0.0032) \n","EVAL: [1000/3575] Elapsed 3m 27s (remain 8m 52s) Loss: 0.0000(0.0033) \n","EVAL: [1100/3575] Elapsed 3m 47s (remain 8m 31s) Loss: 0.0000(0.0032) \n","EVAL: [1200/3575] Elapsed 4m 8s (remain 8m 11s) Loss: 0.0000(0.0031) \n","EVAL: [1300/3575] Elapsed 4m 29s (remain 7m 50s) Loss: 0.0000(0.0030) \n","EVAL: [1400/3575] Elapsed 4m 49s (remain 7m 29s) Loss: 0.0004(0.0029) \n","EVAL: [1500/3575] Elapsed 5m 10s (remain 7m 8s) Loss: 0.0000(0.0028) \n","EVAL: [1600/3575] Elapsed 5m 31s (remain 6m 48s) Loss: 0.0000(0.0029) \n","EVAL: [1700/3575] Elapsed 5m 51s (remain 6m 27s) Loss: 0.0000(0.0029) \n","EVAL: [1800/3575] Elapsed 6m 12s (remain 6m 6s) Loss: 0.0002(0.0029) \n","EVAL: [1900/3575] Elapsed 6m 33s (remain 5m 46s) Loss: 0.0189(0.0030) \n","EVAL: [2000/3575] Elapsed 6m 53s (remain 5m 25s) Loss: 0.0000(0.0031) \n","EVAL: [2100/3575] Elapsed 7m 14s (remain 5m 4s) Loss: 0.0267(0.0033) \n","EVAL: [2200/3575] Elapsed 7m 34s (remain 4m 43s) Loss: 0.0000(0.0035) \n","EVAL: [2300/3575] Elapsed 7m 55s (remain 4m 23s) Loss: 0.0000(0.0034) \n","EVAL: [2400/3575] Elapsed 8m 16s (remain 4m 2s) Loss: 0.0040(0.0034) \n","EVAL: [2500/3575] Elapsed 8m 37s (remain 3m 42s) Loss: 0.0013(0.0034) \n","EVAL: [2600/3575] Elapsed 8m 58s (remain 3m 21s) Loss: 0.0000(0.0033) \n","EVAL: [2700/3575] Elapsed 9m 18s (remain 3m 0s) Loss: 0.0000(0.0033) \n","EVAL: [2800/3575] Elapsed 9m 39s (remain 2m 40s) Loss: 0.0000(0.0033) \n","EVAL: [2900/3575] Elapsed 10m 0s (remain 2m 19s) Loss: 0.0000(0.0033) \n","EVAL: [3000/3575] Elapsed 10m 20s (remain 1m 58s) Loss: 0.0000(0.0032) \n","EVAL: [3100/3575] Elapsed 10m 41s (remain 1m 38s) Loss: 0.0000(0.0032) \n","EVAL: [3200/3575] Elapsed 11m 2s (remain 1m 17s) Loss: 0.0000(0.0032) \n","EVAL: [3300/3575] Elapsed 11m 23s (remain 0m 56s) Loss: 0.0049(0.0031) \n","EVAL: [3400/3575] Elapsed 11m 43s (remain 0m 36s) Loss: 0.0048(0.0031) \n","EVAL: [3500/3575] Elapsed 12m 4s (remain 0m 15s) Loss: 0.0000(0.0031) \n","EVAL: [3574/3575] Elapsed 12m 19s (remain 0m 0s) Loss: 0.0000(0.0030) \n","Epoch 2 - avg_train_loss: 0.0013  avg_val_loss: 0.0030  time: 6040s\n","Epoch 2 - Score: 0.8623\n","Epoch 2 - Save Best Score: 0.8623 Model\n","Epoch: [3][0/10725] Elapsed 0m 0s (remain 160m 0s) Loss: 0.0065(0.0065) Grad: 5494.6313  LR: 0.000013  \n","Epoch: [3][100/10725] Elapsed 0m 54s (remain 95m 58s) Loss: 0.0000(0.0007) Grad: 95.0835  LR: 0.000013  \n","Epoch: [3][200/10725] Elapsed 1m 43s (remain 90m 23s) Loss: 0.0000(0.0008) Grad: 327.3958  LR: 0.000013  \n","Epoch: [3][300/10725] Elapsed 2m 32s (remain 88m 13s) Loss: 0.0137(0.0009) Grad: 55812.7734  LR: 0.000013  \n","Epoch: [3][400/10725] Elapsed 3m 21s (remain 86m 29s) Loss: 0.0014(0.0009) Grad: 15665.5420  LR: 0.000013  \n","Epoch: [3][500/10725] Elapsed 4m 10s (remain 85m 16s) Loss: 0.0000(0.0009) Grad: 1000.0175  LR: 0.000013  \n","Epoch: [3][600/10725] Elapsed 5m 1s (remain 84m 38s) Loss: 0.0008(0.0009) Grad: 7513.5610  LR: 0.000013  \n","Epoch: [3][700/10725] Elapsed 5m 50s (remain 83m 31s) Loss: 0.0000(0.0008) Grad: 1000.6204  LR: 0.000013  \n","Epoch: [3][800/10725] Elapsed 6m 39s (remain 82m 32s) Loss: 0.0000(0.0008) Grad: 96.2799  LR: 0.000013  \n","Epoch: [3][900/10725] Elapsed 7m 28s (remain 81m 32s) Loss: 0.0000(0.0008) Grad: 12.5158  LR: 0.000013  \n","Epoch: [3][1000/10725] Elapsed 8m 17s (remain 80m 33s) Loss: 0.0000(0.0008) Grad: 58.5466  LR: 0.000013  \n","Epoch: [3][1100/10725] Elapsed 9m 6s (remain 79m 38s) Loss: 0.0000(0.0007) Grad: 80.4127  LR: 0.000013  \n","Epoch: [3][1200/10725] Elapsed 9m 55s (remain 78m 45s) Loss: 0.0000(0.0008) Grad: 12.2073  LR: 0.000013  \n","Epoch: [3][1300/10725] Elapsed 10m 44s (remain 77m 51s) Loss: 0.0000(0.0008) Grad: 406.4099  LR: 0.000013  \n","Epoch: [3][1400/10725] Elapsed 11m 34s (remain 76m 59s) Loss: 0.0112(0.0009) Grad: 59074.0039  LR: 0.000013  \n","Epoch: [3][1500/10725] Elapsed 12m 23s (remain 76m 6s) Loss: 0.0001(0.0009) Grad: 90.2906  LR: 0.000013  \n","Epoch: [3][1600/10725] Elapsed 13m 12s (remain 75m 15s) Loss: 0.0000(0.0010) Grad: 999.9301  LR: 0.000013  \n","Epoch: [3][1700/10725] Elapsed 14m 1s (remain 74m 22s) Loss: 0.0000(0.0009) Grad: 985.9478  LR: 0.000013  \n","Epoch: [3][1800/10725] Elapsed 14m 50s (remain 73m 32s) Loss: 0.0091(0.0010) Grad: 150309.2812  LR: 0.000013  \n","Epoch: [3][1900/10725] Elapsed 15m 39s (remain 72m 40s) Loss: 0.0000(0.0010) Grad: 153.8662  LR: 0.000013  \n","Epoch: [3][2000/10725] Elapsed 16m 28s (remain 71m 48s) Loss: 0.0000(0.0009) Grad: 988.1696  LR: 0.000013  \n","Epoch: [3][2100/10725] Elapsed 17m 17s (remain 70m 59s) Loss: 0.0000(0.0009) Grad: 26.6679  LR: 0.000012  \n","Epoch: [3][2200/10725] Elapsed 18m 6s (remain 70m 8s) Loss: 0.0000(0.0009) Grad: 1000.0094  LR: 0.000012  \n","Epoch: [3][2300/10725] Elapsed 18m 55s (remain 69m 18s) Loss: 0.0010(0.0010) Grad: 1341.8334  LR: 0.000012  \n","Epoch: [3][2400/10725] Elapsed 19m 44s (remain 68m 27s) Loss: 0.0000(0.0010) Grad: 1.4323  LR: 0.000012  \n","Epoch: [3][2500/10725] Elapsed 20m 33s (remain 67m 37s) Loss: 0.0000(0.0009) Grad: 1000.0685  LR: 0.000012  \n","Epoch: [3][2600/10725] Elapsed 21m 22s (remain 66m 46s) Loss: 0.0002(0.0010) Grad: 460.3053  LR: 0.000012  \n","Epoch: [3][2700/10725] Elapsed 22m 12s (remain 65m 57s) Loss: 0.0000(0.0010) Grad: 0.6928  LR: 0.000012  \n","Epoch: [3][2800/10725] Elapsed 23m 1s (remain 65m 7s) Loss: 0.0027(0.0010) Grad: 86403.7031  LR: 0.000012  \n","Epoch: [3][2900/10725] Elapsed 23m 50s (remain 64m 16s) Loss: 0.0036(0.0010) Grad: 30361.2695  LR: 0.000012  \n","Epoch: [3][3000/10725] Elapsed 24m 39s (remain 63m 27s) Loss: 0.0650(0.0010) Grad: 146095.1406  LR: 0.000012  \n","Epoch: [3][3100/10725] Elapsed 25m 28s (remain 62m 38s) Loss: 0.0000(0.0010) Grad: 39.2996  LR: 0.000012  \n","Epoch: [3][3200/10725] Elapsed 26m 17s (remain 61m 48s) Loss: 0.0009(0.0010) Grad: 14234.6689  LR: 0.000012  \n","Epoch: [3][3300/10725] Elapsed 27m 6s (remain 60m 58s) Loss: 0.0061(0.0010) Grad: 49096.1289  LR: 0.000012  \n","Epoch: [3][3400/10725] Elapsed 27m 56s (remain 60m 9s) Loss: 0.0000(0.0010) Grad: 1000.1904  LR: 0.000012  \n","Epoch: [3][3500/10725] Elapsed 28m 45s (remain 59m 20s) Loss: 0.0004(0.0010) Grad: 1306.0872  LR: 0.000012  \n","Epoch: [3][3600/10725] Elapsed 29m 34s (remain 58m 31s) Loss: 0.0000(0.0010) Grad: 5.6107  LR: 0.000012  \n","Epoch: [3][3700/10725] Elapsed 30m 23s (remain 57m 40s) Loss: 0.0000(0.0010) Grad: 275.0388  LR: 0.000012  \n","Epoch: [3][3800/10725] Elapsed 31m 12s (remain 56m 50s) Loss: 0.0000(0.0010) Grad: 998.3972  LR: 0.000012  \n","Epoch: [3][3900/10725] Elapsed 32m 1s (remain 56m 1s) Loss: 0.0000(0.0010) Grad: 3.9803  LR: 0.000012  \n","Epoch: [3][4000/10725] Elapsed 32m 50s (remain 55m 11s) Loss: 0.0061(0.0009) Grad: 9135.7158  LR: 0.000012  \n","Epoch: [3][4100/10725] Elapsed 33m 39s (remain 54m 21s) Loss: 0.0001(0.0009) Grad: 191.7161  LR: 0.000012  \n","Epoch: [3][4200/10725] Elapsed 34m 28s (remain 53m 31s) Loss: 0.0000(0.0009) Grad: 12.3622  LR: 0.000012  \n","Epoch: [3][4300/10725] Elapsed 35m 17s (remain 52m 42s) Loss: 0.0000(0.0010) Grad: 16.2659  LR: 0.000012  \n","Epoch: [3][4400/10725] Elapsed 36m 6s (remain 51m 52s) Loss: 0.0011(0.0009) Grad: 3657.1997  LR: 0.000012  \n","Epoch: [3][4500/10725] Elapsed 36m 55s (remain 51m 3s) Loss: 0.0052(0.0010) Grad: 30715.7344  LR: 0.000011  \n","Epoch: [3][4600/10725] Elapsed 37m 44s (remain 50m 13s) Loss: 0.0000(0.0009) Grad: 46.5449  LR: 0.000011  \n","Epoch: [3][4700/10725] Elapsed 38m 33s (remain 49m 24s) Loss: 0.0053(0.0010) Grad: 18413.1836  LR: 0.000011  \n","Epoch: [3][4800/10725] Elapsed 39m 22s (remain 48m 34s) Loss: 0.0000(0.0010) Grad: 0.9014  LR: 0.000011  \n","Epoch: [3][4900/10725] Elapsed 40m 11s (remain 47m 45s) Loss: 0.0000(0.0010) Grad: 999.5049  LR: 0.000011  \n","Epoch: [3][5000/10725] Elapsed 40m 59s (remain 46m 55s) Loss: 0.0001(0.0010) Grad: 271.5072  LR: 0.000011  \n","Epoch: [3][5100/10725] Elapsed 41m 48s (remain 46m 5s) Loss: 0.0000(0.0010) Grad: 11.0035  LR: 0.000011  \n","Epoch: [3][5200/10725] Elapsed 42m 37s (remain 45m 16s) Loss: 0.0000(0.0010) Grad: 16.1476  LR: 0.000011  \n","Epoch: [3][5300/10725] Elapsed 43m 25s (remain 44m 26s) Loss: 0.0000(0.0010) Grad: 14.1259  LR: 0.000011  \n","Epoch: [3][5400/10725] Elapsed 44m 15s (remain 43m 37s) Loss: 0.0000(0.0010) Grad: 4.7426  LR: 0.000011  \n","Epoch: [3][5500/10725] Elapsed 45m 4s (remain 42m 47s) Loss: 0.0042(0.0010) Grad: 2508.4509  LR: 0.000011  \n","Epoch: [3][5600/10725] Elapsed 45m 52s (remain 41m 58s) Loss: 0.0000(0.0010) Grad: 130.9734  LR: 0.000011  \n","Epoch: [3][5700/10725] Elapsed 46m 41s (remain 41m 9s) Loss: 0.0001(0.0010) Grad: 365.8500  LR: 0.000011  \n","Epoch: [3][5800/10725] Elapsed 47m 30s (remain 40m 19s) Loss: 0.0027(0.0010) Grad: 55016.4023  LR: 0.000011  \n","Epoch: [3][5900/10725] Elapsed 48m 19s (remain 39m 30s) Loss: 0.0000(0.0010) Grad: 27.1097  LR: 0.000011  \n","Epoch: [3][6000/10725] Elapsed 49m 8s (remain 38m 41s) Loss: 0.0001(0.0010) Grad: 355.3752  LR: 0.000011  \n","Epoch: [3][6100/10725] Elapsed 49m 57s (remain 37m 51s) Loss: 0.0000(0.0010) Grad: 94.6589  LR: 0.000011  \n","Epoch: [3][6200/10725] Elapsed 50m 46s (remain 37m 2s) Loss: 0.0000(0.0010) Grad: 1000.3781  LR: 0.000011  \n","Epoch: [3][6300/10725] Elapsed 51m 35s (remain 36m 13s) Loss: 0.0000(0.0010) Grad: 0.5633  LR: 0.000011  \n","Epoch: [3][6400/10725] Elapsed 52m 24s (remain 35m 24s) Loss: 0.0000(0.0010) Grad: 999.8608  LR: 0.000011  \n","Epoch: [3][6500/10725] Elapsed 53m 13s (remain 34m 35s) Loss: 0.0000(0.0010) Grad: 1000.2034  LR: 0.000011  \n","Epoch: [3][6600/10725] Elapsed 54m 2s (remain 33m 45s) Loss: 0.0000(0.0010) Grad: 5.0201  LR: 0.000011  \n","Epoch: [3][6700/10725] Elapsed 54m 51s (remain 32m 56s) Loss: 0.0000(0.0010) Grad: 799.2181  LR: 0.000011  \n","Epoch: [3][6800/10725] Elapsed 55m 40s (remain 32m 7s) Loss: 0.0001(0.0010) Grad: 996.7240  LR: 0.000011  \n","Epoch: [3][6900/10725] Elapsed 56m 29s (remain 31m 18s) Loss: 0.0000(0.0010) Grad: 4.1568  LR: 0.000010  \n","Epoch: [3][7000/10725] Elapsed 57m 18s (remain 30m 29s) Loss: 0.0000(0.0010) Grad: 11.2668  LR: 0.000010  \n","Epoch: [3][7100/10725] Elapsed 58m 7s (remain 29m 39s) Loss: 0.0000(0.0010) Grad: 472.0697  LR: 0.000010  \n","Epoch: [3][7200/10725] Elapsed 58m 56s (remain 28m 50s) Loss: 0.0000(0.0010) Grad: 0.3859  LR: 0.000010  \n","Epoch: [3][7300/10725] Elapsed 59m 46s (remain 28m 1s) Loss: 0.0000(0.0010) Grad: 999.8305  LR: 0.000010  \n","Epoch: [3][7400/10725] Elapsed 60m 35s (remain 27m 12s) Loss: 0.0000(0.0010) Grad: 0.8867  LR: 0.000010  \n","Epoch: [3][7500/10725] Elapsed 61m 24s (remain 26m 23s) Loss: 0.0000(0.0010) Grad: 0.3383  LR: 0.000010  \n","Epoch: [3][7600/10725] Elapsed 62m 13s (remain 25m 34s) Loss: 0.0000(0.0010) Grad: 999.9933  LR: 0.000010  \n","Epoch: [3][7700/10725] Elapsed 63m 2s (remain 24m 45s) Loss: 0.0000(0.0010) Grad: 35.3561  LR: 0.000010  \n","Epoch: [3][7800/10725] Elapsed 63m 51s (remain 23m 56s) Loss: 0.0000(0.0010) Grad: 0.8014  LR: 0.000010  \n","Epoch: [3][7900/10725] Elapsed 64m 40s (remain 23m 7s) Loss: 0.0000(0.0010) Grad: 23.1399  LR: 0.000010  \n","Epoch: [3][8000/10725] Elapsed 65m 29s (remain 22m 17s) Loss: 0.0000(0.0010) Grad: 40.4372  LR: 0.000010  \n","Epoch: [3][8100/10725] Elapsed 66m 18s (remain 21m 28s) Loss: 0.0000(0.0010) Grad: 0.3388  LR: 0.000010  \n","Epoch: [3][8200/10725] Elapsed 67m 7s (remain 20m 39s) Loss: 0.0000(0.0010) Grad: 155.0399  LR: 0.000010  \n","Epoch: [3][8300/10725] Elapsed 67m 56s (remain 19m 50s) Loss: 0.0007(0.0010) Grad: 3380.8743  LR: 0.000010  \n","Epoch: [3][8400/10725] Elapsed 68m 44s (remain 19m 1s) Loss: 0.0018(0.0010) Grad: 50023.8047  LR: 0.000010  \n","Epoch: [3][8500/10725] Elapsed 69m 33s (remain 18m 11s) Loss: 0.0000(0.0010) Grad: 1001.0047  LR: 0.000010  \n","Epoch: [3][8600/10725] Elapsed 70m 22s (remain 17m 22s) Loss: 0.0065(0.0010) Grad: 12524.6230  LR: 0.000010  \n","Epoch: [3][8700/10725] Elapsed 71m 11s (remain 16m 33s) Loss: 0.0010(0.0010) Grad: 459.6477  LR: 0.000010  \n","Epoch: [3][8800/10725] Elapsed 72m 0s (remain 15m 44s) Loss: 0.0000(0.0010) Grad: 996.2277  LR: 0.000010  \n","Epoch: [3][8900/10725] Elapsed 72m 49s (remain 14m 55s) Loss: 0.0000(0.0010) Grad: 69.2831  LR: 0.000010  \n","Epoch: [3][9000/10725] Elapsed 73m 38s (remain 14m 6s) Loss: 0.0000(0.0010) Grad: 18.7263  LR: 0.000010  \n","Epoch: [3][9100/10725] Elapsed 74m 27s (remain 13m 17s) Loss: 0.0014(0.0010) Grad: 26367.2246  LR: 0.000010  \n","Epoch: [3][9200/10725] Elapsed 75m 15s (remain 12m 27s) Loss: 0.0003(0.0010) Grad: 3608.9558  LR: 0.000010  \n","Epoch: [3][9300/10725] Elapsed 76m 4s (remain 11m 38s) Loss: 0.0001(0.0010) Grad: 3504.1331  LR: 0.000009  \n","Epoch: [3][9400/10725] Elapsed 76m 53s (remain 10m 49s) Loss: 0.0000(0.0010) Grad: 18.2911  LR: 0.000009  \n","Epoch: [3][9500/10725] Elapsed 77m 42s (remain 10m 0s) Loss: 0.0001(0.0010) Grad: 525.8981  LR: 0.000009  \n","Epoch: [3][9600/10725] Elapsed 78m 31s (remain 9m 11s) Loss: 0.0000(0.0010) Grad: 95.8552  LR: 0.000009  \n","Epoch: [3][9700/10725] Elapsed 79m 20s (remain 8m 22s) Loss: 0.0000(0.0010) Grad: 57.2713  LR: 0.000009  \n","Epoch: [3][9800/10725] Elapsed 80m 9s (remain 7m 33s) Loss: 0.0000(0.0010) Grad: 895.0818  LR: 0.000009  \n","Epoch: [3][9900/10725] Elapsed 80m 58s (remain 6m 44s) Loss: 0.0036(0.0010) Grad: 3804.8521  LR: 0.000009  \n","Epoch: [3][10000/10725] Elapsed 81m 47s (remain 5m 55s) Loss: 0.0007(0.0010) Grad: 40877.0312  LR: 0.000009  \n","Epoch: [3][10100/10725] Elapsed 82m 36s (remain 5m 6s) Loss: 0.0000(0.0010) Grad: 173.9247  LR: 0.000009  \n","Epoch: [3][10200/10725] Elapsed 83m 25s (remain 4m 17s) Loss: 0.0000(0.0010) Grad: 8.7123  LR: 0.000009  \n","Epoch: [3][10300/10725] Elapsed 84m 14s (remain 3m 28s) Loss: 0.0000(0.0010) Grad: 999.3757  LR: 0.000009  \n","Epoch: [3][10400/10725] Elapsed 85m 3s (remain 2m 38s) Loss: 0.0000(0.0010) Grad: 267.0581  LR: 0.000009  \n","Epoch: [3][10500/10725] Elapsed 85m 53s (remain 1m 49s) Loss: 0.0000(0.0010) Grad: 0.6784  LR: 0.000009  \n","Epoch: [3][10600/10725] Elapsed 86m 41s (remain 1m 0s) Loss: 0.0000(0.0010) Grad: 106.2221  LR: 0.000009  \n","Epoch: [3][10700/10725] Elapsed 87m 30s (remain 0m 11s) Loss: 0.0000(0.0010) Grad: 1012.2416  LR: 0.000009  \n","Epoch: [3][10724/10725] Elapsed 87m 42s (remain 0m 0s) Loss: 0.0000(0.0010) Grad: 21.7260  LR: 0.000009  \n","EVAL: [0/3575] Elapsed 0m 0s (remain 35m 15s) Loss: 0.0000(0.0000) \n","EVAL: [100/3575] Elapsed 0m 21s (remain 12m 11s) Loss: 0.0000(0.0035) \n","EVAL: [200/3575] Elapsed 0m 41s (remain 11m 44s) Loss: 0.0000(0.0032) \n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-30-0f6b8dbff6ab>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi_fold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi_fold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_fold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                 \u001b[0m_oof_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                 \u001b[0moof_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moof_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_oof_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moof_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"oof_df.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-29-7a6be4f46c72>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(df, i_fold, device)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         )\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-27-351228308a52>\u001b[0m in \u001b[0;36mvalid_fn\u001b[0;34m(val_dataloader, model, criterion, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappings_from_token_to_char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-8841cf057069>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, mappings_from_token_to_char)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mmappings_from_token_to_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmappings_from_token_to_char\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappings_from_token_to_char\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# [batch, seq_len, d_model]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 692\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"name":"nbme-exp057.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"papermill":{"default_parameters":{},"duration":641.572862,"end_time":"2022-02-27T11:39:50.972497","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-02-27T11:29:09.399635","version":"2.3.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d2031e5f3510429f977a8accb73623ad":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0f7365a2a54e4f3289867fc78fa5d8f0","IPY_MODEL_4ae813d03aee4dbc9ea70b16e867b8d7","IPY_MODEL_0acd16bc1b774b27b8a1d0027251caf2"],"layout":"IPY_MODEL_6441d4d15bb14c6798ef59080d3efd5b"}},"0f7365a2a54e4f3289867fc78fa5d8f0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f59fe1bf5c8644d5a41d476dffa39d61","placeholder":"​","style":"IPY_MODEL_91205e4ebe844c0c88f5d1b6731ff233","value":"100%"}},"4ae813d03aee4dbc9ea70b16e867b8d7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef1a0ffadb9e438fb08d33c0e093388f","max":42146,"min":0,"orientation":"horizontal","style":"IPY_MODEL_177a811a8f0d4c3abd9c5b034bd33556","value":42146}},"0acd16bc1b774b27b8a1d0027251caf2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ec95d8a55984afb8eb9efd55b921d80","placeholder":"​","style":"IPY_MODEL_77726cf0c093411085c59646885d2b28","value":" 42146/42146 [00:35&lt;00:00, 1916.52it/s]"}},"6441d4d15bb14c6798ef59080d3efd5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f59fe1bf5c8644d5a41d476dffa39d61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91205e4ebe844c0c88f5d1b6731ff233":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef1a0ffadb9e438fb08d33c0e093388f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"177a811a8f0d4c3abd9c5b034bd33556":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3ec95d8a55984afb8eb9efd55b921d80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77726cf0c093411085c59646885d2b28":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac2475c1acb4499eab3945dfa5c56779":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c7e5b695ef7c4acea0b1eb9bd2725a98","IPY_MODEL_0ba1e9bd83154395a37575d2e4a724f9","IPY_MODEL_a9ad179331184ee18e5ef2f6a09dc967"],"layout":"IPY_MODEL_a2a872834ce548658aa1f85dcfc27d48"}},"c7e5b695ef7c4acea0b1eb9bd2725a98":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a6e09e641e84d919d0ea487e738a6a9","placeholder":"​","style":"IPY_MODEL_16ec8142da5f457490909b5d5ecc5a2c","value":"100%"}},"0ba1e9bd83154395a37575d2e4a724f9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0a0ea9a68804c459463520533a6ddb2","max":143,"min":0,"orientation":"horizontal","style":"IPY_MODEL_243f2cea17124685b7275199d0197947","value":143}},"a9ad179331184ee18e5ef2f6a09dc967":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f0eeaa852284fae82e92f09f611c5f6","placeholder":"​","style":"IPY_MODEL_b33cf6ce0b314dc1a403827b3c2f5125","value":" 143/143 [00:00&lt;00:00, 1994.02it/s]"}},"a2a872834ce548658aa1f85dcfc27d48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a6e09e641e84d919d0ea487e738a6a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16ec8142da5f457490909b5d5ecc5a2c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a0a0ea9a68804c459463520533a6ddb2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"243f2cea17124685b7275199d0197947":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9f0eeaa852284fae82e92f09f611c5f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b33cf6ce0b314dc1a403827b3c2f5125":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b7530e27be384f3fbdc650a521b80962":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7d615121ce48438084f98f9dc0c5b663","IPY_MODEL_44edeaff57294f3182ba2f00b3d379a1","IPY_MODEL_046b59dfab82487eaf5e1268f996b018"],"layout":"IPY_MODEL_9059fb0eb80642afbc8c4a6625c853df"}},"7d615121ce48438084f98f9dc0c5b663":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e3f145cc1564614aba29374a43dfe45","placeholder":"​","style":"IPY_MODEL_03f973838c924885b7600241d729d183","value":"100%"}},"44edeaff57294f3182ba2f00b3d379a1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_752ddda179d24fd287fee23f274a1206","max":42146,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e77baa3975c2484595060dbdcad867d4","value":42146}},"046b59dfab82487eaf5e1268f996b018":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_96f479f1c36143b5a0c16de2651c5ecb","placeholder":"​","style":"IPY_MODEL_df31fc87a42b4f5199b5f4b5d02dd64c","value":" 42146/42146 [00:00&lt;00:00, 519145.33it/s]"}},"9059fb0eb80642afbc8c4a6625c853df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e3f145cc1564614aba29374a43dfe45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03f973838c924885b7600241d729d183":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"752ddda179d24fd287fee23f274a1206":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e77baa3975c2484595060dbdcad867d4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"96f479f1c36143b5a0c16de2651c5ecb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df31fc87a42b4f5199b5f4b5d02dd64c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}